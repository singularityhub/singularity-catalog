54r4/sara-server-vre:
  data_format: 2
  description: Virtual Research Environment for Sara Server - container build scripts
  filenames:
  - Singularity
  full_name: 54r4/sara-server-vre
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-sara-server-vre\" class=\"anchor\" href=\"#sara-server-vre\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>sara-server-vre</h1>\n<p>Virtual Research Environment for Sara Server\
    \ - container build scripts</p>\n<p>This is the VRE main spec containing a Java\
    \ Runtime Environment plus Eclipse\nused for the development of the SARA service.\n\
    A local postgres database is integrated, too. The source is a docker repo\nwhich\
    \ is being pulled on build time and used to locally run a postgresql\nserver using\
    \ udocker.\nThis VRE has no external requirements whatsoever once the image has\
    \ been built.</p>\n<h2>\n<a id=\"user-content-use-prebuild-image\" class=\"anchor\"\
    \ href=\"#use-prebuild-image\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Use prebuild image</h2>\n<pre><code>\
    \   cd /tmp\n   singularity pull --name \"sara-server-vre.img\" shub://c1t4r/sara-server-vre\n\
    \   ./sara-server-vre.img\n</code></pre>\n<h2>\n<a id=\"user-content-build-local-image-singularity-23\"\
    \ class=\"anchor\" href=\"#build-local-image-singularity-23\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Build local\
    \ image (Singularity 2.3)</h2>\n<pre><code>cd /tmp\nsingularity create -s 2048\
    \ sara-server-vre.img\nsingularity bootstrap sara-server-vre.img ./Singularity\n\
    ./sara-server-vre.img\n</code></pre>\n<h2>\n<a id=\"user-content-build-local-image-singularity-24\"\
    \ class=\"anchor\" href=\"#build-local-image-singularity-24\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Build local\
    \ image (Singularity 2.4)</h2>\n<pre><code>sudo singularity build sara-server-vre.simg\
    \ ./Singularity\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1546985098.0
AAIR-lab/AIA-AAAI21:
  data_format: 2
  description: 'Code for Asking the Right Questions: Learning Interpretable Action
    Models Through Query Answering. AAAI 2021.'
  filenames:
  - dependencies/FD/misc/releases/19.12/Singularity.19.12
  - dependencies/FD/misc/releases/20.06/Singularity.20.06
  - dependencies/FD/misc/releases/19.06/Singularity.19.06
  - dependencies/FD/misc/releases/latest/Singularity
  full_name: AAIR-lab/AIA-AAAI21
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-agent-interrogation-algorithm-aia\" class=\"\
    anchor\" href=\"#agent-interrogation-algorithm-aia\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Agent Interrogation\
    \ Algorithm (AIA)</h1>\n<p>This repository contains the code for the paper:</p>\n\
    <p>Asking the Right Questions: Learning Interpretable Action Models through Query\
    \ Answering.<br>\n<a href=\"https://pulkitverma.net\" rel=\"nofollow\">Pulkit\
    \ Verma</a>,\n<a href=\"https://marpally-raoshashank.netlify.app/\" rel=\"nofollow\"\
    >Shashank Rao Marpally</a>, and\n<a href=\"http://siddharthsrivastava.net/\" rel=\"\
    nofollow\">Siddharth Srivastava</a>. <br>\n35th AAAI Conference on Artificial\
    \ Intelligence, 2021.</p>\n<p><a href=\"https://aair-lab.github.io/Publications/vms_aaai21.pdf\"\
    \ rel=\"nofollow\">Paper</a> | <a href=\"https://pulkitverma.net/assets/pdf/vms_aaai21/vms_aaai21_slides.pdf\"\
    \ rel=\"nofollow\">Slides</a> | <a href=\"https://pulkitverma.net/assets/pdf/vms_aaai21/vms_aaai21_poster.pdf\"\
    \ rel=\"nofollow\">Poster</a></p>\n<h2>\n<a id=\"user-content-directory-structure\"\
    \ class=\"anchor\" href=\"#directory-structure\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Directory Structure</h2>\n<pre><code>|--\
    \ dependencies/\n|   |-- FD/\n|   |-- FF/\n|   |-- pddlgym/\n|   |-- VAL/\n|--\
    \ domains/\n|-- random_states/\n|-- results/\n|-- src/\n|   |-- agent.py\n|  \
    \ |-- config.py\n|   |-- generate_random_states.py\n|   |-- main.py\n|   |-- interrogation/\n\
    |   |-- lattice/\n|   |-- query/\n|   |-- sim/\n|   |-- utils/\n|-- README.md\n\
    |-- LICENSE\n</code></pre>\n<ul>\n<li>\n<p>dependencies: This directory includes\
    \ the external software used to run the code. This includes FF, FD, VAL, and PDDLGym.</p>\n\
    <ul>\n<li>FF: <a href=\"https://fai.cs.uni-saarland.de/hoffmann/ff/FF-v2.3.tgz\"\
    \ rel=\"nofollow\">https://fai.cs.uni-saarland.de/hoffmann/ff/FF-v2.3.tgz</a>\n\
    </li>\n<li>FD: <a href=\"https://github.com/aibasel/downward\">https://github.com/aibasel/downward</a>\n\
    </li>\n<li>PDDLGym: <a href=\"https://github.com/tomsilver/pddlgym\">https://github.com/tomsilver/pddlgym</a>\n\
    </li>\n<li>VAL: <a href=\"https://github.com/KCL-Planning/VAL\">https://github.com/KCL-Planning/VAL</a>\n\
    </li>\n</ul>\n</li>\n<li>\n<p>dependencies: Place all the domains in this directory.\
    \ There must be a directory for each domain containing:</p>\n<ul>\n<li>domain.pddl\
    \ (domain file for that domain), and</li>\n<li>instances directory containing\
    \ the problem files for that domain.</li>\n</ul>\n</li>\n<li>\n<p>random_states:\
    \ This directory stores the set of states in serialized form. For each domain,\
    \ there is a .pkl file containing 60 states approximately. These are generated\
    \ using src/generate_random_states.py.</p>\n</li>\n<li>\n<p>src: This directory\
    \ stores the source code for AIA. It contains 4 files:</p>\n<ul>\n<li>agent.py:\
    \ Contains the agent code.</li>\n<li>config.py: Declares the configuration parameters.</li>\n\
    <li>generate_random_states.py: Generates the random states for each domain.</li>\n\
    <li>main.py : Contains the main driver code which runs the code end-to-end.</li>\n\
    </ul>\n<p>src also contains code structured into following sub-directories:</p>\n\
    <ul>\n<li>interrogation: Contains the AIA code.</li>\n<li>lattice: Contains the\
    \ model and lattice classes.</li>\n<li>query: Contains the plan outcome query\
    \ code.</li>\n<li>sim: Simulator specific code. Contains a separate agent file\
    \ for each simulator domain.</li>\n<li>utils: Contains general utilities.\n<ul>\n\
    <li>utils/parser: Code based on <a href=\"https://github.com/tomsilver/pddlgym\"\
    >PDDLGym</a>.</li>\n<li>utils/translate: Code based on <a href=\"https://github.com/aibasel/downward\"\
    >FD</a>.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-configuration\"\
    \ class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Configuration</h2>\n<p>Configuration\
    \ parameters are set in src/config.py</p>\n<ul>\n<li>FF_PATH, FD_PATH, and VAL_PATH\
    \ stores the relative path of FF, FD, and VAL respectively.</li>\n<li>NUM_PER_DOMAIN\
    \ denotes how many instances per domain must be run. Keep minimum 2 for symbolic\
    \ agent.</li>\n<li>PLANNER specifies which planner to use. Set it to either FF\
    \ or FD.</li>\n<li>Comment out either Symbolic Agent Settings or Simulator Agent\
    \ Settings.</li>\n</ul>\n<h2>\n<a id=\"user-content-how-to-run\" class=\"anchor\"\
    \ href=\"#how-to-run\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>How to Run</h2>\n<ol>\n<li>Install the required\
    \ software</li>\n</ol>\n<pre><code>pip3 install -r requirements.txt \n</code></pre>\n\
    <ol start=\"2\">\n<li>\n<p>Adjust variables/paramters as needed in src/config.py.</p>\n\
    </li>\n<li>\n<p>Run main.py</p>\n</li>\n</ol>\n<pre><code>cd src\npython3 main.py\n\
    </code></pre>\n<h2>\n<a id=\"user-content-common-installation-issues\" class=\"\
    anchor\" href=\"#common-installation-issues\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Common Installation Issues</h2>\n\
    <ol>\n<li>\n<p>OpenCV (Tested on Ubuntu 18.04)</p>\n<p>Refer to <a href=\"https://linuxize.com/post/how-to-install-opencv-on-ubuntu-18-04/\"\
    \ rel=\"nofollow\">https://linuxize.com/post/how-to-install-opencv-on-ubuntu-18-04/</a></p>\n\
    </li>\n<li>\n<p>FF:</p>\n<ul>\n<li>\n<p>Please install flex and bison for FF to\
    \ compile.</p>\n</li>\n<li>\n<p>On newer versions of gcc (tested on gcc 10.2.0)\
    \ please make the following changes:</p>\n<ul>\n<li>main.c:150 : Comment out the\
    \ gbracket_count definition\n<pre><code>int gbracket_count; --&gt; /* int gbracket_count;\
    \ */\n</code></pre>\n</li>\n<li>relax.c:111 : Define lcurrent_goals as static\n\
    <pre><code>State lcurrent_goals; --&gt; static State lcurrent_goals;\n</code></pre>\n\
    </li>\n<li>search.c:110 : Define lcurrent_goals as static\n<pre><code>State lcurrent_goals;\
    \ --&gt; static State lcurrent_goals;\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n\
    </li>\n</ol>\n<p>Please note that this is research code and not yet ready for\
    \ public delivery,\nhence most parts are not documented.</p>\n<p>In case of any\
    \ queries, please contact <a href=\"mailto:verma.pulkit@asu.edu\">verma.pulkit@asu.edu</a>,\n\
    or <a href=\"mailto:smarpall@asu.edu\">smarpall@asu.edu</a>.</p>\n<h1>\n<a id=\"\
    user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citation</h1>\n\
    <pre><code>@inproceedings{verma_2021_asking,\n    author = {Verma, Pulkit and\
    \ Rao Marpally, Shashank and Srivastava, Siddharth},\n    title = {{Asking the\
    \ Right Questions: Learning Interpretable Action Models Through Query Answering}},\n\
    \    booktitle = {Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)},\n\
    \    year={2021}\n}\n</code></pre>\n"
  stargazers_count: 3
  subscribers_count: 3
  topics: []
  updated_at: 1623121132.0
APSIMInitiative/APSIMClassic:
  data_format: 2
  description: APSIM
  filenames:
  - Release/Containers/Singularity/Singularity.ubuntu
  full_name: APSIMInitiative/APSIMClassic
  latest_release: null
  readme: '<h1>

    <a id="user-content-apsim" class="anchor" href="#apsim" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>APSIM</h1>

    <p>The Agricultural Production Systems sIMulator (APSIM) is internationally recognised
    as a highly advanced simulator of agricultural systems. It contains a suite of
    modules which enable the simulation of systems that cover a range of plant, animal,
    soil, climate and management interactions. APSIM is undergoing continual development,
    with new capability added to regular releases of official versions. Its development
    and maintenance is underpinned by rigorous science and software engineering standards.
    The APSIM Initiative has been established to promote the development and use of
    the science modules and infrastructure software of APSIM.</p>

    <p>CI builds of this repository can be found <a href="https://apsimdev.apsim.info/APSIM.Builds.Portal/Bob.aspx"
    rel="nofollow">Here</a>.</p>

    '
  stargazers_count: 17
  subscribers_count: 17
  topics: []
  updated_at: 1621547465.0
Amjadhpc/PHEnix:
  data_format: 2
  description: This is singularity 2.6.0 image for PHEnix -1.4a
  filenames:
  - Singularity-2.6.0
  full_name: Amjadhpc/PHEnix
  latest_release: null
  readme: '<h1>

    <a id="user-content-game-container" class="anchor" href="#game-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>game-container</h1>

    <p>Containers for game AI</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1539686949.0
Asap7772/railrl_evalsawyer:
  data_format: 2
  description: null
  filenames:
  - docker/Singularity
  - docker/railrl_v6_cuda8/Singularity
  - docker/railrl_v9_cuda10-1_mj1-50-1-59_torch0-4-1_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_v6_cuda9/Singularity
  - docker/railrl_hand_v2/Singularity
  - docker/railrl_hand_v2/Singularity_cpu
  - docker/railrl_v7_cuda8/Singularity
  - docker/railrl_ray/Singularity
  - docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity
  - docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity_cpu
  - docker/railrl_v11_cuda10-1_mj2-0-2-2_torch0-3-1_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_v7/Singularity
  - docker/railrl_hand_v1/Singularity
  - docker/railrl_hand_v1/Singularity_cpu
  - docker/railrl_ray_gym-0-12-0/Singularity_from_scratch
  - docker/railrl_ray_gym-0-12-0/Singularity_from_scratch_cuda8
  - docker/railrl_gpu_mujoco1-5-v4/singularity/Singularity
  - docker/railrl_v8_cuda10-1/Singularity
  - docker/railrl_hand_tf_v1/Singularity
  - docker/railrl_hand_tf_v1/Singularity_cpu
  - docker/railrl_v10_cuda10-1_mj2-0-2-2_torch0-4-1_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_v9-5_cuda10-1_mj1-50-1-59_torch1-1-0_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_hand_v3/Singularity
  - docker/railrl_hand_v3/Singularity_cpu
  - docker/railrl_v5/singularity/Singularity
  - experiments/ashvin/icml2020/singularity/Singularity
  full_name: Asap7772/railrl_evalsawyer
  latest_release: null
  readme: '<p>README last updated on: 01/24/2018</p>

    <h1>

    <a id="user-content-railrl" class="anchor" href="#railrl" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>railrl</h1>

    <p>Reinforcement learning framework.

    Some implemented algorithms:</p>

    <ul>

    <li><a href="examples/ddpg.py">Deep Deterministic Policy Gradient (DDPG)</a></li>

    <li><a href="examples/sac.py">Soft Actor Critic</a></li>

    <li><a href="examples/dqn_and_double_dqn.py">(Double) Deep Q-Network (DQN)</a></li>

    <li><a href="examples/her.py">Hindsight Experience Replay (HER)</a></li>

    <li><a href="examples/model_based_dagger.py">MPC with Neural Network Model</a></li>

    <li>

    <a href="examples/naf.py">Normalized Advantage Function (NAF)</a>

    <ul>

    <li>WARNING: I haven''t tested this NAF implementation much, so it may not match
    the paper''s performance. I''m pretty confident about the other two implementations
    though.</li>

    </ul>

    </li>

    </ul>

    <p>To get started, checkout the example scripts, linked above.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <h3>

    <a id="user-content-some-dependancies" class="anchor" href="#some-dependancies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Some
    dependancies</h3>

    <ul>

    <li><code>sudo apt-get install swig</code></li>

    </ul>

    <h3>

    <a id="user-content-create-conda-env" class="anchor" href="#create-conda-env"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    Conda Env</h3>

    <p>Install and use the included ananconda environment</p>

    <pre><code>$ conda env create -f docker/railrl/railrl-env.yml

    $ source activate railrl-env

    (railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py

    </code></pre>

    <p>Or if you want you can use the docker image included.</p>

    <h3>

    <a id="user-content-download-simulation-env-code" class="anchor" href="#download-simulation-env-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download
    Simulation Env Code</h3>

    <ul>

    <li>

    <a href="https://github.com/vitchyr/multiworld">multiworld</a> (contains environments):<code>git
    clone https://github.com/vitchyr/multiworld</code>

    </li>

    </ul>

    <h3>

    <a id="user-content-optional-install-doodad" class="anchor" href="#optional-install-doodad"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>(Optional)
    Install doodad</h3>

    <p>I recommend installing <a href="https://github.com/justinjfu/doodad">doodad</a>
    to

    launch jobs. Some of its nice features include:</p>

    <ul>

    <li>Easily switch between running code locally, on a remote compute with

    Docker, on EC2 with Docker</li>

    <li>Easily add your dependencies that can''t be installed via pip (e.g. you

    borrowed someone''s code)</li>

    </ul>

    <p>If you install doodad, also modify <code>CODE_DIRS_TO_MOUNT</code> in <code>config.py</code>
    to

    include:</p>

    <ul>

    <li>Path to rllab directory</li>

    <li>Path to railrl directory</li>

    <li>Path to other code you want to juse</li>

    </ul>

    <p>You''ll probably also need to update the other variables besides the docker

    images/instance stuff.</p>

    <h3>

    <a id="user-content-setup-config-file" class="anchor" href="#setup-config-file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup
    Config File</h3>

    <p>You must setup the config file for launching experiments, providing paths to
    your code and data directories. Inside <code>railrl/config/launcher_config.py</code>,
    fill in the appropriate paths. You can use <code>railrl/config/launcher_config_template.py</code>
    as an example reference.</p>

    <p><code>cp railrl/launchers/config-template.py railrl/launchers/config.py</code></p>

    <h2>

    <a id="user-content-visualizing-a-policy-and-seeing-results" class="anchor" href="#visualizing-a-policy-and-seeing-results"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visualizing
    a policy and seeing results</h2>

    <p>During training, the results will be saved to a file called under</p>

    <pre><code>LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;

    </code></pre>

    <ul>

    <li>

    <code>LOCAL_LOG_DIR</code> is the directory set by <code>railrl.launchers.config.LOCAL_LOG_DIR</code>

    </li>

    <li>

    <code>&lt;exp_prefix&gt;</code> is given either to <code>setup_logger</code>.</li>

    <li>

    <code>&lt;foldername&gt;</code> is auto-generated and based off of <code>exp_prefix</code>.</li>

    <li>inside this folder, you should see a file called <code>params.pkl</code>.
    To visualize a policy, run</li>

    </ul>

    <pre><code>(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;/params.pkl

    </code></pre>

    <p>If you have rllab installed, you can also visualize the results

    using <code>rllab</code>''s viskit, described at

    the bottom of <a href="http://rllab.readthedocs.io/en/latest/user/cluster.html"
    rel="nofollow">this page</a></p>

    <p>tl;dr run</p>

    <div class="highlight highlight-source-shell"><pre>python rllab/viskit/frontend.py
    LOCAL_LOG_DIR/<span class="pl-k">&lt;</span>exp_prefix<span class="pl-k">&gt;</span>/</pre></div>

    <h3>

    <a id="user-content-add-paths" class="anchor" href="#add-paths" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Add paths</h3>

    <pre><code>export PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo

    </code></pre>

    <h2>

    <a id="user-content-credit" class="anchor" href="#credit" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credit</h2>

    <p>A lot of the coding infrastructure is based on <a href="https://github.com/rll/rllab">rllab</a>.

    Also, the serialization and logger code are basically a carbon copy.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623985699.0
BU-ISCIII/bacterial_wgs_training:
  data_format: 2
  description: In this training course you will find theory and practice material
    for introducing yourself to wgs analysis for bacterial, including outbreak investigation.
  filenames:
  - Singularity
  full_name: BU-ISCIII/bacterial_wgs_training
  latest_release: ISCIII2018
  readme: '<p><a href="https://circleci.com/gh/BU-ISCIII/bacterial_wgs_training" rel="nofollow"><img
    src="https://camo.githubusercontent.com/0f0038e6bcac192fb10dace1b0ea3475aa34f39f969123b37fdc8730f1f845b0/68747470733a2f2f636972636c6563692e636f6d2f67682f636972636c6563692f636972636c6563692d646f63732e7376673f7374796c653d736869656c64"
    alt="CircleCI Build Status" data-canonical-src="https://circleci.com/gh/circleci/circleci-docs.svg?style=shield"
    style="max-width:100%;"></a> <a href="https://www.gnu.org/licenses/gpl-3.0" rel="nofollow"><img
    src="https://camo.githubusercontent.com/9e54064fb698af20a2b6089b4f16ec3e31f31f72b47f15a5bb215bfd2e41d1b2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d626c75652e737667"
    alt="License: GPL v3" data-canonical-src="https://img.shields.io/badge/License-GPL%20v3-blue.svg"
    style="max-width:100%;"></a> <a href="https://discuss.circleci.com" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4595d2a3dbf792d4810e309e7cf08e0aeecdd155a48934cc46a625ce669280e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6d6d756e6974792d436972636c654349253230446973637573732d3334333433342e737667"
    alt="CircleCi Community" data-canonical-src="https://img.shields.io/badge/community-CircleCI%20Discuss-343434.svg"
    style="max-width:100%;"></a> <a href="http://nextflow.io" rel="nofollow"><img
    src="https://camo.githubusercontent.com/36a03a9b995f400d6adfcfda96e16b0b61f0d0ae8e859aa8acde1162d6517bfe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d253345302e32392e302d677265656e2e737667"
    alt="Nextflow version" data-canonical-src="https://img.shields.io/badge/nextflow-%3E0.29.0-green.svg"
    style="max-width:100%;"></a> <a href="https://sci-f.github.io" rel="nofollow"><img
    src="https://camo.githubusercontent.com/1ee06357ac79da293d08136619bdf903a80f520229e0916813d4a6eca768a963/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f46696c6573797374656d2d536369656e74696669632d627269676874677265656e2e737667"
    alt="Scif" data-canonical-src="https://img.shields.io/badge/Filesystem-Scientific-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-bacterial-wgs-training" class="anchor" href="#bacterial-wgs-training"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bacterial
    WGS training</h1>

    <p>In this training course you will find theory and practice material for introducing
    yourself to wgs analysis for bacterial, including outbreak investigation.</p>

    <p>The material includes slides with theory concepts and a bunch of practical
    exercises using nextflow and singularity, focusing on the interpretation of results.</p>

    <h2>

    <a id="user-content-slides" class="anchor" href="#slides" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Slides</h2>

    <h3>

    <a id="user-content-day-1" class="anchor" href="#day-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Day 1</h3>

    <ul>

    <li>

    <strong>Talk 1:</strong> <a href="slides/talk1/curso_SeqGenBac_session1.1_Introduccion_ICuesta_v3.pdf">Massive
    sequencing of bacterial genomes. State-of-the-art</a>

    </li>

    <li>

    <strong>Talk 2:</strong> <a href="slides/talk4/curso_SeqGenBac_session2.1_aplicaciones_ICuesta.pdf">Bacterial
    genomes sequencing. Applications.</a>

    </li>

    <li>

    <strong>Talk 3:</strong> <a href="slides/talk2/curso_SeqGenBac_session1.2_linux.pdf">Linux
    environment review.</a>

    </li>

    <li>

    <a href="exercises/00_SetUp.md"><strong>Exercise 0</strong></a> -- <a href="exercises/00_Setup.pdf">Download
    pdf</a>

    </li>

    </ul>

    <h3>

    <a id="user-content-day-2" class="anchor" href="#day-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Day 2</h3>

    <ul>

    <li>

    <p><strong>Talk 4:</strong> <a href="slides/talk2/curso_SeqGenBac_session1.2_linux.pdf">Shell
    usage and basic commands.</a>

    <a href="slides/talk3/curso_SeqGenBac_session1.3_ChangingComputingParadigm.pdf">The
    computing revolution in Biosciences. Nextflow and Singularity introduction.</a></p>

    </li>

    <li>

    <p><a href="exercises/01_LinuxNextflowSingularity.md"><strong>Exercise 1</strong></a>
    -- <a href="exercises/01_LinuxNextflowSingularity.pdf">Download pdf</a></p>

    </li>

    <li>

    <p><strong>Talk 5:</strong> <a href="slides/talk5/curso_SeqGenBac_session2.2_quality_assesment.pdf">Quality
    analysis and control of HTS data</a></p>

    </li>

    <li>

    <p><strong>Talk 6:</strong> <a href="slides/talk6/curso_SeqGenBac_session2.3_assembly.pdf">Bacterial
    genomes assembly</a></p>

    </li>

    <li>

    <p><a href="exercises/02_QualityAndAssembly.md"><strong>Exercise 2</strong></a>
    -- <a href="exercises/02_QualityAndAssembly.pdf">Download pdf</a></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-day-3" class="anchor" href="#day-3" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Day 3</h3>

    <ul>

    <li>

    <p><strong>Talk 7:</strong> <a href="slides/talk7/curso_SeqGenBac_session3.1_MappingAndVariantCalling.pdf">Mapping
    against reference genome and Variant Calling.</a></p>

    </li>

    <li>

    <p><strong>Talk 8:</strong> <a href="slides/talk8/curso_SeqGenBac_session3.2_SNPMatrixAndPhylogenetics.pdf">SNP
    matrix and phylogenetics.</a></p>

    </li>

    <li>

    <p><a href="exercises/03_outbreakSNP.md"><strong>Exercise 3</strong></a> -- <a
    href="exercises/03_outbreakSNP.pdf">Download pdf</a></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-day-4" class="anchor" href="#day-4" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Day 4</h3>

    <ul>

    <li>

    <p><strong>Talk 9:</strong> <a href="slides/talk9/curso_SeqGenBac_session4.1_tipificacion-gen-by-gene_ICuesta.pdf">Typing
    based on allelic profile or gene-by-gene</a></p>

    </li>

    <li>

    <p><strong>Talk 10:</strong> <a href="slides/talk10/curso_SeqGenBac_session4.2_GeneByGenevsSNPs_v2.pdf">Gene-by-gene
    WGS analysis</a></p>

    </li>

    <li>

    <p><a href="exercises/04_outbreakcgMLST.md"><strong>Exercise 4</strong></a> --
    <a href="exercises/04_outbreakcgMLST.pdf">Download pdf</a></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-day-5" class="anchor" href="#day-5" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Day 5</h3>

    <ul>

    <li>

    <strong>Talk 11:</strong> <a href="https://github.com/BU-ISCIII/bacterial_wgs_training/blob/master/slides/talk11/curso_SeqGenBac_session5.1_annotation.pdf">Sequence
    annotation</a>

    </li>

    <li>

    <strong>Talk 12:</strong> [Genome characterization, Resistance and Virulence genes]</li>

    <li>

    <a href="exercises/05_annotation.md"><strong>Exercise 5</strong></a> -- <a href="exercises/05_annotation.pdf">Download
    pdf</a>

    </li>

    </ul>

    '
  stargazers_count: 5
  subscribers_count: 8
  topics:
  - wgs
  - genome
  - sequencing
  - bacterial-genomes
  - ngs-analysis
  - outbreak-detection
  - outbreaks
  - nextflow
  - bacterial-wgs-training
  - wgs-analysis
  updated_at: 1624636272.0
BennerLab/pipelines:
  data_format: 2
  description: Lab pipelines using Snakemake + Singularity + SCIF
  filenames:
  - chip-seq.scif/Singularity
  - rna-seq-multisamples/Singularity
  full_name: BennerLab/pipelines
  latest_release: null
  readme: '<p><a href="https://sci-f.github.io" rel="nofollow"><img src="https://camo.githubusercontent.com/a7912c9863e897576e5d434d91e359d254976266bee2f9b1405197941f940bdf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66696c6573797374656d2d736369656e74696669632d626c75652e737667"
    alt="scif" data-canonical-src="https://img.shields.io/badge/filesystem-scientific-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://snakemake.readthedocs.io/en/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/93b6e9faa8e75932017af0ff2ca7db9493cc08e51c462e71143809db606cb04d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d253345253344253230342e362e302d626c75652e737667"
    alt="snakemake" data-canonical-src="https://img.shields.io/badge/snakemake-%3E%3D%204.6.0-blue.svg"
    style="max-width:100%;"></a>

    <a href="http://singularity.lbl.gov/" rel="nofollow"><img src="https://camo.githubusercontent.com/8ed7d71f6eadf7149f22c334c2b29e0479f493cfaced652052e122f59b5920be/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d253345253344253230322e342e322d626c75652e737667"
    alt="singularity" data-canonical-src="https://img.shields.io/badge/singularity-%3E%3D%202.4.2-blue.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-svenner-lab-documentation" class="anchor" href="#svenner-lab-documentation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Svenner
    Lab Documentation</h1>

    <p>This repository contains the Svenner lab pipelines for various types of sequencing
    data. All pipelines are implemented in Snakemake and use the Singularity + Scientific
    Filesystem to create reproducible research environments.</p>

    <h2>

    <a id="user-content-pipelines" class="anchor" href="#pipelines" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pipelines</h2>

    <ul>

    <li><a href="https://github.com/BennerLab/pipelines/tree/master/chip-seq.scif">ChIP-seq
    Pipeline</a></li>

    <li><a href="https://github.com/BennerLab/pipelines/tree/master/rna-seq-multisamples">RNA-seq
    Multisample Pipeline</a></li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1527699137.0
Betterton-Lab/C-GLASS:
  data_format: 2
  description: Open source simulation engine for coarse-grained Brownian dynamics
  filenames:
  - Singularity
  full_name: Betterton-Lab/C-GLASS
  latest_release: v0.2.3
  readme: "<h1>\n<a id=\"user-content-c-glass\" class=\"anchor\" href=\"#c-glass\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>C-GLASS</h1>\n<p>A <strong>C</strong>oarse-<strong>G</strong>rained\
    \ <strong>L</strong>iving <strong>A</strong>ctive <strong>S</strong>ystem <strong>S</strong>imulator</p>\n\
    <p><a href=\"https://travis-ci.com/Betterton-Lab/C-GLASS\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/061b295758d2c64d80b7d3e97ceebc53e21cc632602b7b32c77f046c105d84ac/68747470733a2f2f7472617669732d63692e636f6d2f426574746572746f6e2d4c61622f432d474c4153532e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/Betterton-Lab/C-GLASS.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://doi.org/10.5281/zenodo.3841613\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/96212f14675a05209e745328444de906fc58f72d1794af88b9db02b4fe6f24e5/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333834313631332e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3841613.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"figs/cglass_snapshot.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"figs/cglass_snapshot.png\"\
    \ alt=\"A simulation using C-GLASS\" title=\"A simulation using C-GLASS\" style=\"\
    max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-installation\" class=\"\
    anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>First clone\
    \ the repo, including submodule dependencies.</p>\n<pre><code>git clone --recursive\
    \ https://github.com/Betterton-Lab/C-GLASS\ncd C-GLASS\n</code></pre>\n<p>C-GLASS\
    \ can either be run in a container using Docker or Singularity, or be built from\
    \ source using CMake.</p>\n<h3>\n<a id=\"user-content-running-with-docker\" class=\"\
    anchor\" href=\"#running-with-docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running with Docker</h3>\n<p>A\
    \ pre-built image of C-GLASS is available as a <a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> image. To download the image, run</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>docker pull jeffmm/cglass</pre></div>\n\
    <p>To use the image, run the provided script to launch a Docker container named\
    \ <code>cglass_latest</code> in the background</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>./launch_docker.sh</pre></div>\n<p>You may also build the Docker image yourself\
    \ by providing the launch script with the <code>-b</code> flag.</p>\n<p>To launch\
    \ C-GLASS, run</p>\n<div class=\"highlight highlight-source-shell\"><pre>docker\
    \ <span class=\"pl-c1\">exec</span> cglass_latest cglass.exe [optional-flags]\
    \ [parameter-file]</pre></div>\n<h3>\n<a id=\"user-content-running-with-singularity\"\
    \ class=\"anchor\" href=\"#running-with-singularity\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running with\
    \ Singularity</h3>\n<p>If you are using Singularity, C-GLASS is also available\
    \ as a Singularity image. The command</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull docker://jeffmm/cglass</pre></div>\n<p>will create a local\
    \ file named <code>cglass_latest.sif</code>. You may then run</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ cglass_latest.sif cglass.exe [optional-flags] [parameter-file]</pre></div>\n\
    <p>The Singularity image may also be built locally using the provided recipe in\
    \ the file <code>Singularity</code></p>\n<h3>\n<a id=\"user-content-building-from-source\"\
    \ class=\"anchor\" href=\"#building-from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building from source</h3>\n<p>C-GLASS\
    \ is ready to be built from source using CMake, provided several dependencies\
    \ are installed:</p>\n<ul>\n<li>CMake (version 3.13+)</li>\n<li><a href=\"https://github.com/jbeder/yaml-cpp\"\
    >libyaml-cpp</a></li>\n<li>libgsl-dev</li>\n<li>libopenmpi-dev</li>\n<li>libfftw3-dev</li>\n\
    <li>libboost-math1.67-dev</li>\n</ul>\n<p>Included is a script for building C-GLASS\
    \ with CMake. To build C-GLASS (without graphics or parallelization) run</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./install.sh</pre></div>\n\
    <p>There are additional flags for building with OpenMP, building with graphics,\
    \ installing C-GLASS in <code>/usr/local</code>, etc. To see a menu of options,\
    \ run</p>\n<div class=\"highlight highlight-source-shell\"><pre>./install.sh -h</pre></div>\n\
    <h3>\n<a id=\"user-content-building-with-graphics\" class=\"anchor\" href=\"#building-with-graphics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building with graphics</h3>\n<p>C-GLASS is available with graphics\
    \ for Mac OSX. To install on Mac OSX, you will need the glew and glfw3 libraries,\
    \ both of which can be installed using <a href=\"https://brew.sh/\" rel=\"nofollow\"\
    >Homebrew</a>.</p>\n<div class=\"highlight highlight-source-shell\"><pre>brew\
    \ install glew\nbrew install glfw</pre></div>\n<p>You may also need to help CMake\
    \ find your OpenGL Framework libraries.</p>\n<p>Several other libraries are required\
    \ for running C-GLASS with graphics on Linux or in WSL. See the <code>src/CMakeLists.txt</code>\
    \ file for a comprehensive list of libraries passed to the compiler when building\
    \ C-GLASS with graphics on WSL.</p>\n<h2>\n<a id=\"user-content-running-c-glass\"\
    \ class=\"anchor\" href=\"#running-c-glass\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running C-GLASS</h2>\n<p>The\
    \ C-GLASS executable is run as</p>\n<pre><code>cglass.exe [optional-flags] [parameter-file]\
    \ \n</code></pre>\n<p>The following flags are available:</p>\n<pre><code>--help,\
    \ -h\n    Show the help menu which gives short descriptions about each of the\
    \ flags\n    as well as binary usage\n \n --run-name rname, -r rname \n    Overwrites\
    \ the parameter \"run_name\" with rname which serves as a prefix for\n    all\
    \ outputs \n\n--n-runs num, -n num\n    Overwrites the parameter \"n_runs\" with\
    \ num, which tells the simulation how\n    many times to run the given parameter\
    \ set with different random number\n    generator seeds.\n\n--movie, -m\n    Uses\
    \ the parameters file params_file to load any output files that were\n    generated\
    \ from previous runs of the simulation to replay the graphics and\n    record\
    \ the frames as bitmaps into the directory specified with the\n    \"movie_directory\"\
    \ parameter\n\n--analysis, -a\n    Loads posit/spec files into the simulation\
    \ for analysis in the same manner\n    as the movie flag\n\n-reduce reduce_factor,\
    \ -R reduce_factor\n    Reads in output files and writes new output files that\
    \ are smaller by a\n    factor of reduce_factor, effectively reducing time resolution\
    \ of output\n    data.\n\n--load, -l\n    Specifies to load any checkpoint files\
    \ corresponding to the given parameter\n    file, which can be used to continue\
    \ a simulation that ended prematurely.\n    New simulation will be given the name\
    \ old_simulation_name_reload00n where n\n    is the number of reloads performed\
    \ on that simulation.\n\n--with-reloads, -w\n    If running analyses or making\
    \ movies, C-GLASS will look for parameter files\n    that have the same run name\
    \ but with the reload00n addendum and attempt to\n    open the corresponding output\
    \ files whenever it reached EOF while reading\n    an output file.\n\n--blank,\
    \ -b\n    Generates all relevant parameter files using the SimulationManager without\n\
    \    running the simulations. Useful for generating many parameter files from\n\
    \    parameter sets (discussed below) and deploying simulations on different\n\
    \    processors and/or machines.\n\n--auto-graph, -G\n    By default, C-GLASS\
    \ will wait for the user to press the ESC key in the\n    OpenGL graphics window\
    \ before starting to run the simulation. Providing\n    this flag will cause the\
    \ simulation to begin immediately without user\n    input. Goes great with the\
    \ -m flag for creating multiple movies without\n    input from the user.\n</code></pre>\n\
    <h2>\n<a id=\"user-content-parameter-files\" class=\"anchor\" href=\"#parameter-files\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Parameter files</h2>\n<p>All parameters used in the simulation, along\
    \ with their default values and data types, are specified in the <code>default_config.yaml</code>\
    \ file in the <code>config</code> folder.</p>\n<p>The parameter file is a YAML\
    \ file and looks like:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span\
    \ class=\"pl-ent\">global_param_1</span>: <span class=\"pl-s\">gp1_value</span>\n\
    <span class=\"pl-ent\">global_param_2</span>: <span class=\"pl-s\">gp2_value</span>\n\
    <span class=\"pl-ent\">species</span>:\n    <span class=\"pl-ent\">global_species_param_1</span>:\
    \ <span class=\"pl-s\">gsp1_value</span>\n    <span class=\"pl-ent\">global_species_param_2</span>:\
    \ <span class=\"pl-s\">gsp2_value</span>\n<span class=\"pl-ent\">specific_species_name</span>:\n\
    \    <span class=\"pl-ent\">species_param_1</span>: <span class=\"pl-s\">sp1_value</span>\n\
    \    <span class=\"pl-ent\">species_param_2</span>: <span class=\"pl-s\">sp2_value</span></pre></div>\n\
    <p>See the <code>examples</code> folder for examples of parameter files.</p>\n\
    <p>Notice that there are three parameter types: global parameters, global species\
    \ parameters, and species parameters. Global parameters are parameters that are\
    \ common to the entire system, such system size, integration time step, etc. Species\
    \ parameters are unique to the specified species, such as <code>filament</code>.\
    \ There is also an optional global species parameter type that affects every species,\
    \ such as the frequency to write to output files.</p>\n<p>What do I mean by species?\
    \ C-GLASS assumes that any given simulation will likely have many copies of one\
    \ kind of thing, which I call a species, perhaps interacting with other species\
    \ of other kinds. In a system of interacting spheres, the species is 'sphere.'\
    \ In a system of interacting semiflexible filaments, the species is 'filament.'\
    \ Simulations can have many types of species all interacting with each other with\
    \ different species-species interaction potentials.</p>\n<p>If any parameter is\
    \ not specified in the parameter file, any instance of that parameter in the simulation\
    \ will assume its default value specified in the <code>config/default_config.yaml</code>\
    \ file.</p>\n<p>Some important global parameters are:</p>\n<pre><code>seed\n \
    \   simulation seed to use with random number generator \nrun_name\n    prefix\
    \ for all output files\nn_runs\n    number of individual runs of each parameter\
    \ type\nn_random\n    number of samples from a random parameter space (see more\
    \ below)\nn_dim\n    number of dimensions of simulation\nn_periodic\n    number\
    \ of periodic dimensions of simulation\ndelta   \n    simulation time step\nn_steps\n\
    \    total number of steps in each simulation\nsystem_radius\n    \"box radius\"\
    \ of system\ngraph_flag\n    run with graphics enabled\nn_graph\n    how many\
    \ simulation steps to take between updating graphics\nmovie_flag\n    whether\
    \ to record the graphics frames into bitmaps\nmovie_directory\n    local directory\
    \ used to save the recorded bitmaps\nthermo_flag\n    whether to output thermodynamics\
    \ outputs (stress tensors, etc)\nn_thermo\n    how often to output the thermodynamics\
    \ outputs\npotential_type\n    can be 'wca' or 'soft' for now\n</code></pre>\n\
    <p>Some important global species parameters are:</p>\n<pre><code>num\n    how\
    \ many to insert into system\ninsertion_type\n    how to insert object into system\
    \ (e.g. random)\noverlap\n    whether species can overlap at initiation\ndraw_type\n\
    \    (orientation, fixed, or bw) how to color the object\ncolor\n    a double\
    \ that specifies the RGB value of the object\nposit_flag\n    whether to output\
    \ position files\nn_posit\n    how often to output position files\nspec_flag\n\
    \    whether to output species files\nn_spec\n    how often to output species\
    \ files\ncheckpoint_flag\n    whether to output checkpoint files\nn_checkpoint\n\
    \    how often to output checkpoint files\n</code></pre>\n<h3>\n<a id=\"user-content-anchor-parameters\"\
    \ class=\"anchor\" href=\"#anchor-parameters\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Anchor parameters</h3>\n<p>C-GLASS\
    \ has the capability to independently control crosslinker and motor protein anchor\
    \ parameters. Anchor parameters are controlled within the Crosslink map in the\
    \ input Yaml file:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span\
    \ class=\"pl-ent\">Crosslink</span>:\n  <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> other crosslink params here</span>\n  <span class=\"pl-ent\">Anchors</span>:\n\
    \     - <span class=\"pl-ent\">velocity_s</span>: <span class=\"pl-c1\">50</span>\n\
    \       <span class=\"pl-ent\">color</span>: <span class=\"pl-c1\">3.5</span>\n\
    \     - <span class=\"pl-ent\">color</span>: <span class=\"pl-c1\">4.5</span></pre></div>\n\
    <p>Only two anchors are permitted per crosslinker or motor protein. The anchor\
    \ parameters obey the following rules when parameters are left blank:</p>\n<ul>\n\
    <li>If no anchors are listed, the anchor parameters will both be set to default.</li>\n\
    <li>If one anchor is listed, the other anchor will copy its parameters. Any unlisted\
    \ parameters will be set to default.</li>\n<li>If two anchors are listed, and\
    \ one anchor has a parameter that the other doesn't, the one that doesn't have\
    \ the parameter will copy the parameter from the other.</li>\n</ul>\n<p>In the\
    \ above example, Anchor 1 will have velocity_s=50, velocity_d=0 (default), color=3.5,\
    \ and Anchor 2 will have velocity_s=50 (copied), velocity_d=0 (default), color=4.5.</p>\n\
    <h2>\n<a id=\"user-content-advanced-usage\" class=\"anchor\" href=\"#advanced-usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Advanced usage</h2>\n<h3>\n<a id=\"user-content-running-unit-tests\"\
    \ class=\"anchor\" href=\"#running-unit-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running unit tests</h3>\n<p>One\
    \ may run C-GLASS's unit tests by passing <code>-DTESTS=TRUE</code> to CMake</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>mkdir build\n<span class=\"\
    pl-c1\">cd</span> build\ncmake -DTESTS=TRUE ..\nmake\nmake <span class=\"pl-c1\"\
    >test</span></pre></div>\n<h3>\n<a id=\"user-content-adding-new-parameters\" class=\"\
    anchor\" href=\"#adding-new-parameters\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Adding new parameters</h3>\n\
    <p>C-GLASS comes with it's own parameter initialization tool, <code>configure_C-GLASS.exe</code>,\
    \ which is installed automatically along with the C-GLASS binary using CMake.\
    \ The configurator makes it easy to add new parameters to the simulation without\
    \ mucking around in the source code. Just add your new parameter to <code>config/default_config.yaml</code>\
    \ file using the following format:</p>\n<pre><code>new_parameter_name: [default_parameter_value,\
    \ parameter_type] \n</code></pre>\n<p>Then run the configurator using</p>\n<pre><code>./configure_cglass.exe\
    \ config/default_config.yaml\n</code></pre>\n<p>Running configure_cglass.exe will\
    \ look at all the parameters in the default config file and add them seamlessly\
    \ to the proper C-GLASS headers, and you can begin using them after recompiling\
    \ C-GLASS using CMake.</p>\n<h3>\n<a id=\"user-content-parameter-sets\" class=\"\
    anchor\" href=\"#parameter-sets\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Parameter sets</h3>\n<p>Using parameter\
    \ sets, it becomes easier to run many simulations over a given parameter space.\
    \ There are two types of parameter sets possible with C-GLASS: defined and random.\
    \ Each parameter set type works the same with both global parameters and species\
    \ parameters.</p>\n<h4>\n<a id=\"user-content-defined-parameter-sets\" class=\"\
    anchor\" href=\"#defined-parameter-sets\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Defined parameter sets</h4>\n\
    <p>Defined parameter sets are specified by the <code>V</code> prefix in the parameter\
    \ file:</p>\n<pre><code>seed: 4916819461895\nrun_name: defined_set\nn_runs: N\n\
    parameter_name1: param_value1\nparameter_name2: [V, param_value2, param_value3]\n\
    parameter_name3: [V, param_value4, param_value5]\n</code></pre>\n<p>Parameters\
    \ specified in this way (as lists of parameters) will be iterated over until every\
    \ possible combination of parameters has been run. In this example, C-GLASS will\
    \ run N simulations each of the following 4 parameter sets:</p>\n<pre><code>seed:\
    \ random_seed_1\nrun_name: defined_set_v000\nn_runs: N\nparameter_name1: param_value1\n\
    parameter_name2: param_value2\nparameter_name3: param_value4\n\nseed: random_seed_2\n\
    run_name: defined_set_v001\nn_runs: N\nparameter_name1: param_value1\nparameter_name2:\
    \ param_value2\nparameter_name3: param_value5\n\nseed: random_seed_3\nrun_name:\
    \ defined_set_v002\nn_runs: N\nparameter_name1: param_value1\nparameter_name2:\
    \ param_value3\nparameter_name3: param_value4\n\nseed: random_seed_4\nrun_name:\
    \ defined_set_v003\nn_runs: N\nparameter_name1: param_value1\nparameter_name2:\
    \ param_value3\nparameter_name3: param_value5\n</code></pre>\n<h4>\n<a id=\"user-content-random-parameter-sets\"\
    \ class=\"anchor\" href=\"#random-parameter-sets\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Random parameter\
    \ sets</h4>\n<p>Random parameter sets are designed specifically to be used with\
    \ polynomial-chaos theory for n-dimensional parameter spaces for large n. Random\
    \ sets are used in the following way:</p>\n<pre><code>seed: 2546954828254\nn_runs:\
    \ N\nn_random: M\nparameter_name1: param_value1\nparameter_name2: [R, A, B] #\
    \ sets to random real in range (A,B)\nparameter_name3: [RINT, C, D] # sets to\
    \ random int in range [C,D]\nparameter_name4: [RLOG, F, G] # sets to 10^K for\
    \ rand real K in range (F,G)\n</code></pre>\n<p>Given this parameter file, C-GLASS\
    \ will run N simulations each of M random parameter sets. The random parameter\
    \ sets are generated in ranges specified in the lists that are prefixed by the\
    \ R, RINT, RLOG options.</p>\n<p>In this example, the sampled parameter space\
    \ has dimensionality of n=3, since there are only three parameters we are sampling\
    \ over. Each parameter set will have a random real number for parameter_name2\
    \ in the the range (A,B), a random integer in the range [C,D] for parameter_name3,\
    \ and will set parameter_name4 to 10^K for random real number K in the range (F,G).\
    \  C-GLASS will then run each parameter set N times each with a unique seed, and\
    \ repeat this random process M times. It will therefore take N samples of M random\
    \ points in the n-dimensional parameter space.</p>\n<h3>\n<a id=\"user-content-interactions\"\
    \ class=\"anchor\" href=\"#interactions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Interactions</h3>\n<p>The Interaction\
    \ Manager in C-GLASS was written with short-range interactions in mind. For this\
    \ reason, interactions are treated by considering pair-wise interactions between\
    \ neighboring interactor-elements that make up a composite object (e.g. small,\
    \ rigid segments that compose a flexible filament). For this reason, interactions\
    \ use cell lists to improve performance. Furthermore, simulating large objects\
    \ in C-GLASS requires representing the object as a composite of smaller, simple\
    \ objects. An example of how a large object should be decomposed into simple objects\
    \ is done in the Filament class.</p>\n<h3>\n<a id=\"user-content-potentials\"\
    \ class=\"anchor\" href=\"#potentials\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Potentials</h3>\n<p>C-GLASS is\
    \ designed to be able to use interchangable potentials for various objects. However,\
    \ potentials need to be added manually as a subclass of PotentialBase, included\
    \ in PotentialManager, and a corresponding potential_type added to definitions.h\
    \ for lookup purposes (see the InitPotentials method in PotentialManager.h for\
    \ examples).</p>\n<h3>\n<a id=\"user-content-outputs\" class=\"anchor\" href=\"\
    #outputs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Outputs</h3>\n<p>C-GLASS has four output types. Three are species\
    \ specific (posit, spec, checkpoint), and the fourth is the statistical information\
    \ file (thermo). All files are written in binary.</p>\n<p>The posit file has the\
    \ following header format:</p>\n<pre><code>int n_steps, int n_posit, double delta\
    \ \n</code></pre>\n<p>Followed by n_steps/n_posit lines of data with the format:</p>\n\
    <pre><code>double position[3]\ndouble scaled_position[3]\ndouble orientation[3]\n\
    double diameter\ndouble length\n</code></pre>\n<p>Where the scaled position is\
    \ position mapped into the periodic coordinate space. The position itself gives\
    \ the particle trajectory over time independent of periodicity.</p>\n<p>The spec\
    \ file is a custom output file for each species, and can have the same information\
    \ as the posit file or additional information if needed.</p>\n<p>The checkpoint\
    \ file is almost a copy of the spec file, except it also contains the random number\
    \ generator information and is overwritten every n_checkpoint steps in the simulation.\
    \ It can therefore be used to resume a simulation that ended prematurely.</p>\n\
    <p>The thermo file contains the following header information:</p>\n<pre><code>int\
    \ n_steps, int n_thermo, double delta, int n_dim\n</code></pre>\n<p>followed by\
    \ n_steps/n_thermo lines of data in the following format:</p>\n<pre><code>double\
    \ unit_cell[9]\ndouble pressure_tensor[9]\ndouble pressure\ndouble volume\n</code></pre>\n\
    <p>Where the pressure is the isometric pressure, and the pressure tensor is calculated\
    \ from the time-averaged stress tensor.</p>\n<h3>\n<a id=\"user-content-data-analysis\"\
    \ class=\"anchor\" href=\"#data-analysis\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Data analysis</h3>\n<p>If analysis\
    \ operations of output files are already defined for your species, as is the case\
    \ for the Filament species, analyzing outputs is a simple matter. First, make\
    \ sure the desired analysis flag is set in the species parameters for that species.</p>\n\
    <p>For example, in the Filament species there is a persistence length analysis\
    \ that produces .mse2e files that tracks the mean-square end-to-end distance of\
    \ semiflexible filaments. This is triggered by a parameter lp_analysis=1, which\
    \ can be set in the parameter file.</p>\n<p>Anaylses are run by running C-GLASS\
    \ in the following way:</p>\n<pre><code>cglass.exe -a parameter_file.yaml.\n</code></pre>\n\
    <p>NOTE: It is important to keep in mind that the parameter_file should be identical\
    \ to the parameter file used to generate the outputs. There are a few exceptions\
    \ that only affect post-processing, such as analysis flags, but this is true in\
    \ general.</p>\n<p>The way inputs and outputs are meant to work in C-GLASS is\
    \ such that during a simulation, output data are generated in the posit, spec,\
    \ and checkpoint formats, and during analysis, the same output data are read back\
    \ into the data structures in C-GLASS for processing. The .posit files just contain\
    \ bare-bones information that allow many types of simple analyses, but .spec files\
    \ should in general contain all the necessary information to recreate the trajectory\
    \ for a member of a species.</p>\n<p>For a new species analysis method, the analysis\
    \ routines should be defined in the species container class, rather than the species\
    \ member class, and called by the inherited RunAnalysis method of the SpeciesBase\
    \ class (and likewise for analysis initialization and finalization, see examples\
    \ for details).</p>\n<p>For example, the RunSpiralAnalysis routine is called by\
    \ the RunAnalysis method in FilamentSpecies, which uses the Filament .spec file\
    \ as an input to do the necessary analysis, whose results are placed into a new\
    \ file ending in filament.spiral. See Filament and FilamentSpecies for examples\
    \ of how analyses can be initialized, processed, etc.</p>\n<h2>\n<a id=\"user-content-directory-structure\"\
    \ class=\"anchor\" href=\"#directory-structure\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Directory structure</h2>\n<p>The\
    \ directory structure is as follows:</p>\n<pre><code>C-GLASS\n\u251C\u2500\u2500\
    \ include\n\u2502   \u2514\u2500\u2500 cglass\n\u2502       \u2514\u2500\u2500\
    \ (header files)\n\u251C\u2500\u2500 src\n\u2502   \u251C\u2500\u2500 CMakeLists.txt\n\
    \u2502   \u251C\u2500\u2500 executable\n\u2502   \u2502   \u251C\u2500\u2500 CMakeLists.txt\n\
    \u2502   \u2502   \u2514\u2500\u2500 cglass_main.cpp\n\u2502   \u251C\u2500\u2500\
    \ configurator\n\u2502   \u2502   \u251C\u2500\u2500 CMakeLists.txt\n\u2502  \
    \ \u2502   \u2514\u2500\u2500 configurator.cpp\n\u2502   \u2514\u2500\u2500 (source\
    \ files)\n\u251C\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 default_config.yaml\n\
    \u251C\u2500\u2500 analysis\n\u2502   \u2514\u2500\u2500 (Python analysis files)\n\
    \u251C\u2500\u2500 scripts\n\u2502   \u2514\u2500\u2500 (utility files)\n\u251C\
    \u2500\u2500 examples\n\u2502   \u2514\u2500\u2500 (parameter file examples)\n\
    \u251C\u2500\u2500 docker\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251C\u2500\
    \u2500 extern\n\u2502   \u2514\u2500\u2500 KMC\n\u251C\u2500\u2500 tests\n\u2502\
    \   \u251C\u2500\u2500 CMakeLists.txt\n\u2502   \u251C\u2500\u2500 catch2\n\u2502\
    \   \u2502   \u2514\u2500\u2500 catch.hpp\n\u2502   \u2514\u2500\u2500 (C-GLASS\
    \ unit tests)\n\u251C\u2500\u2500 docs\n\u2502   \u251C\u2500\u2500 CMakeLists.txt\n\
    \u2502   \u2514\u2500\u2500 main.md\n\u251C\u2500\u2500 figs\n\u2502   \u2514\u2500\
    \u2500 (example simulation figures)\n\u251C\u2500\u2500 README.md\n\u251C\u2500\
    \u2500 LICENSE\n\u251C\u2500\u2500 CMakeLists.txt\n\u251C\u2500\u2500 install.sh\n\
    \u251C\u2500\u2500 launch_docker.sh\n\u251C\u2500\u2500 .travis.yml\n\u2514\u2500\
    \u2500 .gitignore\n</code></pre>\n<h2>\n<a id=\"user-content-about-c-glass\" class=\"\
    anchor\" href=\"#about-c-glass\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>About C-GLASS</h2>\n<p>C-GLASS is\
    \ written in C++ and designed for general coarse-grained physics simulations of\
    \ active living matter, produced with modularity and scalability in mind. All\
    \ objects in the simulation are representable as a composite of what I call \"\
    simple\" objects (points, spheres, rigid cylinders, and 2d polygon surfaces would\
    \ all qualify). For short-range interactions, C-GLASS uses cell and neighbor lists\
    \ for improved performance and OpenMP for parallelization.</p>\n<h2>\n<a id=\"\
    user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>This software is licensed under the terms of the BSD-3 Clause license. See\
    \ the <code>LICENSE</code> for more details.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1624041463.0
BiomedicalMachineLearning/HEMnet:
  data_format: 2
  description: 'A neural network software for using Molecular labelling to improve
    pathological annotation of H and E tissues '
  filenames:
  - Environments/Singularity.cpu
  full_name: BiomedicalMachineLearning/HEMnet
  latest_release: v1.0.0
  readme: "<p><a href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\"\
    \ alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HEMnet - Haematoxylin &amp; Eosin and Molecular neural network</h1>\n\
    <h2>\n<a id=\"user-content-description\" class=\"anchor\" href=\"#description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Description</h2>\n<p>A deep learning automated cancer diagnosis software\
    \ using molecular labelling to improve pathological annotation of\nHaematoxylin\
    \ and Eosin (H&amp;E) stained tissue.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<ol>\n<li>\n\
    <p>Docker</p>\n<p>You can download and run the docker image using the following\
    \ commands:</p>\n<pre><code>docker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\
    </code></pre>\n</li>\n<li>\n<p>Conda</p>\n<p>Install Openslide (this is necessary\
    \ to open whole slide images) - download it <a href=\"https://openslide.org/download/\"\
    \ rel=\"nofollow\">here</a></p>\n<p>Create a conda environment from the <code>environment.yml</code>\
    \ file</p>\n<pre><code>conda env create -f environment.yml\nconda activate HEMnet\n\
    </code></pre>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-slide-preparation\"\
    \ class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Slide Preparation</h3>\n<p>Name\
    \ slides in the format: <code>slide_id_TP53</code> for TP53 slides and <code>slide_id_HandE</code>\
    \ for H&amp;E slides\nThe <code>TP53</code> and <code>HandE</code> suffix is used\
    \ by HEMnet to identify the stain used.</p>\n<h3>\n<a id=\"user-content-1-generate-training-and-testing-datasets\"\
    \ class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.\
    \ Generate training and testing datasets</h3>\n<p>a. Generate train dataset</p>\n\
    <p><code>python HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides\
    \ -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs\
    \ -v</code></p>\n<p>b. Generate test dataset</p>\n<p><code>python HEMnet_test_dataset.py\
    \ -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory\
    \ -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh\
    \ -n non_cancer_thresh</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is the relative path to the template slide from which all other slides will\
    \ be normalised against. The template\nslide should be the same for each step.</li>\n\
    <li>\n<code>-m</code> is the tile magnification. e.g. if  the input is <code>10</code>\
    \ then the tiles will be output at 10x</li>\n<li>\n<code>-a</code> is the align\
    \ magnification. Paired TP53 and H&amp;E slides will be registered at this magnification.\n\
    To reduce computation time we recommend this be less than the tile magnification\
    \ - a five times downscale generally works well.</li>\n<li>\n<code>-c</code> cancer\
    \ threshold to apply to the DAB channel. DAB intensities less than this threshold\
    \ indicate cancer.</li>\n<li>\n<code>-n</code> non-cancer threshold to apply to\
    \ the DAB channel. DAB intensities greater than this threshold indicate no cancer.</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\"\
    \ href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2. Train and evaluate model</h3>\n\
    <p>a. Training model</p>\n<p><code>python train.py -b /path/to/base/directory\
    \ -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory\
    \ -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size\
    \ -s -w -f -v</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-m</code>\
    \ is CNN base model. eg. <code>resnet50</code>, <code>vgg16</code>, <code>vgg19</code>,\
    \ <code>inception_v3</code> and <code>xception</code>.</li>\n<li>\n<code>-g</code>\
    \ is number of GPUs for training.</li>\n<li>\n<code>-e</code> is training epochs.\
    \ Default is <code>100</code> epochs.</li>\n<li>\n<code>-a</code> is batch size.\
    \ Default is <code>32</code>\n</li>\n<li>\n<code>-s</code> is option to save the\
    \ trained model weights.</li>\n<li>\n<code>-w</code> is option to used transfer\
    \ learning. Model will used pre-trained weights from ImageNet at the initial stage.</li>\n\
    <li>\n<code>-f</code> is fine-tuning option. Model will re-train CNN base.</li>\n\
    </ul>\n<p>b. Test model prediction</p>\n<p><code>python test.py  -b /path/to/base/directory\
    \ -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory\
    \ -w model_weights -m cnn_base -g num_gpus -v</code></p>\n<p>Other parameters:</p>\n\
    <ul>\n<li>\n<code>-w</code> is path to trained model. eg. <code>trained_model.h5</code>.</li>\n\
    <li>\n<code>-m</code> is CNN base model (same to training step).</li>\n<li>\n\
    <code>-g</code> is number of GPUs for prediction.</li>\n</ul>\n<p>c. Evaluate\
    \ model performance and visualise model prediction</p>\n<p><code>python visualisation.py\
    \ -b /path/to/base/directory -t /relative/path/to/training_output_directory -p\
    \ /relative/path/to/test_output_directory  -o /relative/path/to/output/directory\
    \ -i sample</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is path to training outputs.</li>\n<li>\n<code>-p</code> is path to test outputs.</li>\n\
    <li>\n<code>-i</code> is name of Whole Slide Image for visualisation.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\"\
    \ href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3. Apply model to diagnose new\
    \ images</h3>\n<p><code>python HEMnet_inference.py -s '/path/to/new/HE/Slides/'\
    \ -o '/path/to/output/directory/' -t '/path/to/template/slide/' -nn '/path/to/trained/model/'\
    \ -v</code></p>\n<p>Predict on TCGA images with our pretrained model for colorectal\
    \ cancer using <a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\">google colab</a></p>\n<h2>\n<a id=\"user-content-results\"\
    \ class=\"anchor\" href=\"#results\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Results</h2>\n<h2>\n<a id=\"\
    user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citing\
    \ HEMnet</h2>\n<h2>\n<a id=\"user-content-the-team\" class=\"anchor\" href=\"\
    #the-team\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>The Team</h2>\n<p>Please contact Dr Quan Nguyen (<a href=\"mailto:quan.nguyen@uq.edu.au\"\
    >quan.nguyen@uq.edu.au</a>), Andrew Su (<a href=\"mailto:a.su@uqconnect.edu.au\"\
    >a.su@uqconnect.edu.au</a>),\nand Xiao Tan (<a href=\"mailto:xiao.tan@uqconnect.edu.au\"\
    >xiao.tan@uqconnect.edu.au</a>) for issues, suggestions,\nand we are very welcome\
    \ to collaboration opportunities.</p>\n"
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1624098942.0
BrendelGroup/BWASP:
  data_format: 2
  description: Bisulfite-seq data Workflow Automation Software and Protocols
  filenames:
  - Singularity.v1.1
  - Singularity.v0.9
  - Singularity
  - Singularity.v1.0
  full_name: BrendelGroup/BWASP
  latest_release: null
  readme: '<h1>

    <a id="user-content-bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols"
    class="anchor" href="#bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BWASP
    : Bisulfite-seq data Workflow Automation Software and Protocols</h1>

    <p>The BWASP repository encompasses code and scripts developed in the

    <a href="http://brendelgroup.org/" rel="nofollow">Brendel Group</a> for analyses
    of bisulfite sequencing

    data.

    The entire workflow relies on various other open source software as well as

    <a href="https://www.r-project.org/" rel="nofollow">R</a> scripts from the companion

    <a href="https://github.com/BrendelGroup/BWASPR">BWASPR</a> repository.

    The code conforms to our <a href="https://brendelgroup.github.io/" rel="nofollow">RAMOSE</a>

    philosophy: it generates <strong>reproducible</strong>, <strong>accurate</strong>,
    and <strong>meaningful</strong>

    results; it is <strong>open</strong> (source) and designed to be <strong>scalable</strong>
    and

    <strong>easy</strong> to use.</p>

    <h2>

    <a id="user-content-quick-start-" class="anchor" href="#quick-start-" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start <a href="https://singularity-hub.org/collections/1203"
    rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    </h2>

    <p>Input to the BWASP workflow consists of accession numbers or fastq files of

    bisulfite-sequencing reads as well as the appropriate genome assembly (and, if

    available, genome annotation).

    Output (after read quality control and mapping) are <em>*.mcalls</em> files that
    list

    the sufficiently covered genomic Cs and their methylation percentage in the

    given sample.

    The scripts in the <em>bin</em> directory take care of minor tasks in the overall

    workflow, but configuration and execution is via

    <a href="https://www.gnu.org/software/make/" rel="nofollow">GNU make</a> using
    edited copies of the

    makefiles provided in the <em>makefiles</em> directory.

    All the BWASP dependencies are encapsulated in a

    <a href="https://www.sylabs.io/docs/" rel="nofollow">Singularity</a> container
    available from our

    <a href="http://BrendelGroup.org/SingularityHub/" rel="nofollow">Singularity Hub</a>.

    Thus, once you know what you are doing, execution could be as simple as</p>

    <pre><code>singularity pull http://BrendelGroup.org/SingularityHub/bwasp.sif

    singularity exec bwasp.sif make

    </code></pre>

    <p>(assuming you have prepared a suitable makefile in your working directory).</p>

    <h2>

    <a id="user-content-realistic-start" class="anchor" href="#realistic-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Realistic Start</h2>

    <p>Please find detailed installation instructions and options in the

    <a href="./INSTALL.md">INSTALL</a> document.

    Once all preparatory steps are taken care of, see the <a href="./HOWTO.md">HOWTO</a>

    document for a complete example of how to implement and run a workflow.</p>

    <h2>

    <a id="user-content-reference" class="anchor" href="#reference" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Reference</h2>

    <p>Amy L. Toth, Murat Ozturk, Saranya Sankaranarayanan, and Volker P. Brendel

    (2018) <em>Estimating the size and dynamics of the CpG methylome of social

    insects.</em> To be submitted.</p>

    <h2>

    <a id="user-content-contact" class="anchor" href="#contact" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

    <p>Please direct all comments and suggestions to

    <a href="mailto:vbrendel@indiana.edu">Volker Brendel</a>

    at <a href="http://brendelgroup.org/" rel="nofollow">Indiana University</a>.</p>

    '
  stargazers_count: 2
  subscribers_count: 5
  topics: []
  updated_at: 1624742255.0
CAIsr/qsm:
  data_format: 2
  description: This docker and singularity image bundles the tgv-qsm algorithm with
    bet2, dcm2niix and provides a complete QSM processing pipeline.
  filenames:
  - Singularity.tgvqsm_amd
  - Singularity.tgvqsm
  full_name: CAIsr/qsm
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-qsm-pipeline\" class=\"anchor\" href=\"#qsm-pipeline\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>QSM Pipeline</h1>\n<p>This docker and singularity image provides the\
    \ tgv-qsm algorithm (<a href=\"http://www.neuroimaging.at/pages/qsm.php\" rel=\"\
    nofollow\">http://www.neuroimaging.at/pages/qsm.php</a>).</p>\n<p>If you use this\
    \ image, this is the reference to cite describing the QSM algorithm:\nLangkammer,\
    \ C; Bredies, K; Poser, BA; Barth, M; Reishofer, G; Fan, AP; Bilgic, B; Fazekas,\
    \ F; Mainero; C; Ropele, S\nFast Quantitative Susceptibility Mapping using 3D\
    \ EPI and Total Generalized Variation.\nNeuroimage. 2015 May 1;111:622-30. doi:\
    \ 10.1016/j.neuroimage.2015.02.041. PubMed</p>\n<h1>\n<a id=\"user-content-if-you-are-looking-for-a-full-qsm-pipeline-including-dicom-conversion-qsm-solution-image-segmentation-atlas-building\"\
    \ class=\"anchor\" href=\"#if-you-are-looking-for-a-full-qsm-pipeline-including-dicom-conversion-qsm-solution-image-segmentation-atlas-building\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>If you are looking for a full QSM pipeline including dicom conversion,\
    \ QSM solution, image segmentation, atlas building</h1>\n<p><a href=\"https://github.com/QSMxT/QSMxT\"\
    >https://github.com/QSMxT/QSMxT</a></p>\n<h1>\n<a id=\"user-content-using-the-image-in-singularity\"\
    \ class=\"anchor\" href=\"#using-the-image-in-singularity\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using the\
    \ image in singularity</h1>\n<p>installing singularity will depend on your operating\
    \ system, here an exampe for a debian based system</p>\n<pre><code>sudo apt-get\
    \ update &amp;&amp; sudo apt-get install -y \\\n    build-essential \\\n    uuid-dev\
    \ \\\n    libgpgme-dev \\\n    squashfs-tools \\\n    libseccomp-dev \\\n    wget\
    \ \\\n    pkg-config \\\n    git \\\n    cryptsetup-bin\n\nwget https://golang.org/dl/go1.15.2.linux-amd64.tar.gz\n\
    \ntar -C /usr/local -xzf go1.15.2.linux-amd64.tar.gz\n\nexport PATH=$PATH:/usr/local/go/bin\n\
    \nexport VERSION=3.6.3 &amp;&amp; # adjust this as necessary \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz\
    \ &amp;&amp; \\\n    tar -xzf singularity-${VERSION}.tar.gz &amp;&amp; \\\n  \
    \  cd singularity\n\n\n./mconfig &amp;&amp; \\\n    make -C ./builddir &amp;&amp;\
    \ \\\n    sudo make -C ./builddir install\n\n</code></pre>\n<p>then you can download\
    \ and run the container:</p>\n<pre><code>git clone https://github.com/NeuroDesk/transparent-singularity\
    \ tgvqsm_1.0.0_20210317\ncd tgvqsm_1.0.0_20210317\n./run_transparent_singularity.sh\
    \ tgvqsm_1.0.0_20210317\n</code></pre>\n<p>this will download the image, unpack\
    \ it and provide a wrapper script for starting tgv_qsm:</p>\n<p>The wrapper script\
    \ can be started using</p>\n<pre><code>./tgv_qsm\n\n</code></pre>\n<p>Or you can\
    \ open a shell into the container:</p>\n<pre><code> singularity shell tgvqsm_1.0.0_20210317.*\n\
    </code></pre>\n<p>you can also bind a different directory to your image (e.g.\
    \ bind /data from your host to /data in your singularity image)</p>\n<pre><code>singularity\
    \ shell --bind /data:/data/ tgvqsm_1.0.0_20210317.*\n</code></pre>\n<p>Here is\
    \ an example for a single echo QSM processing:</p>\n<pre><code>dcm2niix -o ./\
    \ -f magnitude GR_M_5_QSM_p2_1mmIso_TE20/\ndcm2niix -o ./ -f phase GR_P_6_QSM_p2_1mmIso_TE20/\n\
    \nbet2 magnitude.nii magnitude_bet2\n\ntgv_qsm \\\n  -p phase.nii \\\n  -m magnitude_bet2_mask.nii.gz\
    \ \\\n  -f 2.89 \\\n  -t 0.02 \\\n  -s \\\n  -o qsm\n</code></pre>\n<p>The -s\
    \ option will scale the phase correctly if the phase dicom values are between\
    \ -2048 and 2048 (should be default on Siemens VD and VE platforms). On the VB\
    \ platform the phase is between 0 and 4096, so omit the -s option and scale the\
    \ phase between -pi and pi:</p>\n<h1>\n<a id=\"user-content-using-the-image-in-docker\"\
    \ class=\"anchor\" href=\"#using-the-image-in-docker\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using the image\
    \ in docker</h1>\n<pre><code>docker pull vnmd/tgvqsm_1.0.0:20210317\nsudo docker\
    \ run -it -v $PWD:/data vnmd/tgvqsm_1.0.0:20210317\n\ncd /data\ndcm2niix -o ./\
    \ -f magnitude GR_M_5_QSM_p2_1mmIso_TE20/\ndcm2niix -o ./ -f phase GR_P_6_QSM_p2_1mmIso_TE20/\n\
    \nbet2 magnitude.nii magnitude_bet2\n\ntgv_qsm -p phase.nii -m magnitude_bet2_mask.nii.gz\
    \ -f 2.89 -t 0.02 -s -o qsm\n</code></pre>\n<h1>\n<a id=\"user-content-optimizing-for-your-cpu\"\
    \ class=\"anchor\" href=\"#optimizing-for-your-cpu\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Optimizing for\
    \ your CPU</h1>\n<p>By default, QSM is compiled with the <code>-O3 -march=x86-64</code>\
    \ which should provide a good balance between speed and portability. If you know\
    \ what CPU you're going to be using you can compile with that instruction set\
    \ to improve performance (e.g. <code>-march=ivybridge</code> for Intel Ivy Bridge\
    \ CPUs, <code>-march=native</code> for whatever CPU you're currently on). If you\
    \ would like maximum portability, you can recompile omitting the <code>-march</code>\
    \ flag altogether.</p>\n<h1>\n<a id=\"user-content-using-tgv_qsm-in-windows-subsystem-for-linux-example-debian-based-system\"\
    \ class=\"anchor\" href=\"#using-tgv_qsm-in-windows-subsystem-for-linux-example-debian-based-system\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using tgv_qsm in Windows Subsystem for Linux (example: Debian based\
    \ system)</h1>\n<p>WSL 1.0 doesn't support singularity or docker containers (but\
    \ WSL 2.0 will). But it is possible to directly install TGV QSM in a miniconda\
    \ environment:</p>\n<pre><code>sudo apt install wget unzip gcc\nwget https://repo.anaconda.com/miniconda/Miniconda2-4.6.14-Linux-x86_64.sh\n\
    bash Miniconda2-4.6.14-Linux-x86_64.sh\n(install, accept agreement with yes, after\
    \ install source bash again:)\nbash\nconda install -c anaconda cython==0.25.2\n\
    conda install numpy\nconda install pyparsing\n(make sure pip is not your system\
    \ pip, but the one in miniconda: which pip)\npip install scipy==0.17.1 nibabel==2.1.0\n\
    wget http://www.neuroimaging.at/media/qsm/TGVQSM-plus.zip\nunzip TGVQSM-plus.zip\n\
    cd TGVQSM-master-011045626121baa8bfdd6633929974c732ae35e3\npython setup.py install\n\
    cd test_data\ntgv_qsm  -p epi3d_test_phase.nii.gz -m epi3d_test_mask.nii.gz -f\
    \ 2.89 -t 0.027 -o epi3d_test_QSM\n</code></pre>\n<h1>\n<a id=\"user-content-adding-fsl-to-wsl-ubuntu-1804\"\
    \ class=\"anchor\" href=\"#adding-fsl-to-wsl-ubuntu-1804\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Adding fsl\
    \ to WSL Ubuntu 18.04</h1>\n<pre><code>wget -O- http://neuro.debian.net/lists/bionic.us-ca.full\
    \ | sudo tee /etc/apt/sources.list.d/neurodebian.sources.list\nsudo apt-key adv\
    \ --recv-keys --keyserver hkp://pool.sks-keyservers.net:80 0xA5D32F012649A5A9\n\
    sudo apt-get update\nsudo apt-get install fsl-5.0-core\n</code></pre>\n<p>add\
    \ \". /etc/fsl/5.0/fsl.sh\" to the end of your .profile file</p>\n"
  stargazers_count: 6
  subscribers_count: 4
  topics: []
  updated_at: 1615972979.0
CN-Healthborn/el7tf1.12gpu:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: CN-Healthborn/el7tf1.12gpu
  latest_release: null
  readme: '<h1>

    <a id="user-content-nova-el7-tensorflow-gpu" class="anchor" href="#nova-el7-tensorflow-gpu"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nova-el7-tensorflow-gpu</h1>

    <p>Configurations for docker and singularity for making OSG-compatible CENTOS7
    container with GPU-accelerated tensorflow and keras installed.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1603475388.0
CWRU-MSL/GammaDoublePrime:
  data_format: 2
  description: Python repository of code for preprocessing and extracting metrics
    of the volume fraction and size of the gamma double prime and gamma prime in a
    nickel-based superalloy microstructure
  filenames:
  - Singularity
  full_name: CWRU-MSL/GammaDoublePrime
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterization-of-nanoscale-precipitates-in-superalloy-718-using-high-resolution-sem-imaging\"\
    \ class=\"anchor\" href=\"#characterization-of-nanoscale-precipitates-in-superalloy-718-using-high-resolution-sem-imaging\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Characterization of nanoscale precipitates in superalloy 718 using\
    \ high resolution SEM imaging</h1>\n<h2>\n<a id=\"user-content-tm-smith-a-nm-senanayake-b-ck-sudbrack-c-p-bonacuse-a-rb-rogers-a-p-chao-d-j-carter-b\"\
    \ class=\"anchor\" href=\"#tm-smith-a-nm-senanayake-b-ck-sudbrack-c-p-bonacuse-a-rb-rogers-a-p-chao-d-j-carter-b\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>T.M. Smith a*, N.M. Senanayake b, C.K. Sudbrack c, P. Bonacuse a,\
    \ R.B. Rogers a, P. Chao d, J. Carter b</h2>\n<p>a NASA Glenn Research Center,\
    \ Materials and Structures Division, Cleveland, OH 44135, United States of America\n\
    b Case Western Reserve University, Department of Materials Science and Engineering,\
    \ Cleveland, OH 44106, United States of America\nc QuesTek Innovations LLC, Evanston,\
    \ IL 60201, United States of America\nd Carnegie Mellon University, Department\
    \ of Materials Science and Engineering, Pittsburgh, PA 15213, United States of\
    \ America</p>\n<h2>\n<a id=\"user-content-materials-characterization-212019-v148-p-178-197\"\
    \ class=\"anchor\" href=\"#materials-characterization-212019-v148-p-178-197\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Materials Characterization, (2/1/2019) V148, p 178-197</h2>\n<h2>\n\
    <a id=\"user-content-httpwwwsciencedirectcomsciencearticlepiis1044580318328444\"\
    \ class=\"anchor\" href=\"#httpwwwsciencedirectcomsciencearticlepiis1044580318328444\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"http://www.sciencedirect.com/science/article/pii/S1044580318328444\"\
    \ rel=\"nofollow\">http://www.sciencedirect.com/science/article/pii/S1044580318328444</a>\n\
    </h2>\n<h2>\n<a id=\"user-content-doi-101016jmatchar201812018\" class=\"anchor\"\
    \ href=\"#doi-101016jmatchar201812018\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>DOI: 10.1016/j.matchar.2018.12.018</h2>\n\
    <h3>\n<a id=\"user-content-repo-information\" class=\"anchor\" href=\"#repo-information\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Repo Information</h3>\n<p>This repository contains the codes necessary\
    \ to utilize the algorthims presented in the paper below. When implimented, the\
    \ can be used to obtain accurate volume fraction and size measurements of gamma\
    \ double prime, and gamma prime, precipitates in Superalloy</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-clone-a-repository\" class=\"anchor\" href=\"#clone-a-repository\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Clone a repository</h2>\n<p>Use these steps to clone from SourceTree,\
    \ our client for using the repository command-line free. Cloning allows you to\
    \ work on your files locally. If you don't yet have SourceTree, <a href=\"https://www.sourcetreeapp.com/\"\
    \ rel=\"nofollow\">download and install first</a>. If you prefer to clone from\
    \ the command line, see <a href=\"https://confluence.atlassian.com/x/4whODQ\"\
    \ rel=\"nofollow\">Clone a repository</a>.</p>\n<ol>\n<li>You\u2019ll see the\
    \ clone button under the <strong>Source</strong> heading. Click that button.</li>\n\
    <li>Now click <strong>Check out in SourceTree</strong>. You may need to create\
    \ a SourceTree account or log in.</li>\n<li>When you see the <strong>Clone New</strong>\
    \ dialog in SourceTree, update the destination path and name if you\u2019d like\
    \ to and then click <strong>Clone</strong>.</li>\n<li>Open the directory you just\
    \ created to see your repository\u2019s files.</li>\n</ol>\n<p>Now that you're\
    \ more familiar with your Bitbucket repository, go ahead and add a new file locally.\
    \ You can <a href=\"https://confluence.atlassian.com/x/iqyBMg\" rel=\"nofollow\"\
    >push your change back to Bitbucket with SourceTree</a>, or you can <a href=\"\
    https://confluence.atlassian.com/x/8QhODQ\" rel=\"nofollow\">add, commit,</a>\
    \ and <a href=\"https://confluence.atlassian.com/x/NQ0zDQ\" rel=\"nofollow\">push\
    \ from the command line</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1613426965.0
Characterisation-Virtual-Laboratory/CharacterisationVL-Software:
  data_format: 2
  description: null
  filenames:
  - fiji/Singularity.fiji
  - datalad/Singularity.datalad_0.13.3
  - pyprismatic/Singularity.pyprismatic_1_2_1-cuda-11.0
  - paraview/Singularity.paraview_5.6.0-cuda-9.0
  - crisprcas/Singularity.crisprcas
  - fsl/Singularity.fsl
  - cloudstor/Singularity.cloudstor-2.4.1
  - colmap/Singularity.colmap_3.6-dev.3
  - colmap/Singularity.colmap_3.5
  - caffe/Singularity.caffe_1.0
  - anaconda3/Singularity.anaconda3_5.3.0-cuda-11.0.3
  - anaconda3/Singularity.anaconda3_5.3.0
  - atom/Singularity.atom_1.45.0
  - atom/Singularity.atom_1.39.1
  - darknet/Singularity.darknet_yolo_v3-cuda-9.0
  - ilastik/Singularity.ilastik_1.3.3post3
  - libertem/Singularity.libertem-v0.7.0
  - libertem/Singularity.libertem-v0.4.0
  - libertem/Singularity.libertem-v0.6.0
  - libertem/Singularity.libertem-v0.4.1
  - libertem/Singularity.libertem-21-May-2019
  - libertem/Singularity.libertem-v0.5.1
  - libertem/Singularity.libertem-v0.5.0
  - libertem/Singularity.libertem-v0.2.2
  - haystack/Singularity.haystack_bio_v0_5_0
  - eman/Singularity.eman_2.91
  - eman/Singularity.eman_2.3
  - eman/Singularity.eman_2.22
  - eman/Singularity.eman_2.9
  - eman/Singularity.eman_2.3.1
  - globus-cli/Singularity.globus-cli-v2.0.0
  - amide/Singularity.amide-1.0.5
  - cistem/Singularity.cisTEM-1.0.0-beta
  - git-annex/Singularity.git-annex.6.20180227
  - cellprofiler/Singularity.cellprofiler_3.1.5
  - cellprofiler/Singularity.cellprofiler_3.1.9
  - argos/Singularity.argos_3.0.0-beta53
  - argos/Singularity.argos_3.0.0-beta52
  - mrtrix3tissue/Singularity.mrtrix3tissue-5.2.8
  - mydata-python/Singularity.mydata-python_20200603
  - cryolo/Singularity.cryolo_v1_6_1
  - cryolo/Singularity.cryolo_v1_0_0
  - cryolo/Singularity.cryolo_v1_5_6
  - cryolo/Singularity.cryolo_v1_0_4
  - mango/Singularity.mango_4.0.1
  - R/Singularity.R_4.0.5
  - octave/Singularity.octave-4.2.2
  - cvmfs-client/Singularity.cvmfs-client
  - meshlab/Singularity.meshlab-2019.03-cuda-9.0
  - openrefine/Singularity.openrefine-3.1
  - openmodelica/Singularity.openmodelica_1.14.2-cuda-10.1
  - dragondisk/Singularity.dragondisk_v1_0_5
  - ashs/Singularity.ashs_2.0.0
  - omero-insight/Singularity.1804
  - connectome-workbench/Singularity.connectome-workbench_1.4.2
  - ants/Singularity.ants_2.3.4
  - ants/Singularity.ants_2.3.1
  - globus-connect-personal/Singularity.globus-connect-personal_latest
  - ariba/Singularity.ariba_2.14.4
  - ariba/Singularity.ariba_2.12.1
  - jupyter-ml/Singularity.jupyter-ml_20201120
  - jupyter-ml/Singularity.jupyter-ml_20210415
  - mantid/Singularity.mantid_v_3_13_0
  - caffe-unet/Singularity.caffe-unet_1.0
  - matlab/Singularity.MATLAB_SAMPLE
  - volview/Singularity.VolView_3.4-cuda-9.0
  - imagej/Singularity.imagej_1.50e
  - quit/Singularity.quit_2.0.2
  - bidscoin/Singularity.bidscoin_3
  - imblproc/Singularity.imblproc
  - gimp/Singularity.gimp_2.8
  - gimp/Singularity.gimp_2.8.22
  - omero.insight/Singularity.omero_5.5.10
  - bids-validator/Singularity.bids-validator-1.3.1
  - bids-validator/Singularity.bids-validator-1.2.2
  - ubuntu-base-image/Singularity.1804-cuda10.1
  - ubuntu-base-image/Singularity.1804
  - ubuntu-base-image/Singularity.2004
  - ubuntu-base-image/Singularity.1804-cuda9
  - ubuntu-base-image/Singularity.2004-cuda11.0
  - deeplabcut/Singularity.latest
  - imagemagick/Singularity.imagemagick-7.0.8-68
  - apex/Singularity.apex_master
  - 3dslicer/Singularity.3dslicer_4.10.2
  - 3dslicer/Singularity.3dslicer_4.8.1
  - imod/Singularity.imod_v4_9_9
  - graphviz/Singularity.graphviz-2.40.1
  - dristhi/Singularity.dristhi_2.6.4
  - mydata/Singularity.mydata_0.9.2-1
  - mrtrix/Singularity.mrtrix_3_beta
  - cytoscape/Singularity.cytoscape_3.8.0
  - octopus/Singularity.octopus_8.4_parallel
  - octopus/Singularity.octopus_8.4
  full_name: Characterisation-Virtual-Laboratory/CharacterisationVL-Software
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterisationvl-software\" class=\"anchor\"\
    \ href=\"#characterisationvl-software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CharacterisationVL-Software</h1>\n\
    <p>The purpose of this repository is for storing definition files to submit to\
    \ <a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity Hub.</a></p>\n\
    <p>If you are new to Singularity containers, please refer to <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.5/user-guide/</a> or a newer version\
    \ of this documentation.</p>\n<p>Each software package is located in its own folder.\
    \ The files are tagged with the software name and version number or date of build.\
    \ Please read below for the naming convention.</p>\n<p>To add software to the\
    \ repository you will need to create a new branch. The new branch is the name\
    \ of the software product. By convention, the new branch will be checked and merged\
    \ into the master branch and then deleted.</p>\n<h2>\n<a id=\"user-content-steps-to-add-a-software-package\"\
    \ class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Steps to\
    \ add a software package</h2>\n<ol>\n<li>Clone this repository</li>\n<li>Create\
    \ a branch</li>\n</ol>\n<pre><code>$ git branch &lt;software name&gt;\n</code></pre>\n\
    <ol start=\"3\">\n<li>Make a subdirectory for the software product.</li>\n</ol>\n\
    <pre><code>$ mkdir &lt;software name&gt;\n</code></pre>\n<ol start=\"4\">\n<li>Add\
    \ all the necessary files.</li>\n</ol>\n<ul>\n<li>Singularity definition file\
    \ or installation script</li>\n<li>Readme file including install and testing notes</li>\n\
    <li>Desktop files for adding to menus with necessary tags</li>\n<li>For full details,\
    \ <a href=\"template/README.md\">please refer to the 'template' folder in this\
    \ repository.</a>\n</li>\n</ul>\n<ol start=\"4\">\n<li>Commit all changes, including\
    \ a helpful message</li>\n</ol>\n<pre><code>$ git commit -m \"&lt;software name&gt;\
    \ added as requested in support ticket\"\n</code></pre>\n<ol start=\"6\">\n<li>Push\
    \ to the remote repository. i.e. this one.</li>\n<li>Submit merge request</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Naming your Singularity definition file, Singularity Hub and Licensing</h2>\n\
    <p>For all Singularity recipes where the software licensing permits redistribution,\
    \ please use this naming convention:</p>\n<pre><code>   Singularity.applicationName_version\n\
    \   Singularity.applicationName_version-cuda-cudaVersion\n\n</code></pre>\n<p>This\
    \ is where Singularity Hub fits into the equation. There is a webhook between\
    \ this repository and <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >Singularity Hub</a>. When a commit is merged into the master branch, Singularity\
    \ Hub will build the container.</p>\n<p>If successfully built, the path to the\
    \ container on Singularity Hub is:</p>\n<pre><code>  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n\
    \  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\
    \n</code></pre>\n<p>For software where licensing does not support redistribution,\
    \ the container recipe can still be defined, but the container should not be built\
    \ on Singularity Hub.</p>\n<p>An example on how to handle this situation is the\
    \ recipe for CCP-EM.\nThe <a href=\"ccp-em/README.md\">README.md</a> contains\
    \ a section on Prerequisites. This section lists the required files to build the\
    \ container. The license must be accepted by the end user to obtain them.</p>\n\
    <p>Prerequisite files should not be committed to this repository.</p>\n<p>To prevent\
    \ Singularity Hub from attempting to build the container, we simply use a different\
    \ recipe naming convention as follows:</p>\n<pre><code>   applicationName_version.def\n\
    \   applicationName_version-cuda-cudaVersion.def\n\n</code></pre>\n<h2>\n<a id=\"\
    user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Ubuntu Base Images</h2>\n<p>The folder 'ubuntu-base-image' contains\
    \ recipes for pre built base containers. These can be used as a starting point\
    \ to aid/speed up the development of your container recipe.</p>\n<p>The current\
    \ versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.</p>\n\
    <p>These are available on Singularity Hub.</p>\n<p>For example: from the Graphviz\
    \ Singularity.graphviz-2.40.1 recipe</p>\n<pre><code>Bootstrap: shub\nFrom:  \
    \    Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n</code></pre>\n\
    <p>These two lines, will tell Singularity to use the 'shub' bootstrap to obtain\
    \ the '1804' ubuntu-base-image container from Singularity Hub.</p>\n<p>From here\
    \ you just need to add the requirements to build a container for your required\
    \ piece of software. Please see <a href=\"graphviz/Singularity.graphviz-2.40.1\"\
    >Singularity.graphviz-2.40.1</a>\nfor the full recipe.</p>\n<p>The current ubuntu-base-images\
    \ include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.</p>\n\
    <h2>\n<a id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"\
    anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ GUI applications on a non-GPU node</h2>\n<p>The applications in the Singularity\
    \ container should run without the need for a dedicated GPU.</p>\n<p>However,\
    \ an X server needs to be running for this to work. On nodes with GPU, X Server\
    \ is started with NVIDIA driver, and on non-GPU nodes, the X Server is started\
    \ with MESA library.</p>\n<p>X Server can be started during boot (for example,\
    \ using <code>systemctl set-default graphical.target</code>).</p>\n<p>Make sure\
    \ that VirtualGL package is installed in the container. The code below will download\
    \ and install VirtualGL.</p>\n<pre><code>wget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\
    \ndpkg -i virtualgl_2.6.2_amd64.deb\n</code></pre>\n<p>The application startup\
    \ script doesn't need to be modified, however, if the application needs to be\
    \ manually started, then <code>vglrun</code> needs to be appended before running\
    \ the application. For example: <code>singularity exec --nv -B /projects:/projects\
    \ -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX</code></p>\n\
    <p><a href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 6
  subscribers_count: 5
  topics: []
  updated_at: 1623365754.0
ChunCun/container:
  data_format: 2
  description: null
  filenames:
  - Singularity.torch_mmf
  - Singularity.torch
  full_name: ChunCun/container
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos8_roar" class="anchor" href="#centos8_roar" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos8_roar</h1>

    <p>Centos 8 base image for Roar</p>

    <h3>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTE</h3>

    <ul>

    <li>This recipe may include unnecessary packages for certain software installation</li>

    <li>More packages will be added in the future</li>

    </ul>

    <h2>

    <a id="user-content-updates" class="anchor" href="#updates" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Updates</h2>

    <ul>

    <li>

    <p>2020/11/13</p>

    <ul>

    <li>Initial recipe added</li>

    </ul>

    </li>

    <li>

    <p>2021/03/22</p>

    <ul>

    <li>Default Python3 is updated to Python 3.8</li>

    <li>Lapack, BLAS, OpenBLAS, ATLAS, and NetCDF are added</li>

    <li>CMake 3.19.7, Boost 1.75.0, and R 4.0.4 are added</li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605677713.0
Clinical-Genomics-Lund/nextflow_wgs:
  data_format: 2
  description: null
  filenames:
  - container/Singularity
  - container/Singularity_madeline2
  - container/reviewer/Singularity
  - container/stranger/Singularity
  full_name: Clinical-Genomics-Lund/nextflow_wgs
  latest_release: null
  readme: '<h1>

    <a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Results</h1>

    <h2>

    <a id="user-content-spring-mass-h--02" class="anchor" href="#spring-mass-h--02"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spring
    mass h = 0.2</h2>

    <h3>

    <a id="user-content-hypereuler" class="anchor" href="#hypereuler" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperEuler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4</a></p>

    <h3>

    <a id="user-content-heun" class="anchor" href="#heun" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Heun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4</a></p>

    <h3>

    <a id="user-content-hyperheun" class="anchor" href="#hyperheun" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperHeun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4</a></p>

    <h3>

    <a id="user-content-velocity-verlet" class="anchor" href="#velocity-verlet" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Velocity Verlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4</a></p>

    <h3>

    <a id="user-content-hyperverlet" class="anchor" href="#hyperverlet" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperVerlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4</a></p>

    <h3>

    <a id="user-content-fr4" class="anchor" href="#fr4" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>FR4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4</a></p>

    <h3>

    <a id="user-content-rk4" class="anchor" href="#rk4" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>RK4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4</a></p>

    <h2>

    <a id="user-content-pendulum-h--008" class="anchor" href="#pendulum-h--008" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pendulum h = 0.08</h2>

    <h3>

    <a id="user-content-euler" class="anchor" href="#euler" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Euler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4</a></p>

    <h3>

    <a id="user-content-hypereuler-1" class="anchor" href="#hypereuler-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperEuler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4</a></p>

    <h3>

    <a id="user-content-heun-1" class="anchor" href="#heun-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Heun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4</a></p>

    <h3>

    <a id="user-content-hyperheun-1" class="anchor" href="#hyperheun-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperHeun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4</a></p>

    <h3>

    <a id="user-content-velocity-verlet-1" class="anchor" href="#velocity-verlet-1"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Velocity
    Verlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4</a></p>

    <h3>

    <a id="user-content-hyperverlet-1" class="anchor" href="#hyperverlet-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperVerlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4</a></p>

    <h3>

    <a id="user-content-fr4-1" class="anchor" href="#fr4-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FR4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4</a></p>

    <h3>

    <a id="user-content-rk4-1" class="anchor" href="#rk4-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>RK4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4</a></p>

    <h2>

    <a id="user-content-three-body-spring-mass-h--006" class="anchor" href="#three-body-spring-mass-h--006"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Three
    body spring mass h = 0.06</h2>

    <h3>

    <a id="user-content-euler-1" class="anchor" href="#euler-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Euler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4</a></p>

    <h3>

    <a id="user-content-hypereuler-2" class="anchor" href="#hypereuler-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperEuler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4</a></p>

    <h3>

    <a id="user-content-heun-2" class="anchor" href="#heun-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Heun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4</a></p>

    <h3>

    <a id="user-content-hyperheun-2" class="anchor" href="#hyperheun-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperHeun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4</a></p>

    <h3>

    <a id="user-content-velocity-verlet-2" class="anchor" href="#velocity-verlet-2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Velocity
    Verlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4</a></p>

    <h3>

    <a id="user-content-hyperverlet-2" class="anchor" href="#hyperverlet-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperVerlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4</a></p>

    <h3>

    <a id="user-content-fr4-2" class="anchor" href="#fr4-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FR4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4</a></p>

    <h3>

    <a id="user-content-rk4-2" class="anchor" href="#rk4-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>RK4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4</a></p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1623850871.0
CompBio-TDU-Japan/containers:
  data_format: 2
  description: recipes of Singularity
  filenames:
  - Singularity.snpeff
  - Singularity.blast-latest
  - Singularity.cufflinks
  - Singularity.blast-legacy
  - Singularity.vcftools
  - Singularity.bwa
  - Singularity.gatk
  - Singularity.samtools
  - Singularity.trimmomatic
  - Singularity.rooting_nj
  full_name: CompBio-TDU-Japan/containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>containers</h1>

    <p>recipes of Singularity</p>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1548055469.0
ComputationalRadiationPhysics/picongpu:
  data_format: 2
  description: 'Particle-in-Cell Simulations for the Exascale Era :sparkles:'
  filenames:
  - share/picongpu/dockerfiles/ubuntu-1604/Singularity
  full_name: ComputationalRadiationPhysics/picongpu
  latest_release: 0.5.0
  readme: '<h1>

    <a id="user-content-picongpu---particle-in-cell-simulations-for-the-exascale-era"
    class="anchor" href="#picongpu---particle-in-cell-simulations-for-the-exascale-era"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PIConGPU
    - Particle-in-Cell Simulations for the Exascale Era</h1>

    <p><a href="https://travis-ci.org/ComputationalRadiationPhysics/picongpu/branches"
    rel="nofollow"><img src="https://camo.githubusercontent.com/8001013459189259d918b4f4e2993dbc331e8a13903199c8cc3a1cf561247c40/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6d61737465722e7376673f6c6162656c3d6d6173746572"
    alt="Code Status master" data-canonical-src="https://img.shields.io/travis/ComputationalRadiationPhysics/picongpu/master.svg?label=master"
    style="max-width:100%;"></a>

    <a href="https://travis-ci.org/ComputationalRadiationPhysics/picongpu/branches"
    rel="nofollow"><img src="https://camo.githubusercontent.com/372e2d54a4f7af960e27fc9a084fc93f5cf2b9accfe4e983febe644963c03f1a/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6465762e7376673f6c6162656c3d646576"
    alt="Code Status dev" data-canonical-src="https://img.shields.io/travis/ComputationalRadiationPhysics/picongpu/dev.svg?label=dev"
    style="max-width:100%;"></a>

    <a href="http://picongpu.readthedocs.io" rel="nofollow"><img src="https://camo.githubusercontent.com/dd84c49cf1a8e134ca1a5a87517257a4317eecc4a80ff90195b2d2eebeeb7ced/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7069636f6e6770752f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/picongpu/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="http://computationalradiationphysics.github.io/picongpu" rel="nofollow"><img
    src="https://camo.githubusercontent.com/bea8b749e6bc63f677e6ccfc18ae8a6a4a4e39d55e3aac6c872acc8d1ecdc22b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4150492d446f787967656e2d626c75652e737667"
    alt="Doxygen" data-canonical-src="https://img.shields.io/badge/API-Doxygen-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/compare/master...dev"><img
    src="https://camo.githubusercontent.com/b36d60b89d6eb05b7977d0522b2c74389a9182e2a00879fa4a09361bfd3a4912/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6c61746573742f6465762e737667"
    alt="GitHub commits since last release" data-canonical-src="https://img.shields.io/github/commits-since/ComputationalRadiationPhysics/picongpu/latest/dev.svg"
    style="max-width:100%;"></a>

    <a href="https://isocpp.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/6b5671d1cd5cb649d92fdacc5e3ddc75dc6f6ea22e81efce926664d09ecf04ff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231312d6f72616e67652e737667"
    alt="Language" data-canonical-src="https://img.shields.io/badge/language-C%2B%2B11-orange.svg"
    style="max-width:100%;"></a>

    <a href="https://www.gnu.org/licenses/gpl-3.0.html" rel="nofollow"><img src="https://camo.githubusercontent.com/d9724a7d6c0aab25ad100b9c974369e2ea7585e6d0d0b87b66aa5bd34f6c2abe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c76332d626c75652e7376673f6c6162656c3d5049436f6e475055"
    alt="License PIConGPU" data-canonical-src="https://img.shields.io/badge/license-GPLv3-blue.svg?label=PIConGPU"
    style="max-width:100%;"></a>

    <a href="https://www.gnu.org/licenses/lgpl-3.0.html" rel="nofollow"><img src="https://camo.githubusercontent.com/d8685de1336e9f80f82625f4271fbd11d00beaf91013768daef9ac1a62e6fe2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4c47504c76332d626c75652e7376673f6c6162656c3d504d616363"
    alt="License PMacc" data-canonical-src="https://img.shields.io/badge/license-LGPLv3-blue.svg?label=PMacc"
    style="max-width:100%;"></a></p>

    <p><a href="http://www.youtube.com/watch?v=nwZuG-XtUDE" rel="nofollow"><img src="https://camo.githubusercontent.com/be7eb258510f59135b17c487a2bd50e76f186e1c71e56ab87f5cd1afe4df35d3/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6e775a75472d58745544452f302e6a7067"
    alt="PIConGPU Presentation Video" data-canonical-src="http://img.youtube.com/vi/nwZuG-XtUDE/0.jpg"
    style="max-width:100%;"></a>

    <a href="http://www.youtube.com/watch?v=nwZuG-XtUDE" rel="nofollow"><img src="docs/logo/pic_logo_vert_158x360.png"
    alt="PIConGPU Release" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

    <p>PIConGPU is a fully relativistic,

    <a href="https://en.wikipedia.org/wiki/Manycore_processor" rel="nofollow">manycore</a>,

    3D3V particle-in-cell (<a href="http://en.wikipedia.org/wiki/Particle-in-cell"
    rel="nofollow">PIC</a>)

    code. The Particle-in-Cell algorithm is a central tool in plasma physics.

    It describes the dynamics of a plasma by computing the motion of

    electrons and ions in the plasma based on

    <a href="http://en.wikipedia.org/wiki/Maxwell%27s_equations" rel="nofollow">Maxwell''s
    equations</a>.</p>

    <p>PIConGPU implements various numerical schemes to solve the PIC cycle.

    Its features for the electro-magnetic PIC algorithm include:</p>

    <ul>

    <li>a central or Yee-lattice for fields</li>

    <li>particle pushers that solve the equation of motion for charged and neutral

    particles, e.g., the <em>Boris-</em> and the

    <a href="http://dx.doi.org/10.1063/1.2837054" rel="nofollow"><em>Vay-Pusher</em></a>

    </li>

    <li>Maxwell field solvers, e.g.

    <a href="http://dx.doi.org/10.1109/TAP.1966.1138693" rel="nofollow"><em>Yee''s</em></a>
    and

    <a href="http://dx.doi.org/10.1103/PhysRevSTAB.16.021301" rel="nofollow"><em>Lehe''s</em></a>
    scheme</li>

    <li>rigorously charge conserving current deposition schemes, such as

    <a href="http://dx.doi.org/10.1016/0010-4655%2892%2990169-Y" rel="nofollow"><em>Villasenor-Buneman</em></a>,

    <a href="http://dx.doi.org/10.1016/S0010-4655%2800%2900228-9" rel="nofollow"><em>Esirkepov</em></a>

    and <em>ZigZag</em>

    </li>

    <li>macro-particle form factors ranging from NGP (0th order), CIC (1st),

    TSC (2nd), PSQ (3rd) to P4S (4th)</li>

    </ul>

    <p>and the electro-magnetic PIC algorithm is further self-consistently coupled
    to:</p>

    <ul>

    <li>classical radiation reaction

    (<a href="http://dx.doi.org/10.1016/j.cpc.2016.04.002" rel="nofollow">DOI:10.1016/j.cpc.2016.04.002</a>)</li>

    <li>QED synchrotron radiation (photon emission)

    (<a href="http://dx.doi.org/10.1103/PhysRevE.92.023305" rel="nofollow">DOI:10.1103/PhysRevE.92.023305</a>)</li>

    <li>advanced field ionization methods

    (<a href="http://dx.doi.org/10.1103/PhysRevA.59.569" rel="nofollow">DOI:10.1103/PhysRevA.59.569</a>,

    <a href="http://www.jetp.ac.ru/cgi-bin/dn/e_020_05_1307.pdf" rel="nofollow">LV
    Keldysh</a>, BSI)</li>

    </ul>

    <p>Besides the electro-magnetic PIC algorithm and extensions to it, we developed

    a wide range of tools and diagnostics, e.g.:</p>

    <ul>

    <li>online, far-field radiation diagnostics for coherent and incoherent radiation

    emitted by charged particles</li>

    <li>full restart and output capabilities via <a href="http://openPMD.org" rel="nofollow">openPMD</a>,

    including <a href="http://hdfgroup.org/" rel="nofollow">parallel HDF5</a> (via

    <a href="https://github.com/ComputationalRadiationPhysics/libSplash">libSplash</a>)
    and

    <a href="https://csmd.ornl.gov/adios/" rel="nofollow">ADIOS</a>, allowing for

    extreme I/O scalability and massively parallel online-analysis</li>

    <li>2D and 3D live view and diagnostics tools</li>

    <li>a large selection of extensible

    <a href="http://picongpu.readthedocs.io/en/latest/usage/plugins.html" rel="nofollow">online-plugins</a>

    </li>

    </ul>

    <p>As one of our supported compute platforms, GPUs provide a computational

    performance of several

    <a href="http://en.wikipedia.org/wiki/FLOPS" rel="nofollow">TFLOP/s</a> at considerable
    lower invest and

    maintenance costs compared to multi CPU-based compute architectures of similar

    performance. The latest high-performance systems

    (<a href="http://www.top500.org/" rel="nofollow">TOP500</a>) are enhanced by accelerator
    hardware that

    boost their peak performance up to the multi-PFLOP/s level. With its

    outstanding performance and scalability to more than 18''000 GPUs,

    PIConGPU was one of the <strong>finalists</strong> of the 2013

    <a href="http://sc13.supercomputing.org/content/acm-gordon-bell-prize" rel="nofollow">Gordon
    Bell Prize</a>.</p>

    <p>PIConGPU is developed and maintained by the

    <a href="https://www.hzdr.de/db/Cms?pNid=2097" rel="nofollow">Computational Radiation
    Physics Group</a>

    at the <a href="http://www.hzdr.de/db/Cms?pNid=132" rel="nofollow">Institute for
    Radiation Physics</a>

    at <a href="http://www.hzdr.de/" rel="nofollow">HZDR</a> in close collaboration
    with the Center

    for Information Services and High Performance Computing

    (<a href="http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih" rel="nofollow">ZIH</a>)
    of the

    Technical University Dresden (<a href="http://www.tu-dresden.de" rel="nofollow">TUD</a>).
    We are a

    member of the <a href="http://ccoe-dresden.de/" rel="nofollow">Dresden GPU Center
    of Excellence</a> that

    cooperates on a broad range of scientific GPU and manycore applications,

    workshops and teaching efforts.</p>

    <h2>

    <a id="user-content-attribution" class="anchor" href="#attribution" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Attribution</h2>

    <p>PIConGPU is a <em>scientific project</em>. If you <strong>present and/or publish</strong>
    scientific

    results that used PIConGPU, you should set a <strong>reference</strong> to show
    your support.</p>

    <p>Our according <strong>up-to-date publication</strong> at <strong>the time of
    your publication</strong>

    should be inquired from:</p>

    <ul>

    <li><a href="https://raw.githubusercontent.com/ComputationalRadiationPhysics/picongpu/master/REFERENCE.md"
    rel="nofollow">REFERENCE.md</a></li>

    </ul>

    <p>Please also consider adding yourself to our <a href="https://github.com/ComputationalRadiationPhysics/picongpu-communitymap">community
    map</a>.

    We would love to hear from you!</p>

    <h2>

    <a id="user-content-oral-presentations" class="anchor" href="#oral-presentations"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Oral
    Presentations</h2>

    <p>The following slide should be part of <strong>oral presentations</strong>.
    It is intended to

    acknowledge the team maintaining PIConGPU and to support our community:</p>

    <p>(<em>coming soon</em>) presentation_picongpu.pdf

    (svg version, key note version, png version: 1920x1080 and 1024x768)</p>

    <h2>

    <a id="user-content-software-license" class="anchor" href="#software-license"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    License</h2>

    <p><em>PIConGPU</em> is licensed under the <strong>GPLv3+</strong>. Furthermore,
    you can develop your

    own particle-mesh algorithms based on our general library <em>PMacc</em> that
    is

    shipped alongside PIConGPU. <em>PMacc</em> is <em>dual licensed</em> under both
    the

    <strong>GPLv3+ and LGPLv3+</strong>.

    For a detailed description, please refer to <a href="LICENSE.md">LICENSE.md</a></p>

    <hr>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>See our notes in <a href="INSTALL.rst">INSTALL.rst</a>.</p>

    <h2>

    <a id="user-content-users" class="anchor" href="#users" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Users</h2>

    <p>Dear User, please be aware that this is an <strong>open beta release</strong>!

    We hereby emphasize that we are still actively developing PIConGPU at great

    speed and do, from time to time, break backwards compatibility.</p>

    <p>When using this software, please stick to the <code>master</code> branch containing
    the

    latest <em>stable</em> release. It also contains a file <code>CHANGELOG.md</code>
    with the

    latest changes (and how to update your simulations). Read it first before

    updating between two versions! Also, we add a git <code>tag</code> according to
    a version

    number for each release in <code>master</code>.</p>

    <p>For any questions regarding the usage of PIConGPU please <strong>do not</strong>
    contact the

    developers and maintainers directly.</p>

    <p>Instead, please sign up to our <strong>PIConGPU-Users</strong> mailing list
    so we can

    distribute and archive user questions:

    <a href="https://cg.hzdr.de/Lists/picongpu-users/List.html" rel="nofollow">Subscribe
    (select "Feed" on bottom left)</a>.</p>

    <p>Before you post a question, browse the PIConGPU

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/search?l=markdown">documentation</a>,

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/wiki">wiki</a>,

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/issues">issue
    tracker</a> and the

    <a href="https://cg.hzdr.de/Lists/picongpu-users/List.html" rel="nofollow">mailing
    list history</a>

    to see if your question has been answered, already.</p>

    <p>PIConGPU is a collaborative project.

    We thus encourage users to engage in answering questions of other users and post
    solutions to problems to the list.

    A problem you have encountered might be the future problem of another user.</p>

    <p>In addition, please consider using the collaborative features of GitHub if
    you have questions or comments on code or documentation.

    This will allow other users to see the piece of code or documentation you are
    referring to.</p>

    <p>Main ressources are in our <a href="https://picongpu.readthedocs.io" rel="nofollow">online
    manual</a>, the <a href="https://github.com/ComputationalRadiationPhysics/picongpu/wiki">user
    section</a> of our wiki, documentation files in <a href="http://commonmark.org/help/"
    rel="nofollow"><code>.md</code> (Markdown)</a> and <a href="http://www.sphinx-doc.org/en/stable/rest.html"
    rel="nofollow"><code>.rst</code> (reStructuredText)</a> format in this repository
    and a <a href="http://www.youtube.com/watch?v=7ybsD8G4Rsk" rel="nofollow">getting
    started video</a>.

    Feel free to visit <a href="http://picongpu.hzdr.de" rel="nofollow">picongpu.hzdr.de</a>
    to learn more about the PIC algorithm.</p>

    <h2>

    <a id="user-content-software-upgrades" class="anchor" href="#software-upgrades"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    Upgrades</h2>

    <p>PIConGPU follows a

    <a href="http://nvie.com/posts/a-successful-git-branching-model/" rel="nofollow">master
    - dev</a>

    development model. That means our latest stable release is shipped in a branch

    called <code>master</code> while new and frequent changes to the code are incooporated

    in the development branch <code>dev</code>.</p>

    <p>Every time we update the <em>master</em> branch, we publish a new release

    of PIConGPU. Before you pull the changes in, please read our

    <a href="CHANGELOG.md">ChangeLog</a>!

    You may have to update some of your simulation <code>.param</code> and <code>.cfg</code>
    files by

    hand since PIConGPU is an active project and new features often require changes

    in input files. Additionally, a full description of new features and fixed bugs

    in comparison to the previous release is provided in that file.</p>

    <p>In case you decide to use <em>new, potentially buggy and experimental</em>
    features

    from our <code>dev</code> branch, be aware that support is very limited and you
    must

    participate or at least follow the development yourself. Syntax changes

    and in-development bugs will <em>not</em> be announced outside of their according
    pull

    requests and issues.</p>

    <p>Before drafting a new release, we open a new <code>release-*</code> branch
    from <code>dev</code> with

    the <code>*</code> being the version number of the upcoming release. This branch
    only

    receives bug fixes (feature freeze) and users are welcome to try it out

    (however, the change log and a detailed announcement might still be missing in

    it).</p>

    <h2>

    <a id="user-content-developers" class="anchor" href="#developers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Developers</h2>

    <h3>

    <a id="user-content-how-to-participate" class="anchor" href="#how-to-participate"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How
    to participate</h3>

    <p>See <a href="CONTRIBUTING.md">CONTRIBUTING.md</a></p>

    <p>If you like to jump in right away, see<br>

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"><img
    src="https://camo.githubusercontent.com/32b730b309ff90c1713e8ae39a73ae2145c4cceb0b7e72bacef523db9fc85d62/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f676f6f64253230666972737425323069737375652e7376673f636f6c6f723d353663626566"
    alt=''open "good first issue" issues'' data-canonical-src="https://img.shields.io/github/issues-raw/ComputationalRadiationPhysics/picongpu/good%20first%20issue.svg?color=56cbef"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-active-team" class="anchor" href="#active-team" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Active Team</h2>

    <h3>

    <a id="user-content-scientific-supervision" class="anchor" href="#scientific-supervision"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scientific
    Supervision</h3>

    <ul>

    <li>Dr. Michael Bussmann</li>

    <li>Dr. Axel Huebl</li>

    </ul>

    <h3>

    <a id="user-content-maintainers-and-core-developers" class="anchor" href="#maintainers-and-core-developers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Maintainers*
    and core developers</h3>

    <ul>

    <li>Dr. Sergei Bastrakov*</li>

    <li>Dr. Alexander Debus</li>

    <li>Marco Garten*</li>

    <li>Dr. Axel Huebl*</li>

    <li>Alexander Matthes</li>

    <li>Dr. Richard Pausch*</li>

    <li>Sophie Rudat</li>

    <li>Sebastian Starke</li>

    <li>Dr. Klaus Steiniger</li>

    <li>Rene Widera*</li>

    </ul>

    <h3>

    <a id="user-content-former-members-contributions-and-thanks" class="anchor" href="#former-members-contributions-and-thanks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Former
    Members, Contributions and Thanks</h3>

    <p>The PIConGPU Team expresses its gratitude to:</p>

    <p>Florian Berninger, Heiko Burau, Robert Dietrich, Carlchristian Eckert,

    Wen Fu, Ph.D., Alexander Grund, Sebastian Hahn, Anton Helm, Wolfgang Hoehnig,

    Dr.-Ing. Guido Juckeland, Jeffrey Kelling, Maximilian Knespel, Dr. Remi Lehe,

    Felix Schmitt, Benjamin Schneider, Joseph Schuchart, Conrad Schumann,

    Stefan Tietze, Marija Vranic, Ph.D., Benjamin Worpitz, and Erik Zenker.</p>

    <p>Kudos to everyone, mentioned or unmentioned, who contributed further in any

    way!</p>

    <hr>

    <p><a href="docs/images/lwfa_iso.png" target="_blank" rel="noopener noreferrer"><img
    src="docs/images/lwfa_iso.png" alt="image of an lwfa" title="LWFA" style="max-width:100%;"></a>

    <a href="docs/images/StrongScalingPIConGPU_log.png" target="_blank" rel="noopener
    noreferrer"><img src="docs/images/StrongScalingPIConGPU_log.png" alt="image of
    our strong scaling" title="Strong Scaling" style="max-width:100%;"></a></p>

    '
  stargazers_count: 474
  subscribers_count: 46
  topics:
  - laser
  - plasma
  - physics
  - gpu
  - physics-simulation
  - gpu-computing
  - particle-accelerator
  - particle-in-cell
  - pic
  - research
  updated_at: 1624915975.0
D-Lo/bambi:
  data_format: 2
  description: R wrapper for bamdb
  filenames:
  - src/bamdb/Singularity.bamdb
  full_name: D-Lo/bambi
  latest_release: null
  readme: "<p><a href=\"https://travis-ci.org/mskilab/bambi\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/47c82ab2d405aa684f3a5004ed8fc79887c025105127effda9ce1d35b5568974/68747470733a2f2f7472617669732d63692e6f72672f6d736b696c61622f62616d62692e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/mskilab/bambi.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/github/mskilab/bambi?branch=master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ccb3814df2f3f1c65e518dd49a10732518ba754f251e50546a0d42ec9fd9cdab/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6d736b696c61622f62616d62692e737667\"\
    \ alt=\"codecov.io\" data-canonical-src=\"https://img.shields.io/codecov/c/github/mskilab/bambi.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-bambi\" class=\"\
    anchor\" href=\"#bambi\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>bambi</h1>\n<p>R package for querying 10x WGS\
    \ and single-cell BAMs</p>\n<h2>\n<a id=\"user-content-dependencies\" class=\"\
    anchor\" href=\"#dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Dependencies</h2>\n<pre lang=\"{r}\"\
    ><code>devtools::install_github('mskilab/gUtils')\n</code></pre>\n<pre lang=\"\
    {r}\"><code>devtools::install_github('mskilab/bamUtils')\n</code></pre>\n<h2>\n\
    <a id=\"user-content-bambi-commands\" class=\"anchor\" href=\"#bambi-commands\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>bambi commands</h2>\n<p>Instantiate a bambi object:</p>\n<p>Methods:</p>\n\
    <ul>\n<li>\n<p>grab_bx()</p>\n<ul>\n<li><code>grab_bx(barcodes, query=NULL, data.table\
    \ = FALSE, verbose = FALSE, mc.cores = 1)</code></li>\n</ul>\n</li>\n<li>\n<p>grab_cb()</p>\n\
    <ul>\n<li><code>grab_cb(barcodes, query=NULL, data.table = FALSE, verbose = FALSE,\
    \ mc.cores = 1)</code></li>\n</ul>\n</li>\n<li>\n<p>grab_ub()</p>\n<ul>\n<li><code>grab_ub(barcodes,\
    \ query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)</code></li>\n\
    </ul>\n</li>\n<li>\n<p>fetch_by_tag()</p>\n<ul>\n<li><code>fetch_by_tag(tag, tag_queries,\
    \ query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)</code></li>\n\
    </ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-demo\" class=\"anchor\" href=\"\
    #demo\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Demo</h2>\n<ul>\n<li>Instantiate a <code>bambi</code> object</li>\n\
    </ul>\n<pre lang=\"{r}\"><code>library(bambi)\n\n&gt; hcc1143_subset = bambi$new(bam_file\
    \ = \"subsetHCC1143_phased_possorted0001.bam\", bamdb_path=\"subsetHCC1143_phased_possorted0001_lmdb\"\
    )\n</code></pre>\n<ul>\n<li>Call methods</li>\n</ul>\n<pre lang=\"{r}\"><code>&gt;\
    \ hcc1143_subset$grab_bx('CGACGTGTCCTCTAGC-1')\nGRanges object with 2 ranges and\
    \ 11 metadata columns:\n      seqnames                 ranges strand |\n     \
    \    &lt;Rle&gt;              &lt;IRanges&gt;  &lt;Rle&gt; |\n  [1]     chr1 [147975454,\
    \ 147975580]      + |\n  [2]     chr1 [147975675, 147975824]      - |\n      \
    \                                   qname      flag      mapq       cigar\n  \
    \                                 &lt;character&gt; &lt;numeric&gt; &lt;numeric&gt;\
    \ &lt;character&gt;\n  [1] ST-K00126:3:H5TL3BBXX:2:2109:25926:37800        99\
    \        16        127M\n  [2] ST-K00126:3:H5TL3BBXX:2:2109:25926:37800      \
    \ 147        16        150M\n            rnext     pnext      tlen\n      &lt;character&gt;\
    \ &lt;numeric&gt; &lt;numeric&gt;\n  [1]           = 147975676       371\n  [2]\
    \           = 147975455      -371\n                                          \
    \                                                                            \
    \                                   seq\n                                    \
    \                                                                            \
    \                                 &lt;character&gt;\n  [1]                   \
    \     ATGTCTTCTTCCTCATTATCTGGCACTGGTTAGGAAGCACTCATCTCCATGAAGTCATCTTTTGTTAATTCCTCTGGTGTGGTGTGTATTAGCTCTTAAATTCCTCCAAGATCCATATCTTGCAACC\n\
    \  [2] ATCTGGACACAAATTGTACTTTTGTCCAGCACGAATTTATTGTTTTGAGTTTCATGGTTTTCTATATCAACTGATGACATCTTGAAAGGTGTAAGCCTTCCAGACTTCCATGATGTTCTCTCTATTGGGTTTCTCTTTTGCAATGTTGAC\n\
    \                                                                            \
    \                                                                            qual\n\
    \                                                                            \
    \                                                                     &lt;character&gt;\n\
    \  [1]                        JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJFJJJJJJJJJJJAJFJJJJJJJJJFJJJJJJJJJJFJJJJFFFJJJFJJJJJJAAJFJJJFAFAFFFJAA&lt;7F&lt;\n\
    \  [2] A&lt;7FFFJFFFAJJAAAJJF&lt;F&lt;7A-&lt;AA-&lt;&lt;&lt;AFFJJJJJJJJFFJAFFAAFJFJJJAFFJJJJJJJJJJFJFAJJJJJJFJJJJJJ&lt;FFJJJFJJJFJJJJJJJJJJJJJFJJJJFFJ7JJJJF&lt;JJJJJJJJJJJJJJJJJJJFFAA&lt;\n\
    \                      BX    qwidth\n             &lt;character&gt; &lt;integer&gt;\n\
    \  [1] CGACGTGTCCTCTAGC-1       127\n  [2] CGACGTGTCCTCTAGC-1       150\n  -------\n\
    \  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1531085438.0
DiamondLightSource/Opt-ID:
  data_format: 2
  description: Code for the Optimisation of ID's using Python and Opt-AI
  filenames:
  - Singularity
  - Singularity.env-v2
  full_name: DiamondLightSource/Opt-ID
  latest_release: v2.0
  readme: '<p><a href="https://travis-ci.org/DiamondLightSource/Opt-ID" rel="nofollow"><img
    src="https://camo.githubusercontent.com/54cbf520664efa3f8fc3298323da50593160dd744d2ec6bd5de8ed8dd3593e0d/68747470733a2f2f7472617669732d63692e6f72672f4469616d6f6e644c69676874536f757263652f4f70742d49442e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/DiamondLightSource/Opt-ID.svg?branch=master"
    style="max-width:100%;"></a>  <a href="https://coveralls.io/github/DiamondLightSource/Opt-ID?branch=master&amp;service=github"
    rel="nofollow"><img src="https://camo.githubusercontent.com/dc50340a825cc5da0454649fde18840b6c0ec2d3b4dd91c5e8319d2319850548/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4469616d6f6e644c69676874536f757263652f4f70742d49442f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562"
    alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/DiamondLightSource/Opt-ID/badge.svg?branch=master&amp;service=github"
    style="max-width:100%;"></a>  <a href="https://scrutinizer-ci.com/g/DiamondLightSource/Opt-ID/?branch=master"
    rel="nofollow"><img src="https://camo.githubusercontent.com/c0b7776aa669724907a66bc7d335a00b5606e9f3dc41409d189185f47b791cbb/68747470733a2f2f7363727574696e697a65722d63692e636f6d2f672f4469616d6f6e644c69676874536f757263652f4f70742d49442f6261646765732f7175616c6974792d73636f72652e706e673f623d6d6173746572"
    alt="Scrutinizer Code Quality" data-canonical-src="https://scrutinizer-ci.com/g/DiamondLightSource/Opt-ID/badges/quality-score.png?b=master"
    style="max-width:100%;"></a> <a href="https://doi.org/10.5281/zenodo.3968577"
    rel="nofollow"><img src="https://camo.githubusercontent.com/5bb0569d502774e80da711f971a82ba8beb8a3decdc75b595131dd5a79f03bf9/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333936383537372e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.3968577.svg"
    style="max-width:100%;"></a> <a href="https://singularity-hub.org/collections/4728"
    rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-opt-id" class="anchor" href="#opt-id" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Opt-ID</h1>

    <p>Code for the Optimisation of ID''s using Python and Opt-AI</p>

    <h2>

    <a id="user-content-overview-of-how-to-use-opt-id" class="anchor" href="#overview-of-how-to-use-opt-id"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview
    of how to use Opt-ID</h2>

    <p>Opt-ID is run is by providing:</p>

    <ul>

    <li>a main configuration file in YAML format which contains all the various

    parameters for the sort/shim job</li>

    <li>an existing directory in which output data will be written to</li>

    </ul>

    <p>There are two main flags, <code>--sort</code> and <code>--shim</code>, to run
    sort and shim jobs. The

    idea is that using either of these flags in conjunction with the YAML config

    file will go through and run all the scripts that are used to produce

    intermediate files and pass them around appropriately, so then there''s only one

    command needed to be executed to run a sort or shim job, and the YAML config

    file is the single source of all the parameter information used for that

    particular job.</p>

    <p>There are several other processes that Opt-ID provides that are desired to
    be

    done after a sort/shim but don''t require the sequence of scripts that a

    sort/shim job does (for example, the use of <code>compare.py</code> to compare
    a shimmed

    genome to the original genome), so the <code>--sort</code> and <code>--shim</code>
    flags aren''t able

    to provide these sorts of processes. To do so, there are several shell scripts

    that are autogenerated when a sort or shim job is run that can be executed.

    These scripts run Opt-ID in the particular way that is needed to perform the

    process, without the user needing to worry about extra configuration on top of

    the YAML file.</p>

    <p>Taking the <code>compare.py</code> example previously mentioned, a script would
    be

    autogenerated after a shim job called <code>compare_shim.sh</code> that can be
    passed any

    shimmed genome file in the data directory, and it will take care of calling

    Opt-ID in the particular way it needs to in order to run the <code>compare.py</code>
    script

    with the appropriate parameters. More details on how to use these autogenerated

    shell scripts are below in the "Using the autogenerated shell scripts" section.</p>

    <h2>

    <a id="user-content-data-directory" class="anchor" href="#data-directory" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Data directory</h2>

    <p>The data outputted by Opt-ID is split roughly into two categories:</p>

    <ul>

    <li>large files such as <code>.h5</code> files</li>

    <li>smaller files such as <code>.json</code>, <code>.mag</code>, <code>.sh</code>
    files</li>

    </ul>

    <p>The smaller files get written to the directory passed as the second parameter
    to

    Opt-ID, so if OptID was passed <code>/home/FedID/my_dir</code> then the smaller
    files would

    get written to <code>/home/FedID/my_dir</code>.</p>

    <p>The larger files get written to a directory within <code>/dls/tmp/FedID</code>
    whose path

    is based on the user''s FedID and also the name of the data directory passed to

    Opt-ID. The name of the directory created in <code>/dls/tmp/FedID</code> will
    be the name

    of the very last directory in the path passed to Opt-ID. For example, if the

    path <code>/home/FedID/my_dir</code> is passed to Opt-ID, then the directory

    <code>/dls/tmp/FedID/my_dir</code> will be created. Symlinks are then created
    in

    <code>/home/FedID/my_dir</code> to point to the larger files inside

    <code>/dls/tmp/FedID/my_dir</code>.</p>

    <p>One reason behind having two separate directories containing different data

    files is due to the large size of the <code>.h5</code> files produced by Opt-ID
    and not

    having the space to put them just anywhere in the filesystem (<code>/dls/tmp</code>
    has

    much more available space than, for example, the home directory associated to
    a

    FedID). Another reason is that the automatic deletion of files in <code>/dls/tmp</code>
    can

    be used to do some automatic periodic cleanup of old, large files.</p>

    <h3>

    <a id="user-content-intended-usage" class="anchor" href="#intended-usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Intended usage</h3>

    <p>The intended usage of this dual-directory structure is that the smaller files

    are written to somewhere away from <code>/dls/tmp</code> so then they''re not
    deleted

    periodically and can be referred to later if needed, whilst the larger files are

    written to the user''s directory in <code>/dls/tmp</code> so then they <em>are</em>
    deleted

    periodically. Therefore, it is advised that the directory provided to Opt-ID is

    not a directory in <code>/dls/tmp/FedID</code>; this is not only because of potential

    deletion of the smaller files, but also because passing a directory in

    <code>/dls/tmp/FedID</code> can cause some confusion regarding the directory that
    is

    subsequently created by Opt-ID in <code>/dls/tmp/FedID</code>.</p>

    <p><strong>In particular, it is advised that the directory passed to Opt-ID is
    one within

    <code>/dls/technical/id</code></strong>, as this is where output data from other
    Opt-ID jobs has

    typically been placed.</p>

    <h3>

    <a id="user-content-example-directory-structures" class="anchor" href="#example-directory-structures"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example
    directory structures</h3>

    <p>For example, if the directory <code>/dls/technical/id/test/</code> is passed
    to Opt-ID, the

    expected directory structures right after having run a sort job on a cluster is

    given below:</p>

    <p><code>/dls/technical/id/test/</code>:</p>

    <ul>

    <li>

    <code>test_sort.json</code> (file)</li>

    <li>

    <code>test_sort.mag</code> (file)</li>

    <li>

    <code>test_sort.h5 -&gt; /dls/tmp/FedID/test/test_sort.h5</code> (symlink to a
    file)</li>

    <li>

    <code>generate_report.sh</code> (file)</li>

    <li>

    <code>restart_sort.sh</code> (file)</li>

    <li>

    <code>logfiles/</code> (directory)</li>

    <li>

    <code>genomes -&gt; /dls/tmp/FedID/test/genomes/</code> (symlink to a directory)</li>

    <li>

    <code>process_genome_output -&gt; /dls/tmp/FedID/test/process_genome_output/</code>

    (symlink to a directory)</li>

    </ul>

    <p><code>/dls/tmp/FedID/test/</code>:</p>

    <ul>

    <li>

    <code>test_sort.h5</code> (file, the symlink <code>/dls/technical/id/test/test_sort.h5</code>
    points

    to this file)</li>

    <li>

    <code>genomes/</code> (directory, the symlink <code>/dls/technical/id/test/genomes</code>
    points to

    this directory)</li>

    <li>

    <code>process_genome_output/</code> (directory, the symlink

    <code>/dls/technical/id/test/process_genome_output</code> points to this directory)</li>

    </ul>

    <p>As another example, for the same directory being passed but instead a shim
    job

    being run on a cluster, the expected directory structures right after the job

    are:</p>

    <p><code>/dls/technical/id/test/</code>:</p>

    <ul>

    <li>

    <code>test_shim.json</code> (file)</li>

    <li>

    <code>test_shim.mag</code> (file)</li>

    <li>

    <code>test_shim.h5 -&gt; /dls/tmp/FedID/test/test_shim.h5</code> (symlink to a
    file)</li>

    <li>

    <code>generate_report.sh</code> (file)</li>

    <li>

    <code>compare_shim.sh</code> (file)</li>

    <li>

    <code>logfiles/</code> (directory)</li>

    <li>

    <code>shimmed_genomes -&gt; /dls/tmp/FedID/test/shimmed_genomes/</code> (symlink
    to a

    directory)</li>

    <li>

    <code>process_genome_output -&gt; /dls/tmp/FedID/test/process_genome_output/</code>

    (symlink to a directory)</li>

    </ul>

    <p><code>/dls/tmp/FedID/test/</code>:</p>

    <ul>

    <li>

    <code>test_shim.h5</code> (file, the symlink <code>/dls/technical/id/test/test_shim.h5</code>
    points

    to this file)</li>

    <li>

    <code>shimmed_genomes/</code> (directory, the symlink

    <code>/dls/technical/id/test/shimmed_genomes</code> points to this directory)</li>

    <li>

    <code>process_genome_output/</code> (directory, the symlink

    <code>/dls/technical/id/test/process_genome_output</code> points to this directory)</li>

    </ul>

    <p>Note that the filenames <code>test_sort.*</code> and <code>test_shim.*</code>
    are just placeholders

    and have been chosen only for illustrative purposes, these files can be named
    as

    desired in the YAML config file.</p>

    <h2>

    <a id="user-content-preliminary-steps-to-be-able-to-run-opt-id" class="anchor"
    href="#preliminary-steps-to-be-able-to-run-opt-id" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Preliminary steps to be able to run Opt-ID</h2>

    <p>A process that is not done by Opt-ID is the transfer of magnet information
    in

    the Excel files provided by the supplier to <code>.sim</code> files. To do so,
    from the

    Excel files supplied by the supplier, create tab delimited <code>.sim</code> files
    of

    magnetisation. This is a manual procedure done only on Windows. Note that,

    currently, Opt-ID requires the magnet names in the <code>.sim</code> files to
    have leading

    zeros that pad out the name to 3 digits. For example, instead of ''1'' it should

    be ''001''.</p>

    <p>To get the code, clone the Opt-ID repo to the desired place in the filesystem.

    To set up the environment for running Opt-ID on a Linux machine, in a terminal

    run the following commands:</p>

    <pre><code>module load python/3

    module load global/cluster

    export PYTHONPATH=$PYTHONPATH:/path/to/Opt-ID

    </code></pre>

    <p>where <code>/path/to/Opt-ID</code> is the path to the root directory of the
    cloned repo.

    (There is a change to how <code>python</code> is used to run the code which is
    detailed in

    the next section, and so the third command is to enable <code>python</code> to
    find the

    code in the repo).</p>

    <h2>

    <a id="user-content-running-opt-id-with-the-python-command" class="anchor" href="#running-opt-id-with-the-python-command"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    Opt-ID with the <code>python</code> command</h2>

    <p>The main script that is used for running Opt-ID is <code>IDSort/src/optid.py</code>.
    It

    should be run using the syntax <code>python -m IDSort.src.optid</code> as opposed
    to

    <code>python /path/to/Opt-ID/IDSort/src/optid.py</code>.</p>

    <h2>

    <a id="user-content-different-options-that-opt-id-can-be-run-with" class="anchor"
    href="#different-options-that-opt-id-can-be-run-with" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Different options that
    Opt-ID can be run with</h2>

    <p>There are two sets of flags from which one flag from each set is mandatory
    to be

    passed to Opt-ID, and the rest are optional and have sensible default values if

    they are not provided.</p>

    <p>The mandatory sets of flags are</p>

    <ul>

    <li>

    <code>--sort</code> vs <code>--shim</code>

    </li>

    <li>

    <code>--cluster-on</code> vs <code>--cluster-off</code>

    </li>

    </ul>

    <p>where only one flag from each bullet point should be provided.</p>

    <p>Examples of running Opt-ID with the bare mininum flags and parameters it needs

    are:</p>

    <pre><code>python -m IDSort.src.optid --sort --cluster-on /path/to/yaml /path/to/data/dir

    python -m IDSort.src.optid --shim --cluster-off /path/to/yaml /path/to/data/dir

    </code></pre>

    <h3>

    <a id="user-content---sort-and---shim" class="anchor" href="#--sort-and---shim"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--sort</code>
    and <code>--shim</code>

    </h3>

    <p>These are used for specifying what type of job is desired.</p>

    <h3>

    <a id="user-content---cluster-on-and---cluster-off" class="anchor" href="#--cluster-on-and---cluster-off"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--cluster-on</code>
    and <code>--cluster-off</code>

    </h3>

    <p>These are used for specifying whether the job is run on the local machine or

    submitted to run on a cluster.</p>

    <h4>

    <a id="user-content---num-threads---queue-and---node-os" class="anchor" href="#--num-threads---queue-and---node-os"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--num-threads</code>,
    <code>--queue</code>, and <code>--node-os</code>

    </h4>

    <p>These are used in conjunction with <code>--cluster-on</code>. Some examples
    of using these

    flags would be</p>

    <pre><code>python -m IDSort.src.optid --sort --cluster-on --node-os rhel7 /path/to/yaml
    /path/to/data/dir

    python -m IDSort.src.optid --shim --cluster-on --queue low.q /path/to/yaml /path/to/data/dir

    </code></pre>

    <h4>

    <a id="user-content---seed-and---seed-value" class="anchor" href="#--seed-and---seed-value"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--seed</code>
    and <code>--seed-value</code>

    </h4>

    <p>These are used in conjunction with <code>--cluster-off</code>. <code>--seed</code>
    is used to specify

    that the random number generator (RNG) should be seeded and thus produce the

    same output across multiple runs with the same parameters. <code>--seed-value</code>
    is

    specified if a particular value to seed the RNG is desired (by default its value

    is 1). Some examples of using these flags would be</p>

    <pre><code>python -m IDSort.src.optid --sort --cluster-off --seed /path/to/yaml
    /path/to/data/dir

    python -m IDSort.src.optid --shim --cluster-off --seed --seed-value 30 /path/to/yaml
    /path/to/data/dir

    </code></pre>

    <h2>

    <a id="user-content-yaml-config-files" class="anchor" href="#yaml-config-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>YAML
    config files</h2>

    <p>The YAML config files contain the parameters used by the various scripts that

    Opt-ID runs. The top-level sections of the YAML config files are the script

    names minus the <code>.py</code> and the subsections are the different parameters
    passed to

    that particular script. For the most part, the subsection names are exactly the

    same as the script parameters they''re associated to, for example, the

    <code>id_setup.py</code> script has a <code>--periods</code> flag, and the YAML
    subsection

    corresponding to that parameter is <code>id_setup.periods</code>.</p>

    <p>A few exceptions exist to try and be more descriptive with what the parameter

    is, for example, <code>process_genome.py</code> refers to the files it''s given
    as elements

    of the <code>args</code> list, but in the YAML the corresponding subsection for
    a shim job

    is <code>process_genome.readable_genome_file</code> which is hopefully a more
    useful

    description.</p>

    <p>Examples of YAML config files can be found in the <code>IDSort/example_configs</code>

    directory. There are some placeholder values in these config files that aren''t

    valid values for their associated section in the YAML, and the following

    sections detail the changes that need to be made to the example config files to

    get them in a state ready to run a job.</p>

    <h3>

    <a id="user-content-sort-config-example" class="anchor" href="#sort-config-example"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sort
    config example</h3>

    <p>There are three values that need to be changed:</p>

    <ul>

    <li><code>magnets.hmags</code></li>

    <li><code>magnets.hemags</code></li>

    <li><code>magnets.htmags</code></li>

    </ul>

    <p>Their values should be absolute paths to any <code>.sim</code> files of the
    relevant type.</p>

    <h3>

    <a id="user-content-shim-config-example" class="anchor" href="#shim-config-example"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Shim
    config example</h3>

    <p>There are five values that need to be changed:</p>

    <ul>

    <li><code>magnets.hmags</code></li>

    <li><code>magnets.hemags</code></li>

    <li><code>magnets.htmags</code></li>

    <li><code>process_genome.readable_genome_file</code></li>

    <li><code>mpi_runner_for_shim_opt.bfield_filename</code></li>

    </ul>

    <p>The first three are the same as in the sort config example. The value of

    <code>process_genome.readable_genome_file</code> should be an absolute path to
    the <code>.inp</code>

    file that is used to start the shim job from. The value of

    <code>mpi_runner_for_shim_opt.bfield_filename</code> should be an absolute path
    to the

    <code>.h5</code> file that is converted from <code>.bfield</code> files that are
    produced by igor.</p>

    <p>Note that, currently, the use of the <code>igor2h5.py</code> script hasn''t
    yet been

    integrated into the YAML configuration file for Opt-ID, so the process of

    converting <code>.bfield</code> data into <code>.h5</code> data is one that needs
    to be done by

    manually executing the <code>igor2h5.py</code> script (or by any other means)
    prior to

    running a shim job with Opt-ID.</p>

    <h2>

    <a id="user-content-using-the-autogenerated-shell-scripts" class="anchor" href="#using-the-autogenerated-shell-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    the autogenerated shell scripts</h2>

    <p>All the autogenerated scripts can be executed from anywhere in the filesystem,

    it''s not necessary for the current working directory to be the same directory

    that the script is in.</p>

    <p>Due to the facts that</p>

    <ul>

    <li>these scripts are generated on a job-by-job basis and are only meant to be
    run

    for the particular data within the directory the scripts are in</li>

    <li>the structure of the data directories are fixed and known in advance</li>

    </ul>

    <p>when it comes to passing parameters to these scripts they are aware of the

    specific directories that the files they''re expecting should be in, so only

    filenames need to be given to them and not absolute or even relative filepaths.

    Concrete examples are given below in the <code>generate_report.sh</code> and

    <code>compare_shim.sh</code> sections that hopefully explain in more detail how
    to pass

    parameters to these scripts.</p>

    <h3>

    <a id="user-content-generate_reportsh" class="anchor" href="#generate_reportsh"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>generate_report.sh</code>

    </h3>

    <p>This script is used to create a report with some useful data visualisation
    in a

    PDF file. For a sort job it can be passed multiple <code>.genome</code> and <code>.inp</code>
    files,

    and for a shim job it can be passed multiple <code>.h5</code> files that are associated
    to

    the "full genomes" (as opposed to the smaller-sized "compare genomes") in the

    shim output.</p>

    <p>For a sort job, Opt-ID will look in both the <code>genomes/</code> and

    <code>process_genome_output/</code> directories for the given <code>.genome</code>
    and <code>.inp</code> files,

    and for a shim job Opt-ID will look in the <code>shimmed_genomes/</code> directory
    for the

    given <code>.h5</code> files. Therefore, the parameters passed to <code>generate_report.sh</code>

    should only be the filenames and not filepaths.</p>

    <p>For example, for a sort job, the correct way to pass a genome and a <code>.inp</code>
    file

    to the script would be</p>

    <pre><code>/path/to/generate_report.sh foo.genome bar.inp

    </code></pre>

    <p>as opposed to</p>

    <pre><code>/path/to/generate_report.sh genomes/foo.genome process_genome_output/bar.inp

    </code></pre>

    <p>Another example: for a shim job, the correct way to pass <code>.h5</code> files
    to the

    script would be</p>

    <pre><code>/path/to/generate_report.sh foo.h5 bar.h5

    </code></pre>

    <p>as opposed to</p>

    <pre><code>/path/to/generate_report.sh shimmed_genomes/foo.h5 shimmed_genomes/bar.h5

    </code></pre>

    <p>An optional <code>--report-filename</code> flag can be passed before the files
    to specify

    the name of the PDF file, and genome reports are stored in the <code>genome_reports/</code>

    directory within the directory passed to Opt-ID. Report filenames should have
    a

    <code>.pdf</code> extension to enable a simple check between the report filename
    parameter

    and <code>.genome</code>/<code>.inp</code> file parameters that follow it. The
    <code>--report-filename</code>

    option can be omitted and in that case the report filename will be a

    concatenation of all the filenames passed with an underscore character "_" as

    the separator between the filenames.</p>

    <p>An example of using the <code>--report-filename</code> flag is</p>

    <pre><code>/path/to/generate_report --report-filename report.pdf foo.genome bar.inp

    </code></pre>

    <h3>

    <a id="user-content-restart_sortsh" class="anchor" href="#restart_sortsh" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>restart_sort.sh</code>

    </h3>

    <p>This script requires no parameters and can be run simply as

    <code>/path/to/restart_sort.sh</code>, Opt-ID will take care of loading the YAML
    config of

    the previous sort job and will use all the same flags and paramters as the

    original sort job. One example is that if the original sort job was run on a

    cluster, so will the restart-sort job, and another example is that the same

    <code>.json</code>, <code>.mag</code> and <code>.h5</code> (lookup table) files
    from the original sort job will

    be reused in the restart-sort job instead of being regenerated.</p>

    <h3>

    <a id="user-content-compare_shimsh" class="anchor" href="#compare_shimsh" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>compare_shim.sh</code>

    </h3>

    <p>This can be passed a single <code>.genome</code> file that is in the <code>shimmed_genomes/</code>

    directory and it will generate a human readable diff between the original and

    shimmed genomes that will be written to the <code>shim_diffs/</code> directory.
    It''s not

    necessary to pass the original genome to this script, Opt-ID will take care of

    finding it so only the shimmed genome needs to be given as a parameter.</p>

    <p>Similarly to what <code>generate_report.sh</code> does, <code>compare_shim.sh</code>
    will look in the

    <code>shimmed_genomes/</code> directory so only filenames should be passed to
    it and not

    filepaths. An example of using this script would be:</p>

    <pre><code>/path/to/compare_shim.sh foo.genome

    </code></pre>

    <p>An optional <code>--diff-filename</code> flag can be passed before the shimmed
    genome file

    to specify the filename of the human readable diff. Currently Opt-ID appends a

    <code>.txt</code> extension to the filename so it''s not necessary to put that
    in the

    parameter. Again, similarly to what <code>generate_report.sh</code> does, if this
    flag is

    omitted then the diff filename is a concatenation of the original genome and

    shimmed genome filenames with an underscore character as the separator, and then

    also prepended with <code>shim_</code>. For example, if the original genome is
    <code>foo.genome</code>

    and the shimmed genome is <code>bar.genome</code>, then if the <code>--diff-filename</code>
    flag is

    omitted then the diff filename would be <code>shim_foo.genome_bar.genome.txt</code>.
    An

    example of using the <code>--diff-filename</code> flag is</p>

    <pre><code>/path/to/compare_shim.sh --diff-filename my_shim foo.genome

    </code></pre>

    <h2>

    <a id="user-content-hidden-options-of-opt-id" class="anchor" href="#hidden-options-of-opt-id"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"Hidden"
    options of Opt-ID</h2>

    <p>There are several options that Opt-ID has but are only meant to be used by
    the

    autogenerated shell scripts and not intended to be invoked directly by a user;

    therefore, these options aren''t of much interest to users and only of potential

    interest to developers. The following are just some useful notes to any

    developers viewing this document:</p>

    <ul>

    <li>these options are related to those kinds of processes that a user would want

    to do that aren''t full sort/shim jobs that were referred to in the "Overview

    of how to use Opt-ID" section of this document</li>

    <li>these options are all used by the autogenerated shell scripts that were also

    referred to in the "Overview of how to use Opt-ID" section, hence why the

    users need not directly use them, the autogenerated scripts should take care

    of using these "hidden options" where necessary</li>

    <li>these are also processes that are done after a sort/shim, so they assume the

    existence of a YAML config that has already been used for the sort/shim job,

    as well as any output data from a sort/shim job</li>

    </ul>

    <h3>

    <a id="user-content---generate-report" class="anchor" href="#--generate-report"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--generate-report</code>

    </h3>

    <p>This option starts off the process of using the

    <code>IDSort/src/genome_report_template.ipynb</code> file to generate a Jupyter
    notebook

    file, and then running it to produce a PDF report.</p>

    <h3>

    <a id="user-content---restart-sort" class="anchor" href="#--restart-sort" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--restart-sort</code>

    </h3>

    <p>This option starts off the process of reusing the same YAML config file that
    was

    used for the sort job to get all the parameters used for the original sort job,

    and then running Opt-ID to generate genomes from an initial population as

    opposed to generating genomes from scratch.</p>

    <h3>

    <a id="user-content---compare-shim" class="anchor" href="#--compare-shim" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--compare-shim</code>

    </h3>

    <p>This option starts off the process of comparing the given shimmed genome to
    the

    original genome that was used to start the shim job.</p>

    <h2>

    <a id="user-content-running-the-tests" class="anchor" href="#running-the-tests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    the tests</h2>

    <p>Navigate to the root directory of the Opt-ID repo:</p>

    <pre><code>cd /path/to/Opt-ID

    </code></pre>

    <p>To run all the tests:</p>

    <pre><code>python -m pytest IDSort/test/

    </code></pre>

    <p>To run a particular test in the <code>test/</code> directory, it can be specified
    in the

    path in the above command. For example, to run <code>IDSort/test/magnets_test.py</code>:</p>

    <pre><code>python -m pytest IDSort/test/magnets_test.py

    </code></pre>

    <p><a href="https://codescene.io/projects/6289/jobs/latest-successful/results"
    rel="nofollow"><img src="https://camo.githubusercontent.com/f3827d47125aaed62ec3276ebe498b2f14e96da020a3a3c25000597585019c5a/68747470733a2f2f636f64657363656e652e696f2f70726f6a656374732f363238392f7374617475732e737667"
    alt="" data-canonical-src="https://codescene.io/projects/6289/status.svg" style="max-width:100%;">
    Get more details at <strong>codescene.io</strong>.</a></p>

    '
  stargazers_count: 6
  subscribers_count: 8
  topics: []
  updated_at: 1609847329.0
DiamondLightSource/Savu:
  data_format: 2
  description: Tomography Reconstructon Pipeline
  filenames:
  - install/savu_singularity/conda-recipes/Singularity
  - install/savu_singularity/singularity-recipes/Singularity.SavuAstra
  - install/savu_singularity/singularity-recipes/Singularity.SavuDeps
  - install/savu_singularity/singularity-recipes/Singularity.SavuCore
  full_name: DiamondLightSource/Savu
  latest_release: v3.0
  readme: "<p><a href=\"doc/source/images/portcullis_logo.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"doc/source/images/portcullis_logo.png\"\
    \ alt=\"alt text\" title=\"Portcullis\" style=\"max-width:100%;\"></a></p>\n<h1>\n\
    <a id=\"user-content-portcullis\" class=\"anchor\" href=\"#portcullis\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Portcullis</h1>\n\
    <p><a href=\"https://github.com/maplesond/portcullis/releases\"><img src=\"https://camo.githubusercontent.com/b9c31b04d2671e6317cdfd9e4fdf893512936091302d1b1b56c99cb89ab43df7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7461672f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Version\" data-canonical-src=\"https://img.shields.io/github/tag/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://jenkins.sdlmapleson.net/job/portcullis/job/develop/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f696e3e0136cfb90f0e05f4f4e0a257ece7cd1e52ff19a0c8963b32df756d3a7/68747470733a2f2f6a656e6b696e732e73646c6d61706c65736f6e2e6e65742f6275696c645374617475732f69636f6e3f6a6f623d706f727463756c6c6973253246646576656c6f70\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://jenkins.sdlmapleson.net/buildStatus/icon?job=portcullis%2Fdevelop\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/ad4d6f3e16da4f0dddcd142fa3b6088042b13242787f5ad939d2db28282d3eb5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d627269676874677265656e2e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPL%20v3-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/issues\"\
    ><img src=\"https://camo.githubusercontent.com/d3bedf8e24750956939d66108f9ba197e72b83d1de8fc7305708ab2d67c20c17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues-raw/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Portcullis stands for PORTable CULLing\
    \ of Invalid Splice junctions from pre-aligned RNA-seq data.  It is known that\
    \ RNAseq mapping tools generate many invalid junction predictions, particularly\
    \ in deep datasets with high coverage over splice sites.  In order to address\
    \ this, instead for creating a new RNAseq mapper, with a focus on SJ accuracy\
    \ we created a tool that takes in a BAM file generated by an RNAseq mapper of\
    \ the user's own choice (e.g. Tophat2, Gsnap, STAR2 or HISAT2) as input (i.e.\
    \ it's portable).  It then, analyses and quantifies all splice junctions in the\
    \ file before, filtering (culling) those which are unlikely to be genuine.  Portcullis\
    \ output's junctions in a variety of formats making it suitable for downstream\
    \ analysis (such as differential splicing analysis and gene modelling) without\
    \ additional work.  Portcullis can also filter the original BAM file removing\
    \ alignments associated with <em>bad</em> junctions.  Both the filtered junctions\
    \ and BAM files are cleaner and more usable resources which can more effectively\
    \ be used to assist in downstream analyses such as gene prediction and genome\
    \ annotation.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h2>\n<p>We support multiple methods\
    \ for installing and running portcullis.  Hopefully your favourite container or\
    \ package manager is supported below.  If not let us know and we'll try to work\
    \ to get it integrated there.</p>\n<p><strong>Docker</strong></p>\n<p><a href=\"\
    https://hub.docker.com/r/maplesond/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/de966674ebe7a3dec2fed423683dd2c64e3630527fab6a691add53421292e384/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code># Keep in mind you need to mount\
    \ in any working directories to the container with the `-v` option.\n# Ideally,\
    \ mount these into the /data directory which is the container's working directory.\n\
    docker run --it --rm -v /abspath/to/data/on/host:/data maplesond/portcullis:stable\
    \ portcullis --help\n</code></pre>\n<p><strong>Singularity</strong></p>\n<pre><code>#\
    \ First download the container:\nsingularity pull --name portcullis.img shub://maplesond/portcullis:master\n\
    \n# Then to execute commands in the container:\nsingularity exec portcullis.img\
    \ portcullis --help\n</code></pre>\n<p><strong>Conda</strong></p>\n<p><a href=\"\
    https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/381a7739b713a2bae02343a6ac934de39148a7866dbf4e52b597391b2a07fd4b/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f6c61746573745f72656c656173655f646174652e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/latest_release_date.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/77a7c650d2675de3588df907d8e8aec11957abc95bcfd87d3b1b07f78a2bc4ec/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f706c6174666f726d732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/platforms.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/83781f462972e76ba4f2d046533fd48deb7cb72a0512481ff304f79c51bc01e3/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f646f776e6c6f6164732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/downloads.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code>conda install portcullis --channel=bioconda\n\
    </code></pre>\n<p><strong>Brew</strong></p>\n<pre><code>brew install brewsci/bio/portcullis\n\
    </code></pre>\n<p><strong>From source</strong></p>\n<p><a href=\"https://github.com/maplesond/portcullis/releases\"\
    ><img src=\"https://camo.githubusercontent.com/3885a69f4777ec0c98cf3d0bee17eb7ca3d3eb69bbf850df2f36895b80168ade/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d61706c65736f6e642f706f727463756c6c69732f746f74616c2e737667\"\
    \ alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/github/downloads/maplesond/portcullis/total.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>If you wish to install from source please\
    \ first confirm that first you have these dependencies are installed and configured:</p>\n\
    <ul>\n<li>\n<strong>GCC</strong> V4.8+</li>\n<li>\n<strong>autoconf</strong> V2.53+</li>\n\
    <li>\n<strong>automake</strong> V1.11+</li>\n<li><strong>make</strong></li>\n\
    <li>\n<strong>libtool</strong> V2.4.2+</li>\n<li><strong>zlib-dev</strong></li>\n\
    <li><strong>pthreads</strong></li>\n<li>\n<strong>boost-dev</strong> V1.52+</li>\n\
    <li>\n<strong>samtools</strong> V1.2+</li>\n<li>\n<strong>Python3-dev</strong>\
    \ V3.5+ (Make sure the following packages are installed: <em>pandas</em>, <em>matplotlib</em>,\
    \ <em>setuptools</em>, <em>sphinx</em>, <em>tabulate</em>)</li>\n</ul>\n<p>Then\
    \ proceed with the following steps:</p>\n<pre><code># Clone the repo\ngit clone\
    \ git@github.com:maplesond/portcullis.git\n\n# Move into repo directory\ncd portcullis\n\
    \n# Generate configure script\n./autogen.sh\n\n# Confirm dependencies and generate\
    \ makefiles\n# Adding --prefix &lt;dir&gt; will tell make install to put everything\
    \ in a \n# particular directory.  Default is /usr/local.\n./configure\n\n# Compile\
    \ (increasing -j will make it go faster!\nmake -j 2\n\n# Run some unit tests (you\
    \ can increase -j here too)\nmake -j 2 check\n\n# Install to prefix dir\nmake\
    \ install\n</code></pre>\n<p><strong>Common problems</strong></p>\n<ul>\n<li>\n\
    <p>Many system python installations do not come with the C API immediately available,\
    \ which prevents Portcullis from embedding python code.  We typically would recommend\
    \ installing anaconda3 as this would include the latest version of python, all\
    \ required python packages as well as the C API.  If you are running a debian\
    \ system and the C libraries are not available by default and you wish to use\
    \ the system python installation the you can install them using: <code>sudo apt-get\
    \ install python-dev</code>.  Also, if you have installed python to a custom location\
    \ please verify that the <em>bin</em> directors on the <em>PATH</em> environment\
    \ variable, and the lib (or lib64) directory is on the <em>LD_LIBRARY_PATH</em>\
    \ or <em>LD_RUN_PATH</em> as appropriate.</p>\n</li>\n<li>\n<p>If Portcullis is\
    \ failing at the <code>./autogen.sh</code> step you will likely need to install\
    \ autotools.  The following command should do this on MacOS: <code>brew install\
    \ autoconf automake libtool</code>.  On a debian system this can be done with:\
    \ <code>sudo apt-get install autoconf automake libtool</code>.</p>\n</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quickstart</h2>\n<p>After portcullis has been installed, the <code>portcullis</code>\
    \ executable should be available.  Typing <code>portcullis</code> or <code>portcullis\
    \ --help</code> at the command line will present you with the portcullis help\
    \ message.</p>\n<p>These modes are available:</p>\n<ul>\n<li>\n<strong>prep</strong>\
    \    - Prepares input data so that it is suitable for junction analysis</li>\n\
    <li>\n<strong>junc</strong>    - Calculates junction metrics for the prepared\
    \ data</li>\n<li>\n<strong>filter</strong>  - Separates alignments based on whether\
    \ they are likely to represent genuine splice junctions or not</li>\n<li>\n<strong>bamfilt</strong>\
    \ - Filters a BAM to remove any reads associated with invalid junctions</li>\n\
    <li>\n<strong>full</strong>    - Runs prep, junc, filter and optionally bamfilt\
    \ as a complete pipeline</li>\n</ul>\n<p>Typing <code>portcullis &lt;mode&gt;\
    \ --help</code> will bring up help and usage information specific to that mode.</p>\n\
    <p>In addition to portcullis, we provide a tool-suite for manipulating junction\
    \ files called junctools.  Typing <code>junctools --help</code> will provide you\
    \ with the program options.</p>\n<p>For much more information about portcullis'\
    \ capabilities and how to configure and run it, an online version of the manual\
    \ can be found here: <a href=\"https://portcullis.readthedocs.org/en/latest/\"\
    \ rel=\"nofollow\">https://portcullis.readthedocs.org/en/latest/</a>.</p>\n<h2>\n\
    <a id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Licensing</h2>\n\
    <p>GNU GPL V3.  See COPYING file for more details.</p>\n<h2>\n<a id=\"user-content-authors\"\
    \ class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Authors</h2>\n<ul>\n<li>Daniel\
    \ Mapleson</li>\n<li>Luca Venturini</li>\n<li>David Swarbreck</li>\n</ul>\n<p>See\
    \ AUTHORS file for more details.</p>\n<h2>\n<a id=\"user-content-acknowledgements\"\
    \ class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgements</h2>\n<p>Affiliation:\
    \ The Earlham Institute (EI)\nFunding: The Biotechnology and Biological Sciences\
    \ Research Council (BBSRC)</p>\n"
  stargazers_count: 28
  subscribers_count: 14
  topics: []
  updated_at: 1625055906.0
DoaneAS/atacflow:
  data_format: 2
  description: Analysis pipeline for ATACseq data using Nextflow
  filenames:
  - Singularity
  full_name: DoaneAS/atacflow
  latest_release: null
  readme: '

    <h1>

    <a id="user-content-atacflow" class="anchor" href="#atacflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>AtacFlow</h1>

    <h2>

    <a id="user-content-analysis-pipeline-for-atac-seq-data-using-nextflow" class="anchor"
    href="#analysis-pipeline-for-atac-seq-data-using-nextflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Analysis pipeline for
    ATAC-seq data using Nextflow</h2>

    <p>This pipeline inspired by and based on the <a href="https://www.encodeproject.org/atac-seq/"
    rel="nofollow">ENCODE ATAC-seq processubg pipeline</a> and

    the <em>prototype</em> ATAC-seq pipeline

    developed by <a href="https://github.com/kundajelab/atac_dnase_pipelines">Anshul
    Kundaje''s lab</a> at Stanford University</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <ul>

    <li>Install <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>

    </li>

    <li>Clone repository

    <ul>

    <li>using nextflow: <code>nextflow clone DoaneAS/atacflow ./</code>

    </li>

    <li>or using git: <code>git clone https://github.com/DoaneAS/atacflow.git</code>

    </li>

    </ul>

    </li>

    <li>Install conda dependencies:

    <pre><code>conda update conda

    conda env create --file requirements.atacFlow.yml

    conda env create --file deep.yml

    </code></pre>

    </li>

    </ul>

    <h2>

    <a id="user-content-setup-data" class="anchor" href="#setup-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup data</h2>

    <ul>

    <li>ATAC-seq reads go in <code>data/&lt;Sample&gt;/*_001.fastq.gz</code>

    <ul>

    <li>Concatenate read pairs per sample <code>parallel -j8 ''./bin/catlanes.sh {}''
    ::: data/Sample*</code>

    </li>

    </ul>

    </li>

    <li>Create sample index: <code>python bin/makeIndex.py</code>

    </li>

    </ul>

    <h2>

    <a id="user-content-execution" class="anchor" href="#execution" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Execution</h2>

    <pre><code>nextflow run -with-trace -with-dag flow.html main.nf --index sampleIndex.csv
    --genome hg38

    </code></pre>

    <ul>

    <li>supported genomes on panda WCM cluster:  hg38, mm10</li>

    </ul>

    '
  stargazers_count: 5
  subscribers_count: 1
  topics: []
  updated_at: 1623367033.0
Emresav/popcorn:
  data_format: 2
  description: The source code of POPCORN planner (also a MILP Compilation collaborated
    with Chiara Piacentini)
  filenames:
  - Singularity
  full_name: Emresav/popcorn
  latest_release: null
  readme: '<h1>

    <a id="user-content-popcorn" class="anchor" href="#popcorn" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>popcorn</h1>

    <p>POPCORN is a temporal-numeric planning software that I have developed during
    my PhD studies at King''s College London under supervision of Prof Maria Fox and
    Prof Derek Long. POPCORN can reason about action-specific numeric parameters that
    can take their values from relatively large domains. The planner has a freedom
    of choosing their values during planning process. Practically speaking, this means
    that you can now model actions that can have multiple flexible numeric parameters,
    such as the withdrawal amount from the cashpoint (e.g. <code>10&lt;= ?cash &lt;=
    100</code>), or the refuel amount, as in PDDL community these parameters can only
    be defined with fixed values. The language we use when developing our domains,
    the extended version of PDDL, is quite straightforward; so I highly recommend
    users to inspect them to decide whether this is something that you want in your
    planning scenarios. This work was published on ECAI 2016 under the title of <a
    href="https://kclpure.kcl.ac.uk/portal/files/56331945/FAIA285_1185.pdf" rel="nofollow">Planning
    Using Actions with Control Parameters</a>.</p>

    <p>Also note that this code base includes a MILP Compilation of optimal numeric
    planning problems with control parameters. I collaborated with <em>Chiara Piacentini</em>
    on this work.</p>

    <h1>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h1>

    <p>POPCORN makes use of various other tools that are available online, these are:</p>

    <p>For parsing the PDDL domain: Flex, Bison

    Optimisation tools: CLP, CoinUtils, CBC, CBLAS, CGL, CPLEX</p>

    <pre><code>apt-get update

    apt-get -y install g++ make flex bison cmake doxygen coinor-clp coinor-libcbc-dev
    coinor-libclp-dev coinor-libcoinutils-dev coinor-libosi-dev coinor-libcgl-dev
    libbz2-dev libgsl-dev libz-dev

    </code></pre>

    <p><strong>NOTE</strong>: You will need to install CPLEX from their website!</p>

    <h1>

    <a id="user-content-running-popcorn" class="anchor" href="#running-popcorn" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running POPCORN</h1>

    <ol>

    <li>Clone this repository to your local machine:</li>

    </ol>

    <pre><code>https://github.com/Emresav/popcorn.git

    </code></pre>

    <ol start="2">

    <li>Export the directories of CPLEX libraries:</li>

    </ol>

    <pre><code>export CPLEX="/my/location/to/libcplex.a"

    export ILOCPLEX="/my/location/to/libilocplex.a"

    export CONCERT="/my/location/to/libconcert.a"

    export CPLEX_INCLUDES="/my/location/to/cplex/include"

    export CONCERT_INCLUDES="/my/location/to/concert/include"

    </code></pre>

    <ol start="3">

    <li>Compiling the source code:</li>

    </ol>

    <pre><code>cd /planner/planner

    ls -la

    cmake .

    make clean

    make popf3-clp

    </code></pre>

    <ol start="4">

    <li>

    <p>Features of POPCORN

    In order to see the available features of POPCORN, simply run the executable:
    <code>/planner/planner/popf/popf3-clp</code></p>

    </li>

    <li>

    <p>Running POPCORN:</p>

    </li>

    </ol>

    <pre><code>/planner/planner/popf/popf3-clp $DOMAINFILE $PROBLEMFILE &gt; $PLANFILE

    </code></pre>

    <p>That''s it! I have included various domains and problem instances that are
    used during my experiments. Almost all of them are new and quite rich in terms
    of temporal and numeric features. Have fun, and please do not hesitate to contact
    me if you have any issues during compiling and running the planner.</p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1595445458.0
EnriqueDoster/bioinformatic-nextflow-pipelines:
  data_format: 2
  description: Collection of bioinformatic pipelines written in nextflow
  filenames:
  - containers/Singularity
  - containers/Singularity.RGI
  - containers/Singularity.qiime2
  - containers/Singularity.cfsansnp
  full_name: EnriqueDoster/bioinformatic-nextflow-pipelines
  latest_release: null
  readme: '<h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667"
    alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/6b7af09ab5d3e54feb3acda4c7b70aef9718f2928a49a50c92ea6ce95e96b2f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e657874666c6f772d254532253839254135302e32352e312d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/Nextflow-%E2%89%A50.25.1-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p>The goal of many metagenomics studies is to characterize the content and relative
    abundance of sequences of interest from the DNA of a given sample or set of samples.
    You may want to know what is contained within your sample or how abundant a given
    sequence is relative to another.</p>

    <p>Often, metagenomics is performed when the answer to these questions must be
    obtained for a large number of targets where techniques like multiplex PCR and
    other targeted methods would be too cumbersome to perform. AmrPlusPlus can process
    the raw data from the sequencer, identify the fragments of DNA, and count them.
    It also provides a count of the polymorphisms that occur in each DNA fragment
    with respect to the reference database.</p>

    <p>Additionally, you may want to know if the depth of your sequencing (how many
    reads you obtain that are on target) is high enough to identify rare organisms
    (organisms with low abundance relative to others) in your population. This is
    referred to as rarefaction and is calculated by randomly subsampling your sequence
    data at intervals between 0% and 100% in order to determine how many targets are
    found at each depth. AmrPlusPlus can perform this analysis as well.</p>

    <p>With AmrPlusPlus, you will obtain count files for each sample that can be combined
    into a count matrix and analyzed using any statistical and mathematical techniques
    that can operate on a matrix of observations.</p>

    <h2>

    <a id="user-content-more-information" class="anchor" href="#more-information"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More
    Information</h2>

    <ul>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/requirements.md">Software
    Requirements</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/installation.md">Installation</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/usage.md">Usage</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/configuration.md">Configuration</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/output.md">Output</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/dependencies.md">Dependencies</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/contact.md">Contact</a></li>

    </ul>

    <h2>

    <a id="user-content-description-of-scripts" class="anchor" href="#description-of-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Description
    of scripts</h2>

    <p>main_qiime2.nf</p>

    <pre><code>nextflow run main_qiime2.nf --reads "/s/angus/index/projs/mega_tylan/concat_16S_LN/raw_data/*_{1,2}.fq"
    --output XIT_LN_qiime2 -profile local --metadata /media/AngusWorkspace/run_Jake/LN_metadata.tsv
    --classifier /media/AngusWorkspace/run_Jake/bioinformatic-nextflow-pipelines/gg-13-8-99-515-806-nb-classifier.qza
    -resume --threads 25

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619390181.0
EnriqueDoster/sing_biotools:
  data_format: 2
  description: Bioinformatic tools in a singularity container
  filenames:
  - containers/Singularity.etoki
  - containers/Singularity
  - containers/Singularity.lyveset
  full_name: EnriqueDoster/sing_biotools
  latest_release: null
  readme: '<h1>

    <a id="user-content-sing_biotools" class="anchor" href="#sing_biotools" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>sing_biotools</h1>

    <p>Bioinformatic tools in a singularity container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606287922.0
HERA-Team/hera-singularity:
  data_format: 2
  description: Singularity recipe for HERA software
  filenames:
  - Singularity.rtp
  - Singularity.casa6_modular
  - Singularity.hera1
  - Singularity.casa_imaging
  full_name: HERA-Team/hera-singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hera-singularity\" class=\"anchor\" href=\"\
    #hera-singularity\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>hera-singularity</h1>\n<p><a href=\"https://singularity-hub.org/collections/4892\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-notice\" class=\"\
    anchor\" href=\"#notice\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Notice</h2>\n<p><strong>April 27, 2021</strong>:\n\
    <a href=\"https://singularityhub.github.io/singularityhub-docs/2021/going-read-only/\"\
    \ rel=\"nofollow\">Singularity Hub remote built service is no longer available.</a>\
    \ We are considering other alternative. The old Singularity Hub builds can still\
    \ be accessed via the badge link above, which now redirects to DataLad. The <code>singularity\
    \ pull</code> can also do the pull from datalad URL. We will manually build and\
    \ upload to Ilifu for the time being.</p>\n<p><strong>April 28, 2021</strong>:\n\
    The containers have been built and are available at <code>/ilifu/astro/projects/hera/containers</code>,\
    \ with a few additional new containers that will be documented soon. We will keep\
    \ rebuilding and replacing these files weekly, to keep the software stack up to\
    \ date with the development, until we have an automated solution.</p>\n<hr>\n\
    <p>This repository contains recipe files for building singularity containers for\
    \ the HERA software suits. The containers are remotely built on Singularity Hub\
    \ when the recipes are pushed to the <code>main</code> branch. Container images\
    \ can be directly download from the the badge link above or by using the <code>singularity\
    \ pull</code> command line (see <a href=\"##-Singularity-Commands\">below</a>).\
    \ Ilifu users, make sure to read <a href=\"###-Specific-Usages-for-Ilifu\">Specific\
    \ Usages for Ilifu</a> section and check the relevant page on the HERA wiki.</p>\n\
    <h2>\n<a id=\"user-content-about-container-and-singularity\" class=\"anchor\"\
    \ href=\"#about-container-and-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>About Container and Singularity</h2>\n\
    <p>Containers are encapsulated software environments and abstract the software\
    \ and applications from the underlying operating system. This allows users to\
    \ run workflows in customized environments, switch between environments, and to\
    \ share these environments with colleagues and research teams.</p>\n<p>Singularity\
    \ is a free, cross-platform and open-source computer program that performs operating-system-level\
    \ virtualization also known as containerization (another widely used one being\
    \ Docker).</p>\n<p>A singularity container is required for computing on the Ilifu\
    \ cloud-computing cluster, which HERA has access (see the HERA wiki page on this).</p>\n\
    <p>Suggestion for other container recipes and implementations are welcome!</p>\n\
    <h2>\n<a id=\"user-content-container-content\" class=\"anchor\" href=\"#container-content\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Container Content</h2>\n<h3>\n<a id=\"user-content-python-packages\"\
    \ class=\"anchor\" href=\"#python-packages\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Python Packages</h3>\n<p>All\
    \ containers are built with <code>Ubuntu 20.04</code> and <code>miniconda</code>\
    \ with <code>python=3.8</code> unless otherwise specify <a href=\"###-Different-Between-Containers:\"\
    >below</a>, and come standard with the following packages:</p>\n<table>\n<thead>\n\
    <tr>\n<th>Data Analysis</th>\n<th>Astronomical</th>\n<th>HERA</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td><code>dask</code></td>\n<td><code>aipy</code></td>\n<td><code>linsolve</code></td>\n\
    </tr>\n<tr>\n<td><code>jupyterlab</code></td>\n<td><code>astropy</code></td>\n\
    <td><code>uvtools</code></td>\n</tr>\n<tr>\n<td><code>matplotlib</code></td>\n\
    <td><code>astropy-healpix</code></td>\n<td><code>hera_qm</code></td>\n</tr>\n\
    <tr>\n<td><code>numpy</code></td>\n<td><code>astroquery</code></td>\n<td><code>hera_cal</code></td>\n\
    </tr>\n<tr>\n<td><code>pandas</code></td>\n<td><code>cartopy</code></td>\n<td><code>hera_sim</code></td>\n\
    </tr>\n<tr>\n<td><code>scipy</code></td>\n<td><code>healpy</code></td>\n<td><code>hera_psepc</code></td>\n\
    </tr>\n<tr>\n<td><code>scikit-learn</code></td>\n<td>\n<code>pyuvdata</code><sup><a\
    \ href=\"#myfootnote1\">1</a></sup>\n</td>\n<td></td>\n</tr>\n<tr>\n<td><code>xarray</code></td>\n\
    <td>\n<code>pyuvsim</code><sup><a href=\"#myfootnote2\">2</a></sup>\n</td>\n<td></td>\n\
    </tr>\n</tbody>\n</table>\n<p><a name=\"user-content-myfootnote2\">1</a>: with\
    \ CASA measurement sets, HEALPix beam, and CST beam functionalities<br>\n<a name=\"\
    user-content-myfootnote1\">2</a>: with profiling and full simulator</p>\n<h3>\n\
    <a id=\"user-content-different-between-containers\" class=\"anchor\" href=\"#different-between-containers\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Different Between Containers:</h3>\n<ul>\n<li>\n<code>hera1</code>:\n\
    <ul>\n<li>Intended for general-purpose computing with most of the commonly used\
    \ data analysis, astronomical, and HERA software packages</li>\n</ul>\n</li>\n\
    <li>\n<code>rtp</code>:\n<ul>\n<li>For running the RTP pipeline and analysis with\
    \ <code>makeflow</code>.</li>\n<li>Equivalent to <code>hera1</code> with an addition\
    \ of <code>hera_opm</code>, <code>hera_mc</code>, and  <code>hera_notebook_templates</code>.</li>\n\
    <li>\n<code>hera_pipelines</code> is cloned to <code>/usr/local</code>\n</li>\n\
    </ul>\n</li>\n<li>\n<code>casa_imaging</code>:\n<ul>\n<li>Equivalent to <code>hera1</code>\
    \ with a full installation of <code>casa-6</code>\n</li>\n</ul>\n</li>\n<li>\n\
    <code>casa6_modular</code>:\n<ul>\n<li>Equivalent to <code>hera1</code> with a\
    \ pip-wheel installation of <code>casa-6</code>, making <code>casatasks</code>,\
    \ <code>casatools</code>, and <code>casampi</code> packages available (see <a\
    \ href=\"https://casa-pip.nrao.edu/\" rel=\"nofollow\">https://casa-pip.nrao.edu/</a>).</li>\n\
    <li>Based on <code>Python 3.6</code> and <code>Ubuntu 18.04</code> for casa-pip\
    \ compatibility.</li>\n</ul>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-environment-variables\"\
    \ class=\"anchor\" href=\"#environment-variables\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Environment Variables</h3>\n\
    <p>The following environment variables are also exported in all containers:</p>\n\
    <pre><code>CONDA_INSTALL_PATH=\"/usr/local/miniconda3\"\nCONDA_INIT_SCRIPT=\"\
    $CONDA_INSTALL_PATH/etc/profile.d/conda.sh\"\n</code></pre>\n<p>The <code>rtp</code>\
    \ container has an additional environment variable that point to <code>hera_pipelines</code>.</p>\n\
    <pre><code>HERA_PIPELINES_PATH=\"/usr/local/hera_pipelines\"\n</code></pre>\n\
    <h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <h3>\n<a id=\"user-content-singularity-commands\" class=\"anchor\" href=\"#singularity-commands\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity Commands</h3>\n<h4>\n<a id=\"user-content-pull\" class=\"\
    anchor\" href=\"#pull\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><code>pull</code>\n</h4>\n<p>Use <code>singularity\
    \ pull</code> to download the container from Singularity Hub</p>\n<pre><code>$\
    \ singularity pull [name_to_save_the_image_(optional)] shub://HERA-Team/hera-singularity:&lt;recipe&gt;\n\
    </code></pre>\n<p>For example,</p>\n<pre><code>$ singularity pull rtp.sif shub://HERA-Team/hera-singularity:rtp\n\
    INFO:    Downloading shub image\n 1.98 GiB / 1.98 GiB [=======================================================]\
    \ 100.00% 13.12 MiB/s 2m34s\n</code></pre>\n<h4>\n<a id=\"user-content-shell\"\
    \ class=\"anchor\" href=\"#shell\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><code>shell</code>\n</h4>\n<p>The\
    \ <code>singularity shell</code> command allows you to spawn a new shell within\
    \ your container and interact with it as though it were a small virtual machine.</p>\n\
    <p>By default, <code>shell</code> invokes <code>/bin/sh --norc</code>, which means\
    \ that <code>.bashrc</code> will not be executed (more on this <a href=\"https://github.com/hpcng/singularity/issues/643\"\
    >here</a>) and thus <code>conda</code> will not be initialized. To have <code>conda</code>\
    \ working, you can do one of the following:</p>\n<p>a) Run <code>exec $SHELL</code>\
    \ inside the singularity shell. If <code>$SHELL</code> is <code>\\bin\\bash</code>\
    \ (as in our Ubuntu build), <code>.bashrc</code> will be read.</p>\n<pre><code>$\
    \ singularity shell rtp.sif\nSingularity&gt; exec $SHELL\n</code></pre>\n<p>b)\
    \ Manually execute the conda initialization script inside singularity shell. A\
    \ <code>CONDA_INIT_SCRIPT</code> environment variable pointing to the absolute\
    \ path of the script (<code>/usr/local/miniconda3/etc/profile.d/conda.sh</code>),\
    \ is made available for this purpose. Note that <code>.</code> must be used as\
    \ <code>source</code> won't work under <code>sh</code>.</p>\n<pre><code>$ singularity\
    \ shell rtp.sif\nSingularity&gt; . $CONDA_INIT_SCRIPT\n</code></pre>\n<p>b) Specify\
    \ <code>\\bin\\bash</code> as a shell to use when executing the <code>shell</code>\
    \ command, either by using the <code>SINGULARITY_SHELL</code> environment variable,</p>\n\
    <pre><code>$ SINGULARITY_SHELL=/bin/bash singularity shell hera-rtp.sif\n</code></pre>\n\
    <p>or <code>-s</code> option,</p>\n<pre><code>$ singularity shell -s /bin/bash\
    \ hera-rtp.sif\n</code></pre>\n<h4>\n<a id=\"user-content-exec\" class=\"anchor\"\
    \ href=\"#exec\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><code>exec</code>\n</h4>\n<p>The <code>singularity\
    \ exec</code> command allows you to execute a custom command within a container\
    \ by specifying the image file.</p>\n<pre><code>$ singularity exec rtp.sif echo\
    \ \"Hello World!\"\nHello World!\n</code></pre>\n<pre><code>$ cat myscript.sh\n\
    Hello World!\n$ singularity exec rtp.sif bash myscript.sh\nHello World!\n</code></pre>\n\
    <h3>\n<a id=\"user-content-file-permission-and-bind-path\" class=\"anchor\" href=\"\
    #file-permission-and-bind-path\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>File Permission and Bind Path</h3>\n\
    <p>Singularity containers run as the user and share host services. When Singularity\
    \ \u2018switch\u2019 from the host operating system to the containerized operating\
    \ system, the OS-level system files on the host becomes inaccessible. (the root\
    \ user on the host system is also different from the root in the container!)</p>\n\
    <p>By default, the user home directory on the host system will be mapped to the\
    \ user home directory in the container, preserving all file permission. On Ilifu,\
    \ the shared data paths on the host are also mapped.</p>\n<h3>\n<a id=\"user-content-specific-usages-for-ilifu\"\
    \ class=\"anchor\" href=\"#specific-usages-for-ilifu\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Specific Usages\
    \ for Ilifu</h3>\n<h4>\n<a id=\"user-content-container-file-locations\" class=\"\
    anchor\" href=\"#container-file-locations\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Container File Locations</h4>\n\
    <p>Recent builds are available at <code>/ilifu/astro/projects/hera/containers</code></p>\n\
    <h4>\n<a id=\"user-content-using-a-hera-container-as-a-jupyter-kernel\" class=\"\
    anchor\" href=\"#using-a-hera-container-as-a-jupyter-kernel\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using a\
    \ HERA container as a Jupyter kernel</h4>\n<p>See <a href=\"https://docs.ilifu.ac.za/#/tech_docs/software_environments?id=using-a-custom-container-as-a-jupyter-kernel\"\
    \ rel=\"nofollow\">this page</a> on Ilifu documentation. We may try to semi-automate\
    \ this process for users with a shell script in the future.</p>\n"
  stargazers_count: 0
  subscribers_count: 13
  topics: []
  updated_at: 1624348511.0
HanLabUNLV/hanlab_singularity_defs:
  data_format: 2
  description: definition files for containers used in Hanlab
  filenames:
  - singularity.R.3.6.3.phylo/R.3.6.3.phylo.def
  - singularity.phylo/phylo.def
  - singularity.mkl/mkl.ubuntu.def
  - singularity.mkl/mkl.def
  - singularity.SAD/SAD.def
  - singularity.rnaseq/rnaseq.def
  - singularity.py37.ml.mkl/py37.ml.mkl.def
  - singularity.Rconda/R.3.6.3.def
  - singularity.py37.ml.openblas/py37.ml.openblas.def
  - singularity.R.4.0.2.Bioc/R.4.0.2.Bioc.def
  - singularity.R.3.6.3.Bioc/R.3.6.3.Bioc.def
  full_name: HanLabUNLV/hanlab_singularity_defs
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc_mpi_cuda_singu_def_file" class="anchor" href="#hpc_mpi_cuda_singu_def_file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>hpc_mpi_cuda_singu_def_file</h1>

    <p>A collect of definition files to build images for singularity containers, which
    includes hpc benchmarks and mpis with cuda support.</p>

    <p><a href="https://singularity-hub.org/collections/4181" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1617130184.0
Irfanwustl/Research_code:
  data_format: 2
  description: null
  filenames:
  - BlueprintPipeline/Resource/gemBS-2.1.1/Singularity
  full_name: Irfanwustl/Research_code
  latest_release: null
  readme: '<h1>

    <a id="user-content-research_code" class="anchor" href="#research_code" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Research_code</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624721934.0
JCSDA-internal/containers:
  data_format: 2
  description: Contains tools for building and distributing the JEDI/JCSDA Containers
  filenames:
  - Singularity.tutorial
  - Singularity.tutorial-test
  - Singularity.gnu-openmpi-dev
  - Singularity.clang-mpich-dev
  full_name: JCSDA-internal/containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Containers</h1>

    <p>This repository contains tools for building Singularity and Charliecloud containers.</p>

    <p>Furthermore, since Intel Docker containers cannot be distributed through Docker
    Hub, they are also handled here.</p>

    <p>The instructions below are intended for the JEDI core team, who are responsible
    for maintaining JEDI containers and distributing them publicly or privately.</p>

    <p>However, since the JEDI core team cannot legally distribute intel containers
    for licensing reasons, JEDI users and developers are encouraged to build their
    own intel development container.</p>

    <p><a href="myIntel/Intel.md">See here for instructions on how to build your own
    JEDI Intel development container: Docker, Singularity, or Charliecloud</a></p>

    <h2>

    <a id="user-content-organization-of-repository" class="anchor" href="#organization-of-repository"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Organization
    of Repository</h2>

    <ul>

    <li>top-level directory: tools for building Singularity, Docker, and Charliecloud
    containers</li>

    <li>

    <code>vagrant</code>: tools for building Vagrant virtual machines that are provisioned
    to run JEDI containers</li>

    <li>

    <code>modulefiles</code>, <code>runscripts</code>: These directories contain sample
    modulefiles and batch scripts for running JEDI "Supercontainers" across nodes
    on HPC systems</li>

    <li>

    <code>myIntel</code> is intended to help users from the general JEDI community
    build their own JEDI intel development containers.</li>

    <li>

    <code>intel19</code> contains deprecated build tools for intel Parallel Studio.</li>

    <li>

    <code>examples</code> is a sandbox, containing instructive examples of how to
    implement features that may not be used now but might be used in the future.  An
    example is how to build writable singularity containers.   These scripts are not
    maintained; there is no guarantee that they will run as is.</li>

    </ul>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>In order to build Docker, Singularity, or Charliecloud containers, you will
    of course need to have the appropriate software installed, namely <code>docker</code>,
    <code>singularity</code>, or <code>charliecloud</code>.  Members of the JEDI core
    team can launch an AWS node with all of these pre-installed.  Or, you can install
    them yourself as described in the JEDI documentation.</p>

    <p>The scripts in this directory also assume that you have root privileges.</p>

    <p>Also, core developers often find it necessary to access feature or bugfix branches
    of the jedi stack for testing purposes.  So, the <code>build_container.sh</code>
    script uses the JCSDA-internal (private) jedi-stack repo.  For this reason, you
    need to provide an ssh key for access.  This script uses a generic academy ssh
    key to ensure that it has read-only access to selected JCSDA repositories.  If
    you do not have access to this key, you can replace it with another by changing
    the <code>KEY</code> variable in <code>build_containers.sh</code>.  But it is
    recommended to retain the read-only access.  You can build the <code>myIntel</code>
    container without an ssh key.</p>

    <p>Note: to build the tutorial container you have to copy the ssh key into the
    directory <code>ssh-key</code> and modify the singularity recipe file accordingly
    if it has a different name.</p>

    <h2>

    <a id="user-content-build-a-dev-container" class="anchor" href="#build-a-dev-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    a dev container</h2>

    <p>To build a Singularity, Charliecloud, and/or a Docker container, enter this
    and respond to the prompts to build the containers of your choice.</p>

    <div class="highlight highlight-source-shell"><pre>./build_containers.sh <span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span></pre></div>

    <p>where <code>&lt;name&gt;</code> matches one of the available Dockerfile extensions,
    e.g. <code>gnu-openmpi-dev</code>.  It also accepts an optional second argument
    to specify a tag.  The default tag is <code>beta</code>.</p>

    <p>For the the gnu and clang containers, the Singularity containers are built
    directly from the images on Docker Hub.  A Docker container will only be created
    if you choose to build a Charliecloud container, which is then built from the
    Docker container.</p>

    <p>For the <code>intel-impi-dev</code> container, a Docker file is always created
    and then the Singularity and Charliecloud containers are created from that.</p>

    <p>The intel Docker container is the one used for CI so it is kept relatively
    compact.  If you wish to add additional components such as Vtune, it is recommended
    you use the companion scripts in the <code>myIntel</code> directory.  These scripts
    are simplified in the root directory but they are intended for use by the general
    JEDI user and developer community.  The main simplification is that there is no
    need to supply an ssh key because those scripts only access the public jedi-stack
    repo.</p>

    <p>To build the tutorial container, just specify <code>tutorial</code> for the
    <code>&lt;name&gt;</code>.  The tutorial container is exclusively a Singularity
    container and uses GNU-OpenMPI: There are no clang or intel options and there
    are no Docker or Charliecloud containers created.</p>

    <p>The Singularity and Charliecloud container files will be placed in a subdirectory
    called <code>containers</code>.</p>

    <p>Note: building the Mellanox-enabled HPC container isn''t yet automated.  For
    this or other non-standard cases, you can edit the Dockerfiles, Singularity files,
    and scripts manually as needed.</p>

    <h2>

    <a id="user-content-test-the-container" class="anchor" href="#test-the-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Test
    the container</h2>

    <p>Before distributing a container, it''s always important to test it.  A good
    test is usually to enter the container and then build and test fv3-bundle.</p>

    <p>To enter the Singularity container, enter:</p>

    <div class="highlight highlight-source-shell"><pre>singularity shell -e <span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span>.sif</pre></div>

    <p>And, for CharlieCloud, you can do this:</p>

    <div class="highlight highlight-source-shell"><pre>mkdir -p <span class="pl-k">~</span>/ch-jedi

    <span class="pl-c1">cd</span> <span class="pl-k">~</span>/ch-jedi

    ch-tar2dir <span class="pl-k">&lt;</span>path-to-tarfile<span class="pl-k">&gt;</span>/ch-jedi-gnu-openmpi-dev.tar.gz
    <span class="pl-c1">.</span>

    ch-run ch-jedi-gnu-openmpi-dev -- bash</pre></div>

    <h2>

    <a id="user-content-distribute-the-latest-container" class="anchor" href="#distribute-the-latest-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Distribute
    the latest container</h2>

    <p>The latest Singularity containers are made available on Sylabs Cloud, the latest
    Charliecloud containers are made available on a public AWS S3 bucket, and the
    latest intel containers are made available on a private AWS S3 bucket.  The purpose
    of the <code>push_containers.sh</code> script is to push the new container to
    these distribution sites.</p>

    <p>The <code>beta</code> tag is a special case.  If the tag is <code>beta</code>,
    it is assumed that, after it passes tests, this container is ready to be deployed
    as <code>latest</code>.  In this case, a copy of the current <code>latest</code>
    container is saved with the tag <code>revert</code>.</p>

    <p>So, the typical workflow would be to enter</p>

    <div class="highlight highlight-source-shell"><pre>./push_containers.sh <span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span></pre></div>

    <p>As with <code>build_containers.sh</code>, <code>push_containers.sh</code> accepts
    an optional second argument which is a tag.  This is sometimes useful for experimental
    cases but is not part of the normal workflow.</p>

    <p>For instructions on how to download these containers, see <a href="https://jointcenterforsatellitedataassimilation-jedi-docs.readthedocs-hosted.com/en/latest/using/jedi_environment/containers.html#available-containers"
    rel="nofollow">the JEDI Documentation</a>.</p>

    <h2>

    <a id="user-content-tagged-releases" class="anchor" href="#tagged-releases" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tagged Releases</h2>

    <p>Most developers use the latest development containers but it''s also useful
    to have tagged containers that accompany JEDI releases.  This is particularly
    relevant for scientific users (as opposed to developers) who may wish to use tagged
    releases and containers for reproducibility in research.  Tagged containers can
    also be used to provide stability for operational or Near-Real-Time (NRT) workflows.</p>

    <p>Sylabs cloud has a storage quota (currently 11 GB) that would be quickly overwhelmed
    if we were to store many release containers there.  So, this is reserved for "latest"
    and "revert".</p>

    <p>Tagged singularity containers are distributed on the <a href="http://data.jcsda.org/pages/containers.html"
    rel="nofollow">JCSDA Public Container Repository</a> along with the latest and
    tagged Charliecloud containers.  Tagged docker release containers can be obtained
    from Docker Hub.  For example:</p>

    <div class="highlight highlight-source-shell"><pre>docker pull jcsda/docker-gnu-openmpi-dev:v1.0.0</pre></div>

    '
  stargazers_count: 2
  subscribers_count: 6
  topics: []
  updated_at: 1624985725.0
JoshLoecker/MAPT:
  data_format: 2
  description: The purpose of this project is to map Oxford Nanopore Sequencing data
    down to the species level
  filenames:
  - setup/Singularity
  full_name: JoshLoecker/MAPT
  latest_release: null
  readme: '<h1>

    <a id="user-content-mapt" class="anchor" href="#mapt" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MAPT</h1>

    <p><a href="https://github.com/JoshLoecker/MAPT/wiki">Please view the Wiki</a>
    for more information.</p>

    <h3>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h3>

    <p>If you need help, have questions, or have feature ideas please <a href="https://github.com/JoshLoecker/MAPT/issues">open
    a new issue</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620064890.0
LLNL/MaPPeRTrac:
  data_format: 2
  description: Massively Parallel, Portable, and Reproducible Tractography
  filenames:
  - container/Singularity
  full_name: LLNL/MaPPeRTrac
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-mappertrac\" class=\"anchor\" href=\"#mappertrac\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>MaPPeRTrac</h1>\n<p>Massively Parallel, Portable, and Reproducible\
    \ Tractography (MaPPeRTrac) is a brain tractography workflow for high performance\
    \ computing. It incorporates novel technologies to simplify and accelerate neuroimaging\
    \ research.\n<br>\n<br></p>\n<h3>\n<a id=\"user-content-setup\" class=\"anchor\"\
    \ href=\"#setup\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Setup</h3>\n<p>Requirements:</p>\n<ul>\n<li>Python\
    \ 3.5+</li>\n<li>SLURM job scheduling on a multi-node system</li>\n</ul>\n<p><b>1.\
    \ Install NumPy and Parsl</b></p>\n<p><code>pip3 install parsl numpy scipy</code><br>\n\
    (<code>pip3 install parsl numpy scipy --user</code> for non-root systems)</p>\n\
    <p><b>2. Clone repository</b></p>\n<p><code>git clone git@github.com:LLNL/MaPPeRTrac.git</code><br>\n\
    <code>cd MaPPeRTrac/</code></p>\n<p><b>3. Load a Singularity container</b></p>\n\
    <p>Requirements:</p>\n<ul>\n<li>Singularity 3.0+ (<a href=\"https://www.sylabs.io/guides/3.0/user-guide/\"\
    \ rel=\"nofollow\">https://www.sylabs.io/guides/3.0/user-guide/</a>)</li>\n</ul>\n\
    <p>Building the container:<br>\ni. Obtain root access (you can copy and run the\
    \ image in a non-root system afterwards).<br>\nii. Place a Freesurfer <code>license.txt</code>\
    \ in the repo directory (<a href=\"https://surfer.nmr.mgh.harvard.edu/fswiki/License\"\
    \ rel=\"nofollow\">https://surfer.nmr.mgh.harvard.edu/fswiki/License</a>).<br>\n\
    iii. <code>./container/build.sh</code>\n<br>\nNotes:</p>\n<ul>\n<li>Make sure\
    \ to set <code>container_path</code> to the Singularity container's location.</li>\n\
    <li>If you are having trouble building the container, try branch <code>no_viz</code>.\
    \ This will disable render functionality.</li>\n<li>Alternatively, download the\
    \ image <a href=\"https://drive.google.com/file/d/1lh0_5GO6-7qIznjvIcSMY-Ua8iBpZ4DJ/view?usp=sharing\"\
    \ rel=\"nofollow\">here</a>.\n<br>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-launch\"\
    \ class=\"anchor\" href=\"#launch\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Launch</h3>\n<p><code>./s_run_all.py\
    \ &lt;config_json&gt;</code></p>\n<p><strong>OR</strong></p>\n<p><code>./s_run_all.py\
    \ &lt;arg1&gt;=val1 &lt;arg2&gt;=val2 etc...</code>\n<br>\nSpecify parameters\
    \ either in a config JSON file or as command line arguments. See <code>examples/dummy_config.json</code>\
    \ for an example.</p>\n<h3>\n<a id=\"user-content-file-overview\" class=\"anchor\"\
    \ href=\"#file-overview\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>File Overview</h3>\n<pre><code>TracktographyScripts/\n\
    +- container/\n|  +- build.sh\n|  +- Singularity               # Singularity build\
    \ recipe\n|\n+- examples\n|  +- dataset_description.json  # Example of the BIDS\
    \ dataset description\n|  +- dummy_config.json         # Example of the config\
    \ JSON\n|  +- dummy_dicom/\n|  +- dummy_nifti/\n|  +- dummy_subjects.json    \
    \   # Example of the subjects JSON\n|\n+- license.txt                  # Freesurfer\
    \ license. NOTE: not included, required to build Singularity container\n+- LICENSE\
    \                      # MaPPeRTrac license.\n|\n+- lists/\n|  +- connectome_idxs.txt\
    \       # Brain region indices for .mat connectome files\n|  +- list_edges_reduced.txt\
    \    # Default edges to compute with Probtrackx and EDI (930 edges)\n|  +- list_edges_all.txt\
    \        # All possible edges (6643 edges)\n|  +- render_targets.txt        #\
    \ NiFTI files to visualize with s4_render\n|\n+- README.md\n|\n+- s_run_all.py\
    \                 # Main script\n|\n+- subscripts/\n   +- __init__.py\n   +- maskseeds.py\
    \              # Helper functions for s2b_freesurfer.py\n   +- run_vtk.py    \
    \            # Helper script for s4_render.py\n   +- s_debug.py              \
    \  # For debugging\n   +- s1_dti_preproc.py\n   +- s2a_bedpostx.py\n   +- s2b_freesurfer.py\n\
    \   +- s3_probtrackx.py\n   +- s4_render.py\n   +- utilities.py              #\
    \ General utility functions\n</code></pre>\n<p><br>\n<br></p>\n<h3>\n<a id=\"\
    user-content-output-overview\" class=\"anchor\" href=\"#output-overview\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Output\
    \ Overview</h3>\n<p>The following are the most important output files. This list\
    \ is not comprehensive.</p>\n<pre><code>&lt;OUTPUT DIRECTORY&gt;/\n+- sourcedata/\
    \                                              # DICOM preprocessing data\n+-\
    \ rawdata/                                                 # BIDS-compliant NiFTI\
    \ imaging data\n+- derivatives/\n   +- sub-&lt;SUBJECT NAME&gt;\n      +- [ses-&lt;SESSION\
    \ NAME&gt;]                               # If session name specified, outputs\
    \ will be in a session directory\n         +- connectome_idxs.txt            \
    \                 # Brain region indices for .mat connectome files\n         +-\
    \ connectome_#samples_oneway.txt                  # Oneway connectome in list\
    \ form. Each edge has four columns:\n                                        \
    \                          Column 1 is the source region\n                   \
    \                                               Column 2 is the destination region\n\
    \                                                                  Column 3 is\
    \ number of fibers (NOF): the total count of successful streamlines between the\
    \ two regions\n                                                              \
    \    Column 4 is normalized NOF: the average density of successful streamlines\
    \ the target region.\n         +- connectome_#samples_twoway.txt             \
    \     # Twoway connectome in list form\n         +- connectome_#samples_oneway_nof.mat\
    \              # Oneway NOF connectome in matrix form\n         +- connectome_#samples_twoway_nof.mat\
    \              # Twoway NOF connectome in matrix form (should be symmetric)\n\
    \         +- connectome_#samples_oneway_nof_normalized.mat   # Oneway normalized\
    \ NOF connectome in matrix form\n         +- connectome_#samples_twoway_nof_normalized.mat\
    \   # Twoway normalized NOF connectome in matrix form (should be symmetric)\n\
    \         |\n         +- EDI/\n         |  +- EDImaps/\n         |     +- FAtractsumsRaw.nii.gz\
    \                     # NiFTI image of total streamline density\n         |  \
    \   +- FAtractsumsTwoway.nii.gz                  # NiFTI image of edge density\
    \ (EDI). See Payabvash et al. (2019) for details.\n         |\n         +- log/\
    \                                            # Directory containing stdout and\
    \ performance logs\n         |\n         +- render/                          \
    \               # Directory containing NiFTI image renders from step s4_render\n\
    </code></pre>\n<p><br>\n<br></p>\n<h3>\n<a id=\"user-content-config-parameterscommand-line-arguments\"\
    \ class=\"anchor\" href=\"#config-parameterscommand-line-arguments\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Config\
    \ Parameters/Command Line Arguments</h3>\n<table>\n<thead>\n<tr>\n<th>Required\
    \ Parameter</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>subjects_json</td>\n\
    <td>JSON file with input directories for each subject</td>\n</tr>\n<tr>\n<td>output_dir</td>\n\
    <td>The super-directory that will contain output directories for each subject.</td>\n\
    </tr>\n<tr>\n<td>scheduler_name</td>\n<td>Scheduler to be used for running jobs.\
    \ Value is \"slurm\" for LLNL, \"cobalt\" for ANL, and \"grid_engine\" for UCSF.</td>\n\
    </tr>\n</tbody>\n</table>\n<p><br></p>\n<table>\n<thead>\n<tr>\n<th>Optional Parameter</th>\n\
    <th>Default</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>steps</td>\n\
    <td>s1 s2a s2b s3 s4</td>\n<td>Steps to run</td>\n</tr>\n<tr>\n<td>gpu_steps</td>\n\
    <td>s2a</td>\n<td>Steps to enable CUDA-enabled binaries</td>\n</tr>\n<tr>\n<td>scheduler_bank</td>\n\
    <td></td>\n<td>Scheduler bank to charge for jobs. Required for slurm and cobalt.</td>\n\
    </tr>\n<tr>\n<td>scheduler_partition</td>\n<td></td>\n<td>Scheduler partition\
    \ to assign jobs. Required for slurm and cobalt.</td>\n</tr>\n<tr>\n<td>scheduler_options</td>\n\
    <td></td>\n<td>String to prepend to the submit script to the scheduler</td>\n\
    </tr>\n<tr>\n<td>gpu_options</td>\n<td></td>\n<td>String to prepend to the submit\
    \ blocks for GPU-enabled steps, such as 'module load cuda/8.0;'</td>\n</tr>\n\
    <tr>\n<td>worker_init</td>\n<td></td>\n<td>String to run before starting a worker,\
    \ such as \u2018module load Anaconda; source activate env;\u2019</td>\n</tr>\n\
    <tr>\n<td>container_path</td>\n<td>container/image.simg</td>\n<td>Path to Singularity\
    \ container image</td>\n</tr>\n<tr>\n<td>unix_username</td>\n<td>[[current user]]</td>\n\
    <td>Unix username for Parsl job requests</td>\n</tr>\n<tr>\n<td>unix_group</td>\n\
    <td></td>\n<td>Unix group to assign file permissions</td>\n</tr>\n<tr>\n<td>force</td>\n\
    <td>False</td>\n<td>Force re-compute if checkpoints already exist</td>\n</tr>\n\
    <tr>\n<td>gssapi</td>\n<td>False</td>\n<td>Use Kerberos GSS-API authentication</td>\n\
    </tr>\n<tr>\n<td>local_host_only</td>\n<td>True</td>\n<td>Request all jobs on\
    \ local machine, ignoring other hostnames</td>\n</tr>\n<tr>\n<td>parsl_path</td>\n\
    <td></td>\n<td>Path to Parsl binaries, if not installed in /usr/bin or /usr/sbin</td>\n\
    </tr>\n<tr>\n<td>render_list</td>\n<td>lists/render_targets.txt</td>\n<td>Text\
    \ file list of NIfTI outputs for s4_render (relative to each subject output directory)</td>\n\
    </tr>\n<tr>\n<td>pbtx_sample_count</td>\n<td>1000</td>\n<td>Number of streamlines\
    \ per seed voxel in s3_probtrackx</td>\n</tr>\n<tr>\n<td>pbtx_random_seed</td>\n\
    <td>[[random number]]</td>\n<td>Random seed in s3_probtrackx</td>\n</tr>\n<tr>\n\
    <td>pbtx_max_memory</td>\n<td>0</td>\n<td>Maximum memory per node (in GB) for\
    \ s3_probtrackx. Default value of 0 indicates unlimited memory bound</td>\n</tr>\n\
    <tr>\n<td>connectome_idx_list</td>\n<td>lists/connectome_idxs.txt</td>\n<td>Text\
    \ file with pairs of volumes and connectome indices</td>\n</tr>\n<tr>\n<td>histogram_bin_count</td>\n\
    <td>256</td>\n<td>Number of bins in NiFTI image histograms</td>\n</tr>\n<tr>\n\
    <td>pbtx_edge_list</td>\n<td>lists/list_edges_reduced.txt</td>\n<td>Text file\
    \ list of edges for steps s3_probtrackx</td>\n</tr>\n<tr>\n<td>compress_pbtx_results</td>\n\
    <td>True</td>\n<td>Compress probtrackx outputs to reduce inode and disk space\
    \ usage</td>\n</tr>\n<tr>\n<td>dynamic_walltime</td>\n<td>False</td>\n<td>Request\
    \ dynamically shortened walltimes, to gain priority on job queue</td>\n</tr>\n\
    <tr>\n<td>s1_job_time</td>\n<td>00:15:00</td>\n<td>Max time to finish s1 on 1\
    \ subject with 1 node, if dynamic_walltime is true</td>\n</tr>\n<tr>\n<td>s2a_job_time</td>\n\
    <td>00:45:00</td>\n<td>Max time to finish s2a on 1 subject with 1 node, if dynamic_walltime\
    \ is true</td>\n</tr>\n<tr>\n<td>s2b_job_time</td>\n<td>10:00:00</td>\n<td>Max\
    \ time to finish s2b on 1 subject with 1 node, if dynamic_walltime is true</td>\n\
    </tr>\n<tr>\n<td>s3_job_time</td>\n<td>23:59:00</td>\n<td>Max time to finish s3\
    \ on 1 subject with 1 node, if dynamic_walltime is true</td>\n</tr>\n<tr>\n<td>s4_job_time</td>\n\
    <td>00:15:00</td>\n<td>Max time to finish s4 on 1 subject with 1 node, if dynamic_walltime\
    \ is true</td>\n</tr>\n<tr>\n<td>s1_cores_per_task</td>\n<td>1</td>\n<td>Number\
    \ of cores to assign each task for step s1_dti_preproc</td>\n</tr>\n<tr>\n<td>s2a_cores_per_task</td>\n\
    <td>[[core count on head node]]</td>\n<td>Number of cores to assign each task\
    \ for step s2a_bedpostx</td>\n</tr>\n<tr>\n<td>s2b_cores_per_task</td>\n<td>[[core\
    \ count on head node]]</td>\n<td>Number of cores to assign each task for step\
    \ s2b_freesurfer</td>\n</tr>\n<tr>\n<td>s3_cores_per_task</td>\n<td>1</td>\n<td>Number\
    \ of cores to assign each task for step s3_probtrackx</td>\n</tr>\n<tr>\n<td>s4_cores_per_task</td>\n\
    <td>1</td>\n<td>Number of cores to assign each task for step s4_render</td>\n\
    </tr>\n<tr>\n<td>s1_hostname</td>\n<td></td>\n<td>Hostname of machine to run step\
    \ s1_dti_preproc, if local_host_only is false</td>\n</tr>\n<tr>\n<td>s2a_hostname</td>\n\
    <td></td>\n<td>Hostname of machine to run step s2a_bedpostx, if local_host_only\
    \ is false</td>\n</tr>\n<tr>\n<td>s2b_hostname</td>\n<td></td>\n<td>Hostname of\
    \ machine to run step s2b_freesurfer, if local_host_only is false</td>\n</tr>\n\
    <tr>\n<td>s3_hostname</td>\n<td></td>\n<td>Hostname of machine to run step s3_probtrackx,\
    \ if local_host_only is false</td>\n</tr>\n<tr>\n<td>s4_hostname</td>\n<td></td>\n\
    <td>Hostname of machine to run step s4_render, if local_host_only is false</td>\n\
    </tr>\n<tr>\n<td>s1_walltime</td>\n<td>23:59:00</td>\n<td>Walltime for step s1</td>\n\
    </tr>\n<tr>\n<td>s2a_walltime</td>\n<td>23:59:00</td>\n<td>Walltime for step s2a</td>\n\
    </tr>\n<tr>\n<td>s2b_walltime</td>\n<td>23:59:00</td>\n<td>Walltime for step s2b</td>\n\
    </tr>\n<tr>\n<td>s3_walltime</td>\n<td>23:59:00</td>\n<td>Walltime for step s3</td>\n\
    </tr>\n<tr>\n<td>s4_walltime</td>\n<td>23:59:00</td>\n<td>Walltime for step s4</td>\n\
    </tr>\n<tr>\n<td>s1_nodes</td>\n<td>[[floor(0.2 * num_subjects)]]</td>\n<td>Node\
    \ count for step s1</td>\n</tr>\n<tr>\n<td>s2a_nodes</td>\n<td>[[floor(1.0 * num_subjects)]]</td>\n\
    <td>Node count for step s2a</td>\n</tr>\n<tr>\n<td>s2b_nodes</td>\n<td>[[floor(1.0\
    \ * num_subjects)]]</td>\n<td>Node count for step s2b</td>\n</tr>\n<tr>\n<td>s3_nodes</td>\n\
    <td>[[floor(1.0 * num_subjects)]]</td>\n<td>Node count for step s3</td>\n</tr>\n\
    <tr>\n<td>s4_nodes</td>\n<td>[[floor(0.1 * num_subjects)]]</td>\n<td>Node count\
    \ for step s4</td>\n</tr>\n<tr>\n<td>s1_cores</td>\n<td>[[core count on head node]]</td>\n\
    <td>Cores per node for step s1</td>\n</tr>\n<tr>\n<td>s2a_cores</td>\n<td>[[core\
    \ count on head node]]</td>\n<td>Cores per node for step s2a</td>\n</tr>\n<tr>\n\
    <td>s2b_cores</td>\n<td>[[core count on head node]]</td>\n<td>Cores per node for\
    \ step s2b</td>\n</tr>\n<tr>\n<td>s3_cores</td>\n<td>[[core count on head node]]</td>\n\
    <td>Cores per node for step s3</td>\n</tr>\n<tr>\n<td>s4_cores</td>\n<td>[[core\
    \ count on head node]]</td>\n<td>Cores per node for step s4</td>\n</tr>\n<tr>\n\
    <td>bids_json</td>\n<td>examples/dummy_bids_desc.json</td>\n<td>Description file\
    \ dataset_description.json, as specified at <a href=\"https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html\"\
    \ rel=\"nofollow\">https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html</a>\n\
    </td>\n</tr>\n<tr>\n<td>bids_readme</td>\n<td></td>\n<td>Free form text file describing\
    \ the dataset in more detail</td>\n</tr>\n<tr>\n<td>bids_session_name</td>\n<td></td>\n\
    <td>Name for the session timepoint (e.g. 2weeks)</td>\n</tr>\n</tbody>\n</table>\n\
    <h3>\n<a id=\"user-content-download-mri-images-from-openneuro\" class=\"anchor\"\
    \ href=\"#download-mri-images-from-openneuro\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Download MRI Images from OpenNeuro</h3>\n\
    <p>Download MRI images from OpenNeuro repository by providing path to install\
    \ data and accession ID of the MRI image.</p>\n<pre><code>usage: subscripts/download_openneuro.py\
    \ [-h] [--install-directory INSTALL_DIR] [-a ACC_NUM]\n\narguments:\n  -h, --help\
    \            show this help message and exit\n  --install-directory INSTALL_DIR\n\
    \                        Path where data will be installed\n  -a ACC_NUM, --accession\
    \ ACC_NUM\n                        MRI Accession ID from OpenNeuro\n\n</code></pre>\n\
    <p>Requirements:\npython package datalad, git-annex\nInstallation:</p>\n<pre><code>conda\
    \ install -c conda-forge datalad\n</code></pre>\n<p>on mac:</p>\n<pre><code>brew\
    \ install git-annex\n</code></pre>\n<p>on linux:</p>\n<pre><code>conda install\
    \ -c conda-forge git-annex\n</code></pre>\n<h3>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h3>\n<p>MaPPeRTrac is\
    \ distributed under the terms of the BSD-3 License.</p>\n<p>LLNL-CODE-811655</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1624378985.0
Lizhen0909/LSHVec:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity
  full_name: Lizhen0909/LSHVec
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-lshvec-a-vector-representation-of-dna-sequences-using-locality-sensitive-hashing\"\
    \ class=\"anchor\" href=\"#lshvec-a-vector-representation-of-dna-sequences-using-locality-sensitive-hashing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>LSHVec: A Vector Representation of DNA Sequences Using Locality Sensitive\
    \ Hashing</h1>\n<h2>\n<a id=\"user-content-summary\" class=\"anchor\" href=\"\
    #summary\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Summary</h2>\n<p>LSHVec is a k-mer/sequence embedding/classfication\
    \ software which extends <a href=\"https://fasttext.cc/\" rel=\"nofollow\">FastText</a>\
    \ . It applies LSH (Locality Sensitive Hashing) to reduce the size of k-mer vocabulary\
    \ and improve the performance of embedding.</p>\n<p>Besides building from source\
    \ code, LSHVec can run using docker or singularity.</p>\n<p>Please refer to <a\
    \ href=\"https://www.biorxiv.org/content/10.1101/726729v1\" rel=\"nofollow\">A\
    \ Vector Representation of DNA Sequences Using Locality Sensitive Hashing</a>\
    \ for the idea and experiments.</p>\n<p>There are also some pretained models that\
    \ can be used, please see <a href=\"https://github.com/Lizhen0909/PyLSHvec/blob/master/README.md\"\
    >PyLSHvec</a> for details.</p>\n<h2>\n<a id=\"user-content-requirements\" class=\"\
    anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<p>Here is the\
    \ environment I worked on.  Other versions may also work. Python 3 should work,\
    \ but I don't use it a lot.</p>\n<ol>\n<li>Linux, gcc with C++11</li>\n<li>Python\
    \ 2.7 or Python 3.6 or 3.7\n<ul>\n<li>joblib 0.12.4</li>\n<li>tqdm 4.28.1</li>\n\
    <li>numpy 1.15.0</li>\n<li>pandas 0.23.4</li>\n<li>sklearn 0.19.1 (only for evaluation)</li>\n\
    <li>MulticoreTSNE (only for visualization)</li>\n<li>cython 0.28.5</li>\n<li>csparc\
    \ (included)</li>\n</ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-build-from-source\"\
    \ class=\"anchor\" href=\"#build-from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build from Source</h2>\n<ul>\n\
    <li>\n<p>clone from git</p>\n<p><code>git clone https://LizhenShi@bitbucket.org/LizhenShi/lshvec.git</code></p>\n\
    <p><code>cd lshvec</code></p>\n</li>\n<li>\n<p>install csparc which wraps a c\
    \ version of k-mer generator I used in another project</p>\n<p>for python 2.7</p>\n\
    <p><code>pip install pysparc-0.1-cp27-cp27mu-linux_x86_64.whl</code></p>\n<p>or\
    \ for python 3.6</p>\n<p><code>pip install pysparc-0.1-cp36-cp36m-linux_x86_64.whl</code></p>\n\
    <p>or for python 3.7</p>\n<p><code>pip install pysparc-0.1-cp37-cp37m-linux_x86_64.whl</code></p>\n\
    </li>\n<li>\n<p>make</p>\n<p><code>make</code></p>\n</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-jupyter-notebook-examples\" class=\"anchor\" href=\"#jupyter-notebook-examples\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Jupyter Notebook Examples</h2>\n<p>A toy example, which is laptop\
    \ friendly and should finish in 10 minutes,  can be found in <a href=\"notebook/Tutorial_Toy_Example.ipynb\"\
    >Tutorial_Toy_Example.ipynb</a>. Because of randomness the result may be different.</p>\n\
    <p><a href=\"notebook/Tutorial_Toy_Example.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"notebook/Tutorial_Toy_Example.png\" alt=\"Tutorial_Toy_Example\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>A practical example which uses ActinoMock\
    \ Nanopore data can be found at <a href=\"notebook/Tutorial_ActinoMock_Nanopore.ipynb\"\
    >Tutorial_ActinoMock_Nanopore.ipynb</a>. The notebook ran on a 16-core 64G-mem\
    \ node and took a few hours (I think 32G mem should work too).</p>\n<p>\u200B\t\
    \t\t\t\t\t <a href=\"notebook/Tutorial_ActinoMock_Nanopore.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"notebook/Tutorial_ActinoMock_Nanopore.png\"\
    \ alt=\"Tutorial_ActinoMock_Nanopore\" style=\"max-width:100%;\"></a></p>\n<h2>\n\
    <a id=\"user-content-command-line-options\" class=\"anchor\" href=\"#command-line-options\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Command line options</h2>\n<h3>\n<a id=\"user-content-fastqtoseqpy\"\
    \ class=\"anchor\" href=\"#fastqtoseqpy\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>fastqToSeq.py</h3>\n<p>convert\
    \ a fastq file to a seq file</p>\n<pre><code>python fastqToSeq.py -i &lt;fastq_file&gt;\
    \ -o &lt;out seq file&gt; -s &lt;1 to shuffle, 0 otherwise&gt;\n</code></pre>\n\
    <h3>\n<a id=\"user-content-hashseqpy\" class=\"anchor\" href=\"#hashseqpy\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>hashSeq.py</h3>\n\
    <p>Encode reads in a seq file use an encoding method.</p>\n<pre><code>python hashSeq.py\
    \ -i &lt;seq_file&gt; --hash &lt;fnv or lsh&gt; -o &lt;outfile&gt; [-k &lt;kmer_size&gt;]\
    \ [--n_thread &lt;n&gt;] [--hash_size &lt;m&gt;] [--batch_size &lt;n&gt;] [--bucket\
    \ &lt;n&gt;] [--lsh_file &lt;file&gt;] [--create_lsh_only]\n\n  --hash_size &lt;m&gt;:\
    \        only used by lsh which defines 2^m bucket.\n  --bucket &lt;n&gt;:   \
    \        number of bucket for hash trick, useless for onehot.\n   \t\t\t\t   \
    \       For fnv and lsh it limits the max number of words.\n   \t\t\t\t      \
    \    For lsh the max number of words is min(2^m, n).\n  --batch_size &lt;b&gt;:\
    \       how many reads are processed at a time. A small value uses less memory.\n\
    </code></pre>\n<h3>\n<a id=\"user-content-lshvec\" class=\"anchor\" href=\"#lshvec\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>lshvec</h3>\n<p>Please refer to <a href=\"https://fasttext.cc/docs/en/options.html\"\
    \ rel=\"nofollow\">fasttext options</a>.  However note that options of <code>wordNgrams</code>,\
    \ <code>minn</code>,<code>maxn</code> does not work with lshvec.</p>\n<h2>\n<a\
    \ id=\"user-content-example-of-docker-run\" class=\"anchor\" href=\"#example-of-docker-run\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Example of Docker Run</h2>\n<p>Pull from docker hub:</p>\n<pre><code>docker\
    \ pull lizhen0909/lshvec:latest\n</code></pre>\n<p>Assume <code>data.fastq</code>\
    \ file is in folder <code>/path/in/host</code>.</p>\n<p>convert fastq to a seq\
    \ file:</p>\n<pre><code>docker run -v /path/in/host:/host lshvec:latest bash -c\
    \ \"cd /host &amp;&amp; fastqToSeq.py  -i data.fastq -o data.seq\"\n</code></pre>\n\
    <p>create LSH:</p>\n<pre><code>docker run -v /path/in/host:/host lshvec:latest\
    \ bash -c \"cd /host &amp;&amp; hashSeq.py -i data.seq --hash lsh -o data.hash\
    \ -k 15\"\n</code></pre>\n<p>run lshvec:</p>\n<pre><code>docker run -v /path/in/host:/host\
    \ lshvec:latest bash -c \"cd /host &amp;&amp; lshvec skipgram -input data.hash\
    \ -output model\"\n</code></pre>\n<h2>\n<a id=\"user-content-example-of-singularity-run\"\
    \ class=\"anchor\" href=\"#example-of-singularity-run\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Example of Singularity\
    \ Run</h2>\n<p>When running using Singularity, it is probably in an HPC environment.\
    \ The running is similar to docker. However depending on the version of singularity,\
    \ commands and paths might be different, especially from 2.x to 3.x. Here is an\
    \ example for version 2.5.0.</p>\n<p>Also it is better to specify number of threads,\
    \ otherwise max number of cores will be used which is not desired in HPC environment.</p>\n\
    <p>Pull from docker hub:</p>\n<pre><code>singularity pull --name lshvec.sif shub://Lizhen0909/LSHVec\n\
    </code></pre>\n<p>Put <code>data.fastq</code> file is in host <code>/tmp</code>,\
    \  since Singularity automatically mount <code>/tmp</code> folder.</p>\n<p>convert\
    \ fastq to a seq file:</p>\n<pre><code>singularity run /path/to/lshvec.sif bash\
    \ -c \"cd /tmp &amp;&amp; fastqToSeq.py  -i data.fastq -o data.seq\"\n</code></pre>\n\
    <p>create LSH:</p>\n<pre><code>singularity run /path/to/lshvec.sif bash -c \"\
    cd /tmp &amp;&amp; hashSeq.py -i data.seq --hash lsh -o data.hash -k 15 --n_thread\
    \ 12\"\n</code></pre>\n<p>run lshvec:</p>\n<pre><code>singularity run /path/to/lshvec.sif\
    \ bash -c \"cd /tmp &amp;&amp; lshvec skipgram -input data.hash -output model\
    \ -thread 12\"\n</code></pre>\n<h2>\n<a id=\"user-content-questions\" class=\"\
    anchor\" href=\"#questions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Questions</h2>\n<ul>\n<li>\n<code>lshvec</code>\
    \ gets stuck at <code>Read xxxM words</code>\n</li>\n</ul>\n<p>Search <code>MAX_VOCAB_SIZE</code>\
    \ in the source code and change it to a bigger one.  When a word's index is bigger\
    \ than that number, a loop is carried to query it, which is costly. The number\
    \ is 30M in FastText which is good for languages. But it is too small for k-mers.\
    \ The number has been already increased to 300M in FastSeq. But for large and/or\
    \ high-error-rate data, it may be still not enough.</p>\n<ul>\n<li>\n<p>I have\
    \ big data</p>\n<p>hashSeq reads all data into memory to sample k-mers for hyperplanes.\
    \ If data is too big it may not fit into memory. One can</p>\n<ol>\n<li>Try sampling.\
    \ DNA reads generally have high coverage. Such high coverage may not be necessary.</li>\n\
    <li>Or use <code>create_hash_only</code> to create lsh on a small (sampled) data;\
    \ then split your data into multiple files and run hashSeq with <code>lsh_file</code>\
    \ option on many nodes.</li>\n</ol>\n</li>\n<li>\n<p>core dumped when hashing</p>\n\
    <p>Error like</p>\n<pre><code>terminate called after throwing an instance of 'std::out_of_range'\n\
    what(): map::at\nAborted (core dumped)\n</code></pre>\n<p>mostly because a sequence\
    \ contains characters other than ACGTN. So please convert non-ACGT characters\
    \ to N's.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\"\
    \ href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>License</h2>\n<p>Inherit license from FastText which\
    \ is BSD License</p>\n"
  stargazers_count: 3
  subscribers_count: 1
  topics:
  - locality-sensitive-hashing
  - sequence-vector
  - classfication
  updated_at: 1624367592.0
Lizhen0909/PyLSHvec:
  data_format: 2
  description: LSHVec pre-trained models and its Python bindings
  filenames:
  - singularity/Singularity
  full_name: Lizhen0909/PyLSHvec
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-lshvec-pre-trained-models-and-its-python-bindings\"\
    \ class=\"anchor\" href=\"#lshvec-pre-trained-models-and-its-python-bindings\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>LSHVec pre-trained models and its Python bindings</h1>\n<h2>\n<a id=\"\
    user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Summary</h2>\n\
    <p>This repository presents a few of pre-tained models with JLSHVec (which is\
    \ a rewritten java version of  <a href=\"https://github.com/Lizhen0909/LSHVec\"\
    >LSHVec</a>).  See <a href=\"#remark\">Remark</a> for technical details.</p>\n\
    <p>Python codes and examples to uses these models are also provided.</p>\n<h2>\n\
    <a id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\
    <ol>\n<li>Python 3.6</li>\n<li>cython&gt;=0.28.5</li>\n<li>Jnius &gt;=1.1.0</li>\n\
    <li>java &gt;=1.8</li>\n</ol>\n<h2>\n<a id=\"user-content-install\" class=\"anchor\"\
    \ href=\"#install\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Install</h2>\n<h3>\n<a id=\"user-content-build-from-source\"\
    \ class=\"anchor\" href=\"#build-from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>build from source</h3>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/Lizhen0909/PyLSHvec.git\
    \ <span class=\"pl-k\">&amp;&amp;</span> <span class=\"pl-c1\">cd</span> PyLSHvec\
    \ <span class=\"pl-k\">&amp;&amp;</span> python setup.py install</pre></div>\n\
    <h3>\n<a id=\"user-content-or-use-pip\" class=\"anchor\" href=\"#or-use-pip\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>or use pip</h3>\n<div class=\"highlight highlight-source-shell\"><pre>pip\
    \ install pylshvec</pre></div>\n<h3>\n<a id=\"user-content-or-use-docker\" class=\"\
    anchor\" href=\"#or-use-docker\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>or use docker</h3>\n<div class=\"\
    highlight highlight-source-shell\"><pre>docker pull lizhen0909/pylshvec</pre></div>\n\
    <h3>\n<a id=\"user-content-or-use-singularity-3\" class=\"anchor\" href=\"#or-use-singularity-3\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>or use singularity 3</h3>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull --name pylshvec.sif shub://Lizhen0909/PyLSHvec</pre></div>\n\
    <h2>\n<a id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to use</h2>\n<p>Put things simply, just</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">pylshvec</span> <span class=\"pl-k\">import</span> <span class=\"pl-c1\"\
    >*</span>\n\n<span class=\"pl-c\">#here needs jlshvec jar file, download it first</span>\n\
    <span class=\"pl-en\">set_lshvec_jar_path</span>(<span class=\"pl-s\">\"/mnt/jlshvec-assembly-0.1.jar\"\
    </span>)\n\n<span class=\"pl-c\">#since vector model is usually large, set a big\
    \ java memory limit is preferred. </span>\n<span class=\"pl-en\">add_java_options</span>(<span\
    \ class=\"pl-s\">\"-Xmx32G\"</span>)\n\n<span class=\"pl-c\">#here need model\
    \ file and lsh function file, download them first</span>\n<span class=\"pl-c\"\
    >#use help(model) to see all the methods and constructor options </span>\n<span\
    \ class=\"pl-s1\">model</span><span class=\"pl-c1\">=</span> <span class=\"pl-v\"\
    >LSHVec</span>(<span class=\"pl-s1\">model_file</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s\">\"/mnt/refdb_viruses_model_gs_k23_l3000_rand_model_299\"</span>,\
    \ \n              <span class=\"pl-s1\">hash_file</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s\">\"/mnt/lsh_nt_NonEukaryota_k23_h25.crp\"</span>)\n\
    \n<span class=\"pl-s1\">reads</span> <span class=\"pl-c1\">=</span> [<span class=\"\
    pl-s\">'ACGTACGT.....'</span>, <span class=\"pl-s\">'ACGTACGT.....'</span>, <span\
    \ class=\"pl-s\">'ACGTACGT.....'</span>, <span class=\"pl-s\">'ACGTACGT.....'</span>,\
    \ ....]\n\n<span class=\"pl-s1\">predicts</span> <span class=\"pl-c1\">=</span>\
    \ <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">predict</span>(<span\
    \ class=\"pl-s1\">reads</span>)</pre></div>\n<p>For more complete examples please\
    \ see the notebooks (see <a href=\"#download\">Download</a> for minimum memory\
    \ requirement):</p>\n<p><a href=\"notebook/example_use_virus_classfication_model.ipynb\"\
    >example_use_virus_classfication_model.ipynb</a></p>\n<p><a href=\"notebook/example_use_bacteria_classfication_model.ipynb\"\
    >example_use_bacteria_classfication_model.ipynb</a></p>\n<p><a href=\"notebook/example_use_vectors_in_bacteria_classfication_model.ipynb\"\
    >example_use_vectors_in_bacteria_classfication_model.ipynb</a></p>\n<p><a href=\"\
    notebook/example_use_Illumina_bacteria_classfication_model.ipynb\">example_use_Illumina_bacteria_classfication_model.ipynb</a></p>\n\
    <p><a href=\"notebook/example_use_Pacbio_bacteria_classfication_model.ipynb\"\
    >example_use_Pacbio_bacteria_classfication_model.ipynb</a></p>\n<h3>\n<a id=\"\
    user-content-use-docker\" class=\"anchor\" href=\"#use-docker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use\
    \ Docker</h3>\n<p>Assume you put your data in /mnt/data and your notebook in /mnt/notebook.</p>\n\
    <ul>\n<li>run python or ipython</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker run -v /mnt/data:/data -it lizhen0909/pylshvec python <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span>or ipython</span></pre></div>\n<ul>\n<li>run\
    \ Jupyter notebook</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker run -v /mnt/data:/data -v /mnt/notebook:/notebook -p 8888:8888  -it\
    \ lizhen0909/pylshvec jupyter_notebook</pre></div>\n<p>Find connection url in\
    \ the console output.</p>\n<h3>\n<a id=\"user-content-use-singularity\" class=\"\
    anchor\" href=\"#use-singularity\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Use Singularity</h3>\n<p>Since singularity\
    \ maps the $HOME directory, here just assumes data/model are going to locate in\
    \ $HOME. Otherwise, you need map the directories like docker.</p>\n<ul>\n<li>run\
    \ python or ipython</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity run pylshvec.sif python <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span>the nrun any pylshvec code </span></pre></div>\n<ul>\n<li>run Jupyter\
    \ notebook</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span>It should work, however singularity\
    \ maps too many things that host settings may affect the notebook</span>\nsingularity\
    \ run  --bind <span class=\"pl-smi\">$HOME</span>/notebook:/notebook pylshvec.sif\
    \ jupyter_notebook </pre></div>\n<h2>\n<a id=\"user-content-download\" class=\"\
    anchor\" href=\"#download\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Download</h2>\n<h3>\n<a id=\"user-content-jlshvec-jar-file\"\
    \ class=\"anchor\" href=\"#jlshvec-jar-file\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>JLSHVec jar file</h3>\n<p>The\
    \ pre-trained models were trained with a rewritten  <a href=\"https://github.com/Lizhen0909/LSHVec\"\
    >LSHVec</a> in java.\nThe assembly jar file is needed to load the models.</p>\n\
    <p><a href=\"https://www.amazon.com/clouddrive/share/4NiogpuW1lzBMyGmMlkrDbjhSMYpQgWjW5GUcKFR7Q6\"\
    \ rel=\"nofollow\">Download jlshvec-assembly-0.1.jar</a></p>\n<p><strong>md5sum</strong>:\
    \ aeb207b983b3adc27e14fd9c431e2130</p>\n<h3>\n<a id=\"user-content-pre-trained-models\"\
    \ class=\"anchor\" href=\"#pre-trained-models\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pre-trained models</h3>\n<p><strong>Be\
    \ Warned</strong> that like all the machine learning models, the model cannot\
    \ preform better beyond the data. If your data is significant other than the pre-trained\
    \ model data, training your own model is preferred.</p>\n<p>Here are issues I\
    \ can think of:</p>\n<ul>\n<li>Some NCBI taxonomy id may never be predicted since\
    \ not all ids have train data.</li>\n<li>Data is not balanced. Some ids (e.g.\
    \ a specified species) have much more data than others, which makes prediction\
    \ may prefer to the rich-data ids.</li>\n<li>Strain (even some species) prediction\
    \ is terrible. Don't expect it.</li>\n</ul>\n<h4>\n<a id=\"user-content-refdb-viruses-classfication-model\"\
    \ class=\"anchor\" href=\"#refdb-viruses-classfication-model\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RefDB\
    \ viruses classfication model</h4>\n<p>Trainned with 9.3k viruses assemblies of\
    \ RefDB. Minimum Java memory: 16G.</p>\n<ul>\n<li>\n<p>model file: <a href=\"\
    https://www.amazon.com/clouddrive/share/RmoJ1lduzlqstAJFnKg0aAlx82AyCjnzKncfGjQIQMg\"\
    \ rel=\"nofollow\">refdb_viruses_model_gs_k23_l3000_rand_model_299</a> [size:\
    \ 5.3G]</p>\n<p><strong>md5sum</strong> 2502b284b336734300c2297d23d1d349</p>\n\
    </li>\n<li>\n<p>hash function file: <a href=\"https://www.amazon.com/clouddrive/share/6ZNvMXMy30b4vc0RYNVG1lbf1ih8WgpoQ9w4lX91IXy\"\
    \ rel=\"nofollow\">lsh_nt_NonEukaryota_k23_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ 5eea8a98d224b7ff505091bd483ca75c</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-refdb-bacteria-classfication-model\"\
    \ class=\"anchor\" href=\"#refdb-bacteria-classfication-model\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RefDB\
    \ bacteria classfication model</h4>\n<p>Trainned with 42k bacteria assemblies\
    \ of RefDB. Minimum Java memory: 32G.</p>\n<ul>\n<li>\n<p>model file: <a href=\"\
    https://www.amazon.com/clouddrive/share/LoXz6k229SwYuElPTHvu0SSJOq56nJenvBbOTGVeb9a\"\
    \ rel=\"nofollow\">refdb_bacteria_model_gs_k23_l3000_rand_model_214</a> [size:\
    \ 11G]</p>\n<p><strong>md5sum</strong> 402e9a286b71068999caa9766b2dbf8c</p>\n\
    </li>\n<li>\n<p>hash function file: <a href=\"https://www.amazon.com/clouddrive/share/6ZNvMXMy30b4vc0RYNVG1lbf1ih8WgpoQ9w4lX91IXy\"\
    \ rel=\"nofollow\">lsh_nt_NonEukaryota_k23_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ 5eea8a98d224b7ff505091bd483ca75c</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-genbank-bacteria-and-viruses-classfication-model-illumina-simulation\"\
    \ class=\"anchor\" href=\"#genbank-bacteria-and-viruses-classfication-model-illumina-simulation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>GenBank bacteria and viruses classfication model (Illumina Simulation)</h4>\n\
    <p>Trainned with 54k assemblies from GenBank. <strong>Only one assembly was sampled\
    \ for each species.</strong> Because viruses data is too samll compared to bateria,\
    \ it rarely predicts any viruses. Just take it as a bateria model.</p>\n<p><a\
    \ href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3278762/\" rel=\"nofollow\"\
    >art_illumina</a> was used to simulate the paired-end reads with length of 150,\
    \ mean size of 270 and stddev of 27.</p>\n<p>Minimum Java memory: 48G.</p>\n<ul>\n\
    <li>\n<p>model file: <a href=\"https://www.amazon.com/clouddrive/share/zQnu2ti1vfBMGcXrRqsohgfzuaYzZs4HGESP58vobRn\"\
    \ rel=\"nofollow\">genbank_model_ill_k23_model_299</a> [size: 12G]</p>\n<p><strong>md5sum</strong>\
    \ d6b117a4c7ffe4f25e6c532a88bb3a47</p>\n</li>\n<li>\n<p>hash function file: <a\
    \ href=\"https://www.amazon.com/clouddrive/share/efWceiTHId4EVhY1DEppmW6amyBQoEt3iIU6oW5FbcX\"\
    \ rel=\"nofollow\">lsh_CAMI2_illumina_k23_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ 706633919e347f920ce6ab3277091efb</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-genbank-bacteria-and-viruses-classfication-model-pacbio-simulation\"\
    \ class=\"anchor\" href=\"#genbank-bacteria-and-viruses-classfication-model-pacbio-simulation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>GenBank bacteria and viruses classfication model (Pacbio Simulation)</h4>\n\
    <p>Trainned with 54k assemblies from GenBank. <strong>Only one assembly was sampled\
    \ for each species.</strong> Because viruses data is too samll compared to bateria,\
    \ it rarely predicts any viruses. Just take it as a bateria model.</p>\n<p><a\
    \ href=\"https://github.com/pfaucon/PBSIM-PacBio-Simulator\">pbsim</a> was used\
    \ to simulate the pacbio reads with Continuous Long Read (CLR) profile, mean size\
    \ of 3000 and stddev of 1000.</p>\n<p>Minimum Java memory: 16G.</p>\n<ul>\n<li>\n\
    <p>model file: <a href=\"https://www.amazon.com/clouddrive/share/OmU9cmVKknacpt0W9HpI6QY2jXC17dQpWaaERpLhOGl\"\
    \ rel=\"nofollow\">genbank_model_pb_k9_model_299</a> [size: 121M]</p>\n<p><strong>md5sum</strong>\
    \ 351275531493a4866be4afcd9df3932c</p>\n</li>\n<li>\n<p>hash function file: <a\
    \ href=\"https://www.amazon.com/clouddrive/share/zw4JwJCE4Lst5I4q36ijwrhc3db9rHYsCuyQ4KkihVC\"\
    \ rel=\"nofollow\">lsh_CAMI2_pacbio_k9_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ df7ee38cf8b58d5f8034bb9b266e3334</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-sample-data\"\
    \ class=\"anchor\" href=\"#sample-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sample data</h3>\n<ul>\n<li>\n\
    <p>ActinoMock Nanopore Sample [size: 500M].</p>\n<p>The data is used in example\
    \ notebook <a href=\"notebook/example_use_vectors_in_bacteria_classfication_model.ipynb\"\
    >example_use_vectors_in_bacteria_classfication_model.ipynb</a>.</p>\n<p><a href=\"\
    http://ww2.cs.fsu.edu/~lshi/ActinoMock_Nanopore.seq.gz\" rel=\"nofollow\">Download\
    \ from FSU</a>\n\u2003\u2003\n<a href=\"https://www.amazon.com/clouddrive/share/eTIKYVLckXUCMnMQSpO8TCqZOwekmBrx23ZhMa3XO8d\"\
    \ rel=\"nofollow\">Download from Amazon Drive</a></p>\n<p><strong>md5sum</strong>:\
    \ b7f3e55438fdc05920aee693a98ded2e</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-remark\"\
    \ class=\"anchor\" href=\"#remark\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Remark</h2>\n<h3>\n<a id=\"user-content-what-is-jlshvec--why-jlshvec-instead-of-lshvec\"\
    \ class=\"anchor\" href=\"#what-is-jlshvec--why-jlshvec-instead-of-lshvec\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>What\
    \ is JLSHVec ? Why JLSHVec instead of LSHVec?</h3>\n<p>JLSHVec is a rewritten\
    \ version of <a href=\"https://github.com/Lizhen0909/LSHVec\">LSHVec</a> in Java\
    \ language.</p>\n<p>When we use LSHVec with big dataset (e.g. <a href=\"https://www.ncbi.nlm.nih.gov/genbank/\"\
    \ rel=\"nofollow\">GenBank</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/12652131\"\
    \ rel=\"nofollow\">RefDB</a>), we found that LSHVec is hard to process such a\
    \ big data size.</p>\n<p>The reason is that LSHVec which inherits from <a href=\"\
    https://fasttext.cc/\" rel=\"nofollow\">FastText</a> requires the input is text\
    \ format separated by white space and then loads all the text in memory. This\
    \ is acceptable for natural languages since the data size is at most tens GBs.</p>\n\
    <p>However in LSHVec k-mers are used instead of words. Suppose we want to train\
    \ a k-mer embedding of simulated Illumina reads with RefDB bacteria assemblies\
    \ (about 500G genetic bits). The number of kmers is about D*n, where D is the\
    \ assembly data size and n is coverage. In our case, assuming n=10 and k=23, the\
    \ number of kmers is 5T and requires a disk space of 125TB, of which the data\
    \ preparation and loading process will take forever.</p>\n<h3>\n<a id=\"user-content-how-were-jlshvec-pre-trained-models-trained-\"\
    \ class=\"anchor\" href=\"#how-were-jlshvec-pre-trained-models-trained-\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How\
    \ were JLSHVec pre-trained models trained ?</h3>\n<p>First we prepared a <a href=\"\
    https://rocksdb.org/\" rel=\"nofollow\">RockDB</a> for the reference sequences\
    \ (e.g. all bacteria assemblies in RefDB).</p>\n<p>Then we have several nodes\
    \ to train the model: one node (train node) trains the model and others (hash\
    \ nodes) generate and hash kmers. The nodes communicates by passing <a href=\"\
    https://developers.google.com/protocol-buffers\" rel=\"nofollow\">protocol-buf</a>\
    \ message with a <a href=\"https://redis.io/\" rel=\"nofollow\">Redis</a> server.</p>\n\
    <p>A hash node randomly reads reference sequences from the RockDB, simulates (e.g.\
    \ simulations Illumina, Pacbio, Gold Standard) reads, generates kmers and hashes\
    \ them, then feeds the hashed-kmer-sequences to a Redis queue.</p>\n<p>Train node\
    \ reads from the Redis queue and does jobs of embedding or classification training.\
    \  Our training code supports hierarchical softmax using NCBI taxonomy tree, which\
    \ is essential for multi-label(an instance can have a label for each rank) and\
    \ multi-class(an instance can only have one label for a rank)  mixture classification\
    \ model.</p>\n<h2>\n<a id=\"user-content-citation\" class=\"anchor\" href=\"#citation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Citation</h2>\n<p>Please cite:</p>\n<p><a href=\"https://www.biorxiv.org/content/biorxiv/early/2019/08/06/726729.full.pdf\"\
    \ rel=\"nofollow\">A Vector Representation of DNA Sequences Using Locality Sensitive\
    \ Hashing</a></p>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"\
    #license\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h2>\n<p><a href=\"https://www.gnu.org/licenses/gpl-3.0\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/400c4e52df43f6a0ab8a89b74b1a78d1a64da56a7848b9110c9d2991bb7c3105/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1572883846.0
Lizhen0909/graph_clustering_toolkit:
  data_format: 2
  description: graph clustering toolkit
  filenames:
  - singularity/Singularity
  full_name: Lizhen0909/graph_clustering_toolkit
  latest_release: null
  readme: "<h2>\n<a id=\"user-content-graph-clustering-toolkit\" class=\"anchor\"\
    \ href=\"#graph-clustering-toolkit\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Graph Clustering Toolkit</h2>\n\
    <h3>\n<a id=\"user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Summary:</h3>\n\
    <p>The toolkit collects many academic graph clustering programs and make them\
    \ avaliable as package. Docker image is provided for easy access.</p>\n<h3>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation:</h3>\n\
    <p>Use docker is convenient as</p>\n<pre><code>docker pull lizhen0909/graph_clustering_toolkit\n\
    </code></pre>\n<p>For more information, please refer to <a href=\"https://lizhen0909.github.io/graph_clustering_toolkit/\"\
    \ rel=\"nofollow\">online document</a> for a full description</p>\n<h3>\n<a id=\"\
    user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage:</h3>\n\
    <p>Start python from docker:</p>\n<pre><code>docker run -it --rm lizhen0909/graph_clustering_toolkit\
    \ python\n</code></pre>\n<p>Run the script from the command line:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span\
    \ class=\"pl-s1\">gct</span>\n<span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">gct</span>.<span class=\"pl-s1\">dataset</span> <span class=\"pl-k\">import</span>\
    \ <span class=\"pl-s1\">random_dataset</span>\n<span class=\"pl-c\">#create a\
    \ random graph use LFR generator</span>\n<span class=\"pl-s1\">ds</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s1\">random_dataset</span>.<span class=\"\
    pl-en\">generate_undirected_unweighted_random_graph_LFR</span>(<span class=\"\
    pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"random_graph\"\
    </span>, \\\n                                       <span class=\"pl-v\">N</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-s1\"\
    >k</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">16</span>, <span\
    \ class=\"pl-s1\">maxk</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >32</span>, <span class=\"pl-s1\">mu</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">0.2</span>, <span class=\"pl-s1\">minc</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">32</span>)\n<span class=\"pl-c\"># run pScan\
    \ graph algorithm</span>\n<span class=\"pl-s1\">pscan_clustering</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">gct</span>.<span class=\"pl-en\">scan_pScan</span>(<span\
    \ class=\"pl-s\">\"get_start_pscan\"</span>, <span class=\"pl-s1\">ds</span>)</pre></div>\n\
    <p>See more to visit <a href=\"https://lizhen0909.github.io/graph_clustering_toolkit/usage/usage.html\"\
    \ rel=\"nofollow\">online usage</a>.</p>\n<h3>\n<a id=\"user-content-citation\"\
    \ class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Citation:</h3>\n<p>Please cite\
    \ <a href=\"https://arxiv.org/abs/2005.04806\" rel=\"nofollow\">Comparison and\
    \ Benchmark of Graph Clustering Algorithms</a> for this work.</p>\n<p>For individual\
    \ algorithms, see <a href=\"https://lizhen0909.github.io/graph_clustering_toolkit/usage/pydoc_alg.html\"\
    \ rel=\"nofollow\">Algorithms</a> for their publications.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1589386210.0
LuisBonillaR/singularity:
  data_format: 2
  description: containers
  filenames:
  - Singularity.py3_tfstable
  - Singularity.pyhon3
  full_name: LuisBonillaR/singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3399" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PAML tool for phylogenetic analyses of DNA
    or protein sequences using maximum likelihood.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610738330.0
MASILab/PreQual:
  data_format: 2
  description: An automated pipeline for integrated preprocessing and quality assurance
    of diffusion weighted MRI images
  filenames:
  - Singularity
  full_name: MASILab/PreQual
  latest_release: v1.0.6
  readme: "<h1>\n<a id=\"user-content-prequal-dtiqa-v7-multi-user-guide\" class=\"\
    anchor\" href=\"#prequal-dtiqa-v7-multi-user-guide\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>PreQual (dtiQA\
    \ v7 Multi) User Guide</h1>\n<h2>\n<a id=\"user-content-contents\" class=\"anchor\"\
    \ href=\"#contents\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Contents</h2>\n<ul>\n<li><a href=\"#overview\"\
    >Overview</a></li>\n<li><a href=\"#authors-and-reference\">Authors and Reference</a></li>\n\
    <li><a href=\"#getting-started\">Getting Started</a></li>\n<li><a href=\"#containerization-of-source-code\"\
    >Containerization of Source Code</a></li>\n<li><a href=\"#command\">Command</a></li>\n\
    <li><a href=\"#arguments-and-io\">Arguments and I/O</a></li>\n<li><a href=\"#configuration-file\"\
    >Configuration File</a></li>\n<li><a href=\"#examples\">Examples</a></li>\n<li><a\
    \ href=\"#running-bids-data\">Running BIDS Data</a></li>\n<li><a href=\"#options\"\
    >Options</a></li>\n<li><a href=\"#pipeline-assumptions\">Pipeline Assumptions</a></li>\n\
    <li><a href=\"#pipeline-processing-steps\">Pipeline Processing Steps</a></li>\n\
    <li><a href=\"#pipeline-quality-assurance-steps\">Pipeline Quality Assurance Steps</a></li>\n\
    <li><a href=\"#outputs\">Outputs</a></li>\n<li><a href=\"#note-on-versioning-for-vuiis-xnat-users\"\
    >Note on Versioning for VUIIS XNAT Users</a></li>\n</ul>\n<h2>\n<a id=\"user-content-overview\"\
    \ class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Overview</h2>\n<p><a href=\"\
    https://github.com/MASILab/PreQual/blob/master/overview.png?raw=true\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/MASILab/PreQual/raw/master/overview.png?raw=true\"\
    \ alt=\"Pipeline Overview\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li>\n<p><strong>Summary:</strong>\
    \ Perform integrated preprocessing and quality assurance of diffusion MRI data</p>\n\
    </li>\n<li>\n<p><strong>Preprocessing Steps:</strong></p>\n<ol>\n<li>MP-PCA denoising\
    \ (default on)</li>\n<li>Gibbs de-ringing (default off)</li>\n<li>Rician correction\
    \ (default off)</li>\n<li>Inter-scan normalization (default on)</li>\n<li>Susceptibility-induced\
    \ distortion correction, with or without reverse gradient images or field maps</li>\n\
    <li>Eddy current-induced distortion correction</li>\n<li>Inter-volume motion correction</li>\n\
    <li>Slice-wise signal dropout imputation</li>\n<li>N4 B1 bias field correction\
    \ (default off)</li>\n</ol>\n</li>\n<li>\n<p><strong>Quality Assurance Steps:</strong></p>\n\
    <ol>\n<li>Verification of phase encoding schemes</li>\n<li>Analysis of gradient\
    \ directions</li>\n<li>Shell-wise analysis of signal-to-noise and contrast-to-noise\
    \ ratios</li>\n<li>Visualization of Gibbs de-ringing changes (if applicable)</li>\n\
    <li>Visualization of within brain intensity distributions before and after Rician\
    \ correction (if applicable)</li>\n<li>Correction (if applicable) or visualization\
    \ of inter-scan intensity relationships</li>\n<li>Shell-wise analysis of distortion\
    \ corrections</li>\n<li>Analysis of inter-volume motion and slice-wise signal\
    \ dropout</li>\n<li>Analysis of B1 bias fields (if applicable)</li>\n<li>Verification\
    \ of intra-pipeline masking</li>\n<li>Analysis of tensor goodness-of-fit</li>\n\
    <li>Voxel-wise and region-wise quantification of FA</li>\n<li>Voxel-wise quantification\
    \ of MD</li>\n</ol>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-authors-and-reference\"\
    \ class=\"anchor\" href=\"#authors-and-reference\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Authors and Reference</h2>\n\
    <p><a href=\"mailto:leon.y.cai@vanderbilt.edu\">Leon Y. Cai</a>, Qi Yang, Colin\
    \ B. Hansen, Vishwesh Nath, Karthik Ramadass, Graham W. Johnson, Benjamin N. Conrad,\
    \ Brian D. Boyd, John P. Begnoche, Lori L. Beason-Held, Andrea T. Shafer, Susan\
    \ M. Resnick, Warren D. Taylor, Gavin R. Price, Victoria L. Morgan, Baxter P.\
    \ Rogers, Kurt G. Schilling, Bennett A. Landman. <em>PreQual: An automated pipeline\
    \ for integrated preprocessing and quality assurance of diffusion weighted MRI\
    \ images</em>. <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.28678\"\
    \ rel=\"nofollow\">Magnetic Resonance in Medicine</a>, 2021.</p>\n<p><a href=\"\
    https://my.vanderbilt.edu/masi\" rel=\"nofollow\">Medical-image Analysis and Statistical\
    \ Interpretation (MASI) Lab</a>, Vanderbilt University, Nashville, TN, USA</p>\n\
    <h2>\n<a id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Getting Started</h2>\n<p>The PreQual software is designed to run inside\
    \ a <a href=\"#containerization-of-source-code\">Singularity container</a>. The\
    \ container requires an \"<a href=\"#arguments-and-io\">inputs</a>\" folder that\
    \ holds all required input diffusion image files (i.e., .nii.gz, .bval, and .bvec\
    \ files) and a <a href=\"#configuration-file\">configuration file</a>. For those\
    \ running Synb0-DisCo to correct susceptibility distortions without reverse phase-encoded\
    \ images, this folder will also contain the <a href=\"#arguments-and-io\">structural\
    \ T1 image</a>. The container also requires an \"<a href=\"#arguments-and-io\"\
    >outputs</a>\" folder that will hold all the outputs after the pipeline runs.\
    \ We also need to know the image <em><a href=\"#arguments-and-io\">axis</a></em>\
    \ on which phase encoding was performed for all inputs (i.e., \"i\" for the first\
    \ dimension, \"j\" for the second). To build the configuration file, we need to\
    \ know the <em><a href=\"#configuration-file\">direction</a></em> along said axis\
    \ in which each image was phase encoded (i.e., \"+\" for positive direction and\
    \ \"-\" for the negative direction) and the <a href=\"#configuration-file\">readout\
    \ time</a> for each input image. Once we have this information, we bind the inputs\
    \ and outputs directories into the container to <a href=\"#command\">run the pipeline</a>.</p>\n\
    <p>Note: The phase encoding axis, direction, and readout time must be known ahead\
    \ of time, as this information is not stored in NIFTI headers. Depending on the\
    \ scanner used, they may be available in JSON sidecars when NIFTIs are converted\
    \ from DICOMs with <a href=\"#pipeline-assumptions\">dcm2niix</a>.</p>\n<h2>\n\
    <a id=\"user-content-containerization-of-source-code\" class=\"anchor\" href=\"\
    #containerization-of-source-code\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Containerization of Source Code</h2>\n\
    <pre><code>git clone https://github.com/MASILab/PreQual.git\ncd /path/to/repo/PreQual\n\
    git checkout v1.0.6\nsudo singularity build /path/to/prequal.simg Singularity\n\
    </code></pre>\n<p>We use Singularity version 3.4 with root permissions.</p>\n\
    <h2>\n<a id=\"user-content-command\" class=\"anchor\" href=\"#command\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Command</h2>\n\
    <pre><code>singularity run \n-e \n--contain\n-B /path/to/inputs/directory/:/INPUTS\n\
    -B /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n\
    -B /path/to/cuda:/usr/local/cuda\n--nv\n/path/to/prequal.simg\npe_axis\n[options]\n\
    </code></pre>\n<ul>\n<li>Binding the freesurfer license is optional and only needed\
    \ for Synb0-DisCo</li>\n<li>Binding the tmp directory is necessary when running\
    \ the image with <code>--contain</code>.</li>\n<li>\n<code>--nv</code> and <code>-B\
    \ /path/to/cuda:/usr/local/cuda</code> are optional. See options <code>--eddy_cuda</code>\
    \ and <code>--eddy_extra_args</code>. <strong>GPU support is currently experimental.</strong>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-arguments-and-io\" class=\"anchor\" href=\"\
    #arguments-and-io\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Arguments and I/O</h2>\n<ul>\n<li>\n<p><strong>Input\
    \ Directory:</strong> The dtiQA_config.csv configuration file and at least one\
    \ diffusion weighted image must be provided.</p>\n<ul>\n<li>\n<p>dtiQA_config.csv\
    \ (see <a href=\"#configuration-file\">below</a> for format, must be named exactly)</p>\n\
    </li>\n<li>\n<p>&lt;image1&gt;.nii.gz (diffusion weighted image)</p>\n</li>\n\
    <li>\n<p>&lt;image1&gt;.bval (units of s/mm<sup>2</sup>, in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n</li>\n<li>\n<p>&lt;image1&gt;.bvec (normalized\
    \ unit vectors in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n<p>:</p>\n</li>\n<li>\n<p>&lt;imageN&gt;.nii.gz\
    \ (diffusion weighted image)</p>\n</li>\n<li>\n<p>&lt;imageN&gt;.bval (units of\
    \ s/mm<sup>2</sup>, in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n</li>\n<li>\n<p>&lt;imageN&gt;.bvec (normalized\
    \ unit vectors in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n</li>\n<li>\n<p>t1.nii.gz (Optional, used\
    \ for Synb0-DisCo, must be named exactly)</p>\n</li>\n<li>\n<p>Other files as\
    \ needed (see <code>--extra_eddy_args</code> for more information)</p>\n</li>\n\
    </ul>\n</li>\n<li>\n<p><strong>Output Directory:</strong> Full outputs listed\
    \ at the <a href=\"#outputs\">end</a> of this document</p>\n<ul>\n<li>\n<p>The\
    \ output preprocessed images are available in the PREPROCESSED subfolder in the\
    \ output directory:</p>\n<ul>\n<li>PREPROCESSED/dwmri.nii.gz</li>\n<li>PREPROCESSED/dwmri.bval</li>\n\
    <li>PREPROCESSED/dwmri.bvec</li>\n</ul>\n</li>\n<li>\n<p>The QA document is available\
    \ in the PDF subfolder in the output directory:</p>\n<ul>\n<li>PDF/dtiQA.pdf</li>\n\
    </ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>pe_axis:</strong> Phase encoding\
    \ axis of all the input images. We do NOT support different phase encoding axes\
    \ between different input images at this time. The options are i and j and correspond\
    \ to the first and second dimension of the input images, respectively. Note that\
    \ FSL does not currently support phase encoding in the third dimension (i.e. k,\
    \ the dimension in which the image slices were acquired, commonly axial for RAS\
    \ and LAS oriented images). <strong>This parameter is direction AGNOSTIC</strong>.\
    \ The phase encoding directions of the input images along this axis are specified\
    \ in the dtiQA_config.csv file. See <a href=\"#configuration-file\">Configuration\
    \ File</a> and <a href=\"#examples\">Examples</a> for more information.</p>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-configuration-file\" class=\"anchor\"\
    \ href=\"#configuration-file\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Configuration File</h2>\n<p>The format\
    \ for the lines of the configuration CSV file, dtiQA_config.csv (must be named\
    \ exactly), are as follows:</p>\n<pre><code>&lt;image1&gt;,pe_dir,readout_time\n\
    :\n&lt;imageN&gt;,pe_dir,readout_time\n</code></pre>\n<ul>\n<li>\n<p><strong>&lt;image&gt;</strong>\
    \ is the shared file PREFIX between the corresponding NIFTI, BVAL, and BVEC files\
    \ for that particular image in the input directory (i.e., my_dwi.nii.gz/.bval/.bvec\
    \ -&gt; my_dwi). Do NOT include the path to the input directory.</p>\n</li>\n\
    <li>\n<p><strong>pe_dir</strong> is either + or -, corresponding to the direction\
    \ along the phase encoding axis (as defined by the parameter <code>pe_axis</code>)\
    \ on which the image is phase encoded.</p>\n<ul>\n<li>Note that a combination\
    \ of phase encoding axis and direction map to specific anatomical (i.e. APA, APP,\
    \ etc.) directions based on the orientation of the image. So, for instance in\
    \ a RAS image, an axis of j and direction of + map to APP. We infer the orientation\
    \ of the image from the header of the NIFTI using nibabel tools and output the\
    \ best anatomical phase encoding direction interpretation of the input direction\
    \ in the PDF for QA.</li>\n</ul>\n</li>\n<li>\n<p><strong>readout_time</strong>\
    \ is a non-negative number, the readout_time parameter required by FSL\u2019s\
    \ eddy. The absolute value of this parameter is used to scale the estimated b0\
    \ field. Note a value of 0 indicates that the images are infinite bandwidth (i.e.\
    \ no susceptibility distortion). See <a href=\"#examples\">Examples</a> for more\
    \ information.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-examples\" class=\"\
    anchor\" href=\"#examples\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Examples</h2>\n<p>Here are some different example\
    \ combinations of pe_axis, pe_dir, and readout_time parameters and the corresponding\
    \ FSL acquisition parameters lines:</p>\n<table>\n<thead>\n<tr>\n<th>pe_axis</th>\n\
    <th>pe_dir</th>\n<th>readout_time</th>\n<th>acqparams line</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td>i</td>\n<td>+</td>\n<td>0.05</td>\n<td>1, 0, 0, 0.05</td>\n\
    </tr>\n<tr>\n<td>j</td>\n<td>-</td>\n<td>0.1</td>\n<td>0, -1, 0, 0.1</td>\n</tr>\n\
    </tbody>\n</table>\n<p>These are examples of common use cases. They also all share\
    \ the same command, as detailed above. The PREPROCESSED output folder will contain\
    \ the final outputs and the PDF folder will contain the QA report.</p>\n<table>\n\
    <thead>\n<tr>\n<th>Phase Encoding<br>Axis</th>\n<th>Reverse Phase<br>Encoded (RPE)\
    \ Image</th>\n<th>T1<br>Image</th>\n<th>Contents of<br>Input Directory</th>\n\
    <th>Contents of<br>dtiQA_config.csv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>j</td>\n\
    <td>Yes</td>\n<td>N/A</td>\n<td>dti1.nii.gz<br>dti1.bval<br>dti1.bvec<br>dti2.nii.gz<br>dti2.bval<br>dti2.bvec<br>rpe.nii.gz<br>rpe.bval<br>rpe.bvec<br>dtiQA_config.csv</td>\n\
    <td>dti1,+,0.05<br>dti2,+,0.05<br>rpe,-,0.05</td>\n</tr>\n<tr>\n<td>j</td>\n<td>No</td>\n\
    <td>Yes</td>\n<td>dti1.nii.gz<br>dti1.bval<br>dti1.bvec<br>dti2.nii.gz<br>dti2.bval<br>dti2.bvec<br>t1.nii.gz<br>dtiQA_config.csv</td>\n\
    <td>dti1,+,0.05<br>dti2,+,0.05</td>\n</tr>\n<tr>\n<td>j</td>\n<td>No</td>\n<td>No</td>\n\
    <td>dti1.nii.gz<br>dti1.bval<br>dti1.bvec<br>dti2.nii.gz<br>dti2.bval<br>dti2.bvec<br>dtiQA_config.csv</td>\n\
    <td>dti1,+,0.05<br>dti2,+,0.05</td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"\
    user-content-running-bids-data\" class=\"anchor\" href=\"#running-bids-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running BIDS Data</h2>\n<p>While not a BIDS pipeline, data in BIDS\
    \ format can be run with PreQual without moving or copying data. The key is that\
    \ the input directory structure must be as described relative to <em>inside the\
    \ container</em>. By creatively binding files/folders into the container, we can\
    \ achieve the same effect:</p>\n<pre><code>-B /path/to/sub-X/ses-X/dwi/:/INPUTS\n\
    -B /path/to/sub-X/ses-X/anat/sub-X_ses-X_T1w.nii.gz:/INPUTS/t1.nii.gz (optional,\
    \ Synb0-DisCo only)\n-B /path/to/config/file.csv:/INPUTS/dtiQA_config.csv\n-B\
    \ /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n\
    </code></pre>\n<p>The outputs directory and configuration file can be created\
    \ wherever makes the most sense for the user. The contents of the configuration\
    \ file will look something like this:</p>\n<pre><code>sub-X_ses-X_acq-1_dwi,pe_dir,readout_time\n\
    :\nsub-X_ses-X_acq-N_dwi,pe_dir,readout_time\n</code></pre>\n<h2>\n<a id=\"user-content-options\"\
    \ class=\"anchor\" href=\"#options\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Options</h2>\n<p><strong>--bval_threshold\
    \ N</strong></p>\n<p>A non-negative integer threshold under which to consider\
    \ a b-value to be zero. Useful when some MRI machines do not allow for more than\
    \ one b0 volume to be acquired so some users acquire scans with extremely low\
    \ b-values to be treated like b0 volumes. Setting this value to 0 results in no\
    \ thresholding. Units = s/mm<sup>2</sup>.</p>\n<p>Default = 50</p>\n<p><strong>--nonzero_shells\
    \ s1,s2,...,sn/auto</strong></p>\n<p>A comma separated list of positive integers\
    \ (s/mm<sup>2</sup>) indicating nonzero shells for SNR/CNR analysis when there\
    \ are more unique b-values than shells determined by eddy or automatically determine\
    \ shells by rounding to nearest 100. Useful when b-values are modulated around\
    \ a shell value instead of set exactly at that value. Only used when determining\
    \ shells for SNR/CNR analysis. Original b-values used elsewhere in pipeline.</p>\n\
    <p>Default = auto</p>\n<p><strong>--denoise on/off</strong></p>\n<p>Denoise images\
    \ prior to preprocessing using Marchenko-Pastur PCA <a href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwidenoise.html\"\
    \ rel=\"nofollow\">implemented in MRTrix3</a>. The SNR of the b0s of the final\
    \ preprocessed images are reported in the PDF output regardless of whether this\
    \ option is on or off.</p>\n<p>Default = on</p>\n<p><strong>--degibbs on/off</strong></p>\n\
    <p>Remove Gibbs ringing artifacts using the local subvoxel-shifts method as <a\
    \ href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/mrdegibbs.html\"\
    \ rel=\"nofollow\">implemented in MRTrix3</a>. We caution against using this feature\
    \ as it not designed for the partial Fourier schemes with which most echo planar\
    \ diffusion images are acquired. It is also difficult to quality check, but we\
    \ include a visualization of averaged residuals across all b = 0 s/mm<sup>2</sup>\
    \ volumes, looking for larger signals near high contrast (i.e. parenchyma-CSF)\
    \ interfaces.</p>\n<p>Default = off</p>\n<p><strong>--rician on/off</strong></p>\n\
    <p>Perform Rician correction using the method of moments. We normally do not perform\
    \ this step as we empirically do not find it to affect results drastically. It\
    \ is also difficult to quality check, but we include a plot of the shell-wise\
    \ within brain intensity distributions for each input before and after correction,\
    \ looking for a slight drop in intensity with correction.</p>\n<p>Default = off</p>\n\
    <p><strong>--prenormalize on/off</strong></p>\n<p>Intensity normalize images prior\
    \ to preprocessing by maximizing the intra-mask intensity-histogram intersections\
    \ between the averaged b0s of the scans. If this option is on, these histograms\
    \ before and after prenormalization will be reported in the output PDF. This is\
    \ done to avoid gain differences between different diffusion scans. If this option\
    \ is off, we assume that the various input images all have the same gain. That\
    \ being said, we still estimate and report the gain factors and intensity histograms\
    \ in a gain QA page and report warnings if estimated gains greater than 5% are\
    \ found.</p>\n<p>Default = on</p>\n<p><strong>--synb0 on/off</strong></p>\n<p>Run\
    \ <code>topup</code> with a synthetic b0 generated with the Synb0-DisCo deep-learning\
    \ framework if no reverse phase encoded images are supplied and a T1 image is\
    \ supplied. Synb0-DisCo requires at least 24GB of RAM.</p>\n<p>Default = on</p>\n\
    <p><strong>--topup_first_b0s_only</strong></p>\n<p>Run <code>topup</code> with\
    \ only the first b0 from each input image. At the time of writing, <strong>FSL's\
    \ topup cannot be parallelized</strong>, and the runtime of topup can increase\
    \ dramatically as more b0 volumes are included. This flag allows for faster processing\
    \ at the expense of information lost from any interleaved b0s.</p>\n<p>Default\
    \ = use ALL b0s</p>\n<p><strong>--extra_topup_args=string</strong></p>\n<p>Extra\
    \ arguments to pass to FSL\u2019s <code>topup</code>. <code>Topup</code> will\
    \ run with the following by default (as listed in the <code>/SUPPLEMENTAL/topup.cnf</code>\
    \ configuration file) but will be overwritten by arguments passed to <code>--extra_topup_args</code>:</p>\n\
    <pre><code># Resolution (knot-spacing) of warps in mm\n--warpres=20,16,14,12,10,6,4,4,4\n\
    # Subsampling level (a value of 2 indicates that a 2x2x2 neighbourhood is collapsed\
    \ to 1 voxel)\n--subsamp=1,1,1,1,1,1,1,1,1\n# FWHM of gaussian smoothing\n--fwhm=8,6,4,3,3,2,1,0,0\n\
    # Maximum number of iterations\n--miter=10,10,10,10,10,20,20,30,30\n# Relative\
    \ weight of regularisation\n--lambda=0.00033,0.000067,0.0000067,0.000001,0.00000033,0.000000033,0.0000000033,0.000000000033,0.00000000000067\n\
    # If set to 1 lambda is multiplied by the current average squared difference\n\
    --ssqlambda=1\n# Regularisation model\n--regmod=bending_energy\n# If set to 1\
    \ movements are estimated along with the field\n--estmov=1,1,1,1,1,0,0,0,0\n#\
    \ 0=Levenberg-Marquardt, 1=Scaled Conjugate Gradient\n--minmet=0,0,0,0,0,1,1,1,1\n\
    # Quadratic or cubic splines\n--splineorder=3\n# Precision for calculation and\
    \ storage of Hessian\n--numprec=double\n# Linear or spline interpolation\n--interp=spline\n\
    # If set to 1 the images are individually scaled to a common mean intensity \n\
    --scale=0\n</code></pre>\n<p>These parameters should be formatted as a list separated\
    \ by +'s with no spaces (i.e., <code>--extra_topup_args=--scale=1+--regrid=0</code>).\
    \ For <code>topup</code> options that require additional inputs, place the file\
    \ in the inputs directory and use the following syntax: <code>--&lt;myinputoption&gt;=/INPUTS/&lt;file.ext&gt;</code>.\
    \ For <code>topup</code> options that produce additional outputs, the file will\
    \ save in the output directory under the \u201CTOPUP\u201D folder by using the\
    \ following syntax: <code>--&lt;myoutputoption&gt;=/OUTPUTS/TOPUP/&lt;file.ext&gt;</code>.\
    \ Note that in this case <code>/INPUTS</code> and <code>/OUTPUTS</code> should\
    \ be named exactly as is and are NOT the path to the input and output directory\
    \ on your file system.</p>\n<p>Default = none</p>\n<p><strong>--eddy_cuda 8.0/9.1/off</strong></p>\n\
    <p>Run FSL\u2019s <code>eddy</code> with NVIDIA GPU acceleration. If this parameter\
    \ is 8.0 or 9.1, either CUDA 8.0 or 9.1 must be installed, properly configured\
    \ on your system, and bound into the container, respectively. Additionally the\
    \ <code>--nv</code> flag must be run in the singularity command. If this parameter\
    \ is off, <code>eddy</code> is run with OPENMP CPU multithreading. See <code>--num_threads</code>\
    \ for more information. CUDA is required to run <code>eddy</code> with <code>--mporder</code>\
    \ (intra-volume slice-wise motion correction). See <code>--extra_eddy_args</code>\
    \ for more information.</p>\n<p>Default = off</p>\n<p><strong>--eddy_mask on/off</strong></p>\n\
    <p>Run <code>eddy</code> with or without a brain mask. If on, FSL\u2019s brain\
    \ extraction tool (<code>bet</code>) is used with a low threshold to create a\
    \ rough brain mask for <code>eddy</code>. This can sometimes produce poor results.\
    \ If off, no mask is used and produces empirically minor differences in results\
    \ than when a mask is used. If this option is on, the contour of this mask is\
    \ drawn in the PDF.</p>\n<p>Default = on</p>\n<p><strong>--eddy_bval_scale N/off</strong></p>\n\
    <p>Run <code>eddy</code> with b-values scaled by the positive number N. All other\
    \ steps of the pipeline use the original b-values. This can help <code>eddy</code>\
    \ finish distortion correction when extremely low b-values (&lt;200) are involved.\
    \ If off, no scaling of b-values is used.</p>\n<p>Default = off</p>\n<p><strong>--extra_eddy_args=string</strong></p>\n\
    <p>Extra arguments to pass to FSL\u2019s <code>eddy</code>. <code>Eddy</code>\
    \ will always run with the following:</p>\n<pre><code>--repol\n</code></pre>\n\
    <p>Note that if <code>--mporder</code> is passed here, <code>--eddy_cuda</code>\
    \ must be 8.0 or 9.1 and the singularity option <code>--nv</code> must be passed\
    \ into the container, as intra-volume slice-wise motion correction requires GPU\
    \ acceleration.</p>\n<p>These parameters should be formatted as a list separated\
    \ by +'s with no spaces (i.e., <code>--extra_eddy_args=--data_is_shelled+--ol_nstd=1</code>).\
    \ For <code>eddy</code> options that require additional inputs, place the file\
    \ in the inputs directory and use the following syntax: <code>--&lt;myinputoption&gt;=/INPUTS/&lt;file.ext&gt;</code>.\
    \ For <code>eddy</code> options that produce additional outputs, the file will\
    \ save in the output directory under the \u201CEDDY\u201D folder by using the\
    \ following syntax: <code>--&lt;myoutputoption&gt;=/OUTPUTS/EDDY/&lt;file.ext&gt;</code>.\
    \ Note that in this case <code>/INPUTS</code> and <code>/OUTPUTS</code> should\
    \ be named exactly as is and are NOT the path to the input and output directory\
    \ on your file system.</p>\n<p>Default = none</p>\n<p><strong>--postnormalize\
    \ on/off</strong></p>\n<p>Intensity normalize images after preprocessing by maximizing\
    \ the intra-mask intensity-histogram intersections between the averaged b0s of\
    \ the scans. If this option is on, these histograms before and after postnormalization\
    \ will be reported in the output PDF.</p>\n<p>Note: This option was intended for\
    \ testing and is left for posterity. It is not recommended at this time and will\
    \ be deprecated.</p>\n<p>Default = off</p>\n<p><strong>--correct_bias on/off</strong></p>\n\
    <p>Perform <a href=\"https://manpages.debian.org/testing/ants/N4BiasFieldCorrection.1.en.html\"\
    \ rel=\"nofollow\">ANTs N4 bias field correction</a> as <a href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwibiascorrect.html\"\
    \ rel=\"nofollow\">called in MRTrix3</a>. If this option is on, the calculated\
    \ bias field will be visualized in the output PDF.</p>\n<p>Default = off</p>\n\
    <p><strong>--improbable_mask on/off</strong></p>\n<p>Create an additional mask\
    \ on the preprocessed data that omits voxels where the minimum b0 signal is smaller\
    \ than the minimum diffusion weighted signal. This can be helpful for reducing\
    \ artifacts near the mask border when fitting models.</p>\n<p>Default = off</p>\n\
    <p><strong>--glyph_type tensor/vector</strong></p>\n<p>Visualize either tensors\
    \ or principal eigenvectors in the QA document.</p>\n<p>Default = tensor</p>\n\
    <p><strong>--atlas_reg_type FA/b0</strong></p>\n<p>Perform JHU white matter atlas\
    \ registration to the subject by either deformably registering the subject's FA\
    \ map or average b0 to the MNI FA or T2 template, respectively. Empirically, the\
    \ FA approach tends to be more accurate for white matter whereas the b0 approach\
    \ tends to be more accurate globally. The b0 approach is more robust for acquisitions\
    \ with low shells (i.e., b &lt; 500 s/mm<sup>2</sup>) or poor masking that result\
    \ in the inclusion of a lot of facial structure.</p>\n<p>Default = FA</p>\n<p><strong>--split_outputs</strong></p>\n\
    <p>Split the fully preprocessed output (a concatenation of the input images) back\
    \ into their component parts and do NOT keep the concatenated preprocessed output.</p>\n\
    <p>Default = Do NOT split and return only the concatenated output</p>\n<p><strong>--keep_intermediates</strong></p>\n\
    <p>Keep intermediate copies of diffusion data (i.e. denoised, prenormalized, bias-corrected,\
    \ etc.) used to generate final preprocessed data. Using this flag may result in\
    \ a large consumption of hard disk space.</p>\n<p>Note: Due to space concerns,\
    \ special permission needed to use this option on XNAT.</p>\n<p>Default = do NOT\
    \ keep intermediates</p>\n<p><strong>--num_threads N</strong></p>\n<p>A positive\
    \ integer indicating the number of threads to use when running portions of the\
    \ pipeline that can be multithreaded (i.e. MRTrix3, ANTs, and FSL\u2019s eddy\
    \ without GPU acceleration). Please note that at the time of writing, <strong>FSL's\
    \ topup cannot be parallelized</strong>, and that the runtime of topup can increase\
    \ dramatically as more b0 volumes are included. See <code>--topup_first_b0s_only</code>\
    \ for more information.</p>\n<p>Note: Due to resource concerns, special permission\
    \ needed to multi-thread on XNAT.</p>\n<p>Default = 1 (do NOT multithread)</p>\n\
    <p><strong>--project string</strong></p>\n<p>String describing project in which\
    \ the input data belong to label PDF output</p>\n<p>Default = proj</p>\n<p><strong>--subject\
    \ string</strong></p>\n<p>String describing subject from which the input data\
    \ were acquired to label PDF output</p>\n<p>Default = subj</p>\n<p><strong>--session\
    \ string</strong></p>\n<p>String describing session in which the input data were\
    \ acquired to label PDF output</p>\n<p>Default = sess</p>\n<p><strong>--help,\
    \ -h</strong></p>\n<h2>\n<a id=\"user-content-pipeline-assumptions\" class=\"\
    anchor\" href=\"#pipeline-assumptions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Assumptions</h2>\n<ul>\n\
    <li>\n<p>All NIFTI images are consistent with a conversion from a DICOM using\
    \ <code>dcm2niix</code> (<a href=\"https://github.com/rordenlab/dcm2niix/releases/tag/v1.0.20180622\"\
    >at least v1.0.20180622</a>) by Chris Rorden and are raw NIFTIs without distortion\
    \ correction. We require this as dcm2niix exports b-value/b-vector files in FSL\
    \ format and removes ADC or trace images auto-generated in some Philips DICOMs.\
    \ In addition <code>dcm2niix</code> correctly moves the gradients from scanner\
    \ to subject space and does not re-order volumes, both of which can cause spurious\
    \ results or pipeline failure.</p>\n<ul>\n<li>\n<p><strong>We expect raw volumes\
    \ only, no ADC or trace volumes.</strong> ADC volumes are sometimes encoded as\
    \ having a b-value greater than 0 with a corresponding b-vector of (0,0,0) and\
    \ trace volumes are sometimes encoded as having a b-value of 0 with a corresponding\
    \ non-unit normalized b-vector, as in the case of some Philips PARREC converters.\
    \ We check for these cases, remove the affected volumes, and report a warning\
    \ in the console and in the PDF.</p>\n</li>\n<li>\n<p>We cannot, unfortunately,\
    \ account for failure of reorientation of gradients into subject space. Visualization\
    \ of tensor glyphs or principal eigenvectors can be helpful in distinguishing\
    \ this. However, this error can be subtle so we suggest proper DICOM to NIFTI\
    \ conversion with the above release of <code>dcm2niix</code>.</p>\n</li>\n</ul>\n\
    </li>\n<li>\n<p>Images will be processed in the order they are listed in dtiQA_config.csv.</p>\n\
    </li>\n<li>\n<p>The size of all the volumes across all images must all be the\
    \ same.</p>\n</li>\n<li>\n<p>The location of b0 images inside the input images\
    \ do not matter.</p>\n</li>\n<li>\n<p>As per the FSL format, we do not support\
    \ non-unit normalized gradients. We also do not support gradient directions of\
    \ 0,0,0 when the corresponding b-value is non-zero. Gradients with the latter\
    \ configurations may cause pipeline failure. We report warnings in the output\
    \ PDF if we identify these.</p>\n</li>\n<li>\n<p>The phase encoding axis of all\
    \ volumes across all images is the same.</p>\n</li>\n<li>\n<p>The phase encoding\
    \ direction along the axis is the same for all volumes inside an image and is\
    \ specified in the dtiQA_config.csv file.</p>\n</li>\n<li>\n<p>Unless <code>--prenormalize</code>\
    \ is on, we assume all input images have the same gain.</p>\n</li>\n<li>\n<p>We\
    \ will preferentially preprocess images with FSL\u2019s topup using available\
    \ images with complementary phase encoding directions (i.e. + and -, \"reverse\
    \ phase encodings\"). If none are available and a T1 is available, we will synthesize\
    \ a susceptibility-corrected b0 from the first image listed in dtiQA_config.csv\
    \ with Synb0-DisCo for use with topup, unless the user turns the <code>--synb0</code>\
    \ parameter off. The readout time of this synthetic b0 will be zero and the phase\
    \ encoding direction will be equal to that of the first image in dtiQA_config.csv.\
    \ Otherwise, we will preprocess without topup and move straight to FSL\u2019s\
    \ eddy.</p>\n</li>\n<li>\n<p>We use topup and eddy for preprocessing, both of\
    \ which at the present moment do NOT officially support DSI acquisitions but only\
    \ single- and multi-shell. We will force topup and eddy to run on DSI data, but\
    \ may not produce quality results. Please carefully check the PDF output as we\
    \ report a warning if eddy detected non-shelled data and thus required the use\
    \ of the force flag.</p>\n<ul>\n<li>Note that eddy may erroneously detect data\
    \ as non-shelled if there are fewer directions in one of the shells than others.\
    \ Because we merge the images for preprocessing, a notable example of this is\
    \ when a reverse-phase encoded image uses a different shell than the forward images\
    \ and has significantly fewer directions.</li>\n</ul>\n</li>\n<li>\n<p>For preprocessing,\
    \ eddy will motion correct to the first b0 of each image.</p>\n</li>\n<li>\n<p>MRTrix3\
    \ by default preferentially uses the qform for understanding NIFTI orientations.\
    \ Nibabel uses the sform. We set MRTrix3 to use the sform in our pipeline, and\
    \ thus we preferentially use the sform when the two don\u2019t match.</p>\n</li>\n\
    <li>\n<p>No b0 drift correction is performed.</p>\n</li>\n<li>\n<p>We use the\
    \ fit tensor model primarily for QA. If b-values less than 500 s/mm<sup>2</sup>\
    \ or greater than 1500 s/mm<sup>2</sup> are present, we suggest careful review\
    \ of the fit prior to use for non-QA purposes.</p>\n</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-pipeline-processing-steps\" class=\"anchor\" href=\"#pipeline-processing-steps\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pipeline Processing Steps</h2>\n<ol>\n<li>\n<p>Threshold all b-values\
    \ such that values less than the <code>--bval_threshold</code> parameter are 0.</p>\n\
    </li>\n<li>\n<p>Check that all b-vectors are unit normalized and all b-values\
    \ greater than zero have associated non-zero b-vectors. For any volumes where\
    \ this is not the case, we remove them, flag a warning for the output PDF, and\
    \ continue the pipeline.</p>\n</li>\n<li>\n<p>If applicable, denoise all diffusion\
    \ scans with <code>dwidenoise</code> (Marchenko-Pastur PCA) from MrTrix3 and save\
    \ the noise profiles (needed for Rician correction later).</p>\n</li>\n<li>\n\
    <p>If applicable, perform Gibbs de-ringing on all diffusion scans with <code>mrdegibbs</code>\
    \ from MRTrix3.</p>\n</li>\n<li>\n<p>If applicable, perform Rician correction\
    \ on all diffusion scans with the method of moments.</p>\n</li>\n<li>\n<p>If applicable,\
    \ prenormalize all diffusion scans. To accomplish this, extract all b0 images\
    \ from each diffusion scan and average them. Then find a rough brain-mask with\
    \ FSL\u2019s bet and calculate an intensity scale factor such that the histogram\
    \ intersection between the intra-mask histogram of the different scans\u2019 averaged\
    \ b0s to that of the first scan is maximized. Apply this scale factor to the entire\
    \ diffusion weighted scan. This is done to avoid gain differences between different\
    \ diffusion scans.</p>\n<ol>\n<li>If prenormalization is not indicated, we still\
    \ run the prenormalization algorithms to calculate rough gain differences and\
    \ report the gain factors and intensity histograms in a gain QA page. The outputs\
    \ of the algorithms, however, are NOT propagated through to the rest of the pipeline.</li>\n\
    </ol>\n</li>\n<li>\n<p>Prepare data for and run preprocessing with topup and eddy</p>\n\
    <ol>\n<li>\n<p>Topup:</p>\n<ol>\n<li>\n<p>Extract all b0s from all scans, maintaining\
    \ their relative order.</p>\n</li>\n<li>\n<p>(Optional) If a T1 is supplied and\
    \ no complementary (i.e. reverse) phase encoded images are provided, use Synb0-DisCo\
    \ to convert the first b0 of the first scan to a susceptibility-corrected b0.</p>\n\
    </li>\n<li>\n<p>Build the acquisition parameters file required by both topup and\
    \ eddy</p>\n<ol>\n<li>\n<p>For the number of b0s from each image, add the same\
    \ phase encoding and readout time line to the acquisition parameters file, as\
    \ outlined in \"Example Phase Encoding Schemes\".</p>\n<ol>\n<li>Example: In the\
    \ case where we have a phase encoding axis of j and two images, one with 7 b0s,\
    \ + direction, and 0.05 readout time and one with 3 b0s, - direction, and 0.02\
    \ readout time, this file will have 10 lines. The first 7 lines are identical\
    \ and equal to [0, 1, 0, 0.05]. The last three lines are also identical and equal\
    \ to [0, -1, 0, 0.02].</li>\n</ol>\n</li>\n<li>\n<p>(Optional) If Synb0-DisCo\
    \ is run because no complementary phase encoding directions are supplied and --synb0\
    \ is not off, we add an additional line to the end of the file. This line is the\
    \ same as the first line of the file except that the readout time is 0 instead.</p>\n\
    <ol>\n<li>Example: In the case where we have a phase encoding axis of j and two\
    \ images, one with 7 b0s, + direction, and 0.05 readout time and one with 3 b0s,\
    \ + direction, and 0.02 readout time, this file will have 11 lines. The first\
    \ 7 lines are identical and equal to [0, 1, 0, 0.05]. The next three lines are\
    \ also identical and equal to [0, 1, 0, 0.02]. Finally, the last line is equal\
    \ to [0, 1, 0, 0].</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>\n<p>We then concatenate\
    \ all the b0s maintaining their order and run topup with the acquisition parameters\
    \ file if images with complementary phase encoding directions are supplied or\
    \ if a T1 was supplied. Otherwise, we move on to the next step, eddy.</p>\n</li>\n\
    </ol>\n</li>\n<li>\n<p>Eddy</p>\n<ol>\n<li>\n<p>Using the acquisition parameters\
    \ file from the topup step, regardless of whether topup was performed, we build\
    \ the eddy index file such that each volume in each image corresponds to the line\
    \ in the acquisition parameters file associated with the first b0 of each scan.\
    \ This is done to tell eddy that each volume in a given scan has the same underlying\
    \ phase encoding scheme as the first b0 of that scan.</p>\n<ol>\n<li>Example:\
    \ In the case where we have two images, one with 7 b0s and 100 total volumes and\
    \ one with 3 b0s and 10 total volumes, the eddy index file has 100 1\u2019s followed\
    \ by 10 8\u2019s.</li>\n</ol>\n</li>\n<li>\n<p>Eddy is then run with either a\
    \ mask generated with bet and the -f 0.25 and -R options or without a mask (aka\
    \ with a mask of all 1\u2019s), depending on user input (see the --eddy_mask option)\
    \ and with the output of topup if topup was run. Eddy also runs with the --repol\
    \ option for outlier slice replacement. We also first run eddy with a check looking\
    \ for shelled data. If the check fails, eddy is then run with the --data_is_shelled\
    \ flag to force eddy to run on all scans, DSI included. Note that DSI data is\
    \ not officially supported by FSL\u2026 yet?</p>\n<ol>\n<li>\n<p>If eddy detects\
    \ data is not shelled, we report this as a warning</p>\n</li>\n<li>\n<p>As noted\
    \ in the assumptions section above, eddy may erroneously detect data as non-shelled\
    \ if there are fewer directions in one of the shells than others. Because we merge\
    \ the images for preprocessing, a notable example of this is when a reverse-phase\
    \ encoded image uses a different shell than the forward images and has significantly\
    \ fewer directions.</p>\n</li>\n</ol>\n</li>\n<li>\n<p>Eddy also performs bvec\
    \ rotation correction and calculates the voxel-wise signal-to-noise ratios of\
    \ the b0 images and the voxel-wise contrast-to-noise ratios for the diffusion\
    \ weighted images. SNR is defined as the mean value divided by the standard deviation.\
    \ CNR is defined as the standard deviation of the Gaussian Process predictions\
    \ (GP) divided by the standard deviation of the residuals between the measured\
    \ data and the GP predictions.</p>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>\n\
    <p>If the user chooses to, we then perform post-normalization in the same fashion\
    \ as pre-normalization.</p>\n</li>\n<li>\n<p>If the user chooses to, we then wrap\
    \ up preprocessing with an N4 bias field correction as implemented in ANTs via\
    \ MRTrix3\u2019s dwibiascorrect.</p>\n</li>\n<li>\n<p>We generate a brain mask\
    \ using FSL\u2019s bet2 with the following options. If applicable, we omit the\
    \ voxels where the minimum b0 signal is less than the minimum diffusion weighted\
    \ signal in an additional \"improbable mask\".</p>\n<p><code>-f 0.25 -R</code></p>\n\
    </li>\n<li>\n<p>We then apply the mask to the preprocessed images while we calculate\
    \ tensors using MRTrix3\u2019s dwi2tensor function. For visualization we discard\
    \ tensors that have diagonal elements greater than 3 times the apparent diffusion\
    \ coefficient of water at 37\xB0C (~0.01).</p>\n<ol>\n<li>We also reconstruct\
    \ the preprocessed image from the tensor fit for further analysis later. dwi2tensor\
    \ does this for us.</li>\n</ol>\n</li>\n<li>\n<p>We then convert the tensor to\
    \ FA and MD images (and visualize them later too) as well as AD, RD, and V1 eigenvector\
    \ images for the user. The latter 3 are not visualized.</p>\n</li>\n</ol>\n<h2>\n\
    <a id=\"user-content-pipeline-quality-assurance-steps\" class=\"anchor\" href=\"\
    #pipeline-quality-assurance-steps\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Quality Assurance Steps</h2>\n\
    <ol>\n<li>\n<p>We start with the brain mask generated above to generate a mask\
    \ used for the following quantification of tensor fit using a chi-squared statistic.</p>\n\
    <ol>\n<li>\n<p>First, we calculate the mean image for each unique b-value (0 not\
    \ included). Then we run FSL\u2019s FAST to isolate the CSF on each meaned image.\
    \ We then take the average probability of a voxel being CSF across all unique\
    \ b-values and assign &gt;15% probability to be a positive CSF voxel.</p>\n</li>\n\
    <li>\n<p>Then we call the final chi-squared mask to be the intersection of the\
    \ inverted CSF mask and a 1-pixel eroded version of the brain mask.</p>\n</li>\n\
    </ol>\n</li>\n<li>\n<p>On the voxels inside the chi-squared mask, we perform the\
    \ following quality assurance:</p>\n<ol>\n<li>\n<p>We perform a chi-squared analysis\
    \ for each slice for each volume in the main image by calculating the ratio between\
    \ the sum-squared error of the fit and the sum-squared intensities of the slice.</p>\n\
    </li>\n<li>\n<p>We extract the average FA for a number of white matter ROIs defined\
    \ by the Hopkins atlas. We do this by non-rigidly registering the atlas to our\
    \ FA output and extracting the FA values contained in each ROI.</p>\n</li>\n<li>\n\
    <p>We check the gradients output by eddy (i.e. the preprocessed gradients) with\
    \ <a href=\"https://mrtrix.readthedocs.io/en/3.0.0/reference/commands/dwigradcheck.html\"\
    \ rel=\"nofollow\">dwigradcheck from MRTrix3</a>. This performs tractography and\
    \ finds the optimal sign and order permutation of the b-vectors such that the\
    \ average tract length in the brain is most physiological.</p>\n<ol>\n<li>\n<p>These\
    \ optimized gradients are saved in the OPTIMIZED_BVECS output folder, and the\
    \ gradients output by eddy in the PREPROCESSED folder are NOT overwritten.</p>\n\
    </li>\n<li>\n<p>The original, preprocessed, and preprocessed + optimized gradients\
    \ are visualized as outlined below.</p>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>\n\
    <p>We then visualize the entire pipeline.</p>\n<ol>\n<li>\n<p>On the first page\
    \ we describe the methods used for that run of the pipeline (what inputs were\
    \ provided, what sort of preprocessing happened, etc.).</p>\n</li>\n<li>\n<p>We\
    \ then visualize the raw images with the interpreted phase encoding schemes.</p>\n\
    </li>\n<li>\n<p>If Gibbs de-ringing was run, we visualize central slices of the\
    \ averaged residuals across b0 volumes before and after Gibbs de-ringing, looking\
    \ for large residuals near high contrast interfaces (i.e. parenchyma against CSF)</p>\n\
    </li>\n<li>\n<p>If Rician correction was performed, we visualize the within brain\
    \ intensity distributions of each shell of each image before and after correction,\
    \ looking for downward shifts after correction.</p>\n</li>\n<li>\n<p>If Synb0-DisCo\
    \ was run, we then visualize the distorted b0 (first b0 of first scan) and T1\
    \ used as inputs as well as the output susceptibility corrected b0 in their native\
    \ space.</p>\n</li>\n<li>\n<p>If pre- or post-normalization was performed, we\
    \ then visualize the intra-mask histograms before and after these steps as well\
    \ as the calculated scaling factors. If pre-normalization is not performed, we\
    \ visualize the histograms that would have been generated with pre-normalization\
    \ ONLY as a check for gain differences.</p>\n</li>\n<li>\n<p>We then visualize\
    \ the first b0 of the images before and after preprocessing with the contours\
    \ of the brain and stats masks overlaid as well as the contours of the eddy mask\
    \ overlaid if it is used. We also report the percent of \"improbable voxels\"\
    \ in the preprocessed mask, regardless of whether the improbable mask is saved.</p>\n\
    </li>\n<li>\n<p>We plot the motion and angle correction done by eddy as well as\
    \ the RMS displacement and median intensity for each volume and the volume\u2019\
    s associated b-value. These values are read in from an eddy output text file and\
    \ we also compute and save the average of these values. In addition, we plot the\
    \ outlier slices removed and then imputed by eddy as well as the chi-squared fit,\
    \ with maximal bounds 0 to 0.2. The median chi-squared values are shown across\
    \ volumes and slices.</p>\n</li>\n<li>\n<p>We then plot the original raw b-vectors\
    \ scaled by their b-values, the preprocessed ones output by eddy, and the optimized\
    \ ones determined by <code>dwigradcheck</code> applied to the preprocessed ones.</p>\n\
    </li>\n<li>\n<p>If bias field correction was performed, we then visualize the\
    \ calculated fields.</p>\n</li>\n<li>\n<p>We then visualize some central slices\
    \ of the average volumes for all unique b-values, including b = 0 and report the\
    \ median intra-mask SNR or CNR calculated by eddy as appropriate. If there are\
    \ more unique b-values than shells deteremined by eddy, we round the b-values\
    \ to the nearest 100 by default to assign volumes to shells or we choose the nearest\
    \ shell indicated by the user (see <code>--nonzero_shells</code>).</p>\n</li>\n\
    <li>\n<p>We visualize the tensors (or principal eigenvectors depending on <code>--glyph_type</code>)\
    \ using MRTrix3\u2019s mrview, omitting the tensors with negative eigenvalues\
    \ or eigenvalues greater than 3 times the ADC of water at 37\xB0C.</p>\n</li>\n\
    <li>\n<p>We then visualize some central slices of the FA map clipped from 0 to\
    \ 1 as well as the average FA for the Hopkins ROIs and the quality of the atlas\
    \ registration.</p>\n</li>\n<li>\n<p>Lastly, we visualize some central slices\
    \ of the MD map clipped from 0 to 0.003 (ADC of water at 37\xB0C).</p>\n</li>\n\
    </ol>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-outputs\" class=\"anchor\" href=\"\
    #outputs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Outputs</h2>\n<p>&lt;imageN_%&gt; denotes the original prefix of imageN\
    \ with the preceding preprocessing step descriptors tacked on the end. For example,\
    \ in the case of the PRENORMALIZED directory, the prefix for imageJ may or may\
    \ not include \"_denoised\" depending on whether the denoising step was run.</p>\n\
    <p>Folders and files in <strong>bold</strong> are always included.</p>\n<p>Folders\
    \ and files in <em>italics</em> are removed if <code>--keep_intermediates</code>\
    \ is NOT indicated</p>\n<ol>\n<li>\n<p><strong>THRESHOLDED_BVALS</strong></p>\n\
    <ul>\n<li>\n<p><strong>&lt;image1&gt;.bval</strong></p>\n<p>:</p>\n</li>\n<li>\n\
    <p><strong>&lt;imageN&gt;.bval</strong></p>\n</li>\n</ul>\n</li>\n<li>\n<p><em>CHECKED</em>\
    \ (these contain the volumes that have passed the bval/bvec checks)</p>\n<ul>\n\
    <li>\n<p><em>&lt;image1&gt;_checked.nii.gz</em></p>\n</li>\n<li>\n<p><em>&lt;image1&gt;_checked.bval</em></p>\n\
    </li>\n<li>\n<p><em>&lt;image1&gt;_checked.bvec</em></p>\n<p>:</p>\n</li>\n<li>\n\
    <p><em>&lt;imageN&gt;_checked.nii.gz</em></p>\n</li>\n<li>\n<p><em>&lt;imageN&gt;_checked.bval</em></p>\n\
    </li>\n<li>\n<p><em>&lt;imageN&gt;_checked.bvec</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>DENOISED</em> (these files are only created if <code>--denoise</code>\
    \ is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_denoised.nii.gz</em></p>\n</li>\n\
    <li>\n<p><em>&lt;image1_%&gt;_noise.nii.gz</em> (needed for Rician correction)</p>\n\
    <p>:</p>\n</li>\n<li>\n<p><em>&lt;imageN_%&gt;_denoised.nii.gz</em></p>\n</li>\n\
    <li>\n<p><em>&lt;imageN_%&gt;_noise.nii.gz</em></p>\n</li>\n</ul>\n</li>\n<li>\n\
    <p><em>DEGIBBS</em> (these files are only created if <code>--degibbs</code> is\
    \ on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_degibbs.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_degibbs.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>RICIAN</em> (these files are only created if <code>--rician</code>\
    \ is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_rician.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_rician.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>PRENORMALIZED</em> (these files are only created if <code>--prenormalize</code>\
    \ is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_norm.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_norm.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>GAIN_CHECK</em> (these files are only created if <code>--prenormalize</code>\
    \ is off)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_norm.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_norm.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><strong>TOPUP</strong> (these files are only created if <code>topup</code>\
    \ was run)</p>\n<ul>\n<li>\n<p>acqparams.txt (same as OUTPUTS/EDDY/acqparams.txt)</p>\n\
    </li>\n<li>\n<p><em>preproc_input_b0_first.nii.gz</em> (only if Synb0-DisCo is\
    \ run)</p>\n</li>\n<li>\n<p>b0_syn.nii.gz (only if Synb0-DisCo is run)</p>\n</li>\n\
    <li>\n<p><em>preproc_input_b0_all.nii.gz</em> or <em>preproc_input_b0_all_smooth_with_b0_syn.nii.gz</em></p>\n\
    </li>\n<li>\n<p><em>preproc_input_b0_all_topped_up.nii.gz</em> or <em>preproc_input_b0_all_smooth_with_b0_syn_topped_up.nii.gz</em></p>\n\
    </li>\n<li>\n<p>preproc_input_b0_all.topup_log or preproc_input_b0_all_smooth_with_b0_syn.topup_log</p>\n\
    </li>\n<li>\n<p>topup_field.nii.gz</p>\n</li>\n<li>\n<p>topup_results_fieldcoef.nii.gz</p>\n\
    </li>\n<li>\n<p>topup_results_movpar.txt</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>EDDY</strong></p>\n\
    <ul>\n<li>\n<p><strong>acqparams.txt</strong> (same as OUTPUTS/TOPUP/acqparams.txt)</p>\n\
    </li>\n<li>\n<p><strong>index.txt</strong></p>\n</li>\n<li>\n<p><em>preproc_input.nii.gz</em></p>\n\
    </li>\n<li>\n<p><em>preproc_input.bval</em></p>\n</li>\n<li>\n<p><em>preproc_input.bvec</em></p>\n\
    </li>\n<li>\n<p><em>preproc_input_eddyed.nii.gz</em> (renamed from \"eddy_results.nii.gz\"\
    )</p>\n</li>\n<li>\n<p><em>preproc_input_eddyed.bval</em></p>\n</li>\n<li>\n<p><em>preproc_input_eddyed.bvec</em></p>\n\
    </li>\n<li>\n<p>eddy_mask.nii.gz (only included if <code>--eddy_mask</code> is\
    \ on)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_command_txt</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_movement_rms</strong> (describes volume-wise\
    \ RMS displacement)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_outlier_free_data.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_outlier_map</strong> (describes which\
    \ slices were deemed outliers)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_outlier_n_sqr_stdev_map</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_outlier_n_stdev_map</strong></p>\n</li>\n\
    <li>\n<p><strong>eddy_results.eddy_outlier_report</strong></p>\n</li>\n<li>\n\
    <p><strong>eddy_results.eddy_parameters</strong> (describes volume-wise rotation\
    \ and translation)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_post_eddy_shell_alignment_parameters</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_post_eddy_shell_PE_translation_parameters</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_restricted_movement_rms</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_rotated_bvecs (describes properly rotated\
    \ b-vectors)</strong></p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_values_of_all_input_parameters</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_cnr_maps.nii.gz</strong></p>\n</li>\n\
    </ul>\n</li>\n<li>\n<p><em>POSTNORMALIZED</em> (these files are only created if\
    \ <code>--postnormalize</code> is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_topup_eddy_norm.nii.gz</em>\
    \ (\"_topup\" only applies if topup was run)</p>\n<p>:</p>\n</li>\n<li>\n<p><em>&lt;imageN_%&gt;_topup_eddy_norm.nii.gz</em></p>\n\
    </li>\n</ul>\n</li>\n<li>\n<p><em>UNBIASED</em> (these files are only created\
    \ if <code>--correct_bias</code> is on; this folder is removed if <code>--correct_bias</code>\
    \ is off)</p>\n<ul>\n<li>\n<p><em>normed_unbiased.nii.gz</em> (if postnormalization\
    \ is run) or <em>preproc_input_eddyed_unbiased.nii.gz</em> (if postnormalization\
    \ is not run)</p>\n</li>\n<li>\n<p>bias_field.nii.gz</p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><strong>PREPROCESSED</strong> (these represent the final output of the\
    \ pipeline)</p>\n<ul>\n<li>\n<p><em>dwmri.nii.gz</em> (dwmri* files deleted only\
    \ if <code>--split_outputs</code> is also set)</p>\n</li>\n<li>\n<p><em>dwmri.bval</em></p>\n\
    </li>\n<li>\n<p><em>dwmri.bvec</em></p>\n</li>\n<li>\n<p>&lt;image1&gt;_preproc.nii.gz\
    \ (*_preproc files exist only if <code>--split_outputs</code> is set)</p>\n</li>\n\
    <li>\n<p>&lt;image1&gt;_preproc.bval</p>\n</li>\n<li>\n<p>&lt;image1&gt;_preproc.bvec</p>\n\
    <p>:</p>\n</li>\n<li>\n<p>&lt;imageN&gt;_preproc.nii.gz</p>\n</li>\n<li>\n<p>&lt;imageN&gt;_preproc.bval</p>\n\
    </li>\n<li>\n<p>&lt;imageN&gt;_preproc.bvec</p>\n</li>\n<li>\n<p><strong>mask.nii.gz</strong></p>\n\
    </li>\n<li>\n<p>improbable_mask.nii.gz (only included if <code>--improbable_mask</code>\
    \ is on)</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>TENSOR</strong></p>\n<ul>\n\
    <li>\n<p><strong>dwmri_tensor.nii.gz</strong></p>\n</li>\n<li>\n<p><em>dwmri_recon.nii.gz</em></p>\n\
    </li>\n</ul>\n</li>\n<li>\n<p><strong>SCALARS</strong></p>\n<ul>\n<li>\n<p><strong>dwmri_tensor_fa.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>dwmri_tensor_md.nii.gz</strong></p>\n</li>\n<li>\n<p><strong>dwmri_tensor_ad.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>dwmri_tensor_rd.nii.gz</strong></p>\n</li>\n<li>\n<p><strong>dwmri_tensor_v1.nii.gz</strong></p>\n\
    </li>\n</ul>\n</li>\n<li>\n<p><strong>STATS</strong></p>\n<ul>\n<li>\n<p><strong>atlas2subj.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>b02template_0GenericAffine.mat</strong> or <strong>fa2template_0GenericAffine.mat</strong>\
    \ depending on <code>--atlas_reg_type</code></p>\n</li>\n<li>\n<p><strong>b02template_1Warp.nii.gz</strong>\
    \ or <strong>fa2template_1Warp.nii.gz</strong> depending on <code>--atlas_reg_type</code></p>\n\
    </li>\n<li>\n<p><strong>b02template_1InverseWarp.nii.gz</strong> or <strong>fa2template_1InverseWarp.nii.gz</strong>\
    \ depending on <code>--atlas_reg_type</code></p>\n</li>\n<li>\n<p><strong>chisq_mask.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>chisq_matrix.txt</strong></p>\n</li>\n<li>\n<p><strong>eddy_avg_abs_displacement.txt</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_median_cnr.txt</strong></p>\n</li>\n<li>\n<p><strong>eddy_avg_rel_displacement.txt</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_avg_rotations.txt</strong></p>\n</li>\n<li>\n<p><strong>eddy_avg_translations.txt</strong></p>\n\
    </li>\n<li>\n<p><strong>roi_avg_fa.txt</strong></p>\n</li>\n<li>\n<p><strong>stats.csv</strong>\
    \ (contains summary of all motion, SNR/CNR, and average FA stats)</p>\n</li>\n\
    </ul>\n</li>\n<li>\n<p><strong>OPTIMIZED_BVECS</strong> (these are sign/axis permuted\
    \ per <code>dwigradcheck</code> and are only used for QA purposes)</p>\n<ul>\n\
    <li>\n<p><strong>dwmri.bval</strong></p>\n</li>\n<li>\n<p><strong>dwmri.bvec</strong></p>\n\
    </li>\n</ul>\n</li>\n<li>\n<p><strong>PDF</strong></p>\n<ul>\n<li>\n<strong>dtiQA.pdf</strong>\
    \ (final QA document)</li>\n</ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-note-on-versioning-for-vuiis-xnat-users\"\
    \ class=\"anchor\" href=\"#note-on-versioning-for-vuiis-xnat-users\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Note\
    \ on Versioning for VUIIS XNAT Users</h2>\n<p>PreQual was developed at Vanderbilt\
    \ under the project name \"dtiQA v7 Multi\". PreQual v1.0.0 represents dtiQA v7.2.0.\
    \ Thus, on XNAT, dtiQA v7.2.x refers to PreQual v1.0.x.</p>\n"
  stargazers_count: 9
  subscribers_count: 1
  topics:
  - diffusion
  - mri
  - preprocessing
  - quality
  - assurance
  updated_at: 1623437728.0
MI-911/cold-start-framework:
  data_format: 2
  description: Data generation, model training and evaluation pipelines for the cold-start
    setting.
  filenames:
  - Singularity.def
  full_name: MI-911/cold-start-framework
  latest_release: null
  readme: '<h1>

    <a id="user-content-cold-start-framework" class="anchor" href="#cold-start-framework"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cold-start
    Framework</h1>

    <p>Data partitioning, model training and evaluation pipelines for the cold-start
    setting.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>We have fully dockerized an evaluation pipeline, from downloading the most
    recent dataset to conducting interviews.

    The pipeline was developed using Docker version 19.03.5-ce.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick start</h2>

    <p>From a clean slate, run the pipeline by running the script <code>scripts/run_pipeline.sh</code>.
    The pipeline will:</p>

    <ul>

    <li>Download the latest stable MindReader version and the related entities.</li>

    <li>Partition the downloaded dataset into training (warm-start) and testing (cold-start).</li>

    <li>Run all models on the partitioned dataset.</li>

    </ul>

    <p>We recommend running the entire pipeline initially.

    Following this, one can run the experiments alone by running <code>scripts/run_interview.sh</code>.

    Note that if changes are made to the code, the base image should be rebuilt by
    running <code>scripts/build_base.sh</code>.</p>

    '
  stargazers_count: 4
  subscribers_count: 2
  topics:
  - pipeline
  - recommender-system
  - cold-start
  - evaluation-pipelines
  - dataset
  - interview
  - hacktoberfest
  updated_at: 1607268817.0
MPIB/singularity-fsl:
  data_format: 2
  description: various singularity recipes for FSL
  filenames:
  - Singularity.5.0.9
  - Singularity.6.0.2-Cuda8
  - Singularity.6.0.3
  - Singularity.6.0.1
  - Singularity.5.0.11
  - Singularity.6.0.0
  - Singularity.6.0.2-Cuda8-xtract_viewer
  - Singularity.5.0.10
  - Singularity.5-Cuda8
  - Singularity.6.0.4-Cuda8
  - Singularity.6.0.4
  - Singularity.6.0.2
  full_name: MPIB/singularity-fsl
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/702" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick start</h2>

    <pre><code># Download a (versioned) container

    singularity pull shub://MPIB/singularity-fsl:6.0.4


    # Run it

    singularity exec singularity-fsl_6.0.4.sif fslmaths

    singularity exec --nv singularity-fsl_6.0.4.sif eddy_cuda9.1

    </code></pre>

    <h2>

    <a id="user-content-fsl" class="anchor" href="#fsl" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>FSL</h2>

    <p>Project Home: <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/" rel="nofollow">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/</a></p>

    <p>These are containers primarily used at the MPI for Human Development.</p>

    <h2>

    <a id="user-content-cuda" class="anchor" href="#cuda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cuda</h2>

    <p>Starting with Singularity 6.0.2 we include Nvidia CUDA through Debian backports
    repositories.

    Make sure your Nvidia driver on the host <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility"
    rel="nofollow">supports it</a> and add the <code>--nv</code> flag with singularity.</p>

    <h2>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Note</h2>

    <p>Please be aware of FSL''s strict license regarding non-commercial use.</p>

    '
  stargazers_count: 5
  subscribers_count: 3
  topics:
  - containers
  - science
  updated_at: 1622785432.0
MRtrix3/containers:
  data_format: 2
  description: Hosts DockerFiles to build MRtrix3 containers
  filenames:
  - Singularity
  full_name: MRtrix3/containers
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-containers-for-mrtrix3\" class=\"anchor\" href=\"\
    #containers-for-mrtrix3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Containers for <em>MRtrix3</em>\n</h1>\n<p>Hosts\
    \ recipe files to build <em>MRtrix3</em> containers</p>\n<h2>\n<a id=\"user-content-using-docker\"\
    \ class=\"anchor\" href=\"#using-docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Docker</h2>\n<h4>\n<a id=\"\
    user-content-run-terminal-command\" class=\"anchor\" href=\"#run-terminal-command\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run terminal command</h4>\n<pre><code>docker run --rm -it mrtrix3\
    \ &lt;command&gt;\n</code></pre>\n<p>If not built locally, <code>docker</code>\
    \ will download the latest image from DockerHub.</p>\n<h4>\n<a id=\"user-content-run-gui\"\
    \ class=\"anchor\" href=\"#run-gui\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run GUI</h4>\n<p>These instructions\
    \ are for Linux.</p>\n<pre><code>xhost +local:root\ndocker run --rm -it -v /tmp/.X11-unix:/tmp/.X11-unix\
    \ -e DISPLAY=$DISPLAY mrtrix3 mrview\nxhost -local:root  # Run this when finished.\n\
    </code></pre>\n<h4>\n<a id=\"user-content-locally-build-docker-image\" class=\"\
    anchor\" href=\"#locally-build-docker-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Locally build Docker image</h4>\n\
    <pre><code>docker build --tag mrtrix3 .\n</code></pre>\n<p>Set <code>DOCKER_BUILDKIT=1</code>\
    \ to build parts of the Docker image in parallel, which can speed up build time.\n\
    Use <code>--build-arg MAKE_JOBS=4</code> to build <em>MRtrix3</em> with 4 processors\
    \ (can substitute this with any number of processors &gt; 0); if omitted, <em>MRtrix3</em>\
    \ will be built using a single thread only.</p>\n<h2>\n<a id=\"user-content-using-singularity\"\
    \ class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Singularity</h2>\n<h4>\n\
    <a id=\"user-content-build-container-natively\" class=\"anchor\" href=\"#build-container-natively\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Build container natively</h4>\n<pre><code>singularity build MRtrix3_&lt;version&gt;.sif\
    \ Singularity\n</code></pre>\n<h4>\n<a id=\"user-content-convert-from-docker-container\"\
    \ class=\"anchor\" href=\"#convert-from-docker-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Convert\
    \ from Docker container</h4>\n<pre><code>singularity build MRtrix3_&lt;version&gt;.sif\
    \ docker://mrtrix/mrtrix3:&lt;version&gt;\n</code></pre>\n<h4>\n<a id=\"user-content-run-terminal-command-1\"\
    \ class=\"anchor\" href=\"#run-terminal-command-1\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run terminal\
    \ command</h4>\n<pre><code>MRtrix3_&lt;version&gt;.sif &lt;command&gt;\n</code></pre>\n\
    <h4>\n<a id=\"user-content-run-gui-1\" class=\"anchor\" href=\"#run-gui-1\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run\
    \ GUI</h4>\n<pre><code>singularity exec -B /run MRtrix3_&lt;version&gt;.sif mrview\n\
    </code></pre>\n<hr>\n<h2>\n<a id=\"user-content-developers-update-minified-external-dependencies\"\
    \ class=\"anchor\" href=\"#developers-update-minified-external-dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Developers: Update minified external dependencies</h2>\n<p>This process\
    \ can only be completed by those with write access to the <a href=\"https://osf.io/5rwp3/\"\
    \ rel=\"nofollow\">\"<em>MRtrix3</em> container dependencies\" OSF project</a>.\n\
    These files contain \"minified\" versions of external neuroimaging software package\
    \ dependencies, containing only those components that are utilised by <em>MRtrix3</em>\
    \ scripts.\nThese files should only need to be updated if:</p>\n<ul>\n<li>An <em>MRtrix3</em>\
    \ update introduces a new feature that invokes some new external software tool\
    \ not previously utilised;</li>\n<li>A requisite update occurs in one of these\
    \ external softwares.</li>\n</ul>\n<ol>\n<li>\n<p>Install the <code>docker</code>\
    \ and <code>neurodocker</code> Python packages.</p>\n<pre><code>pip install docker\
    \ neurodocker\n</code></pre>\n</li>\n<li>\n<p>Download the ART ACPCdetect tool\
    \ from NITRC into the working directory.</p>\n<p>This cannot be downloaded directly\
    \ via e.g. <code>wget</code>, as it requires logging in to NITRC; instead, visit\
    \ the following link with a web browser:\n<a href=\"https://www.nitrc.org/frs/download.php/10595/acpcdetect_v2.0_LinuxCentOS6.7.tar.gz\"\
    \ rel=\"nofollow\"><code>https://www.nitrc.org/frs/download.php/10595/acpcdetect_v2.0_LinuxCentOS6.7.tar.gz</code></a></p>\n\
    </li>\n<li>\n<p>Download test data necessary for minification process.</p>\n<pre><code>curl\
    \ -fL -# https://github.com/MRtrix3/script_test_data/archive/master.tar.gz | tar\
    \ xz\n</code></pre>\n</li>\n<li>\n<p>Update file <code>minify.Dockerfile</code>\
    \ to install the desired versions of external software packages.</p>\n</li>\n\
    <li>\n<p>Build Docker image for <code>neurodocker-minify</code>, with complete\
    \ installations of external packages.</p>\n<pre><code>DOCKER_BUILDKIT=1 docker\
    \ build --tag mrtrix3:minify --file minify.Dockerfile --build-arg MAKE_JOBS=4\
    \ .\n</code></pre>\n<p><code>DOCKER_BUILDKIT=1</code> enables BuildKit, which\
    \ builds separate build stages in parallel.\nThis can speed up Docker build times\
    \ in some circumstances.\nIn this case, ANTs and <em>MRtrix3</em> will be compiled\
    \ in parallel, and other downloads will be performed at the same time as well.</p>\n\
    <p>The <code>MAKE_JOBS</code> argument controls how many cores are used for compilation\
    \ of ANTs and <em>MRtrix3</em>.\nIf BuildKit is utilised, do not specify all of\
    \ the available threads; specify half or fewer, so that threads are not unnecessarily\
    \ split across jobs and RAM usage is not excessive.</p>\n</li>\n<li>\n<p>Create\
    \ a minified version of the Docker image.</p>\n<pre><code>docker run --rm -itd\
    \ --name mrtrix3 --security-opt=seccomp:unconfined --volume $(pwd)/script_test_data-master:/mnt\
    \ mrtrix3:minify\nneurodocker-minify --dirs-to-prune /opt --container mrtrix3\
    \ --commands \"bash cmds-to-minify.sh\"\ndocker export mrtrix3 | docker import\
    \ - mrtrix3:minified\ndocker stop mrtrix3\n</code></pre>\n</li>\n<li>\n<p>Generate\
    \ tarballs for each of the utilised dependencies.</p>\n<pre><code>mkdir -p tarballs\n\
    docker run --rm -itd --workdir /opt --name mrtrix3 \\\n    --volume $(pwd)/tarballs:/output\
    \ mrtrix3:minified bash\ndocker exec mrtrix3 bash -c \"tar c art | pigz -9 &gt;\
    \ /output/acpcdetect_&lt;version&gt;.tar.gz\"\ndocker exec mrtrix3 bash -c \"\
    tar c ants | pigz -9 &gt; /output/ants_&lt;version&gt;.tar.gz\"\ndocker exec mrtrix3\
    \ bash -c \"tar c fsl | pigz -9 &gt; /output/fsl_&lt;version&gt;.tar.gz\"\ndocker\
    \ stop mrtrix3\n</code></pre>\n<p>For each tarball, manually replace text \"<code>&lt;version&gt;</code>\"\
    \ with the version number of that particular software that was installed in the\
    \ container.</p>\n</li>\n<li>\n<p>Upload these files to <a href=\"https://osf.io/nfx85/\"\
    \ rel=\"nofollow\">OSF</a>.</p>\n</li>\n</ol>\n<p>File <code>Dockerfile</code>\
    \ can then be modified to download the desired versions of external software packages.\n\
    As OSF file download links do not contain file names, which would otherwise indicate\
    \ the version of each software to be downloaded, please ensure that comments within\
    \ that file are updated to indicate the version of that software within the tarball.</p>\n"
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1612696118.0
Martybird/7TBEaST:
  data_format: 2
  description: Adapt the BEaST skull stripping method for 7T MRI as a BIDS app
  filenames:
  - Singularity.v0.0.1a
  full_name: Martybird/7TBEaST
  latest_release: null
  readme: '<h1>

    <a id="user-content-7tbeast" class="anchor" href="#7tbeast" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>7TBEaST</h1>

    <p>Adapt the BEaST skull stripping method for 7T MRI as a BIDS app</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1530840788.0
MiddelkoopT/RC-2019-Spring:
  data_format: 2
  description: Research Computing Spring 2019 (IMSE 8410)
  filenames:
  - examples/containers/Singularity
  full_name: MiddelkoopT/RC-2019-Spring
  latest_release: null
  readme: '<h1>

    <a id="user-content-nipype-tutorial-notebooks" class="anchor" href="#nipype-tutorial-notebooks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nipype
    Tutorial Notebooks</h1>

    <p><a href="https://circleci.com/gh/miykael/nipype_tutorial/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64"
    alt="CircleCi" data-canonical-src="https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/issues/"><img src="https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/pulls/"><img src="https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub pull-requests" data-canonical-src="https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/"><img
    src="https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/commits/master"><img src="https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub Commits" data-canonical-src="https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/archive/master.zip"><img src="https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub size" data-canonical-src="https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/miykael/nipype_tutorial/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000"
    style="max-width:100%;"></a>

    <a href="http://hits.dwyl.io/miykael/nipype_tutorial" rel="nofollow"><img src="https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub HitCount" data-canonical-src="http://hits.dwyl.io/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a></p>

    <p>This is the Nipype Tutorial in Jupyter Notebook format. You can access the
    tutorial in two ways:</p>

    <ol>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/" rel="nofollow">Nipype Tutorial
    Homepage</a>: This website contains a static, read-only version of all the notebooks.</li>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html"
    rel="nofollow">Nipype Tutorial Docker Image</a>: This guide explains how to use
    Docker to run the notebooks interactively on your own computer. The nipype tutorial
    docker image is the best interactive way to learn Nipype.</li>

    </ol>

    <h1>

    <a id="user-content-feedback-help--support" class="anchor" href="#feedback-help--support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feedback,
    Help &amp; Support</h1>

    <p>If you want to help with this tutorial or have any questions, feel free to
    fork the repo of the <a href="https://github.com/miykael/nipype_tutorial">Notebooks</a>
    or interact with other contributors on the slack channel <a href="https://brainhack.slack.com/messages/nipype/"
    rel="nofollow">brainhack.slack.com/messages/nipype/</a>. If you have any questions
    or found a problem, open a new <a href="https://github.com/miykael/nipype_tutorial/issues">issue
    on github</a>.</p>

    <h1>

    <a id="user-content-thanks-and-acknowledgment" class="anchor" href="#thanks-and-acknowledgment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thanks
    and Acknowledgment</h1>

    <p>A huge thanks to <a href="https://github.com/mwaskom">Michael Waskom</a>, <a
    href="https://github.com/oesteban">Oscar Esteban</a>, <a href="https://github.com/chrisfilo">Chris
    Gorgolewski</a> and <a href="https://github.com/satra">Satrajit Ghosh</a> for
    their input to this tutorial! And a huge thanks to <a href="https://github.com/djarecka/">Dorota
    Jarecka</a> who updated this tutorial to Python 3 and is helping me with keeping
    this tutorial updated and running!</p>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1569468582.0
MiguelJulia/GCC2019_GalaxyAnsibleDeplyoment_CVMFS:
  data_format: 2
  description: Example of deployment of a Galaxy Production Instance using CVMFS with
    Ansible
  filenames:
  - Singularity
  full_name: MiguelJulia/GCC2019_GalaxyAnsibleDeplyoment_CVMFS
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-gcc2019_galaxyansibledeplyoment_cvmfs\" class=\"\
    anchor\" href=\"#gcc2019_galaxyansibledeplyoment_cvmfs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>GCC2019_GalaxyAnsibleDeplyoment_CVMFS</h1>\n\
    <p>Example of deployment of a Galaxy Production Instance using CVMFS with Ansible.\n\
    For more info, look into <a href=\"https://galaxyproject.github.io/training-material/topics/admin/\"\
    \ rel=\"nofollow\">galaxy admin training materials</a></p>\n<h4>\n<a id=\"user-content-deploying-a-galaxy-stance\"\
    \ class=\"anchor\" href=\"#deploying-a-galaxy-stance\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Deploying a galaxy\
    \ stance</h4>\n<pre><code>ansible-playbook -i host cvmfs_playbook.yml\n</code></pre>\n\
    <h4>\n<a id=\"user-content-restart-galaxy\" class=\"anchor\" href=\"#restart-galaxy\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Restart galaxy</h4>\n<pre><code>sudo su - galaxy\nsupervisorctl restart\
    \ galaxy\n</code></pre>\n<h4>\n<a id=\"user-content-variables-to-modify-for-quick-deployment\"\
    \ class=\"anchor\" href=\"#variables-to-modify-for-quick-deployment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Variables\
    \ to modify for quick deployment</h4>\n<p>Admin user name. This user is not created,\
    \ still has to be registered the first time and it will automatically get admin\
    \ permissions:</p>\n<pre><code>galaxy_config:\n  galaxy:\n    admin_users: admin@example.com\n\
    </code></pre>\n<p>Brand: Whatever appears on the banner</p>\n<pre><code>galaxy_config:\n\
    \  galaxy:\n    brand: \"Freiburg GCC\"\n</code></pre>\n<h4>\n<a id=\"user-content-welcomehtml\"\
    \ class=\"anchor\" href=\"#welcomehtml\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>welcome.html</h4>\n<p>Frontpage\
    \ is not created by default. You can find the template inside <code>galaxy_root:\
    \ /srv/galaxy</code>, in <code>server/static/welcome.html.sample</code>. Just\
    \ create a <code>welcome.html</code> page from this template in that same location\
    \ and restart galaxy.</p>\n<h4>\n<a id=\"user-content-deploying-your-ansible-managed-galaxy-into-a-container-not-working-yet\"\
    \ class=\"anchor\" href=\"#deploying-your-ansible-managed-galaxy-into-a-container-not-working-yet\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Deploying your ansible-managed galaxy into a container (not working\
    \ yet!)</h4>\n<p>We will use <a href=\"https://github.com/ansible-community/ansible-bender\"\
    >ansible-bender</a> for this task. Your playbook will have to be adapted to this\
    \ plugging standars as described in their documentation, or compare the differences\
    \ between my cvmfs_playbook.yml and ansible-bender-test.yml to have a quick idea\
    \ of how it has to be done.</p>\n<p>Make sure you are running the right version\
    \ of ansible, as ansible-bender only works with python3. Still, playbooks designed\
    \ for python2 can still be used. You will also need to install <a href=\"https://github.com/containers/buildah/blob/master/install.md\"\
    >buildah</a> and <a href=\"https://github.com/containers/libpod/blob/master/install.md\"\
    >podman</a>.</p>\n<p>Finally, you will need to configurate podman repo config\
    \ file <code>/etc/containers/registries.conf</code> to tell it where to look for\
    \ your containers. For example, to search in dokerhub add <code>'docker.io'</code>\
    \ inside</p>\n<pre><code>[registries.search]\nregistries = ['docker.io']\n</code></pre>\n\
    <p>The image is required to have python interpreter build in.</p>\n<h4>\n<a id=\"\
    user-content-building-galaxy-container-with-docker-idea---not-testet-yet\" class=\"\
    anchor\" href=\"#building-galaxy-container-with-docker-idea---not-testet-yet\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building galaxy container with Docker (idea - not testet yet)</h4>\n\
    <p>Use galaxy-container <a href=\"https://github.com/bgruening/docker-galaxy-stable/blob/master/galaxy/Dockerfile\"\
    >Dockerfile</a> as template.</p>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1562583598.0
MontrealSergiy/deformation:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: MontrealSergiy/deformation
  latest_release: null
  readme: '<h1>

    <a id="user-content-deformation-field" class="anchor" href="#deformation-field"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deformation
    field</h1>

    <p>This PERL script is a wrapper that is calling sequence of commands for generating
    deformation fields scrips

    <a href="https://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields"
    rel="nofollow">https://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields</a>

    Source code for deformation pipeline and dependencies (MINC):

    <a href="https://github.com/Mouse-Imaging-Centre/generate_deformation_fields">https://github.com/Mouse-Imaging-Centre/generate_deformation_fields</a></p>

    <p>Usage</p>

    <p>deformation_2.pl -input ICBM_00100_t1_final.mnc &lt;&lt;this could be any anatomical
    minc file, for a collection of minc files&gt;&gt; -output dummy_hoho -deformation_ratio
    0.6 -coordinate 70 100 70 10 10 10 -tolerance_space 4 &lt;&gt; -blur_determinant
    0.25 &lt;&gt; -error 0.00001 &lt;&gt; -iteration 100</p>

    <p>The output of running this command looks like this:

    ICBM_00100_t1_final_deformed_by_0.4atROIx70-y100-z70dimx10.dimy10.dimz10.mnc.
    </p>

    <p>We will also have a directory dummy_hoho/TMP that will contain the in-between-files.</p>

    <p>$:/dummy_hoho/TMP$ ls</p>

    <p>block.mnc</p>

    <p>blurred0.25determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc</p>

    <p>DDDDdilated.mnc</p>

    <p>DDDDring.mnc</p>

    <p>determinant_r_0.4_grid.mnc</p>

    <p>determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc</p>

    <p>determinant_r_0.4.xfm</p>

    <p>mask.mnc</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623632255.0
Mykrobe-tools/mykrobe:
  data_format: 2
  description: Antibiotic resistance prediction in minutes
  filenames:
  - Singularity.def
  full_name: Mykrobe-tools/mykrobe
  latest_release: v0.9.0
  readme: "<p><a href=\"https://travis-ci.com/Mykrobe-tools/mykrobe\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/da2ddf6fcf06b53f15b9d00f38e707b0d71ba8fb2a5ffefb316ef464440308d0/68747470733a2f2f7472617669732d63692e636f6d2f4d796b726f62652d746f6f6c732f6d796b726f62652e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Travis build Status\" data-canonical-src=\"https://travis-ci.com/Mykrobe-tools/mykrobe.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-mykrobe\" class=\"\
    anchor\" href=\"#mykrobe\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Mykrobe</h1>\n<p><a href=\"http://www.mykrobe.com\"\
    \ rel=\"nofollow\">http://www.mykrobe.com</a></p>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Please\
    \ see the <a href=\"https://github.com/Mykrobe-tools/mykrobe/wiki\">mykrobe wiki</a>\
    \ for documentation.</p>\n<h2>\n<a id=\"user-content-quick-start\" class=\"anchor\"\
    \ href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Quick start</h2>\n<p><strong>Install</strong>:</p>\n\
    <ul>\n<li>bioconda - <code>conda install -c bioconda mykrobe</code>\n</li>\n<li>from\
    \ source - <code>pip3 install . &amp;&amp; mykrobe panels update_metadata &amp;&amp;\
    \ mykrobe panels update_species all</code>\n</li>\n<li>or using singularity or\
    \ docker (see wiki for details)</li>\n</ul>\n<p><strong>Run</strong> on Mtb, making\
    \ a JSON file of results:</p>\n<pre><code>mykrobe predict --sample my_sample_name\
    \ \\\n  --species tb \\\n  --output out.json \\\n  --format json \\\n  --seq reads.fq.gz\n\
    </code></pre>\n<p>Test reads can be obtained by running:</p>\n<pre><code>wget\
    \ -O reads.fq.gz https://ndownloader.figshare.com/files/21059229\n</code></pre>\n"
  stargazers_count: 44
  subscribers_count: 6
  topics: []
  updated_at: 1624532770.0
NIH-HPC/singularity-def-files:
  data_format: 2
  description: definition files and wrapper scripts used by NIH HPC staff to install
    user-facing apps on the Biowulf cluster
  filenames:
  - systems-biology/cellphonedb/2.1.7/cellphonedb.def
  - systems-biology/cellphonedb/2.1.2/cellphonedb.def
  - utilities/datalad/0.13.0rc2/datalad.def
  - utilities/vcf2db/2020.09.14/vcf2db.def
  - utilities/atom/1.13.1/atom.def
  - utilities/pdf2svg/0.2.3/pdf2svg.def
  - utilities/visidata/2.2/visidata.def
  - utilities/gdc-client/1.5.0/gdc-client.def
  - utilities/whatshap/0.18/whatshap.def
  - utilities/snp-sites/2.4.1/snp-sites.def
  - utilities/sysbench/1.0.20/sysbench.def
  - utilities/sysbench/1.0.11/sysbench.def
  - utilities/longshot/0.3.5/longshot.def
  - utilities/ariba/2.14.4/ariba.def
  - utilities/uropa/3.5.0/uropa.def
  - utilities/pyega3/3.3.0/pyega3.def
  - utilities/xvfb/1.19.6/xvfb.def
  - mass-spectrometry/maxquant/1.6.7.0/maxquant.def
  - mass-spectrometry/maxquant/1.6.17.0/maxquant.def
  - mass-spectrometry/maxquant/1.6.3.3/maxquant.def
  - linkage-phylogenetics/bali-phy/3.5/bali-phy.def
  - linkage-phylogenetics/gubbins/2.3.4/gubbins.def
  - structural-biology/pymol/2.4.0/pymol.def
  - structural-biology/pymol/2.3.0/pymol_2.3.0.def
  - structural-biology/parsnip/20180507/parsnip.def
  - structural-biology/rdock/2013.1/rdock.def
  - computational-chemistry/ampl/f35623d4/ampl.def
  - high-throughput-sequencing/maggie/0.3.4/maggie.def
  - high-throughput-sequencing/surpi/1.0.67/surpi.def
  - high-throughput-sequencing/salmon/1.4.0/salmon.def
  - high-throughput-sequencing/xengsort/28762aac/xengsort.def
  - high-throughput-sequencing/rmats/4.0.2/rmats.def
  - high-throughput-sequencing/medaka/1.0.3/medaka.def
  - high-throughput-sequencing/medaka/1.2.0/medaka.def
  - high-throughput-sequencing/medaka/0.12.1/medaka.def
  - high-throughput-sequencing/sicer/2-1.0.2/sicer.def
  - high-throughput-sequencing/hicexplorer/3.5.1/hicexplorer.def
  - high-throughput-sequencing/raremetal/4.15.1/raremetal.def
  - high-throughput-sequencing/canvas/1.40/canvas.def
  - high-throughput-sequencing/bamliquidator/1.3.8/bamliquidator.def
  - high-throughput-sequencing/taiji/1.2.0/taiji.def
  - high-throughput-sequencing/taiji/1.1.0/taiji.def
  - high-throughput-sequencing/flye/2.7/flye.def
  - high-throughput-sequencing/flye/2.8-1/flye.def
  - high-throughput-sequencing/mitosuite/1.0.9b/mitosuite.def
  - high-throughput-sequencing/svtyper/0.7.1/svtyper.def
  - high-throughput-sequencing/atac_dnase_pipelines/0.3.4-19-gcbd2a00/atac_dnase_pipelines.def
  - high-throughput-sequencing/abruijn/1.0/abruijn.def
  - high-throughput-sequencing/pcap-core/4.3.5/pcap-core.def
  - high-throughput-sequencing/brass/6.1.2/brass.def
  - high-throughput-sequencing/brass/6.3.4/brass.def
  - high-throughput-sequencing/seqlinkage/1.0/seqlinkage.def
  - high-throughput-sequencing/sve/0.1.0/sve.def
  - high-throughput-sequencing/flappie/2.1.3/flappie.def
  - high-throughput-sequencing/flappie/1.0.0/flappie.def
  - high-throughput-sequencing/humann/3.0.0-alpha.3/humann.def
  - high-throughput-sequencing/biom-format/2.1.10/biom-format.def
  - high-throughput-sequencing/svtk/0.1/svtk.def
  - high-throughput-sequencing/mtoolbox/1.1/mtoolbox.def
  - high-throughput-sequencing/cnvnator/0.4.1/cnvnator.def
  - high-throughput-sequencing/metabat/2.13/metabat.def
  - high-throughput-sequencing/multiqc/1.10/multiqc.def
  - high-throughput-sequencing/multiqc/1.9/multiqc.def
  - high-throughput-sequencing/idep/0.81/idep.def
  - high-throughput-sequencing/ricopili/2019_Jun_25.001/ricopili.def
  - high-throughput-sequencing/atropos/1.1.18/atropos.def
  - high-throughput-sequencing/stream/20180816/stream.def
  - high-throughput-sequencing/tetoolkit/2.1.4/tetoolkit.def
  - high-throughput-sequencing/tetoolkit/2.2.1/tetoolkit.def
  - high-throughput-sequencing/vagrent/3.3.4/vagrent.def
  - high-throughput-sequencing/deeptools/3.4.2/deeptools.def
  - high-throughput-sequencing/deeptools/3.5.0/deeptools.def
  - high-throughput-sequencing/rsd/1.1.7/rsd.def
  - high-throughput-sequencing/guppy/3.4.5/guppy.def
  - high-throughput-sequencing/guppy/4.2.2/guppy.def
  - high-throughput-sequencing/guppy/4.0.15/guppy.def
  - high-throughput-sequencing/transvar/2.5.9/transvar.def
  - high-throughput-sequencing/parliament/0.1.7/parliament.def
  - high-throughput-sequencing/pepr/1.1.24/pepr.def
  - high-throughput-sequencing/csvkit/1.0.5/csvkit.def
  - high-throughput-sequencing/delly/0.8.7/delly.def
  - high-throughput-sequencing/mageck-vispr/0.5.4/mageck-vispr.def
  - high-throughput-sequencing/megalodon/2.2.9/megalodon.def
  - high-throughput-sequencing/cicero/0.3.0/cicero.def
  - high-throughput-sequencing/repeatmodeler/2.0.1/repeatmodeler.def
  - high-throughput-sequencing/cnvkit/0.9.6/cnvkit.def
  - high-throughput-sequencing/cnvkit/0.9.8/cnvkit.def
  - high-throughput-sequencing/rnapeg/current/rnapeg.def
  - high-throughput-sequencing/deepsignal/0.1.8/deepsignal.def
  - high-throughput-sequencing/bigscale2/20191119/bigscale2.def
  - high-throughput-sequencing/tandemtools/current/tandemtools.def
  - high-throughput-sequencing/hap.py/0.3.9/hap.py.def
  - high-throughput-sequencing/epic2/0.0.41/epic2.def
  - high-throughput-sequencing/neusomatic/0.2.1/neusomatic.def
  - high-throughput-sequencing/fusioninspector/2.3.0/fusioninspector.def
  - high-throughput-sequencing/fusioninspector/2.5.0/fusioninspector.def
  - high-throughput-sequencing/bison/0.4.0/bison.def
  - high-throughput-sequencing/pychopper/2.4.0/pychopper.def
  - high-throughput-sequencing/crossmap/0.5.2/crossmap.def
  - high-throughput-sequencing/htseq/0.11.4/htseq.def
  - high-throughput-sequencing/lefse/1.0.7/lefse.def
  - high-throughput-sequencing/lefse/1.0.8/lefse.def
  - high-throughput-sequencing/rilseq/0.75/rilseq.def
  - high-throughput-sequencing/umitools/1.1.1/umitools.def
  - high-throughput-sequencing/freebayes/1.3.5/freebayes.def
  - high-throughput-sequencing/slamdunk/0.4.3/slamdunk.def
  - high-throughput-sequencing/bamutil/1.0.15/bamutil.def
  - high-throughput-sequencing/cancerit-wgs/2.1.0/cancerit-wgs.def
  - high-throughput-sequencing/vireosnp/0.5.1/vireosnp.def
  - high-throughput-sequencing/vireosnp/0.3.2/vireosnp.def
  - high-throughput-sequencing/humann2/2.8.1/humann2.def
  - high-throughput-sequencing/macs/2.2.7.1/macs.def
  - high-throughput-sequencing/pvactools/1.5.5/pvactools.def
  - high-throughput-sequencing/pvactools/2.0.1/pvactools.def
  - high-throughput-sequencing/ascatngs/4.3.3/ascatngs.def
  - high-throughput-sequencing/ascatngs/4.3.4/ascatngs.def
  - high-throughput-sequencing/ascatngs/4.5.0/ascatngs.def
  - high-throughput-sequencing/tvc/5.10.1/tvc.def
  - high-throughput-sequencing/cgpbattenberg/3.5.3/cgpbattenberg.def
  - high-throughput-sequencing/eager/1.92/eager.def
  - high-throughput-sequencing/vep/103/vep.def
  - high-throughput-sequencing/vep/101/vep.def
  - high-throughput-sequencing/vep/97/vep.def
  - high-throughput-sequencing/bamsurgeon/1111e5d/bamsurgeon.def
  - high-throughput-sequencing/busco/4.1.3/busco.def
  - high-throughput-sequencing/busco/5.0.0/busco.def
  - high-throughput-sequencing/bamreadcount/cram-v0.0.1/bamreadcount.def
  - high-throughput-sequencing/scramble/1.0.1-32893ef/scramble.def
  - high-throughput-sequencing/scramble/0.0.20190211.82c78b9/scramble.def
  - high-throughput-sequencing/dropest/0.8.6/dropest.def
  - high-throughput-sequencing/rseqc/4.0.0/rseqc.def
  - high-throughput-sequencing/cellsnp/0.1.7/cellsnp.def
  - high-throughput-sequencing/cellsnp/0.3.2/cellsnp.def
  - high-throughput-sequencing/cutadapt/2.10/cutadapt.def
  - high-throughput-sequencing/cutadapt/3.0/cutadapt.def
  - high-throughput-sequencing/cutadapt/1.18/cutadapt.def
  - high-throughput-sequencing/gridss/2.9.4/gridss.def
  - high-throughput-sequencing/tpmcalculator/0.0.3/tpmcalculator.def
  - high-throughput-sequencing/tpmcalculator/0.0.4/tpmcalculator.def
  - high-throughput-sequencing/hicpro/2.11.4/hicpro.def
  - high-throughput-sequencing/deepvariant/1.1.0/deepvariant.def
  - high-throughput-sequencing/deepvariant/0.9.0/deepvariant.def
  - high-throughput-sequencing/deepvariant/0.10.0/deepvariant.def
  - high-throughput-sequencing/metaphlan/3.0/metaphlan.def
  - high-throughput-sequencing/metaphlan/3.0.6/metaphlan.def
  - high-throughput-sequencing/hail/0.2.3/hail.def
  - high-throughput-sequencing/hail/0.2.61/hail.def
  - high-throughput-sequencing/hail/0.2.56/hail.def
  - high-throughput-sequencing/crispresso/2.0.45/crispresso.def
  - high-throughput-sequencing/crispresso/2.0.40/crispresso.def
  - high-throughput-sequencing/gossamer/ac492a8/gossamer.def
  - sequence-analysis/orffinder/0.4.3-sing-install/orffinder.def
  - sequence-analysis/eukrep/20180308/eukrep.def
  - sequence-analysis/glu/1.0b3/glu.def
  - sequence-analysis/augustus/3.3.3/augustus.def
  - sequence-analysis/chipseq_pipeline/1.2.0/chipseq_pipeline.def
  - sequence-analysis/htgtsrep/9fe74ff/htgtsrep.def
  - sequence-analysis/arriba/2.0.0/arriba.def
  - sequence-analysis/arriba/1.2.0/arriba.def
  - sequence-analysis/braker/2/braker.def
  - sequence-analysis/saige/0.44.1/saige.def
  - sequence-analysis/acfs/20180316/acfs.def
  - sequence-analysis/cactus/1.2.3/cactus.def
  - sequence-analysis/roary/3.12.0/roary.def
  - sequence-analysis/roary/3.13.0/roary.def
  - sequence-analysis/annogesic/1.0.2/annogesic.def
  - sequence-analysis/svtools/0.5.1/svtools.def
  - sequence-analysis/sonicparanoid/1.3.5/sonicparanoid.def
  - sequence-analysis/sonicparanoid/1.3.2/sonicparanoid.def
  - sequence-analysis/vcf-kit/0.1.6/vcf-kit.def
  - sequence-analysis/qtltools/1.3.1/qtltools.def
  - sequence-analysis/wisexome/20180814/wisexome.def
  - sequence-analysis/focus/0.6.10/focus.def
  - sequence-analysis/smoove/0.2.5/smoove.def
  - sequence-analysis/smoove/0.2.1/smoove.def
  - sequence-analysis/cicero/1.4.0/cicero.def
  - sequence-analysis/m-tools/20210208/m-tools.def
  - sequence-analysis/ldsc/3d0c4464/ldsc.def
  - sequence-analysis/prokka/1.13/prokka.def
  - sequence-analysis/prokka/1.14.6/prokka.def
  - sequence-analysis/deepsea/0.94c/deepsea.def
  - sequence-analysis/anvio/7/anvio.def
  - sequence-analysis/mmarge/1.0/mmarge.def
  - sequence-analysis/xhla/2018-04-04/xhla.def
  - sequence-analysis/intarna/3.2.0/intarna.def
  - sequence-analysis/phaser/1.1.1/phaser.def
  - sequence-analysis/accurity/20180724/accurity.def
  - sequence-analysis/accurity/20210209/accurity.def
  - sequence-analysis/asgal/1.0/asgal.def
  - sequence-analysis/glnexus/1.1.11/glnexus.def
  - sequence-analysis/glnexus/1.2.7/glnexus.def
  - sequence-analysis/bamgineer/2-20200624/bamgineer.def
  - sequence-analysis/netoglyc/3.1d/netoglyc.def
  - mathematical-statistics/omeclust/1.1.6/omeclust.def
  - mathematical-statistics/omeclust/1.1.4/omeclust.def
  - mathematical-statistics/m2clust/1.1.3/m2clust.def
  - mathematical-statistics/m2clust/0.0.7/m2clust.def
  - mathematical-statistics/m2clust/0.0.8/m2clust.def
  - molecular-modeling-graphics/starseqr/0.6.7/starseqr.def
  - molecular-modeling-graphics/chimerax/1.1/chimerax.def
  - molecular-modeling-graphics/chimerax/0.93/chimerax.def
  - molecular-modeling-graphics/blender/2.82/blender.def
  - image-analysis/resmap/1.95/resmap.def
  - image-analysis/terastitcher/1.11.10/terastitcher.def
  - image-analysis/terastitcher/1.10.8/terastitcher.def
  - image-analysis/minc-toolkit/1.9.18/minc-toolkit.def
  - image-analysis/minc-toolkit/1.9.16/minc-toolkit.def
  - image-analysis/civet/2.1.1/civet.def
  - image-analysis/xcpengine/1.2.1/xcpengine.def
  - image-analysis/xcpengine/1.0/xcpengine.def
  - image-analysis/xcpengine/1.2.3/xcpengine.def
  - image-analysis/baracus/1.1.4/baracus.def
  - image-analysis/qsiprep/0.8.0/qsiprep.def
  - image-analysis/deepmedic/0.8.0/deepmedic.def
  - image-analysis/deepmedic/0.8.2/deepmedic.def
  - image-analysis/broccoli/1.0.1/broccoli.def
  - image-analysis/tesseract/4.1.1/tesseract.def
  - image-analysis/topaz/0.2.5/topaz.def
  - image-analysis/fitlins/0.8.0/fitlins.def
  - image-analysis/fitlins/0.7.0/fitlins.def
  - image-analysis/mriqc/0.15.2/mriqc.def
  - image-analysis/mriqc/0.15.1/mriqc.def
  - image-analysis/mriqc/0.16.1/mriqc.def
  - image-analysis/mriqc/0.15.2-0be03bf/mriqc.def
  - image-analysis/fmriprep/20.2.0/fmriprep.def
  - image-analysis/fmriprep/20.2.1/fmriprep.def
  - image-analysis/fmriprep/20.0.5/fmriprep.def
  - image-analysis/fmriprep/20.1.1/fmriprep.def
  - image-analysis/fmriprep/20.1.3/fmriprep.def
  - image-analysis/mrtrix/3.0.1/mrtrix.def
  - image-analysis/mrtrix/3.0.2-cuda9.1/mrtrix.def
  - image-analysis/mrtrix/3.0.0/mrtrix.def
  - deep-learning/dextr-pytorch/20180710/dextr-pytorch.def
  - deep-learning/digits/6.0/digits.def
  - deep-learning/unet/20180704/unet.def
  - deep-learning/deeplab/20180816/deeplab.def
  - deep-learning/few-shot-ssl/20180723/few-shot-ssl.def
  - deep-learning/caffe2/0.8.1/caffe2.def
  - deep-learning/clairvoyante/1.0/clairvoyante.def
  - deep-learning/basset/0.1.0/basset.def
  - deep-learning/tensorrt/18.09/tensorrt.def
  - deep-learning/polyrnnpp/20180718/polyrnnpp.def
  full_name: NIH-HPC/singularity-def-files
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-nih-hpc-singularity-definition-files\" class=\"\
    anchor\" href=\"#nih-hpc-singularity-definition-files\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>NIH HPC Singularity\
    \ Definition Files</h1>\n<p>These definition files and wrapper scripts are used\
    \ by the <a href=\"https://hpc.nih.gov/\" rel=\"nofollow\">NIH HPC (Biowulf)</a>\
    \ staff to install containerized applications using <a href=\"https://github.com/sylabs/singularity\"\
    >Singularity</a>. Each app is installed in a self-contained directory and access\
    \ to the app is controlled through a module system (<a href=\"https://github.com/TACC/Lmod\"\
    >Lmod</a>). This strategy allows users to transparently access apps that are installed\
    \ within containers as though they were installed directly on the host system.\
    \ More details can be found <a href=\"https://hpc.nih.gov/apps/singularity.html#bind-stationary\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>Typically, apps are installed under in a\
    \ directory structure like so:</p>\n<pre><code>$ tree appname/ver\nappname/ver\n\
    |-- bin\n|   |-- cmd1 -&gt; ../libexec/wrapper.sh\n|   |-- cmd2 -&gt; ../libexec/wrapper.sh\n\
    |   `-- cmd3 -&gt; ../libexec/wrapper.sh\n`-- libexec\n    |-- app.sif\n    `--\
    \ wrapper.sh\n</code></pre>\n<p>Because <code>wrapper.sh</code> is written to\
    \ be introspective, any command symlinked to it will be carried through and executed\
    \ within the associated container. The wrapper script is also sufficiently generic\
    \ that it can be reused across apps with little or no modification.</p>\n<p>Each\
    \ app has its own <code>README.md</code> that contains:</p>\n<ul>\n<li>a link\
    \ to the NIH HPC app page or developer's documentation</li>\n<li>a list of symlinks\
    \ that should be created to the wrapper script to expose executables within the\
    \ container</li>\n<li>any app specific installation notes</li>\n</ul>\n<p>Finally,\
    \ please note that these definition files <strong>are not guaranteed to reproduce\
    \ the same container, or even to produce any container at all</strong>. The internet,\
    \ upon which these definition files are based, is subject to change without notice.\
    \ These definition files are therefore intended to be treated as (potentially)\
    \ helpful suggestions.</p>\n<h2>\n<a id=\"user-content-computational-chemistry\"\
    \ class=\"anchor\" href=\"#computational-chemistry\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"/computational-chemistry\"\
    >Computational Chemistry</a>\n</h2>\n<ul>\n<li><a href=\"/computational-chemistry/ampl\"\
    >ampl</a></li>\n</ul>\n<h2>\n<a id=\"user-content-deep-learning\" class=\"anchor\"\
    \ href=\"#deep-learning\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"/deep-learning\">Deep Learning</a>\n\
    </h2>\n<ul>\n<li><a href=\"/deep-learning/caffe2\">Caffe2</a></li>\n<li><a href=\"\
    /deep-learning/dextr-pytorch\">DEXTR-PyTorch</a></li>\n<li><a href=\"/deep-learning/polyrnnpp\"\
    >PolyRNNpp</a></li>\n<li><a href=\"/deep-learning/basset\">basset</a></li>\n<li><a\
    \ href=\"/deep-learning/clairvoyante\">clairvoyante</a></li>\n<li><a href=\"/deep-learning/deeplab\"\
    >deeplab</a></li>\n<li><a href=\"/deep-learning/digits\">digits</a></li>\n<li><a\
    \ href=\"/deep-learning/few-shot-ssl\">few-shot-ssl</a></li>\n<li><a href=\"/deep-learning/tensorrt\"\
    >tensorrt</a></li>\n<li><a href=\"/deep-learning/unet\">unet</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-high-throughput-sequencing\" class=\"anchor\" href=\"\
    #high-throughput-sequencing\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><a href=\"/high-throughput-sequencing\"\
    >High Throughput Sequencing</a>\n</h2>\n<ul>\n<li><a href=\"/high-throughput-sequencing/atac_dnase_pipelines\"\
    >ATAC-Seq / DNase-Seq Pipeline</a></li>\n<li><a href=\"/high-throughput-sequencing/ascatngs\"\
    >AscatNGS</a></li>\n<li><a href=\"/high-throughput-sequencing/atropos\">Atropos</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/bamsurgeon\">BAMSurgeon</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/brass\">BRASS</a></li>\n<li><a href=\"/high-throughput-sequencing/canvas\"\
    >Canvas</a></li>\n<li><a href=\"/high-throughput-sequencing/maggie\">MAGGIE</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/pcap-core\">PCAP-core</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/pepr\">PePr</a></li>\n<li><a href=\"/high-throughput-sequencing/rsd\"\
    >RSD</a></li>\n<li><a href=\"/high-throughput-sequencing/surpi\">SURPI</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/tpmcalculator\">TPMCalculator</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/tvc\">TVC</a></li>\n<li><a href=\"/high-throughput-sequencing/vagrent\"\
    >VAGrENT</a></li>\n<li><a href=\"/high-throughput-sequencing/vep\">VEP</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/abruijn\">abruijn</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamliquidator\">bamliquidator</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamreadcount\">bamreadcount</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamutil\">bamutil</a></li>\n<li><a href=\"/high-throughput-sequencing/bigscale2\"\
    >bigscale2</a></li>\n<li><a href=\"/high-throughput-sequencing/biom-format\">biom-format</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/bison\">bison</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/busco\">busco</a></li>\n<li><a href=\"/high-throughput-sequencing/cancerit-wgs\"\
    >cancerit-wgs</a></li>\n<li><a href=\"/high-throughput-sequencing/cellsnp\">cellsnp</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/cgpbattenberg\">cgpBattenberg</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/cicero\">cicero</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/cnvkit\">cnvkit</a></li>\n<li><a href=\"/high-throughput-sequencing/cnvnator\"\
    >cnvnator</a></li>\n<li><a href=\"/high-throughput-sequencing/crispresso\">crispresso</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/crossmap\">crossmap</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/csvkit\">csvkit</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/cutadapt\">cutadapt</a></li>\n<li><a href=\"/high-throughput-sequencing/deepsignal\"\
    >deepsignal</a></li>\n<li><a href=\"/high-throughput-sequencing/deeptools\">deeptools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/deepvariant\">deepvariant</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/delly\">delly</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/dropest\">dropest</a></li>\n<li><a href=\"/high-throughput-sequencing/eager\"\
    >eager</a></li>\n<li><a href=\"/high-throughput-sequencing/epic2\">epic2</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/flappie\">flappie</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/flye\">flye</a></li>\n<li><a href=\"/high-throughput-sequencing/freebayes\"\
    >freebayes</a></li>\n<li><a href=\"/high-throughput-sequencing/fusioninspector\"\
    >fusioninspector</a></li>\n<li><a href=\"/high-throughput-sequencing/gossamer\"\
    >gossamer</a></li>\n<li><a href=\"/high-throughput-sequencing/gridss\">gridss</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/guppy\">guppy</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/hail\">hail</a></li>\n<li><a href=\"/high-throughput-sequencing/hap.py\"\
    >hap.py</a></li>\n<li><a href=\"/high-throughput-sequencing/hicexplorer\">hicexplorer</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/hicpro\">hicpro</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/htseq\">htseq</a></li>\n<li><a href=\"/high-throughput-sequencing/humann2\"\
    >humann2</a></li>\n<li><a href=\"/high-throughput-sequencing/idep\">idep</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/lefse\">lefse</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/macs\">macs</a></li>\n<li><a href=\"/high-throughput-sequencing/mageck-vispr\"\
    >mageck-vispr</a></li>\n<li><a href=\"/high-throughput-sequencing/medaka\">medaka</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/megalodon\">megalodon</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/metabat\">metabat</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/metaphlan\">metaphlan</a></li>\n<li><a href=\"/high-throughput-sequencing/mitosuite\"\
    >mitosuite</a></li>\n<li><a href=\"/high-throughput-sequencing/mtoolbox\">mtoolbox</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/multiqc\">multiqc</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/neusomatic\">neusomatic</a></li>\n<li><a href=\"/high-throughput-sequencing/parliament\"\
    >parliament</a></li>\n<li><a href=\"/high-throughput-sequencing/pvactools\">pvactools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/pychopper\">pychopper</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/raremetal\">raremetal</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/repeatmodeler\">repeatmodeler</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/ricopili\">ricopili</a></li>\n<li><a href=\"/high-throughput-sequencing/rilseq\"\
    >rilseq</a></li>\n<li><a href=\"/high-throughput-sequencing/rmats\">rmats</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/rnapeg\">rnapeg</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/rseqc\">rseqc</a></li>\n<li><a href=\"/high-throughput-sequencing/salmon\"\
    >salmon</a></li>\n<li><a href=\"/high-throughput-sequencing/scramble\">scramble</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/seqlinkage\">seqlinkage</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/sicer\">sicer</a></li>\n<li><a href=\"/high-throughput-sequencing/slamdunk\"\
    >slamdunk</a></li>\n<li><a href=\"/high-throughput-sequencing/stream\">stream</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/sve\">sve</a></li>\n<li><a href=\"/high-throughput-sequencing/svtk\"\
    >svtk</a></li>\n<li><a href=\"/high-throughput-sequencing/svtyper\">svtyper</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/taiji\">taiji</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/tandemtools\">tandemtools</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/tetoolkit\">tetoolkit</a></li>\n<li><a href=\"/high-throughput-sequencing/transvar\"\
    >transvar</a></li>\n<li><a href=\"/high-throughput-sequencing/umitools\">umitools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/vireosnp\">vireosnp</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/xengsort\">xengsort</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-image-analysis\" class=\"anchor\" href=\"#image-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/image-analysis\">Image Analysis</a>\n</h2>\n<ul>\n<li><a\
    \ href=\"/image-analysis/resmap\">ResMap</a></li>\n<li><a href=\"/image-analysis/terastitcher\"\
    >TeraStitcher</a></li>\n<li><a href=\"/image-analysis/baracus\">baracus</a></li>\n\
    <li><a href=\"/image-analysis/broccoli\">broccoli</a></li>\n<li><a href=\"/image-analysis/civet\"\
    >civet</a></li>\n<li><a href=\"/image-analysis/ctf\">ctf</a></li>\n<li><a href=\"\
    /image-analysis/deepmedic\">deepmedic</a></li>\n<li><a href=\"/image-analysis/fitlins\"\
    >fitlins</a></li>\n<li><a href=\"/image-analysis/fmriprep\">fmriprep</a></li>\n\
    <li><a href=\"/image-analysis/minc-toolkit\">minc-toolkit</a></li>\n<li><a href=\"\
    /image-analysis/mriqc\">mriqc</a></li>\n<li><a href=\"/image-analysis/mrtrix\"\
    >mrtrix</a></li>\n<li><a href=\"/image-analysis/qsiprep\">qsiprep</a></li>\n<li><a\
    \ href=\"/image-analysis/tesseract\">tesseract</a></li>\n<li><a href=\"/image-analysis/topaz\"\
    >topaz</a></li>\n<li><a href=\"/image-analysis/xcpengine\">xcpengine</a></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-linkage-phylogenetics\" class=\"anchor\" href=\"\
    #linkage-phylogenetics\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"/linkage-phylogenetics\">Linkage Phylogenetics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/linkage-phylogenetics/bali-phy\">bali-phy</a></li>\n\
    <li><a href=\"/linkage-phylogenetics/gubbins\">gubbins</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-mass-spectrometry\" class=\"anchor\" href=\"#mass-spectrometry\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/mass-spectrometry\">Mass Spectrometry</a>\n</h2>\n<ul>\n\
    <li><a href=\"/mass-spectrometry/maxquant\">maxquant</a></li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-mathematicalstatistics\" class=\"anchor\" href=\"#mathematicalstatistics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/mathematical-statistics\">Mathematical/Statistics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/mathematical-statistics/m2clust\">m2clust</a></li>\n\
    <li><a href=\"/mathematical-statistics/omeclust\">omeClust</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-molecular-modeling-graphics\" class=\"anchor\" href=\"#molecular-modeling-graphics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/molecular-modeling-graphics\">Molecular Modeling Graphics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/molecular-modeling-graphics/chimerax\">ChimeraX</a></li>\n\
    <li><a href=\"/molecular-modeling-graphics/blender\">blender</a></li>\n<li><a\
    \ href=\"/molecular-modeling-graphics/starseqr\">starseqr</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-sequence-analysis\" class=\"anchor\" href=\"#sequence-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/sequence-analysis\">Sequence Analysis</a>\n</h2>\n<ul>\n\
    <li><a href=\"/sequence-analysis/acfs\">ACFS</a></li>\n<li><a href=\"/sequence-analysis/annogesic\"\
    >ANNOgesic</a></li>\n<li><a href=\"/sequence-analysis/asgal\">ASGAL</a></li>\n\
    <li><a href=\"/sequence-analysis/accurity\">Accurity</a></li>\n<li><a href=\"\
    /sequence-analysis/eukrep\">EukRep</a></li>\n<li><a href=\"/sequence-analysis/glu\"\
    >GLU</a></li>\n<li><a href=\"/sequence-analysis/htgtsrep\">HTGTSrep</a></li>\n\
    <li><a href=\"/sequence-analysis/orffinder\">ORFfinder</a></li>\n<li><a href=\"\
    /sequence-analysis/saige\">SAIGE</a></li>\n<li><a href=\"/sequence-analysis/vcf-kit\"\
    >VCF-kit</a></li>\n<li><a href=\"/sequence-analysis/anvio\">anvio</a></li>\n<li><a\
    \ href=\"/sequence-analysis/arriba\">arriba</a></li>\n<li><a href=\"/sequence-analysis/augustus\"\
    >augustus</a></li>\n<li><a href=\"/sequence-analysis/bamgineer\">bamgineer</a></li>\n\
    <li><a href=\"/sequence-analysis/braker\">braker</a></li>\n<li><a href=\"/sequence-analysis/cactus\"\
    >cactus</a></li>\n<li><a href=\"/sequence-analysis/chipseq_pipeline\">chipseq_pipeline</a></li>\n\
    <li><a href=\"/sequence-analysis/cicero\">cicero</a></li>\n<li><a href=\"/sequence-analysis/deepsea\"\
    >deepsea</a></li>\n<li><a href=\"/sequence-analysis/focus\">focus</a></li>\n<li><a\
    \ href=\"/sequence-analysis/glnexus\">glnexus</a></li>\n<li><a href=\"/sequence-analysis/intarna\"\
    >intarna</a></li>\n<li><a href=\"/sequence-analysis/ldsc\">ldsc</a></li>\n<li><a\
    \ href=\"/sequence-analysis/m-tools\">m-tools</a></li>\n<li><a href=\"/sequence-analysis/mmarge\"\
    >mmarge</a></li>\n<li><a href=\"/sequence-analysis/netoglyc\">netOglyc</a></li>\n\
    <li><a href=\"/sequence-analysis/phaser\">phaser</a></li>\n<li><a href=\"/sequence-analysis/prokka\"\
    >prokka</a></li>\n<li><a href=\"/sequence-analysis/qtltools\">qtltools</a></li>\n\
    <li><a href=\"/sequence-analysis/roary\">roary</a></li>\n<li><a href=\"/sequence-analysis/smoove\"\
    >smoove</a></li>\n<li><a href=\"/sequence-analysis/sonicparanoid\">sonicparanoid</a></li>\n\
    <li><a href=\"/sequence-analysis/svtools\">svtools</a></li>\n<li><a href=\"/sequence-analysis/wisexome\"\
    >wisexome</a></li>\n<li><a href=\"/sequence-analysis/xhla\">xHLA</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-structural-biology\" class=\"anchor\" href=\"#structural-biology\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/structural-biology\">Structural Biology</a>\n</h2>\n<ul>\n\
    <li><a href=\"/structural-biology/parsnip\">parsnip</a></li>\n<li><a href=\"/structural-biology/pymol\"\
    >pymol</a></li>\n<li><a href=\"/structural-biology/rdock\">rDock</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-systems-biology\" class=\"anchor\" href=\"#systems-biology\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/systems-biology\">Systems Biology</a>\n</h2>\n<ul>\n<li><a\
    \ href=\"/systems-biology/cellphonedb\">cellphonedb</a></li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-utilities\" class=\"anchor\" href=\"#utilities\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"/utilities\">Utilities</a>\n</h2>\n<ul>\n<li><a href=\"/utilities/xvfb\"\
    >Xvfb</a></li>\n<li><a href=\"/utilities/ariba\">ariba</a></li>\n<li><a href=\"\
    /utilities/atom\">atom</a></li>\n<li><a href=\"/utilities/datalad\">datalad</a></li>\n\
    <li><a href=\"/utilities/gdc-client\">gdc-client</a></li>\n<li><a href=\"/utilities/longshot\"\
    >longshot</a></li>\n<li><a href=\"/utilities/pdf2svg\">pdf2svg</a></li>\n<li><a\
    \ href=\"/utilities/pyega3\">pyega3</a></li>\n<li><a href=\"/utilities/snp-sites\"\
    >snp-sites</a></li>\n<li><a href=\"/utilities/sysbench\">sysbench</a></li>\n<li><a\
    \ href=\"/utilities/uropa\">uropa</a></li>\n<li><a href=\"/utilities/vcf2db\"\
    >vcf2db</a></li>\n<li><a href=\"/utilities/visidata\">visidata</a></li>\n<li><a\
    \ href=\"/utilities/whatshap\">whatshap</a></li>\n</ul>\n"
  stargazers_count: 9
  subscribers_count: 7
  topics: []
  updated_at: 1625108554.0
NYU-Molecular-Pathology/NGS580-nf:
  data_format: 2
  description: Target exome sequencing analysis for NYU NGS580 gene panel
  filenames:
  - containers/variant-calling-0.0.2/Singularity.variant-calling-0.0.2
  - containers/manta-1.5.0/Singularity.manta-1.5.0
  - containers/R-3.2.3/Singularity.R-3.2.3
  - containers/trimmomatic-0.36/Singularity.trimmomatic-0.36
  - containers/bwa-0.7.17-sambamba-0.6.8/Singularity.bwa-0.7.17-sambamba-0.6.8
  - containers/python-3.6/Singularity.python-3.6
  - containers/reporting-3.4.3/Singularity.reporting-3.4.3
  - containers/varscan-2.4.3/Singularity.varscan-2.4.3
  - containers/R-3.5.1/Singularity.R-3.5.1
  - containers/R-3.4.3/Singularity.R-3.4.3
  - containers/bcftools-1.3/Singularity.bcftools-1.3
  - containers/multiqc-1.5/Singularity.multiqc-1.5
  - containers/deconstructSigs-1.8.0/Singularity.deconstructSigs-1.8.0
  - containers/bedtools-2.27.1/Singularity.bedtools-2.27.1
  - containers/cnvkit-0.9.0/Singularity.cnvkit-0.9.0
  - containers/samtools-1.7/Singularity.samtools-1.7
  - containers/sambamba-0.6.8/Singularity.sambamba-0.6.8
  - containers/fastqc-0.11.7/Singularity.fastqc-0.11.7
  - containers/strelka-2.9.10/Singularity.strelka-2.9.10
  - containers/cnv_facets-0.14.0/Singularity.cnv_facets-0.14.0
  - containers/delly2-0.7.7/Singularity.delly2-0.7.7
  - containers/pindel-0.2.5b9/Singularity.pindel-0.2.5b9
  - containers/annovar-150617/Singularity.annovar-150617
  - containers/lofreq-2.1.3/Singularity.lofreq-2.1.3
  - containers/bwa-0.7.17/Singularity.bwa-0.7.17
  - containers/multiqc-1.4/Singularity.multiqc-1.4
  - containers/python-2.7/Singularity.python-2.7
  - containers/base/Singularity.base
  - containers/sambamba-0.6.6/Singularity.sambamba-0.6.6
  - containers/sambamba-0.6.6/Singularity.sambamba-0.6.6.old
  - containers/msisensor-0.2/Singularity.msisensor-0.2
  - containers/IGV-2.4.10/Singularity.IGV-2.4.10
  - containers/htslib-1.7/Singularity.htslib-1.7
  - containers/bedtools-2.26.0/Singularity.bedtools-2.26.0
  - containers/cnvkit-0.9.5/Singularity.cnvkit-0.9.5
  - containers/variant-calling-0.0.1/Singularity.variant-calling-0.0.1
  full_name: NYU-Molecular-Pathology/NGS580-nf
  latest_release: 19.10.1
  readme: '<h1>

    <a id="user-content-ngs580-nf" class="anchor" href="#ngs580-nf" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NGS580-nf</h1>

    <p>Target exome analysis for 580 gene panel (NGS580)</p>

    <p><strong>NOTE:</strong> Details listed here may change during development</p>

    <h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p>This pipeline is designed to run targeted exome analysis on Illumina Next-Gen
    sequencing genomic data, in support of the NGS580 cancer diagnostic panel for
    NYU''s Molecular Pathology Department.</p>

    <p>This pipeline starts from paired-end fastq data (<code>.fastq.gz</code>), and
    is meant to accompany the output from the Illumina demultiplexing pipeline listed
    here: <a href="https://github.com/NYU-Molecular-Pathology/demux-nf">https://github.com/NYU-Molecular-Pathology/demux-nf</a>.</p>

    <p>The NGS580-nf analysis workflow includes read trimming, QC, alignment, variant
    calling, annotation, and reporting, along with many other steps.</p>

    <h2>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h2>

    <p>Some key pipeline components included in this repository:</p>

    <ul>

    <li>

    <p><code>bin</code>: directory of custom scripts used throughout the pipeline</p>

    </li>

    <li>

    <p><code>containers</code>: directory of container recipes (Docker, Singularity)
    for use with the pipeline</p>

    </li>

    <li>

    <p><code>example</code>: directory of example samplesheets, etc., to show the
    format used with this pipeline</p>

    </li>

    <li>

    <p><code>targets</code>: directory of target region .bed files included with the
    pipeline for typical analyses</p>

    </li>

    <li>

    <p><code>Makefile</code>: A Makefile with recipes for configuring, starting, and
    managing the pipeline. This is meant to be the main interface between the end-user
    and the pipeline. The Makefile should be reviewed as-needed to familiarize yourself
    with the methods and configurations that are meant to be used for running and
    managing the pipeline.</p>

    </li>

    <li>

    <p><code>main.nf</code>: the main Nextflow pipeline script</p>

    </li>

    <li>

    <p><code>nextflow.config</code>: configuration file for the main Nextflow pipeline
    script</p>

    </li>

    <li>

    <p><code>.config.json</code>: a template for the required <code>config.json</code>
    file used in the pipeline, shows the default pipeline settings that are meant
    to be easily modified by the end-user and used within the pipeline for data processing.</p>

    </li>

    <li>

    <p><code>annovar_db.nf</code>, <code>cnv-pool.nf</code>, <code>hapmap-pool.nf</code>,
    <code>ref.nf</code>: workflows for generating and downloading extra reference
    files used in the main pipeline.</p>

    </li>

    <li>

    <p><code>ref</code>: default location for the storage of reference files (not
    used on NYU Big Purple HPC)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-pipeline-items" class="anchor" href="#pipeline-items" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline Items</h2>

    <p>Some key components that are created during setup, configuration, and execution
    of the pipeline:</p>

    <ul>

    <li>

    <p><code>samples.analysis.tsv</code>: the main samplesheet definig input items
    for the pipeline (described below)</p>

    </li>

    <li>

    <p><code>config.json</code>: configuration file used for pipeline settings (see
    <code>.config.json</code> template for example)</p>

    </li>

    <li>

    <p><code>output</code>: analysis output files published by the Nextflow pipeline</p>

    </li>

    <li>

    <p><code>work</code>: Nextflow temporary directories for execution of pipeline
    tasks</p>

    </li>

    <li>

    <p><code>trace.txt</code>, <code>nextflow.html</code>, <code>timeline.html</code>,
    <code>.nextflow.log</code>: Nextflow execution logs and reports</p>

    </li>

    <li>

    <p><code>logs</code>: directory for pipeline execution logs</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-setup" class="anchor" href="#setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h1>

    <p>This repository should first be cloned from GitHub:</p>

    <pre><code>git clone --recursive https://github.com/NYU-Molecular-Pathology/NGS580-nf.git

    cd NGS580-nf

    </code></pre>

    <ul>

    <li>Once a copy of the repo is made, it can be used to "deploy" new copies of
    the workflow in a pre-configured state</li>

    </ul>

    <h2>

    <a id="user-content-reference-data" class="anchor" href="#reference-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Reference Data</h2>

    <p>Nextflow pipelines have been included for downloading required reference data,
    including ANNOVAR reference databases. You can run them with the following command:</p>

    <pre><code>make setup

    </code></pre>

    <h3>

    <a id="user-content-hapmap-pool-bam" class="anchor" href="#hapmap-pool-bam" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HapMap Pool .bam</h3>

    <p>A negative control HapMap pool .bam file can be prepared using the following
    command:</p>

    <pre><code>make hapmap-pool

    </code></pre>

    <ul>

    <li>Requires <code>samples.hapmap.tsv</code> file specifying the .bam files to
    be combined (example included at <code>example/samples.hapmap.tsv</code>).</li>

    </ul>

    <p>This file is typically built from multiple HapMap samples previously aligned
    by this pipeline. For demonstration purposes, you can provide any .bam and .bai
    files.</p>

    <p>The HapMap Pool files to be used in the pipeline should be set under the <code>HapMapBam</code>
    and <code>HapMapBai</code> keys of <code>config.json</code>.</p>

    <h3>

    <a id="user-content-cnv-pool" class="anchor" href="#cnv-pool" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CNV Pool</h3>

    <p>A control normal sample .cnn file for CNV calling can be prepared using the
    following command:</p>

    <pre><code>make cnv-pool

    </code></pre>

    <ul>

    <li>Requires <code>samples.cnv.tsv</code> file specifying the .bam files to be
    used (example included at <code>example/samples.cnv.tsv</code>)</li>

    </ul>

    <p>This file is typically built from .bam files of specially chosen normal tissue
    sequencing samples previously aligned by this pipeline. For demonstration purposes,
    you can create the .cnn file from any desired .bam file. Note that the targets
    .bed file used to create the .cnn file must match the targets used in the rest
    of the pipeline.</p>

    <p>The .cnn file to be used in the pipeline should be set under the <code>CNVPool</code>
    key in <code>config.json</code>.</p>

    <h2>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Containers</h2>

    <p>The <code>containers</code> directory contains instructions and recipes for
    building the Docker and Singularity containers used in the pipeline.</p>

    <p>Docker is typically used for local container development, while Singularity
    containers are used on the NYU Big Purple HPC cluster. The current pipeline configuration
    for Big Purple uses <code>.simg</code> files stored in a common location on the
    file system.</p>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <p>The pipeline is designed to start from demultiplexed paired end <code>.fastq.gz</code>
    files, with sample ID, tumor ID, and matched normal ID associations defined for
    each set of R1 and R2 .fastq file using a file <code>samples.analysis.tsv</code>
    (example included at <code>example/samples.analysis.tsv</code>).</p>

    <h2>

    <a id="user-content-deployment" class="anchor" href="#deployment" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deployment</h2>

    <p>The easiset way to use the pipeline is to "deploy" a new instance of it based
    on output from the demultiplexing pipeline <a href="https://github.com/NYU-Molecular-Pathology/demux-nf"><code>demux-nf</code></a>.
    This will automatically propagate configurations and information from the demultiplexing
    output.</p>

    <p>The pipeline can also deploy a new, pre-configured copy of itself using the
    included <code>deploy</code> recipe:</p>

    <pre><code>make deploy PRODDIR=/path/to/NGS580_analyses RUNID=Name_for_analysis
    FASTQDIR=/path/to/fastq_files

    </code></pre>

    <ul>

    <li>An optional argument <code>DEMUX_SAMPLESHEET</code> can be used to provide
    a specially formatted demultiplexing samplesheet to be used for extracting extra
    sample information (example included at <code>example/demux-SampleSheet.csv</code>;
    note the extra columns labeling tumor-normal pair IDs, used later).</li>

    </ul>

    <h2>

    <a id="user-content-create-config" class="anchor" href="#create-config" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Create Config</h2>

    <p>A file <code>config.json</code> is required to hold settings for the pipeline.
    It should be created using the built-in methods:</p>

    <pre><code>make config RUNID=my_run_ID FASTQDIR=/path/to/fastqs

    </code></pre>

    <p>or</p>

    <pre><code>make config RUNID=my_run_ID FASTQDIRS=''/path/to/fastqs1 /path/to/fastqs2''

    </code></pre>

    <p>or</p>

    <pre><code>cp .config.json config.json

    </code></pre>

    <p>and then simply edit the new <code>config.json</code> and update the items
    to match your pipeline settings.</p>

    <p>Once created, the <code>config.json</code> file can be updated manually as
    needed. The template and default values can be viewed in the included <code>.config.json</code>
    file.</p>

    <ul>

    <li>

    <code>config.json</code> should be generated automatically if you used <code>make
    deploy</code>

    </li>

    </ul>

    <h2>

    <a id="user-content-create-samplesheet" class="anchor" href="#create-samplesheet"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    Samplesheet</h2>

    <p>A samplesheet file <code>samples.analysis.tsv</code> is required in order to
    define the input samples and their associated .fastq files (example included at
    <code>example/samples.analysis.tsv</code>). Create a samplesheet, based on the
    config file, using the built-in methods:</p>

    <pre><code>make samplesheet

    </code></pre>

    <ul>

    <li>Note that this uses the values previously saved in <code>config.json</code>
    to create the samplesheet</li>

    </ul>

    <h3>

    <a id="user-content-sample-pairs-optional" class="anchor" href="#sample-pairs-optional"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sample
    Pairs (Optional)</h3>

    <p>The NGS580-nf pipeline has special processing for tumor-normal pairs. These
    pairs should be defined in the <code>samples.analysis.tsv</code> file, by listing
    the matched Normal sample for each applicable sample.</p>

    <p>In order to update <code>samples.analysis.tsv</code> automatically with these
    sample pairs, an extra samplesheet can be provided with the tumor-normal pairs.</p>

    <p>Create a <code>samples.tumor.normal.csv</code> samplesheet (example included
    at <code>example/samples.tumor.normal.csv</code>) with the tumor-normal groupings
    for your samples, and update the original samplesheet with it by running the following
    script:</p>

    <pre><code>python update-samplesheets.py --tumor-normal-sheet samples.tumor.normal.csv

    </code></pre>

    <p>If a demultiplexing samplesheet with extra tumor-normal pairs information was
    supplied (see example: <code>example/demux-SampleSheet.csv</code>), then it can
    be used to update the samplesheet with pairs information with the following recipe:</p>

    <pre><code>make pairs PAIRS_SHEET=demux-SampleSheet.csv PAIRS_MODE=demux

    </code></pre>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <p>The pipeline includes an auto-run functionality that attempts to determine
    the best configuration to use for NYU phoenix and Big Purple HPC clusters:</p>

    <pre><code>make run

    </code></pre>

    <p>This will run the pipeline in the current session.</p>

    <p>In order to run the pipeline in the background as a job on NYU''s Big Purple
    HPC, you should instead use the <code>submit</code> recipe:</p>

    <pre><code>make submit SUBQ=fn_medium

    </code></pre>

    <p>Where <code>SUBQ</code> is the name of the SLURM queue you wish to use.</p>

    <p>Refer to the <code>Makefile</code> for more run options.</p>

    <p>Due to the scale of the pipeline, a "local" run option is not currently configured,
    but can be set up easily based on the details shown in the Makefile and <code>nextflow.config</code>.</p>

    <h3>

    <a id="user-content-extra-parameters" class="anchor" href="#extra-parameters"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Extra
    Parameters</h3>

    <p>You can supply extra parameters for Nextflow by using the <code>EP</code> variable
    included in the Makefile, like this:</p>

    <pre><code>make run EP=''--runID 180320_NB501073_0037_AH55F3BGX5

    </code></pre>

    <h3>

    <a id="user-content-demo" class="anchor" href="#demo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Demo</h3>

    <p>A demo dataset can be loaded using the following command:</p>

    <pre><code>make demo

    </code></pre>

    <p>This will:</p>

    <ul>

    <li>

    <p>checkout a demo dataset</p>

    </li>

    <li>

    <p>create a <code>samples.analysis.tsv</code> samplesheet for the analysis</p>

    </li>

    </ul>

    <p>You can then proceed to run the analysis with the commands described above.</p>

    <h2>

    <a id="user-content-more-functionality" class="anchor" href="#more-functionality"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More
    Functionality</h2>

    <p>Extra functions included in the Makefile for pipeline management include:</p>

    <h3>

    <a id="user-content-make-clean" class="anchor" href="#make-clean" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>make clean</code>

    </h3>

    <p>Removes all Nextflow output except for the most recent run. Use <code>make
    clean-all</code> to remove all pipeline outputs.</p>

    <h3>

    <a id="user-content-make-record-presome_prefix_" class="anchor" href="#make-record-presome_prefix_"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>make
    record PRE=some_prefix_</code>

    </h3>

    <p>"Records" copies of the most recent pipeline run''s output logs, configuration,
    Nextflow reports, etc.. Useful for recording analyses that failed or had errors
    in order to debug. Include the optional argument <code>TASK</code> to specify
    a Nextflow <code>work</code> directory to include in the records (example: <code>make
    record PRE=error_something_broke_ TASK=e9/d9ff34</code>).</p>

    <h3>

    <a id="user-content-make-kill" class="anchor" href="#make-kill" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>make kill</code>

    </h3>

    <p>Attempts to cleanly shut down a pipeline running on a remote host e.g. inside
    a SLURM HPC compute job. Note that you can also use <code>scancel</code> to halt
    the parent Nextflow pipeline job as well.</p>

    <h3>

    <a id="user-content-make-fix-permissions-make-fix-group" class="anchor" href="#make-fix-permissions-make-fix-group"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>make
    fix-permissions</code>, <code>make fix-group</code>

    </h3>

    <p>Attempts to fix usergroup and permissions issues that may arise on shared systems
    with multiple users. Be sure to use the extra argument <code>USERGROUP=somegroup</code>
    to specify the usergroup to update to.</p>

    <h3>

    <a id="user-content-make-finalize-work-rm" class="anchor" href="#make-finalize-work-rm"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>make
    finalize-work-rm</code>

    </h3>

    <p>Examines the <code>trace.txt</code> output from the most recent completed pipeline
    run in order to determine while subdirectories in the Nextflow <code>work</code>
    dir are no longer needed, and then deletes them. Can delete multiple subdirs in
    parallel when run with <code>make finalize-work-rm -j 20</code> e.g. specifying
    to delete 20 at a time, etc.</p>

    <h1>

    <a id="user-content-software" class="anchor" href="#software" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Software</h1>

    <p>Developed under Centos 6, RHEL 7, macOS 10.12</p>

    <ul>

    <li>

    <p>bash</p>

    </li>

    <li>

    <p>GNU <code>make</code>, standard GNU tools</p>

    </li>

    <li>

    <p>Python 2/3</p>

    </li>

    <li>

    <p>Java 8+ for Nextflow</p>

    </li>

    <li>

    <p>Docker/Singularity as needed for containers</p>

    </li>

    </ul>

    '
  stargazers_count: 8
  subscribers_count: 2
  topics:
  - nextflow
  - pipeline
  - docker
  - singularity
  - exome
  - exome-sequencing-analysis
  updated_at: 1599538191.0
NYU-Molecular-Pathology/demux-nf:
  data_format: 2
  description: Nextflow pipeline for Illumina NGS demultiplexing
  filenames:
  - containers/dos2unix-7.4.0/Singularity.dos2unix-7.4.0
  - containers/multiqc-1.5/Singularity.multiqc-1.5
  - containers/fastqc-0.11.7/Singularity.fastqc-0.11.7
  - containers/report-r-3.4.3/Singularity.report-r-3.4.3
  - containers/python-2.7/Singularity.python-2.7
  - containers/bcl2fastq-2.17.1/Singularity.bcl2fastq-2.17.1
  full_name: NYU-Molecular-Pathology/demux-nf
  latest_release: 19.04.0
  readme: "<h1>\n<a id=\"user-content-demux-nf\" class=\"anchor\" href=\"#demux-nf\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>demux-nf</h1>\n<p>Nextflow pipeline for demultiplexing Illumina Next-Gen\
    \ sequencing data.</p>\n<h1>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"\
    #usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h1>\n<p>Clone this repository:</p>\n<pre><code>git clone --recursive\
    \ https://github.com/NYU-Molecular-Pathology/demux-nf.git\n</code></pre>\n<h2>\n\
    <a id=\"user-content-deployment\" class=\"anchor\" href=\"#deployment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Deployment</h2>\n\
    <p>The included <code>deploy</code> recipe should be used to create a new directory\
    \ for demultiplexing based on a currently existing sequencing run directory. Include\
    \ arguments that describe the configuration for your sequencing run.</p>\n<pre><code>cd\
    \ demux-nf\nmake deploy RUNID=170809_NB501073_0019_AH5FFYBGX3 SAMPLESHEET=SampleSheet.csv\
    \ SEQTYPE=Archer\n</code></pre>\n<p>arguments:</p>\n<ul>\n<li>\n<p><code>RUNID</code>:\
    \ the identifier given to the run by the sequencer</p>\n</li>\n<li>\n<p><code>SAMPLESHEET</code>:\
    \ the samplesheet required for demultiplexing with <code>bcl2fastq</code></p>\n\
    </li>\n<li>\n<p><code>SEQTYPE</code>: the type of sequencing; currently only <code>Archer</code>\
    \ or <code>NGS580</code> are used</p>\n</li>\n<li>\n<p><code>SEQDIR</code>: parent\
    \ directory where the sequencer outputs its data (pre-configured for NYU server\
    \ locations)</p>\n</li>\n<li>\n<p><code>PRODDIR</code>: parent directory where\
    \ demultiplexing output should be stored (pre-configured for NYU server locations)</p>\n\
    </li>\n</ul>\n<p>This will first check that the specified run exists on the server\
    \ before cloning into a new directory at the given production output location\
    \ and configuring it for demultiplexing using the subsequent commands described\
    \ here.</p>\n<h2>\n<a id=\"user-content-run-workflow\" class=\"anchor\" href=\"\
    #run-workflow\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Run Workflow</h2>\n<p>Assuming you used <code>make\
    \ deploy</code> or <code>make config</code> to prepare your demultiplexing directory,\
    \ the following command can be used to automatically run the workflow based on\
    \ the pre-defined settings and settings from your current system.</p>\n<pre><code>make\
    \ run\n</code></pre>\n<p>Extra parameters to be passed to Nextflow can be supplied\
    \ with the <code>EP</code> argument:</p>\n<pre><code>make run EP='--samplesheet\
    \ SampleSheet.csv --runDir /path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3'\n\
    </code></pre>\n<p>To submit the parent Nextflow pipeline as a job on the HPC cluster:</p>\n\
    <pre><code>make submit\n\n# with a different submission queue:\nmake submit SUBQ=fn_long\n\
    \n# with a different submission time:\nmake submit SUBQ=cpu_long SUBTIME='--time=6-00:00:00'\n\
    </code></pre>\n<p>For alternative <code>run</code> methods, consult the <code>Makefile</code>.</p>\n\
    <h1>\n<a id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Configuration</h1>\n<p>Demultiplexing metadata for the workflow can\
    \ be provided through several methods, evaluated in the following order:</p>\n\
    <ul>\n<li>parameters can be supplied directly to Nextflow via CLI</li>\n</ul>\n\
    <pre><code>nextflow run main.nf --runID 12345\n</code></pre>\n<ul>\n<li>if the\
    \ file <code>config.json</code> is present, non-<code>null</code> parameters will\
    \ be retrieved</li>\n</ul>\n<pre><code>{\n    \"runDir\": \"/path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3\"\
    ,\n    \"samplesheet\": \"SampleSheet.csv\",\n    \"runID\": \"170809_NB501073_0019_AH5FFYBGX3\"\
    \n}\n</code></pre>\n<ul>\n<li>\n<p>this file is generated automatically during\
    \ the <code>deploy</code> step, using the included <code>config.py</code> script</p>\n\
    </li>\n<li>\n<p>the following items in the current directory will be used if present:</p>\n\
    <ul>\n<li>\n<p><code>SampleSheet.csv</code>: default samplesheet file</p>\n</li>\n\
    <li>\n<p><code>runDir</code> : default sequencing run source directory (can be\
    \ a symlink)</p>\n</li>\n<li>\n<p><code>runID.txt</code>: a text file, the first\
    \ line of which will be used as the run ID</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1>\n\
    <a id=\"user-content-extras\" class=\"anchor\" href=\"#extras\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Extras</h1>\n\
    <ul>\n<li>(re)initialize configurations (overwrites old <code>config.json</code>):</li>\n\
    </ul>\n<pre><code>make config RUNDIR=/path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3\
    \ SAMPLESHEET=SampleSheet.csv RUNID=170809_NB501073_0019_AH5FFYBGX3\n</code></pre>\n\
    <ul>\n<li>update an existing directory to the latest version of this repo:</li>\n\
    </ul>\n<pre><code>make update\n</code></pre>\n<ul>\n<li>clean up workflow intermediary\
    \ files to save space (workflow cannot be resumed after this):</li>\n</ul>\n<pre><code>make\
    \ finalize\n</code></pre>\n<ul>\n<li>clean up output from all old workflows (saves\
    \ current workflow output):</li>\n</ul>\n<pre><code>make clean\n</code></pre>\n\
    <ul>\n<li>delete the output from all workflows:</li>\n</ul>\n<pre><code>make clean-all\n\
    </code></pre>\n<ul>\n<li>mark that the demultiplexing suceeded and the results\
    \ passed QC for downstream analysis:</li>\n</ul>\n<pre><code>make passed\n</code></pre>\n\
    <ul>\n<li>deploy a new NGS580 analysis using the current results:</li>\n</ul>\n\
    <pre><code>make deploy-NGS580\n</code></pre>\n<ul>\n<li>make a 'deliverables'\
    \ directory with just the results for samples for a specific client</li>\n</ul>\n\
    <pre><code>make deliverable CLIENT=somelab SHEET=list_of_clients_samples.txt\n\
    </code></pre>\n<h1>\n<a id=\"user-content-software\" class=\"anchor\" href=\"\
    #software\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Software</h1>\n<p>Required:</p>\n<ul>\n<li>\n<p>Java 8 (Nextflow)</p>\n\
    </li>\n<li>\n<p>Python 2.7+</p>\n</li>\n<li>\n<p>GNU <code>make</code></p>\n</li>\n\
    </ul>\n<p>Optional; must be installed to system or available with Singularity\
    \ containers:</p>\n<ul>\n<li>\n<p><code>bcl2fastq</code> version 2.17.1</p>\n\
    </li>\n<li>\n<p>FastQC version 0.11.7</p>\n</li>\n<li>\n<p>R (3.3.0+, with <code>knitr</code>\
    \ and <code>rmarkdown</code> libraries)</p>\n</li>\n<li>\n<p>Pandoc 1.13.1+</p>\n\
    </li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics:
  - nextflow
  - pipeline
  - demultiplexing
  - bcl2fastq
  updated_at: 1599538142.0
Nahuel-Mk2/def-space:
  data_format: 2
  description: Def File of Singularity
  filenames:
  - def/edge-connect.def
  - def/contextual-attention.def
  - def/wav2pix.def
  - def/sc-fegan.def
  - def/vae-mnist.def
  - def/singan.def
  - def/stargan.def
  - def/lafin.def
  full_name: Nahuel-Mk2/def-space
  latest_release: null
  readme: '<h1>

    <a id="user-content-def-space" class="anchor" href="#def-space" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>def-space</h1>

    <p>This repository is def-space for Singularity</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606189900.0
ORNL/faro:
  data_format: 2
  description: 'Face Recognition from Oak Ridge (FaRO) provides a well-defined server-client
    interface to a some of the best open source face recognition projects on the web. '
  filenames:
  - services/rcnn/Singularity
  full_name: ORNL/faro
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-faro-readme\" class=\"anchor\" href=\"#faro-readme\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>FARO: Readme</h1>\n<h2>\n<a id=\"user-content-overview\" class=\"\
    anchor\" href=\"#overview\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Overview</h2>\n<p>Face Recognition from Oak\
    \ Ridge (FaRO) provides a well-defined server-client\ninterface to some of the\
    \ best open source face recognition projects on the\nweb.  The intention is to\
    \ support an open platform for face recognition research\nand to provide a well-defined\
    \ and modern baseline for face recognition accuracy.<br>\nWhile many universities\
    \ and independent developers have released high quality\nface recognition models,\
    \ they often lack many useful features such as\nconfiguration management, easy\
    \ to use interfaces, deployment tools, backend\ndatabases, and analysis tools\
    \ that FaRO provides.</p>\n<p>In our research we have found that there are many\
    \ high quality and open source\nface analysis and recognition algorithms available\
    \ for research; however,\nend-to-end systems that can support larger systems or\
    \ that can be retrained for niche\napplications are lacking. We hope FARO can\
    \ fill some of those needs.</p>\n<p>The primary goals of this project are:</p>\n\
    <ol>\n<li>Create an easy to use foundation that can support complex face recognition\
    \ systems.</li>\n<li>Provide well-defined benchmark algorithms.</li>\n<li>Allow\
    \ for algorithm improvements via open source software and models and to support\
    \ improvements using techniques like transfer learning.</li>\n</ol>\n<p>FaRO is\
    \ designed as a client/server system to accomodate the need for high speed GPU\n\
    hardware to support deep learning face processing.  GRPC calls are used to communicate\n\
    with the server components which allows the clients to be written in many languages\
    \ and\nimplemented on a varity of computationally limited platforms such as cellphones\
    \ or biometric\ncollection devices.</p>\n<h2>\n<a id=\"user-content-publications\"\
    \ class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Publications</h2>\n<p>If you\
    \ use FARO for publications please cite as:</p>\n<pre><code>@misc{bolme2019faro,\n\
    \    title={{FaRO}: {FA}ce {R}ecognition From {O}ak ridge},\n    author={David\
    \ S. Bolme and David C. Cornett III and Nisha Srinivas},\n    year={2019},\n \
    \   howpublished={https://github.com/ORNL/faro}\n}\n</code></pre>\n<h2>\n<a id=\"\
    user-content-system-requirements\" class=\"anchor\" href=\"#system-requirements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>System Requirements:</h2>\n<p>Many FaRO services should run nicely\
    \ on limited hardware resources.  As we\nintegrate more deep learning algorithms,\
    \ those may require GPUs and additional\nhardware.</p>\n<ul>\n<li>Software: python3,\
    \ virtualenv, cmake, wget</li>\n<li>Python Libraries: see requirements.txt</li>\n\
    <li>NVidia GPU with 8GB of Ram - GTX Titan X/1070/1080 or better</li>\n<li>nvidia-docker2\
    \ - supporting Cuda 9.0</li>\n</ul>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick Start</h2>\n<p>This is\
    \ intended to get Dlib algorithm up and running quickly.  This is a good\nplace\
    \ to start and will allow you to test the FaRO interface.  A few\ndependencies\
    \ may be needed on a fresh Ubuntu installation including: cmake,\npython2, and\
    \ python3.  The install scripts will download and install many other\ndependencies\
    \ in the user directory as well as some large machine learning\nmodels.  To get\
    \ some initial dependencies install:</p>\n<pre><code>$ sudo apt install cmake\n\
    $ sudo apt install python2-dev\n$ sudo apt install python3-dev\n$ sudo apt install\
    \ virtualenv\n$ sudo apt install wget\n</code></pre>\n<p>First build the client\
    \ environment and compile the proto interfaces.</p>\n<pre><code>$ ./build-env-universal.sh\n\
    #For Mac users run - $echo \"export PYTHONPATH=`pwd`/src:$PYTHONPATH\" &gt;&gt;\
    \ \"$HOME/.bash_profile\" - after running build-env-universal.sh\nif using virtualenv,\n\
    \    $ source env_faro_server/bin/activate\n\nif using conda,\n    $ source activate\
    \ env_faro_server\n    or\n    $ conda activate env_faro_server\n\n$ ./build-proto.sh\n\
    </code></pre>\n<p>In one terminal run the Dlib service.  When you do this for\
    \ the first time it\nwill create a \"faro-storage\" directory and will download\
    \ and extract the machine\nlearning models.  At the end it will print out messages\
    \ for each started worker:\n\"Worker N Started.\"  By default the service is started\
    \ on port localhost:50030.</p>\n<p>If using virtualenv,</p>\n<pre><code>$ source\
    \ env_faro_server/bin/activate\n$ cd services/dlib\n$ ./run-dlib.sh\n</code></pre>\n\
    <p>If using conda,</p>\n<pre><code>$ source activate env_faro_server or conda\
    \ activate env_faro_server\n$ cd services/dlib\n$ ./run_dlib.sh\n</code></pre>\n\
    <p>The VGG2Resnet model can also be run using similar commands, but only run one\n\
    service at a time unless you carefully configure the ports and check available\n\
    memory, etc.</p>\n<p>If using virtualenv,</p>\n<pre><code>$ source env_faro_server/bin/activate\n\
    $ cd services/vggface2\n$ ./run-vgg2.sh\n</code></pre>\n<p>If using conda,</p>\n\
    <pre><code>$ source activate env_faro_server or conda activate env_faro_server\n\
    $ cd services/vggface2\n$ ./run_vgg2.sh\n</code></pre>\n<p>Similarly, InsightFace\
    \ algorithms can be executed using similar commands.\nFace detection is performed\
    \ using RetinaFace and features are extracted using ArcFace.\nCurrently, InsightFace\
    \ works only with 1 GPU and worker.</p>\n<p>If using virtualenv,</p>\n<pre><code>$\
    \ source env_faro_server/bin/activate \n$ cd services/arcface\n$ ./run_arcface.sh\n\
    </code></pre>\n<p>If using conda,</p>\n<pre><code>$ source activate env_faro_server\
    \ or conda activate env_faro_server    \n$ cd services/arcface\n$ ./run_arcface.sh\n\
    </code></pre>\n<p>In a second terminal run client applications. For this you can\
    \ use either the\n\"env_faro\" or \"env_faro_server\" environments.  Test scripts\
    \ are available in\nthe test directory to test the workings of the different functionalities\
    \ in FaRO.</p>\n<p>To test the scripts,</p>\n<p>If using virtualenv,</p>\n<pre><code>$\
    \ source env_faro/bin/activate\n$ cd tests\n</code></pre>\n<p>If using conda,</p>\n\
    <pre><code>$ source activate env_faro or conda activate env_faro\n$ cd tests\n\
    </code></pre>\n<p>To test the detect functionality on images execute,</p>\n<pre><code>$./test_detect.sh\n\
    </code></pre>\n<p>To test the detect functionality in videos execute,</p>\n<pre><code>$./test_detect_videos.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-install-with-pip\" class=\"anchor\"\
    \ href=\"#install-with-pip\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Install With PIP</h2>\n<p>This is a simple way\
    \ to add FaRO to the environment.  It should install everything needed to run\
    \ client api calls, but it may not provide all the configurations or models needed\
    \ to run services.</p>\n<pre><code>$ pip install git+https://github.com/ORNL/faro.git\n\
    </code></pre>\n<h2>\n<a id=\"user-content-run-a-service-command-line\" class=\"\
    anchor\" href=\"#run-a-service-command-line\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run a Service Command Line</h2>\n\
    <p>Starting python services can be done with a simple command line.  This will\
    \ start the service specifying the port, the number of workers, and the algorithm.</p>\n\
    <pre><code>$ python -m faro.FaceService --port=localhost:50030 --worker-count=2\
    \ --algorithm=dlib\n</code></pre>\n<h2>\n<a id=\"user-content-using-the-client-api\"\
    \ class=\"anchor\" href=\"#using-the-client-api\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using the Client API</h2>\n<p>Examples\
    \ can be found in the Notebooks directory.  The best place to start is the <a\
    \ href=\"https://github.com/ORNL/faro/blob/master/Notebooks/FaRO%20Client%20Usage.ipynb\"\
    >FaRO Client Usage notebook</a>.</p>\n<p>or</p>\n<p>FaRO_Client_Face_Detection_Video_and_Images.ipynb</p>\n\
    <p>The client can access the services using the FaRO command line interface. The\
    \ CLI includes the following functions/commands</p>\n<pre><code>#client environment\
    \ has to be activated\n$ cd bin\n$ ./faro \n\nusage : ./faro &lt;command&gt; --help\n\
    list the commands to be used\nCommands:\n    flist - List the faces in a gallery.\n\
    \    detectExtract - Run face detection and template extraction.\n    glist -\
    \ List the galleries on the service.\n    test - Process a probe and gallery directory\
    \ and produce a distance matrix.\n    extractOnly - Only run face extraction and\
    \ attribute extraction.\n    enroll - Extract faces and enroll faces in a gallery.\n\
    \    search - Search images for faces in a gallery.\n    detect - Only run face\
    \ detection.\n    \n#to run detect command and find its input options execute,\n\
    $./faro detect --help\n\nUsage: ./faro command [OPTIONS] [image] [image_directory]\
    \ [video] [...]\n\nRun detection on a collection of images.\n\nOptions:\n  --version\
    \             show program's version number and exit\n  -h, --help           \
    \ show this help message and exit\n  -v, --verbose         Print out more program\
    \ information.\n  -n MAX_IMAGES, --max-images=MAX_IMAGES\n                   \
    \     Process at N images and then stop.\n  --maximum-size=MAX_SIZE\n        \
    \                If too large, images will be scaled to have this\n          \
    \              maximum size. Default=1920\n\n  Detector Options:\n    Configuration\
    \ for the face detector.\n\n    -d DETECTIONS_CSV, --detections-csv=DETECTIONS_CSV\n\
    \                        Save detection data to the file.\n    -a ATTRIBUTES_CSV,\
    \ --attributes-csv=ATTRIBUTES_CSV\n                        Save attributes data\
    \ to the file.\n    --detect-log=DETECT_LOG\n                        A directory\
    \ for detection images.\n    --face-log=FACE_LOG\n                        A directory\
    \ for faces.\n    -b, --best          Detect the 'best' highest scoring face in\
    \ the image.\n    --detect-thresh=DETECT_THRESH\n                        The threshold\
    \ for a detection.\n    --min-size=MIN_SIZE\n                        Faces with\
    \ a height less that this will be ignored.\n    --attribute-filter=ATTRIBUTE_FILTER\n\
    \                        A comma separated list of filters example: 'Male&gt;0.5'\n\
    \n  Connection Options:\n    Control the connection to the FaRO service.\n\n \
    \   --max-async=MAX_ASYNC\n                        The maximum number of asyncronous\
    \ call to make at a\n                        time. Default=8\n    --max-message-size=MAX_MESSAGE_SIZE\n\
    \                        Maximum GRPC message size. Set to -1 for unlimited.\n\
    \                        Default=67108864\n    -p DETECT_PORT, --port=DETECT_PORT\n\
    \                        The port used for the recognition service.\n    --detect-port=DETECT_PORT\n\
    \                        The port used for the recognition service.\n    --recognition-port=REC_PORT\n\
    \                        The port used for the recognition service.\n\n</code></pre>\n\
    <h2>\n<a id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Getting Help</h2>\n<p>We currently have limited resources to support\
    \ FaRO but will do our best to provide support.  If you encounter\nproblems please\
    \ submit tickets to the issues list so that they can be properly tracked.</p>\n\
    <p><a href=\"https://github.com/ORNL/faro/issues\">https://github.com/ORNL/faro/issues</a></p>\n\
    <p>We would also like to see new features or fixes submitted as pull requests.</p>\n\
    <p><a href=\"https://github.com/ORNL/faro/pulls\">https://github.com/ORNL/faro/pulls</a></p>\n"
  stargazers_count: 4
  subscribers_count: 9
  topics: []
  updated_at: 1624819064.0
OSC/bc_osc_example_shiny:
  data_format: 2
  description: Batch Connect - Example Shiny App that runs on OSC OnDemand
  filenames:
  - ext/Singularity
  full_name: OSC/bc_osc_example_shiny
  latest_release: null
  readme: '<h1>

    <a id="user-content-wip-batch-connect---osc-example-shiny-app" class="anchor"
    href="#wip-batch-connect---osc-example-shiny-app" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>[WIP] Batch Connect - OSC Example Shiny
    App</h1>

    <p><a href="https://camo.githubusercontent.com/a8152a2780451d58acdca1e79b03f771d6e84ae12087e6e7a824b6759b715dc1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f6578616d706c655f7368696e792e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a8152a2780451d58acdca1e79b03f771d6e84ae12087e6e7a824b6759b715dc1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f6578616d706c655f7368696e792e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_example_shiny.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>A Batch Connect app designed for OSC OnDemand that launches a Shiny App within

    an Owens batch job.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://shiny.rstudio.com/" rel="nofollow">Shiny</a> x.y.z+</li>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module purge</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>Use git to clone this app and checkout the desired branch/version you want
    to

    use:</p>

    <div class="highlight highlight-source-shell"><pre>scl <span class="pl-c1">enable</span>
    git19 -- git clone <span class="pl-k">&lt;</span>repo<span class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>You will not need to do anything beyond this as all necessary assets are

    installed. You will also not need to restart this app as it isn''t a Passenger

    app.</p>

    <p>To update the app you would:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git fetch

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_example_shiny/fork">https://github.com/OSC/bc_osc_example_shiny/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1527005209.0
OSC/bc_osc_rstudio_server:
  data_format: 2
  description: Batch Connect - OSC RStudio Server
  filenames:
  - Singularity
  full_name: OSC/bc_osc_rstudio_server
  latest_release: v0.10.0
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Owens batch job.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module restore</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job before

    launching the RStudio Server.</li>

    </ul>

    <p><strong>without Singularity</strong></p>

    <ul>

    <li>

    <a href="https://www.r-project.org/" rel="nofollow">R</a> 3.3.2+ (earlier versions
    are untested but may work for you)</li>

    <li>

    <a href="https://www.rstudio.com/products/rstudio-server/" rel="nofollow">RStudio
    Server</a> 1.0.136+ (earlier versions are untested by may work for you)</li>

    <li>

    <a href="https://proot-me.github.io/" rel="nofollow">PRoot</a> 5.1.0+ (used to
    setup fake bind mount)</li>

    </ul>

    <p><strong>or with Singularity</strong></p>

    <ul>

    <li>

    <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a> 2.4.2+</li>

    <li>A Singularity image similar to <a href="https://www.singularity-hub.org/collections/463"
    rel="nofollow">nickjer/singularity-rstudio</a>

    </li>

    <li>Corresponding module to launch the above Singularity image (see

    <a href="https://github.com/nickjer/singularity-rstudio/blob/master/example_module/">example_module</a>)</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>Use git to clone this app and checkout the desired branch/version you want
    to

    use:</p>

    <div class="highlight highlight-source-shell"><pre>scl <span class="pl-c1">enable</span>
    git19 -- git clone <span class="pl-k">&lt;</span>repo<span class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>You will not need to do anything beyond this as all necessary assets are

    installed. You will also not need to restart this app as it isn''t a Passenger

    app.</p>

    <p>To update the app you would:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git fetch

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_rstudio_server/fork">https://github.com/OSC/bc_osc_rstudio_server/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <ul>

    <li>Documentation, website content, and logo is licensed under

    <a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow">CC-BY-4.0</a>

    </li>

    <li>Code is licensed under MIT (see LICENSE.txt)o</li>

    <li>RStudio, Shiny and the RStudio logo are all registered trademarks of RStudio.</li>

    </ul>

    '
  stargazers_count: 4
  subscribers_count: 10
  topics: []
  updated_at: 1614963192.0
OSC/bc_osc_rstudio_server_pitzer:
  data_format: 2
  description: Batch Connect - OSC RStudio Server - Pitzer
  filenames:
  - Singularity
  full_name: OSC/bc_osc_rstudio_server_pitzer
  latest_release: v0.1.5
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server_pitzer.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Pitzer batch job.</p>

    <h2>

    <a id="user-content-deprecated-application-warning" class="anchor" href="#deprecated-application-warning"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deprecated
    application warning</h2>

    <p>This application no longer works.  It raises an exception when users attempt
    to submit jobs.

    This is because we now have functionality to submit to multiple clusters and

    <a href="https://github.com/OSC/bc_osc_rstudio_server">the generic application</a>
    now submits

    to pitzer rendering this application useless.</p>

    <p>For historic versions, see the last released you can still view

    <a href="https://github.com/OSC/bc_osc_rstudio_server_pitzer/tree/v0.3.0">v0.3.0</a>
    as it was the last

    working version of this application.</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1598640242.0
OSC/bc_osc_rstudio_server_quick:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: OSC/bc_osc_rstudio_server_quick
  latest_release: v0.0.1
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Owens batch job.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module restore</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job before

    launching the RStudio Server.</li>

    </ul>

    <p><strong>without Singularity</strong></p>

    <ul>

    <li>

    <a href="https://www.r-project.org/" rel="nofollow">R</a> 3.3.2+ (earlier versions
    are untested but may work for you)</li>

    <li>

    <a href="https://www.rstudio.com/products/rstudio-server/" rel="nofollow">RStudio
    Server</a> 1.0.136+ (earlier versions are untested by may work for you)</li>

    <li>

    <a href="https://proot-me.github.io/" rel="nofollow">PRoot</a> 5.1.0+ (used to
    setup fake bind mount)</li>

    </ul>

    <p><strong>or with Singularity</strong></p>

    <ul>

    <li>

    <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a> 2.4.2+</li>

    <li>A Singularity image similar to <a href="https://www.singularity-hub.org/collections/463"
    rel="nofollow">nickjer/singularity-rstudio</a>

    </li>

    <li>Corresponding module to launch the above Singularity image (see

    <a href="https://github.com/nickjer/singularity-rstudio/blob/master/example_module/">example_module</a>)</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>Use git to clone this app and checkout the desired branch/version you want
    to

    use:</p>

    <div class="highlight highlight-source-shell"><pre>scl <span class="pl-c1">enable</span>
    git19 -- git clone <span class="pl-k">&lt;</span>repo<span class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>You will not need to do anything beyond this as all necessary assets are

    installed. You will also not need to restart this app as it isn''t a Passenger

    app.</p>

    <p>To update the app you would:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git fetch

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_rstudio_server/fork">https://github.com/OSC/bc_osc_rstudio_server/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1570733859.0
OSC/centos7-launcher:
  data_format: 2
  description: A thin Singularity image used as an alternative to Proot to wrap applications
    in an arbitrary file system.
  filenames:
  - Singularity
  full_name: OSC/centos7-launcher
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7-launcher" class="anchor" href="#centos7-launcher"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>centos7-launcher</h1>

    <p>A Singularity image used wrap applications RStudio <code>rserver</code> instances
    in an arbitrary file system for use with <a href="http://openondemand.org/" rel="nofollow">OnDemand</a>.
    Tested as compatible with Singularity 2.x and 3.x.</p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage:</h2>

    <h3>

    <a id="user-content-singularity-2x" class="anchor" href="#singularity-2x" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity 2.x</h3>

    <p>TODO...</p>

    <h3>

    <a id="user-content-singularity-3x" class="anchor" href="#singularity-3x" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity 3.x</h3>

    <p>TODO...</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1550176998.0
OSC/sa_singularity_iqmol:
  data_format: 2
  description: IQmol in a Singularity container
  filenames:
  - Singularity
  - Singularity.2.11.2
  - Singularity.2.13b
  - Singularity.2.14
  full_name: OSC/sa_singularity_iqmol
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-iqmol" class="anchor" href="#singularity-iqmol"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    IQmol</h1>

    <p><a href="https://singularity-hub.org/collections/3599" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="http://iqmol.org/index.html" rel="nofollow">IQmol</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a> or CentOS image <a href="https://hub.docker.com/_/centos"
    rel="nofollow">centos</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>iqmol.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build iqmol.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull iqmol.sif
    shub://OSC/sa_singularity_iqmol</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-start-iqmol" class="anchor" href="#start-iqmol" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start IQmol</h3>

    <p>IQmol is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run iqmol.sif</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./iqmol.sif</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1599018735.0
OSC/sa_singularity_molgfx:
  data_format: 2
  description: ' Molecular graphics systems in a Singularity container'
  filenames:
  - Singularity
  - Singularity.1.0
  full_name: OSC/sa_singularity_molgfx
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-molgfx" class="anchor" href="#singularity-molgfx"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Molgfx</h1>

    <p><a href="https://singularity-hub.org/collections/4301" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://github.com/OpenChemistry">Open Chemistry</a>,
    Gabedit and Jmol. It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>molgfx.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build molgfx.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull molgfx.sif
    shub://OSC/sa_singularity_molgfx</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-find-versions-of-molecular-graphics-systems" class="anchor"
    href="#find-versions-of-molecular-graphics-systems" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Find versions of molecular graphics systems</h3>

    <div class="highlight highlight-source-shell"><pre>singularity inspect -H molgfx.sif</pre></div>

    <h3>

    <a id="user-content-start-avogadro2" class="anchor" href="#start-avogadro2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start Avogadro2</h3>

    <p>Avogadro2 is started using the default exec command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity <span class="pl-c1">exec</span>
    molgfx.sif avogadro2</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1588619360.0
OSC/sa_singularity_openexr:
  data_format: 2
  description: OpenEXR in a Singularity container
  filenames:
  - Singularity
  - Singularity.2.2
  full_name: OSC/sa_singularity_openexr
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-openexr" class="anchor" href="#singularity-openexr"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    OpenEXR</h1>

    <p><a href="https://singularity-hub.org/collections/3586" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://www.openexr.com/" rel="nofollow">OpenEXR</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a></p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>openexr.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build openexr.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name openexr.sif
    shub://OSC/sa_singularity_openexr</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-render-exr-image" class="anchor" href="#render-exr-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Render
    .EXR image</h3>

    <p>The <code>exrdisplay</code> command is launched using the command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity <span class="pl-c1">exec</span>
    openexr.sif exrdisplay -h</pre></div>

    <p>Example:</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    <span class="pl-c1">exec</span> openexr.sif exrdisplay rendertest_0001.exr</span></pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1569951214.0
OSC/sa_singularity_qgis:
  data_format: 2
  description: QGIS in a Singularity container
  filenames:
  - Singularity
  - Singularity.3.4.12
  full_name: OSC/sa_singularity_qgis
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-qgis" class="anchor" href="#singularity-qgis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    QGIS</h1>

    <p><a href="https://singularity-hub.org/collections/3587" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://qgis.org/en/site/index.html" rel="nofollow">QGIS</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>. Packages installed: <code>qgis qgis-plugin-grass</code></p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>qgis.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build qgis.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull qgis.sif shub://OSC/sa_singularity_qgis</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-start-qgis" class="anchor" href="#start-qgis" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start QGIS</h3>

    <p>QGIS is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run qgis.sif</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./qgis.sif</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1569951156.0
OSC/sa_singularity_winehq:
  data_format: 2
  description: WineHQ in a Singularity container
  filenames:
  - Singularity
  - Singularity.5.0.0
  - Singularity.4.0.3
  full_name: OSC/sa_singularity_winehq
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-winehq" class="anchor" href="#singularity-winehq"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    WineHQ</h1>

    <p><a href="https://singularity-hub.org/collections/3891" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://www.winehq.org/" rel="nofollow">WineHQ</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>winehq.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build winehq.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull winehq.sif
    shub://OSC/sa_singularity_winehq</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-run-64-bit-windows-binary" class="anchor" href="#run-64-bit-windows-binary"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    64-bit Windows binary</h3>

    <p>WineHQ is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run winehq.sif
    /path/to/windows_64bit_exe</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./winehq.sif /path/to/windows_64bit_exe</pre></div>

    <h3>

    <a id="user-content-run-32-bit-windows-binary" class="anchor" href="#run-32-bit-windows-binary"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    32-bit Windows binary</h3>

    <div class="highlight highlight-source-shell"><pre>singularity <span class="pl-c1">exec</span>
    winehq.sif wine /path/to/windows_32bit_exe</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1581361908.0
OSC/sa_singularity_xcrysden:
  data_format: 2
  description: XCrySDen in a Singularity container
  filenames:
  - Singularity
  - Singularity.1.6.2
  full_name: OSC/sa_singularity_xcrysden
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-xcrysden" class="anchor" href="#singularity-xcrysden"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    XCrySDen</h1>

    <p><a href="https://singularity-hub.org/collections/4445" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="http://www.xcrysden.org/Download.html" rel="nofollow">XCrysDen</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>xcrysden.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build xcrysden.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull xcrysden.sif
    shub://OSC/sa_singularity_xcrysden</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-start-xcrysden" class="anchor" href="#start-xcrysden" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start XCrysDen</h3>

    <p>XCrysDen is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run xcrysden.sif</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./xcrysden.sif</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1592244254.0
OSC/shiny_launcher:
  data_format: 2
  description: null
  filenames:
  - ext/Singularity
  full_name: OSC/shiny_launcher
  latest_release: null
  readme: '<h1>

    <a id="user-content-wip-batch-connect---osc-shiny-app-launcher" class="anchor"
    href="#wip-batch-connect---osc-shiny-app-launcher" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>[WIP] Batch Connect - OSC Shiny App Launcher</h1>

    <p><a href="https://camo.githubusercontent.com/5e71a5a07e8e6e15f922edb76b5fb68e7ee6087e336807c38095861b8c7f5fc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f7368696e795f6c61756e636865722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/5e71a5a07e8e6e15f922edb76b5fb68e7ee6087e336807c38095861b8c7f5fc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f7368696e795f6c61756e636865722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/shiny_launcher.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>A Batch Connect app designed for OSC OnDemand that launches a Shiny App within

    an Owens batch job.</p>

    <p>The Shiny app is included a submodule and each deployment can modified which

    Shiny app to deploy using git config to specify the URL for the submodule. This

    way the launcher code can be reused for multiple apps but the launcher and the

    app itself can be managed separately.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://shiny.rstudio.com/" rel="nofollow">Shiny</a> x.y.z+</li>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module purge</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p><strong>TODO</strong></p>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_example_shiny/fork">https://github.com/OSC/bc_osc_example_shiny/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1569007230.0
PGP-UK/GenomeChronicler:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: PGP-UK/GenomeChronicler
  latest_release: '0.91'
  readme: "<pre><code> #####                                         #####       \
    \                                                     \n#     # ###### #    #\
    \  ####  #    # ######    #     # #    # #####   ####  #    # #  ####  #     \
    \ ###### #####  \n#       #      ##   # #    # ##  ## #         #       #    #\
    \ #    # #    # ##   # # #    # #      #      #    # \n#  #### #####  # #  # #\
    \    # # ## # #####     #       ###### #    # #    # # #  # # #      #      #####\
    \  #    # \n#     # #      #  # # #    # #    # #         #       #    # #####\
    \  #    # #  # # # #      #      #      #####  \n#     # #      #   ## #    #\
    \ #    # #         #     # #    # #   #  #    # #   ## # #    # #      #     \
    \ #   #  \n #####  ###### #    #  ####  #    # ######     #####  #    # #    #\
    \  ####  #    # #  ####  ###### ###### #    #  \n</code></pre>\n<p><a href=\"\
    https://singularity-hub.org/collections/3664\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-welcome\" class=\"\
    anchor\" href=\"#welcome\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Welcome</h1>\n<p>This is the repository for\
    \ Genome Chronicler, the Personal Genome Project United Kingdom (PGP-UK) genomic\
    \ report generation scripts.</p>\n<h1>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h1>\n<p>After\
    \ cloning this repository, run the SetupMeFirst.sh script in your local system\
    \ to retrieve the extra files needed to run the pipeline (around 10GB, so too\
    \ big for git).</p>\n<h1>\n<a id=\"user-content-input-files\" class=\"anchor\"\
    \ href=\"#input-files\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Input files</h1>\n<p>The main script (GenomeChronicler_mainDruid.pl)\
    \ needs a BAM file as input, and optionally can also use a VEP generated summary\
    \ html file, if variants have already been called on the data and summaries are\
    \ to be produced.</p>\n<h1>\n<a id=\"user-content-dependencies\" class=\"anchor\"\
    \ href=\"#dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Dependencies</h1>\n<p>To handle the myriad dependencies\
    \ present in this pipeline, it is avaliable through Singularity Hub as a singularity\
    \ container (see badge at top of the page).</p>\n<h1>\n<a id=\"user-content-easy-start-using-singularity\"\
    \ class=\"anchor\" href=\"#easy-start-using-singularity\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Easy Start\
    \ using Singularity</h1>\n<p>If you don't already have singularity on your system,\
    \ or want to know more about it, head to their userguide at: <a href=\"https://sylabs.io/guides/3.1/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.1/user-guide/</a>\nWhile Singularity\
    \ is not needed to run GenomeChronicler, it does make setup much easier.</p>\n\
    <p>For a manual installation without Singularity, please follow the steps in the\
    \ %post section of the Singularity file in this repository, to install all the\
    \ dependencies.</p>\n<p>Downloading pre-packaged GenomeChronicler from SingularityHub</p>\n\
    <pre><code>singularity pull shub://PGP-UK/GenomeChronicler\n</code></pre>\n<p>Getting\
    \ some test data (NA12878 from ENA, pre-mapped to GRCh38, and the respective reference)</p>\n\
    <pre lang=\"wget\"><code>singularity exec GenomeChronicler_latest.sif wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/CEU/NA12878/alignment/NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram\n\
    \nsingularity exec GenomeChronicler_latest.sif wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa\n\
    \n</code></pre>\n<p>Converting data to BAM format</p>\n<pre><code>singularity\
    \ exec GenomeChronicler_latest.sif samtools view -T GRCh38_full_analysis_set_plus_decoy_hla.fa\
    \ -b -o NA12878wxs.bam NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram\n\
    </code></pre>\n<p>Running GenomeChronicler on the data</p>\n<pre><code>singularity\
    \ run GenomeChronicler_latest.sif --bamFile=NA12878wxs.bam \n</code></pre>\n<h1>\n\
    <a id=\"user-content-command-line-options\" class=\"anchor\" href=\"#command-line-options\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Command Line Options</h1>\n<table>\n<thead>\n<tr>\n<th align=\"center\"\
    >Option</th>\n<th align=\"center\">Requirement</th>\n<th>Description</th>\n</tr>\n\
    </thead>\n<tbody>\n<tr>\n<td align=\"center\">--bamFile</td>\n<td align=\"center\"\
    >REQUIRED</td>\n<td>The path to a BAM file that has been preprocessed through\
    \ markDuplicates and VariantQualityScoreRecalibration. This can be obtained by\
    \ running the first step of the Sarek nextflow pipeline, or through other means\
    \ that do respect the general principles of the GATK Variation Calling Best Practices\
    \ workflow. Note that no variation calling is needed to run GenomeChronicler.</td>\n\
    </tr>\n<tr>\n<td align=\"center\">--vepFile</td>\n<td align=\"center\">OPTIONAL</td>\n\
    <td>For the summary tables to appear in the report, a VEP summary HTML file must\
    \ be provided. This will likely be generated if the data is from whole genome\
    \ sequencing and variants were called (e.g. by running all the germline calling\
    \ steps of the Sarek nextflow pipeline or other GATK Best Practices based workflow).\
    \ If this isn't provided, summary tables and plots will automatically be excluded\
    \ from the final report.</td>\n</tr>\n<tr>\n<td align=\"center\">--resultsDir</td>\n\
    <td align=\"center\">OPTIONAL</td>\n<td>For setting the absolute path of the results\
    \ folder to be produced when running GenomeChronicler.</td>\n</tr>\n<tr>\n<td\
    \ align=\"center\">--customTemplate</td>\n<td align=\"center\">OPTIONAL</td>\n\
    <td>For customising the output report, set this variable to the path of a custom\
    \ LaTeX file to act as a template for the report. The default templates bundled\
    \ with this software can also be found in the project github page.</td>\n</tr>\n\
    <tr>\n<td align=\"center\">--GATKthreads</td>\n<td align=\"center\">OPTIONAL</td>\n\
    <td>Number of threads to use for the GATK genotyping steps of this processing\
    \ pipeline.</td>\n</tr>\n</tbody>\n</table>\n"
  stargazers_count: 5
  subscribers_count: 2
  topics: []
  updated_at: 1617920191.0
Pathogen-Genomics-Cymru/tb-pipeline:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity.clockwork
  - singularity/Singularity.preprocessing
  full_name: Pathogen-Genomics-Cymru/tb-pipeline
  latest_release: null
  readme: '<h1>

    <a id="user-content-tb-pipeline" class="anchor" href="#tb-pipeline" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TB Pipeline</h1>

    <p>This pipeline takes as input reads presumed to be from one of 10 mycobacterial
    genomes: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare,
    kansasii, tuberculosis. Input should be in the form of one directory containing
    pairs of fastq(.gz) or bam files.</p>

    <p>Pipeline cleans and QCs reads with fastp and FastQC, classifies with Kraken2
    &amp; Mykrobe, removes non-bacterial content, and - by alignment to any minority
    genomes - disambiguates mixtures of bacterial reads. Cleaned reads are aligned
    to either of the 10 supported genomes and variants called. Produces as output
    one directory per sample, containing cleaned fastqs, sorted, indexed BAM, VCF,
    and summary reports.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>Requires <code>NXF_VER&gt;=20.11.0-edge</code></p>

    <p>The workflow is designed to run with either docker <code>-profile docker</code>
    or singularity <code>-profile singularity</code>. Before running the workflow,
    the images will need to be built by running either <code>docker/docker_build.sh</code>
    or  <code>singularity/singularity_build.sh</code></p>

    <p>E.g. to run the workflow:</p>

    <pre><code>NXF_VER=20.11.0-edge nextflow run main.nf -profile singularity --filetype
    fastq --input_dir fq_dir --pattern "*_R{1,2}.fastq.gz" --unmix_myco yes \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj


    NXF_VER=20.11.0-edge nextflow run main.nf -profile docker --filetype bam --input_dir
    bam_dir --unmix_myco no \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj

    </code></pre>

    <h2>

    <a id="user-content-params" class="anchor" href="#params" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Params</h2>

    <p>The following parameters should be set in <code>nextflow.config</code> or specified
    on the command line:</p>

    <ul>

    <li>

    <strong>input_dir</strong><br>

    Directory containing fastq OR bam files</li>

    <li>

    <strong>filetype</strong><br>

    File type in input_dir. Either "fastq" or "bam"</li>

    <li>

    <strong>pattern</strong><br>

    Regex to match fastq files in input_dir, e.g. "*_R{1,2}.fq.gz". Only mandatory
    if --filetype is "fastq"</li>

    <li>

    <strong>output_dir</strong><br>

    Output directory for results</li>

    <li>

    <strong>unmix_myco</strong><br>

    Do you want to disambiguate mixed-mycobacterial samples by read alignment? Either
    "yes" or "no":

    <ul>

    <li>If "yes" workflow will remove reads mapping to any minority mycobacterial
    genomes but in doing so WILL ALMOST CERTAINLY ALSO reduce coverage of the principal
    species</li>

    <li>If "no" then mixed-mycobacterial samples will be left alone. Mixtures of mycobacteria
    + non-mycobacteria will still be disambiguated</li>

    </ul>

    </li>

    <li>

    <strong>species</strong><br>

    Principal species in each sample, assuming genus Mycobacterium. Default ''null''.
    If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae,
    chimaera, fortuitum, intracellulare, kansasii, tuberculosis. Using this parameter
    will apply an additional sanity test to your sample

    <ul>

    <li>If you DO NOT use this parameter (default option), pipeline will determine
    principal species from the reads and consider any other species a contaminant</li>

    <li>If you DO use this parameter, pipeline will expect this to be the principal
    species. It will fail the sample if reads from this species are not actually the
    majority</li>

    </ul>

    </li>

    <li>

    <strong>kraken_db</strong><br>

    Directory containing <code>*.k2d</code> Kraken2 database files (k2_pluspf_16gb_20200919
    recommended, obtain from <a href="https://benlangmead.github.io/aws-indexes/k2"
    rel="nofollow">https://benlangmead.github.io/aws-indexes/k2</a>)</li>

    <li>

    <strong>bowtie2_index</strong><br>

    Directory containing Bowtie2 index (obtain from <a href="ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip"
    rel="nofollow">ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip</a>).
    The specified path should NOT include the index name</li>

    <li>

    <strong>bowtie_index_name</strong><br>

    Name of the bowtie index, e.g. hg19_1kgmaj<br>

    </li>

    </ul>

    <br>

    <p>For more information on the parameters run <code>nextflow run main.nf --help</code></p>

    <p>The path to the singularity images can also be changed in the singularity profile
    in <code>nextflow.config</code>. Default value is <code>${baseDir}/singularity</code></p>

    <h2>

    <a id="user-content-stub-run" class="anchor" href="#stub-run" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Stub-run</h2>

    <p>To test the stub run:</p>

    <pre><code>NXF_VER=20.11.0-edge nextflow run main.nf -stub -config testing.config

    </code></pre>

    <h2>

    <a id="user-content-checkpoints" class="anchor" href="#checkpoints" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoints</h2>

    <p>Checkpoints used throughout this workflow to fail a sample/issue warnings:</p>

    <p>processes preprocessing:checkFqValidity or preprocessing:checkBamValidity</p>

    <ol>

    <li>(Fail) If sample does not pass fqtools ''validate'' or samtools ''quickcheck'',
    as appropriate.</li>

    </ol>

    <p>process preprocessing:countReads<br>

    2. (Fail) If sample contains &lt; 100k pairs of raw reads.</p>

    <p>process preprocessing:fastp<br>

    3. (Fail) If sample contains &lt; 100k pairs of cleaned reads, required to all
    be &gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10
    --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size
    1 --cut_tail_mean_quality 20).</p>

    <p>process preprocessing:kraken2<br>

    4. (Fail) If the top family hit is not Mycobacteriaceae<br>

    5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae <br>

    6. (Warn) If the top family classification is mycobacterial, but this is not consistent
    with top genus and species classifications<br>

    7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications
    meet minimum thresholds of &gt; 5000 reads or &gt; 0.5% of the total reads (this
    is not necessarily a concern as not all mycobacteria have a taxonomic classification
    at this rank)<br>

    8. (Warn) If sample is mixed or contaminated - defined as containing reads &gt;
    the 5000/0.5% thresholds from multiple non-human species<br>

    9. (Warn) If sample contains multiple classifications to mycobacterial species
    complexes, each meeting the &gt; 5000/0.5% thresholds<br>

    10. (Warn) If no species classification meets the 5000/0.5% thresholds<br>

    11. (Warn) If no genus classification meets the 5000/0.5% thresholds</p>

    <p>process preprocessing:identifyBacterialContaminants<br>

    12. (Fail) If regardless of what Kraken reports, Mykrobe does not make a species-level
    mycobacterial classification (note that we do not use Kraken mycobacterial classifications
    other than to determine whether 100k reads are family Mycobacteriaceae; for higher-resolution
    classification, we defer to Mykrobe)<br>

    13. (Fail) If the sample is not contaminated and the top species hit is not one
    of the 10 supported Mycobacteria: abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis<br>

    14. (Fail) If the sample is not contaminated and the top species hit is contrary
    to the species expected (e.g. "avium" rather than "tuberculosis" - only tested
    if you provide that expectation)<br>

    15. (Warn) If the top Mykrobe species hit, on the basis of highest % coverage,
    does not also have the highest median depth<br>

    16. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant
    species, which means we will not be able to locate its genome, and thereby remove
    it as a contaminant<br>

    17. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated
    with a contaminant species'' taxon ID<br>

    18. (Warn) If no complete genome could be found for a contaminant species. The
    workflow will proceed with alignment-based contaminant removal, but you''re warned
    that there''s reduced confidence in detecting reads from this species</p>

    <p>process preprocessing:downloadContamGenomes<br>

    19. (Fail) If a contaminant is detected but we are unable to download a representative
    genome, and thereby remove it</p>

    <p>process preprocessing:summarise<br>

    20. (Fail) If after having taken an alignment-based approach to decontamination,
    Kraken still detects a contaminant species<br>

    21. (Fail) If after having taken an alignment-based approach to decontamination,
    the top species hit is not one of the 10 supported Mycobacteria<br>

    22. (Fail) If, after successfully removing contaminants, the top species hit is
    contrary to the species expected (e.g. "avium" rather than "tuberculosis" - only
    tested if you provide that expectation)</p>

    <p>process clockwork:alignToRef<br>

    23. (Fail) If &lt; 100k reads could be aligned to the reference genome<br>

    24. (Fail) If, after aligning to the reference genome, the average read mapping
    quality &lt; 10<br>

    25. (Fail) If &lt; 50% of the reference genome was covered at 10-fold depth</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1624463058.0
PreibischLab/BigStitcher-Singularity:
  data_format: 2
  description: Singularity container description for BigStitcher
  filenames:
  - Singularity-BigStitcher
  full_name: PreibischLab/BigStitcher-Singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-bigstitcher-singularity" class="anchor" href="#bigstitcher-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BigStitcher-Singularity</h1>

    <p>Singularity container description that automatically creates an Uber-JAR of
    the current BigStitcher version (including all dependencies) using local copy
    of the Oracle JDK.</p>

    <p>Can easily be deployed for example on a cluster for parallel resaving.</p>

    '
  stargazers_count: 0
  subscribers_count: 11
  topics: []
  updated_at: 1584624624.0
QTIM-Lab/DeepNeuro:
  data_format: 2
  description: 'A deep learning python package for neuroimaging data. Made by:'
  filenames:
  - deepneuro/pipelines/Skull_Stripping/Singularity.deepneuro_skullstripping
  - deepneuro/pipelines/Segment_Brain_Mets/Singularity.deepneuro_segment_mets
  - deepneuro/pipelines/Ischemic_Stroke/Singularity.deepneuro_segment_ischemic_stroke
  - deepneuro/pipelines/Segment_GBM/Singularity.deepneuro_segment_gbm
  full_name: QTIM-Lab/DeepNeuro
  latest_release: null
  readme: "<p><a href=\"./package_resources/logos/DeepNeuro_alt.PNG?raw=true\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"./package_resources/logos/DeepNeuro_alt.PNG?raw=true\"\
    \ alt=\"Alt text\" title=\"DeepNeuro\" style=\"max-width:100%;\"></a></p>\n<p><a\
    \ href=\"https://travis-ci.org/QTIM-Lab/DeepNeuro\" rel=\"nofollow\"><img src=\"\
    https://camo.githubusercontent.com/645f122a503a7934dcfcfc971aff595f877adc6da0142112c94b4df371fdd88d/68747470733a2f2f7472617669732d63692e6f72672f5154494d2d4c61622f446565704e6575726f2e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/QTIM-Lab/DeepNeuro.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-deepneuro\" class=\"\
    anchor\" href=\"#deepneuro\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>DeepNeuro</h1>\n<p>A deep learning python package\
    \ for neuroimaging data. Focused on validated command-line tools you can use today.\
    \ Created by the Quantitative Tumor Imaging Lab at the Martinos Center (Harvard-MIT\
    \ Program in Health, Sciences, and Technology / Massachusetts General Hospital).</p>\n\
    <h2>\n<a id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Table of Contents</h2>\n<ul>\n<li><a href=\"#about\"><g-emoji class=\"\
    g-emoji\" alias=\"question\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2753.png\"\
    >\u2753</g-emoji> About</a></li>\n<li><a href=\"#installation\"><g-emoji class=\"\
    g-emoji\" alias=\"floppy_disk\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png\"\
    >\U0001F4BE</g-emoji> Installation</a></li>\n<li><a href=\"#tutorials\"><g-emoji\
    \ class=\"g-emoji\" alias=\"mortar_board\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f393.png\"\
    >\U0001F393</g-emoji> Tutorials</a></li>\n<li><a href=\"#modules\"><g-emoji class=\"\
    g-emoji\" alias=\"gift\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f381.png\"\
    >\U0001F381</g-emoji> Modules</a></li>\n<li><a href=\"#contact\"><g-emoji class=\"\
    g-emoji\" alias=\"speech_balloon\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png\"\
    >\U0001F4AC</g-emoji> Contact</a></li>\n<li><a href=\"#citation\"><g-emoji class=\"\
    g-emoji\" alias=\"mega\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4e3.png\"\
    >\U0001F4E3</g-emoji> Citation</a></li>\n<li><a href=\"#acknowledgements\"><g-emoji\
    \ class=\"g-emoji\" alias=\"yellow_heart\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f49b.png\"\
    >\U0001F49B</g-emoji> Acknowledgements</a></li>\n</ul>\n<h2>\n<a id=\"user-content-about\"\
    \ class=\"anchor\" href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>About</h2>\n<p>DeepNeuro is an open-source\
    \ toolset of deep learning applications for neuroimaging. We have several goals\
    \ for this package:</p>\n<ul>\n<li>Provide easy-to-use command line tools for\
    \ neuroimaging using deep learning.</li>\n<li>Create Docker containers for each\
    \ tool and all out-of-package pre-processing steps, so they can each can be run\
    \ without having install prerequisite libraries.</li>\n<li>Provide freely available\
    \ deep learning models trained on a wealth of neuroimaging data.</li>\n<li>Provide\
    \ training scripts and links to publically-available data to replicate the results\
    \ of DeepNeuro's models.</li>\n<li>Provide implementations of popular models for\
    \ medical imaging data, and pre-processed datasets for educational purposes.</li>\n\
    </ul>\n<p>This package is under active development, but we encourage users to\
    \ both try the modules with pre-trained modules highlighted below, and try their\
    \ hand at making their own DeepNeuro modules using the tutorials below.</p>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<ol>\n<li>\n<p>Install Docker from Docker's website\
    \ here: <a href=\"https://www.docker.com/get-started\" rel=\"nofollow\">https://www.docker.com/get-started</a>.\
    \ Follow instructions on that link to get Docker set up properly on your workstation.</p>\n\
    </li>\n<li>\n<p>Install the Docker Engine Utility for NVIDIA GPUs, AKA nvidia-docker.\
    \ You can find installation instructions at their Github page, here: <a href=\"\
    https://github.com/NVIDIA/nvidia-docker\">https://github.com/NVIDIA/nvidia-docker</a></p>\n\
    </li>\n<li>\n<p>Pull the DeepNeuro Docker container from <a href=\"https://hub.docker.com/r/qtimlab/deepneuro_segment_gbm/\"\
    \ rel=\"nofollow\">https://hub.docker.com/r/qtimlab/deepneuro_segment_gbm/</a>.\
    \ Use the command \"docker pull qtimlab/deepneuro\"</p>\n</li>\n<li>\n<p>If you\
    \ want to run DeepNeuro outside of a Docker container, you can install the DeepNeuro\
    \ Python package locally using the pip package manager. On the command line, run\
    \ <code>pip install deepneuro</code></p>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-tutorials\"\
    \ class=\"anchor\" href=\"#tutorials\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Tutorials</h2>\n<p align=\"center\"\
    >\n<a href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Preprocess_and_Augment.ipynb\"\
    \ rel=\"nofollow\">\n<img src=\"./notebooks/resources/train_preprocess_icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Train_Model.ipynb\"\
    \ rel=\"nofollow\">\n<img src=\"./notebooks/resources/train_model_icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Run_Inference.ipynb\"\
    \ rel=\"nofollow\">\n<img src=\"./notebooks/resources/model_inference_icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<h2>\n<a id=\"\
    user-content-modules\" class=\"anchor\" href=\"#modules\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Modules</h2>\n\
    <p align=\"center\">\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Segment_GBM\"\
    >\n<img src=\"./deepneuro/pipelines/Segment_GBM/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Skull_Stripping\"\
    >\n<img src=\"./deepneuro/pipelines/Skull_Stripping/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Segment_Brain_Mets\"\
    >\n<img src=\"./deepneuro/pipelines/Segment_Brain_Mets/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Ischemic_Stroke\"\
    >\n<img src=\"./deepneuro/pipelines/Ischemic_Stroke/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<h2>\n<a id=\"\
    user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citation</h2>\n\
    <p>If you use DeepNeuro in your published work, please cite:</p>\n<p>Beers, A.,\
    \ Brown, J., Chang, K., Hoebel, K., Patel, J., Ly, K. Ina, Tolaney, S.M., Brastianos,\
    \ P., Rosen, B., Gerstner, E., and Kalpathy-Cramer, J. (2020). <a href=\"https://link.springer.com/article/10.1007/s12021-020-09477-5\"\
    \ rel=\"nofollow\">DeepNeuro: an open-source deep learning toolbox for neuroimaging</a>.\
    \ Neuroinformatics. DOI: 10.1007/s12021-020-09477-5. PMID: 32578020</p>\n<p>If\
    \ you use the MRI skull-stripping or glioblastoma segmentation modules, please\
    \ cite:</p>\n<p>Chang, K., Beers, A.L., Bai, H.X., Brown, J.M., Ly, K.I., Li,\
    \ X., Senders, J.T., Kavouridis, V.K., Boaro, A., Su, C., Bi, W.L., Rapalino,\
    \ O., Liao, W., Shen, Q., Zhou, H., Xiao, B., Wang, Y., Zhang, P.J., Pinho, M.C.,\
    \ Wen, P.Y., Batchelor, T.T., Boxerman, J.L., Arnaout, O., Rosen, B.R., Gerstner,\
    \ E.R., Yang, L., Huang, R.Y., and Kalpathy-Cramer, J., 2019. <a href=\"https://academic.oup.com/neuro-oncology/advance-article/doi/10.1093/neuonc/noz106/5514498?searchresult=1\"\
    \ rel=\"nofollow\">Automatic assessment of glioma burden: A deep learning algorithm\
    \ for fully automated volumetric and bi-dimensional measurement</a>. Neuro-Oncology.\
    \ DOI: 10.1093/neuonc/noz106. PMID: 31190077</p>\n<h2>\n<a id=\"user-content-contact\"\
    \ class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contact</h2>\n<p>DeepNeuro is\
    \ under active development, and you may run into errors or want additional features.\
    \ Send any questions or requests for methods to <a href=\"mailto:qtimlab@gmail.com\"\
    >qtimlab@gmail.com</a>. You can also submit a Github issue if you run into a bug.</p>\n\
    <h2>\n<a id=\"user-content-acknowledgements\" class=\"anchor\" href=\"#acknowledgements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Acknowledgements</h2>\n<p>The Center for Clinical Data Science at\
    \ Massachusetts General Hospital and the Brigham and Woman's Hospital provided\
    \ technical and hardware support for the development of DeepNeuro, including access\
    \ to graphics processing units. The DeepNeuro project is also indebted to the\
    \ following <a href=\"https://github.com/ellisdg/3DUnetCNN\">Github repository</a>\
    \ for the 3D UNet by user ellisdg, which formed the original kernel for much of\
    \ its code in early stages. Long live open source deep learning :)</p>\n<h2>\n\
    <a id=\"user-content-disclaimer\" class=\"anchor\" href=\"#disclaimer\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Disclaimer</h2>\n\
    <p>This software package and the deep learning models within are intended for\
    \ research purposes only and have not yet been validated for clinical use.</p>\n"
  stargazers_count: 104
  subscribers_count: 15
  topics: []
  updated_at: 1622068798.0
QsingularityAi/polar-pfc-master_active-crystel:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: QsingularityAi/polar-pfc-master_active-crystel
  latest_release: null
  readme: '<h1>

    <a id="user-content-polar-pfc-master_active-crystel" class="anchor" href="#polar-pfc-master_active-crystel"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>polar-pfc-master_active-crystel</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624399268.0
RBigData/singularity:
  data_format: 2
  description: Singularity configurations for R and pbdR packages.
  filenames:
  - pbdR/pbdR/openmpi/Singularity.1.0-1
  - pbdR/pbdR/mpich/Singularity.1.0-1
  - pbdR/pbdR-minimal/openmpi/Singularity.1.0-1
  - pbdR/pbdR-minimal/mpich/Singularity.1.0-1
  - R/rstudio-server/Singularity.1.1.456
  - R/rstudio/Singularity.1.1.456
  - R/r/Singularity.3.5.1
  - R/jupyter/Singularity
  - R/r-minimal/Singularity.3.5.1
  full_name: RBigData/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-r-and-pbdr-singularity-recipes" class="anchor" href="#r-and-pbdr-singularity-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>R
    and pbdR Singularity Recipes</h1>

    <p>Singularity recipes for R and pbdR.</p>

    <p>Build requirements:</p>

    <ul>

    <li>

    <a href="https://www.sylabs.io/" rel="nofollow">singularity</a> &gt;= 2.3</li>

    <li>Modify the <code>make -j</code> line of each recipe to your liking.</li>

    </ul>

    '
  stargazers_count: 3
  subscribers_count: 4
  topics: []
  updated_at: 1571942199.0
ResearchIT/MolecularGraphicsToolbox:
  data_format: 2
  description: null
  filenames:
  - Singularity.centos7.tbx-MG
  full_name: ResearchIT/MolecularGraphicsToolbox
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterisationvl-software\" class=\"anchor\"\
    \ href=\"#characterisationvl-software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CharacterisationVL-Software</h1>\n\
    <p>The purpose of this repository is for storing definition files to submit to\
    \ <a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity Hub.</a></p>\n\
    <p>If you are new to Singularity containers, please refer to <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.5/user-guide/</a> or a newer version\
    \ of this documentation.</p>\n<p>Each software package is located in its own folder.\
    \ The files are tagged with the software name and version number or date of build.\
    \ Please read below for the naming convention.</p>\n<p>To add software to the\
    \ repository you will need to create a new branch. The new branch is the name\
    \ of the software product. By convention, the new branch will be checked and merged\
    \ into the master branch and then deleted.</p>\n<h2>\n<a id=\"user-content-steps-to-add-a-software-package\"\
    \ class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Steps to\
    \ add a software package</h2>\n<ol>\n<li>Clone this repository</li>\n<li>Create\
    \ a branch</li>\n</ol>\n<pre><code>$ git branch &lt;software name&gt;\n</code></pre>\n\
    <ol start=\"3\">\n<li>Make a subdirectory for the software product.</li>\n</ol>\n\
    <pre><code>$ mkdir &lt;software name&gt;\n</code></pre>\n<ol start=\"4\">\n<li>Add\
    \ all the necessary files.</li>\n</ol>\n<ul>\n<li>Singularity definition file\
    \ or installation script</li>\n<li>Readme file including install and testing notes</li>\n\
    <li>Desktop files for adding to menus with necessary tags</li>\n<li>For full details,\
    \ <a href=\"template/README.md\">please refer to the 'template' folder in this\
    \ repository.</a>\n</li>\n</ul>\n<ol start=\"4\">\n<li>Commit all changes, including\
    \ a helpful message</li>\n</ol>\n<pre><code>$ git commit -m \"&lt;software name&gt;\
    \ added as requested in support ticket\"\n</code></pre>\n<ol start=\"6\">\n<li>Push\
    \ to the remote repository. i.e. this one.</li>\n<li>Submit merge request</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Naming your Singularity definition file, Singularity Hub and Licensing</h2>\n\
    <p>For all Singularity recipes where the software licensing permits redistribution,\
    \ please use this naming convention:</p>\n<pre><code>   Singularity.applicationName_version\n\
    \   Singularity.applicationName_version-cuda-cudaVersion\n\n</code></pre>\n<p>This\
    \ is where Singularity Hub fits into the equation. There is a webhook between\
    \ this repository and <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >Singularity Hub</a>. When a commit is merged into the master branch, Singularity\
    \ Hub will build the container.</p>\n<p>If successfully built, the path to the\
    \ container on Singularity Hub is:</p>\n<pre><code>  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n\
    \  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\
    \n</code></pre>\n<p>For software where licensing does not support redistribution,\
    \ the container recipe can still be defined, but the container should not be built\
    \ on Singularity Hub.</p>\n<p>An example on how to handle this situation is the\
    \ recipe for CCP-EM.\nThe <a href=\"ccp-em/README.md\">README.md</a> contains\
    \ a section on Prerequisites. This section lists the required files to build the\
    \ container. The license must be accepted by the end user to obtain them.</p>\n\
    <p>Prerequisite files should not be committed to this repository.</p>\n<p>To prevent\
    \ Singularity Hub from attempting to build the container, we simply use a different\
    \ recipe naming convention as follows:</p>\n<pre><code>   applicationName_version.def\n\
    \   applicationName_version-cuda-cudaVersion.def\n\n</code></pre>\n<h2>\n<a id=\"\
    user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Ubuntu Base Images</h2>\n<p>The folder 'ubuntu-base-image' contains\
    \ recipes for pre built base containers. These can be used as a starting point\
    \ to aid/speed up the development of your container recipe.</p>\n<p>The current\
    \ versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.</p>\n\
    <p>These are available on Singularity Hub.</p>\n<p>For example: from the Graphviz\
    \ Singularity.graphviz-2.40.1 recipe</p>\n<pre><code>Bootstrap: shub\nFrom:  \
    \    Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n</code></pre>\n\
    <p>These two lines, will tell Singularity to use the 'shub' bootstrap to obtain\
    \ the '1804' ubuntu-base-image container from Singularity Hub.</p>\n<p>From here\
    \ you just need to add the requirements to build a container for your required\
    \ piece of software. Please see <a href=\"graphviz/Singularity.graphviz-2.40.1\"\
    >Singularity.graphviz-2.40.1</a>\nfor the full recipe.</p>\n<p>The current ubuntu-base-images\
    \ include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.</p>\n\
    <h2>\n<a id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"\
    anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ GUI applications on a non-GPU node</h2>\n<p>The applications in the Singularity\
    \ container should run without the need for a dedicated GPU.</p>\n<p>However,\
    \ an X server needs to be running for this to work. On nodes with GPU, X Server\
    \ is started with NVIDIA driver, and on non-GPU nodes, the X Server is started\
    \ with MESA library.</p>\n<p>X Server can be started during boot (for example,\
    \ using <code>systemctl set-default graphical.target</code>).</p>\n<p>Make sure\
    \ that VirtualGL package is installed in the container. The code below will download\
    \ and install VirtualGL.</p>\n<pre><code>wget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\
    \ndpkg -i virtualgl_2.6.2_amd64.deb\n</code></pre>\n<p>The application startup\
    \ script doesn't need to be modified, however, if the application needs to be\
    \ manually started, then <code>vglrun</code> needs to be appended before running\
    \ the application. For example: <code>singularity exec --nv -B /projects:/projects\
    \ -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX</code></p>\n\
    <p><a href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 2
  subscribers_count: 6
  topics: []
  updated_at: 1582812783.0
ResearchIT/NMRPipe:
  data_format: 2
  description: Singularity recipe for NMRPipe
  filenames:
  - Singularity
  - Singularity.212_64
  full_name: ResearchIT/NMRPipe
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-nmrpipe" class="anchor" href="#singularity-recipe-for-nmrpipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for NMRPipe</h1>

    <p>This repo contains the recipe to run <a href="https://www.ibbr.umd.edu/nmrpipe/"
    rel="nofollow">NMRPipe</a>

    within a <a href="https://singularity.lbl.gov" rel="nofollow">Singularity</a>
    container, which can be built using <a href="https://singularity-hub.org" rel="nofollow">Singularity
    Hub</a></p>

    <p>Versions:</p>

    <ul>

    <li>212_64 - NMRPipe linux212_64 built on centos7.4</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1523030864.0
ResearchIT/Scanfold:
  data_format: 2
  description: Singularity container for Scanfold
  filenames:
  - Singularity
  full_name: ResearchIT/Scanfold
  latest_release: null
  readme: '<h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667"
    alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/6b7af09ab5d3e54feb3acda4c7b70aef9718f2928a49a50c92ea6ce95e96b2f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e657874666c6f772d254532253839254135302e32352e312d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/Nextflow-%E2%89%A50.25.1-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p>The goal of many metagenomics studies is to characterize the content and relative
    abundance of sequences of interest from the DNA of a given sample or set of samples.
    You may want to know what is contained within your sample or how abundant a given
    sequence is relative to another.</p>

    <p>Often, metagenomics is performed when the answer to these questions must be
    obtained for a large number of targets where techniques like multiplex PCR and
    other targeted methods would be too cumbersome to perform. AmrPlusPlus can process
    the raw data from the sequencer, identify the fragments of DNA, and count them.
    It also provides a count of the polymorphisms that occur in each DNA fragment
    with respect to the reference database.</p>

    <p>Additionally, you may want to know if the depth of your sequencing (how many
    reads you obtain that are on target) is high enough to identify rare organisms
    (organisms with low abundance relative to others) in your population. This is
    referred to as rarefaction and is calculated by randomly subsampling your sequence
    data at intervals between 0% and 100% in order to determine how many targets are
    found at each depth. AmrPlusPlus can perform this analysis as well.</p>

    <p>With AmrPlusPlus, you will obtain count files for each sample that can be combined
    into a count matrix and analyzed using any statistical and mathematical techniques
    that can operate on a matrix of observations.</p>

    <h2>

    <a id="user-content-more-information" class="anchor" href="#more-information"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More
    Information</h2>

    <ul>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/requirements.md">Software
    Requirements</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/installation.md">Installation</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/usage.md">Usage</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/configuration.md">Configuration</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/output.md">Output</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/dependencies.md">Dependencies</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/contact.md">Contact</a></li>

    </ul>

    <h2>

    <a id="user-content-description-of-scripts" class="anchor" href="#description-of-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Description
    of scripts</h2>

    <p>main_qiime2.nf</p>

    <pre><code>nextflow run main_qiime2.nf --reads "/s/angus/index/projs/mega_tylan/concat_16S_LN/raw_data/*_{1,2}.fq"
    --output XIT_LN_qiime2 -profile local --metadata /media/AngusWorkspace/run_Jake/LN_metadata.tsv
    --classifier /media/AngusWorkspace/run_Jake/bioinformatic-nextflow-pipelines/gg-13-8-99-515-806-nb-classifier.qza
    -resume --threads 25

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1570729149.0
ResearchIT/SimNIBS:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ResearchIT/SimNIBS
  latest_release: null
  readme: '<h3>

    <a id="user-content-simnibs-singularity-recipe" class="anchor" href="#simnibs-singularity-recipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SimNIBS
    singularity recipe</h3>

    <p>Before building, place the SimNIBS source tarball in the /tmp directory. (recipe
    version 2.1.1)</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1546981375.0
ResearchIT/nwchem:
  data_format: 2
  description: Singularity Recipe for NWChem
  filenames:
  - Singularity.6.6-openmpi
  full_name: ResearchIT/nwchem
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-nwchem" class="anchor" href="#singularity-recipe-for-nwchem"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for NWChem</h1>

    <p>This repo contains recipes to run <a href="http://www.nwchem-sw.org/index.php/Main_Page"
    rel="nofollow">NWChem</a>

    within a <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a>
    container, which can be built

    using <a href="https://singularity-hub.org/" rel="nofollow">Singularity Hub</a></p>

    <p>Versions:</p>

    <ul>

    <li>6.6 - NWChem with OpenMPI installed via EPEL</li>

    </ul>

    <h2>

    <a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to Use:</h2>

    <p>You need to have openmpi v1 installed on your local machine (via yum or as
    a module).

    Testing was performed with openmpi 1.10.6.</p>

    <p>Run example:</p>

    <p>mpirun -np 2 singularity run shub://ResearchIT/nwchem:6.6-openmpi test.nw</p>

    <h2>

    <a id="user-content-alternative-method" class="anchor" href="#alternative-method"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Alternative
    method:</h2>

    <p>use the provided bash wrapper and module file to use the nwchem singularity
    container like a standard module

    (this assumes you have a singularity/2.4 and openmpi/1 modules)</p>

    <p>e.g.</p>

    <p>module load nwchem/6.6</p>

    <p>mpirun -np 2 nwchem test.nw</p>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics:
  - nwchem
  - singularity
  updated_at: 1551769652.0
ResearchIT/qiime2:
  data_format: 2
  description: Singularity Recipe for QIIME 2
  filenames:
  - Singularity.2019.10
  - Singularity.2019.4
  - Singularity.2017.12
  - Singularity
  - Singularity.2018.2
  - Singularity.2018.6
  full_name: ResearchIT/qiime2
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-qiime2" class="anchor" href="#singularity-recipe-for-qiime2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for QIIME2</h1>

    <p>This repo contains recipe run <a href="https://qiime2.org" rel="nofollow">qiime2</a>
    within a

    <a href="https://singularity.lbl.gov/" rel="nofollow">Singularity</a> container,
    which can be built

    using <a href="https://singularity-hub.org/" rel="nofollow">Singularity Hub</a></p>

    <p>Versions:

    2017.12 - QIIME2-2017.12 installed on CentOS 7</p>

    <h2>

    <a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to Use:</h2>

    <p>singularity run shub://ResearchIT/qiime2 --help</p>

    '
  stargazers_count: 3
  subscribers_count: 6
  topics:
  - qiime
  - singularity
  updated_at: 1576035067.0
ResearchIT/revbayes-singularity:
  data_format: 2
  description: Singularity container for https://github.com/revbayes/revbayes
  filenames:
  - Singularity
  full_name: ResearchIT/revbayes-singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3722" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PLINK association analysis toolset</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1589324901.0
ResearchIT/scanindel:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ResearchIT/scanindel
  latest_release: null
  readme: '<h3>

    <a id="user-content-scanindel-singularity-recipe" class="anchor" href="#scanindel-singularity-recipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ScanIndel
    Singularity recipe</h3>

    <p>ScanIndel is a python program to detect indels (insertions and deletions) from
    NGS data by re-align and de novo assemble soft clipped reads.</p>

    <p>Original repository <a href="https://github.com/cauyrd/ScanIndel">here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1539032220.0
ResearchIT/scipion:
  data_format: 2
  description: Singularity Recipe for scipion
  filenames:
  - Singularity.2.0.cuda
  - Singularity.2.0
  - Singularity.1.1
  - Singularity.1.1.cuda
  full_name: ResearchIT/scipion
  latest_release: null
  readme: '<p><a href="https://zenodo.org/badge/latestdoi/124456755" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4641d504e79e577f2add43b190e60f3910a1688ac8f26f972d799fd6f3f4b213/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3132343435363735352e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/124456755.svg" style="max-width:100%;"></a>
    <a href="https://github.com/ambv/black"><img src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667"
    alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    class="anchor" href="#ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ancient
    hybridization and adaptive introgression of an invadolysin gene in <em>Schistosoma
    haematobium</em>.</h1>

    <p>Roy N. Platt II, Marina McDew-White, Winka Le Clec''h, Frederic D. Chevalier,
    Fiona Allan, Aidan M. Emery, Amadou Garba, Shaali M. Ame, Joanne P. Webster, David
    Rollinson, Bonnie L. Webster, Timothy J. C. Anderson.</p>

    <p>The parasitic blood fluke <em>Schistosoma</em> <em>haematobium</em> causes
    urogenital schistosomiasis in humans and is a major cause of morbidity and mortality
    across sub-Saharan Africa. <em>S</em>. <em>haematobium</em> can hybridize with
    closely-related livestock schistosomes, including <em>S</em>. <em>bovis</em>,
    however the frequency, direction, age and genomic consequences of hybridization
    in nature are unknown. We sequenced 96 <em>S</em>. <em>haematobium</em> exomes
    from Niger and the Zanzibar archipelago. We found evidence of an ancient, adaptive
    introgression event between Nigerien <em>S</em>. <em>haematobium</em> and <em>S</em>.
    <em>bovis</em> occurring 108-613 generations ago. Introgressed S. bovis alleles
    constitute 3.3-8.2% of Nigerien <em>S</em>. <em>haematobium</em> genomes. Some
    <em>S</em>. <em>bovis</em> alleles have reached high frequency and show signatures
    of directional selection; the strongest signal spans a single gene in the invadolysin
    gene family, an M8 metalloprotease associated with parasitic life-history traits.</p>

    <h4>

    <a id="user-content-biorxiv-pre-print" class="anchor" href="#biorxiv-pre-print"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="https://doi.org/10.1101/539353" rel="nofollow">bioRxiv pre-print</a>

    </h4>

    <hr>

    <h3>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTES:</h3>

    <p>All analyses were conducted on a HPCC in a <code>singularity</code> container
    or in a <code>conda</code> managed environment. The singularity recipe and conda
    environmental yaml are in the <code>config</code> dir.</p>

    <p>Raw code is found in the <code>scripts</code> dir</p>

    <p>Data that is not readily available through the SRA is in the <code>data</code>
    dir.  These will be housed in an online repository (ex. Dryad), but provided here
    for documentation purposes.</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics:
  - singularity
  - scipion
  updated_at: 1592515044.0
ResearchIT/singularity-freesurfer:
  data_format: 2
  description: Singularity recipe for freesurfer
  filenames:
  - Singularity
  full_name: ResearchIT/singularity-freesurfer
  latest_release: null
  readme: '<h1>

    <a id="user-content-uresnet-tomo-seg" class="anchor" href="#uresnet-tomo-seg"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>uresnet-tomo-seg</h1>

    <p>uresnet based deep neutral network for the segmentation of high resolution
    cryo-EM tomographs</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1603915556.0
ResearchIT/spack-singularity:
  data_format: 2
  description: Singularity containers with software installed via Spack
  filenames:
  - Singularity.busco
  - Singularity.gcc
  - Singularity.spack
  - Singularity.trinity
  - Singularity.openmpi
  full_name: ResearchIT/spack-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7-reprozipfslbuild-centos5" class="anchor" href="#centos7-reprozipfslbuild-centos5"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>centos7-reprozip.fslbuild-centos5</h1>

    <p>PreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)</p>

    '
  stargazers_count: 1
  subscribers_count: 7
  topics:
  - spack
  - openmpi
  - singularity
  updated_at: 1557449681.0
ResearchIT/tofu2:
  data_format: 2
  description: Singularity Recipe for Tofu2
  filenames:
  - Singularity
  - Singularity.v17
  full_name: ResearchIT/tofu2
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-tofu2" class="anchor" href="#singularity-recipe-for-tofu2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for Tofu2</h1>

    <p>This repo contains recipes to run <a href="https://github.com/PacificBiosciences/IsoSeq_SA3nUP/wiki/%5BBeta%5D-ToFU2:-running-and-installing-ToFU2#install">Tofu2</a>

    within a <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a>
    container, which can be built

    using <a href="https://singularity-hub.org/" rel="nofollow">Singularity Hub</a></p>

    <p>Versions:</p>

    <ul>

    <li>v17 - Tofu2 installed on Ubuntu</li>

    </ul>

    <h2>

    <a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to Use:</h2>

    <p>Run example:</p>

    <p>singularity run shub://ResearchIT/tofu2 run_preCluster.py --cpus=4</p>

    <h2>

    <a id="user-content-alternative-method" class="anchor" href="#alternative-method"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Alternative
    method:</h2>

    <p>use the provided bash wrapper and module file to use the tofu2 singularity
    container like a standard module

    (this assumes you have a singularity/2.4 module)</p>

    <p>e.g.</p>

    <p>module load tofu2/v17

    tofu2 run_preCluster.py --cpus=4</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics:
  - tofu
  - pacbio
  - singularity
  updated_at: 1522255502.0
SCXsunchenxi/Auto-Pytorch:
  data_format: 2
  description: null
  filenames:
  - scripts/Singularity
  full_name: SCXsunchenxi/Auto-Pytorch
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-auto-pytorch\" class=\"anchor\" href=\"#auto-pytorch\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Auto-PyTorch</h1>\n<p>Copyright (C) 2019  <a href=\"http://www.automl.org/\"\
    \ rel=\"nofollow\">AutoML Group Freiburg</a></p>\n<p>This a very early pre-alpha\
    \ version of our upcoming Auto-PyTorch.\nSo far, Auto-PyTorch supports featurized\
    \ data (classification, regression) and image data (classification).</p>\n<p>The\
    \ newest features in Auto-PyTorch for tabular data are described in the paper\
    \ <a href=\"https://arxiv.org/abs/2006.13799\" rel=\"nofollow\">\"Auto-PyTorch\
    \ Tabular: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL\"</a>.</p>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>Clone repository</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ <span class=\"pl-c1\">cd</span> install/path\n\
    $ git clone https://github.com/automl/Auto-PyTorch.git\n$ <span class=\"pl-c1\"\
    >cd</span> Auto-PyTorch</pre></div>\n<p>If you want to contribute to this repository\
    \ switch to our current develop branch</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ git checkout develop</pre></div>\n<p>Install pytorch:\n<a href=\"https://pytorch.org/\"\
    \ rel=\"nofollow\">https://pytorch.org/</a></p>\n<p>Install Auto-PyTorch:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ cat requirements.txt <span\
    \ class=\"pl-k\">|</span> xargs -n 1 -L 1 pip install\n$ python setup.py install</pre></div>\n\
    <h2>\n<a id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Examples</h2>\n\
    <p>Code for the <a href=\"https://arxiv.org/abs/2006.13799\" rel=\"nofollow\"\
    >paper</a> is available under <code>examples/ensemble</code>.</p>\n<p>For a detailed\
    \ tutorial, please refer to the jupyter notebook in <a href=\"https://github.com/automl/Auto-PyTorch/tree/master/examples/basics\"\
    >https://github.com/automl/Auto-PyTorch/tree/master/examples/basics</a>.</p>\n\
    <p>In a nutshell:</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-k\">from</span> <span class=\"pl-s1\">autoPyTorch</span> <span class=\"\
    pl-k\">import</span> <span class=\"pl-v\">AutoNetClassification</span>\n\n<span\
    \ class=\"pl-c\"># data and metric imports</span>\n<span class=\"pl-k\">import</span>\
    \ <span class=\"pl-s1\">sklearn</span>.<span class=\"pl-s1\">model_selection</span>\n\
    <span class=\"pl-k\">import</span> <span class=\"pl-s1\">sklearn</span>.<span\
    \ class=\"pl-s1\">datasets</span>\n<span class=\"pl-k\">import</span> <span class=\"\
    pl-s1\">sklearn</span>.<span class=\"pl-s1\">metrics</span>\n<span class=\"pl-v\"\
    >X</span>, <span class=\"pl-s1\">y</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-s1\">sklearn</span>.<span class=\"pl-s1\">datasets</span>.<span class=\"\
    pl-en\">load_digits</span>(<span class=\"pl-s1\">return_X_y</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-v\">X_train</span>,\
    \ <span class=\"pl-v\">X_test</span>, <span class=\"pl-s1\">y_train</span>, <span\
    \ class=\"pl-s1\">y_test</span> <span class=\"pl-c1\">=</span> \\\n        <span\
    \ class=\"pl-s1\">sklearn</span>.<span class=\"pl-s1\">model_selection</span>.<span\
    \ class=\"pl-en\">train_test_split</span>(<span class=\"pl-v\">X</span>, <span\
    \ class=\"pl-s1\">y</span>, <span class=\"pl-s1\">random_state</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"># running\
    \ Auto-PyTorch</span>\n<span class=\"pl-s1\">autoPyTorch</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-v\">AutoNetClassification</span>(<span class=\"\
    pl-s\">\"tiny_cs\"</span>,  <span class=\"pl-c\"># config preset</span>\n    \
    \                                <span class=\"pl-s1\">log_level</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">'info'</span>,\n                        \
    \            <span class=\"pl-s1\">max_runtime</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">300</span>,\n                                    <span class=\"\
    pl-s1\">min_budget</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">30</span>,\n\
    \                                    <span class=\"pl-s1\">max_budget</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">90</span>)\n\n<span class=\"pl-s1\"\
    >autoPyTorch</span>.<span class=\"pl-en\">fit</span>(<span class=\"pl-v\">X_train</span>,\
    \ <span class=\"pl-s1\">y_train</span>, <span class=\"pl-s1\">validation_split</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">0.3</span>)\n<span class=\"pl-s1\"\
    >y_pred</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">autoPyTorch</span>.<span\
    \ class=\"pl-en\">predict</span>(<span class=\"pl-v\">X_test</span>)\n\n<span\
    \ class=\"pl-en\">print</span>(<span class=\"pl-s\">\"Accuracy score\"</span>,\
    \ <span class=\"pl-s1\">sklearn</span>.<span class=\"pl-s1\">metrics</span>.<span\
    \ class=\"pl-en\">accuracy_score</span>(<span class=\"pl-s1\">y_test</span>, <span\
    \ class=\"pl-s1\">y_pred</span>))</pre></div>\n<p>More examples with datasets:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ <span class=\"pl-c1\">cd</span>\
    \ examples/\n</pre></div>\n<h2>\n<a id=\"user-content-configuration\" class=\"\
    anchor\" href=\"#configuration\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Configuration</h2>\n<p>How to configure\
    \ Auto-PyTorch for your needs:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre>\n<span class=\"pl-c\"># Print all possible configuration options.</span>\n\
    <span class=\"pl-v\">AutoNetClassification</span>().<span class=\"pl-en\">print_help</span>()\n\
    \n<span class=\"pl-c\"># You can use the constructor to configure Auto-PyTorch.</span>\n\
    <span class=\"pl-s1\">autoPyTorch</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-v\">AutoNetClassification</span>(<span class=\"pl-s1\">log_level</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s\">'info'</span>, <span class=\"pl-s1\"\
    >max_runtime</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">300</span>,\
    \ <span class=\"pl-s1\">min_budget</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">30</span>, <span class=\"pl-s1\">max_budget</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-c1\">90</span>)\n\n<span class=\"pl-c\"># You can overwrite\
    \ this configuration in each fit call.</span>\n<span class=\"pl-s1\">autoPyTorch</span>.<span\
    \ class=\"pl-en\">fit</span>(<span class=\"pl-v\">X_train</span>, <span class=\"\
    pl-s1\">y_train</span>, <span class=\"pl-s1\">log_level</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s\">'debug'</span>, <span class=\"pl-s1\">max_runtime</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">900</span>, <span class=\"pl-s1\"\
    >min_budget</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">50</span>,\
    \ <span class=\"pl-s1\">max_budget</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">150</span>)\n\n<span class=\"pl-c\"># You can use presets to configure\
    \ the config space.</span>\n<span class=\"pl-c\"># Available presets: full_cs,\
    \ medium_cs (default), tiny_cs.</span>\n<span class=\"pl-c\"># These are defined\
    \ in autoPyTorch/core/presets.</span>\n<span class=\"pl-c\"># tiny_cs is recommended\
    \ if you want fast results with few resources.</span>\n<span class=\"pl-c\">#\
    \ full_cs is recommended if you have many resources and a very high search budget.</span>\n\
    <span class=\"pl-s1\">autoPyTorch</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-v\">AutoNetClassification</span>(<span class=\"pl-s\">\"full_cs\"\
    </span>)\n\n<span class=\"pl-c\"># Enable or disable components using the Auto-PyTorch\
    \ config:</span>\n<span class=\"pl-s1\">autoPyTorch</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-v\">AutoNetClassification</span>(<span class=\"pl-s1\"\
    >networks</span><span class=\"pl-c1\">=</span>[<span class=\"pl-s\">\"resnet\"\
    </span>, <span class=\"pl-s\">\"shapedresnet\"</span>, <span class=\"pl-s\">\"\
    mlpnet\"</span>, <span class=\"pl-s\">\"shapedmlpnet\"</span>])\n\n<span class=\"\
    pl-c\"># You can take a look at the search space.</span>\n<span class=\"pl-c\"\
    ># Each hyperparameter belongs to a node in Auto-PyTorch's ML Pipeline.</span>\n\
    <span class=\"pl-c\"># The names of the hyperparameters are prefixed with the\
    \ name of the node: NodeName:hyperparameter_name.</span>\n<span class=\"pl-c\"\
    ># If a hyperparameter belongs to a component: NodeName:component_name:hyperparameter_name.</span>\n\
    <span class=\"pl-c\"># Call with the same arguments as fit.</span>\n<span class=\"\
    pl-s1\">autoPyTorch</span>.<span class=\"pl-en\">get_hyperparameter_search_space</span>(<span\
    \ class=\"pl-v\">X_train</span>, <span class=\"pl-s1\">y_train</span>, <span class=\"\
    pl-s1\">validation_split</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >0.3</span>)\n\n<span class=\"pl-c\"># You can configure the search space of every\
    \ hyperparameter of every component:</span>\n<span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">autoPyTorch</span> <span class=\"pl-k\">import</span>\
    \ <span class=\"pl-v\">HyperparameterSearchSpaceUpdates</span>\n<span class=\"\
    pl-s1\">search_space_updates</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-v\">HyperparameterSearchSpaceUpdates</span>()\n\n<span class=\"pl-s1\">search_space_updates</span>.<span\
    \ class=\"pl-en\">append</span>(<span class=\"pl-s1\">node_name</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">\"NetworkSelector\"</span>,\n           \
    \                 <span class=\"pl-s1\">hyperparameter</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s\">\"shapedresnet:activation\"</span>,\n          \
    \                  <span class=\"pl-s1\">value_range</span><span class=\"pl-c1\"\
    >=</span>[<span class=\"pl-s\">\"relu\"</span>, <span class=\"pl-s\">\"sigmoid\"\
    </span>])\n<span class=\"pl-s1\">search_space_updates</span>.<span class=\"pl-en\"\
    >append</span>(<span class=\"pl-s1\">node_name</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s\">\"NetworkSelector\"</span>,\n                            <span\
    \ class=\"pl-s1\">hyperparameter</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-s\">\"shapedresnet:blocks_per_group\"</span>,\n                           \
    \ <span class=\"pl-s1\">value_range</span><span class=\"pl-c1\">=</span>[<span\
    \ class=\"pl-c1\">2</span>,<span class=\"pl-c1\">5</span>],\n                \
    \            <span class=\"pl-s1\">log</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">False</span>)\n<span class=\"pl-s1\">autoPyTorch</span> <span\
    \ class=\"pl-c1\">=</span> <span class=\"pl-v\">AutoNetClassification</span>(<span\
    \ class=\"pl-s1\">hyperparameter_search_space_updates</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s1\">search_space_updates</span>)</pre></div>\n<p>Enable\
    \ ensemble building (for featurized data):</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">autoPyTorch</span>\
    \ <span class=\"pl-k\">import</span> <span class=\"pl-v\">AutoNetEnsemble</span>\n\
    <span class=\"pl-s1\">autoPyTorchEnsemble</span> <span class=\"pl-c1\">=</span>\
    \ <span class=\"pl-v\">AutoNetEnsemble</span>(<span class=\"pl-v\">AutoNetClassification</span>,\
    \ <span class=\"pl-s\">\"tiny_cs\"</span>, <span class=\"pl-s1\">max_runtime</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">300</span>, <span class=\"pl-s1\"\
    >min_budget</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">30</span>,\
    \ <span class=\"pl-s1\">max_budget</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">90</span>)</pre></div>\n<p>Disable pynisher if you experience issues when\
    \ using cuda:</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-s1\">autoPyTorch</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-v\">AutoNetClassification</span>(<span class=\"pl-s\">\"tiny_cs\"</span>, <span\
    \ class=\"pl-s1\">log_level</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-s\">'info'</span>, <span class=\"pl-s1\">max_runtime</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-c1\">300</span>, <span class=\"pl-s1\">min_budget</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">30</span>, <span class=\"pl-s1\"\
    >max_budget</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">90</span>,\
    \ <span class=\"pl-s1\">cuda</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">True</span>, <span class=\"pl-s1\">use_pynisher</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-c1\">False</span>)</pre></div>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>This program\
    \ is free software: you can redistribute it and/or modify\nit under the terms\
    \ of the Apache license 2.0 (please see the LICENSE file).</p>\n<p>This program\
    \ is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY;\
    \ without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR\
    \ PURPOSE.</p>\n<p>You should have received a copy of the Apache license 2.0\n\
    along with this program (see LICENSE file).</p>\n<h2>\n<a id=\"user-content-reference\"\
    \ class=\"anchor\" href=\"#reference\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Reference</h2>\n<div class=\"\
    highlight highlight-text-bibtex\"><pre><span class=\"pl-k\">@incollection</span>{<span\
    \ class=\"pl-en\">mendoza-automlbook18a</span>,\n  <span class=\"pl-s\">author</span>\
    \    = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Hector Mendoza and\
    \ Aaron Klein and Matthias Feurer and Jost Tobias Springenberg and Matthias Urban\
    \ and Michael Burkart and Max Dippel and Marius Lindauer and Frank Hutter<span\
    \ class=\"pl-pds\">}</span></span>,\n  <span class=\"pl-s\">title</span>     =\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Towards Automatically-Tuned\
    \ Deep Neural Networks<span class=\"pl-pds\">}</span></span>,\n  <span class=\"\
    pl-s\">year</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>2018<span\
    \ class=\"pl-pds\">}</span></span>,\n  <span class=\"pl-s\">month</span>     =\
    \ dec,\n  <span class=\"pl-s\">editor</span>    = <span class=\"pl-s\"><span class=\"\
    pl-pds\">{</span>Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin<span\
    \ class=\"pl-pds\">}</span></span>,\n  <span class=\"pl-s\">booktitle</span> =\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">{</span>AutoML: Methods, Sytems,\
    \ Challenges<span class=\"pl-pds\">}</span></span>,\n  <span class=\"pl-s\">publisher</span>\
    \ = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>Springer<span class=\"\
    pl-pds\">}</span></span>,\n  <span class=\"pl-s\">chapter</span>   = <span class=\"\
    pl-s\"><span class=\"pl-pds\">{</span>7<span class=\"pl-pds\">}</span></span>,\n\
    \  <span class=\"pl-s\">pages</span>     = <span class=\"pl-s\"><span class=\"\
    pl-pds\">{</span>141--156<span class=\"pl-pds\">}</span></span>,\n  <span class=\"\
    pl-s\">note</span>      = <span class=\"pl-s\"><span class=\"pl-pds\">{</span>To\
    \ appear.<span class=\"pl-pds\">}</span></span>,\n}</pre></div>\n<p><strong>Note</strong>:\
    \ Previously, the name of the project was AutoNet. Since this was too generic,\
    \ we changed the name to AutoPyTorch. AutoNet 2.0 in the reference mention above\
    \ is indeed AutoPyTorch.</p>\n<h2>\n<a id=\"user-content-contact\" class=\"anchor\"\
    \ href=\"#contact\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Contact</h2>\n<p>Auto-PyTorch is developed by the\
    \ <a href=\"http://www.automl.org/\" rel=\"nofollow\">AutoML Group of the University\
    \ of Freiburg</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1609655576.0
Sarah145/batch_correct:
  data_format: 2
  description: Comparison of batch correction methods for scRNA-seq data - basically
    a clone of BatchBench
  filenames:
  - Singularity
  full_name: Sarah145/batch_correct
  latest_release: null
  readme: '<h2>

    <a id="user-content-batch-correction-pipeline" class="anchor" href="#batch-correction-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    correction pipeline</h2>

    <p>This repository contains scripts to run a Nextflow pipeline to compare different
    batch correction methods for single-cell RNA-seq data. This is mostly just a clone
    of the <a href="https://github.com/cellgeni/batchbench">BatchBench</a> pipeline
    from the CellGen IT team at Sanger but I couldn''t get that to run so made some
    edits and added one or two extra things.</p>

    <p>The input files for this pipeline must be .Rds files of the uncorrected data
    as a SingleCellExperiment object (all batches in one object) with batch labels
    stored in the <code>batch_key</code> (''Batch'' by default) column and cell type
    labels stored in the <code>celltype_key</code> (''cell_type1'' by default) column.</p>

    <p>The pipeline will run 7 different batch correction methods on the data:</p>

    <ul>

    <li>Scanorama</li>

    <li>BBKNN</li>

    <li>Seurat 3</li>

    <li>Combat</li>

    <li>Harmony</li>

    <li>limma</li>

    <li>MNNCorrect</li>

    </ul>

    <p>For each method, 5 different evaluation metrics are returned:</p>

    <ul>

    <li>Batch entropy (from <a href="https://www.biorxiv.org/content/10.1101/2020.05.22.111211v2"
    rel="nofollow">BatchBench</a>) - measure of how well batches are aligned after
    correction - related to the probability that for each cell, its <em>k</em> nearest
    neighbors come from a different batch - value reported is average entropy scaled
    between 0-1 - high batch entropy = well-mixed batches, low batch entropy = poorly-mixed
    batches.</li>

    <li>Cell type entropy (from <a href="https://www.biorxiv.org/content/10.1101/2020.05.22.111211v2"
    rel="nofollow">BatchBench</a>) - same as batch entropy but using cell type labels
    instead - high cell type entropy = mixing of cell types (not good), low cell type
    entropy = cell types are not mixing (good).</li>

    <li>Batch ASW (from <a href="https://github.com/theislab/scib">scIB</a>) - average
    silhouette width of batches - scaled between -1-1 - high batch ASW = dense, well-separated
    batches (bad), low batch ASW = well mixed batches (good).</li>

    <li>Cell type ASW (from <a href="https://github.com/theislab/scib">scIB</a>) -
    same as batch ASW but for cell type labels - high cell type ASW = good, low cell
    type ASW = bad.</li>

    <li>Recovery of marker genes - this idea was taken from the BatchBench paper but
    couldn''t find code for it so wrote my own - not sure if it''s right. For methods
    that correct the expression matrix (Scanorama, Seurat3, Combat, limma, MNNCorrect),
    found marker genes for each cell type (by batch and in the merged dataset), before
    and after batch correction, then compared the list of total marker genes identified
    before batch correction to the list of total marker genes identified after batch
    correction and calculated the Jaccard similarity index of the two lists. High
    Jaccard index = gene expression was not distorted too much by batch correction,
    most markers genes could still be identified (good), low Jaccard index = batch
    correction highly distorted the gene expression values so not as many marker genes
    could be recovered (bad). Jaccard index = 1 - all marker genes recovered, Jaccard
    index = 0 - no marker genes recovered.</li>

    </ul>

    <p>To run pipeline:</p>

    <ul>

    <li>Need to have Nextflow and Singularity installed.</li>

    <li>Clone this repo and <code>cd</code> into it.</li>

    <li>Pull Singularity image - <code>singularity pull shub://Sarah145/batch_correct</code>.</li>

    <li>Edit the <code>nextflow.config</code> script with location of data, batch
    key, cell type key, etc. <em>Note: profile section of the nextflow.config script
    in this repo is configured to run on cluster with slurm.</em>

    </li>

    <li>Edit <code>dataset_list.txt</code> file with name of files - one file on each
    line, no file extension.</li>

    <li>Run pipeline with <code>nextflow run main.nf -profile singularity -with-trace
    trace.txt -with-dag flowchart.png</code>.</li>

    <li>Compile html report of run by running <code>./compile_report.R &lt;sample_name&gt;</code>.</li>

    </ul>

    <p><strong>Overview of pipeline</strong></p>

    <p><a href="https://github.com/Sarah145/batch_correct/blob/master/imgs/flowchart.png?raw=true"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/Sarah145/batch_correct/raw/master/imgs/flowchart.png?raw=true"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - scrna-seq-analysis
  - batch-correction
  updated_at: 1624883167.0
SouthGreenPlatform/CulebrONT_pipeline:
  data_format: 2
  description: A snakemake pipeline to assembly, polishing, correction and quality
    check from Oxford nanopore reads.
  filenames:
  - Containers/Singularity.report.def
  - Containers/Singularity.culebront_tools.def
  full_name: SouthGreenPlatform/CulebrONT_pipeline
  latest_release: 1.5.1
  readme: "<p><a href=\"./docs/source/_images/culebront_logo.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"./docs/source/_images/culebront_logo.png\"\
    \ alt=\"Culebront Logo\" style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://www.python.org/downloads\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e4779c52a0f8acf7c62517ff771deebcf8ab8913544dd508ccdd6cec2f2b400a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e372532422d626c7565\"\
    \ alt=\"PythonVersions\" data-canonical-src=\"https://img.shields.io/badge/python-3.7%2B-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://snakemake.readthedocs.io\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/35030e6ddc253302ffcdf599ce8a8e387c27d88eb3de9cfe4e103b3ec6161f96/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d254532253839254135352e31302e302d627269676874677265656e2e7376673f7374796c653d666c6174\"\
    \ alt=\"SnakemakeVersions\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%E2%89%A55.10.0-brightgreen.svg?style=flat\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a324f41bf4495d7dc95ac4693962834b38ff77e1a6ed7f5c4dca9c3e3f92a6d3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d254532253839254135332e332e302d3745344337342e737667\"\
    \ alt=\"Singularity\" data-canonical-src=\"https://img.shields.io/badge/singularity-%E2%89%A53.3.0-7E4C74.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://docs.conda.io/projects/conda/en/latest/index.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/cacdb0b19bd30d76ae4faaee3355a6d65ecc448b587bac638adbd5eb04339c20/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612d342e382e352532302d677265656e\"\
    \ alt=\"Conda\" data-canonical-src=\"https://img.shields.io/badge/conda-4.8.5%20-green\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Using data from long reads obtained by\
    \ Oxford Nanopore Technologies sequencing makes genome assembly easier, in particular\
    \ to solve repeats and structural variants, in prokaryotic as well as in eukaryotic\
    \ genomes, resulting in increased contiguity and accuracy.</p>\n<p>Bunch of softwares\
    \ and tools are released or updated every week, and a lot of species see their\
    \ genome assembled using those.</p>\n<p>That\u2019s right.</p>\n<p>\"<em>But which\
    \ assembly tool could give the best results for my favorite organism?</em>\"</p>\n\
    <p><strong>CulebrONT can help you!</strong> CulebrONT is an open-source, scalable,\
    \ modulable and traceable snakemake pipeline, able to launch multiple assembly\
    \ tools in parallel and providing help for choosing the best possible assembly\
    \ between all possibilities.</p>\n<p><strong>Homepage: <a href=\"https://culebront-pipeline.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">https://culebront-pipeline.readthedocs.io/en/latest/</a></strong></p>\n\
    <p><a name=\"user-content-citation\"></a></p>\n<h2>\n<a id=\"user-content-citation\"\
    \ class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Citation</h2>\n<p>@Authors:</p>\n\
    <p>Julie Orjuela (IRD), Aurore Comte(IRD), S\xE9bastien Ravel(CIRAD), Florian\
    \ Charriat(INRAE), Tram Vi(IRD, AGI), Francois Sabot(IRD) and S\xE9bastien Cunnac(IRD).</p>\n\
    <p><a name=\"user-content-notes\"></a></p>\n<h2>\n<a id=\"user-content-useful-notes\"\
    \ class=\"anchor\" href=\"#useful-notes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Useful notes</h2>\n<p>Before\
    \ launching CulebrONT, you could base-calling of arbitrarily multiplexed libraries\
    \ across several Minion runs with sequencing quality control and gather the output\
    \ files by genome for subsequent steps. For that use <a href=\"https://github.com/vibaotram/baseDmux\"\
    >https://github.com/vibaotram/baseDmux</a>.</p>\n<h4>\n<a id=\"user-content-thanks\"\
    \ class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Thanks</h4>\n<p>Thanks to Ndomassi\
    \ Tando (i-Trop IRD) by administration support.</p>\n<p>The authors acknowledge\
    \ the IRD i-Trop HPC (South Green Platform) at IRD Montpellier for providing HPC\
    \ resources that have contributed to this work. <a href=\"https://bioinfo.ird.fr/\"\
    \ rel=\"nofollow\">https://bioinfo.ird.fr/</a> - <a href=\"http://www.southgreen.fr\"\
    \ rel=\"nofollow\">http://www.southgreen.fr</a></p>\n<p>Thanks to Yann Delorme\
    \ for this beautiful logo <a href=\"https://nimarell.github.io/resume\" rel=\"\
    nofollow\">https://nimarell.github.io/resume</a></p>\n<p><a name=\"user-content-licence\"\
    ></a></p>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h2>\n<p>Licencied under CeCill-C (<a href=\"http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html\"\
    \ rel=\"nofollow\">http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html</a>)\
    \ and GPLv3\nIntellectual property belongs to IRD and authors.</p>\n"
  stargazers_count: 18
  subscribers_count: 16
  topics: []
  updated_at: 1624536011.0
TormodLandet/Ocellaris:
  data_format: 2
  description: This is a github MIRROR of the main ocellaris repo on bitbucket (https://bitbucket.org/ocellarisproject/ocellaris).
    NO pull request or issues should go to this repo, please! This repository is only
    here to support Singularity Hub which lacks bitbucket support. The code in this
    repository may be severely out of date! It is synced with bitbucket manually and
    may be months or years behind!
  filenames:
  - containers/Singularity
  full_name: TormodLandet/Ocellaris
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>News</h1>\n\
    <p>First release of gemBS-rs, a complete rewrite of the gemBS pipeline (apart\
    \ from the mapper) in Rust bringing increased\nstability while maintaining the\
    \ high performance of gemBS: <a href=\"https://github.com/heathsc/gemBS-rs.git\"\
    >https://github.com/heathsc/gemBS-rs.git</a></p>\n<h1>\n<a id=\"user-content-gembs\"\
    \ class=\"anchor\" href=\"#gembs\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>gemBS</h1>\n<p>gemBS is a high performance\
    \ bioinformatic pipeline designed for highthroughput analysis\nof DNA methylation\
    \ data from whole genome bisulfites sequencing data\n(WGBS). It combines GEM3,\
    \ a high performance read aligner and\nbs_call, a high performance variant and\
    \ methyation caller, into a streamlined and efficient pipeline for\nbisulfite\
    \ sueqnce analysis.</p>\n<p>The manuscript describing the pipeline is available\
    \ <a href=\"https://www.biorxiv.org/content/early/2017/10/11/201988\" rel=\"nofollow\"\
    >here</a></p>\n<hr>\n<h2>\n<a id=\"user-content-licensing\" class=\"anchor\" href=\"\
    #licensing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Licensing</h2>\n<p>gemBS is licensed under GPL.</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Download</h2>\n\
    <p>Use <code>git clone --recursive</code> to retrieve the complete source code\
    \ including the code from external projects such as <code>bs_call</code> and <code>gem3-mapper</code>.</p>\n\
    <pre><code>git clone --recursive https://github.com/heathsc/gemBS.git\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<ol>\n<li>Before starting the installation of gemBS,\
    \ you will need to install\nor check the installation of several packages.</li>\n\
    </ol>\n<p>a) gcc with development libraries\nb) python3, pip3, matplotlib, multiprocess\n\
    c) zlib, lzma, openssl, libcurl, libncurses, wget, pigz</p>\n<p>If you are working\
    \ on a clean (fairly recent) Ubuntu installation, you\ncan install everything\
    \ required with the followiwg commands:</p>\n<pre><code>sudo apt-get update\n\
    sudo apt-get install -y python3 build-essential git python3-pip wget pigz\nsudo\
    \ apt-get install -y zlib1g-dev libbz2-dev\nsudo apt-get install -y libncurses5-dev\
    \ liblzma-dev libssl-dev libcurl4-openssl-dev\npip3 install matplotlib multiprocess\n\
    </code></pre>\n<ol start=\"2\">\n<li>\n<p>Download the gemBS distribution if you\
    \ haven't already done so:</p>\n<p><code>git clone --recursive https://github.com/heathsc/gemBS.git</code></p>\n\
    </li>\n<li>\n<p>Use python install command:</p>\n</li>\n</ol>\n<p>To install to\
    \ the standard system location (i.e., so that all users\ncan use gemBS):</p>\n\
    <pre><code>``python3 setup.py install``\n</code></pre>\n<p>To install to the user's\
    \ home directory:</p>\n<pre><code>``python3 setup.py install --user``\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-check-your-installation\" class=\"anchor\" href=\"\
    #check-your-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Check your installation</h2>\n<p>For checking\
    \ your installation follow this\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/example.html\"\
    \ rel=\"nofollow\">worked example</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Documentation\
    \ can be found at\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/index.html\"\
    \ rel=\"nofollow\">gemBS</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-changelog\"\
    \ class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Changelog:</h2>\n<pre><code>3.5.5\
    \ Fix logging bug caused by trimming change in 3.5.3\n3.5.4 Fix bug in the output\
    \ of strand specific cpg txt files (not\n      encode Bed files) where the 'C'\
    \ entry was not being printed\n3.5.3 Allow for read end specific trimming in bs_call\n\
    3.5.3 Enable range checks and asserts in bs_call all target; add bs_call debug\
    \ target\n3.5.2 Correct problems with gcc10.  Move to htslib/samtools/bcftools\
    \ version 1.11\n3.5.1 Check if C compiler requires --std=c99 flag for standards\
    \ conformant behaviour\n3.5.1 Make sure bgzip is copied correctly during installation\n\
    3.5.0 Make bs_call process contig pools from largest to smallest (this change\
    \ alters the sqlite db format so\n      if you have a previously started gemBS\
    \ run you should (a) remove the .gemBS directory, (b) redo the\n      'gemBS prepare'\
    \ step to recreate the db file and (3) run 'gemBS db-sync'. \n3.5.0 Switch bs_call\
    \ and snpxtr to use the new dbSNP index format\n3.5.0 Add ability of dbSNP to\
    \ read the new JSON and VCF  dbSNP format files\n      that are now used for human\
    \ and non-human species respectively\n3.5.0 Add multithreading to dbSNP_idx\n\
    3.5.0 Change format of dbSNP index to allow (a) efficient loading\n      of SNP\
    \ data for individual contigs and (b) parallel index creation \n3.5.0 Rewrite\
    \ mextr and snpxtr as standalone tools rather than\n      bcftools plugins.  Now\
    \ multithreaded and (relatively) memoryefficient\n3.5.0 Replace bedToBigBed and\
    \ wigToBigWig to reduce memory usage\n      and improve speed\n3.4.5 Fix crash\
    \ when using the -k (keep-mismatch) flag, and fix rare hangs at end of processing\n\
    3.4.4 Sort input bcf files to bcftools concat stage to ensure reproducibility.\n\
    3.4.4 Add extra sort keys when generating pools to ensure stability of pool membership\
    \ in the event of multiple contigs\n      having the same size\n3.4.3 Remove calculation\
    \ of the goodness of filter (GOF) as this is expensive, non-standard and unreliable.\
    \  Removing this\n      removes the dependency on GSL.\n3.4.3 Add autodetection\
    \ of output format to bs_call (unless explicitly specified on the command line)\n\
    3.4.2 Add CRAM support (via make_cram option in configuration file)\n3.4.1 Add\
    \ benchmark-mode that does not write date or program version numbers into SAM/BAM\
    \ or VCF/BCF files\n      Switch to samtools, bcftools and htslib v1.10\n3.4.0\
    \ Move to new bs_call version (2.1.0) which is more efficient\n      in memory\
    \ use and can read BAMs and write BCFs natively.\n      The new bs_call requires\
    \ a faidx indexed reference, so gemBS\n      no creates this during indexing.\n\
    3.4.0 Add switches to give more control to threads and memory\n      usage in\
    \ mapping and calling stages\n3.3.3 Remove legacy pathway for config files with\
    \ no header line (fix issue 'error in gemBS index #65)\n3.3.2 Fix error where\
    \ header line for wig files could be omitted\n3.3.2 Fix generation of non_cpg\
    \ files\n3.3.1 Fix Attribute error bug due to not checking if conversion is a\
    \ list\n3.3.0 Make new release for IHEC\n3.3.0 Switch conversion default in IHEC_standard\
    \ configuration to 0.01,0.05 rather than auto, which can give odd results if conversion\
    \ controls not present or not working correctly\n3.3.0 Fix bug where conversion\
    \ parameters could be ignored\n3.2.13 Fix formatting bug in mextr with multiple\
    \ samples (not triggered in normal gemBS use)\n3.2.12 Ensure that conversion statistics\
    \ are correctly calculated for non-stranded or reverse conversion protocols\n\
    3.2.11 Introduce reverse_conversion option for mapping where read 1 is G2A converted\
    \ and read 2 is C2T converted\n3.2.10 Correct regex patch for single end reads\n\
    3.2.9 Update Singularity and Dockerfile recipes to allow kemp utils to be built\
    \ correctly\n3.2.9 Make setup.py and gemBS/commands.py read the version information\
    \ from gemBS/version.py, so ensuring consistency\n3.2.9 Fix bug added in last\
    \ version where options in config file were not being taken into account\n3.2.8\
    \ Fix mis specification errors in long options for mextr. \n3.2.8 Fix bug where\
    \ mextr (methyl extract plugin for bcftools) would crash if cpg output  option\
    \ was not set.\n3.2.7 Apply patches for bugs in handling single end reads (suggested\
    \ by I. Moghul)\n3.2.7 Changed regex for filenames to make it more general (suggested\
    \ by I. Moghul)\n3.2.7 Fixed bug (reported by chhylp123) where zero arguments\
    \ to some options were being ignored\n3.2.6 Cleaned up compilation and cleaning\
    \ of gemBS tools\n3.2.6 Fixed python error if either the over conversion reference\
    \ sequence was not defined\n3.2.6 Added check in bs_call that conversion parameters\
    \ are valid (between 0 and 1)\n3.2.6 Perform more stringent sanity checking on\
    \ conversion vaalues when autocomputed by gemBS\n3.2.6 Use --diasble-lzma configuration\
    \ flag for samtools and bcftools as we don't need it and it removes an unneccesary\
    \ dependency\n3.2.6 Add install options --disable-cuda (on by default) and --enable-cuda\
    \ that affect GEM3 comppilation\n3.2.6 Bug fix with incorrect handling of duplicate\
    \ reads\n3.2.5 Minor bug fix - correct error with non-paired end non-bisulfite\
    \ reads\n3.2.4 Modify the bisulfite processing in gem-mapper to be more efficient\
    \ (in particular for the non-stranded option)\n3.2.4 Modify gemBS to use the new\
    \ conversion options for gem-mapper\n3.2.4 Switch gem-mapper to use option --underconversion-sequence\
    \ and --overconversion-sequence rather than --underconversion_sequence to be consistent\
    \ with other options\n3.2.3 Fixed bug if conversion parameters were not set\n\
    3.2.2 Rework non-stranded mode so that both possible conversions are tried and\
    \ the results merged\n3.2.2 Fix bug where non-stranded flag was not being passed\
    \ to mapper in paired end mode\n3.2.1 Move warning message from bscall from stdout\
    \ to stderr\n3.2.1 Switch Singularity build to use Ubuntu 16.04 rather than 18.04\
    \ to allow the image to work in CentOS 6 (Docker build changed as well to keep\
    \ the two in sync)\n3.2.1 Fix undeclared variable bugs and missing --ignore-deps\
    \ option in merge-bcfs\n3.2.1 Add default for dbSNP_index if dbSNP_files is set\n\
    3.2.1 Add gsl-path install option\n3.2.0 Make new release\n3.1.0 Make installation\
    \ process more modular.  Allow for sub-installs\n3.1.0 Add support for reading\
    \ config from ${index_dir}/gemBS.json if it exists\n3.1.0 Add --reference-bias\
    \ option to mextr and gemBS extract\n3.1.0 Add support for non-bisulfite mapping\
    \ of individual datasets\n3.1.0 Allow white space in variable values\n3.1.0 Allow\
    \ fallback to gzip if pigz not present\n3.1.0 Add --dry-run, --json, --ignore-db\
    \ and --ignore-dep to extract command\n3.1.0 Add --ignore-dep option to call and\
    \ merge-bcfs commands\n3.1.0 Add SNP extraction function to extract command\n\
    3.0 Make v3.0 release\n3.0 Merge with master branch.\n3.0 Bump samtools sort memory\
    \ limit to 2G\n3.0 Add extra_references option for reference generation\n3.0 Allow\
    \ input files to mapping to be shell commands\n3.0 Add links to documentation\n\
    3.0 Upload new yeast example and add documentation\n3.0 Add --dir option to gemBS\n\
    3.0 Add --ignore-db options for --dry-run / --json\n3.0 Add --json output option\
    \ for dry runs\n3.0 Update help text to match new functions\n3.0 Introduce standard\
    \ analysis configurations stored within distribution\n3.0 Switch gem3-mapper distribution\
    \ to gembs branch on official gem3-mapper repo\n3.0 Removal of incomplete files\
    \ and roll back of db in the event of pipeline failure\n3.0 Automatic removal\
    \ of individual BAMs and BCFs after successful merging\n3.0 Prevent pipelines\
    \ hanging in event of failure\n3.0 Generate ENCODE bed and bigbed files\n3.0 Switch\
    \ to python 3\n3.0 Switch to mextr for BCF filtering\n3.0 Include fetch and build\
    \ of samtools / bcftools during build process\n3.0 Add dry-run capability to map\
    \ and call commands\n3.0 Introduce contig pools to automatically group small contigs\n\
    3.0 Automatic generation of contig.size files from index build\n3.0 Allow use\
    \ of in memory sqlite3 db as an option\n3.0 Allow multiple instances of gemBS\
    \ (possible on different hosts) to work \n    simultaneously on the same analysis\n\
    3.0 Reduce and simply commands\n3.0 Add Dockerfile\n3.0 Add multi-threading and\
    \ multi-processing options for most commands\n3.0 Use sqlite3 to track progress\
    \ of analyses, file paths etc.\n3.0 Added more flexible configuration options\
    \ (new csv format + new configuration file)\n3.0 Remove test dataset from distribution\
    \ (distribute from web site)\n2.1.0 Ensure commands run during pipeline come from\
    \ installation\n2.1.0 Added Singularity build recipe\n2.1.0 Add new command gemBS\
    \ direct-mapping\n2.1.0 Fixed Makefile clean in tools\n2.0.2 Fixed bug related\
    \ with the percentage of High Quality Variant in Variants summary report.\n2.0.2\
    \ Check temporary directory existence.\n2.0.2 Fixed QualityNonRefCpg sample name\
    \ in png image.\n2.0.2 Fixed mapper issues related with aligning performace.\n\
    2.0.2 Fixed arguments for Under/Over Conversion sequence name in gem3-mapper\n\
    2.0.1 On bscall repository, fixed argument -k about discarded reads that do not\
    \ form proper pairs.\n2.0 Check tmp folder before starting mapping process.\n\
    2.0 Added Left and Right Trimming optional arguments to gemBS bscall.\n2.0 Added\
    \ GC Coverage correlation value to BS Call Stats Summary.\n2.0 Fixed error when\
    \ reporting complete path to not found bam files.\n2.0 Fixed iteration over sampleBams\
    \ dictionary in MergeAll method.\n2.0 Updated: Avoid redo indexing when merging\
    \ just one file.\n2.0 Changed conversion formula.\n2.0 Added parameter for dbSNP.\n\
    2.0 Added threads to bscall.\n2.0 Removed CpGs reports. Already done from bscall\
    \ report.\n2.0 Fixed bs_call makefile for the gcc to be used.\n2.0 New bscall\
    \ version. Generates JSON report.\n2.0 Removed gemBS options snp-stats,cpg-report,cpg-stats.\n\
    2.0 Added summary report from the bs_call json stats\n2.0 New BSCall Report. From\
    \ bscall son file generates three types of reports:\n    Mapping and Coverage\
    \ Report\n    Bs-Genotypes Calls Report\n    Methylation Statistics report\n1.7\
    \ Added non stranded read conversion parameter\n1.7 Fixed SE crash when estimating\
    \ overlapped bases.\n1.7 Fixed gem-index (gem3) to follow fastq and SAM specifications.\
    \ \n    Modified gem3-mapper repository external module.\n    New external module\
    \ https://github.com/heathsc/gem3-mapper.git\n1.7 Fixed threads parameter to samtools\
    \ merge\n1.7 Fixed threads parameter to gem-mapper\n1.7 Removed Indels Field on\
    \ Variants Report.\n1.7 Added Overlapping Bases at Mapping Report\n1.7 Modified\
    \ Base Counts Overall, removed Base Counts general and Base Counts Overall\n1.7\
    \ New Dinucleotide CpGs Report\n    New table dinucleotide stats\n    New plots\
    \ for Informative Reads and CpGs\n    Methylation levels plots for different types\
    \ of CpGs\n    Summary Table\n1.7 New Readme file to inform about report test\n\
    1.7 New basic statis table for Variants Report\n1.7 Removed parameter -r (reference\
    \ length) parameter for mapping reports command (gemBS bsMap).\n1.6 New CpGs Density\
    \ plot, include box plos, bar plot and fitting curve\n1.6 Change name at CpG report:\n\
    \    \"Heterozygous\" for \"Alternative CX\"\n    \"De Novo CpGs Methylation Status\"\
    \ for \"Non Reference CpGs\"\n    \"CpGs with SNP\" for \"SNPs (CX) at Reference\
    \ CpGs\"\n1.6 CpGs Report Simplified to Q&gt;20\n1.6 BigWig Default parameters\
    \ for filtering CpG per a given quality and a total number of supported informative\
    \ reads   \n1.5 Initial Release  \n</code></pre>\n<hr>\n<h2>\n<a id=\"user-content-developers\"\
    \ class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Developers</h2>\n<p>gemBS:</p>\n\
    <ul>\n<li>Marcos Fernandez-Callejo - <a href=\"mailto:marcos.fernandez@cnag.crg.eu\"\
    >marcos.fernandez@cnag.crg.eu</a>\n</li>\n<li>Simon Heath - <a href=\"mailto:simon.heath@gmail.com\"\
    >simon.heath@gmail.com</a>\n</li>\n</ul>\n<p>gem mapper:</p>\n<ul>\n<li>Santiago\
    \ Marco-Sola - <a href=\"mailto:santiagomsola@gmail.com\">santiagomsola@gmail.com</a>\n\
    </li>\n</ul>\n<p>bisulfite caller and filtering:</p>\n<ul>\n<li>Simon Heath -\
    \ <a href=\"mailto:simon.heath@gmail.com\">simon.heath@gmail.com</a>\n</li>\n\
    </ul>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1553974960.0
Transipedia/KaMRaT:
  data_format: 2
  description: null
  filenames:
  - Singularity.def
  full_name: Transipedia/KaMRaT
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-kamrat\" class=\"anchor\" href=\"#kamrat\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>KaMRaT</h1>\n\
    <p>k-mers are substrings of a fixed length <em>k</em> extracted from biological\
    \ sequences.</p>\n<p>Transcriptomics analysis based on k-mer counts allows to\
    \ analyze RNA variation at nucleotide resolution, unlimited by a reference genome\
    \ or transcriptome. Therefore, in principle, all changes in RNA sequences (splice\
    \ variants, SNPs, novel transcripts) can be captured by k-mer analysis. However\
    \ a challenge with k-mers is that there are too many of them. A typical bulk RNA-seq\
    \ sample has in the order of 1e7 to 1e8 unique k-mers, producing a huge matrix\
    \ when many samples are combined.</p>\n<p>KaMRaT provides a set of tools for k-mer\
    \ matrix reduction, for reducing k-mer number and extending k-mers to longer contigs.</p>\n\
    <p>The name KaMRaT means \"k-mer Matrix Reduction Toolkit\", or \"k-mer Matrix,\
    \ Really Tremendous !\".</p>\n<h2>\n<a id=\"user-content-typical-workflow-of-kamrat\"\
    \ class=\"anchor\" href=\"#typical-workflow-of-kamrat\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Typical Workflow\
    \ of KaMRaT</h2>\n<p>KaMRaT <em>per se</em> is shown at the center of the workflow.\
    \ It is a C++ program that takes as input a count matrix and produces another\
    \ matrix as output.\nIn the workflow shown, KaMRaT is used for reducing a count\
    \ matrix produced from a set of fastq files and producing a reduced matrix with\
    \ features of interest with respect to conditions in the input sample-info file.</p>\n\
    <p><a href=\"./docs/workflow.png\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img src=\"./docs/workflow.png\" alt=\"workflow\" style=\"max-width:100%;\"></a></p>\n\
    <p>The feature matrix contains features in row and samples in column. Features\
    \ can be <em>k</em>-mers (for all modules) as well as other general features such\
    \ as genes/transcripts (only for KaMRaT-index, -filter, and -rank). The feature\
    \ counts can be either normalized or non-normalized.</p>\n<p>The <em>k</em>-mer\
    \ feature matrix can be constructed with the following possibilities:</p>\n<ul>\n\
    <li>The <a href=\"./related-tools/prepare_kmer_table/Snakefile\">Snakefile</a>\
    \ provided with the project + <a href=\"https://github.com/Transipedia/dekupl-joinCounts\"\
    >DE-kupl joinCounts</a>\n</li>\n<li>\n<a href=\"https://github.com/tlemane/kmtricks\"\
    >Kmtricks</a> software</li>\n<li>\n<a href=\"https://github.com/Transipedia/dekupl-run\"\
    >DE-kupl</a>'s raw-counts.tsv or masked-counts.tsv matrices</li>\n</ul>\n<p>A\
    \ set of auxiliary tools to be used for upstream and downstream of kamrat are\
    \ provided:</p>\n<ul>\n<li>Upstream tools:\n<ul>\n<li>A matrix generating module\
    \ controlled by Snakemake which applying jellyfish and DE-kupl joinCounts module</li>\n\
    <li>A bash script for generating a submatrix by selecting from it a set of columns</li>\n\
    </ul>\n</li>\n<li>Downstream tools:\n<ul>\n<li>A feature selection model with\
    \ an R script applying ridge/lasso regressions and random forest classifier</li>\n\
    <li>A contig counting module implemented in C++ for estimating the counts of a\
    \ list of contigs in an independent dataset; it also supports evaluation of sample\
    \ count coherence among contig's compositional k-mers</li>\n<li>A model evaluation\
    \ module written in R taking a trained model and evaluating it with a feature\
    \ count matrix and feature conditions</li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <details>\n<summary>Build from source</summary>\n<h3>\n<a id=\"user-content-dependencies\"\
    \ class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Dependencies</h3>\n<ul>\n<li><a\
    \ href=\"https://github.com/mlpack/mlpack/releases/tag/3.3.2\">MLPack 3.3.2</a></li>\n\
    <li><a href=\"https://www.boost.org/doc/libs/1_74_0/libs/iostreams/doc/index.html\"\
    \ rel=\"nofollow\">Boost-iostreams</a></li>\n</ul>\n<p>MLPack can be installed\
    \ on <a href=\"https://mlpack.org/doc/mlpack-3.3.2/doxygen/build.html\" rel=\"\
    nofollow\">Linux/Mac</a>, <a href=\"https://mlpack.org/doc/mlpack-3.3.2/doxygen/build_windows.html\"\
    \ rel=\"nofollow\">Windows</a>, or via <a href=\"https://anaconda.org/conda-forge/mlpack\"\
    \ rel=\"nofollow\">conda</a> by following the corresponding links.<br>\nIf you\
    \ are installing MLPack with conda, please add the following line into your <code>.bashrc</code>\
    \ file in the <code>home/</code> directory before compiling KaMRaT:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span>\
    \ LD_LIBRARY_PATH=/path_to_conda_env/mlpack/lib:<span class=\"pl-smi\">$LD_LIBRARY_PATH</span></pre></div>\n\
    <h3>\n<a id=\"user-content-clone-and-build\" class=\"anchor\" href=\"#clone-and-build\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Clone and Build</h3>\n<p>Firstly, clone the repository:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>git clone --recursive https://github.com/Transipedia/KaMRaT.git\n\
    <span class=\"pl-c1\">cd</span> KaMRaT</pre></div>\n<p>If you installed MLPack\
    \ library with conda:</p>\n<div class=\"highlight highlight-source-shell\"><pre>bash\
    \ compile.bash /path_to_MLPack_conda_environment</pre></div>\n<p>Otherwise, if\
    \ you installed MLPack without conda:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>bash compile.bash</pre></div>\n<p>Finally, an executable binary file is\
    \ available as <code>bin/kamrat</code>.</p>\n</details>\n<details>\n<summary>Use\
    \ singularity</summary>\n<p>If using KaMRaT inside singularity, only by pulling\
    \ from docker hub is enough:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity build KaMRaT.sif docker://xuehl/kamrat:latest</pre></div>\n\
    </details>\n<h2>\n<a id=\"user-content-general-information\" class=\"anchor\"\
    \ href=\"#general-information\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>General Information</h2>\n<h3>\n<a\
    \ id=\"user-content-sample-information-file\" class=\"anchor\" href=\"#sample-information-file\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Sample Information File</h3>\n<p>The sample-info file is indicated\
    \ by the option <code>-smp-info</code>. This file aims to indicate which columns\
    \ in the k-mer count matrix should be considered as sample columns. Please do\
    \ not put any header line in the file, since the columns are already defined by\
    \ convention as below.</p>\n<ul>\n<li>If the file contains only one column, it\
    \ indicates sample names, and all samples are considered as the same condition</li>\n\
    <li>If the file contains two columns, the first column corresponds to sample names,\
    \ and the second corresponds to conditions (<em>e.g.</em> tumor, normal)</li>\n\
    <li>If the file is not provided, all columns in the matrix apart from the first\
    \ one are considered as samples</li>\n</ul>\n<h3>\n<a id=\"user-content-input-count-matrix-for-kamrat\"\
    \ class=\"anchor\" href=\"#input-count-matrix-for-kamrat\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Input Count\
    \ Matrix for KaMRaT</h3>\n<p>The input count matrix should be in .tsv or .tsv.gz\
    \ format, in which fields are separated by tabulations.\nIn the matrix, features\
    \ are presented as rows, and samples as columns. The first column in matrix should\
    \ always be the feature column (sequences or feature names).<br>\n\"Features\"\
    \ can be any quantified feature such as genes, k-mers or contigs. k-mers or contigs\
    \ are represented by their own sequence.\nKaMRaT accepts extra columns representing\
    \ non-count values, e.g. feature's p-value, score, etc. In this case, a smp-info\
    \ file is mandatory for indicating which columns are the count columns.</p>\n\
    <h3>\n<a id=\"user-content-output-count-matrix-by-kamrat\" class=\"anchor\" href=\"\
    #output-count-matrix-by-kamrat\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Output Count Matrix by KaMRaT</h3>\n\
    <p>The output count matrix is also .tsv format table, where fields are separated\
    \ by tabs.<br>\nIn the matrix, the features are presented as rows, and the columns\
    \ are in same order as the input.<br>\nKaMRaT guarantees the information of output\
    \ matrix is coherent with that of the input matrix. For KaMRaT-rank, though there\
    \ are steps of count normalization, log transformation and standardization for\
    \ score evaluation, the count values in output matrix are kept same as input (raw\
    \ count).</p>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h2>\n<p>Note: if you use KaMRaT in command line, please remember\
    \ to indicate the full path to KaMRaT binary file.</p>\n<h3>\n<a id=\"user-content-kamrat-execution\"\
    \ class=\"anchor\" href=\"#kamrat-execution\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>KaMRaT Execution</h3>\n<p>We\
    \ recommande using KaMRaT within <code>singularity</code>:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ -B /bind_src:/bind_des kamrat <span class=\"pl-k\">&lt;</span>CMD<span class=\"\
    pl-k\">&gt;</span> [options] input_table \n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> &lt;CMD&gt; can be one of filter, mask, merge, rank</span></pre></div>\n\
    <p>The <code>-B</code> option is for binding disk partitions to singularity image,\
    \ please check <code>singularity</code> helper for details:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ -h</pre></div>\n<p>It's also executable directly on command line:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>/path_to_KaMRaT_bin_dir/kamrat\
    \ <span class=\"pl-k\">&lt;</span>CMD<span class=\"pl-k\">&gt;</span> [options]\
    \ input_table \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;CMD&gt;\
    \ can be one of filter, mask, merge, rank</span></pre></div>\n<p>In the following\
    \ sections, we present under the situation of using KaMRaT in <code>singularity</code>.<br>\n\
    For running it directly on command line, please replace the leading <code>singularity\
    \ exec -B /bind_src:/bind_des</code> by the path to KaMRaT binary file.</p>\n\
    <h3>\n<a id=\"user-content-kamrat-helper\" class=\"anchor\" href=\"#kamrat-helper\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>KaMRaT Helper</h3>\n<p>KaMRaT's top-level helper is accessible by\
    \ typing one of these commands:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> kamrat\nsingularity <span\
    \ class=\"pl-c1\">exec</span> kamrat -h\nsingularity <span class=\"pl-c1\">exec</span>\
    \ kamrat -help</pre></div>\n<p>Helpers of each KaMRaT modules are accessible via:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> &lt;CMD&gt; can be one from filter, mask, merge, rank\
    \ #</span>\nsingularity <span class=\"pl-c1\">exec</span> kamrat <span class=\"\
    pl-k\">&lt;</span>CMD<span class=\"pl-k\">&gt;</span> -h\nsingularity <span class=\"\
    pl-c1\">exec</span> kamrat <span class=\"pl-k\">&lt;</span>CMD<span class=\"pl-k\"\
    >&gt;</span> -help</pre></div>\n<h3>\n<a id=\"user-content-kamrat-usage-by-module\"\
    \ class=\"anchor\" href=\"#kamrat-usage-by-module\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>KaMRaT Usage\
    \ by Module</h3>\n<details>\n<summary>index: index feature count table on disk</summary>\n\
    <pre lang=\"text\"><code>[USAGE]    kamrat index -intab STR -outdir STR [-klen\
    \ INT -unstrand -nfbase INT]\n\n[OPTION]         -h, -help      Print the helper\n\
    \                 -intab STR     Input table for index, mandatory\n          \
    \       -outdir STR    Output index directory, mandatory\n                 -klen\
    \          k-mer length, mandatory if features are k-mer\n                   \
    \                 if present, indexation will be switched to k-mer mode\n    \
    \             -unstrand      Unstranded mode, indexation with canonical k-mers\n\
    \                                    if present, indexation will be switched to\
    \ k-mer mode\n                 -nfbase INT    Base for calculating normalization\
    \ factor\n                                    normCount_ij &lt;- INT * rawCount_ij\
    \ / sum_i{rawCount_ij}\n                                    if not provided, input\
    \ counts will not be normalized\n</code></pre>\n</details>\n<details>\n<summary>filter:\
    \ filter feature by expression level</summary>\n<pre lang=\"text\"><code>[USAGE]\
    \    kamrat filter -idxdir STR -design STR [-upmin INT1:INT2 -downmax INT1:INT2\
    \ -reverse -outpath STR -withcounts]\n\n[OPTION]         -h,-help            \
    \  Print the helper\n                 -idxdir STR           Indexing folder by\
    \ KaMRaT index, mandatory\n                 -design STR           Path to filter\
    \ design file, a table of two columns, mandatory\n                           \
    \                the first column indicate sample names\n                    \
    \                       the second column should be either UP or DOWN (capital\
    \ letters)\n                                               samples with UP will\
    \ be considered as up-regulated samples\n                                    \
    \           samples with DOWN will be considered as down-regulated samples\n \
    \                                              samples not given will be neutral\
    \ (not considered for filter)\n                                              \
    \ samples can also be all UP or all DOWN\n                 -upmin INT1:INT2  \
    \    Up feature lower bound, [1:1, meaning no filter]\n                      \
    \                     output features counting &gt;= INT1 in &gt;= INT2 UP-samples\n\
    \                 -downmax INT1:INT2    Down feature upper bound [inf:1, meaning\
    \ no filter]\n                                           output features counting\
    \ &lt;= INT1 in &gt;= INT2 DOWN-samples\n                 -reverse           \
    \   Reverse filter, to remove eligible features [false]\n                 -outpath\
    \ STR          Path to results after filter\n                                \
    \           if not provided, output to screen\n                 -withcounts  \
    \         Output sample count vectors [false]\n</code></pre>\n</details>\n<details>\n\
    <summary>mask: mask k-mers from matrix</summary>\n<pre lang=\"text\"><code>[USAGE]\
    \    kamrat mask -idxdir STR -fasta STR [-reverse -outpath STR -withcounts]\n\
    \              \n[OPTION]         -h,-help         Print the helper\n        \
    \         -idxdir STR      Indexing folder by KaMRaT index, mandatory\n      \
    \           -fasta STR       Sequence fasta file as the mask, mandatory;\n   \
    \              -reverse         Reverse mask, to select the k-mers in sequence\
    \ fasta file [false];\n                 -outpath STR     Path to extension results\n\
    \                                      if not provided, output to screen\n   \
    \              -withcounts      Output sample count vectors [false]\n</code></pre>\n\
    </details>\n<details>\n<summary>merge: extend k-mers into contigs</summary>\n\
    <pre lang=\"text\"><code>[USAGE]    kamrat merge -idxdir STR -overlap MAX-MIN\
    \ [-with STR1[:STR2] -interv STR[:FLOAT] -min-nbkmer INT -outpath STR -withcounts\
    \ STR]\n\n[OPTION]         -h,-help               Print the helper;\n        \
    \         -idxdir STR            Indexing folder by KaMRaT index, mandatory;\n\
    \                 -overlap MAX-MIN       Overlap range for extension, mandatory\n\
    \                                            MIN and MAX are integers, MIN &lt;=\
    \ MAX &lt;= k-mer length;\n                 -with STR1[:STR2]      File indicating\
    \ k-mers to be extended (STR1) and rep-mode (STR2)\n                         \
    \                   if not provided, all indexed k-mers are used for extension\n\
    \                                            in the file STR1, a supplementary\
    \ column of rep-value can be provided\n                                      \
    \      STR2 can be one of {min, minabs, max, maxabs} [min];\n                \
    \ -interv STR[:FLOAT]    Intervention method for extension [spearman:0.25]\n \
    \                                           can be one of {none, pearson, spearman,\
    \ mac}\n                                            the threshold may follow a\
    \ ':' symbol;\n                 -min-nbkmer INT        Minimal length of extended\
    \ contigs [0];\n                 -outpath STR           Path to extension results\n\
    \                                            if not provided, output to screen;\n\
    \                 -withcounts STR        Output sample count vectors, STR can\
    \ be one of [mean, median]\n                                            if not\
    \ provided, output without count vector\n</code></pre>\n</details>\n<details>\n\
    <summary>rank: rank features according to their association with sample conditions</summary>\n\
    <pre lang=\"text\"><code>[USAGE]    kamrat rank -idxdir STR -count-mode STR -rankby\
    \ STR -design STR [-with STR1[:STR2] -seltop NUM -outpath STR -withcounts]\n\n\
    [OPTION]         -h,-help             Print the helper\n                 -idxdir\
    \ STR          Indexing folder by KaMRaT index, mandatory\n                 -rankby\
    \ STR          Ranking method, mandatory, can be one of:\n                   \
    \                       ttest.padj      adjusted p-value of t-test between conditions\n\
    \                                          ttest.pi        \\u03C0-value of t-test\
    \ between conditions\n                                          snr          \
    \   signal-to-noise ratio between conditions\n                               \
    \           dids            DIDS score\n                                     \
    \     lr:nfold        accuracy by logistic regression classifier\n           \
    \                               bayes:nfold     accuracy by naive Bayes classifier\n\
    \                                          svm:nfold       accuracy on SVM classifier\n\
    \                 -design STR          Path to file indicating sample-condition\
    \ design\n                                          without header line, each\
    \ row can be either:\n                                          sample name, sample\
    \ condition\n                                          sample name, sample condition,\
    \ sample batch (only for lrc, nbc, and svm)\n                 -with STR1[:STR2]\
    \    File indicating features to rank (STR1) and counting mode (STR2)\n      \
    \                                    if not provided, all indexed features are\
    \ used for ranking\n                                          STR2 can be one\
    \ of [rep, mean, median]\n                 -seltop NUM          Select top ranked\
    \ features\n                                          if NUM &gt; 1, number of\
    \ top features to select (should be integer)\n                               \
    \           if 0 &lt; NUM &lt;= 1, ratio of top features to select\n         \
    \                                 if absent or NUM &lt;= 0, output all features\n\
    \                 -outpath STR         Path to ranking result\n              \
    \                            if not provided, output to screen\n             \
    \    -withcounts          Output sample count vectors [false]\n\n[NOTE]     For\
    \ ranking methods lrc, nbc, and svm, a univariate CV fold number (nfold) can be\
    \ provided\n               if nfold = 0, leave-one-out cross-validation\n    \
    \           if nfold = 1, without cross-validation, training and testing on the\
    \ whole datset\n               if nfold &gt; 1, n-fold cross-validation\n    \
    \       For t-test ranking methods, a transformation log2(x + 1) is applied to\
    \ sample counts\n           For SVM ranking, sample counts standardization is\
    \ applied feature by feature\n</code></pre>\n</details>\n<details>\n<summary>query:\
    \ query sequences</summary>\n<pre lang=\"text\"><code>[USAGE]    kamrat query\
    \ -idxdir STR -fasta STR -toquery STR [-withabsent -outpath STR]\n\n[OPTION] \
    \        -h,-help         Print the helper\n                 -idxdir STR     \
    \ Indexing folder by KaMRaT index, mandatory\n                 -fasta STR    \
    \   Sequence fasta file, mandatory\n                 -toquery STR     Query method,\
    \ mandatory, can be one of:\n                                      mean      \
    \  mean count among all composite k-mers for each sample\n                   \
    \                   median      median count among all composite k-mers for each\
    \ sample\n                 -withabsent      Output also absent queries (count\
    \ vector all 0) [default: false]\n                 -outpath STR     Path to extension\
    \ results\n                                      if not provided, output to screen\n\
    </code></pre>\n</details>\n<h2>\n<a id=\"user-content-softwarelibrary-citations\"\
    \ class=\"anchor\" href=\"#softwarelibrary-citations\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Software/Library\
    \ Citations</h2>\n<p>Armadillo:</p>\n<ul>\n<li>Conrad Sanderson and Ryan Curtin.\
    \ Armadillo: a template-based C++ library for linear algebra. Journal of Open\
    \ Source Software, Vol. 1, pp. 26, 2016.</li>\n<li>Conrad Sanderson and Ryan Curtin.\
    \ A User-Friendly Hybrid Sparse Matrix Class in C++. Lecture Notes in Computer\
    \ Science (LNCS), Vol. 10931, pp. 422-430, 2018.</li>\n</ul>\n<p><a href=\"https://www.boost.org/\"\
    \ rel=\"nofollow\">Boost C++ Library</a></p>\n<p>DE-kupl: Audoux, J., Philippe,\
    \ N., Chikhi, R. et al. DE-kupl: exhaustive capture of biological variation in\
    \ RNA-seq data through k-mer decomposition. Genome Biol 18, 243 (2017).</p>\n\
    <p>MLPack: R.R. Curtin, M. Edel, M. Lozhnikov, Y. Mentekidis, S. Ghaisas, S. Zhang.\
    \ mlpack 3: a fast, flexible machine learning library. Journal of Open Source\
    \ Software 3:26, 2018.</p>\n<p>glmnet: Friedman, Jerome, Trevor Hastie, and Rob\
    \ Tibshirani. \"Regularization paths for generalized linear models via coordinate\
    \ descent.\" Journal of statistical software 33.1 (2010): 1.</p>\n<p>randomForest:\
    \ Liaw, Andy, and Matthew Wiener. \"Classification and regression by randomForest.\"\
    \ R news 2.3 (2002): 18-22.</p>\n"
  stargazers_count: 3
  subscribers_count: 5
  topics: []
  updated_at: 1623772488.0
UNM-CARC/FEniCS:
  data_format: 2
  description: FEniCS containers for CARC systems
  filenames:
  - Singularity.ubuntu
  - Singularity.docker
  full_name: UNM-CARC/FEniCS
  latest_release: null
  readme: '<h1>

    <a id="user-content-fenics" class="anchor" href="#fenics" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FEniCS</h1>

    <p>This repository contains a FEniCS container for UNM CARC high performance systems</p>

    <ul>

    <li>Singularity.docker - Singularity container built from the standard FEniCS
    docker container</li>

    <li>Singularity.ubuntu - Singularity container built from the FEniCS ubuntu packages</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1511832970.0
UNM-CARC/heudiconv:
  data_format: 2
  description: null
  filenames:
  - Singularity.ubuntu
  full_name: UNM-CARC/heudiconv
  latest_release: null
  readme: '<p>Not much</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1536784012.0
UNM-CARC/singularity-test:
  data_format: 2
  description: Singularity recipes for CARC systems
  filenames:
  - Singularity.ubuntu-mpich
  - Singularity.ubuntu-ompi
  - Singularity.centos
  full_name: UNM-CARC/singularity-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-tests" class="anchor" href="#singularity-tests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Tests</h1>

    <p>This repository contains test singularity recipes for Ubuntu and CentOS repository
    builds for

    HPC systems at the UNM Center for Advanced Research Computing. These recipes are
    generally built

    using Singularity Hub, which links to this repository, and are meant for debugging
    basic

    container setups that are then used to develop other more complex recipes.</p>

    <p>Note that these containers pull the CARC modules //into// the containers when
    they run so that

    code compiled outside the container can run inside the container. That''s rarely
    something you want to

    do, as one of the main point of containers is that they''re stable and reproducible.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1536783389.0
UNR-HPC/singularity-recipes:
  data_format: 2
  description: null
  filenames:
  - QE/Singularity.QuantumESPRESSO-6.3-intel-2018b-unrrc
  full_name: UNR-HPC/singularity-recipes
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipes" class="anchor" href="#singularity-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-recipes</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1544465938.0
UPPMAX/install-methods:
  data_format: 2
  description: Install methods for UPPMAX modules plus some helper scripts
  filenames:
  - singularity_info/bonito/Singularity.bonito
  - singularity_info/gapseq-RT-227932/Singularity.gapseq
  - singularity_info/metaWRAP_1.3.2/Singularity.metaWRAP
  full_name: UPPMAX/install-methods
  latest_release: null
  readme: '<h1>

    <a id="user-content-module-installation-methods" class="anchor" href="#module-installation-methods"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Module
    Installation Methods</h1>

    <p>This is a collection of READMEs generated during installation of software

    applications on Uppmax clusters.  It is incomplete in terms of modules

    available on Uppmax, and the individual READMEs may also be incomplete in terms

    of what was actually done to install the modules.  We are publicising these in

    the hopes that they can be helpful.</p>

    <h2>

    <a id="user-content-example-workflow-of-a-basic-installation" class="anchor" href="#example-workflow-of-a-basic-installation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example
    workflow of a basic installation</h2>

    <ol>

    <li>Clone the install methods git repo (<code>git clone https://github.com/UPPMAX/install-methods.git</code>)</li>

    <li>Add the repo to your <code>$PATH</code> and source the <code>uppmax_functions.sh</code>
    file to get access to the functions.</li>

    <li>Run <code>run_makeroom</code> with at least <code>-t</code> and <code>-v</code>,
    to generate a <code>.sh</code> (<code>makeroom_toolname_version.sh</code>) file
    that will create the directory structure needed in <code>/sw</code>

    </li>

    <li>Run the <code>.sh</code> file created in the directory you are standing to
    create the directory structure (<code>/sw/category/toolname/</code> and <code>/sw/mf/common/category</code>)
    and template files.</li>

    <li>Put the source code for the program in <code>/sw/category/toolname/version/src</code>

    </li>

    <li>Compile and/or install the tool in <code>/sw/category/toolname/version/cluster/bin</code>
    etc.</li>

    <li>Edit the readme file, explaining how you did the installation, in <code>/sw/category/toolname/toolname-version_install-README.md</code>

    </li>

    <li>Edit the template module file <code>/sw/category/toolname/mf/version</code>
    to do what you want when the module loads.</li>

    <li>Copy the module file to the live location, <code>/sw/mf/common/category/[section]/toolname</code>

    </li>

    <li>Run <code>all_mflink toolname version</code> to create links for all clusters
    to the module file in <code>/sw/mf/common/category/[section]/toolname</code>

    </li>

    <li>Run <code>fixup /sw/category/toolname/version /sw/mf/common/category/[section]/toolname</code>
    to make sure the ownership and permissions are ok.</li>

    </ol>

    <h2>

    <a id="user-content-scripts" class="anchor" href="#scripts" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Scripts</h2>

    <p><code>gather-READMEs.sh</code> - bash script to scan installation directories,
    looking for

    README files having a particular filename format that we create during

    installation of tools</p>

    <p><code>fixup</code> - bash script fixing up permissions and group membership
    within

    installation trees; our local installation group is <code>sw</code>. With the
    <code>-g</code> option,

    this script will <code>chmod g+s</code> directories in the tree, too.</p>

    <p><code>uppmax_functions.sh</code> - bash helper functions for SLURM job viewing
    and various

    module-related tasks, mostly to do with setting up mf files for loading

    modules; the latter require appexpert privileges.  Source these from <code>.bashrc</code>.</p>

    <h2>

    <a id="user-content-installation-directories" class="anchor" href="#installation-directories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation
    directories</h2>

    <p>The directories contain software installations in major subject areas.</p>

    <h3>

    <a id="user-content-apps" class="anchor" href="#apps" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>apps/</h3>

    <p>General applications.</p>

    <h3>

    <a id="user-content-appsbioinfo" class="anchor" href="#appsbioinfo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>apps/bioinfo/</h3>

    <p>Bioinformatics applications.</p>

    <h3>

    <a id="user-content-libs" class="anchor" href="#libs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>libs/</h3>

    <p>Libraries.</p>

    <h3>

    <a id="user-content-comp" class="anchor" href="#comp" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>comp/</h3>

    <p>Compilers, interpreters, build tools.</p>

    <h2>

    <a id="user-content-database-directories" class="anchor" href="#database-directories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Database
    directories</h2>

    <p>These directories cover installations of databases updated either manually,
    or via update scripts.</p>

    <h3>

    <a id="user-content-data_uppnex" class="anchor" href="#data_uppnex" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>data_uppnex/</h3>

    <p>Installation instructions for databases under <code>/sw/data/uppnex/</code>.  Database

    directories containing <code>*-install-README.md</code> files are updated manually.

    Database directories containing <code>*-db-README.md</code> files and scripts
    (currently,

    <code>Kraken</code>, <code>diamond_databases</code> and <code>RTG</code>) are
    updated monthly via crontab entries.</p>

    <p>Blast database updates are included here, and involve multiple scripts, crontab

    entries and a test directory.  These are updated monthly via crontab entries.</p>

    <h3>

    <a id="user-content-data_other" class="anchor" href="#data_other" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>data_other/</h3>

    <p>Installation instructions for databases under other locations, currently just

    <code>BUSCO</code> lineage sets, which are kept in the module installation directory.

    These are updated monthly via crontab entries.</p>

    '
  stargazers_count: 3
  subscribers_count: 5
  topics: []
  updated_at: 1623153589.0
UniversalDataTool/universal-data-tool:
  data_format: 2
  description: Collaborate & label any type of data, images, text, or documents, in
    an easy web interface or desktop app.
  filenames:
  - Singularity
  full_name: UniversalDataTool/universal-data-tool
  latest_release: v0.14.26
  readme: "<h1>\n<a id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"\
    #universal-data-tool\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Universal Data Tool</h1>\n<p><a href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ alt=\"Master Branch\" style=\"max-width:100%;\"></a>\n<a href=\"https://badge.fury.io/js/universal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    ><img src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\"\
    \ alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\"\
    \ alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://twitter.com/UniversalDataTl\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\"\
    \ alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Try it out at <a href=\"https://udt.dev\"\
    \ rel=\"nofollow\">udt.dev</a>, <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >download the desktop app</a> or <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">run on-premise</a>.</p>\n<p align=\"center\">\n  <a href=\"\
    https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p align=\"center\">\n  <b>\n  <a href=\"\
    https://docs.universaldatatool.com\" rel=\"nofollow\">Docs</a> \u2022 <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">Website</a> \u2022 <a href=\"\
    https://udt.dev\" rel=\"nofollow\">Playground</a> \u2022 <a href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\"\
    \ rel=\"nofollow\">Library Usage</a> \u2022 <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">On-Premise</a>\n  </b>\n</p>\n<p>The Universal Data Tool is\
    \ a web/desktop app for editing and annotating images, text, audio, documents\
    \ and to view and edit any data defined in the extensible <a href=\"https://github.com/UniversalDataTool/udt-format\"\
    >.udt.json and .udt.csv standard</a>.</p>\n<h2>\n<a id=\"user-content-supported-data\"\
    \ class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Supported Data</h2>\n<p align=\"\
    center\">\n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\"\
    \ rel=\"nofollow\">Image Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\"\
    \ rel=\"nofollow\">Image Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\"\
    \ rel=\"nofollow\">Text Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\"\
    \ rel=\"nofollow\">Named Entity Recognition</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\"\
    \ rel=\"nofollow\">Named Entity Relations / Part of Speech Tagging</a> \u2022\
    \ \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\"\
    \ rel=\"nofollow\">Audio Transcription</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\"\
    \ rel=\"nofollow\">Data Entry</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\"\
    \ rel=\"nofollow\">Video Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\"\
    \ rel=\"nofollow\">Landmark / Pose Annotation</a>\n</p>\n<h2>\n<a id=\"user-content-recent-updates\"\
    \ class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recent Updates</h2>\n<p><a href=\"\
    https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\">Follow\
    \ our development on Youtube!</a></p>\n\n<ul>\n<li><a href=\"https://youtu.be/q20WrCRcG4k\"\
    \ rel=\"nofollow\">Community Update Video 9</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\"\
    \ rel=\"nofollow\">Community Update Video 8</a></li>\n<li>\n<a href=\"https://youtu.be/glPPFgXibdw\"\
    \ rel=\"nofollow\">Community Update Video 7</a> <a href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\"\
    \ rel=\"nofollow\">(blog version)</a>\n\n</li>\n</ul>\n<h2>\n<a id=\"user-content-features\"\
    \ class=\"anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Features</h2>\n<ul>\n<li><strong>Collaborate\
    \ with others in real time, no sign up!</strong></li>\n<li>Usable on <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">web</a> or as <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\
    >Windows,Mac or Linux desktop application</a>\n</li>\n<li>Configure your project\
    \ with an easy-to-use GUI</li>\n<li><a href=\"https://universaldatatool.com/courses\"\
    \ rel=\"nofollow\">Easily create courses to train your labelers</a></li>\n<li>Download/upload\
    \ as easy-to-use CSV (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\
    >sample.udt.csv</a>) or JSON (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\
    >sample.udt.json</a>)</li>\n<li>Support for Images, Videos, PDFs, Text, Audio\
    \ Transcription and many other formats</li>\n<li>Can be <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\
    >easily integrated into a React application</a>\n</li>\n<li>Annotate images or\
    \ videos with classifications, tags, bounding boxes, polygons and points</li>\n\
    <li>Fast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly</li>\n\
    <li>Import data from Google Drive, Youtube, CSV, Clipboard and more</li>\n<li>Annotate\
    \ NLP datasets with Named Entity Recognition (NER), classification and Part of\
    \ Speech (PoS) tagging.</li>\n<li>Easily <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\
    >load into pandas</a> or <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\
    >use with fast.ai</a>\n</li>\n<li>Runs <a href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\"\
    \ rel=\"nofollow\">with docker</a> <code>docker run -p 3000:3000 universaldatatool/universaldatatool</code>\n\
    </li>\n<li>Runs <a href=\"https://singularity-hub.org/collections/4792\" rel=\"\
    nofollow\">with singularity</a> <code>singularity run universaldatatool/universaldatatool</code>\n\
    </li>\n</ul>\n<p align=\"center\"><kbd><a href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<h2>\n<a id=\"user-content-sponsors\"\
    \ class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sponsors</h2>\n<p><a href=\"\
    https://wao.ai\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\"\
    \ alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"></a>\n<a href=\"\
    https://momentum-tech.ca/\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\"\
    \ alt=\"momentum image\" style=\"max-width:100%;\"></a>\n<a href=\"https://www.enabledintelligence.net/\"\
    \ rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\"\
    \ alt=\"enabled intelligence image\" style=\"max-width:100%;\"></a></p>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <h3>\n<a id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Web\
    \ App</h3>\n<p>Just visit <a href=\"https://universaldatatool.com\" rel=\"nofollow\"\
    >universaldatatool.com</a>!</p>\n<p><em>Trying to run the web app locally? Run\
    \ <code>npm install</code> then <code>npm run start</code> after cloning this\
    \ repository to start the web server.</em></p>\n<h3>\n<a id=\"user-content-desktop-application\"\
    \ class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Desktop Application</h3>\n<p>Download\
    \ the latest release from the <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >releases page</a> and run the executable you downloaded.</p>\n<h2>\n<a id=\"\
    user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n\
    <ul>\n<li>(Optional) Say hi in the <a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\">Slack channel</a>!</li>\n<li>Read <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\
    >this guide to get started with development</a>.</li>\n</ul>\n<h2>\n<a id=\"user-content-contributors-\"\
    \ class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors <g-emoji class=\"\
    g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\
    >\u2728</g-emoji>\n</h2>\n<p>Thanks goes to these wonderful people (<a href=\"\
    https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\">emoji key</a>):</p>\n\
    \n\n\n<table>\n  <tr>\n    <td align=\"center\">\n<a href=\"https://twitter.com/seveibar\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Severin Ibarluzea</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://puskuruk.github.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Puskuruk</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/CedricJean\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>CedricJean</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://berupon.hatenablog.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>beru</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Ownmarc\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Marc</b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Wafaa-arbash\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Wafaa-arbash</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/pgrimaud\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Pierre Grimaud</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/sreevardhanreddi\"><img src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>sreevardhanreddi</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/mrdadah\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Mohammed Eldadah</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://x8795278.blogspot.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>x213212</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hysios\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>hysios </b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://congdv.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Cong Dao</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Renato Junior</b></sub></a><br><a\
    \ href=\"#translation-MrJunato\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://gitlab.com/rickstaa\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Rick</b></sub></a><br><a\
    \ href=\"#translation-rickstaa\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/anaplian\"><img src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>anaplian</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.behance.net/MiguelCarvalho13\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Miguel Carvalho</b></sub></a><br><a\
    \ href=\"#translation-miguelcarvalho13\" title=\"Translation\"><g-emoji class=\"\
    g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://kyleo.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Kyle OBrien</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hakkiyagiz\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Hakk\u0131 Ya\u011F\u0131z ERD\u0130\
    N\xC7</b></sub></a><br><a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/jvdavim\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Jo\xE3o Victor Davim</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n</table>\n\n\n\n<p>This project follows the <a\
    \ href=\"https://github.com/all-contributors/all-contributors\">all-contributors</a>\
    \ specification. Contributions of any kind welcome!</p>\n"
  stargazers_count: 1447
  subscribers_count: 33
  topics:
  - computer-vision
  - annotate-images
  - entity-recognition
  - desktop
  - classification
  - dataset
  - annotation-tool
  - deep-learning
  - text-annotation
  - named-entity-recognition
  - text-labeling
  - semantic-segmentation
  - image-segmentation
  - image-labeling-tool
  - machine-learning
  - image-annotation
  - csv
  - labeling
  - labeling-tool
  - hacktoberfest
  updated_at: 1625034479.0
VIB-CBD/scanpy-images:
  data_format: 2
  description: Docker and Singularity images for Scanpy
  filenames:
  - Singularity
  full_name: VIB-CBD/scanpy-images
  latest_release: null
  readme: '<p>Dependency full Scanpy Docker and Scanpy images based on Alpine.</p>

    <p>Includes:</p>

    <ul>

    <li>Loompy</li>

    <li>Louvain</li>

    <li>igraph</li>

    <li>ipython</li>

    <li>Jupyter</li>

    <li>Cython</li>

    <li>MulticoreTSNE</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1556798801.0
VUIIS/FMRIQA_app:
  data_format: 2
  description: ' Build for docker and singularity containers for FMRIQA'
  filenames:
  - Singularity
  - Singularity.4.0.0
  full_name: VUIIS/FMRIQA_app
  latest_release: null
  readme: '<h1>

    <a id="user-content-fmriqa_app" class="anchor" href="#fmriqa_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FMRIQA_app</h1>

    <p>This includes everything required (except for the "spm12r6225_with_vbm8r435_compiled"
    directory and "FMRIQA_v4_0_0" compiled MATLAB executable, which are too large
    to commit) to build a docker and corresponding singularity container for the FMRIQA
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/fmriqa/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://www.singularity-hub.org/collections/920" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/FMRIQA_app.git

    cd FMRIQA_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have "spm12r6225_with_vbm8r435_compiled" directory and "FMRIQA_v4_0_0"
    compiled MATLAB executable.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/fmriqa

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/FMRIQA_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1592512932.0
VUIIS/Multi_Atlas_app:
  data_format: 2
  description: ' Build for docker and singularity containers for Multi Atlas'
  filenames:
  - Singularity
  - Singularity.2.1.0
  full_name: VUIIS/Multi_Atlas_app
  latest_release: null
  readme: '<h1>

    <a id="user-content-multi_atlas_app" class="anchor" href="#multi_atlas_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Multi_Atlas_app</h1>

    <p>This includes everything required (except for the "full-multi-atlas" directory)
    to build a docker and corresponding singularity container for the Multi Atlas
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/multi_atlas/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://singularity-hub.org/collections/734" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Multi_Atlas_app.git

    cd Multi_Atlas_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have full-multi-atlas directory which contains atlases.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/multi_atlas

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Multi_Atlas_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1592512956.0
VUIIS/Temporal_Lobe_app:
  data_format: 2
  description: ' Build for docker and singularity containers for temporal lobe segmentation'
  filenames:
  - Singularity
  - Singularity.3.1.0
  full_name: VUIIS/Temporal_Lobe_app
  latest_release: null
  readme: '<h1>

    <a id="user-content-temporal_lobe_app" class="anchor" href="#temporal_lobe_app"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Temporal_Lobe_app</h1>

    <p>This includes everything required to build a docker and corresponding singularity
    container for the Temporal Lobe pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/temporal_lobe/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://www.singularity-hub.org/collections/828" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Temporal_Lobe_app.git

    cd Temporal_Lobe_app/

    ./build.sh

    </code></pre>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/temporal_lobe

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Temporal_Lobe_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1592512741.0
XSEDE/nix-container-perfzero:
  data_format: 2
  description: A container for running the perfzero benchmark in an XSEDE environment
  filenames:
  - Singularity
  full_name: XSEDE/nix-container-perfzero
  latest_release: null
  readme: '<h1>

    <a id="user-content-nix-container-perfzero" class="anchor" href="#nix-container-perfzero"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nix-container-perfzero</h1>


    <p>Singularity container with Nix to be used in XSEDE compute environment (<strong>currently
    in development</strong>)</p>

    '
  stargazers_count: 1
  subscribers_count: 12
  topics: []
  updated_at: 1623176998.0
XSEDE/nix-container-sc-benchmark:
  data_format: 2
  description: Container files for sc-benchmark in Docker and Singularity with Nix
  filenames:
  - Singularity
  full_name: XSEDE/nix-container-sc-benchmark
  latest_release: null
  readme: '<h1>

    <a id="user-content-nix-container-sc-benchmark" class="anchor" href="#nix-container-sc-benchmark"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nix-container-sc-benchmark</h1>

    <p><a href="https://singularity-hub.org/collections/5358" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity container with Nix to be used in XSEDE compute environment (currently
    in development)</p>

    '
  stargazers_count: 1
  subscribers_count: 11
  topics: []
  updated_at: 1623176988.0
XSEDE/singularity-nix-base:
  data_format: 2
  description: Singularity base container with Nix to be used in XSEDE compute environment
    (currently in development)
  filenames:
  - Singularity
  full_name: XSEDE/singularity-nix-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-nix-base" class="anchor" href="#singularity-nix-base"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-nix-base</h1>

    <p><a href="https://singularity-hub.org/collections/4462" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity base container with Nix to be used in XSEDE compute environment
    (currently in development)</p>

    '
  stargazers_count: 0
  subscribers_count: 12
  topics:
  - nix
  - singularity
  - singularity-nix
  updated_at: 1623176963.0
XSEDE/singularity-nix-openmpi:
  data_format: 2
  description: Singularity container with Nix and OpenMPI to be used in XSEDE compute
    environment (currently in development)
  filenames:
  - Singularity
  full_name: XSEDE/singularity-nix-openmpi
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-nix-openmpi" class="anchor" href="#singularity-nix-openmpi"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-nix-openmpi</h1>

    <p><a href="https://singularity-hub.org/collections/4462" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity container with Nix and OpenMPI to be used in XSEDE compute environment
    (currently in development)</p>

    '
  stargazers_count: 0
  subscribers_count: 12
  topics: []
  updated_at: 1623177060.0
Zinoex/hyperverlet:
  data_format: 2
  description: null
  filenames:
  - Singularity.def
  full_name: Zinoex/hyperverlet
  latest_release: null
  readme: '<h1>

    <a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Results</h1>

    <h2>

    <a id="user-content-spring-mass-h--02" class="anchor" href="#spring-mass-h--02"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Spring
    mass h = 0.2</h2>

    <h3>

    <a id="user-content-hypereuler" class="anchor" href="#hypereuler" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperEuler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4</a></p>

    <h3>

    <a id="user-content-heun" class="anchor" href="#heun" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Heun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4</a></p>

    <h3>

    <a id="user-content-hyperheun" class="anchor" href="#hyperheun" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperHeun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4</a></p>

    <h3>

    <a id="user-content-velocity-verlet" class="anchor" href="#velocity-verlet" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Velocity Verlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4</a></p>

    <h3>

    <a id="user-content-hyperverlet" class="anchor" href="#hyperverlet" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperVerlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4</a></p>

    <h3>

    <a id="user-content-fr4" class="anchor" href="#fr4" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>FR4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4</a></p>

    <h3>

    <a id="user-content-rk4" class="anchor" href="#rk4" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>RK4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4</a></p>

    <h2>

    <a id="user-content-pendulum-h--008" class="anchor" href="#pendulum-h--008" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pendulum h = 0.08</h2>

    <h3>

    <a id="user-content-euler" class="anchor" href="#euler" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Euler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4</a></p>

    <h3>

    <a id="user-content-hypereuler-1" class="anchor" href="#hypereuler-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperEuler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4</a></p>

    <h3>

    <a id="user-content-heun-1" class="anchor" href="#heun-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Heun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4</a></p>

    <h3>

    <a id="user-content-hyperheun-1" class="anchor" href="#hyperheun-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperHeun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4</a></p>

    <h3>

    <a id="user-content-velocity-verlet-1" class="anchor" href="#velocity-verlet-1"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Velocity
    Verlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4</a></p>

    <h3>

    <a id="user-content-hyperverlet-1" class="anchor" href="#hyperverlet-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperVerlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4</a></p>

    <h3>

    <a id="user-content-fr4-1" class="anchor" href="#fr4-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FR4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4</a></p>

    <h3>

    <a id="user-content-rk4-1" class="anchor" href="#rk4-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>RK4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4</a></p>

    <h2>

    <a id="user-content-three-body-spring-mass-h--006" class="anchor" href="#three-body-spring-mass-h--006"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Three
    body spring mass h = 0.06</h2>

    <h3>

    <a id="user-content-euler-1" class="anchor" href="#euler-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Euler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4</a></p>

    <h3>

    <a id="user-content-hypereuler-2" class="anchor" href="#hypereuler-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperEuler</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4</a></p>

    <h3>

    <a id="user-content-heun-2" class="anchor" href="#heun-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Heun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4</a></p>

    <h3>

    <a id="user-content-hyperheun-2" class="anchor" href="#hyperheun-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperHeun</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4</a></p>

    <h3>

    <a id="user-content-velocity-verlet-2" class="anchor" href="#velocity-verlet-2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Velocity
    Verlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4</a></p>

    <h3>

    <a id="user-content-hyperverlet-2" class="anchor" href="#hyperverlet-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HyperVerlet</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4</a></p>

    <h3>

    <a id="user-content-fr4-2" class="anchor" href="#fr4-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FR4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4</a></p>

    <h3>

    <a id="user-content-rk4-2" class="anchor" href="#rk4-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>RK4</h3>

    <p><a href="https://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4"
    rel="nofollow">https://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623925877.0
abs-tudelft/ArrowSAM:
  data_format: 2
  description: Genomics datastructures using Apache Arrow
  filenames:
  - Singularity/Singularity
  full_name: abs-tudelft/ArrowSAM
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-arrowsam\" class=\"anchor\" href=\"#arrowsam\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ArrowSAM</h1>\n<p>ArrowSAM is an in-memory Sequence Alignment/Map\
    \ (SAM) representation which uses <a href=\"https://arrow.apache.org/\" rel=\"\
    nofollow\">Apache Arrow framework</a> (A cross-language development platform for\
    \ in-memory data) and <a href=\"https://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/\"\
    \ rel=\"nofollow\">Plasma (Shared-Memory) Object Store</a> to store and process\
    \ SAM columnar data in-memory.</p>\n<h3>\n<a id=\"user-content-citing-arrowsam\"\
    \ class=\"anchor\" href=\"#citing-arrowsam\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-cite\"\
    ></a>Citing ArrowSAM</h3>\n<p>The following paper describes the ArrowSAM format\
    \ and its usage to speedup genomics pipelines. If you use ArrowSAM in your work,\
    \ please cite the following paper.</p>\n<blockquote>\n<p>Ahmad et al., (2020).\
    \ \"ArrowSAM: In-Memory Genomics Data Processing Using Apache Arrow\",\n<em>ICCAIS</em>.\
    \ <a href=\"https://doi.org/10.1109/ICCAIS48893.2020.9096725\" rel=\"nofollow\"\
    >doi.org/10.1109/ICCAIS48893.2020.9096725</a></p>\n</blockquote>\n<blockquote>\n\
    <p>Ahmad et al., \"Optimizing performance of GATK workflows using Apache Arrow\
    \ In-Memory data framework\",\n<em>BMC Genomics, presented at APBC2020</em>. <a\
    \ href=\"https://doi.org/10.1186/s12864-020-07013-y\" rel=\"nofollow\">https://doi.org/10.1186/s12864-020-07013-y</a></p>\n\
    </blockquote>\n<p>This repo contains following three components:</p>\n<ol>\n<li>\n\
    <p>ArrowSAM (In-memory SAM data representation) integrated <a href=\"https://github.com/tahashmi/bwa\"\
    >BWA-MEM</a>, Picard and GATK tools.<br></p>\n</li>\n<li>\n<p>A Singularity container\
    \ def file (To create an environment to use all Apache Arrow related tools and\
    \ libraries for ArrowSAM).<br></p>\n</li>\n<li>\n<p>Scripts to run different GATK\
    \ best practices recommended workflows (using different in-memory data placement\
    \ techniques like ArrowSAM, ramDisk and pipes for fast processing) to run complete\
    \ DNA analysis pipeline efficiently.<br></p>\n</li>\n</ol>\n<p>Note: ArrowSAM\
    \ and all other workflows are based on single node, multi-core machines.</p>\n\
    <h2>\n<a id=\"user-content-how-to-run\" class=\"anchor\" href=\"#how-to-run\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to run</h2>\n<ol>\n<li>\n<p>Install <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> container</p>\n</li>\n<li>\n<p>Download our\
    \ Singularity <a href=\"https://github.com/abs-tudelft/arrow-gen/tree/master/Singularity\"\
    >script</a> and generate singularity image (this image contains all Arrow related\
    \ packges necessary for building/compiling BWA-MEM, Picard and GATK)</p>\n</li>\n\
    <li>\n<p>Now enter into generated image using command:</p>\n<pre><code> sudo singularity\
    \ shell &lt;image_name&gt;.simg\n</code></pre>\n</li>\n<li>\n<p>Download <a href=\"\
    https://github.com/tahashmi/bwa\">BWA-MEM</a> inside image</p>\n<pre><code> git\
    \ clone https://github.com/tahashmi/bwa.git\n</code></pre>\n</li>\n<li>\n<p>Go\
    \ into bwa dir and compile BWA-MEM:</p>\n<pre><code> cd bwa\n make\n</code></pre>\n\
    </li>\n<li>\n<p>Now you can run BWA-MEM.</p>\n</li>\n</ol>\n"
  stargazers_count: 15
  subscribers_count: 3
  topics: []
  updated_at: 1624522487.0
aces/cbrain-containers-recipes:
  data_format: 2
  description: Recipes for singularity and docker containers used in CBRAIN
  filenames:
  - ANTs/Singularity.ants_v2.1.0-gGIT-N
  - QEEG/Singularity.qeeg.v1.0-gGit-S
  - FreeSurfer/Singularity.FreeSurfer_v5.3
  - dcm2nii/Singularity.dcm2nii_v4AUGUST2014
  - FSL/Singularity.fsl_v6.0.1
  - FSL/Singularity.fsl_v5.0.9
  full_name: aces/cbrain-containers-recipes
  latest_release: null
  readme: '<h1>

    <a id="user-content-cbrain-containers-recipes" class="anchor" href="#cbrain-containers-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cbrain-containers-recipes</h1>

    <p>Recipes for singularity and docker containers used in CBRAIN</p>

    '
  stargazers_count: 1
  subscribers_count: 9
  topics:
  - singularity
  - docker
  - cbrain
  updated_at: 1568207307.0
aertslab/SCope:
  data_format: 2
  description: Fast visualization tool for large-scale and high dimensional single-cell
    data
  filenames:
  - Singularity
  full_name: aertslab/SCope
  latest_release: untagged-64f13ec6d922418df08e
  readme: "<p><a href=\"https://www.codefactor.io/repository/github/aertslab/scope\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2a72055500b7462275ffcdff1c3045b8ec007caf55f0aabab6980bf1816e7e60/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f61657274736c61622f73636f70652f6261646765\"\
    \ alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/aertslab/scope/badge\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-scope-v182-visualization-of-large-scale-and-high-dimensional-single-cell-data\"\
    \ class=\"anchor\" href=\"#scope-v182-visualization-of-large-scale-and-high-dimensional-single-cell-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>SCope v1.8.2: Visualization of large-scale and high dimensional single\
    \ cell data</h1>\n<p><a href=\"images/SCope_Logo.png\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img src=\"images/SCope_Logo.png\" width=\"640\" style=\"\
    max-width:100%;\"></a></p>\n<p>SCope is a fast visualization tool for large-scale\
    \ and high dimensional scRNA-seq datasets.\nCurrently the data format supported\
    \ by SCope is <code>.loom</code>. This file format for very large omics datasets\
    \ is maintained by the Linnarsson Lab through the <code>loompy</code> Python package\
    \ (<a href=\"https://github.com/linnarsson-lab/loompy\">https://github.com/linnarsson-lab/loompy</a>).</p>\n\
    <p>View the <a href=\"CHANGELOG.md\">change log here</a>.</p>\n<h2>\n<a id=\"\
    user-content-demo\" class=\"anchor\" href=\"#demo\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Demo</h2>\n<p>Visit\
    \ <a href=\"https://scope.aertslab.org\" rel=\"nofollow\">https://scope.aertslab.org</a>\
    \ to test out SCope on several published datasets! Personal loom file files can\
    \ be uploaded but will only be kept for 5 days.</p>\n<h2>\n<a id=\"user-content-loom-file-generation\"\
    \ class=\"anchor\" href=\"#loom-file-generation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Loom File Generation</h2>\n<p>Currently\
    \ there are two packages to generate extended loom files compatible with SCope.</p>\n\
    <ul>\n<li>R: <a href=\"https://github.com/aertslab/SCopeLoomR\">SCopeLoomR</a>\
    \ - Dedicated R package</li>\n<li>Python: <a href=\"https://github.com/aertslab/pySCENIC\"\
    >pySCENIC</a> - Single function for generation from SCENIC results</li>\n</ul>\n\
    <p>Eventually the functionality from pySCENIC will be expanded and put in its\
    \ own python package.</p>\n<h2>\n<a id=\"user-content-run-scope\" class=\"anchor\"\
    \ href=\"#run-scope\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Run SCope</h2>\n<h3>\n<a id=\"user-content-standalone-app\"\
    \ class=\"anchor\" href=\"#standalone-app\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Standalone App</h3>\n<p>Standalone\
    \ apps for <strong>macOS</strong> and <strong>Linux</strong> can be downloaded\
    \ from <a href=\"https://github.com/aertslab/SCope/releases\">the releases page.</a>.</p>\n\
    <p><g-emoji class=\"g-emoji\" alias=\"exclamation\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2757.png\"\
    >\u2757</g-emoji> SCope standalone app requires Node.js (&gt; v9). To install\
    \ it, go to <a href=\"https://nodejs.org/en/download/\" rel=\"nofollow\">https://nodejs.org/en/download/</a>.</p>\n\
    <p>A <strong>Windows</strong> app is under development, but currently has no ETA.</p>\n\
    <h3>\n<a id=\"user-content-command-line\" class=\"anchor\" href=\"#command-line\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Command Line</h3>\n<p>You will need access to at least Python 3.7\
    \ do run this.</p>\n<ol>\n<li>Clone the GitHub repository and install,</li>\n\
    </ol>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Define where you want to clone the SCope repository.</span>\n\
    LOCAL_SCOPE_REPO=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${HOME}</span>/repos/SCope<span class=\"pl-pds\">\"</span></span>\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Clone SCope git repository.</span>\n\
    git clone https://github.com/aertslab/SCope <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Go to your local cloned SCope repository.</span>\n<span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >${LOCAL_SCOPE_REPO}</span><span class=\"pl-pds\">\"</span></span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Install SCope.</span>\nnpm install</pre></div>\n\
    <ol start=\"2\">\n<li>Run,</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Go to your local cloned\
    \ SCope repository.</span>\n<span class=\"pl-c1\">cd</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span\
    \ class=\"pl-pds\">\"</span></span>\nSCOPE_CONFIG=config.json npm run scope</pre></div>\n\
    <h2>\n<a id=\"user-content-deploy-a-cloud-based-instance\" class=\"anchor\" href=\"\
    #deploy-a-cloud-based-instance\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Deploy a Cloud-based Instance</h2>\n\
    <h3>\n<a id=\"user-content-amazon-web-services\" class=\"anchor\" href=\"#amazon-web-services\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Amazon Web Services</h3>\n<h4>\n<a id=\"user-content-public-ami\"\
    \ class=\"anchor\" href=\"#public-ami\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Public AMI</h4>\n<p>No ETA.</p>\n\
    <h4>\n<a id=\"user-content-source\" class=\"anchor\" href=\"#source\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Source</h4>\n\
    <p>To create a SCope AWS instance from scratch please read the tutorial <a href=\"\
    https://github.com/aertslab/SCope/tree/master/tutorials/aws-deployment-source\"\
    >aws-deployment-source</a>.</p>\n<h2>\n<a id=\"user-content-features\" class=\"\
    anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Features</h2>\n<h3>\n<a id=\"user-content-enabling-orcid-functionality\"\
    \ class=\"anchor\" href=\"#enabling-orcid-functionality\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Enabling\
    \ ORCID Functionality</h3>\n<p>To enable colaborative annotations and login via\
    \ ORCID ID, API credentials (<code>orcidAPIClientID</code>, <code>orcidAPIClientSecret</code>\
    \ and <code>orcidAPIRedirectURI</code>) must be added to the config file provided.\n\
    These can be generated at the <a href=\"https://orcid.org/developer-tools\" rel=\"\
    nofollow\">orcid developer tools page</a>.</p>\n<p>The <code>dataHashSecret</code>\
    \ entry in the config file should be filled in with a randomly generated string\
    \ for example from the python <a href=\"https://docs.python.org/3/library/secrets.html\"\
    \ rel=\"nofollow\">secrets package</a>.\nThis string will be used to salt all\
    \ annotation data, allowing validation of data generated on the instance of SCope.\
    \ Any changes in this string will invalidate all pre-existing annotations.</p>\n\
    <h2>\n<a id=\"user-content-development\" class=\"anchor\" href=\"#development\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Development</h2>\n<ol>\n<li>Clone the GitHub repository and install,</li>\n\
    </ol>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Define where you want to clone the SCope repository.</span>\n\
    LOCAL_SCOPE_REPO=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${HOME}</span>/repos/SCope<span class=\"pl-pds\">\"</span></span>\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Clone SCope git repository.</span>\n\
    git clone https://github.com/aertslab/SCope <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Go to your local cloned SCope repository.</span>\n<span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >${LOCAL_SCOPE_REPO}</span><span class=\"pl-pds\">\"</span></span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Install SCope.</span>\nnpm install</pre></div>\n\
    <ol start=\"2\">\n<li>Run,</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Go to your local cloned\
    \ SCope repository.</span>\n<span class=\"pl-c1\">cd</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span\
    \ class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Start SCope Server (terminal 1).</span>\n<span class=\"pl-c1\">cd</span>\
    \ opt\npoetry run hypercorn main:scope_api --reload\n\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> Start SCope Client (terminal 2).</span>\n<span class=\"\
    pl-c1\">cd</span> ..\nnpm run dev</pre></div>\n<h3>\n<a id=\"user-content-configuration-file-configjson\"\
    \ class=\"anchor\" href=\"#configuration-file-configjson\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Configuration\
    \ file (<code>config.json</code>)</h3>\n<p>Keys:</p>\n<ul>\n<li>\n<code>data</code>:\
    \ This is a directory containing data files (e.g. the <code>motd.txt</code> message\
    \ of the day).\nCan be an absolute path or a relative path from where you start\
    \ SCope. By default it is\n<code>./data/</code>.</li>\n</ul>\n<h3>\n<a id=\"user-content-deploying-scope-with-docker\"\
    \ class=\"anchor\" href=\"#deploying-scope-with-docker\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Deploying\
    \ SCope with Docker</h3>\n<p><code>docker-compose.yml</code> is configured to\
    \ spin up 2 containers: One to run the SCope backend and another to run an Apache\n\
    reverse proxy server.</p>\n<p>The SCope application will be available on port\
    \ <code>80</code> by default. You can specify a port by using env variable: <code>SCOPE_PORT</code>\n\
    before running the docker-compose command. Apache will proxy requests through\
    \ to the appropriate port inside the container.</p>\n<p>The <code>docker-compose.yml</code>\
    \ will serve the assets from inside the scope container, and the <code>docker-compose.host.yml</code>\
    \ will serve them from the host.\nThis supports as many use cases as possible,\
    \ because you can either build the assets on the host yourself using whatever\
    \ configuration you need,\nor serve them from the container if your environment\
    \ doesn't allow for that (e.g. you don't have npm installed on the host).</p>\n\
    <p>Before running the compose build, you can specify a SCOPE_PORT with: <code>docker-compose\
    \ build --build-arg SCOPE_PORT=8080</code></p>\n<p>The scope webpack assets will\
    \ have to be built with the config: <code>\"reverseProxyOn\": true</code>.\nYou\
    \ can use environment variable: <code>SCOPE_CONFIG=path to your config</code>\
    \ to specify a config file instead of changing the main one.</p>\n<p>You can configure\
    \ where the dockerised SCope data directories should be located\non the host machine\
    \ by using the env var <code>SCOPE_DATA_DIR</code> before launching the docker-compose.\n\
    The default location is <code>./scope_data</code> which will be created if you\
    \ do not specify one.</p>\n<p><strong>Note</strong>: in this config, you do not\
    \ need to specify the port in <code>publicHostAddress</code>. The env var <code>SCOPE_PORT</code>\
    \ gets appended for you.</p>\n<p>If deploying the container on a specific port\
    \ with another external apache reverse-proxy server,\nyou may have to add a config\
    \ to the external apache site config to allow http and websocket reverse-proxying.\n\
    Here is an example:</p>\n<pre><code>    ProxyPass / http://0.0.0.0:8080/\n   \
    \ RewriteEngine on\n    RewriteCond %{HTTP:Upgrade} websocket [NC]\n    RewriteCond\
    \ %{HTTP:Connection} upgrade [NC]\n    RewriteRule ^/?(.*) \"ws://0.0.0.0:8080/$1\"\
    \ [P,L]\n</code></pre>\n<h5>\n<a id=\"user-content-example-serve-from-container\"\
    \ class=\"anchor\" href=\"#example-serve-from-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Example\
    \ serve from container</h5>\n<ol>\n<li>Copy <code>config.json</code> to a new\
    \ file and modify with <code>\"reverseProxyOn\": true,</code> and <code>publicHostAddress</code>\
    \ set to your domain</li>\n<li><code>docker-compose build --build-arg SCOPE_PORT=8080</code></li>\n\
    <li><code>SCOPE_DATA_DIR=$HOME/scope_data SCOPE_PORT=8080 docker-compose up -d</code></li>\n\
    </ol>\n<h5>\n<a id=\"user-content-or-serve-from-host\" class=\"anchor\" href=\"\
    #or-serve-from-host\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>OR Serve from host</h5>\n<ol>\n<li><code>npm\
    \ run build</code></li>\n<li><code>SCOPE_DATA_DIR=$HOME/scope_data SCOPE_PORT=8080\
    \ docker-compose -f docker-compose.host.yml up -d</code></li>\n</ol>\n<p>You should\
    \ be able to visit <code>http://localhost:8080</code> and see the app!</p>\n"
  stargazers_count: 49
  subscribers_count: 8
  topics:
  - single-cell
  - large-scale-data-visualization
  - gene-expression
  - gene-regulatory-network
  - aws
  - cloud
  - loom
  - reactjs
  - grpc
  updated_at: 1624266405.0
aertslab/pySCENIC:
  data_format: 2
  description: pySCENIC is a lightning-fast python implementation of the SCENIC pipeline
    (Single-Cell rEgulatory Network Inference and Clustering) which enables biologists
    to infer transcription factors, gene regulatory networks and cell types from single-cell
    RNA-seq data.
  filenames:
  - Singularity.0.9.18
  full_name: aertslab/pySCENIC
  latest_release: 0.11.2
  readme: '<p><a href="https://singularity-hub.org/collections/702" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick start</h2>

    <pre><code># Download a (versioned) container

    singularity pull shub://MPIB/singularity-fsl:6.0.4


    # Run it

    singularity exec singularity-fsl_6.0.4.sif fslmaths

    singularity exec --nv singularity-fsl_6.0.4.sif eddy_cuda9.1

    </code></pre>

    <h2>

    <a id="user-content-fsl" class="anchor" href="#fsl" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>FSL</h2>

    <p>Project Home: <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/" rel="nofollow">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/</a></p>

    <p>These are containers primarily used at the MPI for Human Development.</p>

    <h2>

    <a id="user-content-cuda" class="anchor" href="#cuda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cuda</h2>

    <p>Starting with Singularity 6.0.2 we include Nvidia CUDA through Debian backports
    repositories.

    Make sure your Nvidia driver on the host <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility"
    rel="nofollow">supports it</a> and add the <code>--nv</code> flag with singularity.</p>

    <h2>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Note</h2>

    <p>Please be aware of FSL''s strict license regarding non-commercial use.</p>

    '
  stargazers_count: 154
  subscribers_count: 13
  topics:
  - single-cell
  - transcriptomics
  - gene-regulatory-network
  - transcription-factors
  updated_at: 1623940973.0
agaveplatform/SC17-container-tutorial:
  data_format: 2
  description: 'SC17 tutorial - "HPC via HTTP: Portable, Scalable Computing using
    App Containers and the Agave API"'
  filenames:
  - content/images/funwave-tvd/Singularity
  full_name: agaveplatform/SC17-container-tutorial
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hpc-via-http-portable-scalable-computing-using-app-containers-and-the-agave-api\"\
    \ class=\"anchor\" href=\"#hpc-via-http-portable-scalable-computing-using-app-containers-and-the-agave-api\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HPC via HTTP: Portable, Scalable Computing using App Containers and\
    \ the Agave API</h1>\n<p>Supercomputing matters. So does user experience. Standing\
    \ between the mainstream adoption of supercomputing and a new generation of users\
    \ is the reality that the entry cost to using these systems, both in terms of\
    \ dollars and in time spent learning the technology, has not significantly changed\
    \ in the last 20 years. The rise of cloud computing only complicates the learning\
    \ curve further. Over the last 6 years, the authors have been addressing this\
    \ gap through the development of a Science-as-a-Service platform enabling users\
    \ to go from their desktop, to their local data center, to the cloud, and back\
    \ without sacrificing their existing tool chain or user experience.</p>\n<p>In\
    \ this tutorial, we combine best practices and lessons learned while on-boarding\
    \ the last 70k new users to TACC\u2019s data center through the Agave Platform.\
    \ Participants will walk through the process of scaling their application from\
    \ a local environment to the Jetstream academic cloud and to a high performance\
    \ computing system at the Texas Advanced Computing Center. They will learn to\
    \ use multiple container technologies to harmonize app execution between cloud\
    \ and HPC resources, and they will learn to use modern APIs to orchestrate job\
    \ execution, capture provenance information, and foster collaboration.</p>\n<h1>\n\
    <a id=\"user-content-preview\" class=\"anchor\" href=\"#preview\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Preview</h1>\n\
    <p><a href=\"http://www.youtube.com/watch?v=hVnIrjn_aBI\" title=\"HPC via HTTP:\
    \ Portable, Scalable Computing using App Containers and the Agave API\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/c65f2550bc90ecabb4429c52335a19f58b324a579f4cb9a3f92dfcab4bc7391f/687474703a2f2f696d672e796f75747562652e636f6d2f76692f68566e49726a6e5f6142492f6d617872657364656661756c742e6a7067\"\
    \ alt=\"Intro Video\" data-canonical-src=\"http://img.youtube.com/vi/hVnIrjn_aBI/maxresdefault.jpg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-schedule\" class=\"\
    anchor\" href=\"#schedule\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Schedule</h1>\n<table>\n  <tr>\n    <th>Time</th>\n\
    \    <th>Presenterr</th>\n    <th>Topic</th>\n  </tr>\n  <tr>\n    <td>08:30 -\
    \ 08:45</td>\n    <td>John, Steve</td>\n    <td>[Introductions](01%20Introduction.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>08:45 - 09:05</td>\n    <td>Rion</td>\n    <td>[Agave\
    \ Overview](02%20Agave%20Overview.pdf)</td>\n  </tr>\n  <tr>\n    <td>09:05 -\
    \ 09:15</td>\n    <td>Kathy</td>\n    <td>[Jupyter, Sanbox, and Logging In](03%20Jupyter%2C%20Sandboxes%2C%20and%20Logging%20In.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>09:15 - 09:30</td>\n    <td>Steve</td>\n    <td>[Code,\
    \ Build, and Test](04%20Code%20Build%20and%20Test.ipynb)</td>\n  </tr>\n  <tr>\n\
    \    <td>09:30 - 10:00</td>\n    <td>Rion, John</td>\n    <td>[Hands on with Agave](05%20Hands%20on%20with%20Agave.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>10:00 - 10:30</td>\n    <td>--</td>\n    <td>Break</td>\n\
    \  </tr>\n  <tr>\n    <td>10:30 - 11:00</td>\n    <td>Steve,John</td>\n    <td>[Docker\
    \ and Singularity](06%20Docker%20and%Singularity.ipynb)</td>\n  </tr>\n  <tr>\n\
    \    <td>11:00 - 11:15</td>\n    <td>Rion</td>\n    <td>[Automation an Benchmarking](07%20Automation%20and%20Benchmarking.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>11:15 - 11:45</td>\n    <td>Kathy, Rion</td>\n    <td>[Packaging,\
    \ publishing, and Portability](08%20Packaging%20publishing%20and%20Portability.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>11:45 - 12:00</td>\n    <td>Steve, John</td>\n    <td>[Future\
    \ Directions and Homework)[09%20Future%20Directions%20and%20Homework.ipynb]</td>\n\
    \  </tr>\n</table>\n<h1>\n<a id=\"user-content-table-of-contents\" class=\"anchor\"\
    \ href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Table of Contents</h1>\n<ul>\n<li>01:\
    \ <a href=\"01-Requirements-and-Preparation.md\">Requirements and Preparation</a>\n\
    </li>\n<li>02: <a href=\"02-Installation-and-Infrastructure.md\">Installation\
    \ and Infrastructure</a>\n</li>\n<li>03: <a href=\"03-Auth-Notebooks-and-Web-Console.md\"\
    >Auth, Notebooks, and the Web Interface</a>\n</li>\n<li>04: <a href=\"04-SciOps-and-Sample-Application.md\"\
    >SciOps and our Sample Application</a>\n</li>\n<li>05: <a href=\"05-Code-Build-and-Run-Locally.md\"\
    >Code, Build, and Run Locally</a>\n</li>\n<li>06: <a href=\"06-Containerize-Existing-Applications.md\"\
    >Containerize Existing Applications</a>\n</li>\n<li>07: <a href=\"07-Automation-Registries-and-App-Catalogues\"\
    >Automation, Registries, and App Catalogues</a>\n</li>\n</ul>\n<ul>\n<li>Agave</li>\n\
    <li>CI/CD</li>\n<li>Image publishing</li>\n</ul>\n<ul>\n<li>08: <a href=\"\">Scaling\
    \ and Portability</a>\n</li>\n</ul>\n<ul>\n<li>Image caching</li>\n<li>Runtime\
    \ environments</li>\n<li>Data scheduling</li>\n<li>Reproducibility anti-patterns</li>\n\
    </ul>\n<ul>\n<li>09: <a href=\"\">Viewing simulation results, sharing, provenance</a>\n\
    </li>\n<li>10: <a href=\"\">Packaging and Publishing Experiments</a>\n</li>\n\
    <li>11: <a href=\"\">Benchmarking and Performance Considerations</a>\n</li>\n\
    <li>12: <a href=\"\">Functions, Microcodes, and Exascale</a>\n</li>\n<li>13: <a\
    \ href=\"\">Homework an Further Reading</a>\n</li>\n<li>90: <a href=\"90-Appendix-A.md\"\
    >Appendix A</a>\n</li>\n<li>99: <a href=\"99-References.md\">References</a>\n\
    </li>\n</ul>\n"
  stargazers_count: 3
  subscribers_count: 7
  topics: []
  updated_at: 1529379287.0
agladstein/SimPrily_update:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: agladstein/SimPrily_update
  latest_release: null
  readme: '<h1>

    <a id="user-content-simprily_update" class="anchor" href="#simprily_update" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SimPrily_update</h1>

    <p>Created by Ariella Gladstein, based on <a href="https://agladstein.github.io/SimPrily/index.html"
    rel="nofollow">SimPrily</a>.</p>

    <h2>

    <a id="user-content-about" class="anchor" href="#about" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>About</h2>

    <p>SimPrily runs genome simulations with user defined parameters or parameters
    randomly generated by priors and computes genomic statistics on the simulation
    output.</p>

    <ol>

    <li>Run genome simulation with model defined by prior distributions of parameters
    and demographic model structure.</li>

    <li>Take into account SNP array ascertainment bias by creating pseudo array based
    on priors of number of samples of discovery populations and allele frequency cut-off.</li>

    <li>Calculate genomic summary statistics on simulated genomes and pseudo arrays.</li>

    </ol>

    <p>This is ideal for use with Approximate Bayesian Computation on whole genome
    or SNP array data.</p>

    <p>Uses c++ programs macs and GERMLINE. For more information on these programs,
    see:<br>

    <a href="https://github.com/gchen98/macs">https://github.com/gchen98/macs</a><br>

    <a href="https://github.com/sgusev/GERMLINE">https://github.com/sgusev/GERMLINE</a></p>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>cd to the directory you want to work in,</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/agladstein/SimPrily.git</pre></div>

    <h4>

    <a id="user-content-environment-set-up" class="anchor" href="#environment-set-up"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment
    Set up</h4>

    <p>If using Vagrant (this is recommended if running on non-Linux OS):</p>

    <p>Start Vagrant, ssh into Vagrant, cd to SimPrily directory:</p>

    <div class="highlight highlight-source-shell"><pre>vagrant up

    vagrant ssh

    <span class="pl-c1">cd</span> /vagrant</pre></div>

    <p>Install the virtual environment and install the requirements.</p>

    <div class="highlight highlight-source-shell"><pre>./setup/setup_env_vbox_2.7.sh</pre></div>

    <p>If not using Vagrant, just install the virtual environment and install the
    requirements:</p>

    <div class="highlight highlight-source-shell"><pre>./setup/setup_env_2.7.sh</pre></div>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>e.g. One Test simulation:</p>

    <pre><code>python simprily.py -p examples/eg1/param_file_eg1_asc.txt -m examples/eg1/model_file_eg1_asc.csv
    -g genetic_map_b37/genetic_map_GRCh37_chr1.txt.macshs -a array_template/ill_650_test.bed
    -i 1 -o output_dir -v

    </code></pre>

    <p>For quick help:</p>

    <pre><code>python simprily.py --help

    </code></pre>

    <h4>

    <a id="user-content-input" class="anchor" href="#input" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Input</h4>

    <p><code>simprily.py</code> takes 4 required arguments and 2 optional arguments,
    and help, verbose, and profile options.</p>

    <p>Run as</p>

    <pre><code>python simprily.py [-h] -p PARAM -m MODEL -i ID -o OUT [-g MAP] [-a
    ARRAY] [-v] [--profile]

    </code></pre>

    <h5>

    <a id="user-content-required" class="anchor" href="#required" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Required</h5>

    <p><code>-p PARAM</code> or <code>--param PARAM</code> = The location of the parameter
    file<br>

    <code>-m MODEL</code> or <code>--model MODEL</code> = The location of the model
    file<br>

    <code>-i ID</code> or <code>--id ID</code> = The unique identifier of the job<br>

    <code>-o OUT</code> or <code>--out OUT</code> = The location of the output directory</p>

    <h5>

    <a id="user-content-optional" class="anchor" href="#optional" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Optional</h5>

    <p><code>-h</code> or <code>--help</code> = shows a help message and exists<br>

    <code>-v</code> = increase output verbosity. This includes 3 levels, <code>-v</code>,
    <code>-vv</code>, and <code>-vvv</code><br>

    <code>--profile</code> = Print a log file containing the time in seconds and memory
    use in Mb for main functions<br>

    <code>-g MAP</code> or <code>--map MAP</code> = The location of the genetic map
    file<br>

    <code>-a ARRAY</code> or <code>--array ARRAY</code> = The location of the array
    template file, in bed form</p>

    <h4>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h4>

    <p>Three subdirectories are created in the directory specified in the <code>output_dir</code>
    argument.</p>

    <pre><code>output_dir/results

    output_dir/sim_data

    output_dir/germline_out

    </code></pre>

    <h5>

    <a id="user-content-intermediate-files" class="anchor" href="#intermediate-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Intermediate
    files</h5>

    <p>Intermediate files go to <code>output_dir/sim_data</code> and <code>output_dir/germline_out</code>.<br>

    <code>output_dir/sim_data</code> contains PLINK formated .ped and .map files created
    from the pseudo array, which are necessary to run GERMLINE.<br>

    <code>output_dir/germline_out</code> contains the GERMLINE .match output and .log.
    The .match contains all of the identified IBD segments.<br>

    These files are NOT automatically removed in python script, but are unnecessary
    once the job is complete.</p>

    <h5>

    <a id="user-content-results-files" class="anchor" href="#results-files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Results files</h5>

    <p>Output files go to <code>output_dir/results</code>.<br>

    <code>output_dir/results</code> contains the parameter values used in the simulation
    and the summary statistics calculated from the simulation.<br>

    The first line is a header with the parameter names and summary statistics names.

    The second line is the parameter values and summary statistics values.</p>

    <hr>

    <h2>

    <a id="user-content-abc_update_wfpy" class="anchor" href="#abc_update_wfpy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ABC_update_wf.py</h2>

    <p>This script creates all the necessary files for running ABC on simulations,
    and runs ABC.</p>

    <ol>

    <li>Combines the simulated results into one file in <code>obs{}/chr{}/ABC/results_combined.txt</code>
    (unless the file already exists).</li>

    <li>For chr1 randomly picks one of the simulations to use as observed data,

    and for all other chromosomes uses the parameter values of the observed data from
    chr1 to simulate observed data,

    and create file in <code>obs{}/chr{}/ABC/results_observed.txt</code>.</li>

    <li>Run R to get PLS components.</li>

    <li>Use ABCtoolbox to transform summary stats to PLS components for simulated
    and observed data.</li>

    <li>Use ABCtoolbox to get posteriors of parameters.</li>

    <li>Create parameter file with posterior file.</li>

    </ol>

    <h3>

    <a id="user-content-usage-1" class="anchor" href="#usage-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h3>

    <div class="highlight highlight-source-shell"><pre>ABC_update_wf.py path_sim param_file_name
    chrom obs</pre></div>

    <p>where,</p>

    <ul>

    <li>

    <code>path_sim</code> is the path to simulation output (before <code>obs{}</code>)</li>

    <li>

    <code>param_file_name</code> is the parameter file used to perform the simulations</li>

    <li>

    <code>chrom</code> is the chromosome number</li>

    <li>

    <code>obs</code> is the iteration with observed data.</li>

    </ul>

    <hr>

    <h2>

    <a id="user-content-hpc-workflow" class="anchor" href="#hpc-workflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPC Workflow</h2>

    <p>For chromosome 1 use <code>checkque.sh</code> to submit jobs to Ocelote.</p>

    <p>Arguments are <code>goal_number</code>, <code>max_que</code>, <code>chr</code></p>

    <div class="highlight highlight-source-shell"><pre>/home/u15/agladstein/SimPrily_update/update_test/checkque.sh
    10000 500 1</pre></div>

    <p>Then, run ABC with <code>ABC_update_wf.py</code> with the appropriate chromosome:</p>

    <div class="highlight highlight-source-shell"><pre>rsync -za SimPrily_update/
    /xdisk/agladstein/SimPrily_update

    <span class="pl-c1">cd</span> /xdisk/agladstein/SimPrily_update

    qsub update_test/PBS/run_ABC_chr1.pbs</pre></div>

    <p>Then, run the simulations with the appropriate chromosome:</p>

    <div class="highlight highlight-source-shell"><pre>rsync -za SimPrily_update/
    /xdisk/agladstein/SimPrily_update

    <span class="pl-c1">cd</span> /xdisk/agladstein/SimPrily_update

    qsub update_test/PBS/run_sims_update_chr2.pbs</pre></div>

    <hr>

    <h2>

    <a id="user-content-known-issues" class="anchor" href="#known-issues" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Known Issues</h2>

    <ul>

    <li>If exponential growth is large, macs simulation will not finish. (This is
    a macs bug).</li>

    <li>If the same id is used with the same output dir as a previous run, the .map
    file will be appended to.</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1615073384.0
alestarbucks/ofappdl:
  data_format: 2
  description: null
  filenames:
  - ploi/planning/FD/misc/releases/latest/Singularity
  - ploi/planning/FD/misc/releases/20.06/Singularity.20.06
  - ploi/planning/FD/misc/releases/19.12/Singularity.19.12
  - ploi/planning/FD/misc/releases/19.06/Singularity.19.06
  full_name: alestarbucks/ofappdl
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-object-filtering-in-automatic-planning-problems-using-deep-learning\"\
    \ class=\"anchor\" href=\"#object-filtering-in-automatic-planning-problems-using-deep-learning\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Object Filtering in Automatic Planning Problems using Deep Learning</h1>\n\
    <p>This README file is explicitly dedicated to serve as the guide of use of the\
    \ source code associated to Alejandro \xC1lvarez Conejo's Final Bachelor Thesis\
    \ in order to run the project in any local computer. Note that these instructions\
    \ are described to be applicable to Linux-based systems.</p>\n<p>This repository\
    \ contains three main folders, which are referred to in this annex as <code>modules</code>:</p>\n\
    <ul>\n<li>The <code>ploi</code> folder contains all the code related to the execution\
    \ of the main algorithm for PLOI. It includes the code related to the guiders,\
    \ the planners (including Fast-Downward) and the GNN implementation, as well as\
    \ the main scripts that allow the whole project to work as discussed in the main\
    \ body of the thesis. Note that inside the <code>model</code> folder the model\
    \ and data set files for the conducted tests can be found.</li>\n<li>The <code>generators</code>\
    \ folder contains the scripts that were used to generate the training and test\
    \ problems. Inside, there is a folder dedicated to each of the domains of study\
    \ and all of their versions, including the scripts that were used for the first\
    \ approach described in chapter 5.3 in the <code>unconnectednoise</code> subfolder.</li>\n\
    <li>The <code>pddlgym</code> folder, which contains all the code related to the\
    \ PDDLGym module. It has to be modified in order to include the domains of study\
    \ inside its existing library of domains and example problems. Note that the original\
    \ code for this module was also modified in order to make it more flexible to\
    \ several valid syntaxes in PDDL. These modifications are not related to the core\
    \ algorithm and thus have not been thoroughly detailed but the code inside the\
    \ <code>parser</code> file of this module can be compared to the original parser\
    \ in PDDLGym\u2019s original repository in order to examine the specifics of these\
    \ changes.</li>\n</ul>\n<h2>\n<a id=\"user-content-installing-the-projects-source-code-and-dependencies\"\
    \ class=\"anchor\" href=\"#installing-the-projects-source-code-and-dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installing the project\u2019s source code and dependencies</h2>\n\
    <ol>\n<li>Install basic dependencies: cmake, g++, make, git, Python 3.6 or higher\
    \ and pip, if these are not already installed.</li>\n<li>Clone the thesis\u2019\
    \ repository using the following command:</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>git clone https://github.com/alestarbucks/ofappdl</pre></div>\n\
    <ol start=\"3\">\n<li>Navigate to the <code>ploi</code> folder and install the\
    \ requirements for that module:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pip install -r requirements.txt</pre></div>\n<p>Repeat the same operation\
    \ for the PDDLGym module.\n4.\tAdditionally, install wandb to avoid missing dependencies:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pip install wandb</pre></div>\n\
    <ol start=\"5\">\n<li>Create a symbolic link called <code>validate</code> on the\
    \ machine\u2019s path, pointing to the VAL validator\u2019s binary:</li>\n</ol>\n\
    <div class=\"highlight highlight-source-shell\"><pre>sudo ln -s <span class=\"\
    pl-k\">&lt;</span>path_to_ofappdl<span class=\"pl-k\">&gt;</span>/ofappdl/val/bin/Validate\
    \ /usr/local/bin/validate</pre></div>\n<p>In order to check that the symbolic\
    \ link is working as intended, try to enter the command <code>validate</code>\
    \ in the command line and expect an output showing the usage of the command.\n\
    6.\tBuild the Fast-Downward planner by navigating to ploi/planning/fd and running\
    \ the following command (it may take a few minutes):</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>./build.py</pre></div>\n<ol start=\"7\">\n<li>Before\
    \ the first run and every time that a new domain is added to the PDDLGym module,\
    \ re-install it using the version that exists in the repository. From the root\
    \ folder of the repository, run:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pip install -e ./pddlgym</pre></div>\n<p>This command is automatically included\
    \ in the provided shell script that runs the project, so it is not explicitly\
    \ needed to execute this step if such script is used.</p>\n<h2>\n<a id=\"user-content-including-a-new-domain\"\
    \ class=\"anchor\" href=\"#including-a-new-domain\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Including a new\
    \ domain</h2>\n<p>In order to use PLOI for the purpose of applying it to other\
    \ domains, a few changes must be made inside both the <code>pddlgym</code> module\
    \ and the <code>ploi</code> module:</p>\n<ol>\n<li>First, add the domain. Navigate\
    \ to <code>pddlgym/pddlgym/pddl</code> and copy the domain file inside that folder.</li>\n\
    <li>Likewise, add the training and test problems in two separate folders called\
    \ <code>&lt;domain name&gt;</code> and <code>&lt;domain name&gt;_test</code>,\
    \ respectively, inside the aforementioned folder.</li>\n<li>Open the <code>__init__.py</code>\
    \ file inside pddlgym/pddlgym. Locate the list of environments after line 34 (<code>for\
    \ env_name, kwargs in [</code>) and add the following lines, completing with the\
    \ same name as the domain that was added in 1:</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-python\"><pre>(<span class=\"pl-s\">\"&lt;domain name&gt;\"\
    </span>,\n    {<span class=\"pl-s\">\"operators_as_actions\"</span>: <span class=\"\
    pl-c1\">True</span>,\n    <span class=\"pl-s\">\"dynamic_action_space\"</span>:\
    \ <span class=\"pl-c1\">True</span>}\n)</pre></div>\n<ol start=\"4\">\n<li>The\
    \ domain has now been added to the PDDLGym module and now it must be included\
    \ in the PLOI module. For this, open the <code>main.py</code> file inside the\
    \ ploi module and locate the <code>pddlgym_env_names</code> dictionary. Add an\
    \ entry in which the key is the name to which the domain will be referred in the\
    \ invoking command inside the PLOI module, and the value is the name of the domain\
    \ inside the PDDLGym module that was used for steps 1 to 3. For clarity, using\
    \ the same name for both is recommended.</li>\n<li>In case of using the provided\
    \ shell script to run the project, set the <code>DOMAIN_NAME</code> variable to\
    \ match the key of the previously added entry in the dictionary mentioned in step\
    \ 4.</li>\n</ol>\n<h2>\n<a id=\"user-content-running-the-project\" class=\"anchor\"\
    \ href=\"#running-the-project\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Running the project</h2>\n<p>The main\
    \ command that triggers the start of the project\u2019s execution takes the following\
    \ parameters:</p>\n<ul>\n<li>\n<code>--domain_name</code> (required): The name\
    \ of the domain of study to which the selected method is intended to be applied.\
    \ It must be consistent and match the name chosen in the process detailed in the\
    \ previous section.</li>\n<li>\n<code>--train_planner_name</code>: The name of\
    \ the planner used for training. In the experiments detailed in this report, this\
    \ planner was fd-opt-lmcut (the optimal variant of FD).</li>\n<li>\n<code>--test_planner_name</code>\
    \ (required): The name of the planner used for testing. In the experiments detailed\
    \ in this report, this planner was fd-lama-first (the satisficing variant of FD).</li>\n\
    <li>\n<code>--guider_name</code> (required): The name of the guider to be used,\
    \ between gnn-bce-10 (GNN guider) or no-guidance (for standard planning or random\
    \ score).</li>\n<li>\n<code>--num_seeds</code> (required): The number of seeds\
    \ which will be used to randomly initialize the model\u2019s weights before training.\
    \ The learning phase will be repeated as many times as seeds are specified, and\
    \ only the best model will be selected. Only one seed was used for the experiments\
    \ in this thesis.</li>\n<li>\n<code>--num_train_problems</code> (default to 0):\
    \ The number of training problems to be considered.</li>\n<li>\n<code>--num_test_problems</code>\
    \ (required): The number of testing problems to be considered.</li>\n<li>\n<code>--do_incremental_planning</code>\
    \ (required): 1 or 0. Whether or not to use incremental planning, i.e., for PLOI\
    \ or random scoring, whether it implements random score guidance or GNN-based\
    \ guidance. For standard planning this flag must be set to 0.</li>\n<li>\n<code>--greedy_search</code>\
    \ (default to 0): 1 or 0. Indicates whether the greedy search algorithm is implemented\
    \ in the phase of training data collection.</li>\n<li>\n<code>--timeout</code>\
    \ (required): Time in seconds that each test problem is dedicated before time\
    \ running out and the problem being skipped. For this thesis, this time span was\
    \ of 120 seconds.</li>\n<li>\n<code>--num_epochs</code> (default 1001): Number\
    \ of epochs that will constitute the learning phase.\nThe command is then executed\
    \ as:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre>python3\
    \ main.py [flags]</pre></div>\n<p>The provided shell script called <code>myrun.sh</code>\
    \ inside the PLOI module serves as an easy way to control the experimental process.\
    \ The selected domain and method must be uncommented from the file and the script\
    \ will run the appropriate command to execute the required experimental run.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624570598.0
alexlancaster/pypop:
  data_format: 2
  description: Python for Population Genomics
  filenames:
  - Singularity
  full_name: alexlancaster/pypop
  latest_release: null
  readme: "<h2>\n<a id=\"user-content-python-for-population-genomics-pypop\" class=\"\
    anchor\" href=\"#python-for-population-genomics-pypop\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Python for Population\
    \ Genomics (PyPop)</h2>\n<p>PyPop is a framework for processing genotype and allele\
    \ data and running population genetic analyses.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<h3>\n<a id=\"\
    user-content-1-install-os-specific-development-environment\" class=\"anchor\"\
    \ href=\"#1-install-os-specific-development-environment\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1. Install\
    \ OS-specific development environment</h3>\n<h4>\n<a id=\"user-content-macos-x\"\
    \ class=\"anchor\" href=\"#macos-x\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>MacOS X</h4>\n<ol>\n<li>install\
    \ developer command-line tools: <a href=\"https://developer.apple.com/downloads/\"\
    \ rel=\"nofollow\">https://developer.apple.com/downloads/</a>  (includes <code>git</code>,\
    \ <code>gcc</code>)</li>\n<li>Visit <a href=\"http://macports.org\" rel=\"nofollow\"\
    >http://macports.org</a> and follow the instructions there to install the latest\
    \ version of MacPorts for your version of MacOS X.</li>\n<li>Set environment variables\
    \ to use macports version of Python and other packages, packages add the following\
    \ to <code>~/.bash_profile</code>\n</li>\n</ol>\n<pre><code>export PATH=/opt/local/bin:$PATH\n\
    export LIBRARY_PATH=/opt/local/lib/:$LIBRARY_PATH\nexport CPATH=/opt/local/include:$CPATH\n\
    </code></pre>\n<ol start=\"4\">\n<li>Rerun your bash shell login in order to make\
    \ these new exports active in your environment.  At the command line type:</li>\n\
    </ol>\n<pre><code>exec bash -login\n</code></pre>\n<h3>\n<a id=\"user-content-2-clone-the-repository\"\
    \ class=\"anchor\" href=\"#2-clone-the-repository\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Clone the\
    \ repository:</h3>\n<pre><code>git clone https://github.com/alexlancaster/pypop.git\n\
    </code></pre>\n<h3>\n<a id=\"user-content-3-install-dependencies\" class=\"anchor\"\
    \ href=\"#3-install-dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>3. Install dependencies</h3>\n<h4>\n\
    <a id=\"user-content-macos\" class=\"anchor\" href=\"#macos\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>MacOS:</h4>\n\
    <p>Install the MacPorts packages</p>\n<pre><code>  sudo port install swig-python\
    \ gsl py27-numpy py-libxml2 py27-libxslt py-setuptools py27-pip\n</code></pre>\n\
    <p>Set MacPorts to use the just-installed 2.7 MacPorts version of Python and pip:</p>\n\
    <pre><code>  sudo port select --set python python27\n  sudo port select --set\
    \ pip pip27\n</code></pre>\n<p>Check that the MacPorts version of Python is active\
    \ by typing: <code>which python</code>, if it is working correctly you should\
    \ see the following:</p>\n<pre><code>/opt/local/bin/python\n</code></pre>\n<h4>\n\
    <a id=\"user-content-linux-fedoracentosrhel\" class=\"anchor\" href=\"#linux-fedoracentosrhel\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Linux (Fedora/Centos/RHEL):</h4>\n<p>Need at least Fedora 25 for the\
    \ appropriate dependencies:</p>\n<pre><code>  sudo dnf install swig gsl-devel\
    \ python2-numpy python-libxml2 libxslt-python python-setuptools python-pip\n</code></pre>\n\
    <p>See <a href=\"DEV_NOTES.md\">DEV_NOTES.md</a> for instructions on containerizing\
    \ the install on a Centos/RHEL release.</p>\n<h4>\n<a id=\"user-content-linux-ubuntu\"\
    \ class=\"anchor\" href=\"#linux-ubuntu\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linux (Ubuntu)</h4>\n<p>Install\
    \ the following packages</p>\n<pre><code>  sudo apt install git libgsl-dev python-numpy\
    \ python-libxml2 python-libxslt1 python-setuptools python-pip\n</code></pre>\n\
    <p>The <code>swig</code> package in recent Ubuntu releases has bugs, you will\
    \ need to compile the most recent from source, see also <a href=\"DEV_NOTES.md\"\
    >DEV_NOTES.md</a> for details.</p>\n<h3>\n<a id=\"user-content-4-build\" class=\"\
    anchor\" href=\"#4-build\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>4. Build</h3>\n<pre><code>./setup.py build\n\
    </code></pre>\n<h2>\n<a id=\"user-content-examples\" class=\"anchor\" href=\"\
    #examples\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Examples</h2>\n<p>These are examples of how to use PyPop. Specify\
    \ the <code>--help</code> option to see an\nexplanation of the options available.</p>\n\
    <h3>\n<a id=\"user-content-run-a-minimal-dataset\" class=\"anchor\" href=\"#run-a-minimal-dataset\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run a minimal dataset:</h3>\n<pre><code>./bin/pypop.py -c  tests/data/minimal.ini\
    \ tests/data/USAFEL-UchiTelle-small.pop\n</code></pre>\n<p>This will generate\
    \ the following two files, an XML output file and a plain text version:</p>\n\
    <pre><code>USAFEL-UchiTelle-small-out.xml\nUSAFEL-UchiTelle-small-out.txt\n</code></pre>\n\
    <h2>\n<a id=\"user-content-running-test-suite\" class=\"anchor\" href=\"#running-test-suite\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running test suite</h2>\n<pre><code>  ./setup.py test\n</code></pre>\n\
    <p>If you run into errors, file a bug (as per Support, below), include the output\
    \ of <code>py.test</code> run in verbose mode and capturing the output</p>\n<pre><code>\
    \  py.test -s -v\n</code></pre>\n<p>(See DEV_NOTES.md for more details on installing\
    \ or running <code>py.test</code> outside the context of setuptools.)</p>\n<h2>\n\
    <a id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Support</h2>\n\
    <p>Please submit bug reports and feature requests</p>\n<pre><code>https://github.com/alexlancaster/pypop/issues\n\
    </code></pre>\n<h3>\n<a id=\"user-content-bug-reporting\" class=\"anchor\" href=\"\
    #bug-reporting\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Bug reporting</h3>\n<p>When reporting bugs, especially\
    \ during installation, please run the following and include the output:</p>\n\
    <pre><code>echo $CPATH\necho $LIBRARY_PATH\necho $PATH\nwhich python\n</code></pre>\n\
    <p>If you are running on MacOS please also run and include the output of:</p>\n\
    <pre><code>port installed\n</code></pre>\n<h2>\n<a id=\"user-content-development\"\
    \ class=\"anchor\" href=\"#development\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development</h2>\n<p>The code\
    \ for PyPop is at</p>\n<pre><code>https://github.com/alexlancaster/pypop\n</code></pre>\n\
    <h2>\n<a id=\"user-content-copyright-and-license\" class=\"anchor\" href=\"#copyright-and-license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Copyright and License</h2>\n<p>PyPop is Copyright (C) 2003-2015. The\
    \ Regents of the University of California (Regents)</p>\n<p>PyPop is distributed\
    \ under the terms of GPLv2</p>\n"
  stargazers_count: 11
  subscribers_count: 10
  topics:
  - population-genomics
  - evolutionary-biology
  - bioinformatics
  - open-source
  - free-software
  updated_at: 1615988636.0
andquintero/singularity_builds:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: andquintero/singularity_builds
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_builds" class="anchor" href="#singularity_builds"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_builds</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1554218133.0
andyrevell/docker_GitHub:
  data_format: 2
  description: null
  filenames:
  - imaging/nipy/Singularity
  full_name: andyrevell/docker_GitHub
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CNTdocker</h1>\n<h2>\n<a id=\"user-content-about\" class=\"anchor\"\
    \ href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>About</h2>\n<p>Dockerfiles to create Docker images\
    \ used by the CNT at the university of Pennsylvania</p>\n<h2>\n<a id=\"user-content-directory-contents-explanation\"\
    \ class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directory\
    \ contents explanation</h2>\n<h3>\n<a id=\"user-content-eeg\" class=\"anchor\"\
    \ href=\"#eeg\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>EEG</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common EEG analysis tools. Usually python 3</p>\n<p><strong>echobase</strong>:\
    \ Dockerfiles used to create images that can calculate functional connectivity\
    \ of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase\
    \ code is from <a href=\"https://github.com/andyrevell/paper001\">https://github.com/andyrevell/paper001</a></p>\n\
    <pre><code>Ubuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\n\
    scipy 1.4.1\n</code></pre>\n<h3>\n<a id=\"user-content-imaging\" class=\"anchor\"\
    \ href=\"#imaging\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>Imaging</strong>:</h3>\n<p>Dockerfiles used\
    \ to create images with common MRI analysis tools.</p>\n<pre><code>  Ubuntu 18.04\n\
    \  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n\
    \  FSL 6.0.1\n</code></pre>\n<h3>\n<a id=\"user-content-ml\" class=\"anchor\"\
    \ href=\"#ml\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>ml</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common machine learning tools.</p>\n<p><strong>wavenet</strong>:\
    \ Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet\
    \ paper\n<a href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\"\
    \ rel=\"nofollow\">Wavenet blog</a>\n<a href=\"https://arxiv.org/pdf/1609.03499.pdf\"\
    \ rel=\"nofollow\">Wavenet paper</a></p>\n<pre><code>  Ubuntu 18.04\n  tensorflow\
    \ 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n</code></pre>\n<p><strong>Tensorflow_2.1</strong>:\
    \ Dockerfile to create compatible dependencies to with tensorflow 2.1</p>\n<pre><code>\
    \  Ubuntu 18.04\n  tensorflow 2.1\n</code></pre>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1600370006.0
angelettilab/scMouseBcellFlu:
  data_format: 2
  description: null
  filenames:
  - envs/sauron/Singularity_remote.def
  full_name: angelettilab/scMouseBcellFlu
  latest_release: null
  readme: '<h1>

    <a id="user-content-scmousebcellflu" class="anchor" href="#scmousebcellflu" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>scMouseBcellFlu</h1>

    <p>This is the repository associated with the publication <em>Single cell BCR
    and RNA analysis after respiratory virus infection reveals spatiotemporal dynamics
    of antigen specific B cell response</em>.</p>

    <p>This repository contains the code and supporting files necessary to reproduce
    the analyses reported in the publication. In essence, the anlaysis workflow herein
    was done using the <a href="https://github.com/NBISweden/sauron">Sauron</a> analysis
    philosophy.</p>

    <h2>

    <a id="user-content-single-cell-rna-seq-analysis" class="anchor" href="#single-cell-rna-seq-analysis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Single-cell
    RNA-seq analysis</h2>

    <p>To re-run the analysis of the scRNA-Seq data, the necessary workflow and scripts
    can be found in the <a href="scripts/scRNAseq_pipeline"><code>scripts/scRNAseq_pipeline/</code></a>
    subdirectory.</p>

    <h2>

    <a id="user-content-bcr-seq-analysis" class="anchor" href="#bcr-seq-analysis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BCR
    seq analysis</h2>

    <p>Scripts necessary to reproduce the analysis of the B-cell receptor sequencing
    data can be found in the <a href="scripts/VDJ_analysis"><code>scripts/VDJ_analysis/</code></a>
    subdirectory.</p>

    '
  stargazers_count: 3
  subscribers_count: 1
  topics: []
  updated_at: 1623749451.0
arcsUVA/R:
  data_format: 2
  description: R containers
  filenames:
  - Singularity.3.6.0
  full_name: arcsUVA/R
  latest_release: null
  readme: '<h1>

    <a id="user-content-r" class="anchor" href="#r" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>R</h1>

    <p>R containers</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1573410996.0
arcsUVA/caffe2:
  data_format: 2
  description: null
  filenames:
  - Singularity.0.8.0
  full_name: arcsUVA/caffe2
  latest_release: null
  readme: '<h1>

    <a id="user-content-caffe2" class="anchor" href="#caffe2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>caffe2</h1>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1550983020.0
arcsUVA/cellprofiler:
  data_format: 2
  description: singularity scripts for cellprofiler
  filenames:
  - Singularity.2.2.0
  - Singularity.3.1.8
  - Singularity.3.0.0
  full_name: arcsUVA/cellprofiler
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2270" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Stacks software pipeline for building loci
    from short-read sequences</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1556734065.0
arcsUVA/cryoCARE:
  data_format: 2
  description: null
  filenames:
  - Singularity.0.1.0
  full_name: arcsUVA/cryoCARE
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-images" class="anchor" href="#singularity-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-images</h1>

    <p>Setups for various images used on the dgx.</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1574198978.0
arcsUVA/omero-client:
  data_format: 2
  description: Omero client Singularity recipes.
  filenames:
  - Singularity.5.4.10
  - Singularity.5.4.0
  full_name: arcsUVA/omero-client
  latest_release: null
  readme: '<h1>

    <a id="user-content-omero-client" class="anchor" href="#omero-client" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>omero-client</h1>

    <p><a href="https://singularity-hub.org/collections/2227" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a><br>

    Omero client Singularity recipes</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1557760203.0
arcsUVA/patric:
  data_format: 2
  description: null
  filenames:
  - Singularity.1.026
  full_name: arcsUVA/patric
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CNTdocker</h1>\n<h2>\n<a id=\"user-content-about\" class=\"anchor\"\
    \ href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>About</h2>\n<p>Dockerfiles to create Docker images\
    \ used by the CNT at the university of Pennsylvania</p>\n<h2>\n<a id=\"user-content-directory-contents-explanation\"\
    \ class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directory\
    \ contents explanation</h2>\n<h3>\n<a id=\"user-content-eeg\" class=\"anchor\"\
    \ href=\"#eeg\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>EEG</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common EEG analysis tools. Usually python 3</p>\n<p><strong>echobase</strong>:\
    \ Dockerfiles used to create images that can calculate functional connectivity\
    \ of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase\
    \ code is from <a href=\"https://github.com/andyrevell/paper001\">https://github.com/andyrevell/paper001</a></p>\n\
    <pre><code>Ubuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\n\
    scipy 1.4.1\n</code></pre>\n<h3>\n<a id=\"user-content-imaging\" class=\"anchor\"\
    \ href=\"#imaging\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>Imaging</strong>:</h3>\n<p>Dockerfiles used\
    \ to create images with common MRI analysis tools.</p>\n<pre><code>  Ubuntu 18.04\n\
    \  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n\
    \  FSL 6.0.1\n</code></pre>\n<h3>\n<a id=\"user-content-ml\" class=\"anchor\"\
    \ href=\"#ml\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>ml</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common machine learning tools.</p>\n<p><strong>wavenet</strong>:\
    \ Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet\
    \ paper\n<a href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\"\
    \ rel=\"nofollow\">Wavenet blog</a>\n<a href=\"https://arxiv.org/pdf/1609.03499.pdf\"\
    \ rel=\"nofollow\">Wavenet paper</a></p>\n<pre><code>  Ubuntu 18.04\n  tensorflow\
    \ 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n</code></pre>\n<p><strong>Tensorflow_2.1</strong>:\
    \ Dockerfile to create compatible dependencies to with tensorflow 2.1</p>\n<pre><code>\
    \  Ubuntu 18.04\n  tensorflow 2.1\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1570548606.0
arcsUVA/pytorch:
  data_format: 2
  description: null
  filenames:
  - Singularity.1.0.0-py36
  - Singularity.1.3.1-py36
  full_name: arcsUVA/pytorch
  latest_release: null
  readme: '<h1>

    <a id="user-content-pytorch" class="anchor" href="#pytorch" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pytorch</h1>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1573410610.0
arcsUVA/supernova:
  data_format: 2
  description: Singularity container script for 10x Genomics SuperNova software
  filenames:
  - Singularity.2.0.0
  full_name: arcsUVA/supernova
  latest_release: null
  readme: '<h1>

    <a id="user-content-supernova" class="anchor" href="#supernova" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>supernova</h1>

    <p>Singularity container script for 10x Genomics SuperNova software</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1551891095.0
arcsUVA/tensorflow:
  data_format: 2
  description: TensorFlow Singularity recipes.
  filenames:
  - Singularity.1.12.0-py27
  - Singularity.1.14.0-py36
  - Singularity.1.13.0-py36
  - Singularity.1.6.0-py36
  - Singularity.1.12.0-py36
  - Singularity.1.13.1-py36
  - Singularity.1.6.0-py27
  full_name: arcsUVA/tensorflow
  latest_release: null
  readme: '<h1>

    <a id="user-content-tensorflow" class="anchor" href="#tensorflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>tensorflow</h1>

    <p><a href="https://singularity-hub.org/collections/2235" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a><br>

    TensorFlow Singularity recipes.</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1567631554.0
arcsUVA/theano:
  data_format: 2
  description: Theano Singularity container scripts
  filenames:
  - Singularity.1.0.4-py36
  full_name: arcsUVA/theano
  latest_release: null
  readme: '<h1>

    <a id="user-content-theano" class="anchor" href="#theano" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>theano</h1>

    <p>Theano Singularity container scripts</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1554499739.0
arezaii/pf_singularity_demo:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: arezaii/pf_singularity_demo
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-parflow-singularity-container-demonstration\"\
    \ class=\"anchor\" href=\"#parflow-singularity-container-demonstration\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>ParFlow\
    \ Singularity Container Demonstration</h1>\n<p>The Singularity container is built\
    \ with ParFlow installed as a SCIF-app, providing access to both sequential and\
    \ parallel\nbuilds of ParFlow. See additional information about <a href=\"https://sylabs.io/guides/3.3/user-guide/definition_files.html?highlight=apps#apps\"\
    \ rel=\"nofollow\">Apps in Singularity</a></p>\n<h2>\n<a id=\"user-content-prerequisites\"\
    \ class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h2>\n<ul>\n<li>Host\
    \ OS must have Singularity installed (See <a href=\"https://sylabs.io/guides/3.3/user-guide/installation.html\"\
    \ rel=\"nofollow\">Installing Singularity</a>)</li>\n</ul>\n<h2>\n<a id=\"user-content-linux-hosts\"\
    \ class=\"anchor\" href=\"#linux-hosts\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linux Hosts</h2>\n<p>Verify Singularity\
    \ is installed with the command:</p>\n<pre><code>singularity --version\n</code></pre>\n\
    <p>Then, see the Quickstart directions below</p>\n<h2>\n<a id=\"user-content-windowsmac-hosts\"\
    \ class=\"anchor\" href=\"#windowsmac-hosts\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Windows/Mac Hosts</h2>\n<p>Follow\
    \ the instructions to <a href=\"https://sylabs.io/guides/3.3/user-guide/installation.html#install-on-windows-or-mac\"\
    \ rel=\"nofollow\">install Singularity</a></p>\n<p>Make sure you are ssh'd into\
    \ the Vagrant box before beginning the Quickstart steps below</p>\n<pre><code>vagrant\
    \ ssh\nvagrant@vagrant:~$ singularity --version\n</code></pre>\n<h2>\n<a id=\"\
    user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Quickstart</h2>\n\
    <p>Steps:</p>\n<ol>\n<li>Clone this repository</li>\n</ol>\n<pre><code>git clone\
    \ https://github.com/arezaii/pf_singularity_demo\n</code></pre>\n<ol start=\"\
    2\">\n<li>cd to the repository directory</li>\n</ol>\n<pre><code>cd pf_singularity_demo\n\
    </code></pre>\n<ol start=\"3\">\n<li>run the shell script to execute tests for\
    \ Little Washita domain on 1 processor, for 1 timestep</li>\n</ol>\n<pre><code>./run_test.sh\
    \ LW 1 1 1 1\n</code></pre>\n<h2>\n<a id=\"user-content-running-performance-test-cases\"\
    \ class=\"anchor\" href=\"#running-performance-test-cases\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Performance Test Cases</h2>\n<p>The shell script run_test.sh facilitates running\
    \ tests on different domains.</p>\n<p>Usage:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ ./run_test.sh <span class=\"pl-k\">&lt;</span>domain<span class=\"pl-k\"\
    >&gt;</span> <span class=\"pl-k\">&lt;</span>P<span class=\"pl-k\">&gt;</span>\
    \ <span class=\"pl-k\">&lt;</span>Q<span class=\"pl-k\">&gt;</span> <span class=\"\
    pl-k\">&lt;</span>R<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>TimeSteps<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<p>where</p>\n<ul>\n<li>domain is a\
    \ test domain defined below</li>\n<li>P, Q, R are integers defining processor\
    \ topology in X, Y, Z directions</li>\n<li>Timesteps is number of timesteps to\
    \ execute</li>\n</ul>\n<h2>\n<a id=\"user-content-test-domains\" class=\"anchor\"\
    \ href=\"#test-domains\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Test Domains</h2>\n<p>There are several test\
    \ domains for performance analysis contained in the perf_tests folder.</p>\n<ul>\n\
    <li>LW - Little Washita</li>\n<li>clayl - ClayL</li>\n<li>conus_ru - CONUS Clip\
    \ - Run off</li>\n<li>conus_tfg - CONUS Clip - Terrain Following Grid</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-little-washita\" class=\"anchor\" href=\"#little-washita\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Little Washita</h3>\n<p>Natural model of the Little Washita watershed\
    \ in Oklahoma.</p>\n<p><em><strong>Domain Details</strong></em></p>\n<ul>\n<li>Number\
    \ of Cells: 84,050, 41x41x50 (X,Y,Z)</li>\n<li>Horizontal Resolution: 1km</li>\n\
    <li>Vertical Resolution: 2m</li>\n</ul>\n<p><em><strong>Technical Details</strong></em></p>\n\
    <ul>\n<li>CLM enabled with NLDAS Forcings</li>\n<li>Timestep: 1hr</li>\n<li>Suburface:\
    \ Heterogeneous</li>\n<li>Initial Condition: Pressure file from spin-up</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-clayl\" class=\"anchor\" href=\"#clayl\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>ClayL</h3>\n\
    <p>Synthetic model with completely flat surface and many thin, vertical layers</p>\n\
    <p><em><strong>Domain Details</strong></em></p>\n<ul>\n<li>Number of Cells: 2.4M\
    \ for 1 core. Scales with processor count, 100Px100Qx240 (X,Y,Z)</li>\n<li>Horizontal\
    \ Resolution: 1m</li>\n<li>Vertical Resolution: 0.025m</li>\n</ul>\n<p><em><strong>Technical\
    \ Details</strong></em></p>\n<ul>\n<li>No CLM, constant simulated rain on top\
    \ surface @ .0008 mm/hr</li>\n<li>Timestep 1hr</li>\n<li>Subsurface: Homogeneous</li>\n\
    <li>Initial Condition: Dry</li>\n</ul>\n<h3>\n<a id=\"user-content-conus-run-off\"\
    \ class=\"anchor\" href=\"#conus-run-off\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CONUS Run-off</h3>\n<p>Natural\
    \ topography with an impervious surface (parking lot simulation)</p>\n<p><em><strong>Domain\
    \ Details</strong></em></p>\n<ul>\n<li>Number of Cells: 1,562,500 1250x1250x1\
    \ (X,Y,Z)</li>\n<li>Horizontal Resolution: 1km</li>\n<li>Vertical Resolution:\
    \ 0.10m</li>\n</ul>\n<p><em><strong>Technical Details</strong></em></p>\n<ul>\n\
    <li>No CLM, period of 1 hour simulated rain on top surface @ .005 mm/hr, then\
    \ recession for 1000 hours</li>\n<li>Timestep: 6 minutes</li>\n<li>Subsurface:\
    \ Homogeneous</li>\n<li>Initial Condition: Dry</li>\n</ul>\n<h3>\n<a id=\"user-content-conus-terrain-following-grid\"\
    \ class=\"anchor\" href=\"#conus-terrain-following-grid\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CONUS Terrain\
    \ Following Grid</h3>\n<p>Natural topography with the terrain following grid (TFG)\
    \ feature enabled</p>\n<p><em><strong>Domain Details</strong></em></p>\n<ul>\n\
    <li>Number of Cells: 1,125,000 750x750x2 (X,Y,Z)</li>\n<li>Horizontal Resolution:\
    \ 1km</li>\n<li>Vertical Resolution: toplayer=1m, bottomlayer=100m</li>\n</ul>\n\
    <p><em><strong>Technical Details</strong></em></p>\n<ul>\n<li>No CLM, seepage\
    \ face boundary condition type on top layer, @ 0.00001</li>\n<li>Timestep: 100000</li>\n\
    <li>Subsurface: Homogeneous</li>\n<li>Initial Condition: Water Table at 45m above\
    \ lower boundary</li>\n</ul>\n<h2>\n<a id=\"user-content-about-apps\" class=\"\
    anchor\" href=\"#about-apps\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>About Apps</h2>\n<p>The demo container\
    \ has two apps installed:</p>\n<ul>\n<li>par = distributed build of ParFlow, -DPARFLOW_AMPS_SEQUENTIAL_IO=False</li>\n\
    <li>seq = sequential build of ParFlow, -DPARFLOW_AMPS_SEQUENTIAL_IO=True</li>\n\
    </ul>\n<p>to run:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$\
    \ singularity run --app <span class=\"pl-k\">&lt;</span>app_name<span class=\"\
    pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>/path/to/singularity_container.sif<span\
    \ class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>.tcl input file<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<p>See additional information about\
    \ <a href=\"https://sylabs.io/guides/3.3/user-guide/definition_files.html?highlight=apps#apps\"\
    \ rel=\"nofollow\">Apps in Singularity</a></p>\n<h2>\n<a id=\"user-content-to-build-container\"\
    \ class=\"anchor\" href=\"#to-build-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To Build Container</h2>\n<p>The\
    \ quickest way to build is to use a remote build service such as <a href=\"https://cloud.sylabs.io/builder\"\
    \ rel=\"nofollow\">cloud.sylabs.io</a>\nIf a user has root access, they can build\
    \ from the definition file, conventionally named Singularity.</p>\n<p>General\
    \ build command is of the form:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ sudo singularity build <span class=\"pl-k\">&lt;</span>destination/path/to/singularity_container.sif<span\
    \ class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>Singularity definition\
    \ file<span class=\"pl-k\">&gt;</span></pre></div>\n<p>as a specific example:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ sudo singularity build\
    \ <span class=\"pl-k\">~</span>/pf_singularity_demo.sif Singularity</pre></div>\n\
    <h2>\n<a id=\"user-content-to-use-parflow-in-container\" class=\"anchor\" href=\"\
    #to-use-parflow-in-container\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>To Use ParFlow in Container</h2>\n\
    <p>Example of running the LW test case in <code>parflow/test/washita/tcl_scripts</code>\
    \ directory</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ singularity\
    \ run --app par <span class=\"pl-k\">~</span>/pf_singularity_demo.sif LW_Test.tcl</pre></div>\n\
    <h2>\n<a id=\"user-content-pull-from-sylabs-cloud\" class=\"anchor\" href=\"#pull-from-sylabs-cloud\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pull from Sylabs Cloud</h2>\n<p>To pull the pre-built image from Sylabs\
    \ Cloud:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ singularity\
    \ pull [destination image name] library://arezaii/default/parflow_demo:master</pre></div>\n\
    <h2>\n<a id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Testing</h2>\n\
    <p>Because singularity containers are write protected and ParFlow tests write\
    \ to disk, you must expand the image to a writable sandbox.\nThis requires super\
    \ user access, similar to building a container from the definition file.</p>\n\
    <h3>\n<a id=\"user-content-make-container-writable\" class=\"anchor\" href=\"\
    #make-container-writable\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Make Container Writable</h3>\n<p>First, create\
    \ a writable sandbox from the immutable container using Singularity's build command:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>sudo singularity build --sandbox\
    \ <span class=\"pl-k\">&lt;</span>directory_to_create_for_sandbox/<span class=\"\
    pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>singularity_container<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<p>as an example, if you had pulled\
    \ the parflow_ompi image from shub:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity build --sandbox parflow_demo_master_sandbox/ parflow_demo_master.sif</pre></div>\n\
    <p>There will now be a new directory parflow_demo_master_sandbox/ that is the\
    \ root of the container.\nEditing any of the folder contents will require super\
    \ user permissions.</p>\n<p>You can enter a console into the container now by\
    \ using the Singularity shell command:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity shell --writable <span class=\"pl-k\">&lt;</span>directory_to_create_for_sandbox/<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<h3>\n<a id=\"user-content-run-tests\"\
    \ class=\"anchor\" href=\"#run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run Tests</h3>\n<p>After making\
    \ the container writable and accessing it through a shell, both documented above,\
    \ running the ParFlow\ntests can be done by changing directories and exporting\
    \ the PARFLOW_DIR environment variable for either distributed\nor sequential builds\
    \ of ParFlow.</p>\n<p>Take note of the ParFlow build and install directories in\
    \ the container:</p>\n<p><strong>Sequential Build</strong></p>\n<ul>\n<li>build\
    \ directory: /home/parflow/build_seq</li>\n<li>install directory: /home/parflow/pfdir_seq</li>\n\
    </ul>\n<p><strong>Distributed Build</strong></p>\n<ul>\n<li>build directory: /home/parflow/build_par</li>\n\
    <li>install directory: /home/parflow/pfdir_par</li>\n</ul>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-k\">&gt;</span> <span class=\"\
    pl-c1\">cd</span> /home/parflow/<span class=\"pl-k\">&lt;</span>build_dir<span\
    \ class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;</span> <span class=\"\
    pl-k\">export</span> PARFLOW_DIR=/home/parflow/<span class=\"pl-k\">&lt;</span>install_dir<span\
    \ class=\"pl-k\">&gt;</span> \n<span class=\"pl-k\">&gt;</span> make <span class=\"\
    pl-c1\">test</span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583512107.0
arzwa/wgd:
  data_format: 2
  description: Python package and CLI for whole-genome duplication related analyses
  filenames:
  - Singularity
  full_name: arzwa/wgd
  latest_release: v1.1.1
  readme: "<p><a href=\"http://wgd.readthedocs.io/en/latest/?badge=latest\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/482fadaf050db308e22a1c37fb38a8d39be0f5a0d3b645ed48b114c86e55497e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7767642f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/wgd/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/2097\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9b0b8670bab3cab652cf5c31fdae614cf89b2ceb2e013cd2d7dd570e9f8530f2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d73696e67756c61726974792d2d6875622d626c75652e737667\"\
    \ alt=\"Hosted\" data-canonical-src=\"https://img.shields.io/badge/hosted-singularity--hub-blue.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Copyright (C) 2018 Arthur Zwaenepoel</p>\n\
    <p>VIB/UGent center for plant systems biology -\nBioinformatics &amp; evolutionary\
    \ genomics group <a href=\"https://www.vandepeerlab.org/\" rel=\"nofollow\">https://www.vandepeerlab.org/</a></p>\n\
    <h1>\n<a id=\"user-content-wgd---simple-command-line-tools-for-the-analysis-of-ancient-whole-genome-duplications\"\
    \ class=\"anchor\" href=\"#wgd---simple-command-line-tools-for-the-analysis-of-ancient-whole-genome-duplications\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>wgd - simple command line tools for the analysis of ancient whole-genome\
    \ duplications</h1>\n<p><strong>Note:</strong> If you are interested in the methods\
    \ implemented in <code>wgd</code>, you may also want to\nconsider the <a href=\"\
    https://github.com/VIB-PSB/ksrates\"><code>ksrates</code></a> tool by Sensalari\
    \ <em>et al.</em>\nwhich can be used to carefully compare multiple Ks distributions\
    \ and model them (<code>ksrates</code>\nuses <code>wgd</code> under the hood).</p>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>Python package and command line interface (CLI)\
    \ for the analysis of\nwhole-genome duplications (WGDs). Tested with Python3 on\
    \ Linux. If you don't have\npython or pip installed a simple <code>sudo apt-get\
    \ install python3-pip</code> should do.</p>\n<p>To install, simply run</p>\n<pre><code>git\
    \ clone https://github.com/arzwa/wgd.git\ncd wgd\npip install --user .\n</code></pre>\n\
    <p>Note that depending on your python installation and whether you're in a\nvirtualenv,\
    \ <code>pip</code> may default either to <code>pip2</code> or <code>pip3</code>.\
    \ If the\nabove installation step fails, please try to use <code>pip3</code> instead\
    \ of\n<code>pip</code>.</p>\n<p>For the command line interface, upon installation\
    \ run</p>\n<pre><code>$ wgd\n</code></pre>\n<p>to get a list of the available\
    \ commands. To get usage instructions for\na command (e.g. <code>ksd</code>) run</p>\n\
    <pre><code>$ wgd ksd --help\n</code></pre>\n<p>For <strong>external software</strong>\
    \ requirements: please consult the relevant section\nin the <a href=\"https://wgd.readthedocs.io/en/latest/index.html#external-software\"\
    \ rel=\"nofollow\">docs</a></p>\n<p><strong>Note:</strong> if you encounter issues,\
    \ do verify you have the latest\n<a href=\"http://abacus.gene.ucl.ac.uk/software/#phylogenetic-analysis-by-maximum-likelihood-paml\"\
    \ rel=\"nofollow\">PAML</a> version.\nTo install the latest version, you best\
    \ not rely on <code>apt-get</code> or any other\npackage manager but install from\
    \ source. Something like this should work\n(from within the directory where you\
    \ want to install paml)</p>\n<pre><code>wget http://abacus.gene.ucl.ac.uk/software/paml4.9j.tgz\n\
    tar -xzf paml4.9j.tgz\npushd paml4.9j/src &amp;&amp; make -f Makefile &amp;&amp;\
    \ popd \nexport PATH=$PATH:$PWD/paml4.9j/src/\n</code></pre>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick start</h2>\n<p>The main\
    \ aim of <code>wgd</code> is computing whole-paranome and one-vs.-one ortholog\
    \ Ks\ndistributions. For a whole-paranome distribution of a CDS sequence fasta\
    \ file,\nthe minimal commands are:</p>\n<pre><code>$ wgd dmd ath.cds.fasta\n$\
    \ wgd ksd wgd_dmd/ath.cds.fasta.mcl ath.cds.fasta\n</code></pre>\n<p>For one-vs.one\
    \ orthologs the minimal commands are</p>\n<pre><code>$ wgd dmd ath.cds.fasta vvi.cds.fasta\n\
    $ wgd ksd wgd_dmd/ath1000.fasta_vvi1000.fasta.rbh ath.cds.fasta vvi.cds.fasta\n\
    </code></pre>\n<p>For more information and these methods and other tools implemented\
    \ in <code>wgd</code>,\nplease consult the <a href=\"https://wgd.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">docs</a>.</p>\n<h2>\n<a id=\"user-content-singularity-container\"\
    \ class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Singularity container</h2>\n\
    <p><strong>Note</strong> this hasn't been updated in a while, it may or may not\
    \ work.</p>\n<p>A singularity container is available for <code>wgd</code>, allowing\
    \ all to use\nall tools in <code>wgd</code> except <code>wgd syn</code>, without\
    \ having to install all\nrequired software on your system. To install Singularity\
    \ follow\nthe instructions <a href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\
    >here</a></p>\n<p>If you have singulaity installed (and you're in the virtual\
    \ machine when\nrunning on Windows or Mac), you can run the following to get the\
    \ container</p>\n<pre><code>singularity pull --name wgd.simg shub://arzwa/wgd\n\
    </code></pre>\n<p>Then you can use <code>wgd</code> as follows</p>\n<pre><code>singularity\
    \ exec wgd.simg wgd &lt;command&gt;\n</code></pre>\n<h2>\n<a id=\"user-content-notes\"\
    \ class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Notes</h2>\n<p><strong>Bug tracking:</strong>\
    \ If the program crashes, exits unexpectedly or some\nunexpected results are obtained,\
    \ please run it again with the\n<code>--verbosity debug</code> flag <em>before</em>\
    \ the subcommand of interest (<em>e.g.</em>\n<code>wgd --verbosity debug ksd gf.mcl\
    \ cds.fasta</code>). If the anomaly persists,\nplease open an issue on this GitHub\
    \ site.</p>\n<p><strong>Note on input data:</strong> while the input data is rather\
    \ straightforward\n(a CDS fasta file will do for most analyses) it may be of interest\
    \ that\nthe wgd suite was extensively tested with data from the PLAZA platform,\n\
    so for examples of the right input data formats (in particular CDS fasta\nfiles\
    \ for sequence data and GFF files for structural annotation), please\nhave a look\
    \ <a href=\"https://bioinformatics.psb.ugent.be/plaza/versions/plaza_v4_dicots/download/\"\
    \ rel=\"nofollow\">there</a>.\nIt is generally advised not to include pipe characters\
    \ (<code>|</code>) in your gene\nIDs, since these can have special meanings in\
    \ certain parts of <code>wgd</code>.</p>\n<p><strong>Note on virtualenv:</strong>\
    \ you can install wgd in a <em>virtual environment</em>\n(using <a href=\"https://virtualenv.pypa.io/en/stable/\"\
    \ rel=\"nofollow\"><code>virtualenv</code></a>). If you\nwould however encounter\
    \ problems with running the executable directly\n(e.g. <code>wgd --help</code>\
    \ doesn't work) you can circumvent this by directly\ncalling the CLI, using <code>python3\
    \ ./wgd_cli.py --help</code> (assuming you are\ncurrently in the directory where\
    \ you cloned wgd).</p>\n<h2>\n<a id=\"user-content-citation\" class=\"anchor\"\
    \ href=\"#citation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Citation</h2>\n<p>Please cite us at <a href=\"\
    https://doi.org/10.1093/bioinformatics/bty915\" rel=\"nofollow\">https://doi.org/10.1093/bioinformatics/bty915</a></p>\n\
    <pre><code>Zwaenepoel, A., and Van de Peer, Y. \nwgd - simple command line tools\
    \ for the analysis of ancient whole genome duplications. \nBioinformatics., bty915,\
    \ https://doi.org/10.1093/bioinformatics/bty915\n</code></pre>\n<p>For citation\
    \ of the tools used in wgd, please consult the documentation at\n<a href=\"https://wgd.readthedocs.io/en/latest/index.html#citation\"\
    \ rel=\"nofollow\">https://wgd.readthedocs.io/en/latest/index.html#citation</a>.</p>\n"
  stargazers_count: 56
  subscribers_count: 4
  topics:
  - wgd
  - duplication
  - polyploidy
  - bioinformatics
  - genomics
  - evolution
  updated_at: 1625051985.0
asafpr/singularity:
  data_format: 2
  description: Singularity description files
  filenames:
  - mousegwas/Singularity
  - fusorsv/Singularity
  full_name: asafpr/singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616613441.0
aseetharam/gatk:
  data_format: 2
  description: container for gatk tools
  filenames:
  - Singularity
  full_name: aseetharam/gatk
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4700" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-container-for-the-gatk" class="anchor" href="#container-for-the-gatk"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    for the GATK</h1>

    <h2>

    <a id="user-content-tools-included" class="anchor" href="#tools-included" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tools included</h2>

    <ol>

    <li><a href="http://www.htslib.org/" rel="nofollow">SamTools</a></li>

    <li><a href="http://bio-bwa.sourceforge.net/" rel="nofollow">BWA</a></li>

    <li><a href="https://www.gnu.org/software/datamash/" rel="nofollow">Datamash</a></li>

    <li><a href="https://gatk.broadinstitute.org/hc/en-us" rel="nofollow">GATK</a></li>

    <li><a href="https://broadinstitute.github.io/picard/" rel="nofollow">Picard Tools</a></li>

    <li><a href="https://github.com/lh3/bioawk">BioAWK</a></li>

    <li><a href="https://bedtools.readthedocs.io" rel="nofollow">BedTools</a></li>

    </ol>

    <p>Please be sure to cite all the programs if you use this container.</p>

    <h2>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h2>

    <p>to pull the image</p>

    <pre><code>singularity pull --name gatk.sif shub://aseetharam/gatk:latest

    </code></pre>

    <p>to use the image</p>

    <pre><code>singularity exec gatk.sif samtools

    singularity exec gatk.sif bwa

    singularity exec gatk.sif datamash

    singularity exec gatk.sif java -jar /gatk/gatk-package-4.1.8.1-local.jar

    singularity exec gatk.sif java -jar /picard/picard.jar

    singularity exec gatk.sif bioawk

    singularity exec gatk.sif bedtools

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1623344768.0
bananaeat/Cinnamon_assembly:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity
  full_name: bananaeat/Cinnamon_assembly
  latest_release: null
  readme: '<h1>

    <a id="user-content-cinnamon" class="anchor" href="#cinnamon" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cinnamon</h1>

    <p>This directory contains the code for the Cinnamon language compiler.  This
    compiler is described in the paper:</p>

    <p>Cinnamon: A Domain-Specific Language for Binary Profiling and Monitoring,

    Mahwish Arif, Ruoyu Zhou, Hsi-Ming Ho and Timothy M. Jones,

    CGO 2021</p>

    <p>Please cite this paper if you produce any work that builds upon this code and
    / or data.</p>

    <h2>

    <a id="user-content-licence" class="anchor" href="#licence" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Licence</h2>

    <p>Cinnamon is released under an Apache licence.</p>

    <h2>

    <a id="user-content-building-cinnamon" class="anchor" href="#building-cinnamon"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Cinnamon</h2>

    <p>Cinnamon can currently target three different binary frameworks; Janus, Pin
    and Dyninst.  To build the compiler:</p>

    <pre lang="shell-session"><code>export CINNAMON_ROOT = /path/to/cinnamon-source

    cd $(CINNAMON_ROOT)

    </code></pre>

    <p>To build the Cinnamon backend for Janus:</p>

    <pre lang="shell-session"><code>make TARGET=janus

    </code></pre>

    <p>To build the Cinnamon backend for Pin:</p>

    <pre lang="shell-session"><code>make TARGET=pin

    </code></pre>

    <p>To build the Cinnamon backend for Dyninst:</p>

    <pre lang="shell-session"><code>make TARGET=dyninst

    </code></pre>

    <h2>

    <a id="user-content-compiling-a-sample-program" class="anchor" href="#compiling-a-sample-program"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Compiling
    a sample program</h2>

    <p>Cinnamon sample programs are available in the  <code>tests</code> directory.  The
    following commands will compile the Cinnamon program <code>ins.dsl</code> and
    integrate the resulting code into one of the target frameworks. You will need
    to set the path to your target framework installation in the respective scripts:</p>

    <pre lang="shell-session"><code>$(CINNAMON_ROOT)/Scripts/compileToJanus.py $CINNAMON_ROOT/tests/ins.dsl

    $(CINNAMON_ROOT)/Scripts/compileToPin.py $CINNAMON_ROOT/tests/ins.dsl

    $(CINNAMON_ROOT)/Scripts/compileToDyn.py $CINNAMON_ROOT/tests/ins.dsl

    </code></pre>

    <p>After this, the final tool can be built and run using the target framework''s
    build instructions.</p>

    <p>If you just want to compile the Cinnamon DSL code and not yet integrate it
    into a target framework, run the following command.  This will generate a number
    of different files containing relevant code for the cinnamon program:</p>

    <pre lang="shell-session"><code>cd $CINNAMON_ROOT

    ./bdc $CINNAMON_ROOT/tests/ins.dsl

    </code></pre>

    <h2>

    <a id="user-content-target-frameworks" class="anchor" href="#target-frameworks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Target
    frameworks</h2>

    <h3>

    <a id="user-content-janus" class="anchor" href="#janus" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Janus</h3>

    <p>You can get the Janus implementation with placeholders, templates and utility
    libraries for Cinnamon from the main Janus repository at <a href="https://github.com/timothymjones/Janus.git">https://github.com/timothymjones/Janus.git</a>,
    then switch to the <code>cinnamon</code> branch.</p>

    <pre lang="shell-session"><code>git clone https://github.com/timothymjones/Janus.git

    cd Janus

    git checkout -b cinnamon origin/cinnamon

    </code></pre>

    <p>Next set <code>JanusPATH</code> in <code>compileToJanus.py</code> to be the
    location that you have cloned Janus.</p>

    <p>Once the code for Janus has been generated and integrated (after running the
    <code>compileToJanus.py</code> script from above), you can build the final tool
    using the following commands:</p>

    <pre lang="shell-session"><code>(cd build; cmake ..; make -j8)

    </code></pre>

    <p>To run the final tool on the target binary:</p>

    <pre lang="shell-session"><code>./janus/jdsl_run &lt;target_binary&gt;

    </code></pre>

    <h3>

    <a id="user-content-pin" class="anchor" href="#pin" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Pin</h3>

    <p>Everything required for Pin is contained within the <code>targets/Pin</code>
    directory.  Copy the <code>MyDSLTool</code> directory to <code>path-to-your-pin-root-dir/source/tools</code>,
    where <code>path-to-your-pin-root</code> should be self-explanatory.</p>

    <p>Next set <code>PinPATH=your-pin-root-dir/source/tools/MyDSLTool</code> in <code>compileToPin.py</code>.</p>

    <p>Once the code for Pin has been generated and integrated (after running the
    <code>compileToPin.py</code> script from above), you can build the final tool
    using the following commands:</p>

    <pre lang="shell-session"><code>cd your-pin-root-dir/source/tools/MyDSLTool

    make obj-intel64/MyDSLTool.so

    </code></pre>

    <p>To run the final tool on the target binary:</p>

    <pre lang="shell-session"><code>your-pin-root-dir/pin -t obj-intel64/MyDSLTool.so
    -- &lt;target_binary&gt;

    </code></pre>

    <h3>

    <a id="user-content-dyninst" class="anchor" href="#dyninst" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dyninst</h3>

    <p>You can obtain Dyninst version 10.1.0 as follows:</p>

    <pre lang="shell-session"><code>wget https://github.com/dyninst/dyninst/archive/v10.1.0.tar.gz``

    tar xzvf v10.1.0.tar.gz

    </code></pre>

    <p>Once extracted, add <code>c_LoadInsn</code> and <code>c_StoreInsn</code> into
    <code>enum InsnCategory</code> in <code>dyninst-10.1.0/instructionAPI/h/InstructionCategories.h</code>
    and then build by following the Dyninst build instructions.</p>

    <p>Everything else required for Dyninst is contained within the <code>targets/Dyninst</code>
    directory.  Copy the <code>MyDSLTool</code> directory to <code>path-to-your-dyn-root-dir/examples</code>,
    where <code>path-to-your-dyn-root-dir</code> should be self-explanatory.</p>

    <p>Next set <code>DynPATH=path-to-your-dyn-root-dir/examples/MyDSLTool</code>
    in <code>compileToDyn.py</code>.</p>

    <p>Once the code for Dyninst has been generated and integrated (after running
    the <code>compileToDyn.py</code> script from above), you can build the final tool
    using the following commands:</p>

    <pre lang="shell-session"><code>cd path-to-your-dyn-root-dir/examples/MyDSLTool

    make

    </code></pre>

    <p>To run the final tool on the target binary:</p>

    <pre lang="shell-session"><code>path-to-your-dyn-root-dir/examples/MyDSLTool/DSLtool
    -m static -o &lt;output_binary&gt; &lt;input_binary&gt;

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624454250.0
barbagroup/geoclaw-landspill:
  data_format: 2
  description: An oil land-spill and overland flow simulator for pipeline rupture
    events
  filenames:
  - Singularityfiles/Singularity.v0.1.bionic
  - Singularityfiles/Singularity.v1.0.dev3
  - Singularityfiles/Singularity.v1.0.dev2
  - Singularityfiles/Singularity.v1.0.dev1
  - Singularityfiles/Singularity.v1.0.dev4
  - Singularityfiles/Singularity.v1.0
  - Singularityfiles/Singularity.v0.1.trusty
  full_name: barbagroup/geoclaw-landspill
  latest_release: v1.0
  readme: "<h1>\n<a id=\"user-content-geoclaw-landspill\" class=\"anchor\" href=\"\
    #geoclaw-landspill\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>geoclaw-landspill</h1>\n<p><a href=\"https://github.com/barbagroup/geoclaw-landspill/raw/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/8ccf186e7288af6d88a1f6a930c0fcc4e7a8a9936b34e07629d815d1eab4d977/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.com/barbagroup/geoclaw-landspill\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7f22540fd3a0f3b9a8d8a57ead3744961fd8b2d9edd257d02e4a8e0ae1d6d7a6/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f636f6d2f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6d61737465723f6c6162656c3d5472617669732532304349\"\
    \ alt=\"Travis CI\" data-canonical-src=\"https://img.shields.io/travis/com/barbagroup/geoclaw-landspill/master?label=Travis%20CI\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/barbagroup/geoclaw-landspill/actions?query=workflow%3ACI\"\
    ><img src=\"https://camo.githubusercontent.com/91894ad74ed7c9cc23b3f2fb08cc1ea432c8ed6830abd92e489e7f59ac619ab3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f43492f6d61737465723f6c6162656c3d476974487562253230416374696f6e2532304349\"\
    \ alt=\"GitHub Action CI\" data-canonical-src=\"https://img.shields.io/github/workflow/status/barbagroup/geoclaw-landspill/CI/master?label=GitHub%20Action%20CI\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/036ff156f41dafdb919e29a22cd5aa00a7f0ded742b75831240ea25fe720350e/68747470733a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f66623762303132373939613730633962346335356562346262306633366639372f7374617475732e737667\"\
    \ alt=\"status\" data-canonical-src=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97/status.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/barbagroup/geoclaw-landspill\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9e96d790dd4b2cdbdea7b49eff75628a95ae5cbd3c8f5b7bc902b7f9b603b149/68747470733a2f2f616e61636f6e64612e6f72672f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6261646765732f696e7374616c6c65722f636f6e64612e737667\"\
    \ alt=\"Conda\" data-canonical-src=\"https://anaconda.org/barbagroup/geoclaw-landspill/badges/installer/conda.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><em><strong>Note: if looking for content\
    \ of <code>geoclaw-landspill-cases</code>, please checkout tag\n<code>v0.1</code>.\
    \ This repository has been converted to a fully working solver package.</strong></em></p>\n\
    <p><em>geoclaw-landspill</em> is a package for running oil overland flow simulations\
    \ for\napplications in pipeline risk management. It includes a numerical solver\
    \ and\nsome pre-/post-processing utilities.</p>\n<p><a href=\"./doc/sample.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"./doc/sample.gif\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>The numerical solver is a modified version\
    \ of\n<a href=\"http://www.clawpack.org/geoclaw.html\" rel=\"nofollow\">GeoClaw</a>.\n\
    GeoClaw solves full shallow-water equations. We added several new features and\n\
    utilities to it and make it usable to simulate the overland flow from pipeline\n\
    ruptures. These features include:</p>\n<ul>\n<li>adding point sources to mimic\
    \ the rupture points</li>\n<li>adding evaporation models</li>\n<li>adding Darcy-Weisbach\
    \ bottom friction models with land roughness</li>\n<li>adding temperature-dependent\
    \ viscosity</li>\n<li>recording detail locations and time of oil flowing into\
    \ in-land waterbodies</li>\n<li>downloading topography and hydrology data automatically\
    \ (the US only)</li>\n<li>generating CF-1.7 compliant NetCDF files</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Documentation</h2>\n<ol>\n<li><a href=\"doc/deps_install_tests.md\"\
    >Dependencies, installation, and tests</a></li>\n<li><a href=\"doc/usage.md\"\
    >Usage</a></li>\n<li><a href=\"doc/configuration.md\">Configuration file: <code>setrun.py</code></a></li>\n\
    <li><a href=\"cases/README.md\">Example cases</a></li>\n<li><a href=\"doc/container.md\"\
    >Containers: Docker and Singularity</a></li>\n</ol>\n<hr>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick start</h2>\n<p>We only\
    \ maintain compatibility with Linux. Though using <code>pip</code> or building\
    \ from\nsource may still work in Mac OS or Windows (e.g., through WSL), we are\
    \ not able\nto help with the installation issues on these two systems.</p>\n<p>Beyond\
    \ this quick start, to see more details, please refer to the\n<a href=\"#documentation\"\
    >documentation</a> section.</p>\n<h3>\n<a id=\"user-content-1-installation\" class=\"\
    anchor\" href=\"#1-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>1. Installation</h3>\n<p>The fast\
    \ way to install <em>geoclaw-landspill</em> is through\n<a href=\"https://www.anaconda.com/\"\
    \ rel=\"nofollow\">Anaconda</a>'s <code>conda</code> command. The following command\n\
    creates a conda environment (called <code>landspill</code>) and installs the package\
    \ and\ndependencies:</p>\n<pre><code>$ conda create \\\n    -n landspill -c barbagroup\
    \ -c conda-forge \\\n    python=3.8 geoclaw-landspill\n</code></pre>\n<p>Then\
    \ use <code>conda activate landspill</code> or\n<code>source &lt;conda installation\
    \ prefix&gt;/bin/activate landspill</code> to activate the\nenvironment. Type\
    \ <code>geoclaw-landspill --help</code> in the terminal to see if\n<em>geoclaw-landspill</em>\
    \ is correctly installed.</p>\n<h3>\n<a id=\"user-content-2-running-an-example-case\"\
    \ class=\"anchor\" href=\"#2-running-an-example-case\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Running an\
    \ example case</h3>\n<p>To run an example case under the folder <code>cases</code>,\
    \ users have to clone this\nrepository. We currently don't maintain another repository\
    \ for cases. After\ncloning this repository, run</p>\n<pre><code>$ geoclaw-landspill\
    \ run &lt;path to an example case folder&gt;\n</code></pre>\n<p>For example, to\
    \ run <code>utal-flat-maya</code>:</p>\n<pre><code>$ geoclaw-landspill run ./cases/utah-flat-maya\n\
    </code></pre>\n<p>Users can use environment variable <code>OMP_NUM_THREADS</code>\
    \ to control how many CPU\nthreads the simulation should use for OpenMP parallelization.</p>\n\
    <h3>\n<a id=\"user-content-3-creating-a-cf-compliant-netcdf-raster-file\" class=\"\
    anchor\" href=\"#3-creating-a-cf-compliant-netcdf-raster-file\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3.\
    \ Creating a CF-compliant NetCDF raster file</h3>\n<p>After a simulation is done,\
    \ users can convert flow depth in raw simulation data\ninto a CF-compliant NetCDF\
    \ raster file. For example,</p>\n<pre><code>$ geoclaw-landspill createnc ./case/utah-flat-maya\n\
    </code></pre>\n<p>Replace <code>./cases/utah-flat-maya</code> with the path to\
    \ another desired case.</p>\n<p>QGIS and ArcGIS should be able to read the resulting\
    \ NetCDF raster file.</p>\n<hr>\n<h2>\n<a id=\"user-content-third-party-codes-and-licenses\"\
    \ class=\"anchor\" href=\"#third-party-codes-and-licenses\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Third-party\
    \ codes and licenses</h2>\n<ul>\n<li>amrclaw: <a href=\"https://github.com/clawpack/amrclaw\"\
    >https://github.com/clawpack/amrclaw</a>\n(<a href=\"https://github.com/clawpack/amrclaw/blob/ee85c1fe178ec319a8403503e779d3f8faf22840/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>geoclaw: <a href=\"https://github.com/clawpack/geoclaw\"\
    >https://github.com/clawpack/geoclaw</a>\n(<a href=\"https://github.com/clawpack/geoclaw/blob/3593cb1b418fd52739c186a8845a288037c8f575/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>pyclaw: <a href=\"https://github.com/clawpack/pyclaw\"\
    >https://github.com/clawpack/pyclaw</a>\n(<a href=\"https://github.com/clawpack/pyclaw/blob/a85a01a5f20be1a18dde70b7bb37dc1cdcbd0b26/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>clawutil: <a href=\"https://github.com/clawpack/clawutil\"\
    >https://github.com/clawpack/clawutil</a>\n(<a href=\"https://github.com/clawpack/clawutil/blob/116ffb792e889fbf0854d7ac599657039d7b1f3e/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>riemann: <a href=\"https://github.com/clawpack/riemann\"\
    >https://github.com/clawpack/riemann</a>\n(<a href=\"https://github.com/clawpack/riemann/blob/597824c051d56fa0c8818e00d740867283329b24/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n</ul>\n<hr>\n<h2>\n<a id=\"user-content-contributing\"\
    \ class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n<p>See <a\
    \ href=\"CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-contact\"\
    \ class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contact</h2>\n<p>Pi-Yueh Chuang:\
    \ <a href=\"mailto:pychuang@gwu.edu\">pychuang@gwu.edu</a></p>\n"
  stargazers_count: 4
  subscribers_count: 5
  topics:
  - geoclaw
  - overland-flow
  - pipeline
  - shallow-water-equations
  - pipeline-ruptures
  - land-spill
  updated_at: 1623343615.0
baxpr/bedpost-singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: baxpr/bedpost-singularity
  latest_release: v2.0.0-beta
  readme: '<p>Runs FSL''s bedpostx on the input DWI data set, and creates a PDF report
    of the results.

    Quite simple - see /opt/src/pipeline.sh for the main script.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624550915.0
baxpr/cersuit:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.1.0
  full_name: baxpr/cersuit
  latest_release: v2.1.0
  readme: "<h1>\n<a id=\"user-content-cersuit\" class=\"anchor\" href=\"#cersuit\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>cersuit</h1>\n<p>Cerebellar segmentation with the <a href=\"http://diedrichsenlab.org/imaging/suit.htm\"\
    \ rel=\"nofollow\">SUIT atlas and toolbox</a>. In the container, the pipeline\
    \ is installed in the <code>/opt/cersuit</code> directory. Matlab code is in the\
    \ <code>src</code> directory, and the entrypoint is <code>src/cersuit.m</code>.\
    \ Compiled Matlab code for use in the singularity container without a Matlab license\
    \ is in <code>bin</code>.</p>\n<p>See the <code>external</code> directory for\
    \ links, references, and license information for the underlying SPM12 and SUIT\
    \ Matlab software. <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki\" rel=\"nofollow\"\
    >FSL version 6.0.2</a> is also used for image file manipulation and creating the\
    \ QA PDF.</p>\n<p>The container has a full installation of both SPM12 (compiled)\
    \ and FSL.</p>\n<h2>\n<a id=\"user-content-references-for-suit\" class=\"anchor\"\
    \ href=\"#references-for-suit\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>References for SUIT</h2>\n<ul>\n<li>\n\
    <p><a href=\"https://doi.org/10.1016/j.neuroimage.2006.05.056\" rel=\"nofollow\"\
    >Diedrichsen, J. (2006). A spatially unbiased atlas template of the human cerebellum.\
    \ Neuroimage, 33, 1, p. 127-138.</a></p>\n</li>\n<li>\n<p><a href=\"https://doi.org/10.1016/j.neuroimage.2009.01.045\"\
    \ rel=\"nofollow\">Diedrichsen, J., Balsters, J. H., Flavell, J., Cussans, E.,\
    \ &amp; Ramnani, N. (2009). A probabilistic atlas of the human cerebellum. Neuroimage\
    \ 46(1):39-46.</a></p>\n</li>\n<li>\n<p><a href=\"https://doi.org/10.1016/j.neuroimage.2010.10.035\"\
    \ rel=\"nofollow\">Diedrichsen, J., Maderwald, S., Kuper, M., Thurling, M., Rabe,\
    \ K., Gizewski, E. R., et al. (2011). Imaging the deep cerebellar nuclei: A probabilistic\
    \ atlas and normalization procedure. Neuroimage 54(3):1786-94</a></p>\n</li>\n\
    <li>\n<p><a href=\"https://doi.org/10.1371/journal.pone.0133402\" rel=\"nofollow\"\
    >Diedrichsen, J. &amp; Zotow, E. (2015). Surface-based display of volume-averaged\
    \ cerebellar data. PLoS One, 7, e0133402.</a></p>\n</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Pipeline</h2>\n\
    <ul>\n<li>\n<p>Adjustment of the source T1 file to axial data ordering using fslreorient2std,\
    \ to meet a requirement of the SUIT toolbox.</p>\n</li>\n<li>\n<p>Translation-only\
    \ alignment of the supplied gray matter image to SPM12's gray matter probabilistic\
    \ atlas (TPM.nii). This is accomplished by aligning the centers of mass. Rotations\
    \ are not estimated, to avoid an issue with SUIT's bounding box computation. The\
    \ supplied gray matter image must be in register with the supplied T1. The estimated\
    \ registration is saved to file and also applied to the T1.</p>\n</li>\n<li>\n\
    <p>SUIT estimation of the affine transformation and warp of the cerebellar area\
    \ of the T1 to the SUIT atlas.</p>\n</li>\n<li>\n<p>Resampling of the T1 and related\
    \ images to the SUIT atlas space. Gray matter and white matter images are resampled\
    \ both with and without modulation by the Jacobian.</p>\n</li>\n<li>\n<p>Resampling\
    \ of the SUIT-supplied atlases to the original T1 native space.</p>\n</li>\n<li>\n\
    <p>Computation of regional volumes for the Lobules_SUIT atlas in the native T1\
    \ space.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-usage-of-the-singularity-container\"\
    \ class=\"anchor\" href=\"#usage-of-the-singularity-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage\
    \ of the singularity container</h2>\n<p>See <code>singularity_examples.sh</code>\
    \ for examples of using the container for SUIT warp estimation, and transformation\
    \ from native to SUIT space and back using an existing estimated warp. The transformations\
    \ can also be done directly from matlab with the <code>transform_???.m</code>\
    \ functions in <code>src</code>).</p>\n<h2>\n<a id=\"user-content-parameters-and-inputs\"\
    \ class=\"anchor\" href=\"#parameters-and-inputs\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parameters and\
    \ inputs</h2>\n<pre><code>&lt;temporary-home-dir&gt;      Matlab will use this\
    \ for temp files\n&lt;tmp-dir&gt;                 Other location for temp files\
    \          \n&lt;input-dir&gt;               Directory containing the input T1\
    \ image file\n&lt;output-dir&gt;              Outputs will be stored here\n&lt;t1-niigz-filename&gt;\
    \       Filename of the input T1 - expecting &lt;something&gt;.nii.gz\n&lt;mask-threshold&gt;\
    \          SPM mask threshold for separating brain from background\n&lt;project-name&gt;\
    \            Project/subject/session/scan names from XNAT, if XNAT is\n&lt;subject-name&gt;\
    \               used. These are only used to decorate the PDF report.\n&lt;session-name&gt;\
    \    \n&lt;scan-name&gt;\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<p>PDF report for\
    \ quality assurance</p>\n<pre><code>PDF               cersuit.pdf\n</code></pre>\n\
    <p>Transformation from native to atlas space. Apply in this order</p>\n<pre><code>RIGID\
    \             coreg_t1_to_mni.mat\nAFFINE            Affine_c_t1_seg1.mat\nFLOWFIELD\
    \         u_a_c_t1_seg1.nii.gz\n</code></pre>\n<p>Cropped T1 in both spaces</p>\n\
    <pre><code>T1_CROP_NATIVE    c_t1.nii.gz\nT1_CROP_SUIT      wc_t1.nii.gz\n</code></pre>\n\
    <p>Cerebellum mask, segmented gray matter and white matter volume fraction images\
    \ in native and atlas space</p>\n<pre><code>MASK_NATIVE       c_t1_pcereb.nii.gz\n\
    GRAY_NATIVE       c_t1_seg1.nii.gz\nWHITE_NATIVE      c_t1_seg2.nii.gz\nMASK_SUIT\
    \         wc_t1_pcereb.nii.gz\nGRAY_SUIT         wc_t1_seg1.nii.gz\nWHITE_SUIT\
    \        wc_t1_seg2.nii.gz\n</code></pre>\n<p>Jacobian-modulated gray and white\
    \ matter images in atlas space</p>\n<pre><code>GRAYMOD_SUIT      wdc_t1_seg1.nii.gz\n\
    WHITEMOD_SUIT     wdc_t1_seg2.nii.gz\n</code></pre>\n<p>Segmented regions in native\
    \ and atlas space, with lookup table</p>\n<pre><code>ATLASES_NATIVE    SUIT-supplied\
    \ atlases resampled to original T1 space\nATLASES_SUIT      The SUIT-supplied\
    \ atlases themselves\n</code></pre>\n<p>Volumetry of segmented regions, computed\
    \ from native space images. The \"Total\" is the volume of the atlas region after\
    \ transformation to native space. The \"Gray\" is the sum of voxel gray matter\
    \ fraction within the atlas region, in native space; similar for \"White\".</p>\n\
    <pre><code>NATIVE_VOLS       iw_Lobules-SUIT_u_a_c_t1_seg1-volumes.csv\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1601771070.0
baxpr/connprep:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.2.0
  full_name: baxpr/connprep
  latest_release: v2.2.0
  readme: "<h1>\n<a id=\"user-content-connprep\" class=\"anchor\" href=\"#connprep\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>connprep</h1>\n<p>Produce preprocessed fMRI images ready for connectivity\
    \ analysis.</p>\n<h2>\n<a id=\"user-content-pipeline\" class=\"anchor\" href=\"\
    #pipeline\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pipeline</h2>\n<ol>\n<li>Drop initial or final volumes as specified.\
    \ Default: Analyze all volumes.</li>\n<li>Get the TR (volume acquisition time)\
    \ from pixdim[4] field of the Nifti header.</li>\n<li>Slice timing correction.\
    \ Default: none.</li>\n<li>Head motion realignment (SPM12 two-stage) and production\
    \ of mean fMRI.</li>\n<li>Rigid body coregistration of mean fMRI to T1 structural.</li>\n\
    <li>Compute volume quality metrics FD, DVARS.</li>\n<li>Reslice realigned fMRI\
    \ to native space, and also warp to MNI space using CAT12 transform.</li>\n<li>Remove\
    \ confounds from the native and MNI space fMRIs by simultaneous regression. Defaults:\n\
    <ul>\n<li>0.01 - 0.10 Hz bandpass filter</li>\n<li>6 estimated motion parameters\
    \ and their first differences</li>\n<li>6 principal components from the white\
    \ matter + CSF compartment</li>\n</ul>\n</li>\n<li>Repeat the confound removal,\
    \ additionally removing the mean signal of the gray matter compartment.</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Inputs</h2>\n\
    <pre><code>num_initial_vols_to_drop      0       Number of initial volumes to\
    \ drop\nnum_vols_to_analyze           all     Total number of volumes to analyze\n\
    bandpasslo_hz                 0.01    Low edge of bandpass filter in Hz\nbandpasshi_hz\
    \                 0.10    High edge of bandpass filter\nmot_PCs              \
    \         6       Number of PCs of motion params to remove\nmotderiv_PCs     \
    \             6       Same for motion derivatives\nwmcsf_PCs                 \
    \    6       Same for white matter/CSF compartment\nslorder                  \
    \     none    Slice timing correction, SPM12 nomenclature \nfmri_niigz       \
    \                     fMRI images, 4D Nifti\nmt1_niigz                       \
    \      T1 structural\ndeffwd_niigz                          Forward deformation\
    \ of T1 to MNI\ngray_niigz                            Gray matter volume fraction\n\
    white_niigz                           White matter volume fraction\ncsf_niigz\
    \                             CSF volume fraction\nproject                   \
    \            XNAT project label\nsubject                               XNAT subject\
    \ label\nsession                               XNAT session label\nscan      \
    \                            XNAT scan label\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<pre><code>connprep.pdf\
    \                               Processing report\nrp_adfmri.txt             \
    \                 Realignment parameters\nFD.txt                             \
    \        Framewise displacement\nDVARS.txt                                  Framewise\
    \ noise\nfiltered_keepgm_noscrub_nadfmri.nii.gz     Filtered data, native space,\
    \ gray matter signal retained\nfiltered_keepgm_noscrub_wadfmri.nii.gz     Filtered\
    \ data, MNI space, gray matter signal retained\nfiltered_removegm_noscrub_nadfmri.nii.gz\
    \   Filtered data, native space, gray matter signal removed\nfiltered_removegm_noscrub_wadfmri.nii.gz\
    \   Filtered data, MNI space, gray matter signal removed\nmeanadfmri.nii.gz  \
    \                        Mean fMRI, native space\nwmeanadfmri.nii.gz         \
    \                Mean fMRI, MNI space\nstats_keepgm_noscrub.txt              \
    \     Processing info when gray matter signal retained\nstats_removegm_noscrub.txt\
    \                 Processing info when gray matter signal removed\ngm_mask.nii.gz\
    \                             Native space gray matter mask\nwmcsf_mask.nii.gz\
    \                          Native space white matter/CSF mask\nconfounds_keepgm_noscrub.txt\
    \               Confounds matrix when gray matter signal retained\nconfounds_removegm_noscrub.txt\
    \             Confounds matrix  when gray matter signal removed\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1595372367.0
baxpr/fmri_conncalc:
  data_format: 2
  description: null
  filenames:
  - Singularity.v1.0.0
  full_name: baxpr/fmri_conncalc
  latest_release: v1.0.0-rc0
  readme: "<h1>\n<a id=\"user-content-fmri_conncalc\" class=\"anchor\" href=\"#fmri_conncalc\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>fmri_conncalc</h1>\n<p>Preprocessing and functional connectivity computation\
    \ for fMRI</p>\n<h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"\
    #quickstart\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Quickstart</h2>\n<p>Here is an example for the \"\
    jsins\" version of the processor, as described in\n<a href=\"conncalc_jsins_v1.0.0.yaml\"\
    >conncalc_jsins_v1.0.0.yaml</a>.</p>\n<pre><code>singularity\n  run\n  --bind\
    \ &lt;INDIR&gt;:/INPUTS\n  --bind &lt;OUTDIR&gt;:/OUTPUTS\n  baxpr-fmri_conncalc-master-v1.0.0.simg\n\
    \  magick_path /usr/bin\n  param_file params_JSins.csv\n  wroi_file rois_JSins.nii.gz\n\
    \  roi_file ''\n  roiinfo_file rois_JSins.csv\n  coregmat_file /INPUTS/coreg_mat.txt\
    \ \\\n  deffwd_file /INPUTS/y_deffwd.nii.gz \\\n  ct1_file /INPUTS/ct1.nii.gz\
    \ \\\n  wgm_file /INPUTS/wgm.nii.gz \\\n  wcseg_file /INPUTS/wcseg.nii.gz \\\n\
    \  func_file /INPUTS/fmri.nii.gz \\\n  project PROJECT_LABEL \\\n  subject SUBJECT_LABEL\
    \ \\\n  session SESSION_LABEL \\\n  scan SCAN_LABEL \\\n  out_dir /OUTPUTS\n</code></pre>\n\
    <p>The inputs <code>coregmat_file</code>, <code>deffwd_file</code>, <code>ct1_file</code>,\
    \ <code>wgm_file</code>, <code>wcseg_file</code> would typically be obtained from\
    \ the outputs of the <code>MAGM_Coreg_Normalize_v2</code> spider.</p>\n<p>The\
    \ outputs are:</p>\n<pre><code>fmri_conncalc.pdf    Report\nparams.csv       \
    \    Parameters used in the analysis\nFD.txt               Framewise displacement\
    \ time series\nDVARS.txt            Framewise variance time series\nbadvols.txt\
    \          Scrubbed volumes indicator time series\nrp_adfunc.txt        Realignment\
    \ (motion) values\nwmeanadfunc.nii.gz   Mean functional image in standard space\n\
    wadfunc.nii.gz       Slice time corrected and realigned functional images in standard\
    \ space\nrroi_labels.nii.gz   Region of interest label image\nroi_snr.nii.gz \
    \      ROI SNR image\nroi_info.csv         ROI info\nroi_labels.csv       ROI\
    \ names (if available)\n\nSeries of results repeated for each of the four processing\
    \ streams\n(keep or remove mean gray matter; scrub or no scrub):\n\n  confounds_removegm_noscrub.txt\
    \               Confound (filter) matrix\n  connectivity_matrix_R_removegm_noscrub.csv\
    \   Connectivity matrix\n  filtered_removegm_noscrub.nii.gz             Filtered\
    \ functional images\n  roi_timeseries_removegm_noscrub.csv          Filtered ROI\
    \ time series\n  stats_removegm_noscrub.txt                   Various statistics\n\
    \  Zmap_removegm_noscrub.nii.gz                 Unsmoothed ROI connectivity maps\n\
    \  sZmap_removegm_noscrub.nii.gz                Smoothed ROI connectivity maps\n\
    </code></pre>\n<h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"\
    #dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Dependencies</h2>\n<p>The built singularity container\
    \ <code>baxpr-fmri_conncalc-master-v1.0.0.simg</code> (URL is shub://baxpr/fmri_conncalc:v1.0.0)\
    \ is stand-alone with no external dependencies. The compiled matlab <a href=\"\
    bin/run_fmri_conncalc.sh\">run_fmri_conncalc.sh</a> requires only the appropriate\
    \ MATLAB Runtime to execute. To build these there are two stages:</p>\n<ol>\n\
    <li>\n<p>Compile the MATLAB code into a stand-alone executable, using <a href=\"\
    compile_matlab.sh\">compile_matlab.sh</a>. This requires a full MATLAB installation\
    \ (R2017a, v92) and SPM12 (<a href=\"https://www.fil.ion.ucl.ac.uk/spm/\" rel=\"\
    nofollow\">https://www.fil.ion.ucl.ac.uk/spm/</a>).</p>\n</li>\n<li>\n<p>Build\
    \ the singularity container. In addition to a few specific OS packages, this requires\
    \ the MATLAB Compiled Runtime. All are specified to be downloaded during the build\
    \ in the singularity recipe <a href=\"Singularity.v1.0.0\">Singularity.v1.0.0</a>.\
    \ The container help text gives build instructions. Alternatively the built container\
    \ can be obtained from singularity-hub:\n<code>singularity pull shub://baxpr/fmri_conncalc:v1.0.0</code></p>\n\
    </li>\n</ol>\n<h2>\n<a id=\"user-content-peculiarities-of-specific-pipelines\"\
    \ class=\"anchor\" href=\"#peculiarities-of-specific-pipelines\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Peculiarities\
    \ of specific pipelines</h2>\n<p>Some critical analysis parameters are specified\
    \ in the <code>param_file</code>, e.g. <code>params_JSins.csv</code>. This is\
    \ a reference to a file that's in the built container, but these can also be viewed\
    \ in the code repository e.g. <a href=\"src/params/params_JSins.csv\">src/params/params_JSins.csv</a>.\
    \ The parameters get as detailed as the repetition time of the fMRI scans. If\
    \ the needed parameter file is not in the container already:</p>\n<ul>\n<li>Add\
    \ the new parameter file in <code>src/params</code>\n</li>\n<li>Update the matlab\
    \ compilation code to include it with <code>-a</code>\n</li>\n<li>Recompile the\
    \ matlab</li>\n<li>Commit to github. Note that the compiled matlab executable\
    \ is stored using LFS</li>\n<li>Rebuild the container (increment the patch number,\
    \ e.g. 1.0.0 to 1.0.1)</li>\n<li>Create an updated YAML file appropriate for the\
    \ parameter set</li>\n</ul>\n<h3>\n<a id=\"user-content-jsins-version\" class=\"\
    anchor\" href=\"#jsins-version\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>jsins version</h3>\n<p><a href=\"\
    conncalc_jsins_v1.0.0.yaml\">conncalc_jsins_v1.0.0.yaml</a></p>\n<p>Standard space\
    \ regions of interest are used, <a href=\"src/params/JS_insula/rois_JSins.nii.gz\"\
    >rois_JSins.nii.gz</a>, identical for every subject.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R). A connectivity map is computed\
    \ for each ROI (Fisher Z transform applied to Pearson bivariate correlation).\
    \ Spatial smoothing is applied to the connectivity maps only.</p>\n<p>Parameter\
    \ settings in <a href=\"src/params/params_JSins.csv\">params_JSins.csv</a>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>Use all\
    \ fMRI volumes (none dropped)</li>\n<li>No slice timing correction</li>\n<li>6mm\
    \ FWHM Gaussian spatial smoothing applied to connectivity maps</li>\n<li>Filter\
    \ settings (confound regressor matrix):\n<ul>\n<li>0.01 Hz - 0.10 Hz bandpass\
    \ filter (Fourier basis)</li>\n<li>6 motion parameters (translation and rotation)</li>\n\
    <li>6 first differences of motion parameters</li>\n<li>First 6 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n<li>Connectivity maps\
    \ are saved for each ROI.</li>\n</ul>\n<h3>\n<a id=\"user-content-szhab-version\"\
    \ class=\"anchor\" href=\"#szhab-version\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>szhab version</h3>\n<p>No YAML\
    \ available yet.</p>\n<p>Subject-specific regions of interest are used, as described\
    \ in the native space ROI image supplied as input. This image must be in the same\
    \ space as the subject's native space structural.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R of filtered time series). Spatial\
    \ smoothing is not used.</p>\n<p>Parameter settings in <code>params_SZhab.csv</code>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>5 initial\
    \ volumes are dropped, and the following 60 volumes are used for the analysis</li>\n\
    <li>No slice timing correction</li>\n<li>Filter settings (confound regressor matrix):\n\
    <ul>\n<li>0.01 Hz - 0.15 Hz bandpass filter (Fourier basis)</li>\n<li>6 motion\
    \ parameters (translation and rotation)</li>\n<li>First 3 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-general-pipeline\" class=\"anchor\" href=\"#general-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>General\
    \ pipeline</h2>\n<p>Other than the above, processing proceeds as follows.</p>\n\
    <ol>\n<li>\n<p>Drop functional volumes as specified.</p>\n</li>\n<li>\n<p>Perform\
    \ slice timing correction as specified. (SPM12 slice timing correction)</p>\n\
    </li>\n<li>\n<p>Perform motion realignment: two-stage alignment to mean image.\
    \ (SPM12 realignment)</p>\n</li>\n<li>\n<p>Coregister the mean functional image\
    \ to the T1 weighted structural using a rigid body transform. The structural is\
    \ first skull-stripped by zeroing all voxels that were not labeled by the multiatlas\
    \ segmentation. The transformation is then applied to all functional volumes.\
    \ (SPM12 coregistration)</p>\n</li>\n<li>\n<p>Quality parameters are computed:\
    \ framewise displacement FD and framewise signal variance DVARS. Volumes exceeding\
    \ scrubbing criteria are marked (\"badvols\").</p>\n</li>\n<li>\n<p>The functional\
    \ and structural images are warped to standard space using the supplied nonlinear\
    \ transform (forward deformation image). (SPM12 deformation tools)</p>\n</li>\n\
    <li>\n<p>The supplied standard space ROI image file is resampled to match the\
    \ standard space fMRI geometry. (SPM12 reslice)</p>\n</li>\n<li>\n<p>Connectivity\
    \ computation. All filtering is done in a single step: a design matrix of confounds\
    \ is created (see lists above), it is fit to each voxel time series, and the residuals\
    \ are extracted. Then bivariate Pearson correlation is computed between ROI residual\
    \ time series to produce the connectivity matrix. Fisher transformed correlation\
    \ between ROIs/voxel residual time series is used to produce connectivity maps\
    \ if that option is selected.</p>\n</li>\n</ol>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1543615331.0
baxpr/fmri_modularity:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.0.0
  full_name: baxpr/fmri_modularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-fmri_conncalc\" class=\"anchor\" href=\"#fmri_conncalc\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>fmri_conncalc</h1>\n<p>Preprocessing and functional connectivity computation\
    \ for fMRI</p>\n<h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"\
    #quickstart\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Quickstart</h2>\n<p>Here is an example for the \"\
    jsins\" version of the processor, as described in\n<a href=\"conncalc_jsins_v1.0.0.yaml\"\
    >conncalc_jsins_v1.0.0.yaml</a>.</p>\n<pre><code>singularity\n  run\n  --bind\
    \ &lt;INDIR&gt;:/INPUTS\n  --bind &lt;OUTDIR&gt;:/OUTPUTS\n  baxpr-fmri_conncalc-master-v1.0.0.simg\n\
    \  magick_path /usr/bin\n  param_file params_JSins.csv\n  wroi_file rois_JSins.nii.gz\n\
    \  roi_file ''\n  roiinfo_file rois_JSins.csv\n  coregmat_file /INPUTS/coreg_mat.txt\
    \ \\\n  deffwd_file /INPUTS/y_deffwd.nii.gz \\\n  ct1_file /INPUTS/ct1.nii.gz\
    \ \\\n  wgm_file /INPUTS/wgm.nii.gz \\\n  wcseg_file /INPUTS/wcseg.nii.gz \\\n\
    \  func_file /INPUTS/fmri.nii.gz \\\n  project PROJECT_LABEL \\\n  subject SUBJECT_LABEL\
    \ \\\n  session SESSION_LABEL \\\n  scan SCAN_LABEL \\\n  out_dir /OUTPUTS\n</code></pre>\n\
    <p>The inputs <code>coregmat_file</code>, <code>deffwd_file</code>, <code>ct1_file</code>,\
    \ <code>wgm_file</code>, <code>wcseg_file</code> would typically be obtained from\
    \ the outputs of the <code>MAGM_Coreg_Normalize_v2</code> spider.</p>\n<p>The\
    \ outputs are:</p>\n<pre><code>fmri_conncalc.pdf    Report\nparams.csv       \
    \    Parameters used in the analysis\nFD.txt               Framewise displacement\
    \ time series\nDVARS.txt            Framewise variance time series\nbadvols.txt\
    \          Scrubbed volumes indicator time series\nrp_adfunc.txt        Realignment\
    \ (motion) values\nwmeanadfunc.nii.gz   Mean functional image in standard space\n\
    wadfunc.nii.gz       Slice time corrected and realigned functional images in standard\
    \ space\nrroi_labels.nii.gz   Region of interest label image\nroi_snr.nii.gz \
    \      ROI SNR image\nroi_info.csv         ROI info\nroi_labels.csv       ROI\
    \ names (if available)\n\nSeries of results repeated for each of the four processing\
    \ streams\n(keep or remove mean gray matter; scrub or no scrub):\n\n  confounds_removegm_noscrub.txt\
    \               Confound (filter) matrix\n  connectivity_matrix_R_removegm_noscrub.csv\
    \   Connectivity matrix\n  filtered_removegm_noscrub.nii.gz             Filtered\
    \ functional images\n  roi_timeseries_removegm_noscrub.csv          Filtered ROI\
    \ time series\n  stats_removegm_noscrub.txt                   Various statistics\n\
    \  Zmap_removegm_noscrub.nii.gz                 Unsmoothed ROI connectivity maps\n\
    \  sZmap_removegm_noscrub.nii.gz                Smoothed ROI connectivity maps\n\
    </code></pre>\n<h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"\
    #dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Dependencies</h2>\n<p>The built singularity container\
    \ <code>baxpr-fmri_conncalc-master-v1.0.0.simg</code> (URL is shub://baxpr/fmri_conncalc:v1.0.0)\
    \ is stand-alone with no external dependencies. The compiled matlab <a href=\"\
    bin/run_fmri_conncalc.sh\">run_fmri_conncalc.sh</a> requires only the appropriate\
    \ MATLAB Runtime to execute. To build these there are two stages:</p>\n<ol>\n\
    <li>\n<p>Compile the MATLAB code into a stand-alone executable, using <a href=\"\
    compile_matlab.sh\">compile_matlab.sh</a>. This requires a full MATLAB installation\
    \ (R2017a, v92) and SPM12 (<a href=\"https://www.fil.ion.ucl.ac.uk/spm/\" rel=\"\
    nofollow\">https://www.fil.ion.ucl.ac.uk/spm/</a>).</p>\n</li>\n<li>\n<p>Build\
    \ the singularity container. In addition to a few specific OS packages, this requires\
    \ the MATLAB Compiled Runtime. All are specified to be downloaded during the build\
    \ in the singularity recipe <a href=\"Singularity.v1.0.0\">Singularity.v1.0.0</a>.\
    \ The container help text gives build instructions. Alternatively the built container\
    \ can be obtained from singularity-hub:\n<code>singularity pull shub://baxpr/fmri_conncalc:v1.0.0</code></p>\n\
    </li>\n</ol>\n<h2>\n<a id=\"user-content-peculiarities-of-specific-pipelines\"\
    \ class=\"anchor\" href=\"#peculiarities-of-specific-pipelines\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Peculiarities\
    \ of specific pipelines</h2>\n<p>Some critical analysis parameters are specified\
    \ in the <code>param_file</code>, e.g. <code>params_JSins.csv</code>. This is\
    \ a reference to a file that's in the built container, but these can also be viewed\
    \ in the code repository e.g. <a href=\"src/params/params_JSins.csv\">src/params/params_JSins.csv</a>.\
    \ The parameters get as detailed as the repetition time of the fMRI scans. If\
    \ the needed parameter file is not in the container already:</p>\n<ul>\n<li>Add\
    \ the new parameter file in <code>src/params</code>\n</li>\n<li>Update the matlab\
    \ compilation code to include it with <code>-a</code>\n</li>\n<li>Recompile the\
    \ matlab</li>\n<li>Commit to github. Note that the compiled matlab executable\
    \ is stored using LFS</li>\n<li>Rebuild the container (increment the patch number,\
    \ e.g. 1.0.0 to 1.0.1)</li>\n<li>Create an updated YAML file appropriate for the\
    \ parameter set</li>\n</ul>\n<h3>\n<a id=\"user-content-jsins-version\" class=\"\
    anchor\" href=\"#jsins-version\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>jsins version</h3>\n<p><a href=\"\
    conncalc_jsins_v1.0.0.yaml\">conncalc_jsins_v1.0.0.yaml</a></p>\n<p>Standard space\
    \ regions of interest are used, <a href=\"src/params/JS_insula/rois_JSins.nii.gz\"\
    >rois_JSins.nii.gz</a>, identical for every subject.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R). A connectivity map is computed\
    \ for each ROI (Fisher Z transform applied to Pearson bivariate correlation).\
    \ Spatial smoothing is applied to the connectivity maps only.</p>\n<p>Parameter\
    \ settings in <a href=\"src/params/params_JSins.csv\">params_JSins.csv</a>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>Use all\
    \ fMRI volumes (none dropped)</li>\n<li>No slice timing correction</li>\n<li>6mm\
    \ FWHM Gaussian spatial smoothing applied to connectivity maps</li>\n<li>Filter\
    \ settings (confound regressor matrix):\n<ul>\n<li>0.01 Hz - 0.10 Hz bandpass\
    \ filter (Fourier basis)</li>\n<li>6 motion parameters (translation and rotation)</li>\n\
    <li>6 first differences of motion parameters</li>\n<li>First 6 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n<li>Connectivity maps\
    \ are saved for each ROI.</li>\n</ul>\n<h3>\n<a id=\"user-content-szhab-version\"\
    \ class=\"anchor\" href=\"#szhab-version\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>szhab version</h3>\n<p>No YAML\
    \ available yet.</p>\n<p>Subject-specific regions of interest are used, as described\
    \ in the native space ROI image supplied as input. This image must be in the same\
    \ space as the subject's native space structural.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R of filtered time series). Spatial\
    \ smoothing is not used.</p>\n<p>Parameter settings in <code>params_SZhab.csv</code>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>5 initial\
    \ volumes are dropped, and the following 60 volumes are used for the analysis</li>\n\
    <li>No slice timing correction</li>\n<li>Filter settings (confound regressor matrix):\n\
    <ul>\n<li>0.01 Hz - 0.15 Hz bandpass filter (Fourier basis)</li>\n<li>6 motion\
    \ parameters (translation and rotation)</li>\n<li>First 3 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-general-pipeline\" class=\"anchor\" href=\"#general-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>General\
    \ pipeline</h2>\n<p>Other than the above, processing proceeds as follows.</p>\n\
    <ol>\n<li>\n<p>Drop functional volumes as specified.</p>\n</li>\n<li>\n<p>Perform\
    \ slice timing correction as specified. (SPM12 slice timing correction)</p>\n\
    </li>\n<li>\n<p>Perform motion realignment: two-stage alignment to mean image.\
    \ (SPM12 realignment)</p>\n</li>\n<li>\n<p>Coregister the mean functional image\
    \ to the T1 weighted structural using a rigid body transform. The structural is\
    \ first skull-stripped by zeroing all voxels that were not labeled by the multiatlas\
    \ segmentation. The transformation is then applied to all functional volumes.\
    \ (SPM12 coregistration)</p>\n</li>\n<li>\n<p>Quality parameters are computed:\
    \ framewise displacement FD and framewise signal variance DVARS. Volumes exceeding\
    \ scrubbing criteria are marked (\"badvols\").</p>\n</li>\n<li>\n<p>The functional\
    \ and structural images are warped to standard space using the supplied nonlinear\
    \ transform (forward deformation image). (SPM12 deformation tools)</p>\n</li>\n\
    <li>\n<p>The supplied standard space ROI image file is resampled to match the\
    \ standard space fMRI geometry. (SPM12 reslice)</p>\n</li>\n<li>\n<p>Connectivity\
    \ computation. All filtering is done in a single step: a design matrix of confounds\
    \ is created (see lists above), it is fit to each voxel time series, and the residuals\
    \ are extracted. Then bivariate Pearson correlation is computed between ROI residual\
    \ time series to produce the connectivity matrix. Fisher transformed correlation\
    \ between ROIs/voxel residual time series is used to produce connectivity maps\
    \ if that option is selected.</p>\n</li>\n</ol>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550158474.0
baxpr/fmriqa:
  data_format: 2
  description: null
  filenames:
  - Singularity.v4.2.0
  full_name: baxpr/fmriqa
  latest_release: null
  readme: '<h1>

    <a id="user-content-functional-mri-qa-pipeline" class="anchor" href="#functional-mri-qa-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Functional
    MRI QA pipeline</h1>

    <h2>

    <a id="user-content-building" class="anchor" href="#building" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h2>

    <ol>

    <li>Test the matlab code before compiling: <code>src/testmatlab.m</code>

    </li>

    <li>Compile: <code>compile_matlab.sh</code>

    </li>

    <li>Test the compiled runtime: <code>bin/test_compiled_matlab.sh</code>

    </li>

    <li>Build the Singularity container: <code>Singularity.v4.2.0</code>, <a href="https://www.singularity-hub.org/collections/2945"
    rel="nofollow">https://www.singularity-hub.org/collections/2945</a>

    </li>

    </ol>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ol>

    <li>See <code>test_sing_container.sh</code>.</li>

    </ol>

    <h2>

    <a id="user-content-inputs" class="anchor" href="#inputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Inputs</h2>

    <p>The inputs must all be provided, in the correct order. Paths are with respect
    to the container root.</p>

    <ol>

    <li>Name of the output directory</li>

    <li>Filename of the T1 structural image (.nii.gz)</li>

    <li>Filename of the segmented T1 image (.nii.gz), typically the SEG output of
    a MultiAtlas or SLANT pipeline</li>

    <li>Filename of the 4D fMRI (.nii.gz)</li>

    <li>XNAT project label</li>

    <li>XNAT subject label</li>

    <li>XNAT session label</li>

    <li>XNAT scan label (of the fMRI)</li>

    </ol>

    <h2>

    <a id="user-content-processing" class="anchor" href="#processing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Processing</h2>

    <ol>

    <li>Motion realignment and creation of mean fMRI</li>

    <li>Coregister T1 to mean fMRI</li>

    <li>Compute SNR and quality metrics</li>

    <li>Carpet plots, graphical report</li>

    </ol>

    <h2>

    <a id="user-content-outputs" class="anchor" href="#outputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Outputs</h2>

    <pre><code>fmriqa.pdf                               PDF report

    rp_fmri.txt                              Realignment parameters (SPM12 style)

    fmriqa_stats.csv                         Summary stats

    fmriqa_stats_wide.csv                    Summary stats in wide format (XNAT/REDCap
    compatible)

    FD.txt                                   Framewise displacement time series

    DVARS.txt                                DVARS time series

    global.txt                               Global mean time series

    meanfmri.nii.gz                          Mean fMRI image after realignment

    median_voxel_displacement_mm.txt         Framewise displacement, median over voxels

    temporal_snr.nii.gz                      Temporal signal-to-noise ratio image

    voxel_displacement_mm_95prctile.nii.gz   Framewise displacement image (95th percentile
    over time)

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1558037991.0
baxpr/makerois-PMAT:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: baxpr/makerois-PMAT
  latest_release: v1.0.13
  readme: "<h1>\n<a id=\"user-content-create-study-specific-roi-image-in-mni-space\"\
    \ class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Create\
    \ study-specific ROI image in MNI space</h1>\n<p>PMAT resting state connectivity\
    \ study.</p>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Inputs:</h2>\n<p>All should be matched to the same T1 image.</p>\n\
    <ul>\n<li>T1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)</li>\n\
    <li>Deformation from T1 subject space to atlas space (typically DEF_FWD resource\
    \ of cat12 assessor)</li>\n<li>SUBJECT directory of Freesurfer output (typically\
    \ SUBJECT resource of freesurfer_dev assessor)</li>\n<li>Temporal lobe segmentation\
    \ (typically SEG resource of Temporal_Lobe assessor)</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n\
    <pre><code>rois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv\
    \        Region labels and volumes\nmakerois-PMAT.pdf           Visual report\
    \ of final ROI image\n</code></pre>\n<h2>\n<a id=\"user-content-regions-of-interest\"\
    \ class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Regions of interest</h2>\n<h3>\n\
    <a id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spheres (atlas space)</h3>\n<p>Source: <em>Libby LA, Ekstrom AD, Ragland\
    \ JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal\
    \ cortices within human hippocampal subregions revealed by high-resolution functional\
    \ imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012</em></p>\n\
    <p>Method: <em>Schr\xF6der TN, Haak K V., Jimenez NIZ, et al. Functional topography\
    \ of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"\
    #entorhinal-cortex-atlas-space\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Entorhinal cortex (atlas space)</h3>\n\
    <p>Anterior lateral and posterior medial sections. Source and method: <em>Schr\xF6\
    der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal\
    \ cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\"\
    \ href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Temporal lobe (Subject space,\
    \ warped)</h3>\n<p>Head for anterior hippocampus; body and tail combined for posterior\
    \ hippocampus. Method: <em>Plassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale\
    \ Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE\
    \ Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017\
    \ Feb 24. PMID: 28781411; PMCID: PMC5544133.</em></p>\n<h3>\n<a id=\"user-content-parahippocampal-perirhinal-subject-space-warped\"\
    \ class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parahippocampal,\
    \ perirhinal (Subject space, warped)</h3>\n<p>Generated by Freesurfer 6. Parahippocampal\
    \ (1016,2016) and perirhinal (surface patch resampled to volume, overlap with\
    \ parahippocampus was assigned to perirhinal). Method: <em>Bruce Fischl, Andre\
    \ van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H.\
    \ Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne\
    \ Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating\
    \ the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.</em></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607988459.0
baxpr/mp2rage:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.1.0
  full_name: baxpr/mp2rage
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-mp2rage\" class=\"anchor\" href=\"#mp2rage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>mp2rage</h1>\n<p>Reconstructs a T1-weighted image from images at multiple\
    \ inversion times following Marques et al. 2009. The robust adjustment (beta factor)\
    \ of O'Brien 2014 is also implemented.</p>\n<p><strong>Marques JP, Kober T, Krueger\
    \ G, van der Zwaag W, Van de Moortele PF, Gruetter  R. MP2RAGE, a self bias-field\
    \ corrected sequence for improved segmentation and T1-mapping at high field. Neuroimage.\
    \ 2010 Jan 15;49(2):1271-81. doi:10.1016/j.neuroimage.2009.10.002. PMID: 19819338.</strong></p>\n\
    <p><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/19819338\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/pubmed/19819338</a></p>\n\
    <p>The large spatial inhomogeneity in transmit B(1) field (B(1)(+)) observable\
    \ in human MR images at high static magnetic fields (B(0)) severely impairs image\
    \ quality. To overcome this effect in brain T(1)-weighted images, the MPRAGE sequence\
    \ was modified to generate two different images at different inversion times,\
    \ MP2RAGE. By combining the two images in a novel fashion, it was possible to\
    \ create T(1)-weighted images where the result image was free of proton density\
    \ contrast, T(2) contrast, reception bias field, and, to first order, transmit\
    \ field inhomogeneity. MP2RAGE sequence parameters were optimized using Bloch\
    \ equations to maximize contrast-to-noise ratio per unit of time between brain\
    \ tissues and minimize the effect of B(1)(+) variations through space. Images\
    \ of high anatomical quality and excellent brain tissue differentiation suitable\
    \ for applications such as segmentation and voxel-based morphometry were obtained\
    \ at 3 and 7 T. From such T(1)-weighted images, acquired within 12 min, high-resolution\
    \ 3D T(1) maps were routinely calculated at 7 T with sub-millimeter voxel resolution\
    \ (0.65-0.85 mm isotropic). T(1) maps were validated in phantom experiments. In\
    \ humans, the T(1) values obtained at 7 T were 1.15+/-0.06 s for white matter\
    \ (WM) and 1.92+/-0.16 s for grey matter (GM), in good agreement with literature\
    \ values obtained at lower spatial resolution. At 3 T, where whole-brain acquisitions\
    \ with 1 mm isotropic voxels were acquired in 8 min, the T(1) values obtained\
    \ (0.81+/-0.03 s for WM and 1.35+/-0.05 for GM) were once again found to be in\
    \ very good agreement with values in the literature.</p>\n<p><strong>O'Brien KR,\
    \ Kober T, Hagmann P, et al. Robust T1-weighted Structural Brain Imaging and Morphometry\
    \ at 7T Using MP2RAGE PLoS One. 2014;9(6):e99676. Published 2014 Jun 16. doi:10.1371/journal.pone.0099676</strong></p>\n\
    <p><a href=\"https://pubmed.ncbi.nlm.nih.gov/24932514/\" rel=\"nofollow\">https://pubmed.ncbi.nlm.nih.gov/24932514/</a></p>\n\
    <p>Purpose: To suppress the noise, by sacrificing some of the signal homogeneity\
    \ for numerical stability, in uniform T1 weighted (T1w) images obtained with the\
    \ magnetization prepared 2 rapid gradient echoes sequence (MP2RAGE) and to compare\
    \ the clinical utility of these robust T1w images against the uniform T1w images.</p>\n\
    <p>Materials and methods: 8 healthy subjects (29.0 \xB1 4.1 years; 6 Male), who\
    \ provided written consent, underwent two scan sessions within a 24 hour period\
    \ on a 7T head-only scanner. The uniform and robust T1w image volumes were calculated\
    \ inline on the scanner. Two experienced radiologists qualitatively rated the\
    \ images for: general image quality; 7T specific artefacts; and, local structure\
    \ definition. Voxel-based and volume-based morphometry packages were used to compare\
    \ the segmentation quality between the uniform and robust images. Statistical\
    \ differences were evaluated by using a positive sided Wilcoxon rank test.</p>\n\
    <p>Results: The robust image suppresses background noise inside and outside the\
    \ skull. The inhomogeneity introduced was ranked as mild. The robust image was\
    \ significantly ranked higher than the uniform image for both observers (observer\
    \ 1/2, p-value = 0.0006/0.0004). In particular, an improved delineation of the\
    \ pituitary gland, cerebellar lobes was observed in the robust versus uniform\
    \ T1w image. The reproducibility of the segmentation results between repeat scans\
    \ improved (p-value = 0.0004) from an average volumetric difference across structures\
    \ of \u2248 6.6% to \u2248 2.4% for the uniform image and robust T1w image respectively.</p>\n\
    <p>Conclusions: The robust T1w image enables MP2RAGE to produce, clinically familiar\
    \ T1w images, in addition to T1 maps, which can be readily used in uniform morphometry\
    \ packages.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1595274818.0
baxpr/naleg-roi:
  data_format: 2
  description: null
  filenames:
  - Singularity.v1.0.0
  full_name: baxpr/naleg-roi
  latest_release: null
  readme: '<h1>

    <a id="user-content-roi-analysis-for-sodium-leg-mri-scans" class="anchor" href="#roi-analysis-for-sodium-leg-mri-scans"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ROI
    analysis for sodium leg MRI scans</h1>

    <pre><code>src   Matlab code for extracting calibrating ROI values from sodium
    scans

    bin   Compiled matlab code (needed to create simgularity container)


    compile_matlab.sh              Shell script to compile matlab code

    spm_make_standalone_local.m    Prepare SPM distribution for compilation


    Simgularity.v1.0.0    Singularity recipe to build container with compiled matlab

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1594830481.0
baxpr/segwarp:
  data_format: 2
  description: null
  filenames:
  - Singularity.v1.0.0
  full_name: baxpr/segwarp
  latest_release: null
  readme: '<p>Warp SEG output of a multi-atlas assessor to MNI space using the supplied
    SPM warp field.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605062943.0
bbodi/rustarok:
  data_format: 2
  description: Multiplayer, fast-paced Moba style game
  filenames:
  - docker/Singularity
  - docker/Singularity.wine
  full_name: bbodi/rustarok
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-table-of-contents\" class=\"anchor\" href=\"\
    #table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Table of contents</h1>\n<ul>\n<li><a href=\"\
    #building\">Building</a></li>\n<li><a href=\"#running-on-windows\">Running on\
    \ Windows</a></li>\n<li><a href=\"#running-with-docker\">Running with Docker</a></li>\n\
    <li><a href=\"#how-to-play\">How to play</a></li>\n<li><a href=\"#design-decisions\"\
    >Design decisions</a></li>\n<li><a href=\"#blog\">Blog</a></li>\n<li><a href=\"\
    #current-status-and-gallery\">Current Status and Gallery</a></li>\n<li>\n<a href=\"\
    #background-story\">Background story</a>\n<ul>\n<li><a href=\"#about-the-used-game-assets\"\
    >About the used game assets</a></li>\n</ul>\n</li>\n<li><a href=\"#thanks\">Thanks</a></li>\n\
    </ul>\n<h1>\n<a id=\"user-content-rustarok\" class=\"anchor\" href=\"#rustarok\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Rustarok</h1>\n<p>A project whose primary goals are to have fun developing\
    \ it and experiment with interesting technical problems from the world of game\
    \ development.</p>\n<p>It is intended to be a multiplayer, fast-paced Moba style\
    \ game. Check <a href=\"#background-story\">Background story</a> for details.</p>\n\
    <h2>\n<a id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building</h2>\n\
    <ul>\n<li><code>git clone https://github.com/bbodi/rustarok.git</code></li>\n\
    <li><code>cargo build</code></li>\n</ul>\n<h2>\n<a id=\"user-content-running-on-windows\"\
    \ class=\"anchor\" href=\"#running-on-windows\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running on Windows</h2>\n<p>You\
    \ will need Ragnarok Online asset files to run the game.</p>\n<ul>\n<li>\n<p>Download\
    \ a Ragnarok Online client from <a href=\"https://talonro.com/\" rel=\"nofollow\"\
    >some</a> <a href=\"http://playdreamerro.com/\" rel=\"nofollow\">popular</a> <a\
    \ href=\"https://topg.org/ragnarok-private-servers/\" rel=\"nofollow\">private\
    \ server</a></p>\n</li>\n<li>\n<p>Install it</p>\n</li>\n<li>\n<p>Check the installation\
    \ directory for any <code>*.grf</code> files, and put their paths into <code>config.toml</code>,\
    \ e.g.:</p>\n<pre><code>grf_paths = [\n  \"d:\\\\Games\\\\TalonRO\\\\rdata.grf\"\
    ,\n  \"d:\\\\Games\\\\TalonRO\\\\sdata.grf\",\n  \"d:\\\\Games\\\\TalonRO\\\\\
    tdata.grf\"\n]\n</code></pre>\n</li>\n<li>\n<p>Run <code>cargo run</code> from\
    \ rustarok directory.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-running-with-docker\"\
    \ class=\"anchor\" href=\"#running-with-docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running with Docker</h2>\n<p>See\
    \ the README.md in the <a href=\"docker\">docker</a> folder for complete instructions.</p>\n\
    <h2>\n<a id=\"user-content-how-to-play\" class=\"anchor\" href=\"#how-to-play\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to play</h2>\n<ul>\n<li>Move your character with the right mouse\
    \ button</li>\n<li>Cast skills with Q (fire wall), W (lightning), E (heal), R\
    \ (huge boom) keys</li>\n<li>Spawn entities with the \"Players\" and \"Monsters\"\
    \ sliders in the window</li>\n<li>Move the camera with the cursor keys</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-design-decisions\" class=\"anchor\" href=\"\
    #design-decisions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Design decisions</h2>\n<ul>\n<li><a href=\"https://github.com/bbodi/rustarok/issues/1\"\
    >Statuses</a></li>\n<li><a href=\"https://github.com/bbodi/rustarok/issues/4\"\
    >Rendering system</a></li>\n</ul>\n<h2>\n<a id=\"user-content-blog\" class=\"\
    anchor\" href=\"#blog\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Blog</h2>\n<ul>\n<li><a href=\"https://github.com/bbodi/rustarok/issues/6\"\
    >2019W30</a></li>\n</ul>\n<h2>\n<a id=\"user-content-current-status-and-gallery\"\
    \ class=\"anchor\" href=\"#current-status-and-gallery\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Current Status\
    \ and Gallery</h2>\n<p>Currently the project is in a very early stage. Nothing\
    \ is set in stone yet, I mostly prototyping ideas and techniques.</p>\n<p>List\
    \ of developed features:</p>\n<ul>\n<li>[x] Asset file loading (grf, gnd, rsm,\
    \ rsw, spr, act, str)</li>\n<li>[x] Rendering\n<ul>\n<li>[x] Map (ground, static\
    \ models, lighting)</li>\n</ul>\n</li>\n<li>[x] Sprites for UI\n<ul>\n<li>[x]\
    \ Sprites in 3D world (animated sprites and effects as well)\n<ul>\n<li>[x] Different\
    \ actions (idle, sit, walk, attack, cast etc)</li>\n<li>[x] Directions</li>\n\
    </ul>\n</li>\n</ul>\n</li>\n<li>[x] Amount of damages, heals, etc\n<ul>\n<li>[x]\
    \ Health and Mana bar above the characters</li>\n</ul>\n</li>\n<li>[x] Input handling,\
    \ Control\n<ul>\n<li>[x] Moving around with your character</li>\n<li>[x] Assigning\
    \ skills to Q, W, E, R, etc keys</li>\n<li>[x] Continuous movement towards the\
    \ mouse if RMB is down</li>\n</ul>\n</li>\n<li>[x] Skills\n<ul>\n<li>[x] Skill\
    \ target area/entity selection mode</li>\n<li>[x] Skill casting</li>\n<li>[x]\
    \ Skill manifestations (the manifested outcome of using a skill, e.g. a fire wall\
    \ in the 3D world which can't be walk through and it damages contacting entities)</li>\n\
    <li>[x] Quick cast settings (on, on-release, off)</li>\n</ul>\n</li>\n<li>[x]\
    \ Battle\n<ul>\n<li>[x] Attacking an enemy</li>\n<li>[x] Attack speed</li>\n<li>[x]\
    \ Health, dying</li>\n</ul>\n</li>\n<li>[x] Collision\n<ul>\n<li>[x] Static objects</li>\n\
    <li>[x] Characters, a.k.a <a href=\"https://www.youtube.com/watch?v=nk2O6YsCWwI\"\
    \ rel=\"nofollow\">body block</a>\n</li>\n</ul>\n</li>\n</ul>\n<p>\n<a href=\"\
    readme_assets/body_blocking.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img width=\"300\" src=\"readme_assets/body_blocking.gif\" title=\"Body blocking\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/normal_aspd.gif\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/normal_aspd.gif\"\
    \ title=\"Normal attack\" style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/quick_aspd.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/quick_aspd.gif\"\
    \ title=\"Quick attack\" style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/heal.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/heal.gif\"\
    \ title=\"Heal\" style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/aoe.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/aoe.gif\"\
    \ title=\"AoE skill\" style=\"max-width:100%;\"></a>\n</p>\n<h2>\n<a id=\"user-content-background-story\"\
    \ class=\"anchor\" href=\"#background-story\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Background story</h2>\n<p>I play\
    \ computer games rarely, but when I do, I play 1 or 2 sessions of Heroes of The\
    \ Storm match.</p>\n<p>But still, whenever I play, I am constantly thinking about\
    \ how I would implement some mechanics of the game.</p>\n<p>So finally I reached\
    \ the point where fantasizing is not enough anymore, and wanted to actually try\
    \ myself in this area as well.</p>\n<p>Don't be surprised if the game is heavily\
    \ inspired by HoTS, most probably the playable character styles and skills will\
    \ be based on my favourite characters from it, or the ones whose skill mechanics\
    \ I find challenging or interesting.</p>\n<h3>\n<a id=\"user-content-about-the-used-game-assets\"\
    \ class=\"anchor\" href=\"#about-the-used-game-assets\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>About the used\
    \ game assets</h3>\n<p>The visuals of Rustarok might be familiar to you. It is\
    \ because the game uses assets from an existing game, an older popular Korean\
    \ MMORPG, Ragnarok Online. The reasons I used them:</p>\n<ul>\n<li>\n<p>I am not\
    \ a graphic designer, I don't have the skills nor the temptation to create the\
    \ visuals of a game myself.</p>\n</li>\n<li>\n<p>Again, my primary goal is to\
    \ experiment, learn and have fun while <strong>developing</strong> something challenging.</p>\n\
    </li>\n<li>\n<p>I am in love with the unique 2D/3D graphic style of the game.</p>\n\
    </li>\n<li>\n<p>Ragnarok Online game asset file structures are known and there\
    \ are example implementations for processing them.</p>\n</li>\n<li>\n<p>Ragnarok\
    \ Online had a huge impact on me when I was younger. I played a lot with it, this\
    \ might have been the only game I was obsessed with.</p>\n<p>Thanks to it, I know\
    \ all the skills, sprites, maps, models etc, which is useful when I try to come\
    \ up with visualities of some new skill.</p>\n</li>\n<li>\n<p>The server code\
    \ of Ragnarok Online has been exposed for a very long time. That was the first\
    \ professional C source code I studied, hacked and even fixed when I was around\
    \ 14-15, so it had a huge impact on me as a software developer.</p>\n</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Thanks</h2>\n\
    <ul>\n<li>\n<a href=\"https://github.com/vthibault/roBrowser/\">roBrowser</a>:\
    \ Its source code was useful for decoding the game asset files</li>\n</ul>\n"
  stargazers_count: 225
  subscribers_count: 13
  topics:
  - rust
  - opengl
  - ragnarok
  - moba
  - game
  - multiplayer
  - 2d
  - 3d
  updated_at: 1623498993.0
bioexcel/biobb_structure_utils:
  data_format: 2
  description: Biobb_structure_utils is the Biobb module collection to modify or extract
    information from a PDB structure file, such as pulling out a particular model
    or chain, removing water molecules or ligands, or renumbering or sorting atoms
    or residues.
  filenames:
  - Singularity.latest
  full_name: bioexcel/biobb_structure_utils
  latest_release: null
  readme: '<p><a href="https://biobb-structure-utils.readthedocs.io/en/latest/?badge=latest"
    rel="nofollow"><img src="https://camo.githubusercontent.com/d7c1b5de86a2921c1f759b175820fb443eba3f18bbf45e56e42f2cee72844627/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f62696f62622d7374727563747572652d7574696c732f62616467652f3f76657273696f6e3d6c6174657374"
    alt="" data-canonical-src="https://readthedocs.org/projects/biobb-structure-utils/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="https://anaconda.org/bioconda/biobb_structure_utils" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a477fdb1fd9bc9eb7ffa6cae6a019c6d4c3902fd468b3126f1b78e56c7dcff83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e7376673f7374796c653d666c6174"
    alt="" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat"
    style="max-width:100%;"></a>

    <a href="https://quay.io/repository/biocontainers/biobb_structure_utils" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ca418e4db0b3de91a09a5df4a59446da015b6164598a8bc255918e911484f84f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d517561792e696f2d626c7565"
    alt="" data-canonical-src="https://img.shields.io/badge/docker-Quay.io-blue" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3836" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667"
    alt="" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-biobb_structure_utils" class="anchor" href="#biobb_structure_utils"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>biobb_structure_utils</h1>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>Biobb_structure_utils is the Biobb module collection to modify or extract information
    from a PDB structure file, such as pulling out a particular model or chain, removing
    water molecules or ligands, or renumbering or sorting atoms or residues. Biobb
    (BioExcel building blocks) packages are Python building blocks that create new
    layer of compatibility and interoperability over popular bioinformatics tools.
    The latest documentation of this package can be found in our readthedocs site:

    <a href="https://biobb-structure-utils.readthedocs.io/en/latest/" rel="nofollow">latest
    API documentation</a>.</p>

    <h3>

    <a id="user-content-version" class="anchor" href="#version" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version</h3>

    <p>v3.5.3 2021.2</p>

    <h3>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h3>

    <p>Using PIP:</p>

    <blockquote>

    <p><strong>Important:</strong> PIP only installs the package. All the dependencies
    must be installed separately. To perform a complete installation, please use ANACONDA,
    DOCKER or SINGULARITY.</p>

    </blockquote>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  pip install "biobb_structure_utils&gt;=3.5.3"

    </code></pre>

    </li>

    <li>

    <p>Usage: <a href="https://biobb-structure-utils.readthedocs.io/en/latest/modules.html"
    rel="nofollow">Python API documentation</a></p>

    </li>

    </ul>

    <p>Using ANACONDA:</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  conda install -c bioconda "biobb_structure_utils&gt;=3.5.3"

    </code></pre>

    </li>

    <li>

    <p>Usage: With conda installation BioBBs can be used with the <a href="https://biobb-structure-utils.readthedocs.io/en/latest/modules.html"
    rel="nofollow">Python API documentation</a> and the <a href="https://biobb-structure-utils.readthedocs.io/en/latest/command_line.html"
    rel="nofollow">Command Line documentation</a></p>

    </li>

    </ul>

    <p>Using DOCKER:</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  docker pull quay.io/biocontainers/biobb_structure_utils:3.5.3--py_0

    </code></pre>

    </li>

    <li>

    <p>Usage:</p>

    <pre><code>  docker run quay.io/biocontainers/biobb_structure_utils:3.5.3--py_0
    &lt;command&gt;

    </code></pre>

    </li>

    </ul>

    <p>Using SINGULARITY:</p>

    <p><strong>MacOS users</strong>: it''s strongly recommended to avoid Singularity
    and use <strong>Docker</strong> as containerization system.</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  singularity pull --name biobb_structure_utils.sif shub://bioexcel/biobb_structure_utils

    </code></pre>

    </li>

    <li>

    <p>Usage:</p>

    <pre><code>  singularity exec biobb_structure_utils.sif &lt;command&gt;

    </code></pre>

    </li>

    </ul>

    <p>The command list and specification can be found at the <a href="https://biobb-structure-utils.readthedocs.io/en/latest/command_line.html"
    rel="nofollow">Command Line documentation</a>.</p>

    <h3>

    <a id="user-content-copyright--licensing" class="anchor" href="#copyright--licensing"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Copyright
    &amp; Licensing</h3>

    <p>This software has been developed in the <a href="http://mmb.irbbarcelona.org"
    rel="nofollow">MMB group</a> at the <a href="http://www.bsc.es/" rel="nofollow">BSC</a>
    &amp; <a href="https://www.irbbarcelona.org/" rel="nofollow">IRB</a> for the <a
    href="http://bioexcel.eu/" rel="nofollow">European BioExcel</a>, funded by the
    European Commission (EU H2020 <a href="http://cordis.europa.eu/projects/823830"
    rel="nofollow">823830</a>, EU H2020 <a href="http://cordis.europa.eu/projects/675728"
    rel="nofollow">675728</a>).</p>

    <ul>

    <li>(c) 2015-2020 <a href="https://www.bsc.es/" rel="nofollow">Barcelona Supercomputing
    Center</a>

    </li>

    <li>(c) 2015-2020 <a href="https://www.irbbarcelona.org/" rel="nofollow">Institute
    for Research in Biomedicine</a>

    </li>

    </ul>

    <p>Licensed under the

    <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache License
    2.0</a>, see the file LICENSE for details.</p>

    <h3>

    <a id="user-content-acknolegements" class="anchor" href="#acknolegements" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Acknolegements</h3>

    <p>This software uses functions to read and modify GRO files based in the <a href="https://github.com/caizkun/gropy">GROPY</a>
    library created by Zhikun Cai (<a href="mailto:caizkun@gmail.com">caizkun@gmail.com</a>)
    under the <a href="https://github.com/caizkun/gropy/blob/master/LICENSE">MIT</a>.
    In this project <a href="https://github.com/caizkun/gropy">GROPY</a> has been
    adapted to Python 3 and our own needs.</p>

    <p><a href="https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67"
    alt="" title="Bioexcel" data-canonical-src="https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 8
  topics: []
  updated_at: 1623926777.0
bmichanderson/singularity-containers:
  data_format: 2
  description: A place to keep my Singularity recipes
  filenames:
  - Singularity.trinity
  - Singularity.unicycler
  - Singularity.igv
  - Singularity.faststructure
  - Singularity.kat
  - Singularity.quast
  - Singularity.gapfiller
  - Singularity.ipyrad
  - Singularity.stacks
  full_name: bmichanderson/singularity-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-containers" class="anchor" href="#singularity-containers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-containers</h1>

    <p>A place to keep my Singularity recipes.

    This repository contains recipes in the format "Singularity.[program]" and is
    linked to Singularity Hub so that all commits trigger builds there.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1624448140.0
bobeobibo/phigaro:
  data_format: 2
  description: Phigaro is a scalable command-line tool for predicting phages and prophages
  filenames:
  - Singularity
  full_name: bobeobibo/phigaro
  latest_release: v2.2.6
  readme: "<h1>\n<a id=\"user-content-phigaro-v230\" class=\"anchor\" href=\"#phigaro-v230\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Phigaro v2.3.0</h1>\n<p><a href=\"https://badge.fury.io/py/phigaro\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b9bcb6f0446a21e8f918a9a8253f32a15df7cc3df72ced3bcd42d9f23bbc993b/68747470733a2f2f62616467652e667572792e696f2f70792f7068696761726f2e737667\"\
    \ alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/phigaro.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/bfa41b5d9d74183c62a1e89d4718527f319c054e8090c55ce7837c88f19e5350/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f7068696761726f2f6261646765732f696e7374616c6c65722f636f6e64612e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/bfa41b5d9d74183c62a1e89d4718527f319c054e8090c55ce7837c88f19e5350/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f7068696761726f2f6261646765732f696e7374616c6c65722f636f6e64612e737667\"\
    \ alt=\"Conda installation\" data-canonical-src=\"https://anaconda.org/bioconda/phigaro/badges/installer/conda.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/bobeobibo/phigaro/workflows/Phigaro%20Tests/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/bobeobibo/phigaro/workflows/Phigaro%20Tests/badge.svg\"\
    \ alt=\"Actions Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5fab2edf3816ef9fb3ebcaf6e613fa7b40ff7652ec69e5f6e7f695aa24bf5ce6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5fab2edf3816ef9fb3ebcaf6e613fa7b40ff7652ec69e5f6e7f695aa24bf5ce6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\"\
    \ alt=\"License: MIT\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/psf/black\"><img\
    \ src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"\
    \ alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Phigaro is a standalone command-line\
    \ application that is able to detect prophage regions taking raw genome and metagenome\
    \ assemblies as an input. It also produces dynamic annotated \u201Cprophage genome\
    \ maps\u201D and marks possible transposon insertion spots inside prophages. It\
    \ is applicable for mining prophage regions from large metagenomic datasets.</p>\n\
    <h2>\n<a id=\"user-content-updates-tracker\" class=\"anchor\" href=\"#updates-tracker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Updates tracker</h2>\n<p>You can find the information about updates\
    \ and releases by <a href=\"https://github.com/bobeobibo/phigaro/blob/master/version_tracker.md\"\
    >link.</a></p>\n<h2>\n<a id=\"user-content-documentation-installation--usage\"\
    \ class=\"anchor\" href=\"#documentation-installation--usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Documentation:\
    \ Installation &amp; Usage</h2>\n<p>Please, follow <a href=\"https://phigaro.readthedocs.io/\"\
    \ rel=\"nofollow\">the documentation link</a> to find installation and usage information.</p>\n\
    <h2>\n<a id=\"user-content-methods-overview\" class=\"anchor\" href=\"#methods-overview\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Methods overview</h2>\n<p>Open-reading frames (i.e. proteins) are\
    \ predicted from the input FASTA file using Prodigal. Phage genes are annotated\
    \ with prokaryotic viral orthologous groups (pVOGs) profile Hidden Markov Models\
    \ (HMMs), which can be downloaded stand-alone from <a href=\"http://dmk-brain.ecn.uiowa.edu/pVOGs/\"\
    \ rel=\"nofollow\">http://dmk-brain.ecn.uiowa.edu/pVOGs/</a>. Each contig is represented\
    \ as a sequence of phage and non-phage genes. A smoothing window algorithm (a\
    \ triangular window function) determines regions with a high density of phage\
    \ genes and therefore the prophage regions and boundaries, considering the pVOG\
    \ annotations and the GC content.</p>\n<h2>\n<a id=\"user-content-known-issues\"\
    \ class=\"anchor\" href=\"#known-issues\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Known issues</h2>\n<p>Phigaro\
    \ is tested on Linux systems. For MacOS, you may need to add the following softlink\
    \ <code>ln -s /usr/libexec/locate.updatedb /usr/local/bin/updated</code> and run\
    \ <code>brew install wget</code>. If you encounter any issues while running Phigaro\
    \ on test data, please report them to us at <a href=\"mailto:estarikova@rcpcm.org\"\
    >estarikova@rcpcm.org</a>.</p>\n<h2>\n<a id=\"user-content-publication\" class=\"\
    anchor\" href=\"#publication\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Publication</h2>\n<p>Elizaveta V.\
    \ Starikova, Polina O. Tikhonova, Nikita A. Prianichnikov, Chris M. Rands, Evgeny\
    \ M. Zdobnov, Vadim M. Govorun <br>Phigaro: high throughput prophage sequence\
    \ annotation</p>\n<ul>\n<li>Bioinformatics, 2020; doi: <a href=\"https://doi.org/10.1093/bioinformatics/btaa250\"\
    \ rel=\"nofollow\">https://doi.org/10.1093/bioinformatics/btaa250</a>\n</li>\n\
    <li>bioRxiv, 2019; doi: <a href=\"https://doi.org/10.1101/598243\" rel=\"nofollow\"\
    >https://doi.org/10.1101/598243</a>\n</li>\n</ul>\n<p>(c) E.Starikova, P. Tikhonova,\
    \ N.Pryanichnikov, 2019</p>\n"
  stargazers_count: 25
  subscribers_count: 3
  topics:
  - bioinformatics
  - bioinformatics-tool
  - bioinformatics-pipeline
  - bioinformatics-analysis
  - biological-data-analysis
  - phage
  - phages
  - prophage
  - genomics
  - genomic-data-analysis
  - genomic-regions
  updated_at: 1622668825.0
buregab/openspiel_singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - other_images/Singularity.custom_openspiel
  full_name: buregab/openspiel_singularity
  latest_release: null
  readme: '<p>For building openspiel singularity containers.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604380357.0
cfe-lab/MiCall:
  data_format: 2
  description: Pipeline for processing FASTQ data from an Illumina MiSeq to genotype
    human RNA viruses like HIV and hepatitis C
  filenames:
  - Singularity
  full_name: cfe-lab/MiCall
  latest_release: v7.14.2
  readme: '<h1>

    <a id="user-content-micall" class="anchor" href="#micall" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MiCall</h1>

    <h2>

    <a id="user-content-processing-fastq-data-from-an-illumina-miseq" class="anchor"
    href="#processing-fastq-data-from-an-illumina-miseq" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Processing FASTQ data
    from an Illumina MiSeq</h2>

    <p><a href="https://travis-ci.com/cfe-lab/MiCall" rel="nofollow"><img src="https://camo.githubusercontent.com/8cfcb6fc58992b1a5293f99aab18dc7fc4c352993a0aaeee39aca2186b9e02b4/68747470733a2f2f7472617669732d63692e636f6d2f6366652d6c61622f4d6943616c6c2e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.com/cfe-lab/MiCall.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://codecov.io/github/cfe-lab/MiCall?branch=master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/01a5da8acb0a78aabfb093a63c28db15b9df951d3e52aaa03d54180dae171b07/68747470733a2f2f636f6465636f762e696f2f6769746875622f6366652d6c61622f4d6943616c6c2f636f7665726167652e7376673f6272616e63683d6d6173746572"
    alt="Code Coverage" data-canonical-src="https://codecov.io/github/cfe-lab/MiCall/coverage.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://doi.org/10.5281/zenodo.1289989" rel="nofollow"><img src="https://camo.githubusercontent.com/91063d68c07e035327f80f12cf9c389ffffd45952c4db811e6bf23fc2973b714/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313238393938392e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.1289989.svg"
    style="max-width:100%;"></a></p>

    <p>Maps all the reads from a sample against a set of reference sequences, then

    stitches all the reads into consensus sequences and coverage maps.</p>

    <p>A monitoring system regularly checks the file system for unprocessed runs,

    transfers FASTQ.gz files to the cluster and executes the pipeline.</p>

    <p>See the <a href="https://cfe-lab.github.io/MiCall/steps" rel="nofollow">list
    of steps and files</a> for details of what the pipeline does.

    The <a href="https://cfe-lab.github.io/MiCall/admin" rel="nofollow">admin</a>
    page describes how to look after the pipeline in Kive, and the

    <a href="https://cfe-lab.github.io/MiCall/getting_started" rel="nofollow">getting
    started</a> page describes how to get the docker version set up and run it

    on your own data.</p>

    <h2>

    <a id="user-content-dual-licensing" class="anchor" href="#dual-licensing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dual Licensing</h2>

    <p>Copyright (C) 2016, University of British Columbia</p>

    <p>This program is free software: you can redistribute it and/or modify

    it under the terms of the GNU Affero General Public License as published

    by the Free Software Foundation, either version 3 of the License, or

    (at your option) any later version.</p>

    <p>This program is distributed in the hope that it will be useful,

    but WITHOUT ANY WARRANTY; without even the implied warranty of

    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

    GNU Affero General Public License for more details.</p>

    <p>You should have received a copy of the GNU Affero General Public License

    along with this program.  If not, visit <a href="https://www.gnu.org/licenses/"
    rel="nofollow">gnu.org</a>. The source code for

    this program is available from <a href="https://github.com/cfe-lab/MiCall">github.com</a>.</p>

    <p>The program is also available for a fee under a more permissive license. For

    example, if you want to run a changed version of the program on a network server

    without publishing the changed source code, <a href="mailto:micalldev@cfenet.ubc.ca">contact
    us</a> about

    purchasing a license.</p>

    <h2>

    <a id="user-content-third-party-components" class="anchor" href="#third-party-components"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Third
    Party Components</h2>

    <p>MiCall makes use of several open-source tools. Here is a list of tools with

    their licenses.</p>

    <p>Requests is distributed under the Apache 2.0 license.</p>

    <p>Python 3 is distributed under the <a href="https://docs.python.org/3/license.html"
    rel="nofollow">Python 3 license</a>.</p>

    <p>Bowtie2, IVA, and Python-Levenshtein are distributed under the GNU General

    Public License (GPL).</p>

    <p>Matplotlib is distributed under the <a href="https://matplotlib.org/users/license.html"
    rel="nofollow">Matplotlib license</a>.</p>

    <p>Reportlab is distributed under the BSD license.</p>

    <p>Pyyaml and Cutadapt are distributed under the MIT license.</p>

    '
  stargazers_count: 8
  subscribers_count: 13
  topics:
  - bioinformatics
  - fastq
  - python
  - resistance
  - genotype
  updated_at: 1624465273.0
chaotic-aur/toolbox:
  data_format: 2
  description: Unified kit with all the scripts required for maintaining the repository.
  filenames:
  - guest/Singularity
  full_name: chaotic-aur/toolbox
  latest_release: null
  readme: '<h1>

    <a id="user-content-chaotic-aur" class="anchor" href="#chaotic-aur" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Chaotic AUR</h1>

    <h2>

    <a id="user-content-disclaimer" class="anchor" href="#disclaimer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Disclaimer</h2>

    <p>I receive questions like "Why didn''t you write it in language X? Why didn''t
    you use sudoers instead of <code>setuid</code>? Why don''t you guarantee reproducible
    builds? Why don''t you use submodules and review every PKGBUILD change?"</p>

    <p>Because I can''t, I am only one person, taking care of all the steps required
    for updating 3800 packages non-stop, treating all the differences between those
    PKGBUILDs and their sources, in an infra that runs in donated VMs which are not
    any similar.</p>

    <p>If at some point you see something that could be better, then please open a
    PR. And keep it simple, the more complex the codebase becomes, the more complicated
    will be in the future for one man alone to maintain it.</p>

    <h2>

    <a id="user-content-cli" class="anchor" href="#cli" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>CLI</h2>

    <ul>

    <li>

    <p><code>chaotic pr{,epare} ${INPUTDIR} $@</code></p>

    <p>It generates a building script to be later run in a containerized environment.

    <code>$INPUTDIR</code> is the name of directory in "$PWD" which contains a PKGBUILD.</p>

    </li>

    <li>

    <p><code>chaotic {lw,lowerstrap}</code></p>

    <p>It generates a lowerdir for later chrooting.</p>

    </li>

    <li>

    <p><code>chaotic {mk,makepkg} ${INPUTDIR} $@</code></p>

    <p>Builds the package in a container using systed-nspawn.

    <code>$INPUTDIR</code> is the result of a <code>prepare</code></p>

    </li>

    <li>

    <p><code>chaotic {mkd,makepwd} [${PACKAGES[@]}]</code></p>

    <p>Prepare and build all packages in the current directory.

    If <code>PACKAGES</code> are not provided then it will try to build all sub-directories.</p>

    </li>

    <li>

    <p><code>chaotic {si,iterfere-sync}</code></p>

    <p>Sync packages'' interference repo.</p>

    </li>

    <li>

    <p><code>chaotic {sp,package-lists-sync}</code></p>

    <p>Sync package list repo.</p>

    </li>

    <li>

    <p><code>chaotic {dp,deploy} ${INPUTDIR}</code></p>

    <p>Sign the package and send to primary node.</p>

    </li>

    <li>

    <p><code>chaotic {dbb,db-bump}</code></p>

    <p>Add recently deployed packages to the database, while moving replaced packages
    to archive.

    Uses <code>repoctl</code>.</p>

    </li>

    <li>

    <p><code>chaotic {rm,remove} ${PACKAGES[@]}</code></p>

    <p>Remove and archive all listed packages.

    Uses <code>repoctl</code>.</p>

    </li>

    <li>

    <p><code>chaotic {get,aur-download} [-r] ${PACKAGES[@]}</code></p>

    <p>Download listed packages'' sources from AUR.

    Uses <code>repoctl</code>.</p>

    </li>

    <li>

    <p><code>chaotic cl{,eanup} ${INPUTDIR}</code></p>

    <p>Safely deletes old package sources.</p>

    </li>

    <li>

    <p><code>chaotic help {syncthing,rsync}</code></p>

    <p>Instructions to the mirroring services.

    RSync is one-way (primary-&gt;cluster) only, and Syncthing both ways.</p>

    </li>

    <li>

    <p><code>chaotic routine {hourly,morning,afternoon,nightly,midnight}</code></p>

    <p>Run the specified routine.</p>

    </li>

    <li>

    <p><code>chaotic routine clean-archive</code></p>

    <p>When on a primary node, clean up the archive folder.</p>

    </li>

    <li>

    <p><code>chaotic {clg,clean-logs}</code></p>

    <p>After a <code>chaotic makepwd</code>, remove successfull and "already built"
    logs.</p>

    </li>

    <li>

    <p><code>chaotic {cls,clean-srccache} ${PACKAGE}</code></p>

    <p>Removes cached sources from a specific package.</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-involved-directories" class="anchor" href="#involved-directories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Involved
    directories</h2>

    <ul>

    <li>

    <p><code>/var/cache/chaotic/sources/${PACKAGETAG}</code></p>

    <p>Per-package <code>SRCDEST</code>.</p>

    </li>

    <li>

    <p><code>/var/cache/chaotic/lower/{latest,$DATESTAMP}</code></p>

    <p>Lowerdirs.</p>

    </li>

    <li>

    <p><code>/var/cache/chaotic/cc/{PACKAGETAG}</code></p>

    <p>Per-package <code>~/.ccache</code>.</p>

    </li>

    <li>

    <p><code>/var/cache/chaotic/packages</code></p>

    <p>Container-shared pacman''s cache.</p>

    </li>

    <li>

    <p><code>/var/lib/chaotic/interfere</code></p>

    <p>Cloned version of <a href="https://github.com/chaotic-aur/interfere">interfere
    repository</a></p>

    </li>

    </ul>

    <h1>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h1>

    <p><code>pacman -S --needed base-devel git arch-install-scripts repoctl fuse-overlayfs
    rsync python-telegram-send</code></p>

    <p>One needs an active mirror or a setting (in /etc/chaotic.conf) like this:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span>
    CAUR_URL=<span class="pl-s"><span class="pl-pds">''</span>https://builds.garudalinux.org/repos/chaotic-aur/x86_64<span
    class="pl-pds">''</span></span>

    <span class="pl-k">export</span> REPOCTL_CONFIG=<span class="pl-s"><span class="pl-pds">''</span>/etc/chaotic/repoctl.conf<span
    class="pl-pds">''</span></span>

    <span class="pl-k">export</span> CAUR_REPOCTL_DB_URL=<span class="pl-s"><span
    class="pl-pds">"</span><span class="pl-smi">${CAUR_URL}</span>/chaotic-aur.db.tar.zst<span
    class="pl-pds">"</span></span>

    <span class="pl-k">export</span> CAUR_REPOCTL_DB_FILE=<span class="pl-s"><span
    class="pl-pds">"</span>/tmp/chaotic/db.tar.zst<span class="pl-pds">"</span></span></pre></div>

    <p>To create a gpg key for the root user refer to this <a href="https://wiki.archlinux.org/index.php/GnuPG#Create_a_key_pair"
    rel="nofollow">ArchWiki article</a> for more information. If you find problems
    when using "sudo", read the "<a href="https://wiki.archlinux.org/index.php/GnuPG#su"
    rel="nofollow">su</a>" subsection.

    Then generate a ssh keypair for the root user.</p>

    <div class="highlight highlight-source-shell"><pre>sudo ssh-keygen</pre></div>

    <p>The ssh public key (cat /root/.ssh/id_rsa.pub) then needs to be added to the
    primary servers root authorized keys (/root/.ssh/authorized_keys). After that
    follow these <a href="https://wiki.archlinux.org/index.php/GnuPG#Export_your_public_key"
    rel="nofollow">instructions</a> to export the gpg public key. This key will have
    to be <a href="https://wiki.archlinux.org/index.php/GnuPG#Sending_keys" rel="nofollow">uploaded</a>
    to <a href="keyserver.ubuntu.com">keyserver.ubuntu.com</a> in order for the key
    to be verified.

    Then, configure it as follows in <code>/etc/chaotic.conf</code>, like this:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span>
    CAUR_DEPLOY_PKGS=<span class="pl-s"><span class="pl-pds">"</span>/var/www/chaotic-aur/x86_64<span
    class="pl-pds">"</span></span>

    <span class="pl-k">export</span> CAUR_URL=<span class="pl-s"><span class="pl-pds">"</span>http://localhost:8080/chaotic-aur/x86_64<span
    class="pl-pds">"</span></span>

    <span class="pl-k">export</span> CAUR_SIGN_KEY=<span class="pl-s"><span class="pl-pds">''</span>8A9E14A07010F7E3<span
    class="pl-pds">''</span></span>

    <span class="pl-k">export</span> CAUR_TYPE=<span class="pl-s"><span class="pl-pds">''</span>cluster<span
    class="pl-pds">''</span></span>

    <span class="pl-k">export</span> REPOCTL_CONFIG=<span class="pl-s"><span class="pl-pds">''</span>/etc/chaotic/repoctl.toml<span
    class="pl-pds">''</span></span></pre></div>

    <p>You''ll find more options in <code>src/chaotic</code> first lines.</p>

    <p>Supported <code>type</code> values are: <code>primary</code>, <code>cluster</code>,
    and <code>dev</code>.</p>

    <p>To have clean logs &amp; less bandwidth usage <code>/etc/pacman.conf</code>
    settings need to be adjusted:</p>

    <ul>

    <li>

    <p>Enable <code>NoProgressBar</code></p>

    </li>

    <li>

    <p>Use <code>Server = file:///path-to-local-repo</code> as repo link if a local
    mirror is available</p>

    </li>

    <li>

    <p>Don''t use <code>ILoveCandy</code></p>

    </li>

    </ul>

    <p>To deploy faster replace <code>openssh</code> with <code>openssh-hpn</code>
    on all nodes (adds peformance related <a href="https://www.psc.edu/research/networking/hpn-ssh/"
    rel="nofollow">patches</a>).</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>Install dependencies, then:</p>

    <pre><code>sudo groupadd chaotic_op

    sudo usermod -aG chaotic_op $(whoami)


    make build &amp;&amp; sudo make install

    </code></pre>

    <h2>

    <a id="user-content-lint" class="anchor" href="#lint" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Lint</h2>

    <div class="highlight highlight-source-shell"><pre>pacman -S --needed yarn shellcheck

    yarn install

    yarn run lint</pre></div>

    '
  stargazers_count: 25
  subscribers_count: 10
  topics:
  - archlinux
  - repository-management
  - automation
  - continuous-deployment
  - pkgbuild
  - aur
  updated_at: 1624558848.0
chenhongluo/horovord:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: chenhongluo/horovord
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ class=\"anchor\" href=\"#singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity container for molecular electrostatic calculations using\
    \ PDB2PQR/APBS and Brownian dynamics with BrownDye.</h1>\n<p>This singularity\
    \ image contains a complete software environment for running <a href=\"http://browndye.ucsd.edu/\"\
    \ rel=\"nofollow\">BrownDye (version 1 and 2)</a> simulations. It also includes\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">PDB2PQR</a> and\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">APBS</a>.</p>\n\
    <p>Please <a href=\"http://eepurl.com/by4eQr\" rel=\"nofollow\">register</a> your\
    \ use of APBS and PDB2PQR.</p>\n<p>The image has been verified to work on XSEDE\
    \ <a href=\"https://portal.xsede.org/sdsc-comet\" rel=\"nofollow\">comet</a> and\
    \ <a href=\"https://www.sdsc.edu/support/user_guides/tscc-quick-start.html\" rel=\"\
    nofollow\">TSCC</a> shared cluster at SDSC. It will automatically bind <code>/cvmfs</code>\
    \ <code>/oasis</code> <code>/projects</code> <code>/scratch</code> directories,\
    \ if available on the host.</p>\n<h2>\n<a id=\"user-content-using-the-container\"\
    \ class=\"anchor\" href=\"#using-the-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using the container</h2>\n<p>Pull\
    \ the singularity image:</p>\n<pre><code>singularity pull shub://nbcrrolls/electrostatics-singularity\n\
    </code></pre>\n<p>Start bash shell in the container:</p>\n<pre><code>singularity\
    \ shell nbcrrolls-electrostatics-singularity-master-latest.simg\n</code></pre>\n\
    <p>Now the container is running and we can start a BrownDye2 job (using the Thrombin\
    \ example):</p>\n<pre><code>module load browndye2\ncp -ai $BD2_PATH/examples/thrombin\
    \ .\ncd thrombin\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n_trajectories&gt; 10000\
    \ /&lt;n_trajectories&gt; 1000 /' t_m_simulation.xml.bak\nmake all # takes about\
    \ min to run\nmodule unload browndye2\n</code></pre>\n<p>And if you want to use\
    \ BrownDye version 1:</p>\n<pre><code>module load browndye1\ncp -ai $BD1_PATH/thrombin-example\
    \ .\ncd thrombin-example\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n-trajectories&gt;\
    \ 10000 /&lt;n-trajectories&gt; 1000 /' input.xml.bak # limit the number of calculated\
    \ trajectories\nmake all\nbd_top input.xml\nnam_simulation t-m-simulation.xml\
    \ # this takes about 3 min to run\ncat results.xml\nmodule unload browndye1\n\
    </code></pre>\n<p>After we are finished we can quit the container:</p>\n<pre><code>exit\n\
    </code></pre>\n<p>You can also access individual applications from the electrostatics\
    \ container.</p>\n<p>To list available applications:</p>\n<pre><code>$ singularity\
    \ apps nbcrrolls-electrostatics-singularity-master-latest.simg \napbs\npdb2pqr\n\
    nam_simulation\nwe_simulation\n</code></pre>\n<p>To run, for example, apbs calculation:</p>\n\
    <pre><code>singularity exec nbcrrolls-electrostatics-singularity-master-latest.simg\
    \ apbs input.in\n</code></pre>\n<p>or</p>\n<pre><code>singularity run --app apbs\
    \ nbcrrolls-electrostatics-singularity-master-latest.simg input.in\n</code></pre>\n\
    <p>This Singularity image is hosted on Singularity Hub: <a href=\"https://singularity-hub.org/collections/2497\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h6>\n<a id=\"user-content-this-project-is-supported-by-nbcr\"\
    \ class=\"anchor\" href=\"#this-project-is-supported-by-nbcr\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>This\
    \ project is supported by <a href=\"http://nbcr.ucsd.edu\" rel=\"nofollow\">NBCR</a>.</h6>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1553181936.0
ckrilow/dev-ckrilow:
  data_format: 2
  description: null
  filenames:
  - pipelines/0037-cell_cell_interaction/env/Singularity.cell_cell_interaction
  - pipelines/0025-qc_cluster/env/Singularity.sc_qc_cluster
  - pipelines/0015-preprocessing/env/Singularity.preprocessing
  full_name: ckrilow/dev-ckrilow
  latest_release: null
  readme: '<h1>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h1>

    <p>This README is pulled from a default template for workflows.</p>

    <h1>

    <a id="user-content-workflow-template-setup" class="anchor" href="#workflow-template-setup"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Workflow
    template setup</h1>

    <h2>

    <a id="user-content-lib" class="anchor" href="#lib" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>lib</h2>

    <ul>

    <li>The <code>lib</code> directory contains general libraries that may be referenced
    by multiple workflows, for instance cromwell configs and python configs. Currently
    nothing in this directory is used.</li>

    </ul>

    <h2>

    <a id="user-content-pipelines" class="anchor" href="#pipelines" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pipelines</h2>

    <ul>

    <li>

    <p>Each pipeline is a full analysis. Think of it like the heading of a methods
    section in a paper. For instance if this were genetic summary statistics workflow,
    a pipeline might be "fine-mapping" that does both conditional and credible set
    analysis. Another pipeline may be "colocalization".</p>

    </li>

    <li>

    <p>Pipelines may have numbers prior to their name (e.g., <code>example_pipeline_1</code>
    to <code>0025-example_pipeline_1</code>). These numbers do not mean anything,
    but merely used to keep pipelines in their general order of execution. These are
    optional.</p>

    </li>

    <li>

    <p>A pipeline consists of :</p>

    </li>

    </ul>

    <ol>

    <li>A workflow.</li>

    <li>A <code>scripts</code> directory with <em>all</em> scripts referenced by that
    workflow (unless a general lib script is called). Scripts may have numbers prior
    to their name. These numbers do not mean anything, but merely used to keep scripts
    in their general order of execution. These are optional.</li>

    <li>A <code>docs</code> directory that contains a documentation of the default
    parameters written in a style that is publishable as methods in a paper (including
    citations). Within the <code>docs</code> directory there may be a <code>reference</code>
    with any additional reference materials.</li>

    <li>An <code>example_runtime_setup</code> directory contains files that give an
    example of actual config files and any other files used to run the pipeline.</li>

    </ol>

    <h2>

    <a id="user-content-studies" class="anchor" href="#studies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>studies</h2>

    <ul>

    <li>A studies directory should either exist within the workflow repo or be a separate
    repo that has the same name as the workflow repo, but with <code>studies</code>
    appended to it (e.g. <code>template-workflow</code> becomes <code>template-workflow-studies</code>).</li>

    <li>If there is a standard set of plots that will always look the same way, a
    pipeline should generate such plots. Otherwise, all code to analyze the results
    of a pipeline run should be in the <code>studies</code> directory. For instance
    if this were genetic summary statistics workflow, <code>studies</code> may contain
    a <code>t2d</code> directory and a <code>weight</code> directory.</li>

    <li>Within a study is either an Jupyter notebook (either python or R kernel) or
    an R markdown file. Nearly all plots / analysis of the results of running the
    various pipelines should be done in the notebook / markdown file.</li>

    <li>A study may also contain a scripts directory with scripts to aggregate data
    for a one off analysis (if the analysis is going to be repeated, consider making
    a new pipeline or adding it to an existing pipeline) or for special plots that
    cannot be done in the notebook / markdown file.</li>

    </ul>

    <h1>

    <a id="user-content-new-workflow-reminders" class="anchor" href="#new-workflow-reminders"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>New
    workflow reminders</h1>

    <ul>

    <li>[ ] Documentation</li>

    <li>[ ] Environment version control</li>

    <li>[ ] Pipeline version control</li>

    <li>[ ] Git branches</li>

    <li>[ ] Code review</li>

    </ul>

    <h1>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h1>

    <p>Be sure to document your code!</p>

    <h1>

    <a id="user-content-environment-version-control" class="anchor" href="#environment-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment
    version control</h1>

    <p>Analysis environment is controlled using conda. Each pipeline should have an
    <code>environment.yml</code> file with all of the packages used. If a required
    package or library is missing from conda (and therefore not in the <code>environment.yml</code>),
    it should be noted in the <code>README.md</code> of the pipeline.</p>

    <div class="highlight highlight-source-shell"><pre>conda env <span class="pl-k">export</span>
    --no-builds <span class="pl-k">|</span> grep -v prefix <span class="pl-k">|</span>
    grep -v name <span class="pl-k">&gt;</span> environment.yml</pre></div>

    <h1>

    <a id="user-content-pipeline-version-control" class="anchor" href="#pipeline-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    version control</h1>

    <p>Each pipeline within this workflow uses <a href="https://pypi.org/project/bumpversion"
    rel="nofollow">bumpversion</a> for automatic <a href="https://semver.org" rel="nofollow">semantic
    versioning</a>.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    bump the appropriate increment</span>

    bumpversion patch --verbose --dry-run

    bumpversion minor --verbose --dry-run

    bumpversion major --verbose --dry-run


    <span class="pl-c"><span class="pl-c">#</span> commit with tags</span>

    git push --tags</pre></div>

    <h1>

    <a id="user-content-github-forks" class="anchor" href="#github-forks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GitHub forks</h1>

    <p>Forking the repository allows developers to work independently while retaining
    well-maintained code on the master fork. For instructions on how to fork, follow
    the <a href="https://help.github.com/en/articles/fork-a-repo">Fork a repo</a>
    instructions.</p>

    <p>After forking the repo, clone the repo to your local desktop:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    to use SSH</span>

    git clone git@github.com:<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git


    <span class="pl-c"><span class="pl-c">#</span> to use Https</span>

    git clone https://github.com/<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git</pre></div>

    <p>This creates a replica of the remote repository on your local desktop. <em>Note</em>:
    When you create your local repository, it will also make a local clone of the
    remote repository (typically as <code>origin</code>). So, your local master branch
    would simply be <code>master</code>. But, your remote master branch will be <code>origin/master</code>.
    You can also add multiple remote repositories. For instance, let us say our main
    repository is under the remote repository <code>my_repo</code>. We will want to
    add it as a remote repository, so we can fetch the most up-to-date code. You could
    add it by:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Add the my_repo remote repo to your local desktop -- this will allow you to pull
    and push to branches on the my_repo repository</span>

    git remote add my_repo git@github.com:my_repo/template-workflow.git</pre></div>

    <h1>

    <a id="user-content-git-branches" class="anchor" href="#git-branches" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Git branches</h1>

    <p>Branching is how git actually tracks code development. For more information,
    see the <a href="https://www.atlassian.com/git/tutorials/using-branches" rel="nofollow">Git
    Branch Tutorial</a> on Atlassian. If you want to add a new feature, pipeline,
    or fix a bug, a common work flow would look like this:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Update your local copy of the master branch to make sure you are getting the most
    up-to-date code</span>

    git pull


    <span class="pl-c"><span class="pl-c">#</span> Create the branch on your local
    machine and switch in this branch </span>

    git checkout -b [name_of_your_new_branch]


    <span class="pl-c"><span class="pl-c">#</span> Push the branch on github</span>

    git push origin [name_of_your_new_branch]</pre></div>

    <p>As you develop, you want to commit your work to your branch, so you don''t
    lose it all if something happens!</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Confirm we''re on the right branch</span>

    git branch -a


    <span class="pl-c"><span class="pl-c">#</span> Add all your work to be tracked
    (Note: there are many ways to add specific files, etc. See https://git-scm.com/docs/git-add
    for more information). The following command adds everything in your currently
    directory.</span>

    git add <span class="pl-c1">.</span>


    <span class="pl-c"><span class="pl-c">#</span> Commit your work to the branch
    with a message describing what''s in the commit</span>

    git commit -m <span class="pl-s"><span class="pl-pds">"</span>Created the scATAC-seq
    pipeline!<span class="pl-pds">"</span></span>


    <span class="pl-c"><span class="pl-c">#</span> You can add a -u parameter to set-upstream
    for a push</span>

    <span class="pl-c"><span class="pl-c">#</span> Alternatively, git will also automatically
    query you when you do your first push.</span>

    <span class="pl-c"><span class="pl-c">#</span> You can also set this manually
    by adding a new remote for your branch:</span>

    <span class="pl-c"><span class="pl-c">#</span>git remote add [name_of_your_remote]
    [name_of_your_new_branch]</span>


    <span class="pl-c"><span class="pl-c">#</span> Here is another push where we specify
    HEAD</span>

    <span class="pl-c"><span class="pl-c">#</span>git push origin HEAD # HEAD pushes
    everything up to the most recent commit</span></pre></div>

    <h1>

    <a id="user-content-code-review" class="anchor" href="#code-review" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Code review</h1>

    <p>Create a <a href="https://help.github.com/en/articles/creating-a-pull-request">GitHub
    Pull Request</a>. A PR allows other developers a chance to go through and comment
    on lines of code they believe can be improved. In addition, it will tell you if
    the code you are trying to merge into the <code>my_repo</code> branch actually
    conflicts with code that already exists in the branch, so you don''t overwrite
    someone else''s work.</p>

    <p>Once another developer approves the PR, you have the go-ahead to merge your
    code! Congrats, you finished your feature!</p>

    <p><em>Note</em>: There are some cases where you may just want to push directly
    to the my_repo fork, thereby avoiding code reviews. For instance, if you''re working
    on a one-off project that you want people to be able to see, but no one else is
    necessarily working on, you can always push directly to the branches on my_repo
    fork. Or, you could also still go through the steps of a PR, but simply merge
    your own code without CR.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1596817893.0
clemsonciti/singularity-images:
  data_format: 2
  description: Scripts for building Singularity images
  filenames:
  - caffe/ubuntu.def
  - tensorflow/ubuntu.def
  - circuitscape/ubuntu.def
  - mxnet/ubuntu.def
  - caffe2/ubuntu.def
  - dl/ubuntu.def
  full_name: clemsonciti/singularity-images
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-image-scripts" class="anchor" href="#singularity-image-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    image scripts</h1>

    <p>Scripts to generate singularity images

    for running different software on Palmetto cluster.</p>

    '
  stargazers_count: 10
  subscribers_count: 5
  topics: []
  updated_at: 1597386388.0
cmaumet/nipype_tutorial:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: cmaumet/nipype_tutorial
  latest_release: null
  readme: '<h1>

    <a id="user-content-nipype-tutorial-notebooks" class="anchor" href="#nipype-tutorial-notebooks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nipype
    Tutorial Notebooks</h1>

    <p><a href="https://circleci.com/gh/miykael/nipype_tutorial/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64"
    alt="CircleCi" data-canonical-src="https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/issues/"><img src="https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/pulls/"><img src="https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub pull-requests" data-canonical-src="https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/"><img
    src="https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/commits/master"><img src="https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub Commits" data-canonical-src="https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/archive/master.zip"><img src="https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub size" data-canonical-src="https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/miykael/nipype_tutorial/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000"
    style="max-width:100%;"></a>

    <a href="http://hits.dwyl.io/miykael/nipype_tutorial" rel="nofollow"><img src="https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub HitCount" data-canonical-src="http://hits.dwyl.io/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a></p>

    <p>This is the Nipype Tutorial in Jupyter Notebook format. You can access the
    tutorial in two ways:</p>

    <ol>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/" rel="nofollow">Nipype Tutorial
    Homepage</a>: This website contains a static, read-only version of all the notebooks.</li>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html"
    rel="nofollow">Nipype Tutorial Docker Image</a>: This guide explains how to use
    Docker to run the notebooks interactively on your own computer. The nipype tutorial
    docker image is the best interactive way to learn Nipype.</li>

    </ol>

    <h1>

    <a id="user-content-feedback-help--support" class="anchor" href="#feedback-help--support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feedback,
    Help &amp; Support</h1>

    <p>If you want to help with this tutorial or have any questions, feel free to
    fork the repo of the <a href="https://github.com/miykael/nipype_tutorial">Notebooks</a>
    or interact with other contributors on the slack channel <a href="https://brainhack.slack.com/messages/nipype/"
    rel="nofollow">brainhack.slack.com/messages/nipype/</a>. If you have any questions
    or found a problem, open a new <a href="https://github.com/miykael/nipype_tutorial/issues">issue
    on github</a>.</p>

    <h1>

    <a id="user-content-thanks-and-acknowledgment" class="anchor" href="#thanks-and-acknowledgment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thanks
    and Acknowledgment</h1>

    <p>A huge thanks to <a href="https://github.com/mwaskom">Michael Waskom</a>, <a
    href="https://github.com/oesteban">Oscar Esteban</a>, <a href="https://github.com/chrisfilo">Chris
    Gorgolewski</a> and <a href="https://github.com/satra">Satrajit Ghosh</a> for
    their input to this tutorial! And a huge thanks to <a href="https://github.com/djarecka/">Dorota
    Jarecka</a> who updated this tutorial to Python 3 and is helping me with keeping
    this tutorial updated and running!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1538064645.0
cokelaer/damona:
  data_format: 2
  description: 'singularity environment manager for NGS pipelines '
  filenames:
  - damona/recipes/salmon/Singularity.salmon_1.3.0
  - damona/recipes/conda/Singularity.conda_4.7.12
  - damona/recipes/conda/Singularity.conda_4.9.2
  - damona/recipes/minimap2/Singularity.minimap2_2.17.0
  - damona/recipes/rtools/Singularity.Rtools_1.0.0
  - damona/recipes/rtools/Singularity.Rtools_1.1.0
  - damona/recipes/art/Singularity.art_3.11.14
  - damona/recipes/damona/Singularity.damona_0.4.2
  - damona/recipes/damona/Singularity.damona_0.3.0
  - damona/recipes/falco/Singularity.falco_0.2.1
  - damona/recipes/rnaseqc/Singularity.rnaseqc_2.35.0
  - damona/recipes/trf/Singularity.trf_4.09
  - damona/recipes/trf/Singularity.trf_4.10.0
  - damona/recipes/rnadiff/Singularity.rnadiff_1.7.1
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.9.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.10.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.11.0
  - damona/recipes/R/Singularity.R_3.6.3
  - damona/recipes/R/Singularity.R_4.0.2
  - damona/recipes/kraken/Singularity.kraken_1.1
  - damona/recipes/kraken/Singularity.kraken_2.0.9
  - damona/recipes/fastqc/Singularity.fastqc_0.11.8
  - damona/recipes/fastqc/Singularity.fastqc_0.11.9
  - damona/recipes/gffread/Singularity.gffread_0.12.1
  - damona/recipes/bcl2fastq/Singularity.bcl2fastq_2.20.0
  - damona/recipes/prokka/Singularity.prokka_1.14.5
  - damona/recipes/sequana_perl_tools/Singularity.sequana_perl_tools_0.1.0
  - damona/recipes/ucsc/Singularity.ucsc_0.1.0
  - damona/recipes/fitter/Singularity.fitter_1.3.0
  - damona/recipes/phantompeakqualtools/Singularity.phantompeakqualtools_1.2.2
  - damona/recipes/canu/Singularity.canu_1.8.0
  - damona/recipes/canu/Singularity.canu_1.6.0
  - damona/recipes/graphviz/Singularity.graphviz_2.43.0
  full_name: cokelaer/damona
  latest_release: v0.4.3
  readme: '<h1>

    <a id="user-content-hpc-singularity" class="anchor" href="#hpc-singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>hpc-singularity</h1>

    <p>Singularity for HPC</p>

    <p>Make sure the sigularity is built on <a href="https://singularity-hub.org"
    rel="nofollow">https://singularity-hub.org</a></p>

    <p>if ready use:</p>

    <p><code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers3.0.2-imagecrawl</code></p>

    <p>Transformer 2.11.0:

    <code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers2.11.0-imagecrawl</code></p>

    <p>Make sure the imagecrawl is updated (latest commit)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1620052198.0
cokelaer/graphviz4all:
  data_format: 2
  description: dot and other graphviz executable in a simple singularity container
  filenames:
  - singularity/Singularity.v1
  full_name: cokelaer/graphviz4all
  latest_release: null
  readme: '<h1>

    <a id="user-content-graphviz4all" class="anchor" href="#graphviz4all" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>graphviz4all</h1>

    <p><strong>DEPRECATED, Aug 2020</strong>: This is now part of <a href="https://damona.readthedocs.io"
    rel="nofollow">https://damona.readthedocs.io</a> project.</p>

    <pre><code>damona install graphviz

    </code></pre>

    <p>A container with graphviz (<a href="http://www.graphviz.org/" rel="nofollow">http://www.graphviz.org/</a>)
    executables (dot, circo, etc).</p>

    <p>This is for Singularity 2.4 at least and is available on singularity-hub</p>

    <pre><code>singularity pull --name graphviz.img shub://cokelaer/graphviz4all:v1

    </code></pre>

    <p>Conversion of the dot file into SVG conterpart:</p>

    <pre><code>./graphviz.img dot -Tsvg test.dot -o test.svg

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - dot
  - circo
  - graphviz
  - singularity
  updated_at: 1597173467.0
cokelaer/pacbio4all:
  data_format: 2
  description: pacbio tools
  filenames:
  - singularity/Singularity.v2
  - singularity/Singularity.v3
  - singularity/Singularity.v1
  full_name: cokelaer/pacbio4all
  latest_release: null
  readme: '<h1>

    <a id="user-content-pacbio4all" class="anchor" href="#pacbio4all" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pacbio4all</h1>

    <p>A container with some of the pacbio tools. This is for Singularity 2.4 at least
    !</p>

    <p>::</p>

    <pre><code>singularity pull --name pacbio.img shub://cokelaer/pacbio4all:v2

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1508516491.0
cpezzato/discrete_active_inference:
  data_format: 2
  description: null
  filenames:
  - singularity_environment/Singularity
  full_name: cpezzato/discrete_active_inference
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-discrete_active_inference-for-robotics\" class=\"\
    anchor\" href=\"#discrete_active_inference-for-robotics\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>discrete_active_inference\
    \ for robotics</h1>\n<p>Repository for active inference and behavior trees for\
    \ discrete decision making. This repository relies on a TIAGo simulation in a\
    \ simplified retail store. Please read the associated paper for more theorethical\
    \ considerations about the algorithms.</p>\n<p><em>\"Active Inference and Behavior\
    \ Trees for Reactive Action Planning and Execution in Robotics\"</em></p>\n<p>Corrado\
    \ Pezzato, Carlos Hernandez, Stefan Bonhof, Martijn Wisse, <a href=\"https://arxiv.org/abs/2011.09756\"\
    \ rel=\"nofollow\">https://arxiv.org/abs/2011.09756</a></p>\n<h2>\n<a id=\"user-content-content\"\
    \ class=\"anchor\" href=\"#content\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Content</h2>\n<p>This repositiry\
    \ contains a Matlab examples and a ROS package for active inference for task planning\
    \ and execution.</p>\n<h3>\n<a id=\"user-content-main-files\" class=\"anchor\"\
    \ href=\"#main-files\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Main files</h3>\n<p><strong>Matlab:</strong></p>\n\
    <ul>\n<li>\n<em>aip.m</em> the active inference algorithm for decision making\
    \ is illustrated in the case of heterogeneous states and actions.</li>\n<li>\n\
    <em>example.m</em> example of use of active inference for discrete decision making\
    \ in a robotic case where conflicts and preconditions checks are required. A robot\
    \ is assumed to be able to navigate to a point (MoveBase), reach a location with\
    \ its end effector (Move MPC), and pick and place things. Actions have preconditions\
    \ and are assumed not instantaneous</li>\n</ul>\n<p><strong>ROS:</strong></p>\n\
    <p>The other folders are related to the ROS package containing a Python implementation\
    \ of active inference and behavior trees. You can run an example use case with\
    \ TIAGo in a simplified retail store after installation of the package ad dependancies.</p>\n\
    <h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<p><em><strong>Simulation Environment</strong></em></p>\n\
    <p>A singularity image can be downloaded from <a href=\"https://drive.google.com/drive/folders/1DYuRWgCiiHCG4ck_7Pf_Kw4Kn-ZpZ-Oy?usp=sharing\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>Alternatively, you can build the singularity\
    \ yourself:</p>\n<ol>\n<li>\n<p>create a sub directory called 'pkgs' (in the <code>singularity_environment</code>\
    \ directory)</p>\n<div class=\"highlight highlight-source-shell\"><pre>   mkdir\
    \ pkgs</pre></div>\n</li>\n<li>\n<p>use <code>vcstool</code> (or <code>wstool</code>)\
    \ to clone/download the dependencies (as specified in <code>retail_store_lightweight_sim.repos</code>).</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>   vcs import <span class=\"\
    pl-k\">&lt;</span> retail_store_lightweight_sim.repos pkgs</pre></div>\n<p>Adding\
    \ packages to <code>pkg</code> will allow <code>rosdep</code> to install all required\
    \ build and run dependencies into the image, so students can then proceed to build\
    \ those packages in their own workspaces (otherwise builds would fail due to missing\
    \ dependencies).</p>\n<p><strong>Note</strong>  Packages in <code>pkg</code> will\
    \ be installed on the image, their source will <strong>not</strong> be included\
    \ in the image itself, so there may be some elements that are not installed. So\
    \ far I've only noticed one required change.</p>\n</li>\n<li>\n<p>Modify the <code>CMakeList.txt</code>\
    \ file from the <code>pal_navigation_sm</code> inside the <code>pkgs</code> folder.</p>\n\
    <p>Change the <code>install</code> instruction (starts at line 10) by adding some\
    \ scripts as follows.</p>\n<div class=\"highlight highlight-source-shell\"><pre>install(\n\
    PROGRAMS\n   scripts/map_setup.py\n   scripts/pal_navigation_main_sm.py\n   scripts/navigation.sh\n\
    \   scripts/base_maps_symlink.sh\n   scripts/cp_maps_to_home.sh\n   scripts/cp_pose_to_home.sh\n\
    \   DESTINATION <span class=\"pl-smi\">${CATKIN_PACKAGE_BIN_DESTINATION}</span>)</pre></div>\n\
    </li>\n<li>\n<p>check the <code>VERSION</code> variable inside the <code>docker_build.sh</code>,\
    \ <code>build.sh</code> and <code>Singularity</code> files. This version should\
    \ match the version of your singularity install (<code>singularity -v</code>)</p>\n\
    </li>\n<li>\n<p>run <code>docker_build.sh</code></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>   ./docker_build.sh</pre></div>\n<p>After some time and a successful build,\
    \ a new docker image will be created. This requires Docker to be installed and\
    \ configured.</p>\n</li>\n<li>\n<p>run <code>build.sh</code></p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>   ./build.sh</pre></div>\n</li>\n</ol>\n\
    <p>After some time and a successful build, a new <code>.simg</code> should be\
    \ generated by <code>singularity</code> in the <code>cwd</code>.</p>\n<p><em><strong>Behavior\
    \ trees library</strong></em></p>\n<p>Install the BT library to use this package\
    \ (tested in Ubuntu 18.04 with ROS Melodic). Before proceeding, it is recommended\
    \ to to install the following dependencies:</p>\n<pre><code>sudo apt-get install\
    \ libzmq3-dev libboost-dev\n</code></pre>\n<p>You can also easily install the\
    \ <a href=\"https://github.com/BehaviorTree/BehaviorTree.CPP\">Behavior Tree library</a>\
    \ with the command</p>\n<pre><code>sudo apt-get install ros-$ROS_DISTRO-behaviortree-cpp-v3\n\
    sudo apt-get update   \n</code></pre>\n<h2>\n<a id=\"user-content-running-the-code\"\
    \ class=\"anchor\" href=\"#running-the-code\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running the code</h2>\n<p><em><strong>Using\
    \ the virtual environment</strong></em></p>\n<p>Access the simngularity image\
    \ by using the regular Singularity <code>shell</code> action:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity shell /path/to/discrete_ai_tiago.simg</pre></div>\n\
    <p>Use the flag for nvidia drivers if applicable to your machine:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity shell --nv /path/to/discrete_ai_tiago.simg</pre></div>\n\
    <p>Then source <code>/opt/ros/melodic/setup.bash</code> to access all the TIAGo\
    \ dependencies installed on the image.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">source</span> /opt/ros/melodic/setup.bash</pre></div>\n\
    <p><em><strong>How to run a simple example with TIAGo</strong></em></p>\n<p>Create\
    \ a new workspace and clone this repository in the <code>src</code> folder. Build\
    \ the package using <code>catkin build</code>. Run the three commands below from\
    \ within the singularity image after sourcing <code>source/devel/setup.bash</code>.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>roslaunch retail_store_simulation\
    \ tiago_simulation.launch\nrosrun discrete_ai tiago_perception.py\nrosrun discrete_ai\
    \ active_inference_server.py</pre></div>\n<p>From a terminal outside the singularity\
    \ image run the behavior tree:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>rosrun discrete_ai demo_executeBT</pre></div>\n<p>The expected outcome is\
    \ the following:</p>\n<p><a href=\"tiago_sim.gif\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"tiago_sim.gif\" alt=\"\" style=\"max-width:100%;\"></a></p>\n\
    <p><strong>Note</strong>: The sills used in this simulation are based on standard\
    \ moveBase and moveIt actions, thus robustness (especially of IK solutions) might\
    \ make TIAGo fail the grasp. Aruco detection can also imprecise and will be improved\
    \ over time.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623232591.0
cschu/dada2_container:
  data_format: 2
  description: null
  filenames:
  - Singularity.latest
  full_name: cschu/dada2_container
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-create-study-specific-roi-image-in-mni-space\"\
    \ class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Create\
    \ study-specific ROI image in MNI space</h1>\n<p>PMAT resting state connectivity\
    \ study.</p>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Inputs:</h2>\n<p>All should be matched to the same T1 image.</p>\n\
    <ul>\n<li>T1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)</li>\n\
    <li>Deformation from T1 subject space to atlas space (typically DEF_FWD resource\
    \ of cat12 assessor)</li>\n<li>SUBJECT directory of Freesurfer output (typically\
    \ SUBJECT resource of freesurfer_dev assessor)</li>\n<li>Temporal lobe segmentation\
    \ (typically SEG resource of Temporal_Lobe assessor)</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n\
    <pre><code>rois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv\
    \        Region labels and volumes\nmakerois-PMAT.pdf           Visual report\
    \ of final ROI image\n</code></pre>\n<h2>\n<a id=\"user-content-regions-of-interest\"\
    \ class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Regions of interest</h2>\n<h3>\n\
    <a id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spheres (atlas space)</h3>\n<p>Source: <em>Libby LA, Ekstrom AD, Ragland\
    \ JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal\
    \ cortices within human hippocampal subregions revealed by high-resolution functional\
    \ imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012</em></p>\n\
    <p>Method: <em>Schr\xF6der TN, Haak K V., Jimenez NIZ, et al. Functional topography\
    \ of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"\
    #entorhinal-cortex-atlas-space\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Entorhinal cortex (atlas space)</h3>\n\
    <p>Anterior lateral and posterior medial sections. Source and method: <em>Schr\xF6\
    der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal\
    \ cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\"\
    \ href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Temporal lobe (Subject space,\
    \ warped)</h3>\n<p>Head for anterior hippocampus; body and tail combined for posterior\
    \ hippocampus. Method: <em>Plassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale\
    \ Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE\
    \ Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017\
    \ Feb 24. PMID: 28781411; PMCID: PMC5544133.</em></p>\n<h3>\n<a id=\"user-content-parahippocampal-perirhinal-subject-space-warped\"\
    \ class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parahippocampal,\
    \ perirhinal (Subject space, warped)</h3>\n<p>Generated by Freesurfer 6. Parahippocampal\
    \ (1016,2016) and perirhinal (surface patch resampled to volume, overlap with\
    \ parahippocampus was assigned to perirhinal). Method: <em>Bruce Fischl, Andre\
    \ van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H.\
    \ Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne\
    \ Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating\
    \ the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.</em></p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1618484418.0
csf-ngs/hinkskalle-api:
  data_format: 2
  description: Talking to Hinkskalle
  filenames:
  - Singularity
  full_name: csf-ngs/hinkskalle-api
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hinkskalle-api\" class=\"anchor\" href=\"#hinkskalle-api\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Hinkskalle API</h1>\n<p>Talking to <a href=\"https://github.com/csf-ngs/hinkskalle\"\
    >Hinkskalle</a> made easy</p>\n<p>Use me to</p>\n<ul>\n<li>list available downloads</li>\n\
    <li>download data</li>\n<li>upload data</li>\n</ul>\n<h2>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h2>\n<p>hinkskalle-api\
    \ provides</p>\n<ul>\n<li>a small library with a thin wrapper over the JSON API</li>\n\
    <li>a CLI (<code>hinkli</code>: short for hink-cli, get it?)</li>\n</ul>\n<h3>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h3>\n\
    <p>You will need python3 and pip. Then you can run:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>pip3 install git+https://github.com/csf-ngs/hinkskalle-api</pre></div>\n\
    <h3>\n<a id=\"user-content-command-line-interface\" class=\"anchor\" href=\"#command-line-interface\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Command Line Interface</h3>\n<p>Get a list of available commands and\
    \ options:</p>\n<div class=\"highlight highlight-source-shell\"><pre>hinkli --help</pre></div>\n\
    <p>Your first step should be logging in:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> non-VBCF.NGS users get\
    \ your own instance!</span>\nhinkli --base https://singularity.ngs.vbcf.ac.at/\
    \ login\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> answer prompt for\
    \ username and password</span></pre></div>\n<p>The registry and token should now\
    \ be stored in <code>~/.hink_api.yml</code> and available for further use.</p>\n\
    <h4>\n<a id=\"user-content-discovering--downloading-data\" class=\"anchor\" href=\"\
    #discovering--downloading-data\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Discovering &amp; Downloading Data</h4>\n\
    <p>Your most likely use case will be downloading data provided via Hinkskalle.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> shoows available collections of containers</span>\n\
    hinkli list-collections\nhinkli list-containers [collection]\nhinkli list-downloads\
    \ [collection]/[container]\nhinkli pull [collection]/[container]:[tag]\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> username is optional, but can be\
    \ provided, too:</span>\nhinkli list-collections test.hase\nhinkli list-containers\
    \ test.hase/[collection]\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> etc</span></pre></div>\n\
    <p>Basic structure:</p>\n<ul>\n<li>A Collection holds a bunch of containers (topic,\
    \ type, ...)</li>\n<li>Containers hold tagged data</li>\n<li>Each tag points to\
    \ some data (some tags point to the same data)</li>\n</ul>\n<p>If Hinkskalle shows\
    \ you these downloads in your container <code>test.hase/example/FAQ4711</code>:</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre>- <span class=\"pl-ent\">filename</span>:\
    \ <span class=\"pl-s\">bunch_of_reads.fastq.gz</span>\n  <span class=\"pl-ent\"\
    >size</span>: <span class=\"pl-s\">41.5 MB</span>\n  <span class=\"pl-ent\">tags</span>:\
    \ <span class=\"pl-s\">basecalled,20210621</span>\n- <span class=\"pl-ent\">filename</span>:\
    \ <span class=\"pl-s\">rawdata.tar.gz</span>\n  <span class=\"pl-ent\">size</span>:\
    \ <span class=\"pl-s\">41.5 TB</span>\n  <span class=\"pl-ent\">tags</span>: <span\
    \ class=\"pl-s\">raw</span></pre></div>\n<p>You can use these commands to download:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> either one fetches bunch_of_reads.fastq</span>\nhinkli\
    \ pull example/FAQ4711:basecalled\nhinkli pull example/FAQ4711:20210621\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> fetches rawdata.tar.gz</span>\n\
    hinkli pull example/FAQ4711:raw</pre></div>\n<p>Hinkli will even check the sha256\
    \ checksum for you!</p>\n<h3>\n<a id=\"user-content-api\" class=\"anchor\" href=\"\
    #api\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>API</h3>\n<p>Not documented - use at your own risk!</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span\
    \ class=\"pl-s1\">hinkskalle_api</span> <span class=\"pl-k\">import</span> <span\
    \ class=\"pl-v\">HinkApi</span>\n<span class=\"pl-s1\">api</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-v\">HinkApi</span>()\n<span class=\"pl-s1\"\
    >collections</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">api</span>.<span\
    \ class=\"pl-en\">list_collections</span>()\n<span class=\"pl-c\"># etc</span></pre></div>\n\
    <h2>\n<a id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Configuration</h2>\n<p>By default, hinkli reads its config from <code>~/.hink_api.yml</code>.\
    \ This file should look like this:</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre><span class=\"pl-ent\">hink_api_base</span>: <span class=\"pl-s\">https://singularity.ngs.vbcf.ac.at</span>\n\
    <span class=\"pl-ent\">hink_api_key</span>: <span class=\"pl-s\">your_super_secret_token</span></pre></div>\n\
    <p>You can use these env variables to override:</p>\n<ul>\n<li><code>HINK_API_BASE</code></li>\n\
    <li><code>HINK_API_KEY</code></li>\n<li>\n<code>HINK_API_CFG</code> - to look\
    \ for the config file in a different location</li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1624655216.0
csiro-crop-informatics/nextflow-embl-abr-webinar:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: csiro-crop-informatics/nextflow-embl-abr-webinar
  latest_release: v1.2
  readme: "<p>This repository contains information for the EMBL-ABR webinar on \"\
    Nextflow: Scalable, Sharable and Reproducible Computational Workflows across Clouds\
    \ and Clusters\" presented by Rad Suchecki on 14th March 2019.</p>\n<h1>\n<a id=\"\
    user-content-webinar-details\" class=\"anchor\" href=\"#webinar-details\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Webinar\
    \ details</h1>\n<p><strong>Abstract:</strong>\nLarge analysis workflows are fragile\
    \ ecosystems of software tools, scripts and dependencies. This complexity commonly\
    \ makes these workflows not only irreproducible but sometimes even not re-runnable\
    \ outside their original development environment. Nextflow is a reactive workflow\
    \ framework and a domain specific programming language which follows the dataflow\
    \ paradigm and offers an alternative, and arguably superior, approach to developing,\
    \ executing and sharing pipelines.</p>\n<p>In this webinar we will follow the\
    \ steps required for developing sharable, version controlled, container-backed\
    \ workflows, which can be seamlessly executed across different environments from\
    \ a laptop to cluster to cloud. We will do this by leveraging Nextflow\u2019s\
    \ integration with code and container image hosting services such as GitHub and\
    \ Docker Hub, and out of the box support for various HPC cluster schedulers and\
    \ the Amazon AWS cloud.</p>\n<p><strong>Date/time:</strong> Thursday 14 March\
    \ 2019 13:00-14:00 AEDT /12:00-13:00 AEST</p>\n<p><strong>Presenter:</strong>\
    \ <a href=\"https://orcid.org/0000-0003-4992-9497\" rel=\"nofollow\">Rad Suchecki</a>,\
    \ CSIRO Crop Bioinformatics and Data Science</p>\n<p><a href=\"https://twitter.com/bioinforad\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/059d6c1e6596889bce7982a4745bea213207aae7fa1cd8a3053ed1e6b3f5190f/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f62696f696e666f7261642e7376673f7374796c653d736f6369616c\"\
    \ alt=\"Twitter Follow\" data-canonical-src=\"https://img.shields.io/twitter/follow/bioinforad.svg?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>Registration:</strong> <del><a\
    \ href=\"https://attendee.gotowebinar.com/register/8408436403729692931\" rel=\"\
    nofollow\">https://attendee.gotowebinar.com/register/8408436403729692931</a></del></p>\n\
    <p><strong>Video link:</strong> <a href=\"https://www.youtube.com/channel/UC5WlFNBSfmt3e8Js8o2fFqQ\"\
    \ rel=\"nofollow\">EMBL-ABR YouTube Channel</a></p>\n<p><a href=\"http://www.youtube.com/watch?v=lqm-VV5dOgk\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/72039949253c34d78d283c828b267bdbec41479ad5b39928f41704a9182e4f5c/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6c716d2d565635644f676b2f687164656661756c742e6a7067\"\
    \ alt=\"Nextflow Webinar Video\" data-canonical-src=\"http://img.youtube.com/vi/lqm-VV5dOgk/hqdefault.jpg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>Slides</strong></p>\n<p><a href=\"\
    https://csiro-crop-informatics.github.io/nextflow-embl-abr-webinar/nextflow-embl-abr.html\"\
    \ rel=\"nofollow\">https://csiro-crop-informatics.github.io/nextflow-embl-abr-webinar/nextflow-embl-abr.html</a></p>\n\
    <h1>\n<a id=\"user-content-data-for-the-webinar\" class=\"anchor\" href=\"#data-for-the-webinar\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Data for the Webinar</h1>\n<p>For the purpose of demonstrating a Nextflow\
    \ workflow in reasonable time, we will use the dataset used in <a href=\"https://github.com/UofABioinformaticsHub/2019_EMBL-ABR_Snakemake_webinar#data-for-the-webinar\"\
    >this Snakemake webinar</a>.</p>\n<h1>\n<a id=\"user-content-tutorial\" class=\"\
    anchor\" href=\"#tutorial\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Tutorial</h1>\n<p><a href=\"nextflow-tutorial.md\"\
    >nextflow-tutorial.md</a></p>\n"
  stargazers_count: 5
  subscribers_count: 3
  topics: []
  updated_at: 1568158524.0
ctpelok77/fd-red-black-postipc2018:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ctpelok77/fd-red-black-postipc2018
  latest_release: null
  readme: '<p>Fast Downward is a domain-independent planning system.</p>

    <p>For documentation and contact information see <a href="http://www.fast-downward.org/"
    rel="nofollow">http://www.fast-downward.org/</a>.</p>

    <p>The following directories are not part of Fast Downward as covered by this

    license:</p>

    <ul>

    <li>./src/search/ext</li>

    </ul>

    <p>For the rest, the following license applies:</p>

    <pre><code>Fast Downward is free software: you can redistribute it and/or modify
    it under

    the terms of the GNU General Public License as published by the Free Software

    Foundation, either version 3 of the License, or (at your option) any later

    version.


    Fast Downward is distributed in the hope that it will be useful, but WITHOUT ANY

    WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
    A

    PARTICULAR PURPOSE. See the GNU General Public License for more details.


    You should have received a copy of the GNU General Public License along with

    this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.

    </code></pre>

    '
  stargazers_count: 3
  subscribers_count: 1
  topics: []
  updated_at: 1622797950.0
ctpelok77/kstar:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ctpelok77/kstar
  latest_release: null
  readme: '{"message":"API rate limit exceeded for installation ID 633759.","documentation_url":"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting"}'
  stargazers_count: 5
  subscribers_count: 1
  topics: []
  updated_at: 1617563735.0
czbiohub/demux:
  data_format: 2
  description: Demultiplex sequencing experiments with Nextflow
  filenames:
  - Singularity
  full_name: czbiohub/demux
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-coredemux" class="anchor" href="#nf-coredemux" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/demux</h1>

    <p><strong>Demultiplex sequencing experiments</strong></p>

    <p><a href="https://travis-ci.org/nf-core/demux" rel="nofollow"><img src="https://camo.githubusercontent.com/21a4d7a0262a3b096d2521e08bc4e18bf7340ea1b97ea36ca247a7d06ccd04b1/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f64656d75782e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/demux.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/demux" rel="nofollow"><img src="https://camo.githubusercontent.com/8b87efd0f3651289c43d7f37a2a1092964f35f8501ccbcbe7ef15bfc1b38ae67/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f64656d75782e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/demux.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/demux pipeline comes with documentation about the pipeline, found
    in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1562208393.0
czbiohub/nf-core-crisprvar:
  data_format: 2
  description: Run CRISPResso on genome editing experiments
  filenames:
  - Singularity
  full_name: czbiohub/nf-core-crisprvar
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corecrisprvar" class="anchor" href="#nf-corecrisprvar"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/crisprvar</h1>

    <p><strong>Run CRISPResso on genome editing experiments</strong></p>

    <p><a href="https://travis-ci.org/nf-core/crisprvar" rel="nofollow"><img src="https://camo.githubusercontent.com/8bfb8f49f69e465c023fb291cefc2ccbea97c9a7cc04377260fa6052d9370b28/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f6372697370727661722e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/crisprvar.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/crisprvar" rel="nofollow"><img src="https://camo.githubusercontent.com/5e5f9f1b9479b9d19c758902e0a06e81ab060fa6a9207a5e935aee26edc728ac/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f6372697370727661722e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/crisprvar.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/crisprvar pipeline comes with documentation about the pipeline,
    found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1579658690.0
czbiohub/nf-core-test:
  data_format: 2
  description: test of nf-core create
  filenames:
  - Singularity
  full_name: czbiohub/nf-core-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-coretest" class="anchor" href="#nf-coretest" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/test</h1>

    <p><strong>test of nf-core</strong></p>

    <p><a href="https://travis-ci.org/nf-core/test" rel="nofollow"><img src="https://camo.githubusercontent.com/5656ec3ca80ae8775904761dfc7b47e3357d325de15a8d013edd4a0093630611/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f746573742e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/test.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/test" rel="nofollow"><img src="https://camo.githubusercontent.com/8a74c7ad053a343b2d1b30e0ef0f86afe191999cfc823635773862aefd840fd2/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f746573742e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/test.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/test pipeline comes with documentation about the pipeline, found
    in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1554245021.0
czbiohub/nf-large-assembly:
  data_format: 2
  description: Nextflow workflow for assembling large, diploid, eukaryotic genomes
    (2 gigabases haploid size or bigger)
  filenames:
  - Singularity
  full_name: czbiohub/nf-large-assembly
  latest_release: null
  readme: '<h1>

    <a id="user-content-czbiohubnf-large-assembly" class="anchor" href="#czbiohubnf-large-assembly"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>czbiohub/nf-large-assembly</h1>

    <p><strong>Assemble large diploid eukaryotic genomes (2 gigabases haploid size
    or bigger)</strong></p>

    <p><a href="https://travis-ci.org/czbiohub/nf-large-assembly" rel="nofollow"><img
    src="https://camo.githubusercontent.com/8d428dc306e8c519b4952b8239ab3eace188860f1c5dfabe1a4059c42c067a1e/68747470733a2f2f7472617669732d63692e6f72672f637a62696f6875622f6e662d6c617267652d617373656d626c792e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/czbiohub/nf-large-assembly.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/nf-large-assembly" rel="nofollow"><img
    src="https://camo.githubusercontent.com/767f13dee3d8a1039b493b285b876f4ef216154825cb6401031b09e8d959b916/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f6e662d6c617267652d617373656d626c792e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/nf-large-assembly.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The czbiohub/nf-large-assembly pipeline comes with documentation about the
    pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1556036860.0
czbiohub/splicemotifs:
  data_format: 2
  description: Nextflow workflow for finding conserved motifs intersecting with splice
    junctions
  filenames:
  - Singularity
  full_name: czbiohub/splicemotifs
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corebedtools-intersect" class="anchor" href="#nf-corebedtools-intersect"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/bedtools-intersect</h1>

    <p><strong>Intersect lots of bed files with lots of other bed files</strong></p>

    <p><a href="https://travis-ci.org/nf-core/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/811368779316af4f70b4dd35fc2c24cebcc4dc194cd63234e130384ec38ac89f/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f626564746f6f6c732d696e746572736563742e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/bedtools-intersect.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ca7e06b0d2929a9cba14da1892e90c6d4673a695806cb07ea82e89a1cbecef92/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f626564746f6f6c732d696e746572736563742e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/bedtools-intersect.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/bedtools-intersect pipeline comes with documentation about the
    pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1564673719.0
d-w-moore/anjuta_via_singularity:
  data_format: 2
  description: launch the C++ IDE Anjuta from a Singularity container
  filenames:
  - Singularity
  full_name: d-w-moore/anjuta_via_singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-anjuta-ide-via-singularity" class="anchor" href="#anjuta-ide-via-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Anjuta
    IDE via Singularity</h1>

    <p>The container includes libraries for building and debugging  C++

    programs with GCC 9, with C++17 support and Boost libraries. C/Xlib

    applications are also supported.</p>

    <p>To build the container under Singularity ~2.5.1 :</p>

    <ul>

    <li>get <a href="http://sylabs.io" rel="nofollow">Singularity</a> . If you''re
    on Ubuntu/Debian,

    the <a href="https://neuro.debian.net" rel="nofollow">NeuroDebian</a> repo can
    offer the

    most up-to-date Singularity packages</li>

    <li>in a local copy of this repo, use the build command:</li>

    </ul>

    <pre><code>$ sudo singularity build anjuta.simg Singularity

    </code></pre>

    <ul>

    <li>The IDE can be lauched by running anjuta.simg as an executable</li>

    </ul>

    <pre><code>$ ./anjuta.simg

    </code></pre>

    <p>or via the singularity application</p>

    <pre><code>$ singularity run anjuta.simg

    </code></pre>

    <p>To alter an existing image:</p>

    <pre><code>$ sudo singularity build --sandbox anjuta anjuta.simg

    $ sudo singularity shell --writable anjuta

    Singularity&gt; apt update; apt install {custom-packages...}

    Singularity&gt; exit

    $ sudo singularity build anjuta_updated.simg anjuta

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1600000384.0
d-w-moore/new_d2c:
  data_format: 2
  description: null
  filenames:
  - store_pw/Singularity.4.2.5
  - store_pw/Singularity.pw_embed
  - store_pw/Singularity.python-4.2.5
  - store_pw/Singularity.base-4.2.5
  - store_pw/Singularity.pw_encrypt
  - docs/Singularity.3_0.debian9
  - os_recipes/Singularity.archive.debian
  - os_recipes/Singularity.4.2.5
  - os_recipes/Singularity.SuSE
  - os_recipes/Singularity.centos6
  - os_recipes/Singularity.base-4.2.5
  - os_recipes/Singularity.centos7
  - os_recipes/Singularity.usmirror.debian
  - os_recipes/Singularity.deboot.ubuntu
  full_name: d-w-moore/new_d2c
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-installing-and-running-slurm-on-ubuntu-16-or-18\"\
    \ class=\"anchor\" href=\"#installing-and-running-slurm-on-ubuntu-16-or-18\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ and Running SLURM on ubuntu 16 or 18</h1>\n<h2>\n<a id=\"user-content-install-slurm\"\
    \ class=\"anchor\" href=\"#install-slurm\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install SLURM</h2>\n<pre><code>sudo\
    \ apt install slurm-wlm\ngit clone http://github.com/d-w-moore/new_d2c\ncd new_d2c\n\
    perl process_slurm_template.pl  | sudo dd of=/etc/slurm-llnl/slurm.conf\nsudo\
    \ systemctl restart slurmctld slurmd\nsudo systemctl enable  slurmctld slurmd\n\
    </code></pre>\n<p>to test:</p>\n<ul>\n<li>sudo apt install bc</li>\n<li>locate\
    \ command file slurm_install_test.sh containing:</li>\n</ul>\n<pre><code>  #!/bin/bash\n\
    \  bc -l &lt;&lt;&lt;\"scale=4000;a(1)*4\"\n</code></pre>\n<ul>\n<li>run the above\
    \ mentioned test script using : <code>sbatch &lt;script&gt;</code>\n</li>\n<li>type:\
    \ <code>squeue</code> and note the job present (most likely running)</li>\n<li>when\
    \ it disappears from queue (<code>watch -n1 squeue</code>), look for <code>slurm-&lt;JOBNUM&gt;.out</code>\n\
    containing the job's output</li>\n</ul>\n<h2>\n<a id=\"user-content-datacompute-automated-setup---install-irods-hook-scripts-for-slurm-prolog--epilog\"\
    \ class=\"anchor\" href=\"#datacompute-automated-setup---install-irods-hook-scripts-for-slurm-prolog--epilog\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Data/Compute automated setup - install iRODS hook scripts for slurm\
    \ prolog / epilog</h2>\n<p>The following command will setup prolog and epilog\
    \ scripts to be run (pre- and post-,\nrespectively) for each job executed by SLURM:</p>\n\
    <pre><code>sudo ./slurm_hook_setup.sh\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1561308424.0
d-w-moore/singularity-icommands-4.2.1:
  data_format: 2
  description: for singularity biuld
  filenames:
  - Singularity
  full_name: d-w-moore/singularity-icommands-4.2.1
  latest_release: null
  readme: '<h2>

    <a id="user-content-news" class="anchor" href="#news" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>News</h2>

    <h3>

    <a id="user-content-2019-12-20" class="anchor" href="#2019-12-20" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><em><strong>2019-12-20</strong></em>:</h3>

    <ul>

    <li>cisTopic v3: Faster topic modelling based on WarpLDA (see vignettes for details).</li>

    <li>The function runModels() is deprecated. Use runCGSModels() for modelling based
    on Collapsed Gibbs Sampling (equivalent to runModels()), or runWarpLDAModels()
    (for modelling based on WarpLDA).</li>

    <li>Version 2 objects (with or without models) can be used and analyzed with version
    3.</li>

    </ul>

    <h1>

    <a id="user-content-cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    class="anchor" href="#cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cisTopic:
    Probabilistic modelling of cis-regulatory topics from single cell epigenomics
    data</h1>

    <p>cisTopic is an R-package to simultaneously identify cell states and cis-regulatory
    topics from single cell epigenomics data.</p>

    <h2>

    <a id="user-content-dependencies-for-r--35" class="anchor" href="#dependencies-for-r--35"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies
    (for R &lt; 3.5)</h2>

    <p>The following packages have to be installed manually before installing cisTopic:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/RcisTarget<span
    class="pl-pds">"</span></span>)

    <span class="pl-e">devtools</span><span class="pl-k">::</span>install_github(<span
    class="pl-s"><span class="pl-pds">"</span>aertslab/AUCell<span class="pl-pds">"</span></span>)</pre></div>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>For installing and loading cisTopic, run:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/cisTopic<span
    class="pl-pds">"</span></span>)

    library(<span class="pl-smi">cisTopic</span>)</pre></div>

    <h2>

    <a id="user-content-databases" class="anchor" href="#databases" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Databases</h2>

    <p>RcisTarget feather databases are available at <a href="https://resources.aertslab.org/cistarget/"
    rel="nofollow">https://resources.aertslab.org/cistarget/</a>.</p>

    <h2>

    <a id="user-content-tutorials" class="anchor" href="#tutorials" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tutorials</h2>

    <h3>

    <a id="user-content-version-2" class="anchor" href="#version-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 2</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/Runningwithmm10andhg38.html"
    rel="nofollow">Running GREAT and motif enrichment with the mm10 and hg38 genome
    assemblies</a>.</li>

    </ul>

    <h3>

    <a id="user-content-version-3" class="anchor" href="#version-3" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 3</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1527027070.0
d-w-moore/singularity-python-irodsclient:
  data_format: 2
  description: null
  filenames:
  - Singularity.prc-0_8_0
  full_name: d-w-moore/singularity-python-irodsclient
  latest_release: null
  readme: '<h2>

    <a id="user-content-news" class="anchor" href="#news" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>News</h2>

    <h3>

    <a id="user-content-2019-12-20" class="anchor" href="#2019-12-20" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><em><strong>2019-12-20</strong></em>:</h3>

    <ul>

    <li>cisTopic v3: Faster topic modelling based on WarpLDA (see vignettes for details).</li>

    <li>The function runModels() is deprecated. Use runCGSModels() for modelling based
    on Collapsed Gibbs Sampling (equivalent to runModels()), or runWarpLDAModels()
    (for modelling based on WarpLDA).</li>

    <li>Version 2 objects (with or without models) can be used and analyzed with version
    3.</li>

    </ul>

    <h1>

    <a id="user-content-cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    class="anchor" href="#cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cisTopic:
    Probabilistic modelling of cis-regulatory topics from single cell epigenomics
    data</h1>

    <p>cisTopic is an R-package to simultaneously identify cell states and cis-regulatory
    topics from single cell epigenomics data.</p>

    <h2>

    <a id="user-content-dependencies-for-r--35" class="anchor" href="#dependencies-for-r--35"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies
    (for R &lt; 3.5)</h2>

    <p>The following packages have to be installed manually before installing cisTopic:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/RcisTarget<span
    class="pl-pds">"</span></span>)

    <span class="pl-e">devtools</span><span class="pl-k">::</span>install_github(<span
    class="pl-s"><span class="pl-pds">"</span>aertslab/AUCell<span class="pl-pds">"</span></span>)</pre></div>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>For installing and loading cisTopic, run:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/cisTopic<span
    class="pl-pds">"</span></span>)

    library(<span class="pl-smi">cisTopic</span>)</pre></div>

    <h2>

    <a id="user-content-databases" class="anchor" href="#databases" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Databases</h2>

    <p>RcisTarget feather databases are available at <a href="https://resources.aertslab.org/cistarget/"
    rel="nofollow">https://resources.aertslab.org/cistarget/</a>.</p>

    <h2>

    <a id="user-content-tutorials" class="anchor" href="#tutorials" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tutorials</h2>

    <h3>

    <a id="user-content-version-2" class="anchor" href="#version-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 2</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/Runningwithmm10andhg38.html"
    rel="nofollow">Running GREAT and motif enrichment with the mm10 and hg38 genome
    assemblies</a>.</li>

    </ul>

    <h3>

    <a id="user-content-version-3" class="anchor" href="#version-3" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 3</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1530133683.0
d-w-moore/zipit:
  data_format: 2
  description: parallel gzipper in pure python
  filenames:
  - Singularity.alpine
  full_name: d-w-moore/zipit
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-zipit\" class=\"anchor\" href=\"#zipit\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>zipit</h1>\n\
    <p>This repo contains two scripts useful for gzipping and checking large files\n\
    as quickly as possible leveraging the parallelism of your machine.</p>\n<p>They\
    \ require only that python be installed, and they depend only on modules\nincluded\
    \ in the Python Standard Library -- particularly, of course, gzip.</p>\n<h2>\n\
    <a id=\"user-content-zipitpy\" class=\"anchor\" href=\"#zipitpy\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>zipit.py</h2>\n\
    <p>Example uses:</p>\n<pre><code> $ ./zipit.py -v large.tar    # =&gt; Creates\
    \ large.tar.gz at default level of parallelism.\n                            \
    \  #    (-v verbosely informs of the piece-wise gzip tasks)\n\n $ ./zipit.py -qm\
    \ large.tar   # =&gt; creates large.tar.gz using all available CPU's\n\n $ some_command\
    \ | ./zipit.py - &gt; out.gz   # =&gt; gzips from the stdin stream, onto stdout\n\
    \n $ docker export cimg | ./zipit.py \\      # =&gt; export and compress the filesystem\
    \ of\n      -d cimg.dig - &gt;cimg.tgz             #    a docker container\n</code></pre>\n\
    <h2>\n<a id=\"user-content-testzippy\" class=\"anchor\" href=\"#testzippy\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>testzip.py</h2>\n\
    <p>Example use (for context, see the final <code>zipit.py</code> example above):</p>\n\
    <pre><code> $ ./testzip.py cimg.tgz cimg.dig      # =&gt; tests the gzipped file's\
    \ integrity using a digest file\n                                       #    (returns\
    \ 0 if the integrity is good)\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1602285708.0
darachm/containers:
  data_format: 2
  description: metarepo for tidying up container recipes, currently Singularity
  filenames:
  - base/Singularity.base
  - jupyter/Singularity.jupyter-plus-tensorflow-v2.5.0-compiled
  - jupyter/Singularity.jupyter-plus-tensorflow-v2.4.0-rc4-compiled
  - jupyter/Singularity.jupyter-plus-tensorflow-v2.2.0-compiled
  - jupyter/Singularity.jupyter-plus-bioconda
  - jupyter/Singularity.jupyter-plus
  - jupyter/Singularity.jupyter
  - jupyter/Singularity.jupyter-plus-alignparse
  - tensorflow/Singularity.tensorflow-v2.0.3-compiled
  - tensorflow/Singularity.tensorflow-v1.15.4-compiled-partial
  - tensorflow/Singularity.tensorflow-v2.4.0-rc4-compiled
  - tensorflow/Singularity.tensorflow-v2.5.0-compiled
  - tensorflow/Singularity.tensorflow-v2.2.0-compiled
  - bioinfmunger/Singularity.bioinfmunger
  - shell/Singularity.shell-plus
  - lh3-aligners/Singularity.lh3-aligners
  - ubuntu/Singularity.ubuntu2004
  - pacbio/Singularity.pacbio
  - r/Singularity.r-plus
  - r/Singularity.r
  - bioconda/Singularity.bioconda
  - starcode/Singularity.starcode-v0.1.1
  full_name: darachm/containers
  latest_release: null
  readme: '<p>This is for tracking, hosting recipes for Singularity containers, such
    that

    it can get mirrored on Github and singularity-hub can get it.</p>

    <p>Organzation copied from <a href="https://github.com/jlboat/BioinfoContainers">jlboat</a>.

    (Of course, makes total sense to just use tags to organize things!)</p>

    <p>Some recipes are for individual tools, some are for workflows and so are

    combos.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624074888.0
davecwright3/bart-singularity:
  data_format: 2
  description: Bayesian Atmospheric Radiative Transfer (BART) packaged in a Singularity
    container https://github.com/davecwright3/bart-singularity
  filenames:
  - Singularity
  full_name: davecwright3/bart-singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4946" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-bart-singularity-guide" class="anchor" href="#bart-singularity-guide"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BART
    Singularity Guide</h1>

    <p>The Singularity image has BART installed at <code>/bart_dir</code>. The <code>$topdir</code>
    environment variable is set to this directory inside the image. This means that
    the instructions for the demo listed here <a href="https://github.com/exosports/BART/tree/master/examples/demo">https://github.com/exosports/BART/tree/master/examples/demo</a>
    still work, but we need to mount a directory for outputs into the container for
    two reasons:</p>

    <ol>

    <li>The demo expects your output directory to be parallel to the BART directory</li>

    <li>The container file system is read-only (this is only a problem because of
    (1); being read-only is actually preferred because it helps ensure reproducible
    results)</li>

    </ol>

    <ul>

    <li>If the output directory wasn''t required to be parallel to BART, you could
    run the container anywhere in <code>$HOME</code> because Singularity mounts <code>$HOME</code>
    of the current user into the container by default</li>

    </ul>

    <p>The image has a directory parallel to BART that is meant for output at <code>/bart_dir/run</code>.
    Make a directory on your host system where you want to store results. For the
    sake of this guide, let''s say it''s under your current directory at <code>demo/run</code>
    and you have pulled the singularity image</p>

    <pre><code>singularity pull --name bart.sif shub://davecwright3/bart-singularity

    </code></pre>

    <p>to your current directory as well. Then start a shell in the singularity container
    with the bind mount specified</p>

    <pre><code>singularity shell -B demo/run:/bart_dir/run bart.sif

    </code></pre>

    <p>The BART conda environment will be automatically activated. Now just <code>cd
    $topdir/run</code> and follow the instructions here <a href="https://github.com/exosports/BART/tree/master/examples/demo">https://github.com/exosports/BART/tree/master/examples/demo</a>
    if you would like to do a demo run. You can <code>exit</code> the container whenever
    you are done, and your results will remain in your <code>demo/run</code> directory.</p>

    <h1>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h1>

    <p>Bayesian Atmospheric Radiative Transfer (BART), a code to infer

    properties of planetary atmospheres based on observed spectroscopic

    information.</p>

    <p>This project was completed with the support of the NASA Planetary

    Atmospheres Program, grant NNX12AI69G, held by Principal Investigator

    Joseph Harrington. Principal developers included graduate students

    Patricio E. Cubillos and Jasmina Blecic, programmer Madison Stemm, and

    undergraduates M. Oliver Bowman and Andrew S. D. Foster.  The included

    ''transit'' radiative transfer code is based on an earlier program of

    the same name written by Patricio Rojo (Univ. de Chile, Santiago) when

    he was a graduate student at Cornell University under Joseph

    Harrington.  Statistical advice came from Thomas J. Loredo and Nate

    B. Lust.</p>

    <p>Copyright (C) 2015-2016 University of Central Florida.

    All rights reserved.</p>

    <p>This is a test version only, and may not be redistributed to any third

    party.  Please refer such requests to us.  This program is distributed

    in the hope that it will be useful, but WITHOUT ANY WARRANTY; without

    even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR

    PURPOSE.</p>

    <p>Our intent is to release this software under an open-source,

    reproducible-research license, once the code is mature and the first

    research paper describing the code has been accepted for publication

    in a peer-reviewed journal.  We are committed to development in the

    open, and have posted this code on github.com so that others can test

    it and give us feedback.  However, until its first publication and

    first stable release, we do not permit others to redistribute the code

    in either original or modified form, nor to publish work based in

    whole or in part on the output of this code.  By downloading, running,

    or modifying this code, you agree to these conditions.  We do

    encourage sharing any modifications with us and discussing them

    openly.</p>

    <p>We welcome your feedback, but do not guarantee support.  Please send

    feedback or inquiries to:

    Patricio Cubillos <a href="mailto:patricio.cubillos@oeaw.ac.at">patricio.cubillos@oeaw.ac.at</a>

    Jasmina Blecic <a href="mailto:jasmina@physics.ucf.edu">jasmina@physics.ucf.edu</a>

    Joseph Harrington <a href="mailto:jh@physics.ucf.edu">jh@physics.ucf.edu</a></p>

    <p>or alternatively,

    Joseph Harrington, Patricio Cubillos, and Jasmina Blecic

    UCF PSB 441

    4111 Libra Drive

    Orlando, FL 32816-2385

    USA</p>

    <p>Thank you for testing BART!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604965509.0
daviesdrew/variantcalling:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity_1.0.0
  full_name: daviesdrew/variantcalling
  latest_release: null
  readme: '<h1>

    <a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon
    octicon-link"></span></a><a href="docs/images/nf-core-illuminavariantcalling_logo.png"
    target="_blank" rel="noopener noreferrer"><img src="docs/images/nf-core-illuminavariantcalling_logo.png"
    alt="nf-core/illuminavariantcalling" style="max-width:100%;"></a>

    </h1>

    <p><strong>Illumina paired end reads variant calling pipeline</strong>.</p>

    <p><a href="https://github.com/nf-core/illuminavariantcalling/actions"><img src="https://github.com/nf-core/illuminavariantcalling/workflows/nf-core%20CI/badge.svg"
    alt="GitHub Actions CI Status" style="max-width:100%;"></a>

    <a href="https://github.com/nf-core/illuminavariantcalling/actions"><img src="https://github.com/nf-core/illuminavariantcalling/workflows/nf-core%20linting/badge.svg"
    alt="GitHub Actions Linting Status" style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/illuminavariantcalling" rel="nofollow"><img
    src="https://camo.githubusercontent.com/609e7a6579baf2276f34ef713d9cc0b55f7fd62e2c5c7618d40423779d41fd44/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f696c6c756d696e6176617269616e7463616c6c696e672e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/illuminavariantcalling.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker containers making installation trivial and
    results highly reproducible.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>i. Install <a href="https://nf-co.re/usage/installation" rel="nofollow"><code>nextflow</code></a></p>

    <p>ii. Install either <a href="https://docs.docker.com/engine/installation/" rel="nofollow"><code>Docker</code></a>
    or <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow"><code>Singularity</code></a>
    for full pipeline reproducibility (please only use <a href="https://conda.io/miniconda.html"
    rel="nofollow"><code>Conda</code></a> as a last resort; see <a href="https://nf-co.re/usage/configuration#basic-configuration-profiles"
    rel="nofollow">docs</a>)</p>

    <p>iii. Download the pipeline and test it on a minimal dataset with a single command</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run nf-core/illuminavariantcalling
    -profile test,<span class="pl-k">&lt;</span>docker/singularity/conda/institute<span
    class="pl-k">&gt;</span></pre></div>

    <blockquote>

    <p>Please check <a href="https://github.com/nf-core/configs#documentation">nf-core/configs</a>
    to see if a custom config file to run nf-core pipelines already exists for your
    Institute. If so, you can simply use <code>-profile &lt;institute&gt;</code> in
    your command. This will enable either <code>docker</code> or <code>singularity</code>
    and set the appropriate execution settings for your local compute environment.</p>

    </blockquote>

    <p>iv. Start running your own analysis!</p>


    <div class="highlight highlight-source-shell"><pre>nextflow run nf-core/illuminavariantcalling
    -profile <span class="pl-k">&lt;</span>docker/singularity/conda/institute<span
    class="pl-k">&gt;</span> --reads <span class="pl-s"><span class="pl-pds">''</span>*_R{1,2}.fastq.gz<span
    class="pl-pds">''</span></span> --genome GRCh37</pre></div>

    <p>See <a href="docs/usage.md">usage docs</a> for all of the available options
    when running the pipeline.</p>

    <h2>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>The nf-core/illuminavariantcalling pipeline comes with documentation about
    the pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="https://nf-co.re/usage/installation" rel="nofollow">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="https://nf-co.re/usage/local_installation" rel="nofollow">Local installation</a></li>

    <li><a href="https://nf-co.re/usage/adding_own_config" rel="nofollow">Adding your
    own system config</a></li>

    <li><a href="https://nf-co.re/usage/reference_genomes" rel="nofollow">Reference
    genomes</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="https://nf-co.re/usage/troubleshooting" rel="nofollow">Troubleshooting</a></li>

    </ol>


    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>nf-core/illuminavariantcalling was originally written by Drew Davies.</p>

    <h2>

    <a id="user-content-contributions-and-support" class="anchor" href="#contributions-and-support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributions
    and Support</h2>

    <p>If you would like to contribute to this pipeline, please see the <a href=".github/CONTRIBUTING.md">contributing
    guidelines</a>.</p>

    <p>For further information or help, don''t hesitate to get in touch on <a href="https://nfcore.slack.com/channels/illuminavariantcalling"
    rel="nofollow">Slack</a> (you can join with <a href="https://nf-co.re/join/slack"
    rel="nofollow">this invite</a>).</p>

    <h2>

    <a id="user-content-citation" class="anchor" href="#citation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Citation</h2>



    <p>You can cite the <code>nf-core</code> publication as follows:</p>

    <blockquote>

    <p><strong>The nf-core framework for community-curated bioinformatics pipelines.</strong></p>

    <p>Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg,
    Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p>

    <p><em>Nat Biotechnol.</em> 2020 Feb 13. doi: <a href="https://dx.doi.org/10.1038/s41587-020-0439-x"
    rel="nofollow">10.1038/s41587-020-0439-x</a>.<br>

    ReadCube: <a href="https://rdcu.be/b1GjZ" rel="nofollow">Full Access Link</a></p>

    </blockquote>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1593036214.0
dcgc-bfx/singularity-sc-rhapsody:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: dcgc-bfx/singularity-sc-rhapsody
  latest_release: null
  readme: '<h1>

    <a id="user-content-singlecell-sc-rhapsody" class="anchor" href="#singlecell-sc-rhapsody"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singlecell-sc-rhapsody</h1>

    <p>DCGC singularity recipe for containerized versions of the BD Rhapsody Targeted
    Analysis and Whole Transcriptome Analysis (WTA) pipelines (available at <a href="https://bitbucket.org/CRSwDev/cwl/src/master/"
    rel="nofollow">https://bitbucket.org/CRSwDev/cwl/src/master/</a>).</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1624446752.0
devitocodes/devito:
  data_format: 2
  description: Code generation framework for automated finite difference computation
  filenames:
  - docker/Singularity.nvidia.def
  full_name: devitocodes/devito
  latest_release: v4.4
  readme: '<h1>

    <a id="user-content-devito-fast-stencil-computation-from-symbolic-specification"
    class="anchor" href="#devito-fast-stencil-computation-from-symbolic-specification"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Devito:
    Fast Stencil Computation from Symbolic Specification</h1>

    <p><a href="https://github.com/devitocodes/devito/actions?query=workflow%3ACI-core"><img
    src="https://github.com/devitocodes/devito/workflows/CI-core/badge.svg" alt="Build
    Status for the Core backend" style="max-width:100%;"></a>

    <a href="https://github.com/devitocodes/devito/actions?query=workflow%3ACI-mpi"><img
    src="https://github.com/devitocodes/devito/workflows/CI-mpi/badge.svg" alt="Build
    Status with MPI" style="max-width:100%;"></a>

    <a href="https://github.com/devitocodes/devito/actions?query=workflow%3ACI-gpu"><img
    src="https://github.com/devitocodes/devito/workflows/CI-gpu/badge.svg" alt="Build
    Status on GPU" style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/devitocodes/devito" rel="nofollow"><img src="https://camo.githubusercontent.com/3371fe5bdd570d040c748fb93a3e18ce00797c85315f2d05364781a1e5b9aa53/68747470733a2f2f636f6465636f762e696f2f67682f64657669746f636f6465732f64657669746f2f6272616e63682f6d61737465722f67726170682f62616467652e737667"
    alt="Code Coverage" data-canonical-src="https://codecov.io/gh/devitocodes/devito/branch/master/graph/badge.svg"
    style="max-width:100%;"></a>

    <a href="https://join.slack.com/t/devitocodes/shared_invite/zt-gtd2yxj9-Y31YKk_7lr9AwfXeL2iMFg"
    rel="nofollow"><img src="https://camo.githubusercontent.com/f0d0a8f3b06c0808c75575af15a74159d9d34f2bc02997c0f262dd916e0bf948/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230736c61636b2d253233333643354630"
    alt="Slack Status" data-canonical-src="https://img.shields.io/badge/chat-on%20slack-%2336C5F0"
    style="max-width:100%;"></a>

    <a href="https://devitocodes.github.io/devito-performance" rel="nofollow"><img
    src="https://camo.githubusercontent.com/3015b96f702ce7bd0e41f18aa7d7dfb69af77789127d64634a2223f829dbcee1/687474703a2f2f696d672e736869656c64732e696f2f62616467652f62656e63686d61726b656425323062792d6173762d626c75652e7376673f7374796c653d666c6174"
    alt="asv" data-canonical-src="http://img.shields.io/badge/benchmarked%20by-asv-blue.svg?style=flat"
    style="max-width:100%;"></a>

    <a href="https://badge.fury.io/py/devito" rel="nofollow"><img src="https://camo.githubusercontent.com/249b65986b967e7268f743fa8e3face99c98762feaa8d1417d07769b1d3385bf/68747470733a2f2f62616467652e667572792e696f2f70792f64657669746f2e737667"
    alt="PyPI version" data-canonical-src="https://badge.fury.io/py/devito.svg" style="max-width:100%;"></a>

    <a href="https://mybinder.org/v2/gh/devitocodes/devito/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667"
    alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"></a></p>

    <p><a href="http://www.devitoproject.org" rel="nofollow">Devito</a> is a Python
    package to implement

    optimized stencil computation (e.g., finite differences, image processing,

    machine learning) from high-level symbolic problem definitions.  Devito builds

    on <a href="http://www.sympy.org/en/index.html" rel="nofollow">SymPy</a> and employs
    automated code

    generation and just-in-time compilation to execute optimized computational

    kernels on several computer platforms, including CPUs, GPUs, and clusters

    thereof.</p>

    <ul>

    <li><a href="#about-devito">About Devito</a></li>

    <li><a href="#installation">Installation</a></li>

    <li><a href="#resources">Resources</a></li>

    <li><a href="#performance">Performance</a></li>

    <li><a href="#get-in-touch">Get in touch</a></li>

    <li><a href="#interactive-jupyter-notebooks">Interactive jupyter notebooks</a></li>

    </ul>

    <h2>

    <a id="user-content-about-devito" class="anchor" href="#about-devito" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>About Devito</h2>

    <p>Devito provides a functional language to implement sophisticated operators
    that

    can be made up of multiple stencil computations, boundary conditions, sparse

    operations (e.g., interpolation), and much more.  A typical use case is

    explicit finite difference methods for approximating partial differential

    equations. For example, a 2D diffusion operator may be implemented with Devito

    as follows</p>

    <div class="highlight highlight-source-python"><pre><span class="pl-c1">&gt;&gt;</span><span
    class="pl-c1">&gt;</span> <span class="pl-s1">grid</span> <span class="pl-c1">=</span>
    <span class="pl-v">Grid</span>(<span class="pl-s1">shape</span><span class="pl-c1">=</span>(<span
    class="pl-c1">10</span>, <span class="pl-c1">10</span>))

    <span class="pl-c1">&gt;&gt;</span><span class="pl-c1">&gt;</span> <span class="pl-s1">f</span>
    <span class="pl-c1">=</span> <span class="pl-v">TimeFunction</span>(<span class="pl-s1">name</span><span
    class="pl-c1">=</span><span class="pl-s">''f''</span>, <span class="pl-s1">grid</span><span
    class="pl-c1">=</span><span class="pl-s1">grid</span>, <span class="pl-s1">space_order</span><span
    class="pl-c1">=</span><span class="pl-c1">2</span>)

    <span class="pl-c1">&gt;&gt;</span><span class="pl-c1">&gt;</span> <span class="pl-s1">eqn</span>
    <span class="pl-c1">=</span> <span class="pl-v">Eq</span>(<span class="pl-s1">f</span>.<span
    class="pl-s1">dt</span>, <span class="pl-c1">0.5</span> <span class="pl-c1">*</span>
    <span class="pl-s1">f</span>.<span class="pl-s1">laplace</span>)

    <span class="pl-c1">&gt;&gt;</span><span class="pl-c1">&gt;</span> <span class="pl-s1">op</span>
    <span class="pl-c1">=</span> <span class="pl-v">Operator</span>(<span class="pl-v">Eq</span>(<span
    class="pl-s1">f</span>.<span class="pl-s1">forward</span>, <span class="pl-en">solve</span>(<span
    class="pl-s1">eqn</span>, <span class="pl-s1">f</span>.<span class="pl-s1">forward</span>)))</pre></div>

    <p>An <code>Operator</code> generates low-level code from an ordered collection
    of <code>Eq</code> (the

    example above being for a single equation). This code may also be compiled and

    executed</p>

    <div class="highlight highlight-source-python"><pre><span class="pl-c1">&gt;&gt;</span><span
    class="pl-c1">&gt;</span> <span class="pl-en">op</span>(<span class="pl-s1">t</span><span
    class="pl-c1">=</span><span class="pl-s1">timesteps</span>)</pre></div>

    <p>There is virtually no limit to the complexity of an <code>Operator</code> --
    the Devito

    compiler will automatically analyze the input, detect and apply optimizations

    (including single- and multi-node parallelism), and eventually generate code

    with suitable loops and expressions.</p>

    <p>Key features include:</p>

    <ul>

    <li>A functional language to express finite difference operators.</li>

    <li>Straightforward mechanisms to adjust the discretization.</li>

    <li>Constructs to express sparse operators (e.g., interpolation), classic linear

    operators (e.g., convolutions), and tensor contractions.</li>

    <li>Seamless support for boundary conditions and adjoint operators.</li>

    <li>A flexible API to define custom stencils, sub-domains, sub-sampling,

    and staggered grids.</li>

    <li>Generation of highly optimized parallel code (SIMD vectorization, CPU and
    GPU

    parallelism via OpenMP, multi-node parallelism via MPI, blocking, aggressive

    symbolic transformations for FLOP reduction, etc.).</li>

    <li>Distributed NumPy arrays over multi-node (MPI) domain decompositions.</li>

    <li>Inspection and customization of the generated code.</li>

    <li>Autotuning framework to ease performance tuning.</li>

    <li>Smooth integration with popular Python packages such as NumPy, SymPy, Dask,

    and SciPy, as well as machine learning frameworks such as TensorFlow and

    PyTorch.</li>

    </ul>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>The easiest way to try Devito is through Docker using the following commands:</p>

    <pre><code># get the code

    git clone https://github.com/devitocodes/devito.git

    cd devito


    # start a jupyter notebook server on port 8888

    docker-compose up devito

    </code></pre>

    <p>After running the last command above, the terminal will display a URL such
    as

    <code>https://127.0.0.1:8888/?token=XXX</code>. Copy-paste this URL into a browser
    window

    to start a <a href="https://jupyter.org/" rel="nofollow">Jupyter</a> notebook
    session where you can go

    through the <a href="https://github.com/devitocodes/devito/tree/master/examples">tutorials</a>

    provided with Devito or create your own notebooks.</p>

    <p><a href="http://devitocodes.github.io/devito/download.html" rel="nofollow">See
    here</a> for detailed installation

    instructions and other options. If you encounter a problem during installation,
    please

    see the

    <a href="https://github.com/devitocodes/devito/wiki/Installation-Issues">installation
    issues</a> we

    have seen in the past.</p>

    <h2>

    <a id="user-content-resources" class="anchor" href="#resources" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h2>

    <p>To learn how to use Devito,

    <a href="https://github.com/devitocodes/devito/blob/master/examples">here</a>
    is a good

    place to start, with lots of examples and tutorials.</p>

    <p>The <a href="https://www.devitoproject.org/" rel="nofollow">website</a> also
    provides access to other

    information, including documentation and instructions for citing us.</p>

    <p>Some FAQ are discussed <a href="https://github.com/devitocodes/devito/wiki/FAQ">here</a>.</p>

    <h2>

    <a id="user-content-performance" class="anchor" href="#performance" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Performance</h2>

    <p>If you are interested in any of the following</p>

    <ul>

    <li>Generation of parallel code (CPU, GPU, multi-node via MPI);</li>

    <li>Performance tuning;</li>

    <li>Benchmarking operators;</li>

    </ul>

    <p>then you should take a look at this

    <a href="https://github.com/devitocodes/devito/blob/master/benchmarks/user">README</a>.</p>

    <p>You may also be interested in

    <a href="https://github.com/devitocodes/thematrix">TheMatrix</a> -- a cross-architecture

    benchmarking framework showing the performance of several production-grade

    seismic operators implemented with Devito. This is now our flagship project

    towards neat, open, and reproducible science.</p>

    <h2>

    <a id="user-content-get-in-touch" class="anchor" href="#get-in-touch" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Get in touch</h2>

    <p>If you''re using Devito, we would like to hear from you. Whether you

    are facing issues or just trying it out, join the

    <a href="https://join.slack.com/t/devitocodes/shared_invite/zt-gtd2yxj9-Y31YKk_7lr9AwfXeL2iMFg"
    rel="nofollow">conversation</a>.</p>

    <h2>

    <a id="user-content-interactive-jupyter-notebooks" class="anchor" href="#interactive-jupyter-notebooks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interactive
    jupyter notebooks</h2>

    <p>The tutorial jupyter notebook are available interactively at the public <a
    href="https://mybinder.org/v2/gh/devitocodes/devito/master" rel="nofollow">binder</a>
    jupyterhub.</p>

    '
  stargazers_count: 310
  subscribers_count: 36
  topics:
  - compilers
  - stencil-codes
  - finite-difference
  - sympy
  - jit
  - performance
  - fwi
  - rtm
  updated_at: 1624526965.0
dfornika/nf-core-cpo:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: dfornika/nf-core-cpo
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corecpo" class="anchor" href="#nf-corecpo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/cpo</h1>

    <p><strong>Genomic Analysis of Carbapenem Resistant Organisms</strong></p>

    <p><a href="https://travis-ci.org/nf-core/cpo" rel="nofollow"><img src="https://camo.githubusercontent.com/6b4a4d26450e93f9c13ce85f059bb61ebe27051414d40e4f4ba81966ca0029a4/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f63706f2e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/cpo.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/cpo" rel="nofollow"><img src="https://camo.githubusercontent.com/4bc4e99ea4ca2a2f9b15fda9e4d3855153c0fd74431b920ed885080d46e0cc73/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f63706f2e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/cpo.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/cpo pipeline comes with documentation about the pipeline, found
    in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>


    <h3>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>

    <p>nf-core/cpo was originally written by Dan Fornika.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1544054866.0
dgruber/wfl:
  data_format: 2
  description: ' A Simple Way of Creating Job Workflows in Go running in Processes,
    Containers, Tasks, Pods, or Jobs '
  filenames:
  - examples/singularity/SingularityRecipe
  full_name: dgruber/wfl
  latest_release: v1.2.6
  readme: "<h1>\n<a id=\"user-content-wfl---a-simple-and-pluggable-workflow-language-for-go\"\
    \ class=\"anchor\" href=\"#wfl---a-simple-and-pluggable-workflow-language-for-go\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>wfl - A Simple and Pluggable Workflow Language for Go</h1>\n<p><em>Don't\
    \ mix wfl with <a href=\"https://en.wikipedia.org/wiki/Work_Flow_Language\" rel=\"\
    nofollow\">WFL</a>.</em></p>\n<p><a href=\"https://circleci.com/gh/dgruber/wfl/tree/master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2e68f5d3e6715dff94d43886169409bb991226c5562ede8aa6996cb6cd43277d/68747470733a2f2f636972636c6563692e636f6d2f67682f646772756265722f77666c2f747265652f6d61737465722e7376673f7374796c653d737667\"\
    \ alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/dgruber/wfl/tree/master.svg?style=svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/gh/dgruber/wfl\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c0ba5989abe58a671d275f14915277b3c1ab0782fd00555895dfc667863f9c93/68747470733a2f2f636f6465636f762e696f2f67682f646772756265722f77666c2f6272616e63682f6d61737465722f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/dgruber/wfl/branch/master/graph/badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<blockquote>\n<p><em>Update</em>: In order\
    \ to reflect the underlying drmaa2os changes which separates\ndifferent backends\
    \ more clearly some context creation functions are moved\nto pkg/context. That\
    \ avoids having to deal with dependencies from bigger libraries\nlike Kubernetes\
    \ or Docker when not using them.</p>\n</blockquote>\n<p>Creating process, container,\
    \ pod, task, or job workflows based on raw interfaces of\noperating systems, Docker,\
    \ Singularity, Kubernetes, Cloud Foundry, and HPC job schedulers can be\na tedios.\
    \ Lots of repeating code is required. All workload management systems have a\n\
    different API.</p>\n<p><em>wfl</em> abstracts away from the underlying details\
    \ of the processes, containers, and\nworkload management systems. <em>wfl</em>\
    \ provides a simple, unified interface which allows\nto quickly define and execute\
    \ a job workflow and change between different execution\nbackends without changing\
    \ the workflow itself.</p>\n<p><em>wfl</em> does not come with many features but\
    \ is simple to use and enough to define and\nrun jobs and job workflows with inter-job\
    \ dependencies.</p>\n<p>In its simplest form a process can be started and waited\
    \ for:</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span class=\"\
    pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()).<span class=\"pl-en\"\
    >Run</span>(<span class=\"pl-s\">\"convert\"</span>, <span class=\"pl-s\">\"image.jpg\"\
    </span>, <span class=\"pl-s\">\"image.png\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n\
    <p>If the output of the command needs to be displayed on the terminal you can\
    \ set the out path in the\ndefault <em>JobTemplate</em> (see below) configuration:</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>\t<span class=\"pl-s1\">template</span>\
    \ <span class=\"pl-c1\">:=</span> drmaa2interface.<span class=\"pl-smi\">JobTemplate</span>{\n\
    \t\t<span class=\"pl-c1\">ErrorPath</span>:  <span class=\"pl-s\">\"/dev/stderr\"\
    </span>,\n\t\t<span class=\"pl-c1\">OutputPath</span>: <span class=\"pl-s\">\"\
    /dev/stdout\"</span>,\n\t}\n\t<span class=\"pl-s1\">flow</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContextByCfg</span>(wfl.<span\
    \ class=\"pl-smi\">ProcessConfig</span>{\n\t\t<span class=\"pl-c1\">DefaultTemplate</span>:\
    \ <span class=\"pl-s1\">template</span>,\n\t}))\n\t<span class=\"pl-s1\">flow</span>.<span\
    \ class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"\
    pl-s\">\"hello\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n<p>Running\
    \ a job as a Docker container requires a different context (and the image\nalready\
    \ pulled before).</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-k\">import</span> (\n\t<span class=\"pl-s\">\"github.com/dgruber/drmaa2interface\"\
    </span>\n\t<span class=\"pl-s\">\"github.com/dgruber/wfl\"</span>\n\t<span class=\"\
    pl-s\">\"github.com/dgruber/wfl/pkg/context/docker\"</span>\n    )\n    \n   \
    \ <span class=\"pl-s1\">ctx</span> <span class=\"pl-c1\">:=</span> <span class=\"\
    pl-s1\">docker</span>.<span class=\"pl-en\">NewDockerContextByCfg</span>(docker.<span\
    \ class=\"pl-smi\">Config</span>{<span class=\"pl-c1\">DefaultDockerImage</span>:\
    \ <span class=\"pl-s\">\"golang:latest\"</span>})\n    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>).<span class=\"\
    pl-en\">Run</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\"\
    >\"60\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n<p>Starting a\
    \ Docker container without a <em>run command</em> which exposes ports requires\
    \ more\nconfiguration which can be provided by using a <em>JobTemplate</em> together\
    \ with the <em>RunT()</em>\nmethod.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">jt</span> <span class=\"pl-c1\">:=</span> drmaa2interface.<span\
    \ class=\"pl-smi\">JobTemplate</span>{\n        <span class=\"pl-c1\">JobCategory</span>:\
    \ <span class=\"pl-s\">\"swaggerapi/swagger-editor\"</span>,\n    }\n    <span\
    \ class=\"pl-s1\">jt</span>.<span class=\"pl-c1\">ExtensionList</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-k\">map</span>[<span class=\"pl-smi\">string</span>]<span\
    \ class=\"pl-smi\">string</span>{<span class=\"pl-s\">\"exposedPorts\"</span>:\
    \ <span class=\"pl-s\">\"80:8080/tcp\"</span>}\n    \n    <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">docker</span>.<span\
    \ class=\"pl-en\">NewDockerContext</span>())).<span class=\"pl-en\">RunT</span>(<span\
    \ class=\"pl-s1\">jt</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n\
    <p>Starting a Kubernetes batch job and waiting for its end is not much different.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">kubernetes</span>.<span\
    \ class=\"pl-en\">NewKubernetesContext</span>()).<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"60\"</span>).<span class=\"\
    pl-en\">Wait</span>()</pre></div>\n<p><em>wfl</em> also supports submitting jobs\
    \ into HPC schedulers like SLURM, Grid Engine and so on.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">libdrmaa</span>.<span class=\"\
    pl-en\">NewLibDRMAAContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"60\"</span>).<span class=\"pl-en\"\
    >Wait</span>()</pre></div>\n<p><em>wfl</em> aims to work for any kind of workload.\
    \ It works on a Mac and Raspberry Pi the same way\nas on a high-performance compute\
    \ cluster. Things missing: On small scale you probably miss data\nmanagement -\
    \ moving results from one job to another. That's deliberately not implemented.\n\
    On large scale you are missing checkpoint and restart functionality or HA of the\
    \ workflow\nprocess itself.</p>\n<p><em>wfl</em> works with simple primitives:\
    \ <em>context</em>, <em>workflow</em>, <em>job</em>, and <em>jobtemplate</em></p>\n\
    <p>Experimental: Jobs can also be processed in <a href=\"https://github.com/dgruber/wfl/blob/master/examples/stream/stream.go\"\
    >job control streams</a>.</p>\n<p>First support for logging is also available.\
    \ Log levels can be controlled by environment variables\n(<em>export WFL_LOGLEVEL=DEBUG</em>\
    \ or <em>INFO</em>/<em>WARNING</em>/<em>ERROR</em>/<em>NONE</em>). Applications\
    \ can use the same\nlogging facility by getting the logger from the workflow (<em>workflow.Logger()</em>)\
    \ or registering\nyour own logger in a workflow <em>(workflow.SetLogger(Logger\
    \ interface)</em>). Default is set to ERROR.</p>\n<h3>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h3>\n<p>Dependencies\
    \ of <em>wfl</em> (like drmaa2) are vendored in. The only external package required\
    \ to be installed\nmanually is the <em>drmaa2interface</em>.</p>\n<div class=\"\
    highlight highlight-source-go\"><pre>    <span class=\"pl-k\">go</span> <span\
    \ class=\"pl-s1\">get</span> <span class=\"pl-s1\">github</span>.<span class=\"\
    pl-c1\">com</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">dgruber</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">drmaa2interface</span></pre></div>\n\
    <h2>\n<a id=\"user-content-context\" class=\"anchor\" href=\"#context\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Context</h2>\n\
    <p>A context defines the execution backend for the workflow. Contexts can be easily\
    \ created\nwith the <em>New</em> functions which are defined in the <em>context.go</em>\
    \ file or in the separate\npackages found in <em>pkg/context</em>.</p>\n<p>For\
    \ creating a context which executes the jobs of a workflow in operating system\
    \ processses use:</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()</pre></div>\n\
    <p>If the workflow needs to be executed in containers the <em>DockerContext</em>\
    \ can be used:</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-s1\">docker</span>.<span class=\"pl-en\">NewDockerContext</span>()</pre></div>\n\
    <p>If the Docker context needs to be configured with a default Docker image\n\
    (when Run() is used or RunT() without a configured <em>JobCategory</em> (which\
    \ <em>is</em> the Docker image))\nthen the <em>ContextByCfg()</em> can be called.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">docker</span>.<span\
    \ class=\"pl-en\">NewDockerContextByCfg</span>(docker.<span class=\"pl-smi\">Config</span>{<span\
    \ class=\"pl-c1\">DefaultDockerImage</span>: <span class=\"pl-s\">\"golang:latest\"\
    </span>})</pre></div>\n<p>When you want to run the workflow as Cloud Foundry tasks\
    \ the <em>CloudFoundryContext</em> can be used:</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">cloudfoundry</span>.<span class=\"pl-en\">NewCloudFoundryContext</span>()</pre></div>\n\
    <p>Without a config it uses following environment variables to access the Cloud\
    \ Foundry cloud controller API:</p>\n<ul>\n<li>CF_API (like <a href=\"https://api.run.pivotal.io\"\
    \ rel=\"nofollow\">https://api.run.pivotal.io</a>)</li>\n<li>CF_USER</li>\n<li>CF_PASSWORD</li>\n\
    </ul>\n<p>For submitting Kubernetes batch jobs a Kubernetes context exists.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>   <span class=\"pl-s1\">ctx</span>\
    \ <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">kubernetes</span>.<span\
    \ class=\"pl-en\">NewKubernetesContext</span>()</pre></div>\n<p>Note that each\
    \ job requires a container image specified which can be done by using\nthe JobTemplate's\
    \ JobCategory. When the same container image is used within the whole\njob workflow\
    \ it makes sense to use the Kubernetes config.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>   <span class=\"pl-s1\">ctx</span> <span class=\"pl-c1\">:=</span> <span\
    \ class=\"pl-s1\">kubernetes</span>.<span class=\"pl-en\">NewKubernetesContextByCfg</span>(kubernetes.<span\
    \ class=\"pl-smi\">Config</span>{<span class=\"pl-c1\">DefaultImage</span>: <span\
    \ class=\"pl-s\">\"busybox:latest\"</span>})</pre></div>\n<p><a href=\"https://en.wikipedia.org/wiki/Singularity_(software)\"\
    \ rel=\"nofollow\">Singularity</a> containers can be executed\nwithin the Singularity\
    \ context. When setting the <em>DefaultImage</em> (like in the Kubernetes Context)\n\
    then then <em>Run()</em> methods can be used otherwise the Container image must\
    \ be specified in the\nJobTemplate's <em>JobCategory</em> field separately for\
    \ each job. The <em>DefaultImage</em>\ncan always be overridden by the <em>JobCategory</em>.\
    \ Note that each task / job\nexecutes a separate Singularity container process.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>   <span class=\"pl-s1\">ctx</span>\
    \ <span class=\"pl-c1\">:=</span> wfl.<span class=\"pl-smi\">NewSingularityContextByCfg</span>(wfl.<span\
    \ class=\"pl-smi\">SingularityConfig</span>{<span class=\"pl-c1\">DefaultImage</span>:\
    \ <span class=\"pl-s\">\"\"</span>}))</pre></div>\n<p>For working with HPC schedulers\
    \ the libdrmaa context can be used. This context requires\n<em>libdrmaa.so</em>\
    \ available in the library path at runtime. Grid Engine ships <em>libdrmaa.so</em>\n\
    but the <em>LD_LIBRARY_PATH</em> needs to be typically set. For SLURM <em>libdrmaa.so</em>\
    \ often needs\nto be <a href=\"https://github.com/natefoo/slurm-drmaa\">build</a>.</p>\n\
    <p>Since C go is used under the hood (drmaa2os which uses go drmaa) some compiler\
    \ flags needs\nto be set during build time. Those flags depend on the workload\
    \ manager used. Best check\nout the go drmaa project for finding the right flags.</p>\n\
    <p>For building SLURM requires:</p>\n<pre><code>export CGO_LDFLAGS=\"-L$SLURM_DRMAA_ROOT/lib\"\
    \nexport CGO_CFLAGS=\"-DSLURM -I$SLURM_DRMAA_ROOT/include\"\n</code></pre>\n<p>If\
    \ all set a libdrmaa context can be created by importing:</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>   <span class=\"pl-s1\">ctx</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">libdrmaa</span>.<span class=\"pl-en\"\
    >NewLibDRMAAContext</span>()</pre></div>\n<p>The JobCategory is whatever the workloadmanager\
    \ associates with it. Typically it is a\nset of submission parameters. A basic\
    \ example is <a href=\"https://github.com/dgruber/wfl/blob/master/examples/libdrmaa/libdrmaa.go\"\
    >here</a>.</p>\n<h2>\n<a id=\"user-content-workflow\" class=\"anchor\" href=\"\
    #workflow\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Workflow</h2>\n<p>A workflow encapsulates a set of jobs using the\
    \ same backend (context). Depending on the execution\nbackend it can be seen as\
    \ a namespace.</p>\n<p>It can be created by using:</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">wf</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)</pre></div>\n<p>Errors during creation can be catched\
    \ with</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span class=\"\
    pl-s1\">wf</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>).<span class=\"\
    pl-en\">OnError</span>(<span class=\"pl-k\">func</span>(<span class=\"pl-s1\"\
    >e</span> <span class=\"pl-smi\">error</span>) {<span class=\"pl-en\">panic</span>(<span\
    \ class=\"pl-s1\">e</span>)})</pre></div>\n<p>or with</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-k\">if</span> <span class=\"\
    pl-s1\">wf</span>.<span class=\"pl-en\">HasError</span>() {\n        <span class=\"\
    pl-en\">panic</span>(<span class=\"pl-s1\">wf</span>.<span class=\"pl-en\">Error</span>())\n\
    \    }</pre></div>\n<h2>\n<a id=\"user-content-job\" class=\"anchor\" href=\"\
    #job\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Job</h2>\n<p>Jobs are the main objects in <em>wfl</em>. A job defines\
    \ helper methods. Many of them return the job object itself to allow chaining\
    \ calls in an easy way. A job can also be seen as a container and control unit\
    \ for tasks. Tasks are often mapped to jobs of the underlying\nworkload manager\
    \ (like in Kubernetes, HPC schedulers etc.).</p>\n<p>In some systems it is required\
    \ to delete job related resources after the job is finished\nand no more information\
    \ needs to be queried about its execution. This functionality is\nimplemented\
    \ in the DRMAA2 Reap() method which can be executed by ReapAll() for each\ntask\
    \ in the job object. Afterwards the job object should not be used anymore as some\n\
    information might not be available anymore.</p>\n<p>Methods can be classified\
    \ in blocking, non-blocking, job template based, function based, and error handlers.</p>\n\
    <h3>\n<a id=\"user-content-job-submission\" class=\"anchor\" href=\"#job-submission\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Job Submission</h3>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n\
    <th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td>Run()</td>\n<td>Starts a process, container, or submits a task and comes\
    \ back immediately</td>\n<td>no</td>\n<td></td>\n</tr>\n<tr>\n<td>RunT()</td>\n\
    <td>Like above but with a JobTemplate as parameter</td>\n<td>no</td>\n<td></td>\n\
    </tr>\n<tr>\n<td>RunArray()</td>\n<td>Submits a bulk job which runs many iterations\
    \ of the same command</td>\n<td>no</td>\n<td></td>\n</tr>\n<tr>\n<td>Resubmit()</td>\n\
    <td>Submits a job <em>n</em>-times (Run().Run().Run()...)</td>\n<td>no</td>\n\
    <td></td>\n</tr>\n<tr>\n<td>RunEvery()</td>\n<td>Submits a task every d <em>time.Duration</em>\n\
    </td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>RunEveryT()</td>\n<td>Like <em>RunEvery()</em>\
    \ but with JobTemplate as param</td>\n<td>yes</td>\n<td></td>\n</tr>\n</tbody>\n\
    </table>\n<h3>\n<a id=\"user-content-job-control\" class=\"anchor\" href=\"#job-control\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Job Control</h3>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n\
    <th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td>Suspend()</td>\n<td>Stops a task from execution (e.g. sending SIGTSTP\
    \ to the process group)...</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Resume()</td>\n\
    <td>Continues a task (e.g. sending SIGCONT)...</td>\n<td></td>\n<td></td>\n</tr>\n\
    <tr>\n<td>Kill()</td>\n<td>Stops process (SIGKILL), container, task, job immediately.</td>\n\
    <td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3>\n<a id=\"user-content-function-execution\"\
    \ class=\"anchor\" href=\"#function-execution\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Function Execution</h3>\n<table>\n\
    <thead>\n<tr>\n<th>Function Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>Do()</td>\n<td>Executes a Go function</td>\n\
    <td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Then()</td>\n<td>Waits for end of process\
    \ and executes a Go function</td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>OnSuccess()</td>\n\
    <td>Executes a function if the task run successfully (exit code 0)</td>\n<td>yes</td>\n\
    <td></td>\n</tr>\n<tr>\n<td>OnFailure()</td>\n<td>Executes a function if the task\
    \ failed (exit code != 0)</td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>OnError()</td>\n\
    <td>Executes a function if the task could not be created</td>\n<td>yes</td>\n\
    <td></td>\n</tr>\n</tbody>\n</table>\n<h3>\n<a id=\"user-content-blocker\" class=\"\
    anchor\" href=\"#blocker\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Blocker</h3>\n<table>\n<thead>\n<tr>\n<th>Function\
    \ Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td>After()</td>\n<td>Blocks a specific amount of time and continues</td>\n\
    <td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Wait()</td>\n<td>Waits until the task\
    \ submitted latest finished</td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Synchronize()</td>\n\
    <td>Waits until all submitted tasks finished</td>\n<td>yes</td>\n<td></td>\n</tr>\n\
    </tbody>\n</table>\n<h3>\n<a id=\"user-content-job-flow-control\" class=\"anchor\"\
    \ href=\"#job-flow-control\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Job Flow Control</h3>\n<table>\n<thead>\n<tr>\n\
    <th>Function Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>ThenRun()</td>\n<td>Wait() (last task finished)\
    \ followed by an async Run()</td>\n<td>partially</td>\n<td></td>\n</tr>\n<tr>\n\
    <td>ThenRunT()</td>\n<td>ThenRun() with template</td>\n<td>partially</td>\n<td></td>\n\
    </tr>\n<tr>\n<td>OnSuccessRun()</td>\n<td>Wait() if Success() then Run()</td>\n\
    <td>partially</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>OnSuccessRunT()</td>\n<td>OnSuccessRun()\
    \ but with template as param</td>\n<td>partially</td>\n<td></td>\n</tr>\n<tr>\n\
    <td>OnFailureRun()</td>\n<td>Wait() if Failed() then Run()</td>\n<td>partially</td>\n\
    <td>\_</td>\n</tr>\n<tr>\n<td>OnFailureRunT()</td>\n<td>OnFailureRun() but with\
    \ template as param</td>\n<td>partially</td>\n<td></td>\n</tr>\n<tr>\n<td>Retry()</td>\n\
    <td>wait() + !success() + resubmit() + wait() + !success()</td>\n<td>yes</td>\n\
    <td>\_</td>\n</tr>\n<tr>\n<td>AnyFailed()</td>\n<td>Cchecks if one of the tasks\
    \ in the job failed</td>\n<td>yes</td>\n<td>\_</td>\n</tr>\n</tbody>\n</table>\n\
    <h3>\n<a id=\"user-content-job-status-and-general-checks\" class=\"anchor\" href=\"\
    #job-status-and-general-checks\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Job Status and General Checks</h3>\n\
    <table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n\
    <th>Examples</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JobID()</td>\n<td>Returns\
    \ the ID of the submitted job</td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>JobInfo()</td>\n\
    <td>Returns the DRMAA2 JobInfo of the job</td>\n<td>no</td>\n<td>\_</td>\n</tr>\n\
    <tr>\n<td>Template()</td>\n<td></td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n\
    <td>State()</td>\n<td></td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>LastError()</td>\n\
    <td></td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>Failed()</td>\n<td></td>\n\
    <td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>Success()</td>\n<td></td>\n<td>no</td>\n\
    <td>\_</td>\n</tr>\n<tr>\n<td>ExitStatus()</td>\n<td></td>\n<td>no</td>\n<td>\_\
    </td>\n</tr>\n<tr>\n<td>ReapAll()</td>\n<td>Cleans up all job related resources\
    \ from the workload manager. Do not</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n\
    <td>use the job object afterwards. Calls DRMAA2 Reap() on all tasks.</td>\n<td>no</td>\n\
    <td>\_</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-jobtemplate\"\
    \ class=\"anchor\" href=\"#jobtemplate\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>JobTemplate</h2>\n<p>JobTemplates\
    \ are specifying the details about a job. In the simplest case the job is specified\
    \ by the application name and its arguments like it is typically done in the OS\
    \ shell. In that case the <em>Run()</em> methods (<em>ThenRun()</em>, <em>OnSuccessRun()</em>,\
    \ <em>OnFailureRun()</em>) can be used. Job template based methods (like <em>RunT()</em>)\
    \ can be completely avoided by providing a\ndefault template when creating the\
    \ context (<em>...ByConfig()</em>). Then each <em>Run()</em> inherits the settings\
    \ (like <em>JobCategory</em> for the container image name and <em>OutputPath</em>\
    \ for redirecting output to <em>stdout</em>). If more details for specifying the\
    \ jobs are required the <em>RunT()</em> methods needs to be used.\nI'm using currently\
    \ the <a href=\"https://github.com/dgruber/drmaa2interface/blob/master/jobtemplate.go\"\
    >DRMAA2 Go JobTemplate</a>. In most cases only <em>RemoteCommand</em>, <em>Args</em>,\
    \ <em>WorkingDirectory</em>, <em>JobCategory</em>, <em>JobEnvironment</em>,  <em>StageInFiles</em>\
    \ are evaluated. Functionality and semantic is up to the underlying <a href=\"\
    https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker\">drmaa2os job\
    \ tracker</a>.</p>\n<ul>\n<li><a href=\"https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker/simpletracker\"\
    >For the process mapping see here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/libdrmaa\"\
    >For the mapping to a drmaa1 implementation (libdrmaa.so) for SLURM, Grid Engine,\
    \ PBS, ...</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker/dockertracker\"\
    >For the Docker mapping here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/cftracker\"\
    >For the Cloud Foundry Task mapping here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/kubernetestracker\"\
    >For the Kubernetes batch job mapping here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/singularity\"\
    >Singularity support</a></li>\n</ul>\n<p>The <a href=\"https://github.com/dgruber/wfl/blob/master/template.go\"\
    ><em>Template</em></a> object provides helper functions for job templates and\
    \ required as generators of job <a href=\"https://github.com/dgruber/wfl/blob/master/examples/stream/stream.go\"\
    >streams</a>. For an example see <a href=\"https://github.com/dgruber/wfl/tree/master/examples/template/template.go\"\
    >here</a>.</p>\n<h1>\n<a id=\"user-content-examples\" class=\"anchor\" href=\"\
    #examples\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Examples</h1>\n<p>For examples please have a look into the examples\
    \ directory. <a href=\"https://github.com/dgruber/wfl/tree/master/examples/template/template.go\"\
    >template</a> is a canonical example of a pre-processing job, followed by parallel\
    \ execution, followed by a post-processing job.</p>\n<p><a href=\"https://github.com/dgruber/wfl/blob/master/test/test.go\"\
    >test</a> is an use case for testing. It compiles\nall examples with the local\
    \ go compiler and then within a Docker container using the <em>golang:latest</em>\
    \ image\nand reports errors.</p>\n<p><a href=\"https://github.com/dgruber/wfl/blob/master/examples/cloudfoundry/cloudfoundry.go\"\
    >cloudfoundry</a> demonstrates how a Cloud Foundry taks can be created.</p>\n\
    <p><a href=\"https://github.com/dgruber/wfl/blob/master/examples/singularity/singularity.go\"\
    >Singularity containers</a> can also be created which is helpful when managing\
    \ a simple Singularity <em>wfl</em> container workflow within a single HPC job\
    \ either to fully exploit all resources and reduce the amount of HPC jobs.</p>\n\
    <h2>\n<a id=\"user-content-creating-a-workflow-which-is-executed-as-os-processes\"\
    \ class=\"anchor\" href=\"#creating-a-workflow-which-is-executed-as-os-processes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Creating a Workflow which is Executed as OS Processes</h2>\n<p>The\
    \ allocated context defines which workload management system / job execution backend\
    \ is used.</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span class=\"\
    pl-s1\">ctx</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewProcessContext</span>()</pre></div>\n<p>Different contexts\
    \ can be used within a single program. That way multi-clustering potentially\n\
    over different cloud solutions is supported.</p>\n<p>Using a context a workflow\
    \ can be established.</p>\n<div class=\"highlight highlight-source-go\"><pre>\
    \    <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>())</pre></div>\n\
    <p>Handling an error during workflow generation can be done by specifying a function\
    \ which\nis only called in the case of an error.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()).<span\
    \ class=\"pl-en\">OnError</span>(<span class=\"pl-k\">func</span>(<span class=\"\
    pl-s1\">e</span> <span class=\"pl-smi\">error</span>) {\n\t\t<span class=\"pl-en\"\
    >panic</span>(<span class=\"pl-s1\">e</span>)\n\t})</pre></div>\n<p>The workflow\
    \ is used in order to instantiate the first job using the <em>Run()</em> method.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewProcessContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"123\"</span>)</pre></div>\n<p>But\
    \ you can also create an initial job like that:</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">job</span> <span class=\"pl-c1\">:=</span> <span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewJob</span>(<span class=\"\
    pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()))</pre></div>\n<p>For\
    \ more detailed settings (like resource limits) the DRMAA2 job template can be\
    \ used as parameter for <em>RunT()</em>.</p>\n<p>Jobs allow the execution of workload\
    \ as well as expressing dependencies.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()).<span\
    \ class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"\
    pl-s\">\"2\"</span>).<span class=\"pl-en\">ThenRun</span>(<span class=\"pl-s\"\
    >\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n\
    <p>The line above executes two OS processes sequentially and waits until the last\
    \ job in chain is finished.</p>\n<p>In the following example the two sleep processes\
    \ are executed in parallel. <em>Wait()</em> only waitf for the sleep 1 job. Hence\
    \ sleep 2 still runs after the wait call comes back.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewProcessContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"pl-s\"\
    >\"sleep\"</span>, <span class=\"pl-s\">\"2\"</span>).<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).<span class=\"\
    pl-en\">Wait</span>()</pre></div>\n<p>Running two jobs in parallel and waiting\
    \ until all jobs finished can be done <em>Synchronize()</em>.</p>\n<div class=\"\
    highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewProcessContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"2\"</span>).<span class=\"pl-en\"\
    >Run</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"\
    </span>).<span class=\"pl-en\">Synchronize</span>()</pre></div>\n<p>Jobs can also\
    \ be suspended (stopped) and resumed (continued) - if supported by the execution\
    \ backend (like OS, Docker).</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">wf</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).<span class=\"\
    pl-en\">After</span>(<span class=\"pl-s1\">time</span>.<span class=\"pl-c1\">Millisecond</span>\
    \ <span class=\"pl-c1\">*</span> <span class=\"pl-c1\">100</span>).<span class=\"\
    pl-en\">Suspend</span>().<span class=\"pl-en\">After</span>(<span class=\"pl-s1\"\
    >time</span>.<span class=\"pl-c1\">Millisecond</span> <span class=\"pl-c1\">*</span>\
    \ <span class=\"pl-c1\">100</span>).<span class=\"pl-en\">Resume</span>().<span\
    \ class=\"pl-en\">Wait</span>()</pre></div>\n<p>The exit status is available as\
    \ well. <em>ExitStatus()</em> blocks until the previously submitted job is finished.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>).<span class=\"\
    pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\"\
    >\"hello\"</span>).<span class=\"pl-en\">ExitStatus</span>()</pre></div>\n<p>In\
    \ order to run jobs depending on the exit status the <em>OnFailure</em> and <em>OnSuccess</em>\
    \ methods can be used:</p>\n<div class=\"highlight highlight-source-go\"><pre>\
    \    <span class=\"pl-s1\">wf</span>.<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"false\"</span>).<span class=\"pl-en\">OnFailureRun</span>(<span class=\"\
    pl-s\">\"true\"</span>).<span class=\"pl-en\">OnSuccessRun</span>(<span class=\"\
    pl-s\">\"false\"</span>)</pre></div>\n<p>For executing a function on a submission\
    \ error <em>OnError()</em> can be used.</p>\n<p>More methods can be found in the\
    \ sources.</p>\n<h2>\n<a id=\"user-content-basic-workflow-patterns\" class=\"\
    anchor\" href=\"#basic-workflow-patterns\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Basic Workflow Patterns</h2>\n\
    <h3>\n<a id=\"user-content-sequence\" class=\"anchor\" href=\"#sequence\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Sequence</h3>\n\
    <p>The successor task runs after the completion of the pre-decessor task.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">flow</span>\
    \ <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)\n    <span class=\"\
    pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"\
    </span>, <span class=\"pl-s\">\"first task\"</span>).<span class=\"pl-en\">ThenRun</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"second task\"</span>)\n\
    \    <span class=\"pl-c1\">...</span></pre></div>\n<p>or</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">flow</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)\n    <span class=\"pl-s1\">job</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>)\n\
    \    <span class=\"pl-s1\">job</span>.<span class=\"pl-en\">Wait</span>()\n  \
    \  <span class=\"pl-s1\">job</span>.<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"second task\"</span>)\n    <span\
    \ class=\"pl-c1\">...</span></pre></div>\n<h3>\n<a id=\"user-content-parallel-split\"\
    \ class=\"anchor\" href=\"#parallel-split\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Parallel Split</h3>\n<p>After\
    \ completion of a task run multiple branches of tasks.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>\n    <span class=\"pl-s1\">flow</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)\n    <span class=\"pl-s1\">flow</span>.<span class=\"\
    pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\"\
    >\"first task\"</span>).<span class=\"pl-en\">Wait</span>()\n\n    <span class=\"\
    pl-s1\">notifier</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewNotifier</span>()\n\n    <span class=\"pl-k\"\
    >go</span> <span class=\"pl-k\">func</span>() {\n        <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)).\n   \
    \         <span class=\"pl-en\">TagWith</span>(<span class=\"pl-s\">\"BranchA\"\
    </span>).\n            <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"\
    sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).\n            <span class=\"\
    pl-en\">ThenRun</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\"\
    >\"3\"</span>).\n            <span class=\"pl-en\">Synchronize</span>().\n   \
    \         <span class=\"pl-en\">Notify</span>(<span class=\"pl-s1\">notifier</span>)\n\
    \    }\n\n    <span class=\"pl-k\">go</span> <span class=\"pl-k\">func</span>()\
    \ {\n        <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewJob</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"\
    pl-s1\">ctx</span>)).\n            <span class=\"pl-en\">TagWith</span>(<span\
    \ class=\"pl-s\">\"BranchB\"</span>).\n            <span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).\n     \
    \       <span class=\"pl-en\">ThenRun</span>(<span class=\"pl-s\">\"sleep\"</span>,\
    \ <span class=\"pl-s\">\"3\"</span>).\n            <span class=\"pl-en\">Synchronize</span>().\n\
    \            <span class=\"pl-en\">Notify</span>(<span class=\"pl-s1\">notifier</span>)\n\
    \    }\n\n    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \n    <span class=\"pl-c1\">...</span></pre></div>\n<h3>\n<a id=\"user-content-synchronization-of-tasks\"\
    \ class=\"anchor\" href=\"#synchronization-of-tasks\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Synchronization\
    \ of Tasks</h3>\n<p>Wait until all tasks of a job which are running in parallel\
    \ are finished.</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-s1\">flow</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)\n\
    \    <span class=\"pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>).\n\
    \        <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>,\
    \ <span class=\"pl-s\">\"second task\"</span>).\n        <span class=\"pl-en\"\
    >Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"third\
    \ task\"</span>).\n        <span class=\"pl-en\">Synchronize</span>()</pre></div>\n\
    <h3>\n<a id=\"user-content-synchronization-of-branches\" class=\"anchor\" href=\"\
    #synchronization-of-branches\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Synchronization of Branches</h3>\n\
    <p>Wait until all branches of a workflow are finished.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>\n    <span class=\"pl-s1\">notifier</span> <span\
    \ class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewNotifier</span>()\n\n    <span class=\"pl-k\">go</span> <span class=\"pl-k\"\
    >func</span>() {\n        <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)).\n            <span class=\"pl-en\">TagWith</span>(<span\
    \ class=\"pl-s\">\"BranchA\"</span>).\n            <span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).\n     \
    \       <span class=\"pl-en\">Wait</span>().\n\t\t\t<span class=\"pl-en\">Notify</span>(<span\
    \ class=\"pl-s1\">notifier</span>)\n    }\n\n    <span class=\"pl-k\">go</span>\
    \ <span class=\"pl-k\">func</span>() {\n        <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)).\n            <span\
    \ class=\"pl-en\">TagWith</span>(<span class=\"pl-s\">\"BranchB\"</span>).\n \
    \           <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"sleep\"</span>,\
    \ <span class=\"pl-s\">\"1\"</span>).\n            <span class=\"pl-en\">Wait</span>().\n\
    \t\t\t<span class=\"pl-en\">Notify</span>(<span class=\"pl-s1\">notifier</span>)\n\
    \    }\n\n    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \n    <span class=\"pl-c1\">...</span></pre></div>\n<h3>\n<a id=\"user-content-exclusive-choice\"\
    \ class=\"anchor\" href=\"#exclusive-choice\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Exclusive Choice</h3>\n<div class=\"\
    highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">flow</span> <span\
    \ class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)\n    <span class=\"pl-s1\"\
    >job</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">flow</span>.<span\
    \ class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"\
    pl-s\">\"first task\"</span>)\n    <span class=\"pl-s1\">job</span>.<span class=\"\
    pl-en\">Wait</span>()\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\"\
    >job</span>.<span class=\"pl-en\">Success</span>() {\n        <span class=\"pl-c\"\
    >// do something</span>\n    } <span class=\"pl-k\">else</span> {\n        <span\
    \ class=\"pl-c\">// do something else</span>\n    }\n    <span class=\"pl-c1\"\
    >...</span></pre></div>\n<h3>\n<a id=\"user-content-fork-pattern\" class=\"anchor\"\
    \ href=\"#fork-pattern\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Fork Pattern</h3>\n<p>When a task is finished\
    \ <em>n</em> tasks needs to be started in parallel.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">job</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>).\n       \
    \ <span class=\"pl-en\">ThenRun</span>(<span class=\"pl-s\">\"echo\"</span>, <span\
    \ class=\"pl-s\">\"parallel task 1\"</span>).\n        <span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"parallel task 2\"</span>).\n\
    \        <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>,\
    \ <span class=\"pl-s\">\"parallel task 3\"</span>)\n    <span class=\"pl-c1\"\
    >...</span></pre></div>\n<p>or</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">flow</span> <span class=\"pl-c1\">:=</span> <span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"\
    pl-s1\">ctx</span>)\n    \n    <span class=\"pl-s1\">job</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>)\n\
    \    <span class=\"pl-s1\">job</span>.<span class=\"pl-en\">Wait</span>()\n  \
    \  <span class=\"pl-k\">for</span> <span class=\"pl-s1\">i</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-c1\">1</span>; <span class=\"pl-s1\">i</span>\
    \ <span class=\"pl-c1\">&lt;=</span> <span class=\"pl-c1\">3</span>; <span class=\"\
    pl-s1\">i</span><span class=\"pl-c1\">++</span> {\n        <span class=\"pl-s1\"\
    >job</span>.<span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>,\
    \ <span class=\"pl-s1\">fmt</span>.<span class=\"pl-en\">Sprintf</span>(<span\
    \ class=\"pl-s\">\"parallel task %d\"</span>, <span class=\"pl-s1\">i</span>))\n\
    \    }\n    <span class=\"pl-c1\">...</span></pre></div>\n<p>For missing functionality\
    \ or bugs please open an issue on github. Contributions welcome!</p>\n"
  stargazers_count: 33
  subscribers_count: 4
  topics:
  - docker
  - processes
  - cloud-foundry
  - k8s
  - workflow
  - hpc
  - macos
  - linux
  - high-throughput
  - singularity
  updated_at: 1622034364.0
djarecka/tmp_nipype_tut:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: djarecka/tmp_nipype_tut
  latest_release: null
  readme: '<h1>

    <a id="user-content-nipype-tutorial-notebooks" class="anchor" href="#nipype-tutorial-notebooks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nipype
    Tutorial Notebooks</h1>

    <p><a href="https://circleci.com/gh/miykael/nipype_tutorial/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64"
    alt="CircleCi" data-canonical-src="https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/issues/"><img src="https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/pulls/"><img src="https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub pull-requests" data-canonical-src="https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/"><img
    src="https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/commits/master"><img src="https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub Commits" data-canonical-src="https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/archive/master.zip"><img src="https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub size" data-canonical-src="https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/miykael/nipype_tutorial/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000"
    style="max-width:100%;"></a>

    <a href="http://hits.dwyl.io/miykael/nipype_tutorial" rel="nofollow"><img src="https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub HitCount" data-canonical-src="http://hits.dwyl.io/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a></p>

    <p>This is the Nipype Tutorial in Jupyter Notebook format. You can access the
    tutorial in two ways:</p>

    <ol>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/" rel="nofollow">Nipype Tutorial
    Homepage</a>: This website contains a static, read-only version of all the notebooks.</li>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html"
    rel="nofollow">Nipype Tutorial Docker Image</a>: This guide explains how to use
    Docker to run the notebooks interactively on your own computer. The nipype tutorial
    docker image is the best interactive way to learn Nipype.</li>

    </ol>

    <h1>

    <a id="user-content-feedback-help--support" class="anchor" href="#feedback-help--support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feedback,
    Help &amp; Support</h1>

    <p>If you want to help with this tutorial or have any questions, feel free to
    fork the repo of the <a href="https://github.com/miykael/nipype_tutorial">Notebooks</a>
    or interact with other contributors on the slack channel <a href="https://brainhack.slack.com/messages/nipype/"
    rel="nofollow">brainhack.slack.com/messages/nipype/</a>. If you have any questions
    or found a problem, open a new <a href="https://github.com/miykael/nipype_tutorial/issues">issue
    on github</a>.</p>

    <h1>

    <a id="user-content-thanks-and-acknowledgment" class="anchor" href="#thanks-and-acknowledgment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thanks
    and Acknowledgment</h1>

    <p>A huge thanks to <a href="https://github.com/mwaskom">Michael Waskom</a>, <a
    href="https://github.com/oesteban">Oscar Esteban</a>, <a href="https://github.com/chrisfilo">Chris
    Gorgolewski</a> and <a href="https://github.com/satra">Satrajit Ghosh</a> for
    their input to this tutorial! And a huge thanks to <a href="https://github.com/djarecka/">Dorota
    Jarecka</a> who updated this tutorial to Python 3 and is helping me with keeping
    this tutorial updated and running!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1547566090.0
dptools/dpdash:
  data_format: 2
  description: Deep phenotyping dashboard
  filenames:
  - singularity/Singularity
  full_name: dptools/dpdash
  latest_release: null
  readme: '<p>Read the documentation <a href="http://docs.neuroinfo.org/dpdash/en/latest/"
    rel="nofollow">here</a>!</p>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1622821974.0
dshyshlov/funannotate_singularity:
  data_format: 2
  description: Recipe for funannotate pipeline Singularity recipy for UA HPC
  filenames:
  - Singularity
  full_name: dshyshlov/funannotate_singularity
  latest_release: null
  readme: '<p>Singularity recipe files for SEX-DETector, a tool for the statistical
    inferrence of sex-linked genes from RNA / DNA reads from a cross (parents and
    set of childrens)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602202847.0
dt-np/raser:
  data_format: 2
  description: 'RAdiation SEmiconductoR '
  filenames:
  - Singularity
  full_name: dt-np/raser
  latest_release: v1.1
  readme: '<h1>

    <a id="user-content-raser" class="anchor" href="#raser" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>raser</h1>

    <p>RAdiation SEmiconductoR</p>

    <ol>

    <li>docker_build.sh  (.bat for Window)</li>

    <li>docker_run.sh (.bat for Windows)</li>

    <li>make (compile raser binary)</li>

    <li>./run  (run raser program)</li>

    </ol>

    <h1>

    <a id="user-content-windows-install-software" class="anchor" href="#windows-install-software"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>windows
    install software</h1>

    <p>1.Docker</p>

    <h1>

    <a id="user-content-add-fenics-in-dockerfile" class="anchor" href="#add-fenics-in-dockerfile"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>add
    fenics in dockerfile</h1>

    <p>2.Xming</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1624465934.0
duke-chsi-informatics/singularity-rnaseq:
  data_format: 2
  description: Singularity Image for RNA-Seq analysis
  filenames:
  - Singularity
  - Singularity.test_jupyter
  full_name: duke-chsi-informatics/singularity-rnaseq
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"\
    #singularity-rnaseq\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-rnaseq</h1>\n<h2>\n<a id=\"user-content-running-jupyter\"\
    \ class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Jupyter</h2>\n<p>Run\
    \ this to start Jupyter:</p>\n<pre><code>singularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\
    </code></pre>\n<p>Then follow the instructions that Jupyter printed to the terminal\
    \ when you started it up to access Jupyter in your web browser</p>\n<h3>\n<a id=\"\
    user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Accessing Jupyter on a remote server</h3>\n<p>If you are running the\
    \ container on a remote server, you will need to set up port forwarding with ssh\
    \ to be able to access Jupyter.  Run this command to forward the default Jupyter\
    \ port (8888)</p>\n<pre><code>ssh -L 8888:localhost:8888 bug\n</code></pre>\n\
    <blockquote>\n<p>Note if the default Jupyter port is not available, Jupyter will\
    \ choose a different port.  In this case you will need to substitute the port\
    \ that Jupyter outputs for 8888 in the ssh port forwarding command above.</p>\n\
    </blockquote>\n<h2>\n<a id=\"user-content-running-on-a-slurm-cluster\" class=\"\
    anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running on a SLURM Cluster</h2>\n\
    <p>You can use this image interactively on a SLURM-managed cluster by running\
    \ launching RStudio or Jupyter. The following instructions work on the Duke Compute\
    \ Cluster (DCC).  Doing this on other cluster will require some modification and\
    \ may not work, depending on how the cluster is configured.</p>\n<h3>\n<a id=\"\
    user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RStudio</h3>\n\
    <ol>\n<li>ssh to DCC login node: <code>ssh NETID@dcc-login-01.rc.duke.edu</code>\n\
    </li>\n<li>run tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n\
    <li>Run this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty\
    \ bash -i</code>\n</li>\n<li>Run <code>hostname -A</code> on compute node and\
    \ record results</li>\n<li>Run on the following on a compute node and note the\
    \ port, username, and password that the command prints:</li>\n</ol>\n<pre><code>mkdir\
    \ -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\n\
    singularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind\
    \ /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\
    </code></pre>\n<ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the port returned\
    \ but the \"singularity run\" commmand</li>\n<li>Where COMPUTE_HOSTNAME is the\
    \ hostname returned by running \"hostname -A\" on the compute node</li>\n<li>Where\
    \ NETID is your NetID</li>\n</ul>\n</li>\n<li>Go to \"localhost:PORT\" in a webrowser\
    \ and enter the username and password printed by the \"singularity run\" commmand</li>\n\
    <li>Have fun!!</li>\n<li>At the end of an analysis you will probably want to copy\
    \ results to your directory in <code>/work</code> or <code>/hpc/group</code>\n\
    </li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Jupyter</h3>\n<ol>\n<li>ssh to dcc-login-01.rc.duke.edu</li>\n<li>run\
    \ tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n<li>Run\
    \ this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty bash -i</code>\n\
    </li>\n<li>Run on compute node:</li>\n</ol>\n<pre><code>mkdir -p /scratch/josh/rnaseq_demo/rawdata\
    \ /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\
    \n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace\
    \ \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n</code></pre>\n\
    <ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the number after\
    \ <code>http://127.0.0.1:</code> in the URL given by Jupyter (defaults to 8888,\
    \ but Jupyter will use a different one if the default is in use, or if a different\
    \ port is supplied as an argument using <code>--port</code> when running the singularity\
    \ container</li>\n<li>Where COMPUTE_HOSTNAME is the hostname returned by running\
    \ \"hostname -A\" on the compute node</li>\n<li>Where NETID is your NetID</li>\n\
    </ul>\n</li>\n<li>Copy the URL supplied by jupyter that starts <code>http://127.0.0.1:</code>\
    \ and paste it in a webbrowser</li>\n<li>Have fun!!</li>\n<li>At the end of an\
    \ analysis you will probably want to copy results to your directory in <code>/work</code>\
    \ or <code>/hpc/group</code>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter-on-gpu-node\"\
    \ class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Jupyter on GPU node</h3>\n<p>Same\
    \ as above, but the srun command should use the <code>chsi-gpu</code> partition\
    \ and request a gpu, but less CPUs and Memory:</p>\n<p><code>srun -A chsi -p chsi-gpu\
    \ --gres=gpu:1 --mem=15866 -c 2 --pty bash -i</code></p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1619726636.0
edoapra/nwchem-singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - nwchem-702.ompi313.ivybridge/Singularity
  - nwchem-dev.ompi40x.skylake/Singularity
  - nwchem-dev.ompi40x.ifort.skylake/Singularity
  - nwchem-701.mpich321.ivybridge/Singularity
  - nwchem-701.ifort/Singularity
  - nwchem-701.ompi313.ivybridge/Singularity
  full_name: edoapra/nwchem-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-nwchem-singularity" class="anchor" href="#nwchem-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>NWChem
    singularity</h1>

    <p>Singularity recipes for NWChem</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1613067966.0
ejolly/IntroToSingularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ejolly/IntroToSingularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-getting-setup-with-singularity\" class=\"anchor\"\
    \ href=\"#getting-setup-with-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Setup with Singularity</h1>\n\
    <p><em>This is a guide to getting started with <a href=\"http://singularity.lbl.gov/\"\
    \ rel=\"nofollow\">Singularity containers</a> in conjunction with Dartmouth College's\
    \ <a href=\"http://techdoc.dartmouth.edu/discovery/\" rel=\"nofollow\">Discovery\
    \ HPC</a>.<br>\nQuestions can be addressed to <a href=\"mailto:eshin.jolly.gr@dartmouth.edu\"\
    >eshin.jolly.gr@dartmouth.edu</a> or <a href=\"mailto:mvdoc.gr@dartmouth.edu\"\
    >mvdoc.gr@dartmouth.edu</a>.<br>\nWe're not experts but we're happy to try to\
    \ help!</em></p>\n<h4>\n<a id=\"user-content-i-pre-requisites-osx-only\" class=\"\
    anchor\" href=\"#i-pre-requisites-osx-only\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"#prereqs\">I. Pre-requisites\
    \ (OSX only)</a>\n</h4>\n<h4>\n<a id=\"user-content-ii-creating-a-singularity-container\"\
    \ class=\"anchor\" href=\"#ii-creating-a-singularity-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"#creation\">II. Creating a Singularity container</a>\n</h4>\n<h4>\n<a\
    \ id=\"user-content-iii-basic-container-usage\" class=\"anchor\" href=\"#iii-basic-container-usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"#basicusage\">III. Basic container usage</a>\n</h4>\n<h4>\n\
    <a id=\"user-content-iv-using-a-container-on-discovery\" class=\"anchor\" href=\"\
    #iv-using-a-container-on-discovery\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"#discovery\">IV. Using\
    \ a container on Discovery</a>\n</h4>\n<h4>\n<a id=\"user-content-v-updating-an-existing-container\"\
    \ class=\"anchor\" href=\"#v-updating-an-existing-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"\
    #updating\">V. Updating an existing container</a>\n</h4>\n<h4>\n<a id=\"user-content-vi-sharing-containers\"\
    \ class=\"anchor\" href=\"#vi-sharing-containers\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"#sharing\"\
    >VI. Sharing containers</a>\n</h4>\n<h4>\n<a id=\"user-content-vii-extra-resources\"\
    \ class=\"anchor\" href=\"#vii-extra-resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"#resources\">VII. Extra\
    \ resources</a>\n</h4>\n<h2>\n<a id=\"user-content--pre-requisites-osx-only\"\
    \ class=\"anchor\" href=\"#-pre-requisites-osx-only\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-prereqs\"\
    ></a> Pre-requisites (OSX only!)</h2>\n<p><em>Because singularity runs primarily\
    \ on linux, we need to create a virtual linux environment on OSX in order to build/manipulate\
    \ singularity containers. Follow this step first if you're using OSX.</em></p>\n\
    <h4>\n<a id=\"user-content-install-homebrew-package-manager\" class=\"anchor\"\
    \ href=\"#install-homebrew-package-manager\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install Homebrew package manager</h4>\n\
    <p>Homebrew is a package manager for OSX similar to apt-get or yum on linux. It\
    \ allows you to download and install different software (e.g. wget, or curl) and\
    \ allows you to build your own packages. Just copy and run the command below:</p>\n\
    <pre><code>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\
    \n</code></pre>\n<h4>\n<a id=\"user-content-use-homebrew-to-install-vagrant\"\
    \ class=\"anchor\" href=\"#use-homebrew-to-install-vagrant\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use Homebrew\
    \ to install Vagrant</h4>\n<p>Vagrant is a virtual development environment that\
    \ can be used to create virtual-machines (kind of similar to Virtualbox, but much\
    \ more powerful). It can be used to install and run another operating system on\
    \ your computer that's completely independent from your host OS. First we're going\
    \ to install vagrant via Homebrew.</p>\n<pre><code>brew cask install Virtualbox\n\
    brew cask install vagrant\nbrew cask install vagrant-manager\n</code></pre>\n\
    <h4>\n<a id=\"user-content-use-vagrant-to-create-a-virtual-machine\" class=\"\
    anchor\" href=\"#use-vagrant-to-create-a-virtual-machine\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use Vagrant\
    \ to create a virtual machine</h4>\n<p>Now that we have vagrant installed, we\
    \ can use it to make a brand new linux- based virtual machine, <strong>within</strong>\
    \ which singularity will be installed. It's from inside this vm that we're going\
    \ to do all future singularity container creation, modification etc.</p>\n<p>First\
    \ let's create a folder that our virtual machine will live in.</p>\n<pre><code>mkdir\
    \ singularity-vm\ncd singularity-vm\n</code></pre>\n<p>Now lets download a <em>vagrantfile</em>\
    \ for a prebuilt Ubuntu system that already has singularity installed.</p>\n<pre><code>vagrant\
    \ init singularityware/singularity-2.4\n</code></pre>\n<p>Finally we can start\
    \ up virtual machine and move into it.</p>\n<pre><code>#If this is the first time\
    \ you're building the vm the vagrant up command might take a minute or so to complete\n\
    vagrant up\nvagrant ssh\n</code></pre>\n<p>Whenver you're done using a vagrant\
    \ vm just use <code>ctrl+c</code> to exit the machine and type <code>vagrant halt</code>\
    \ to shut it down.</p>\n<h2>\n<a id=\"user-content-creating-a-singularity-container\"\
    \ class=\"anchor\" href=\"#creating-a-singularity-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"\
    user-content-creation\"></a>Creating a Singularity container</h2>\n<p>Let's begin\
    \ by creating a new folder within our vm for our brand new container (this isn't\
    \ strictly necessary but nice to keep different containers organized):</p>\n<pre><code>mkdir\
    \ miniconda\ncd miniconda\n</code></pre>\n<p>The first thing we need to do in\
    \ order to create a singularity container is make a singularity <em>definition</em>\
    \ file. This is just an instruction set that singularity will use to create a\
    \ container. Think of this definition file as a recipe, and the container as the\
    \ final product. Within this recipe, specify everything you need to in order create\
    \ your custom analysis environment. Sharing this definition file with others will\
    \ enable them to identically reproduce the steps it took to create your container.</p>\n\
    <p>To get you started here's an example definition file that we're going to use\
    \ for this demo. This is a simple neurodebian flavored container with miniconda\
    \ installed along with numpy and scipy.<br>\nLet's save this to a file called\
    \ <code>miniconda.def</code></p>\n<pre><code># Singularity definition example\
    \ with miniconda\n# Matteo Visconti di Oleggio Castello; Eshin Jolly\n# mvdoc.gr@dartmouth.edu;\
    \ eshin.jolly.gr@dartmouth.edu\n# May 2017\n\nbootstrap: docker\nfrom: neurodebian:jessie\n\
    \n# this command assumes at least singularity 2.3\n%environment\n    PATH=\"/usr/local/anaconda/bin:$PATH\"\
    \n%post\n    # install debian packages\n    apt-get update\n    apt-get install\
    \ -y eatmydata\n    eatmydata apt-get install -y wget bzip2 \\\n      ca-certificates\
    \ libglib2.0-0 libxext6 libsm6 libxrender1 \\\n      git git-annex-standalone\n\
    \    apt-get clean\n\n    # install anaconda\n    if [ ! -d /usr/local/anaconda\
    \ ]; then\n         wget https://repo.continuum.io/miniconda/Miniconda2-4.3.14-Linux-x86_64.sh\
    \ \\\n            -O ~/anaconda.sh &amp;&amp; \\\n         bash ~/anaconda.sh\
    \ -b -p /usr/local/anaconda &amp;&amp; \\\n         rm ~/anaconda.sh\n    fi\n\
    \    # set anaconda path\n    export PATH=\"/usr/local/anaconda/bin:$PATH\"\n\n\
    \    # install the bare minimum\n    conda install\\\n      numpy scipy\n    conda\
    \ clean --tarballs\n\n    # make /data and /scripts so we can mount it to access\
    \ external resources\n    if [ ! -d /data ]; then mkdir /data; fi\n    if [ !\
    \ -d /scripts ]; then mkdir /scripts; fi\n\n%runscript\n    echo \"Now inside\
    \ Singularity container woah...\"\n    exec /bin/bash\n\n</code></pre>\n<p>Now\
    \ lets use our vagrant vm and create a blank singularity image allocating 4gb\
    \ of disk space within our container. You may need to adjust this depending on\
    \ how much software you plan to install. By default the vagrant vm will share\
    \ <code>/vagrant</code> with your host OS so lets perform our operation in there\
    \ within the container folder we created earlier.</p>\n<pre><code>vagrant up\n\
    vagrant ssh\ncd /vagrant/miniconda\n# Now let's build it!\nsudo singularity build\
    \ miniconda.img miniconda.def\n</code></pre>\n<h2>\n<a id=\"user-content-basic-container-usage\"\
    \ class=\"anchor\" href=\"#basic-container-usage\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-basicusage\"\
    ></a>Basic container usage</h2>\n<p>If all went well we should be able to issue\
    \ a python command to the python version installed <em>within</em> our container\
    \ like so:</p>\n<pre><code>singularity exec miniconda.img python -c 'print \"\
    Hello from Singularity!\"'\n</code></pre>\n<p>We can also open up our container\
    \ and work <em>inside</em> it interactively:</p>\n<pre><code>singularity run miniconda.img\n\
    conda list\n</code></pre>\n<p>Press <code>ctrl+d</code> to exit the container.</p>\n\
    <p>Most commonly you'll use one of three commands with a container:<br>\n<code>singularity\
    \ exec</code> to run a specific command/file/script using the container<br>\n\
    <code>singularity run</code> to move into a container and use it interactively;\
    \ what   gets run by this command is dictated by your singularity <em>definition</em>\
    \ file<br>\n<code>singularity shell</code> similar to above, but specifically\
    \ open up a shell within the container</p>\n<p>A few other useful flags include:\n\
    <code>-B</code> mount an external folder to the container<br>\n<code>-c</code>\
    \ don't automatically map /home and /tmp to shared folders with the host OS</p>\n\
    <h2>\n<a id=\"user-content-using-a-container-on-discovery\" class=\"anchor\" href=\"\
    #using-a-container-on-discovery\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><a name=\"user-content-discovery\"\
    ></a>Using a container on Discovery</h2>\n<p>In order to use a container on Discovery\
    \ you have to first upload the generated .img file to your home directory. Since\
    \ containers can be rather large lets compress this and then uncompress on Discovery\
    \ (starting with Singularity &gt;=2.3.0 this functionality works through <code>import</code>\
    \ and <code>export</code> commands)</p>\n<pre><code>tar -cvzf miniconda.tar.gz\
    \ miniconda.img\nscp miniconda.tar.gz ejolly@discovery.dartmouth.edu:~\nssh ejolly@discovery.dartmouth.edu\n\
    tar -xvzf miniconda.tar.gz\n</code></pre>\n<p>Now you can utilize the container\
    \ by loading the singularity module and utilizing any of the singularity commands\
    \ above. There is <strong>one catch</strong> however: by default singularity will\
    \ try to melt together any environment variables defined in your account on discovery\
    \ with environment variables defined within the container. The rationale behind\
    \ this is that singularity offers the ability to <em>seamlessly</em> blend a custom\
    \ environment (i.e. your container built with all your goodies) and the functionality\
    \ of your HPC (i.e. all the goodies that already exist on Discovery). However,\
    \ often times you want to turn this functionality off and only use environment\
    \ variables within your container to avoid conflicts (i.e. completely ignore environment\
    \ variables set on Discovery). Here's how we do that:</p>\n<pre><code>module load\
    \ singularity\nsingularity run -e miniconda.img\n</code></pre>\n<p>To make our\
    \ lives easier we can create a simple bash script that executes a command in our\
    \ container making sure to call it with all the extra flags we want (e.g. mounting\
    \ some folders, ignoring environment variables). I personally like to create two\
    \ scripts one for interactively working with a container and one for using it\
    \ to execute commands for example with job submission. Here are some examples,\
    \ you'll need to adapt them to mount the directories you want:<br>\nLet's save\
    \ the following code into a bash file called: exec_miniconda</p>\n<pre><code>#!/bin/bash\n\
    singularity -e  exec \\\n    -B /idata/lchang/Projects:/data \\\n    -B /ihome/ejolly/scripts/:/scripts\
    \ \\\n    miniconda.img \"$@\"\n</code></pre>\n<p>Let's save the following code\
    \ into a bash file called: interact_miniconda</p>\n<pre><code>#!/bin/bash\nsingularity\
    \ -e run \\\n\t-c \\\n\t-B /idata/lchang/Projects/Pinel:/data \\\n\t-B ~/scripts:/scripts\
    \ \\\n\tminiconda.img\n</code></pre>\n<p>Now we issue a command to our container\
    \ (e.g. when submitting a job) like this:</p>\n<pre><code>./exec_miniconda python\
    \ -c 'print \"Hello World!\"'\n</code></pre>\n<p>We can also use our container\
    \ interactively with. Here let's actually serve a jupyter notebook server from\
    \ the cluster and interact with it using our local web browser. To do so we need\
    \ to reconnect to Discovery with port-forwarding.  The demo container here isn't\
    \ built with a jupyter notebook so this won't work, but we you can use the same\
    \ command when building your own container</p>\n<pre><code># You should really\
    \ connect to something other than the head node here!\nssh ejolly@discovery.dartmouth.edu\
    \ -N -f -L localhost:3129:localhost:9999\n\n./exec_miniconda jupyter notebook\
    \ --no-browser --port=9999\n# On local machine navigate to localhost:3129 in a\
    \ web browser\n</code></pre>\n<h2>\n<a id=\"user-content-updating-an-existing-container\"\
    \ class=\"anchor\" href=\"#updating-an-existing-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"\
    user-content-updating\"></a>Updating an existing container</h2>\n<p>The preferred\
    \ way to update a container is to modify the definition file and rebuild the image\
    \ using the steps above. This ensures that any container image is always a product\
    \ of its definition file and is therefore easy to reproduce.</p>\n<p>However,\
    \ singularity makes it easy to make changes to an existing container as well using\
    \ the <code>--writable</code> flag with the <code>exec</code>, <code>run</code>,\
    \ or <code>shell</code> commands, e.g.</p>\n<pre><code>singularity exec --writable\
    \ miniconda.img apt-get install curl\n</code></pre>\n<p>You can also increase\
    \ the size of an existing container with the <code>expand</code> command, e.g.</p>\n\
    <pre><code>#Expand a container by 2gb\nsingularity expand --size 2048 miniconda.img\n\
    </code></pre>\n<h2>\n<a id=\"user-content-sharing-containers\" class=\"anchor\"\
    \ href=\"#sharing-containers\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><a name=\"user-content-sharing\"></a>Sharing\
    \ containers</h2>\n<p>One of the nice things about using singularity (and containers\
    \ in general) is that you can share your analysis environment with others. These\
    \ are served on <a href=\"https://singularity-hub.org\" rel=\"nofollow\">Singularity\
    \ hub</a>. Many prebuilt containers already exist that you easily download and\
    \ use.</p>\n<p>Let's say we want to use this <a href=\"https://singularity-hub.org/containers/105/\"\
    \ rel=\"nofollow\">container</a> prebuilt with tensor flow for GPUs. This is as\
    \ simple as:</p>\n<pre><code>singularity pull shub://researchapps/tensorflow:gpu\n\
    </code></pre>\n<p>Then you can setup run and execute scripts like above to use\
    \ it on Discovery.</p>\n<p>You can also easily share you custom container on Singularity\
    \ hub by committing your singularity definition file to github and flipping the\
    \ switch for that repository on singularity hub.</p>\n<h2>\n<a id=\"user-content-extra-resources\"\
    \ class=\"anchor\" href=\"#extra-resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-resources\"\
    ></a>Extra resources</h2>\n<p>Much of this tutorial is borrowed/integrated from\
    \ several helpful resources:</p>\n<h4>\n<a id=\"user-content-quick-guides\" class=\"\
    anchor\" href=\"#quick-guides\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Quick guides</h4>\n<p><a href=\"http://mvdoc.me/2017/using-singularity-to-make-analyses-reproducible.html\"\
    \ rel=\"nofollow\">Matteo Visconti's blogpost on getting started with singularity</a><br>\n\
    <a href=\"http://jinhyuncheong.com/jekyll/update/2016/07/24/How-to-use-the-Discovery-cluster.html\"\
    \ rel=\"nofollow\">Jin Cheong's quick guide to using the discovery cluster</a></p>\n\
    <h4>\n<a id=\"user-content-more-comprehensive-guides\" class=\"anchor\" href=\"\
    #more-comprehensive-guides\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>More comprehensive guides</h4>\n<p><a href=\"\
    http://singularity.lbl.gov/quickstart\" rel=\"nofollow\">Singularity Documentation</a><br>\n\
    <a href=\"http://techdoc.dartmouth.edu/discovery/\" rel=\"nofollow\">Discovery\
    \ Documentation</a></p>\n<h4>\n<a id=\"user-content-sharing-is-caring\" class=\"\
    anchor\" href=\"#sharing-is-caring\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sharing is caring</h4>\n<p><a\
    \ href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity hub</a><br>\n\
    <a href=\"https://hub.docker.com/\" rel=\"nofollow\">Docker hub</a></p>\n"
  stargazers_count: 11
  subscribers_count: 2
  topics: []
  updated_at: 1624918389.0
emilydolson/force-cover:
  data_format: 2
  description: Modify C++ test coverage reports to show uninstantiated templates
  filenames:
  - Singularity
  full_name: emilydolson/force-cover
  latest_release: v2.0
  readme: "<h1>\n<a id=\"user-content-force-cover\" class=\"anchor\" href=\"#force-cover\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Force-cover</h1>\n<p><a href=\"https://github.com/emilydolson/force-cover/actions/workflows/tests.yml\"\
    ><img src=\"https://github.com/emilydolson/force-cover/actions/workflows/tests.yml/badge.svg\"\
    \ alt=\"Build\" style=\"max-width:100%;\"></a> <a href=\"https://codecov.io/gh/emilydolson/force-cover\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c930205a7ee3eef3e56759777419a15908deea8038f1bf0a91c39ef8e6da97cc/68747470733a2f2f636f6465636f762e696f2f67682f656d696c79646f6c736f6e2f666f7263652d636f7665722f6272616e63682f6d61737465722f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/emilydolson/force-cover/branch/master/graph/badge.svg\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/cd7aa456de7148b9cd70c63dab27300c5ae46df596766aa915c226c27c590490/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f656d696c79646f6c736f6e2f666f7263652d636f7665722e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/cd7aa456de7148b9cd70c63dab27300c5ae46df596766aa915c226c27c590490/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f656d696c79646f6c736f6e2f666f7263652d636f7665722e737667\"\
    \ alt=\"GitHub release\" data-canonical-src=\"https://img.shields.io/github/release/emilydolson/force-cover.svg\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://github.com/emilydolson/force-cover/issues\"\
    ><img src=\"https://camo.githubusercontent.com/f5054ffcd4245c10d3ec85ef059e07aacf787b560f83ad4aec2236364437d097/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174\"\
    \ alt=\"contributions welcome\" data-canonical-src=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://singularity-hub.org/collections/3916\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Getting accurate test coverage information\
    \ about C++ code containing templates is challenging; uninstantiated templates\
    \ don't make it into the compiled binary, so compilers don't instrument them for\
    \ coverage tracking (i.e. if you never use a template the compiler thinks it isn't\
    \ runnable code and doesn't count it as lines that should be covered). Since templates\
    \ with no test coverage are likely to never get instantiated this results in overly\
    \ accurate test coverage metrics.</p>\n<p>Force-cover is a set of tools for dealing\
    \ with this problem. It consists of two parts:</p>\n<ul>\n<li>a C++ program (built\
    \ with Clang Libtooling) that reads your C++ code, finds the templates, and sticks\
    \ comments before and after them to indicate that they should be covered.</li>\n\
    <li>a python program that looks at the final test coverage output, finds the macros,\
    \ and adjusts the file as necessary to indicate that uncovered template code should\
    \ be counted as uncovered code.</li>\n</ul>\n<h2>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<ul>\n<li>Python\
    \ (any version)</li>\n<li>clang (version 7+) (for version 6, use <a href=\"https://github.com/emilydolson/force-cover/releases/tag/v1.5\"\
    >this release of force-cover</a>)</li>\n<li>libclang-dev (version 7+ - must be\
    \ same version as clang)</li>\n</ul>\n<p>Theoretically force-cover should work\
    \ on any operating system, but it's currently only been tested on Ubuntu and Linux\
    \ Mint.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"\
    #installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Installation</h2>\n<p>You can install the requirements\
    \ on Ubuntu-flavored Linux with:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo apt install -y clang llvm-dev</pre></div>\n<p>You can build force-cover\
    \ by cloning this repo and running Make inside it:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>git clone https://github.com/emilydolson/force-cover.git\n\
    <span class=\"pl-c1\">cd</span> force-cover\nmake</pre></div>\n<p>This will create\
    \ the force_cover executable. No additional work is needed to set up the Python\
    \ script.</p>\n<h3>\n<a id=\"user-content-troubleshooting\" class=\"anchor\" href=\"\
    #troubleshooting\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Troubleshooting</h3>\n<p>If you have multiple versions\
    \ of clang or llvm on your computer, the Make command may fail. You may be able\
    \ to fix this by changing the default version as described at the bottom of <a\
    \ href=\"https://blog.kowalczyk.info/article/k/how-to-install-latest-clang-6.0-on-ubuntu-16.04-xenial-wsl.html\"\
    \ rel=\"nofollow\">this page</a>. Alternatively, you can modify the Makefile to\
    \ include absolute paths to the installation location. Set LLVM_SRC_PATH equal\
    \ to the path to your llvm installation location (e.g. <code>/usr/lib/llvm-11</code>).\
    \ Uncomment the <code>LLVM_CONFIG := $(LLVM_BIN_PATH)/llvm-config</code> line\
    \ and comment out the line above it.</p>\n<p>Alternately, save yourself a trip\
    \ through install hell by using a containerized environment a la <a href=\"https://sylabs.io/singularity/\"\
    \ rel=\"nofollow\">Singularity</a>!\nBuild from our handy-dandy Singularity recipe\
    \ (<code>sudo singularity build force-cover.simg Singularity</code>) or grab a\
    \ pre-built container from SingularityHub (<code>singularity pull --name \"force-cover.simg\"\
    \ shub://emilydolson/force-cover</code>).\nThen, hop on to an interactive shell\
    \ by <code>singularity shell force-cover.simg</code>.\nCowabunga!</p>\n<h2>\n\
    <a id=\"user-content-quick-start-guide\" class=\"anchor\" href=\"#quick-start-guide\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quick-start guide</h2>\n<p>Here is the basic sequence of commands\
    \ you need to execute to use force-cover with LLVM Source-Based coverage (the\
    \ recommended approach):</p>\n<pre lang=\"none\"><code>./force_cover [C++ code\
    \ file to be evaluated] -- [any flags you would pass to the compiler when compiling\
    \ this program] &gt; [name of file to store modified code in]\nclang++ -fprofile-instr-generate\
    \ -fcoverage-mapping -O0 -fno-inline -fno-elide-constructors [.cpp file] -o [executable\
    \ name]\n[run executable]\nllvm-profdata merge default.profraw -o default.profdata\n\
    llvm-cov show [executable name] -instr-profile=default.profdata &gt; coverage.txt\n\
    python fix_coverage.py coverage.txt\n</code></pre>\n<p>Example (using included\
    \ example.cc file):</p>\n<div class=\"highlight highlight-source-shell\"><pre>./force_cover\
    \ examples/example.cc -- --language c++ -std=c++11 <span class=\"pl-k\">&gt;</span>\
    \ examples/example_with_template_coverage_info.cc\nclang++ -fprofile-instr-generate\
    \ -fcoverage-mapping -O0 -fno-inline -fno-elide-constructors examples/example_with_template_coverage_info.cc\
    \ -o example\n./example\nllvm-profdata merge default.profraw -o default.profdata\n\
    llvm-cov show ./example -instr-profile=default.profdata <span class=\"pl-k\">&gt;</span>\
    \ coverage.txt\npython fix_coverage.py coverage.txt</pre></div>\n<h2>\n<a id=\"\
    user-content-using-force-cover-in-detail\" class=\"anchor\" href=\"#using-force-cover-in-detail\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using force-cover (in detail)</h2>\n<p>The workflow for using force-cover\
    \ is as follows:</p>\n<ul>\n<li>Run all of your C++ code through the force_cover\
    \ C++ program to insert comments.</li>\n<li>Compile your program using appropriate\
    \ flags for your compiler to indicate that you want to measure test coverage on\
    \ this program</li>\n<li>Run your program</li>\n<li>Run your coverage program</li>\n\
    <li>Run the python script on the coverage program's output</li>\n</ul>\n<p>In\
    \ theory, this should be possible with a variety of compilers and code coverage\
    \ programs. Thus far, I have only tested it with LLVM Source Based coverage. If\
    \ you have tested it and found that it worked with a different toolchain, let\
    \ me know so I can add it to this documentation!</p>\n<h3>\n<a id=\"user-content-step-1-run-force_cover-on-your-code\"\
    \ class=\"anchor\" href=\"#step-1-run-force_cover-on-your-code\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 1: Run force_cover on your code</h3>\n<p>The syntax for running the force_cover\
    \ C++ program is:</p>\n<pre lang=\"none\"><code>./force_cover [C++ code file to\
    \ be evaluated] -- [any flags you would pass to the compiler when compiling this\
    \ program]\n</code></pre>\n<p>For instance, to run it on the example you could\
    \ use:</p>\n<div class=\"highlight highlight-source-shell\"><pre>./force_cover\
    \ examples/example.cc -- --language c++ -std=c++11</pre></div>\n<p>By default,\
    \ it prints the modified version of the code to stdout. In order to compile programs\
    \ using the modified code, you'll need to pipe this new code to a file. For instance:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./force_cover examples/example.cc\
    \ -- --language c++ -std=c++11 <span class=\"pl-k\">&gt;</span> examples/example_with_template_coverage_info.cc</pre></div>\n\
    <p>For larger code-bases, one option is to make a copy of your code, rewrite all\
    \ of the files in the copy, and use those files to compile your tests. This can\
    \ be achieved with a few lines of bash code. For instance, let's say you're writing\
    \ a header-only library and all of the headers live in a directory called <code>source</code>.\
    \ You could run the following code:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>cp -r <span class=\"pl-c1\">source</span> coverage_source\n<span class=\"\
    pl-k\">for</span> <span class=\"pl-smi\">filename</span> <span class=\"pl-k\"\
    >in</span> <span class=\"pl-s\"><span class=\"pl-pds\">`</span>find ../coverage_source\
    \ -name <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.h<span class=\"\
    pl-pds\">\"</span></span><span class=\"pl-pds\">`</span></span>\n<span class=\"\
    pl-k\">do</span>\n    ./force_cover <span class=\"pl-smi\">$filename</span> --\
    \ -I../coverage_source --language c++ -std=c++14 <span class=\"pl-k\">|</span>\
    \ xargs -0 <span class=\"pl-c1\">echo</span> <span class=\"pl-k\">&gt;</span>\
    \ <span class=\"pl-smi\">$filename</span>.temp\n    mv <span class=\"pl-smi\"\
    >$filename</span>.temp <span class=\"pl-smi\">$filename</span>\n<span class=\"\
    pl-k\">done</span></pre></div>\n<p>Then when you go to compile your tests for\
    \ coverage, instead of including <code>source</code> you would include <code>coverage_source</code>\
    \ (i.e. replace <code>-Isource</code> with <code>-Icoverage_source</code>).</p>\n\
    <p>If you are running tests on a continuous integration platform you may choose\
    \ to skip the step of copying the code to a different directory. Just be aware\
    \ that <strong>this is dangerous because it will overwrite your code</strong>.</p>\n\
    <h3>\n<a id=\"user-content-step-2-compile-your-program\" class=\"anchor\" href=\"\
    #step-2-compile-your-program\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Step 2: Compile your program</h3>\n\
    <p>In order to get coverage information, you need to compile your program with\
    \ coverage instrumentation turned on. This can be achieved by passing a few flags\
    \ to the compiler. In LLVM, there are a number of different systems of coverage\
    \ instrumentation. The one I have had by far the most luck with is Source Based\
    \ coverage, which can be enabled with the <code>-fprofile-instr-generate</code>\
    \ and  <code>-fcoverage-mapping</code> flags. The other version, which mirrors\
    \ GCC's gcov system, sometimes optimizes unused class methods out of the binary,\
    \ preventing them from getting appropriately flagged as not covered.</p>\n<p>Some\
    \ other useful flags to prevent the compiler from making optimizations that hide\
    \ uncovered code are: <code>-O0 -fno-inline -fno-elide-constructors</code>.</p>\n\
    <p>So your compilation step will probably look something like:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>clang++ -fprofile-instr-generate -fcoverage-mapping\
    \ -O0 -fno-inline -fno-elide-constructors examples/example_with_template_coverage_info.cc\
    \ -o example</pre></div>\n<p>Note that Source Based coverage is only available\
    \ in clang. Theoretically, the tools in this repo should work on code instrumented\
    \ in other ways but, as mentioned before, it hasn't been tested on them.</p>\n\
    <h3>\n<a id=\"user-content-step-3-run-your-program\" class=\"anchor\" href=\"\
    #step-3-run-your-program\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 3: Run your program</h3>\n<p>The most straightforward\
    \ step! Run your program so that the coverage instrumentation can record which\
    \ lines were executed.</p>\n<p>For instance:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>./example</pre></div>\n<h3>\n<a id=\"user-content-step-4-extract-coverage-information\"\
    \ class=\"anchor\" href=\"#step-4-extract-coverage-information\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 4: Extract coverage information</h3>\n<p>Now that you've run your program, coverage\
    \ data exists but it's probably not in an easy-to-interpret form. You'll have\
    \ to run a program to extract it. For LLVM Source Based coverage, that will look\
    \ like:</p>\n<div class=\"highlight highlight-source-shell\"><pre>llvm-profdata\
    \ merge default.profraw -o default.profdata\nllvm-cov show ./example -instr-profile=default.profdata\
    \ <span class=\"pl-k\">&gt;</span> coverage.txt</pre></div>\n<p>This processes\
    \ the raw coverage data and then compares that information to the executable to\
    \ generate a report indicating the number of time each line was executed. Specifically,\
    \ the format should look like this:</p>\n<pre lang=\"none\"><code>[line_number]\
    \ |     [times_line_executed]|  [code from source file]\n</code></pre>\n<p>Whatever\
    \ compiler and tools you used, you need to end up with data in this format for\
    \ step 5 to work. Fortunately, it seems to be a relatively common format (Note:\
    \ if anyone knows the actual name of this format, send me a PR! I wrote this tool\
    \ because I needed it and thought others might too, not because I'm some kind\
    \ of code coverage expert).</p>\n<h3>\n<a id=\"user-content-step-5-run-fix_coveragepy\"\
    \ class=\"anchor\" href=\"#step-5-run-fix_coveragepy\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 5: Run fix_coverage.py</h3>\n\
    <p>For the final step, run fix_coverage.py on your output file from the previous\
    \ step. <strong>Note that this will overwrite your output file</strong>.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>python fix_coverage.py coverage.txt</pre></div>\n\
    <p>This script will go through and find all of the regions that are erroneously\
    \ being excluded from coverage analysis and modify the coverage file to indicate\
    \ that they should be covered but are not.</p>\n<h3>\n<a id=\"user-content-step-6-profit\"\
    \ class=\"anchor\" href=\"#step-6-profit\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 6: Profit!</h3>\n<p>Ta-da!\
    \ You have code coverage data that includes uninstantiated templates! You can\
    \ look at the file directly, or pass it along to a service like <a href=\"https://codecov.io\"\
    \ rel=\"nofollow\">codecov</a> that will give you a more user-friendly way to\
    \ examine your coverage (codecov's documentation on using llvm-cov isn't super\
    \ clear, but it will accept files in this format with names matching the pattern\
    \ <code>coverage*.txt</code>).</p>\n<h2>\n<a id=\"user-content-caveats\" class=\"\
    anchor\" href=\"#caveats\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Caveats</h2>\n<p>Code coverage is a flawed metric.\
    \ Just because a line of code is executed doesn't mean it's being rigorously tested.\
    \ This is especially true for templates, since different instantiations of the\
    \ same template could be wildly different from each other. That's the whole reason\
    \ uninstantiated templates don't get included in the binary in the first place:\
    \ template definitions only have a meaning with an appropriate set of arguments.\
    \ Force-cover can increase the accuracy of your code coverage and alert you to\
    \ uninstantiated templates, but it can't guarantee that your tests are actually\
    \ good.</p>\n<h2>\n<a id=\"user-content-bugs-contributions\" class=\"anchor\"\
    \ href=\"#bugs-contributions\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Bugs? Contributions?</h2>\n<p>Open\
    \ an issue or send me a PR! I'm not an expert on this stuff, so I'm sure there\
    \ are myriad ways force-cover could be better. I welcome all contributions. The\
    \ code is pretty succinct, so hopefully it's not too overwhelming to wade into.</p>\n\
    <p>In particular I would love to receive:</p>\n<ul>\n<li>Additional rules for\
    \ <code>validate_line</code> in <code>fix_coverage.py</code>. Its goal is to detect\
    \ lines that should not be marked as potentially coverable (e.g. lines containing\
    \ only comments). I wrote some very basic rules, but I'm sure there are a bunch\
    \ of edge cases it's missing.</li>\n<li>Improvements to the AST matching rules\
    \ in <code>force_cover.cpp</code>. I'm sure there are edge cases that they're\
    \ currently missing. Also in general they're a little overzealous at this point\
    \ (in mostly harmless ways).</li>\n<li>There is probably a smoother way to do\
    \ all of this (e.g. one that doesn't require both a pre-processing step and a\
    \ post-processing step). Potential options (some of which I tried and gave up\
    \ on):\n<ul>\n<li>Automatically add code that instantiates templates. Problem:\
    \ you need to know what types to instantiate them with.</li>\n<li>Detect uninstantiated\
    \ templates and replace them with an equivalent number of lines of non-templated\
    \ code. Problem: detecting uninstantiated templates is non-trivial.</li>\n<li>Ditch\
    \ the preprocessing script and let Python find templates in the coverage output.\
    \ Problem: probably requires parsing C++ in Python (although there are Python\
    \ bindings for clang libtools... they're just really poorly documented).</li>\n\
    </ul>\n</li>\n</ul>\n"
  stargazers_count: 5
  subscribers_count: 1
  topics:
  - test-coverage
  - llvm-cov
  - libtooling
  updated_at: 1624480734.0
evanfloden/dpa-analysis:
  data_format: 2
  description: Nextflow Pipeline for the analysis of Double Progressive Alignment
    (DPA)
  filenames:
  - singularity/Singularity
  - singularity/.ipynb_checkpoints/Singularity-checkpoint
  full_name: evanfloden/dpa-analysis
  latest_release: v0.2.6
  readme: '<h1>

    <a id="user-content-fast-and-accurate-large-multiple-sequence-alignments-using-root-to-leave-regressive-computation"
    class="anchor" href="#fast-and-accurate-large-multiple-sequence-alignments-using-root-to-leave-regressive-computation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Fast
    and accurate large multiple sequence alignments using root-to-leave regressive
    computation</h1>

    <p>This repository contains data, documentation, analysis and Nextflow workflow
    for the manuscript "Fast and accurate large multiple sequence alignments using
    root-to-leave regressive computation".</p>

    <h4>

    <a id="user-content-for-details-on-how-to-use-the-regressive-multiple-sequence-alignment-method-see-the-t-coffee-documentation"
    class="anchor" href="#for-details-on-how-to-use-the-regressive-multiple-sequence-alignment-method-see-the-t-coffee-documentation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>For
    details on how to use the Regressive Multiple Sequence Alignment method, see the
    <a href="https://tcoffee.readthedocs.io/en/latest/tcoffee_quickstart_regressive.html"
    rel="nofollow">T-Coffee documentation</a>.</h4>

    <h3>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>

    <p>This workflow was written by Evan Floden (<a href="https://github.com/evanfloden">evanfloden</a>)
    and

    Edgar(<a href="https://github.com/edgano">edgano</a>) at the <a href="http://www.crg.eu"
    rel="nofollow">Center for Genomic Regulation (CRG)</a>.</p>

    <p>The authors who contributed to the analysis and manuscript are:</p>

    <ul>

    <li>Edgar Garriga Nogales</li>

    <li>Paolo Di Tommaso</li>

    <li>Cedrik Magis</li>

    <li>Ionas Erb</li>

    <li>Hafid Laayouni</li>

    <li>Fyodor Kondrashov</li>

    <li>Evan Floden</li>

    <li>Cedric Notredame</li>

    </ul>

    <h3>

    <a id="user-content-notebooks" class="anchor" href="#notebooks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notebooks</h3>

    <p>This repository contains a series of <a href="http://jupyter.org/" rel="nofollow">Jupyter
    Notebooks</a> that contain

    the steps for replicating the analysis, tables and figures in the manuscript.</p>

    <p>The index jupyter notebook can be found <a href="notebook/00_StartHere.ipynb">here</a>.</p>

    <p>The notebook executes the pipeline, some steps of which require a lot of resources.</p>

    <h3>

    <a id="user-content-pipeline" class="anchor" href="#pipeline" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline</h3>

    <p>The pipeline for generating trees, alignments and performing the evaluations
    is built using

    <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>, a workflow tool
    to run tasks across

    multiple compute infrastructures in a very portable manner. It comes with a docker
    container

    making installation trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-pipeline-quick-start" class="anchor" href="#pipeline-quick-start"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    Quick Start</h3>

    <p>Make sure you have either docker/singularity installed or the required dependencies
    listed

    in the last section.</p>

    <p>Install the Nextflow runtime by running the following command:</p>

    <pre><code>$ curl -fsSL get.nextflow.io | bash

    </code></pre>

    <p>When done, you can launch the pipeline execution by entering the command shown
    below:</p>

    <pre><code>$ nextflow run evanfloden/dpa-analysis

    </code></pre>

    <p>By default the pipeline is executed against the provided example dataset.

    Check the <em>Pipeline parameters</em>  section below to see how enter your data
    on the program

    command line.</p>

    <h3>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Containers</h3>

    <p>All the methods above are available in a <a href="http://www.docker.com" rel="nofollow">Docker</a>
    image on DockerHub <a href="https://hub.docker.com/r/cbcrg/regressive-msa/" rel="nofollow">here</a>
    and the image is tested to be compatible with the <a href="http://singularity.lbl.gov/"
    rel="nofollow">Singularity</a>.</p>

    <p>The container also contains test data consisting of protein sequences, reference
    alignments and trees in the directory <code>/test_data</code>.</p>

    <p>To launch the container interactively with Docker run:</p>

    <p><code>docker run cbcrg/regressive-msa</code></p>

    <p>To launch the container interactivly with Singularity run:</p>

    <p><code>singularity shell docker://cbcrg/regressive-msa</code></p>

    <h3>

    <a id="user-content-pipeline-parameters" class="anchor" href="#pipeline-parameters"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    parameters</h3>

    <h4>

    <a id="user-content---seqs" class="anchor" href="#--seqs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--seqs</code>

    </h4>

    <ul>

    <li>Specifies the location of the input <em>fasta</em> file(s).</li>

    <li>Multiple files can be specified using the usual wildcards (*, ?), in this
    case make sure to surround the parameter string

    value by single quote characters (see the example below)</li>

    </ul>

    <p>Example:</p>

    <pre><code>$ nextflow run evanfloden/dpa-analysis --seqs ''/home/seqs/*.fasta''

    </code></pre>

    <p>This will handle each fasta file as a seperate sample.</p>

    <h4>

    <a id="user-content---refs" class="anchor" href="#--refs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--refs</code>

    </h4>

    <ul>

    <li>Specifies the location of the reference <em>aligned fasta</em> file(s).</li>

    </ul>

    <h4>

    <a id="user-content---trees" class="anchor" href="#--trees" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--trees</code>

    </h4>

    <ul>

    <li>Specifies the location of input tree file(s).</li>

    </ul>

    <h4>

    <a id="user-content---align_method" class="anchor" href="#--align_method" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--align_method</code>

    </h4>

    <ul>

    <li>Specifies which alignment methods should be used.</li>

    <li>Options include: "CLUSTALO,MAFFT-FFTNS1,MAFFT-SPARSECORE,MAFFT-GINSI,PROBCONS,UPP"</li>

    </ul>

    <h4>

    <a id="user-content---tree_method" class="anchor" href="#--tree_method" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--tree_method</code>

    </h4>

    <ul>

    <li>Specifies which guide-tree / clustering methods should be used.</li>

    <li>Options include: "CLUSTALO,MAFFT_PARTTREE"</li>

    </ul>

    <h4>

    <a id="user-content---regressive_align" class="anchor" href="#--regressive_align"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--regressive_align</code>

    </h4>

    <ul>

    <li>Flag to generate regressive MSAs.</li>

    <li>See <code>templates/dpa_align</code> for the specific commands executed.</li>

    </ul>

    <h4>

    <a id="user-content---stardard_align" class="anchor" href="#--stardard_align"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--stardard_align</code>

    </h4>

    <ul>

    <li>Flag to perform standard MSAs.</li>

    <li>Standard MSA is alignment where the guide-tree is provided as input.</li>

    <li>See <code>templates/std_align</code> for the specific commands executed.</li>

    </ul>

    <h4>

    <a id="user-content---default_align" class="anchor" href="#--default_align" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--default_align</code>

    </h4>

    <ul>

    <li>Flag to perform default MSAs.</li>

    <li>Default MSA is alignment where the alignment software uses an internally generated
    guide-tree.</li>

    <li>See <code>templates/default_align</code> for the specific commands executed.</li>

    </ul>

    <h4>

    <a id="user-content---evaluate" class="anchor" href="#--evaluate" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--evaluate</code>

    </h4>

    <ul>

    <li>Flag to perform evaluation of the alignments.</li>

    <li>Requires reference sequences to be provided with the <code>--refs</code> parameter.</li>

    </ul>

    <h4>

    <a id="user-content---buckets" class="anchor" href="#--buckets" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--buckets</code>

    </h4>

    <ul>

    <li>List of bucket sizes or maximum size of the subMSAs in the regressive proceedure.</li>

    <li>Default value is "1000" sequences.</li>

    </ul>

    <h4>

    <a id="user-content---output" class="anchor" href="#--output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--output</code>

    </h4>

    <ul>

    <li>Location of the results.</li>

    <li>Default locations is <code>results</code> directory.</li>

    </ul>

    '
  stargazers_count: 5
  subscribers_count: 0
  topics: []
  updated_at: 1620129144.0
faustus123/hdsingularity:
  data_format: 2
  description: Repository used to build Singularity containers of HD software
  filenames:
  - Singularity
  full_name: faustus123/hdsingularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-hdsingularity" class="anchor" href="#hdsingularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>hdsingularity</h1>

    <p>Repository used to build Singularity containers of HD software</p>

    <p>Checkout singularity-hub.org for details</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1501591637.0
federatedcloud/pulsar-pipeline-container:
  data_format: 2
  description: A Docker/Singularity container for packaging pulsar searching software
  filenames:
  - Singularity
  full_name: federatedcloud/pulsar-pipeline-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-docker-pulsar-pipeline" class="anchor" href="#docker-pulsar-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>docker-pulsar-pipeline</h1>

    <p>A Docker/Singularity container for packaging pulsar searching software</p>

    <p><a href="https://singularity-hub.org/collections/4541" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1622819898.0
federatedcloud/singularity-PRESTO:
  data_format: 2
  description: A base Singularity container for PRESTO, including dependencies and
    environment, converted from docker-PRESTO
  filenames:
  - Singularity
  full_name: federatedcloud/singularity-PRESTO
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-presto" class="anchor" href="#singularity-presto"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-PRESTO</h1>

    <p>A base Singularity container for PRESTO, including dependencies and environment,
    converted from docker-PRESTO</p>

    <p><a href="https://singularity-hub.org/collections/4510" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1622819998.0
frankwillmore/alcf-singularity:
  data_format: 2
  description: local settings
  filenames:
  - examples/arch/Singularity
  - examples/asciinema/Singularity
  - examples/busybox/Singularity
  - examples/shub/Singularity
  - examples/apps/Singularity
  - examples/apps/Singularity.cowsay
  - examples/ubuntu/Singularity
  - examples/instances/Singularity
  - examples/docker/Singularity
  - examples/raspbian/Singularity
  - examples/centos/Singularity
  - examples/scratch/Singularity.busybox
  - examples/scratch/Singularity.alpine
  - examples/opensuse/Singularity
  - examples/multistage/Singularity
  - examples/scientific/Singularity
  - examples/debian/Singularity
  - examples/library/Singularity
  - examples/self/Singularity
  full_name: frankwillmore/alcf-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <p><a href="https://travis-ci.org/sylabs/singularity" rel="nofollow"><img src="https://camo.githubusercontent.com/a1646c42a348a1331feb3842e34171e866c139adbae2608ba5fbd2c022c9c20f/68747470733a2f2f7472617669732d63692e6f72672f73796c6162732f73696e67756c61726974792e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/sylabs/singularity.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://circleci.com/gh/sylabs/singularity/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg"
    style="max-width:100%;"></a>

    <a href="https://goreportcard.com/report/github.com/sylabs/singularity" rel="nofollow"><img
    src="https://camo.githubusercontent.com/179d3d939b6a64c4f021860776fdc6243bc26409e966f1aa6bd7d35ca9593fea/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f73796c6162732f73696e67756c6172697479"
    alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/sylabs/singularity"
    style="max-width:100%;"></a></p>

    <ul>

    <li><a href="CONTRIBUTING.md">Guidelines for Contributing</a></li>

    <li><a href=".github/PULL_REQUEST_TEMPLATE.md">Pull Request Template</a></li>

    <li><a href="LICENSE.md">Project License</a></li>

    <li><a href="https://www.sylabs.io/docs/" rel="nofollow">Documentation</a></li>

    <li><a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459"
    rel="nofollow">Citation</a></li>

    </ul>

    <p>Singularity is an open source container platform designed to be simple, fast,
    and secure. Singularity is optimized for <a href="https://www.sylabs.io/2018/09/singularity-is-enterprise-performance-computing/"
    rel="nofollow">EPC</a> and HPC workloads, allowing untrusted users to run untrusted
    containers in a trusted way.</p>

    <p>Check out <a href="https://www.sylabs.io/singularity/whos-using-singularity/"
    rel="nofollow">who is using Singularity</a> and some <a href="https://www.sylabs.io/category/how-tos/"
    rel="nofollow">use cases of Singularity</a> on our website.</p>

    <h2>

    <a id="user-content-getting-started-with-singularity" class="anchor" href="#getting-started-with-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with Singularity</h2>

    <p>To install Singularity from source, see the <a href="INSTALL.md">installation
    instructions</a>. For other installation options, see <a href="https://www.sylabs.io/guides/3.0/user-guide/installation.html"
    rel="nofollow">our website</a>.</p>

    <p>For system administrators, see the <a href="https://www.sylabs.io/guides/3.0/admin-guide/"
    rel="nofollow">administrator documentation</a>.</p>

    <p>For users, see the <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow">user
    documentation</a>.</p>

    <h2>

    <a id="user-content-contributing-to-singularity" class="anchor" href="#contributing-to-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    to Singularity</h2>

    <p>Community contributions are always greatly appreciated. To start developing
    Singularity, check out the <a href="CONTRIBUTING.md">guidelines for contributing</a>.</p>

    <p>We also welcome contributions to our <a href="https://github.com/sylabs/singularity-userdocs">user
    docs</a> and <a href="https://github.com/sylabs/singularity-admindocs">admin docs</a>.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>To get help with Singularity, check out the <a href="https://www.sylabs.io/singularity/community/"
    rel="nofollow">Community Portal</a>.</p>

    <p>For additional support, <a href="https://www.sylabs.io/contact/" rel="nofollow">contact
    us</a> to receive more information.</p>

    <h2>

    <a id="user-content-cite-as" class="anchor" href="#cite-as" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cite as:</h2>

    <pre><code>Kurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers
    for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459

    </code></pre>

    <p>We also have a Zenodo citation:</p>

    <pre><code>Kurtzer, Gregory M.. (2016). Singularity 2.1.2 - Linux application
    and environment

    containers for science. 10.5281/zenodo.60736


    https://doi.org/10.5281/zenodo.60736

    </code></pre>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p><em>Unless otherwise noted, this project is licensed under a 3-clause BSD license
    found in the <a href="LICENSE.md">license file</a>.</em></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1558040154.0
fredjaya/rec-bench:
  data_format: 2
  description: automated benchmarking of recombination detection methods
  filenames:
  - Singularity
  - simg/Singularity.3seq
  full_name: fredjaya/rec-bench
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-rec-bench\" class=\"anchor\" href=\"#rec-bench\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>rec-bench</h1>\n<p>Automated benchmarking of recombination detection\
    \ methods</p>\n<p>Eternally a WIP - many things are hardcoded</p>\n<h2>\n<a id=\"\
    user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Dependencies</h2>\n\
    <p>Nextflow\nconda</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h2>\n<pre><code>git clone https://github.com/fredjaya/rec-bench.git\n\
    </code></pre>\n<p>Nextflow doesn't appear to create the conda environment properly.\
    \ Create manually.</p>\n<pre><code>conda env create -f environment.yml\nconda\
    \ activate fredjaya-rec-bench-0.1.0\n</code></pre>\n<p>Note: conda processes currently\
    \ hardcoded in <code>main.nf</code></p>\n<h2>\n<a id=\"user-content-usage\" class=\"\
    anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Usage</h2>\n<p><code>rec-bench</code> has five\
    \ modes that must be specified with <code>--mode</code> as follows:</p>\n<p><code>--mode\
    \ sim</code>\tGenerate simulation datasets\n<code>--mode sim_v</code>\tVisualise/summarise\
    \ simulation outputs\n<code>--mode div</code>\tBenchmark recombination detection\
    \ methods using simulated data\n<code>--mode emp</code>\tDetect recombination\
    \ in empirical sequence alignments\n<code>--mode class</code>\tCalculate classification\
    \ metrics</p>\n<p><code>nextflow run main.nf --help</code></p>\n<ul>\n<li>[ ]\
    \ Update readme</li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1617772984.0
funkelab/arid:
  data_format: 2
  description: Affinity Representing Instance Descriptors
  filenames:
  - singularity/Singularity
  full_name: funkelab/arid
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-connprep\" class=\"anchor\" href=\"#connprep\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>connprep</h1>\n<p>Produce preprocessed fMRI images ready for connectivity\
    \ analysis.</p>\n<h2>\n<a id=\"user-content-pipeline\" class=\"anchor\" href=\"\
    #pipeline\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pipeline</h2>\n<ol>\n<li>Drop initial or final volumes as specified.\
    \ Default: Analyze all volumes.</li>\n<li>Get the TR (volume acquisition time)\
    \ from pixdim[4] field of the Nifti header.</li>\n<li>Slice timing correction.\
    \ Default: none.</li>\n<li>Head motion realignment (SPM12 two-stage) and production\
    \ of mean fMRI.</li>\n<li>Rigid body coregistration of mean fMRI to T1 structural.</li>\n\
    <li>Compute volume quality metrics FD, DVARS.</li>\n<li>Reslice realigned fMRI\
    \ to native space, and also warp to MNI space using CAT12 transform.</li>\n<li>Remove\
    \ confounds from the native and MNI space fMRIs by simultaneous regression. Defaults:\n\
    <ul>\n<li>0.01 - 0.10 Hz bandpass filter</li>\n<li>6 estimated motion parameters\
    \ and their first differences</li>\n<li>6 principal components from the white\
    \ matter + CSF compartment</li>\n</ul>\n</li>\n<li>Repeat the confound removal,\
    \ additionally removing the mean signal of the gray matter compartment.</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Inputs</h2>\n\
    <pre><code>num_initial_vols_to_drop      0       Number of initial volumes to\
    \ drop\nnum_vols_to_analyze           all     Total number of volumes to analyze\n\
    bandpasslo_hz                 0.01    Low edge of bandpass filter in Hz\nbandpasshi_hz\
    \                 0.10    High edge of bandpass filter\nmot_PCs              \
    \         6       Number of PCs of motion params to remove\nmotderiv_PCs     \
    \             6       Same for motion derivatives\nwmcsf_PCs                 \
    \    6       Same for white matter/CSF compartment\nslorder                  \
    \     none    Slice timing correction, SPM12 nomenclature \nfmri_niigz       \
    \                     fMRI images, 4D Nifti\nmt1_niigz                       \
    \      T1 structural\ndeffwd_niigz                          Forward deformation\
    \ of T1 to MNI\ngray_niigz                            Gray matter volume fraction\n\
    white_niigz                           White matter volume fraction\ncsf_niigz\
    \                             CSF volume fraction\nproject                   \
    \            XNAT project label\nsubject                               XNAT subject\
    \ label\nsession                               XNAT session label\nscan      \
    \                            XNAT scan label\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<pre><code>connprep.pdf\
    \                               Processing report\nrp_adfmri.txt             \
    \                 Realignment parameters\nFD.txt                             \
    \        Framewise displacement\nDVARS.txt                                  Framewise\
    \ noise\nfiltered_keepgm_noscrub_nadfmri.nii.gz     Filtered data, native space,\
    \ gray matter signal retained\nfiltered_keepgm_noscrub_wadfmri.nii.gz     Filtered\
    \ data, MNI space, gray matter signal retained\nfiltered_removegm_noscrub_nadfmri.nii.gz\
    \   Filtered data, native space, gray matter signal removed\nfiltered_removegm_noscrub_wadfmri.nii.gz\
    \   Filtered data, MNI space, gray matter signal removed\nmeanadfmri.nii.gz  \
    \                        Mean fMRI, native space\nwmeanadfmri.nii.gz         \
    \                Mean fMRI, MNI space\nstats_keepgm_noscrub.txt              \
    \     Processing info when gray matter signal retained\nstats_removegm_noscrub.txt\
    \                 Processing info when gray matter signal removed\ngm_mask.nii.gz\
    \                             Native space gray matter mask\nwmcsf_mask.nii.gz\
    \                          Native space white matter/CSF mask\nconfounds_keepgm_noscrub.txt\
    \               Confounds matrix when gray matter signal retained\nconfounds_removegm_noscrub.txt\
    \             Confounds matrix  when gray matter signal removed\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1562764827.0
funkelab/funlib.run:
  data_format: 2
  description: Python wrapper for submitting jobs via bsub with the option to do so
    in a container environment.
  filenames:
  - singularity/Singularity
  full_name: funkelab/funlib.run
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-funlibrun\" class=\"anchor\" href=\"#funlibrun\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>funlib.run</h1>\n<p>Python wrapper for submitting jobs via bsub with\
    \ the option to do so in a container environment.</p>\n<h2>\n<a id=\"user-content-setup\"\
    \ class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Setup</h2>\n<pre><code>make install-full\n\
    </code></pre>\n<p>This creates a funlib.run config file ~/.funlib.run\nthat contains\
    \ default parameters that\ncan be overwritten for each specific run:</p>\n<pre><code>num_gpus\
    \ = 1\nmemory = 25600\nworking_directory = .\nsingularity = \"\"\nhost = \"\"\n\
    queue = \"normal\"\nenvironment = \"\"\nbatch = False\nmount_dirs = \"\"\n</code></pre>\n\
    <h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <p>There are three useful ways to use funlib.run:</p>\n<ol>\n<li>Direct usage\
    \ via command line arguments (overwrites config file defaults):</li>\n</ol>\n\
    <div class=\"highlight highlight-source-shell\"><pre>python run.py -p <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>python train.py<span class=\"pl-pds\">\"\
    </span></span> -c 5 -g 1 -q normal -s path-to-singularity-image\n\npython run_singularity.py\
    \ -p <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python mknet.py<span\
    \ class=\"pl-pds\">\"</span></span> -s path-to-singularity-image</pre></div>\n\
    <ol start=\"2\">\n<li>Indirect call via another script:</li>\n</ol>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span\
    \ class=\"pl-s1\">funlib</span>.<span class=\"pl-s1\">run</span> <span class=\"\
    pl-k\">import</span> <span class=\"pl-s1\">run</span>, <span class=\"pl-s1\">run_singularity</span>\n\
    \n<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">command</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">\"python train.py\"</span>,\n    <span class=\"\
    pl-s1\">num_cpus</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">5</span>,\n\
    \    <span class=\"pl-s1\">num_gpus</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">1</span>,\n    <span class=\"pl-s1\">queue</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">\"normal\"</span>,\n    <span class=\"pl-s1\"\
    >execute</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n\
    \n<span class=\"pl-en\">run_singularity</span>(<span class=\"pl-s1\">command</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s\">\"python mknet.py\"</span>,\n \
    \               <span class=\"pl-s1\">singularity_image</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s\">\"path_to_image\"</span>,\n                <span\
    \ class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >True</span>)</pre></div>\n<ol start=\"3\">\n<li>Command creation and subsequent\
    \ call:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-k\">from</span> <span class=\"pl-s1\">funlib</span>.<span class=\"\
    pl-s1\">run</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">run</span>,\
    \ <span class=\"pl-s1\">run_singularity</span>\n<span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">subprocess</span> <span class=\"pl-k\">import</span> <span\
    \ class=\"pl-s1\">check_call</span>\n\n<span class=\"pl-s1\">run_command</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">run</span>(<span class=\"\
    pl-s1\">command</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"python\
    \ train.py\"</span>,\n                  <span class=\"pl-s1\">num_cpus</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">5</span>,\n                  <span\
    \ class=\"pl-s1\">num_gpus</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">1</span>,\n                  <span class=\"pl-s1\">queue</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">\"normal\"</span>,\n                  <span\
    \ class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >False</span>)\n\n<span class=\"pl-en\">check_call</span>(<span class=\"pl-s1\"\
    >run_command</span>,\n           <span class=\"pl-s1\">shell</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-s1\">run_singularity_command</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">run_singularity</span>(<span\
    \ class=\"pl-s1\">command</span><span class=\"pl-c1\">=</span><span class=\"pl-s\"\
    >\"python mknet.py\"</span>,\n                                          <span\
    \ class=\"pl-s1\">singularity_image</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s\">\"path_to_image\"</span>,\n                                 \
    \         <span class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">False</span>)\n\n<span class=\"pl-en\">check_call</span>(<span\
    \ class=\"pl-s1\">run_singularity_command</span>,\n           <span class=\"pl-s1\"\
    >shell</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n\
    <h2>\n<a id=\"user-content-usage-with-daisy\" class=\"anchor\" href=\"#usage-with-daisy\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage with Daisy</h2>\n<p>When used with daisy.call do not expand\
    \ the cmd to a string via setting expand=False:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">cmd</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-en\">run</span>(<span class=\"pl-s1\">command</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s1\">base_command</span>,\n          <span class=\"pl-s1\">queue</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s1\">queue</span>,\n          <span\
    \ class=\"pl-s1\">num_gpus</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">1</span>,\n          <span class=\"pl-s1\">num_cpus</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">num_cpus</span>,\n          <span class=\"\
    pl-s1\">singularity_image</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\"\
    >singularity_container</span>,\n          <span class=\"pl-s1\">mount_dirs</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s1\">mount_dirs</span>,\n         \
    \ <span class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">False</span>,\n          <span class=\"pl-s1\">expand</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-s1\">daisy</span>.<span\
    \ class=\"pl-en\">call</span>(<span class=\"pl-s1\">cmd</span>, <span class=\"\
    pl-s1\">log_out</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">log_out</span>,\
    \ <span class=\"pl-s1\">log_err</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-s1\">log_err</span>)</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1594370552.0
funkelab/lsd:
  data_format: 2
  description: "\U0001F308"
  filenames:
  - singularity/Singularity
  full_name: funkelab/lsd
  latest_release: null
  readme: "<h2>\n<a id=\"user-content-local-shape-descriptors-for-neuron-segmentation\"\
    \ class=\"anchor\" href=\"#local-shape-descriptors-for-neuron-segmentation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Local\
    \ Shape Descriptors (for Neuron Segmentation)</h2>\n<p><a href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/gifs/lsd_particles.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/gifs/lsd_particles.gif\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n<p>This repository contains code\
    \ to compute Local Shape Descriptors (LSDs) from an instance segmentation. LSDs\
    \ can then be used during training as an auxiliary target, which we found to improve\
    \ boundary prediction and therefore segmentation quality. Read more about it in\
    \ our <a href=\"https://www.biorxiv.org/content/10.1101/2021.01.18.427039v1\"\
    \ rel=\"nofollow\">paper</a> and/or <a href=\"https://localshapedescriptors.github.io/\"\
    \ rel=\"nofollow\">blog post</a>.</p>\n<hr>\n<p><a href=\"#example\">Quick 2d\
    \ Examples</a></p>\n<p><a href=\"#nbook\">Notebooks</a></p>\n<p><a href=\"#networks\"\
    >Example networks &amp; pipelines</a></p>\n<p><a href=\"#parallel\">Parallel processing</a></p>\n\
    <hr>\n<p><strong>Cite:</strong></p>\n<div class=\"highlight highlight-text-bibtex\"\
    ><pre><span class=\"pl-k\">@article</span>{<span class=\"pl-en\">sheridan_local_2021</span>,\n\
    \t<span class=\"pl-s\">title</span> = <span class=\"pl-s\"><span class=\"pl-pds\"\
    >{</span>Local Shape Descriptors for Neuron Segmentation<span class=\"pl-pds\"\
    >}</span></span>,\n\t<span class=\"pl-s\">url</span> = <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">{</span>https://www.biorxiv.org/content/10.1101/2021.01.18.427039v1<span\
    \ class=\"pl-pds\">}</span></span>,\n\t<span class=\"pl-s\">urldate</span> = <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">{</span>2021-01-20<span class=\"pl-pds\"\
    >}</span></span>,\n\t<span class=\"pl-s\">journal</span> = <span class=\"pl-s\"\
    ><span class=\"pl-pds\">{</span>bioRxiv<span class=\"pl-pds\">}</span></span>,\n\
    \t<span class=\"pl-s\">author</span> = <span class=\"pl-s\"><span class=\"pl-pds\"\
    >{</span>Sheridan, Arlo and Nguyen, Tri and Deb, Diptodip and Lee, Wei-Chung Allen\
    \ and Saalfeld, Stephan and Turaga, Srinivas and Manor, Uri and Funke, Jan<span\
    \ class=\"pl-pds\">}</span></span>,\n\t<span class=\"pl-s\">year</span> = <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">{</span>2021<span class=\"pl-pds\">}</span></span>\n\
    }</pre></div>\n<p><strong>Notes:</strong></p>\n<ul>\n<li>\n<p>This is not production\
    \ level software and was developed in a pure research environment. Therefore some\
    \ scripts may not work out of the box. For example, all paper networks were originally\
    \ written using now deprecated tensorflow/cudnn versions and rely on an outdated\
    \ singularity container. Because of this, the singularity image will not build\
    \ from the current recipe - if replicating with the current implementations, please\
    \ reach out for the singularity container (it is too large to upload here). Alternatively,\
    \ consider reimplementing networks in pytorch (there are examples below).</p>\n\
    </li>\n<li>\n<p>Post-proccesing steps were designed for use with a specific cluster\
    \ and will need to be tweaked for individual use cases. If the need / use increases\
    \ then we will look into refactoring, packaging and distributing.</p>\n</li>\n\
    <li>\n<p>Currently, post-processing scripts (e.g <a href=\"https://github.com/funkelab/lsd/blob/master/lsd/fragments.py\"\
    >watershed</a>) are located inside this repo which creates more dependencies than\
    \ needed for using the lsds. One forseeable issue is that agglomeration requires\
    \ networkx==2.2 for the MergeTree. These scripts will be migrated to another repository\
    \ in the future...</p>\n</li>\n<li>\n<p>Tested on Ubuntu 18.04 with Python 3.</p>\n\
    </li>\n</ul>\n<hr>\n<p><a name=\"user-content-example\"></a></p>\n<h2>\n<a id=\"\
    user-content-quick-2d-examples\" class=\"anchor\" href=\"#quick-2d-examples\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quick 2d Examples</h2>\n<details>\n  <summary>Required packages/repos</summary>\n\
    <ul>\n<li>run in conda env or colab notebook with appropriate packages/repos installed</li>\n\
    <li>since some required post-processing scripts are located in this repo, there\
    \ are various packages required along with the lsds.</li>\n<li>if confused, see\
    \ notebook tutorials for further details</li>\n</ul>\n<pre><code>packages (i.e\
    \ pip install {package})\n\ndaisy\ngunpowder\nmahotas\nmatplotlib\nscikit-image\n\
    \nrepos (i.e pip install git+git://github.com/{repo})\n\nfunkelab/funlib.segment.git\n\
    funkelab/lsd.git\nfunkey/waterz.git\n</code></pre>\n</details>\n<details>\n  <summary>Coins\
    \ example</summary>\n<ul>\n<li>logic to create labels borrowed from this <a href=\"\
    https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_expand_labels.html#sphx-glr-auto-examples-segmentation-plot-expand-labels-py\"\
    \ rel=\"nofollow\">tutorial</a>\n</li>\n</ul>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">matplotlib</span>.<span\
    \ class=\"pl-s1\">pyplot</span> <span class=\"pl-k\">as</span> <span class=\"\
    pl-s1\">plt</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">numpy</span>\
    \ <span class=\"pl-k\">as</span> <span class=\"pl-s1\">np</span>\n<span class=\"\
    pl-k\">import</span> <span class=\"pl-s1\">skimage</span>\n\n<span class=\"pl-k\"\
    >from</span> <span class=\"pl-s1\">lsd</span> <span class=\"pl-k\">import</span>\
    \ <span class=\"pl-s1\">local_shape_descriptor</span>\n<span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">skimage</span>.<span class=\"pl-s1\">filters</span> <span\
    \ class=\"pl-k\">import</span> <span class=\"pl-s1\">sobel</span>\n<span class=\"\
    pl-k\">from</span> <span class=\"pl-s1\">skimage</span>.<span class=\"pl-s1\"\
    >measure</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">label</span>\n\
    <span class=\"pl-k\">from</span> <span class=\"pl-s1\">skimage</span>.<span class=\"\
    pl-s1\">segmentation</span> <span class=\"pl-k\">import</span> <span class=\"\
    pl-s1\">watershed</span>\n\n<span class=\"pl-c1\">%</span><span class=\"pl-s1\"\
    >matplotlib</span> <span class=\"pl-s1\">inline</span>\n\n<span class=\"pl-c\"\
    ># get coins dataset</span>\n<span class=\"pl-s1\">data</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-s1\">skimage</span>.<span class=\"pl-s1\">data</span>.<span\
    \ class=\"pl-en\">coins</span>()\n\n<span class=\"pl-c\"># create edges</span>\n\
    <span class=\"pl-s1\">edges</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-en\">sobel</span>(<span class=\"pl-s1\">data</span>)\n\n<span class=\"pl-c\"\
    ># generate markers for watershed</span>\n<span class=\"pl-s1\">markers</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">np</span>.<span class=\"\
    pl-en\">zeros_like</span>(<span class=\"pl-s1\">data</span>)\n<span class=\"pl-s1\"\
    >foreground</span>, <span class=\"pl-s1\">background</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>\n<span\
    \ class=\"pl-s1\">markers</span>[<span class=\"pl-s1\">data</span> <span class=\"\
    pl-c1\">&lt;</span> <span class=\"pl-c1\">30.0</span>] <span class=\"pl-c1\">=</span>\
    \ <span class=\"pl-s1\">background</span>\n<span class=\"pl-s1\">markers</span>[<span\
    \ class=\"pl-s1\">data</span> <span class=\"pl-c1\">&gt;</span> <span class=\"\
    pl-c1\">150.0</span>] <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">foreground</span>\n\
    \n<span class=\"pl-c\"># get unique labels</span>\n<span class=\"pl-s1\">ws</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">watershed</span>(<span\
    \ class=\"pl-s1\">edges</span>, <span class=\"pl-s1\">markers</span>)\n<span class=\"\
    pl-s1\">labels</span> <span class=\"pl-c1\">=</span> <span class=\"pl-en\">label</span>(<span\
    \ class=\"pl-s1\">ws</span> <span class=\"pl-c1\">==</span> <span class=\"pl-s1\"\
    >foreground</span>).<span class=\"pl-en\">astype</span>(<span class=\"pl-s1\"\
    >np</span>.<span class=\"pl-s1\">uint64</span>)\n\n<span class=\"pl-c\"># calculate\
    \ lsds</span>\n<span class=\"pl-s1\">lsds</span> <span class=\"pl-c1\">=</span>\
    \ <span class=\"pl-s1\">local_shape_descriptor</span>.<span class=\"pl-en\">get_local_shape_descriptors</span>(\n\
    \              <span class=\"pl-s1\">segmentation</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s1\">labels</span>,\n              <span class=\"pl-s1\"\
    >sigma</span><span class=\"pl-c1\">=</span>(<span class=\"pl-c1\">15</span>,)<span\
    \ class=\"pl-c1\">*</span><span class=\"pl-c1\">2</span>,\n              <span\
    \ class=\"pl-s1\">voxel_size</span><span class=\"pl-c1\">=</span>(<span class=\"\
    pl-c1\">1</span>,)<span class=\"pl-c1\">*</span><span class=\"pl-c1\">2</span>)\n\
    \n<span class=\"pl-c\"># view lsds</span>\n<span class=\"pl-s1\">fig</span>, <span\
    \ class=\"pl-s1\">axes</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\"\
    >plt</span>.<span class=\"pl-en\">subplots</span>(\n            <span class=\"\
    pl-c1\">1</span>,\n            <span class=\"pl-c1\">6</span>,\n            <span\
    \ class=\"pl-s1\">figsize</span><span class=\"pl-c1\">=</span>(<span class=\"\
    pl-c1\">25</span>, <span class=\"pl-c1\">10</span>),\n            <span class=\"\
    pl-s1\">sharex</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>,\n\
    \            <span class=\"pl-s1\">sharey</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">True</span>,\n            <span class=\"pl-s1\">squeeze</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"\
    pl-k\">def</span> <span class=\"pl-en\">view_channel</span>(<span class=\"pl-s1\"\
    >ax</span>, <span class=\"pl-s1\">data</span>, <span class=\"pl-s1\">channel</span>):\n\
    \  <span class=\"pl-s1\">ax</span>[<span class=\"pl-c1\">0</span>][<span class=\"\
    pl-s1\">channel</span>].<span class=\"pl-en\">imshow</span>(<span class=\"pl-s1\"\
    >np</span>.<span class=\"pl-en\">squeeze</span>(<span class=\"pl-s1\">data</span>[<span\
    \ class=\"pl-s1\">channel</span>:<span class=\"pl-s1\">channel</span><span class=\"\
    pl-c1\">+</span><span class=\"pl-c1\">1</span>,:,:]), <span class=\"pl-s1\">cmap</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s\">'jet'</span>)\n\n<span class=\"\
    pl-k\">for</span> <span class=\"pl-s1\">i</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-en\">range</span>(<span class=\"pl-c1\">6</span>):\n  <span\
    \ class=\"pl-en\">view_channel</span>(<span class=\"pl-s1\">axes</span>,<span\
    \ class=\"pl-s1\">lsds</span>,<span class=\"pl-s1\">channel</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">i</span>)\n  \n  <span class=\"pl-c\">#from\
    \ left to right: mean offset y, mean offset x, orientation y, orientation x, change\
    \ (y-x), voxel count</span></pre></div>\n<p><a href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/coins.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/coins.png\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n </details>\n<details>\n  <summary>Neurons\
    \ example</summary>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-k\">import</span> <span class=\"pl-s1\">h5py</span>\n<span class=\"\
    pl-k\">import</span> <span class=\"pl-s1\">io</span>\n<span class=\"pl-k\">import</span>\
    \ <span class=\"pl-s1\">matplotlib</span>.<span class=\"pl-s1\">pyplot</span>\
    \ <span class=\"pl-k\">as</span> <span class=\"pl-s1\">plt</span>\n<span class=\"\
    pl-k\">import</span> <span class=\"pl-s1\">numpy</span> <span class=\"pl-k\">as</span>\
    \ <span class=\"pl-s1\">np</span>\n<span class=\"pl-k\">import</span> <span class=\"\
    pl-s1\">requests</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\"\
    >lsd</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">local_shape_descriptor</span>\n\
    \ \n<span class=\"pl-c1\">%</span><span class=\"pl-s1\">matplotlib</span> <span\
    \ class=\"pl-s1\">inline</span>\n\n<span class=\"pl-c\"># example data</span>\n\
    <span class=\"pl-s1\">url</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-s\">'https://cremi.org/static/data/sample_A_20160501.hdf'</span>\n\n<span class=\"\
    pl-c\"># convert from binary</span>\n<span class=\"pl-s1\">f</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-s1\">h5py</span>.<span class=\"pl-v\">File</span>(<span\
    \ class=\"pl-s1\">io</span>.<span class=\"pl-v\">BytesIO</span>(<span class=\"\
    pl-s1\">requests</span>.<span class=\"pl-en\">get</span>(<span class=\"pl-s1\"\
    >url</span>).<span class=\"pl-s1\">content</span>), <span class=\"pl-s\">'r'</span>)\n\
    \n<span class=\"pl-c\"># get corner patch</span>\n<span class=\"pl-s1\">labels</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">np</span>.<span class=\"\
    pl-en\">squeeze</span>(<span class=\"pl-s1\">f</span>[<span class=\"pl-s\">'volumes/labels/neuron_ids'</span>][<span\
    \ class=\"pl-c1\">0</span>:<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\"\
    >0</span>:<span class=\"pl-c1\">250</span>,<span class=\"pl-c1\">0</span>:<span\
    \ class=\"pl-c1\">250</span>])\n\n<span class=\"pl-c\"># calc lsds</span>\n<span\
    \ class=\"pl-s1\">lsds</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\"\
    >local_shape_descriptor</span>.<span class=\"pl-en\">get_local_shape_descriptors</span>(\n\
    \              <span class=\"pl-s1\">segmentation</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s1\">labels</span>,\n              <span class=\"pl-s1\"\
    >sigma</span><span class=\"pl-c1\">=</span>(<span class=\"pl-c1\">100</span>,)<span\
    \ class=\"pl-c1\">*</span><span class=\"pl-c1\">2</span>,\n              <span\
    \ class=\"pl-s1\">voxel_size</span><span class=\"pl-c1\">=</span>[<span class=\"\
    pl-c1\">4</span>,<span class=\"pl-c1\">4</span>])\n\n<span class=\"pl-c\"># view</span>\n\
    <span class=\"pl-s1\">fig</span>, <span class=\"pl-s1\">axes</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-s1\">plt</span>.<span class=\"pl-en\">subplots</span>(\n\
    \            <span class=\"pl-c1\">1</span>,\n            <span class=\"pl-c1\"\
    >6</span>,\n            <span class=\"pl-s1\">figsize</span><span class=\"pl-c1\"\
    >=</span>(<span class=\"pl-c1\">15</span>, <span class=\"pl-c1\">10</span>),\n\
    \            <span class=\"pl-s1\">sharex</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">True</span>,\n            <span class=\"pl-s1\">sharey</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>,\n            <span\
    \ class=\"pl-s1\">squeeze</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >False</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">view_channel</span>(<span\
    \ class=\"pl-s1\">ax</span>, <span class=\"pl-s1\">data</span>, <span class=\"\
    pl-s1\">channel</span>):\n\n  <span class=\"pl-s1\">ax</span>[<span class=\"pl-c1\"\
    >0</span>][<span class=\"pl-s1\">channel</span>].<span class=\"pl-en\">imshow</span>(<span\
    \ class=\"pl-s1\">np</span>.<span class=\"pl-en\">squeeze</span>(<span class=\"\
    pl-s1\">data</span>[<span class=\"pl-s1\">channel</span>:<span class=\"pl-s1\"\
    >channel</span><span class=\"pl-c1\">+</span><span class=\"pl-c1\">1</span>,:,:]),\
    \ <span class=\"pl-s1\">cmap</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-s\">'jet'</span>)\n\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\"\
    >i</span> <span class=\"pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span\
    \ class=\"pl-c1\">6</span>):\n  <span class=\"pl-en\">view_channel</span>(<span\
    \ class=\"pl-s1\">axes</span>,<span class=\"pl-s1\">lsds</span>,<span class=\"\
    pl-s1\">channel</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">i</span>)\n\
    \  \n<span class=\"pl-c\">#from left to right: mean offset y, mean offset x, orientation\
    \ y, orientation x, change (y-x), voxel count</span></pre></div>\n<p><a href=\"\
    https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/2d_lsds.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/2d_lsds.png\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n </details>\n<hr>\n<p><a name=\"\
    user-content-nbook\"></a></p>\n<h2>\n<a id=\"user-content-notebooks\" class=\"\
    anchor\" href=\"#notebooks\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Notebooks</h2>\n<ul>\n<li>\n<p>Examble colab\
    \ notebooks are located <a href=\"https://github.com/funkelab/lsd/tree/tutorial/lsd/tutorial/notebooks\"\
    >here</a>. You can download or run below (control + click open in colab). When\
    \ running a notebook, you will probably get the message: \"Warning: This notebook\
    \ was not authored by Google\". This can be ignored, you can run anyway.</p>\n\
    </li>\n<li>\n<p>We uploaded ~1.7 tb of data (raw/labels/masks/rags etc.) to an\
    \ s3 bucket. The following tutorial shows some examples for accessing and visualizing\
    \ the data.</p>\n<ul>\n<li>Data download: <a href=\"https://colab.research.google.com/github/funkelab/lsd/blob/master/lsd/tutorial/notebooks/lsd_data_download.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n</ul>\n</li>\n<li>\n<p>If implementing\
    \ the LSDs in your own training pipeline (i.e pure pytorch/tensorflow), calculate\
    \ the LSDs on a label array of unique objects and use them as the target for your\
    \ network (see quick 2d examples above for calculating).</p>\n</li>\n<li>\n<p>The\
    \ following tutorials show how to set up 2D training/prediction pipelines using\
    \ <a href=\"http://funkey.science/gunpowder/\" rel=\"nofollow\">Gunpowder</a>.\
    \ It is recommended to follow them in order (skip the basic tutorial if familiar\
    \ with gunpowder).</p>\n<ul>\n<li>\n<p>Basic Gunpowder tutorial: <a href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/basic_gp_tutorial.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Train Affinities: <a href=\"\
    https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/train_affinities.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Train LSDs: <a href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/train_lsds.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Train MTLSD: <a href=\"\
    https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/train_mtlsd.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Inference (using pretrained\
    \ MTLSD checkpoint): <a href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/inference.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Watershed, agglomeration,\
    \ segmentation: <a href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/segment.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><a\
    \ name=\"user-content-networks\"></a></p>\n<h2>\n<a id=\"user-content-example-networks--pipelines\"\
    \ class=\"anchor\" href=\"#example-networks--pipelines\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Example\
    \ networks &amp; pipelines</h2>\n<ul>\n<li>There are some example networks and\
    \ training/prediction pipelines from the fib25 dataset <a href=\"https://github.com/funkelab/lsd/tree/tutorial/lsd/tutorial/example_nets/fib25\"\
    >here</a>.</li>\n</ul>\n<h3>\n<a id=\"user-content-training\" class=\"anchor\"\
    \ href=\"#training\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Training</h3>\n<ul>\n<li>\n<p>Since networks\
    \ in this paper were implemented in Tensorflow, there was a two step process for\
    \ training. First the networks were created using the <code>mknet.py</code> files.\
    \ This saved tensor placeholders and meta data in config files that were then\
    \ used for both training and prediction. The mknet files used the now deprecated\
    \ mala repository to create the networks. If reimplementing in Tensorflow, consider\
    \ migrating to <a href=\"https://github.com/funkelab/funlib.learn.tensorflow\"\
    >funlib.learn.tensorflow</a>.</p>\n</li>\n<li>\n<p>If using Pytorch, the networks\
    \ can just be created directly inside the train scripts since placeholders aren't\
    \ required. For example, the logic from this <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/mknet.py\"\
    >mknet script</a> and this <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/train.py\"\
    >train script</a> can be condensed to <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/train_pytorch.py\"\
    >this</a>.</p>\n</li>\n<li>\n<p>For training an autocontext network (e.g <code>acrlsd</code>),\
    \ the current implementation learns the LSDs in a <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/lsd/train.py\"\
    >first pass</a>. A saved checkpoint is then used when creating the <a href=\"\
    https://github.com/funkelab/lsd/blob/4397779ea4702eb3d593898d6240819e761fd41a/lsd/tutorial/example_nets/fib25/acrlsd/mknet.py#L122\"\
    >second pass</a> in order to <a href=\"https://github.com/funkelab/lsd/blob/4397779ea4702eb3d593898d6240819e761fd41a/lsd/tutorial/example_nets/fib25/acrlsd/train.py#L158\"\
    >predict LSDs</a> prior to learning the Affinities. One could modify this to use\
    \ a single setup and remove the need for writing the LSDs to disk.</p>\n</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-inference\" class=\"anchor\" href=\"#inference\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Inference</h3>\n<ul>\n<li>\n<p>By default, the predict scripts (<a\
    \ href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/mtlsd/predict.py\"\
    >example</a>) contain the worker logic to be distributed by the scheduler during\
    \ parallel processing (see below).</p>\n</li>\n<li>\n<p>If you just need to process\
    \ a relatively small volume, it is sometimes not necessary to use blockwise processing.\
    \ In this case, it is recommended to use a <a href=\"http://funkey.science/gunpowder/api.html#scan\"\
    \ rel=\"nofollow\">scan node</a>, and specify input/output shapes + context. An\
    \ example can be found in the inference colab notebook above.</p>\n</li>\n<li>\n\
    <p>Similar to training, the current autocontext implementations assume the predicted\
    \ LSDs are written to a zarr/n5 container and then used as input to the second\
    \ pass to predict affinities. This can also be changed to predict on the fly if\
    \ needed.</p>\n</li>\n</ul>\n<details>\n  <summary>Visualizations of example training/prediction\
    \ pipelines</summary>\n<br><br>\n<details>\n  <summary>Color key</summary>\n<p><a\
    \ href=\"https://camo.githubusercontent.com/a9a97d0a679c3df8d1f607b3d2ae316d26bd7948b9675cdb5252a412ff681153/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6161663265332f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a9a97d0a679c3df8d1f607b3d2ae316d26bd7948b9675cdb5252a412ff681153/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6161663265332f3030303030303f746578743d2b\"\
    \ alt=\"#aaf2e3\" data-canonical-src=\"https://via.placeholder.com/15/aaf2e3/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#source-nodes\"\
    \ rel=\"nofollow\">Source nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/9b7f598e884ee96fac5fea23d7f82d05ae741e128c6e145d32107d6c4fd48074/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666623865372f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9b7f598e884ee96fac5fea23d7f82d05ae741e128c6e145d32107d6c4fd48074/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666623865372f3030303030303f746578743d2b\"\
    \ alt=\"#ffb8e7\" data-canonical-src=\"https://via.placeholder.com/15/ffb8e7/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#image-processing-nodes\"\
    \ rel=\"nofollow\">Image processing nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/d3834c08d34f2ed79d1f788c1d35ce9dcbef13a8c1b813bf3b5df7f2373c9261/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666646561642f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/d3834c08d34f2ed79d1f788c1d35ce9dcbef13a8c1b813bf3b5df7f2373c9261/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666646561642f3030303030303f746578743d2b\"\
    \ alt=\"#ffdead\" data-canonical-src=\"https://via.placeholder.com/15/ffdead/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#location-manipulation-nodes\"\
    \ rel=\"nofollow\">Location manipulation nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/77fc07fd556a8867418042a7732acddec36d245ac5fa8e6fa3982225d7a71488/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6235623362332f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/77fc07fd556a8867418042a7732acddec36d245ac5fa8e6fa3982225d7a71488/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6235623362332f3030303030303f746578743d2b\"\
    \ alt=\"#b5b3b3\" data-canonical-src=\"https://via.placeholder.com/15/b5b3b3/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#provider-combination-nodes\"\
    \ rel=\"nofollow\">Provider combination nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/3f6d952b0f7fce1c2d4f92090f5309333ad278b248ddfd36e34828d88c06e829/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6262662f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3f6d952b0f7fce1c2d4f92090f5309333ad278b248ddfd36e34828d88c06e829/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6262662f3030303030303f746578743d2b\"\
    \ alt=\"#bbf\" data-canonical-src=\"https://via.placeholder.com/15/bbf/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#augmentation-nodes\"\
    \ rel=\"nofollow\">Augmentation nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/f147e3fe453aeaf74231447a042a7a78bd8b94b08cfb80416885830c159eabaa/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666666339312f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f147e3fe453aeaf74231447a042a7a78bd8b94b08cfb80416885830c159eabaa/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666666339312f3030303030303f746578743d2b\"\
    \ alt=\"#fffc91\" data-canonical-src=\"https://via.placeholder.com/15/fffc91/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#label-manipulation-nodes\"\
    \ rel=\"nofollow\">Label manipulation nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/baf49106c9e6e1fc1f75052028413c99f3a2edf219275a9cdcf116683ca6ced2/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6233653766662f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/baf49106c9e6e1fc1f75052028413c99f3a2edf219275a9cdcf116683ca6ced2/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6233653766662f3030303030303f746578743d2b\"\
    \ alt=\"#b3e7ff\" data-canonical-src=\"https://via.placeholder.com/15/b3e7ff/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#performance-nodes\"\
    \ rel=\"nofollow\">Performance nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/0af57b13aab71a8b0b0ed524d4b9addd087127e3c1bb76ac794951fad093f83b/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666393136392f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0af57b13aab71a8b0b0ed524d4b9addd087127e3c1bb76ac794951fad093f83b/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666393136392f3030303030303f746578743d2b\"\
    \ alt=\"#ff9169\" data-canonical-src=\"https://via.placeholder.com/15/ff9169/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#training-and-prediction-nodes\"\
    \ rel=\"nofollow\">Training and prediction nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/82f697552844f276b2ef97711385a6dc558248eed718c239ce971f11d2aadcbc/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f3732626636392f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/82f697552844f276b2ef97711385a6dc558248eed718c239ce971f11d2aadcbc/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f3732626636392f3030303030303f746578743d2b\"\
    \ alt=\"#72bf69\" data-canonical-src=\"https://via.placeholder.com/15/72bf69/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#module-gunpowder\"\
    \ rel=\"nofollow\">Output nodes</a></p>\n<p><a href=\"https://camo.githubusercontent.com/6ee7cadb933bf945d53b440b486621eef6ad7a4ed604f0846c4767ac9efe475d/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6132393166662f3030303030303f746578743d2b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6ee7cadb933bf945d53b440b486621eef6ad7a4ed604f0846c4767ac9efe475d/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6132393166662f3030303030303f746578743d2b\"\
    \ alt=\"#a291ff\" data-canonical-src=\"https://via.placeholder.com/15/a291ff/000000?text=+\"\
    \ style=\"max-width:100%;\"></a> <a href=\"http://funkey.science/gunpowder/api.html#iterative-processing-nodes\"\
    \ rel=\"nofollow\">Iterative processing nodes</a></p>\n </details>\n<p>Vanilla\
    \ affinities <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/train.py\"\
    >training</a>:</p>\n<p><a href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/train_nodes.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/train_nodes.svg\"\
    \ alt=\"\" style=\"max-width:100%;\"></a>\n<br><br>\nAutocontext <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/lsd/predict.py\"\
    >LSD</a> and <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/acrlsd/predict.py\"\
    >affinities</a> prediction:</p>\n<p><a href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/predict_nodes.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/predict_nodes.svg\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n</details>\n<hr>\n<p><a name=\"\
    user-content-parallel\"></a></p>\n<h2>\n<a id=\"user-content-parallel-processing\"\
    \ class=\"anchor\" href=\"#parallel-processing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Parallel processing</h2>\n<ul>\n\
    <li>\n<p>If you are running on small data then this section may be irrelevant.\
    \ See the <code>Watershed, agglomeration, segmentation</code> notebook above if\
    \ you just want to get a sense of obtaining a segmentation from affinities.</p>\n\
    </li>\n<li>\n<p>Example processing scripts can be found <a href=\"https://github.com/funkelab/lsd/tree/tutorial/lsd/tutorial/scripts\"\
    >here</a></p>\n</li>\n<li>\n<p>We create segmentations following the approach\
    \ in <a href=\"https://ieeexplore.ieee.org/document/8364622\" rel=\"nofollow\"\
    >this paper</a>. Generally speaking, after training a network there are five steps\
    \ to obtain a segmentation:</p>\n</li>\n</ul>\n<ol>\n<li>Predict boundaries (this\
    \ can involve the use of LSDs as an auxiliary task)</li>\n<li>Generate supervoxels\
    \ (fragments) using seeded watershed. The fragment centers of mass are stored\
    \ as region adjacency graph nodes.</li>\n<li>Generate edges between nodes using\
    \ hierarchical agglomeration. The edges are weighted by the underlying affinities.\
    \ Edges with lower scores are merged earlier.</li>\n<li>Cut the graph at a predefined\
    \ threshold and relabel connected components. Store the node - component lookup\
    \ tables.</li>\n<li>Use the lookup tables to relabel supervoxels and generate\
    \ a segmentation.</li>\n</ol>\n<p><a href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/pipeline.jpeg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/pipeline.jpeg\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li>\n<p>Everything was done\
    \ in parallel using <a href=\"https://github.com/funkelab/daisy\">daisy</a>, but\
    \ one could use multiprocessing or dask instead.</p>\n</li>\n<li>\n<p>For our\
    \ experiments we used <a href=\"https://www.mongodb.com/\" rel=\"nofollow\">MongoDB</a>\
    \ for all storage (block checks, rags, scores, etc) due to the size of the data.\
    \ Depending on use case, it might be better to read/write to file rather than\
    \ mongo. See watershed for further info.</p>\n</li>\n<li>\n<p>The following examples\
    \ were written for use with the Janelia LSF cluster and are just meant to be used\
    \ as a guide. Users will likely need to customize for their own specs (for example\
    \ if using a SLURM cluster).</p>\n</li>\n<li>\n<p>Need to install <a href=\"https://github.com/funkelab/funlib.segment\"\
    >funlib.segment</a> and <a href=\"https://github.com/funkelab/funlib.evaluate\"\
    >funlib.evaluate</a> if using/adapting segmentation/evaluation scripts.</p>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-inference-1\" class=\"anchor\" href=\"\
    #inference-1\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Inference</h3>\n<p>The worker logic is located in\
    \ individual <code>predict.py</code> scripts (<a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/predict.py\"\
    >example</a>). The <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/01_predict_blockwise.py\"\
    >master script</a> distributes using <code>daisy.run_blockwise</code>. The only\
    \ need for MongoDb here is for the block check function (to check which blocks\
    \ have successfully completed). To remove the need for mongo, one could remove\
    \ the check function (remember to also remove <code>block_done_callback</code>\
    \ in <code>predict.py</code>) or replace with custom function (e.g check chunk\
    \ completion directly in output container).</p>\n<details>\n <summary>Example\
    \ roi config</summary>\n<div class=\"highlight highlight-source-json\"><pre>{\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>container<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hemi_roi_1.zarr<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>offset<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-c1\"\
    >140800</span>, <span class=\"pl-c1\">205120</span>, <span class=\"pl-c1\">198400</span>],\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>size<span class=\"pl-pds\"\
    >\"</span></span>: [<span class=\"pl-c1\">3000</span>, <span class=\"pl-c1\">3000</span>,\
    \ <span class=\"pl-c1\">3000</span>]\n}</pre></div>\n</details>\n<details>\n <summary>Example\
    \ predict config</summary>\n<pre><code> {\n  \"base_dir\": \"/path/to/base/directory\"\
    ,\n  \"experiment\": \"hemi\",\n  \"setup\": \"setup01\",\n  \"iteration\": 400000,\n\
    \  \"raw_file\": \"predict_roi.json\",\n  \"raw_dataset\" : \"volumes/raw\",\n\
    \  \"out_base\" : \"output\",\n  \"file_name\": \"foo.zarr\",\n  \"num_workers\"\
    : 5,\n  \"db_host\": \"mongodb client\",\n  \"db_name\": \"foo\",\n  \"queue\"\
    : \"gpu_rtx\",\n  \"singularity_image\": \"/path/to/singularity/image\"\n}\n</code></pre>\n\
    </details>\n<h3>\n<a id=\"user-content-watershed\" class=\"anchor\" href=\"#watershed\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Watershed</h3>\n<p>The worker logic is located in a single <a href=\"\
    https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/workers/extract_fragments_worker.py\"\
    >script</a> which is then distributed by the <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/02_extract_fragments_blockwise.py\"\
    >master script</a>. By default the nodes are stored in mongo using a <a href=\"\
    https://github.com/funkelab/daisy/blob/master/daisy/persistence/mongodb_graph_provider.py\"\
    >MongoDbGraphProvider</a>. To write to file (i.e compressed numpy arrays), you\
    \ can use the <a href=\"https://github.com/funkelab/daisy/blob/master/daisy/persistence/file_graph_provider.py\"\
    >FileGraphProvider</a> instead (inside the worker script).</p>\n<details>\n <summary>Example\
    \ watershed config</summary>\n<div class=\"highlight highlight-source-json\"><pre>{\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>experiment<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hemi<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>setup<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>setup01<span class=\"pl-pds\">\"</span></span>,\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>iteration<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-c1\">400000</span>,\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>affs_file<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo.zarr<span class=\"\
    pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>affs_dataset<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>/volumes/affs<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>fragments_file<span class=\"pl-pds\">\"\
    </span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo.zarr<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>fragments_dataset<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>/volumes/fragments<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>block_size<span\
    \ class=\"pl-pds\">\"</span></span>: [<span class=\"pl-c1\">1000</span>, <span\
    \ class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>],\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>context<span class=\"pl-pds\">\"</span></span>:\
    \ [<span class=\"pl-c1\">248</span>, <span class=\"pl-c1\">248</span>, <span class=\"\
    pl-c1\">248</span>],\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>db_host<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>mongodb client<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>db_name<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>num_workers<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">6</span>,\n  <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>fragments_in_xy<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-c1\">false</span>,\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>epsilon_agglomerate<span class=\"pl-pds\"\
    >\"</span></span>: <span class=\"pl-c1\">0</span>,\n  <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>queue<span class=\"pl-pds\">\"</span></span>: <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>local<span class=\"pl-pds\">\"\
    </span></span>\n}</pre></div>\n</details>\n<h3>\n<a id=\"user-content-agglomerate\"\
    \ class=\"anchor\" href=\"#agglomerate\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Agglomerate</h3>\n<p>Same as\
    \ watershed. <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/workers/agglomerate_worker.py\"\
    >Worker script</a>, <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/03_agglomerate_blockwise.py\"\
    >master script</a>. Change to FileGraphProvider if needed.</p>\n<details>\n <summary>Example\
    \ agglomerate config</summary>\n<div class=\"highlight highlight-source-json\"\
    ><pre>{\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>experiment<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>hemi<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>setup<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>setup01<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>iteration<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">400000</span>,\n  <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>affs_file<span class=\"pl-pds\"\
    >\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo.zarr<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>affs_dataset<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>/volumes/affs<span class=\"pl-pds\">\"\
    </span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fragments_file<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>foo.zarr<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>fragments_dataset<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/volumes/fragments<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>block_size<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-c1\"\
    >1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>],\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>context<span class=\"\
    pl-pds\">\"</span></span>: [<span class=\"pl-c1\">248</span>, <span class=\"pl-c1\"\
    >248</span>, <span class=\"pl-c1\">248</span>],\n  <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>db_host<span class=\"pl-pds\">\"</span></span>: <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>mongodb client<span class=\"\
    pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>db_name<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>foo<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>num_workers<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-c1\">4</span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>queue<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>local<span class=\"pl-pds\">\"</span></span>,\n  <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>merge_function<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hist_quant_75<span\
    \ class=\"pl-pds\">\"</span></span>\n}</pre></div>\n</details>\n<h3>\n<a id=\"\
    user-content-find-segments\" class=\"anchor\" href=\"#find-segments\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Find\
    \ segments</h3>\n<p>In contrast to the above three methods, when <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/04_find_segments.py\"\
    >creating LUTs</a> there just needs to be enough RAM to hold the RAG in memory.\
    \ The only thing done in parallel is reading the graph (<code>graph_provider.read_blockwise()</code>).\
    \ It could be adapted to use multiprocessing/dask for distributing the connected\
    \ components for each threshold, but if the rag is too large there will be pickling\
    \ errors when passing the nodes/edges. Daisy doesn't need to be used for scheduling\
    \ here since nothing is written to containers.</p>\n<details>\n <summary>Example\
    \ find segments config</summary>\n<div class=\"highlight highlight-source-json\"\
    ><pre>{\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>db_host<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mongodb\
    \ client<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>db_name<span class=\"pl-pds\">\"</span></span>: <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo<span class=\"pl-pds\">\"\
    </span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fragments_file<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>foo.zarr<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>edges_collection<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>edges_hist_quant_75<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>thresholds_minmax<span class=\"pl-pds\">\"</span></span>: [<span class=\"\
    pl-c1\">0</span>, <span class=\"pl-c1\">1</span>],\n  <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>thresholds_step<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-c1\">0.02</span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>block_size<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-c1\"\
    >1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>],\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>num_workers<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-c1\">5</span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>fragments_dataset<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/volumes/fragments<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>run_type<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>test<span class=\"pl-pds\">\"</span></span>\n\
    }</pre></div>\n</details>\n<h3>\n<a id=\"user-content-extract-segmentation\" class=\"\
    anchor\" href=\"#extract-segmentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Extract segmentation</h3>\n<p>This\
    \ <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/05_extract_segmentation_from_lut.py\"\
    >script</a> does use daisy to write the segmentation to file, but doesn't necessarily\
    \ require bsub/sbatch to distribute (you can run locally).</p>\n<details>\n <summary>Example\
    \ extract segmentation config</summary> \n<div class=\"highlight highlight-source-json\"\
    ><pre>{\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fragments_file<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>foo.zarr<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>fragments_dataset<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/volumes/fragments<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>edges_collection<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>edges_hist_quant_75<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>threshold<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">0.4</span>,\n  <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>block_size<span class=\"pl-pds\"\
    >\"</span></span>: [<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>,\
    \ <span class=\"pl-c1\">1000</span>],\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>out_file<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>foo.zarr<span class=\"pl-pds\">\"</span></span>,\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>out_dataset<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>volumes/segmentation_40<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>num_workers<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\"\
    >3</span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>run_type<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>test<span class=\"pl-pds\">\"</span></span>\n}</pre></div>\n</details>\n\
    <h3>\n<a id=\"user-content-evaluate-volumes\" class=\"anchor\" href=\"#evaluate-volumes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Evaluate volumes</h3>\n<p><a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/05_evaluate_volumes.py\"\
    >Evaluate</a> Voi scores. Assumes dense voxel ground truth (not skeletons). This\
    \ also assumes the ground truth (and segmentation) can fit into memory, which\
    \ was fine for hemi and fib25 volumes assuming ~750 GB of RAM. The script should\
    \ probably be refactored to run blockwise.</p>\n<details>\n <summary>Example evaluate\
    \ volumes config</summary>\n<div class=\"highlight highlight-source-json\"><pre>{\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>experiment<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hemi<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>setup<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>setup01<span class=\"pl-pds\">\"</span></span>,\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>iteration<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-c1\">400000</span>,\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>gt_file<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hemi_roi_1.zarr<span class=\"\
    pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gt_dataset<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>volumes/labels/neuron_ids<span class=\"pl-pds\">\"</span></span>,\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fragments_file<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo.zarr<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>fragments_dataset<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>/volumes/fragments<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>db_host<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>mongodb client<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>rag_db_name<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>edges_collection<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>edges_hist_quant_75<span class=\"pl-pds\">\"</span></span>,\n  <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>scores_db_name<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scores<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>thresholds_minmax<span class=\"pl-pds\">\"</span></span>: [<span class=\"\
    pl-c1\">0</span>, <span class=\"pl-c1\">1</span>],\n  <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>thresholds_step<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-c1\">0.02</span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>num_workers<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\"\
    >4</span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>method<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>vanilla<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>run_type<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test<span class=\"pl-pds\"\
    >\"</span></span>\n}</pre></div>\n</details>\n<h3>\n<a id=\"user-content-evaluate-annotations\"\
    \ class=\"anchor\" href=\"#evaluate-annotations\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Evaluate annotations</h3>\n<p>For\
    \ the zebrafinch, ground truth skeletons were used due to the size of the dataset.\
    \ These skeletons were cropped, masked, and relabelled for the sub Rois that were\
    \ tested in the paper. We <a href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/05_evaluate_annotations.py\"\
    >evaluated</a> voi, erl, and the mincut metric on the consolidated skeletons.\
    \ The current implementation could be refactored / made more modular. It also\
    \ uses <code>node_collections</code> which are now deprecated in daisy. To use\
    \ with the current implementation, you should checkout daisy commit <code>39723ca</code>.</p>\n\
    <details>\n <summary>Example evaluate annotations config</summary>\n<div class=\"\
    highlight highlight-source-json\"><pre>{\n  <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>experiment<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>zebrafinch<span class=\"pl-pds\">\"</span></span>,\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>setup<span class=\"pl-pds\"\
    >\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>setup01<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>iteration<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\"\
    >400000</span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>config_slab<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>mtlsd<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>fragments_file<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo.zarr<span class=\"\
    pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fragments_dataset<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>/volumes/fragments<span class=\"pl-pds\">\"</span></span>,\n  <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>edges_db_host<span class=\"pl-pds\"\
    >\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mongodb\
    \ client<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>edges_db_name<span class=\"pl-pds\">\"</span></span>:\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>foo<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>edges_collection<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>edges_hist_quant_75<span class=\"pl-pds\">\"</span></span>,\n  <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>scores_db_name<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scores<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>annotations_db_host<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>mongo client<span class=\"pl-pds\">\"</span></span>,\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>annotations_db_name<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>foo<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>annotations_skeletons_collection_name<span class=\"\
    pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>zebrafinch<span\
    \ class=\"pl-pds\">\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>node_components<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>zebrafinch_components<span class=\"pl-pds\"\
    >\"</span></span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>node_mask<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>zebrafinch_mask<span class=\"pl-pds\">\"</span></span>,\n  <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>roi_offset<span class=\"pl-pds\">\"</span></span>:\
    \ [<span class=\"pl-c1\">50800</span>, <span class=\"pl-c1\">43200</span>, <span\
    \ class=\"pl-c1\">44100</span>],\n  <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>roi_shape<span class=\"pl-pds\">\"</span></span>: [<span class=\"pl-c1\"\
    >10800</span>, <span class=\"pl-c1\">10800</span>, <span class=\"pl-c1\">10800</span>],\n\
    \  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>thresholds_minmax<span\
    \ class=\"pl-pds\">\"</span></span>: [<span class=\"pl-c1\">0.5</span>, <span\
    \ class=\"pl-c1\">1</span>],\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>thresholds_step<span class=\"pl-pds\">\"</span></span>: <span class=\"\
    pl-c1\">1</span>,\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>run_type<span\
    \ class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>11_micron_roi_masked<span class=\"pl-pds\">\"</span></span>\n}</pre></div>\n\
    \ </details>\n"
  stargazers_count: 8
  subscribers_count: 5
  topics: []
  updated_at: 1624944172.0
funkelab/synful:
  data_format: 2
  description: Synaptic Partner Detection in 3D Microscopy Volumes
  filenames:
  - singularity/Singularity_py2.7.recipe
  - singularity/Singularity_py3.recipe
  full_name: funkelab/synful
  latest_release: v1.0
  readme: '<p><a href="https://zenodo.org/badge/latestdoi/166422086" rel="nofollow"><img
    src="https://camo.githubusercontent.com/e31cfa1af0774be894dee535edc05a6536309dc42e048f576dc489a330b1f8ec/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3136363432323038362e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/166422086.svg" style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-synful" class="anchor" href="#synful" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Synful</h1>

    <h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p>Synful: A project for the automated detection of synaptic partners in Electron
    Microscopy brain data using U-Nets (type of Convolutional Neural Network).</p>

    <p>This repository provides train and predict scripts for synaptic partner detection.
    For more details, see our <a href="https://www.biorxiv.org/content/10.1101/2019.12.12.874172v1"
    rel="nofollow">bioRxiv preprint</a>.</p>

    <p>We used the method to predict 244 Million synaptic partners in the full adult
    fly brain (FAFB) dataset.

    Please see <a href="https://github.com/funkelab/synful_fafb">https://github.com/funkelab/synful_fafb</a>
    for data dissemination and benchmark datasets.</p>

    <p>Please don''t hesitate to open

    an issue or write us an email (<a href="mailto:buhmannj@janelia.hhmi.org">Julia

    Buhmann</a> or <a href="mailto:funkej@janelia.hhmi.org">Jan

    Funke</a>) if you have any questions!</p>

    <ul>

    <li>[x] Add train scripts</li>

    <li>[x] Add inference scripts</li>

    <li>[x] Add download links for pretrained models</li>

    </ul>

    <h2>

    <a id="user-content-method" class="anchor" href="#method" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Method</h2>

    <p>The pipeline processes 3D raw data in two steps into synaptic partners:</p>

    <ol>

    <li>inference of a) <code>syn_indicator_mask</code> (postsynaptic locations) and
    b) <code>direction_vector</code> (vector pointing from postsynaptic location to
    its presynaptic partner)</li>

    <li>synapse extraction: a) locations extractions based on <code>syn_indicator_mask</code>
    and b) finding presynaptic partner based on <code>direction_vector</code>

    </li>

    </ol>

    <p><a href="docs/_static/method_overview.png" target="_blank" rel="noopener noreferrer"><img
    src="docs/_static/method_overview.png" alt="method_figure" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-system-requirements" class="anchor" href="#system-requirements"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>System
    Requirements</h2>

    <ul>

    <li>Hardware requirements

    <ul>

    <li>training and prediction requires at least one GPU with sufficient memory (12
    GB)</li>

    <li>For instance, we mostly used <code>GeForce GTX TITAN X 12 GB</code> for our
    project</li>

    </ul>

    </li>

    <li>Software requirements

    <ul>

    <li>Software has been tested on Linux (Ubuntu 16.04)</li>

    </ul>

    </li>

    </ul>

    <h2>

    <a id="user-content-installation-guide" class="anchor" href="#installation-guide"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation
    Guide</h2>

    <p>from source (creating a conda env is optional, but recommended).</p>

    <ul>

    <li>Clone this repository.</li>

    <li>In a terminal:</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>conda create -n <span class="pl-k">&lt;</span>conda_env_name<span
    class="pl-k">&gt;</span> python=3.6

    <span class="pl-c1">source</span> activate <span class="pl-k">&lt;</span>conda_env_name<span
    class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> synful

    pip install -r requirements.txt

    python setup.py install</pre></div>

    <p>If you are interested in using the package for training and prediction, additionally
    add tensorflow and funlib.learn.tensorflow to your conda env:</p>

    <div class="highlight highlight-source-shell"><pre>conda install tensorflow-gpu=1.14
    cudatoolkit=10.0

    pip install git+git://github.com/funkelab/funlib.learn.tensorflow@0712fee6b6c083c6bfc86e76f475b2e40b3c64f2

    </pre></div>

    <h4>

    <a id="user-content-install-time" class="anchor" href="#install-time" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install time</h4>

    <p>Installation should take around 5 mins (including 3 mins for the tensorflow
    installation).</p>

    <h2>

    <a id="user-content-training" class="anchor" href="#training" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Training</h2>

    <p>Training scripts are found in</p>

    <pre><code>train/&lt;setup&gt;

    </code></pre>

    <p>where <code>&lt;setup&gt;</code> is the name of a particular network configuration.

    In such a  directory, you will find two files:</p>

    <ul>

    <li>

    <code>generate_network.py</code> (generates a tensorflow network based on the
    parameter.json file in the same directoy)</li>

    <li>

    <code>train.py</code> (starts training)</li>

    </ul>

    <p>To get started, have a look at the train script in <a href="train/setup01">train/setup01/train.py</a>.</p>

    <p>To start training:</p>

    <div class="highlight highlight-source-shell"><pre>python generate_network.py
    parameter.json

    python train.py parameter.json</pre></div>

    <ul>

    <li>setup01: parameter.json is set to train a network on post-synaptic sites (single-task
    network)</li>

    <li>setup02: parameter.json is set to train on direction vectors (single-task
    network)</li>

    <li>setup03: parameter.json is set to train on both post-synaptic sites and direction
    vectors (multi-task network)</li>

    </ul>

    <h4>

    <a id="user-content-details-on-hyperparameters" class="anchor" href="#details-on-hyperparameters"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Details
    on hyperparameters</h4>

    <p>When training a network, you can set following hyperparameters in <code>scripts/train/&lt;setup01/02/03&gt;/parameter.json</code></p>

    <p>Parameters to set the architecture of the network (also see <a href="https://github.com/funkelab/funlib.learn.tensorflow/blob/master/funlib/learn/tensorflow/models/unet.py#L506">doc</a>
    where we create the U-Net)</p>

    <ul>

    <li>

    <code>input_size</code>: the dimensions of the cube that is used as input (called
    a mini-batch)</li>

    <li>

    <code>downsample_factor</code> = [[1, 3, 3], [1, 3, 3], [3, 3, 3]] creates a U-Net
    with four resolution levels

    <ul>

    <li>the first one being the original resolution, the second one with downsampled
    feature maps with factos [1, 3, 3] etc.</li>

    </ul>

    </li>

    <li>

    <code>fmap_num</code>: Number of feature maps in the first layer (we used 4 in
    the paper)</li>

    <li>

    <code>fmap_inc_factor</code>: In each layer, we use <code>fmap_inc_factor</code>
    to increase our number of feature maps (we used 5 and 12 in the paper)

    <ul>

    <li>Eg. if we have <code>fmap_num = 4</code> and <code>fmap_inc_factor = 5</code>
    , we have 20 in our first layer, 100 in our second layer ...</li>

    </ul>

    </li>

    <li>

    <code>unet_model</code>: vanilla, or dh_unet; vanille=single-task network, dh_unet=multitask
    network with two different upsampling paths</li>

    </ul>

    <p>Training parameters</p>

    <ul>

    <li>

    <code>learning_rate</code>: we used the AdamOptimizer across all experiments,
    with beta1=0.95,beta2=0.999,epsilon=1e-8</li>

    </ul>

    <p>ST / MT parameters</p>

    <ul>

    <li>

    <code>loss_comb_type</code>: in a multi-task setting, how to combine the two different
    losses</li>

    <li>

    <code>m_loss_scale</code> : loss weight for post-synaptic mask</li>

    <li>

    <code>d_loss_scale</code> : loss weight for direction vector field</li>

    </ul>

    <p>Balancing parameters needed to account for sparsity of synaptic sites</p>

    <ul>

    <li>

    <code>reject_probability</code> : 0.95 - p_rej in paper --&gt; reject empty mini-batches
    with probability <code>reject_probability</code>

    </li>

    <li>

    <code>clip_range</code> : the loss is scaled with the inverse class frequency
    ratio of foreground-and background voxels, clipping at <code>clip_range</code>

    </li>

    </ul>

    <h4>

    <a id="user-content-training-runtime" class="anchor" href="#training-runtime"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training
    runtime</h4>

    <p>Training takes between 3 and 10 days (depending on the size of the network),
    but you should see reasonable results within a day (after 90k iterations).</p>

    <h3>

    <a id="user-content-monitoring-training" class="anchor" href="#monitoring-training"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Monitoring
    Training</h3>

    <p>To visualize snapshots that are produced during training use this <a href="scripts/visualization/visualize_snapshot.py">script</a>:</p>

    <pre><code>python -i visualize_snapshot.py 300001 setup01

    </code></pre>

    <p>in order to load iteration <code>300001</code> of training setup <code>setup01</code>
    (use -1 to indicate most recent snapshot)</p>

    <h2>

    <a id="user-content-inference" class="anchor" href="#inference" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Inference</h2>

    <p>Once you trained a network, you can use this script to run inference:</p>

    <pre><code>cd scripts/predict/

    python predict_blockwise.py predict_template.json

    </code></pre>

    <p>Adapt following parameters in the configfile &lt;scripts/predict/predict_template.json&gt;:</p>

    <ul>

    <li>

    <code>db_host</code> --&gt; Put here the name of your running mongodb server (this
    is used to track which chunks are processed)</li>

    <li>

    <code>raw_file</code> --&gt; Put here the filepath of your raw data (as an example
    you can use the CREMI data that you can download from <a href="http://www.cremi.org"
    rel="nofollow">www.cremi.org</a>)</li>

    </ul>

    <p>For a full list of parameters and explanation, see: &lt;scripts/predict/predict_blockwise.py&gt;.</p>

    <h4>

    <a id="user-content-inference-runtime" class="anchor" href="#inference-runtime"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Inference
    runtime</h4>

    <p>Processing a CREMI cube (5 microns X 5 microns x 5 microns) takes ~4 minutes
    on a single GPU.</p>

    <h2>

    <a id="user-content-pretrained-models" class="anchor" href="#pretrained-models"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pretrained
    Models</h2>

    <p>We provide pretrained models, that we discuss in detail in our <a href="https://www.biorxiv.org/content/10.1101/2019.12.12.874172v2"
    rel="nofollow">bioRxiv preprint</a>. You will find the results of our gridsearch
    and the parameters that we used in Figure 3 <code>Validation results on CREMI
    dataset</code>.</p>

    <p>We provide four models that you can download from <a href="https://www.dropbox.com/s/301382766164ism/pretrained.zip?dl=0"
    rel="nofollow">here</a>.</p>

    <p>Please extract the zip file into &lt;scripts/train/&gt; of this repository,
    this will add for each model a setup directory with the necassary config files,
    tensorflow checkpoint and predict script.</p>

    <p>For instance for <code>p_setup52</code> (marked orange in Figure 3, one of
    the best performing models), you will get all relevant files in &lt;scripts/train/p_setup52&gt;.

    To run inference, you have to change the setup parameter in the predict config
    file to <code>p_setup52</code> and proceed according to <a href="#Inference">inference
    section</a>.</p>

    <h4>

    <a id="user-content-details-about-the-provided-models" class="anchor" href="#details-about-the-provided-models"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Details
    about the provided models</h4>

    <table>

    <thead>

    <tr>

    <th>setup</th>

    <th>specs</th>

    <th>f-score with seg</th>

    <th>f-score without</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>p_setup52 (+p_setup10)</td>

    <td>big, curriculum, CE, ST</td>

    <td>0.76</td>

    <td>0.74</td>

    </tr>

    <tr>

    <td>p_setup51</td>

    <td>big, curriculum, CE, MT_2</td>

    <td>0.76</td>

    <td>0.73</td>

    </tr>

    <tr>

    <td>p_setup54 (+p_setup05)</td>

    <td>small, curriculum, MSE, ST</td>

    <td>0.76</td>

    <td>0.7</td>

    </tr>

    <tr>

    <td>p_setup45 (+p_setup05)</td>

    <td>small, standard, MSE, MT2</td>

    <td>0.73</td>

    <td>0.68</td>

    </tr>

    </tbody>

    </table>

    <p>Note, that for the models that have an underlying ST architecture we also indicate
    the setup for the corresponding direction-vector-models (p_setup05+p_setup10).

    If you want to use the model with highest accuracy, pick <code>p_setup52</code>;
    If you want to use a model that gives reasonnable results, but also has fast inference
    runtime, pick <code>p_setup54</code>.</p>

    '
  stargazers_count: 1
  subscribers_count: 7
  topics: []
  updated_at: 1616581090.0
funkelab/synister:
  data_format: 2
  description: Neurotransmitter prediction from EM.
  filenames:
  - singularity/Singularity
  full_name: funkelab/synister
  latest_release: null
  readme: '<h1>

    <a id="user-content-package-for-training-validating-and-testing-synister-networks"
    class="anchor" href="#package-for-training-validating-and-testing-synister-networks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Package
    for training, validating and testing Synister networks.</h1>

    <p><strong>Related Paper</strong> <a href="https://www.biorxiv.org/content/10.1101/2020.06.12.148775v2"
    rel="nofollow">Neurotransmitter Classification from Electron Microscopy Images
    at Synaptic Sites in Drosophila.</a></p>

    <p><strong>Related Repositories</strong></p>

    <ol>

    <li><a href="https://github.com/funkelab/synistereq">Package for on demand predictions
    with a production network.</a></li>

    <li><a href="https://github.com/funkelab/synisterbrain">Package for high performance
    full brain predictions.</a></li>

    <li><a href="https://github.com/nilsec/synisterest">Webservice.</a></li>

    </ol>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <ol>

    <li>Singularity</li>

    </ol>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">cd
    singularity</span>

    <span class="pl-c1">make</span></pre></div>

    <ol start="2">

    <li>Conda</li>

    </ol>

    <pre><code>conda create -n synister python=3.6 numpy scipy cython pylp -c funkey

    conda activate synister

    pip install -r requirements.txt

    pip install .

    </code></pre>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <h3>

    <a id="user-content-0-creating-a-mongo-db-database-with-the-provided-data" class="anchor"
    href="#0-creating-a-mongo-db-database-with-the-provided-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>0. Creating a mongo
    DB database with the provided data.</h3>

    <p>An export of the three collections constituting the synister FAFB database
    used for all described experiments can be found at <code>data/fafb_v3</code>.
    The three files contain:</p>

    <ol>

    <li>Location, id, skid, brain region and split for each synapse (synapses(_v3).json).</li>

    <li>Skid, neurotransmitter, hemilineage id for each skeleton (skeletons(_v3).json).</li>

    <li>Hemilineage name, hemilineage id for each hemilineage (hemilineages(_v3).json).</li>

    </ol>

    <p>To reproduce the experiment each json file should be imported as a collection
    with names "synapses", "skeletons", "hemilineages" in one mongo database (for
    additional instructions on how to import json files in a mongo db click <a href="https://docs.mongodb.com/database-tools/mongoimport/"
    rel="nofollow">here</a>). Dictionary keys are field names. Provided splits can
    be reproduced using <code>synister/split.py</code>, which searches for the optimally
    balanced split in terms of neurotransmitter distribution for any given superset,
    such as hemilineage id, skeleton id or brain region.</p>

    <p>For training on other data, recreate the database scheme shown here (required
    are a "synapses" and a "skeletons" collection) and adapt config files to match
    the new database name.</p>

    <h3>

    <a id="user-content-1-training-a-network" class="anchor" href="#1-training-a-network"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.
    Training a network.</h3>

    <h4>

    <a id="user-content-prepare-training" class="anchor" href="#prepare-training"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Prepare
    training</h4>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">python
    prepare_training.py -d &lt;base_dir&gt; -e &lt;experiment_name&gt; -t &lt;train_id&gt;</span></pre></div>

    <p>This creates a new directory at the specified path and initialises default
    config files for the run.</p>

    <h4>

    <a id="user-content-run-training" class="anchor" href="#run-training" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run Training</h4>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">cd
    &lt;base_dir&gt;/&lt;experiment_name&gt;/02_train/setup_t&lt;train_id&gt;</span></pre></div>

    <p>Edit config files to match architecture, database and compute resources to
    train with.</p>

    <p>For example configs, training a VGG on the skeleton split, inside a singularity
    container, on a gpu queue see:</p>

    <pre><code>example_configs/train_config.ini


    [Training]

    synapse_types = gaba, acetylcholine, glutamate, serotonin, octopamine, dopamine

    input_shape = 16, 160, 160

    fmaps = 12

    batch_size = 8

    db_credentials = synister_data/credentials/db_credentials.ini

    db_name_data = synister_v3

    split_name = skeleton

    voxel_size = 40, 4, 4

    raw_container = /nrs/saalfeld/FAFB00/v14_align_tps_20170818_dmg.n5

    raw_dataset = volumes/raw/s0

    downsample_factors = (1,2,2), (1,2,2), (1,2,2), (2,2,2)

    network = VGG

    fmap_inc = 2, 2, 2, 2

    n_convolutions = 2, 2, 2, 2

    network_appendix = None

    </code></pre>

    <pre><code>example_configs/worker_config.ini


    [Worker]

    singularity_container = synister/singularity/synister.img

    num_cpus = 5

    num_block_workers = 1

    num_cache_workers = 5

    queue = gpu_any

    mount_dirs = /nrs, /scratch, /groups, /misc

    </code></pre>

    <p>Finally, to submit the train job with the desired number of iterations run:</p>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">python
    train.py &lt;num_iterations&gt;</span></pre></div>

    <p>We recommend training for at least 500,000 iterations for FAVB_v3 splits.</p>

    <p>For visualizing training progress run:</p>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">tensorboard
    --logdir &lt;base_dir&gt;/&lt;experiment_name&gt;/02_train/setup_t&lt;train_id&gt;/log</span></pre></div>

    <p>Snapshots are written to:</p>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">&lt;base_dir&gt;/&lt;experiment_name&gt;/02_train/setup_t&lt;train_id&gt;/snapshots</span></pre></div>

    <h3>

    <a id="user-content-2-validating-a-trained-network" class="anchor" href="#2-validating-a-trained-network"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.
    Validating a trained network.</h3>

    <h4>

    <a id="user-content-prepare-validation-runs" class="anchor" href="#prepare-validation-runs"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Prepare
    validation runs</h4>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">python
    prepare_prediction.py -v -d &lt;base_dir&gt; -e &lt;experiment_name&gt; -t &lt;train_id&gt;
    -i &lt;iter_0&gt; &lt;iter_1&gt; &lt;iter_2&gt; ... &lt;iter_N&gt; </span></pre></div>

    <p>This will create N prediction directories with appropriately initialized config
    files, one for each given train iteration &lt;iter_k&gt;. The -v flag sets the
    split part of the chosen split type to validation, only pulling those synapses
    from the DB that are tagged as validation synapses.</p>

    <pre><code>example_configs/worker_config.ini


    [Predict]

    train_checkpoint = &lt;experiment_name&gt;/02_train/setup_t&lt;train_id&gt;/model_checkpoint_&lt;iter_k&gt;

    experiment = &lt;experiment_name&gt;

    train_number = 0

    predict_number = 0

    synapse_types = gaba, acetylcholine, glutamate, serotonin, octopamine, dopamine

    input_shape = 16, 160, 160

    fmaps = 12

    batch_size = 8

    db_credentials = synister_data/credentials/db_credentials.ini

    db_name_data = synister_v3

    split_name = skeleton

    voxel_size = 40, 4, 4

    raw_container = /nrs/saalfeld/FAFB00/v14_align_tps_20170818_dmg.n5

    raw_dataset = volumes/raw/s0

    downsample_factors = (1, 2, 2), (1, 2, 2), (1, 2, 2), (2, 2, 2)

    split_part = validation

    overwrite = False

    network = VGG

    fmap_inc = 2, 2, 2, 2

    n_convolutions = 2, 2, 2, 2

    network_appendix = None

    </code></pre>

    <p>For most use cases the automatically initialized predict config does not require
    any edits. If run as is, predictions will be written into the database under:</p>

    <pre><code>&lt;db_name&gt;_predictions.&lt;split_name&gt;_&lt;experiment_name&gt;_t&lt;train_id&gt;_p&lt;predict_id&gt;

    </code></pre>

    <p>To start the prediction, run:</p>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">cd
    &lt;base_dir&gt;/&lt;experiment_name&gt;/03_predict/setup_t&lt;train_id&gt;_p&lt;predict_id&gt;</span>

    <span class="pl-c1">python predict.py</span></pre></div>

    <p>If the collection already exists the script will abort. A collection can be
    overwritten by setting overwrite=True in the predict config. Parallel prediction
    with multiple GPUs can be done by setting num_block_workers=num_gpus in the worker_config
    file. Prediction speed and expected time to finish will be shown in the console.</p>

    <p>For submitting multiple predictions to the cluster at once run the provided
    convenience script:</p>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">python
    start_predictions -d &lt;base_dir&gt; -e &lt;experiment_name&gt; -t &lt;train_id&gt;
    -p &lt;predict_id_0&gt; &lt;predict_id_1&gt; ... &lt;predict_id_N&gt;</span></pre></div>

    <h3>

    <a id="user-content-3-testing-a-trained-network" class="anchor" href="#3-testing-a-trained-network"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3.
    Testing a trained network.</h3>

    <h4>

    <a id="user-content-prepare-test-runs" class="anchor" href="#prepare-test-runs"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Prepare
    test runs</h4>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">python
    prepare_prediction.py -d &lt;base_dir&gt; -e &lt;experiment_name&gt; -t &lt;train_id&gt;
    -i &lt;iter_0&gt; &lt;iter_1&gt; &lt;iter_2&gt; ... &lt;iter_N&gt;</span></pre></div>

    <p>Similar to validation this prepares the relevant dictionaries and config files
    but sets the split part to "test".

    Starting the prediction follow the same pattern as before:</p>

    <div class="highlight highlight-text-shell-session"><pre><span class="pl-c1">cd
    &lt;base_dir&gt;/&lt;experiment_name&gt;/03_predict/setup_t&lt;train_id&gt;_p&lt;predict_id&gt;</span>

    <span class="pl-c1">python predict.py</span></pre></div>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1624487646.0
fzimmermann89/idi:
  data_format: 2
  description: Simulating, Reconstructing and Analysing Data for FEL IDI Experiments
  filenames:
  - Singularity
  - Singularity.simple
  - Singularity.py38
  full_name: fzimmermann89/idi
  latest_release: '210609'
  readme: '<p>CAVE: Hic sunt dracones</p>

    <p><em>The code is a mess, undocumented and only certain code paths are tested.</em></p>

    <h1>

    <a id="user-content-idi---incoherent-diffraction-imaging" class="anchor" href="#idi---incoherent-diffraction-imaging"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>IDI
    - INCOHERENT DIFFRACTION IMAGING</h1>

    <p><a href="https://singularity-hub.org/collections/4824" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg"
    alt="tests" style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://www.travis-ci.com/fzimmermann89/idi.svg?branch=master"
    style="max-width:100%;"></a></p>

    <p>Singularity Image now at <a href="https://cloud.sylabs.io/library/_container/607b669a4ad4aa1fdea0c43c"
    rel="nofollow">library://fzimmermann89/idi/idi</a></p>

    <p>Conda Pacakges at <a href="https://anaconda.org/zimmf/idi" rel="nofollow">zimmf/idi</a></p>

    <p>PIP Source at <a href="https://pypi.org/project/idi/" rel="nofollow">idi</a></p>

    <p>Wheels at <a href="https://github.com/fzimmermann89/idi/releases/latest">Releases</a></p>

    <h2>

    <a id="user-content-content-of-the-repo" class="anchor" href="#content-of-the-repo"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>content
    of the repo</h2>

    <ul>

    <li>ipynb: example notebooks</li>

    <li>simulation: simulation of incoherent images</li>

    <li>reconstruction: direct and ft based reconstruction</li>

    <li>util: some small utilities for data analysis, geometry and random distributions,
    etc.</li>

    </ul>

    <h2>

    <a id="user-content-preparation-for-slac-sdf" class="anchor" href="#preparation-for-slac-sdf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>preparation
    for slac sdf:</h2>

    <p>Use Singulariy, if using OOD launcher, use the following to start a jupyterhub</p>

    <pre><code>    function jupyter() { singularity run --app jupyter --nv -B /sdf,/gpfs,/scratch,/lscratch
    library://fzimmermann89/idi/idi $@; }

    </code></pre>

    <h2>

    <a id="user-content-preparation-for-sacla" class="anchor" href="#preparation-for-sacla"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>preparation
    for sacla:</h2>

    <ul>

    <li>Download and install miniconda, setup ssh tunnel for web access.</li>

    <li><code>conda create -n local3 python=3.7 numpy mkl mkl-dev ipython ipykernel
    cython jinja2 numba numexpr matplotlib six scipy jupyterlab</code></li>

    <li><code>conda activate local3</code></li>

    <li><code>pip install https://github.com/fzimmermann89/idi/</code></li>

    <li><code>python -m ipykernel install --user --name local-simulation-env3 --display-name
    "local simulation(py37)"</code></li>

    </ul>

    <p>(C) Felix Zimmermann</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - idi
  - reconstruction
  - simulation
  - xray
  - incoherent-images
  - fel
  updated_at: 1624272483.0
garciagenrique/template_project_escape:
  data_format: 2
  description: A template project to provide software to ESCAPE.
  filenames:
  - Singularity/Singularity
  full_name: garciagenrique/template_project_escape
  latest_release: v0.0.3-dev
  readme: "<h1>\n<a id=\"user-content-template_project_escape\" class=\"anchor\" href=\"\
    #template_project_escape\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>template_project_escape</h1>\n<p><a href=\"\
    https://doi.org/10.5281/zenodo.4923992\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/869b462bf3a319fe4fc2ffa52fb6b0f7c8e42eb4e0ed4bc2482306b9fd5aafab/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343932333939322e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4923992.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape/-/commits/master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a25ea71f12537b69d5aca8b409685333243d4e65ceb91c5a401dadb6eacea20e/68747470733a2f2f6769746c61622e696e3270332e66722f657363617065323032302f7770332f74656d706c6174655f70726f6a6563745f6573636170652f6261646765732f6d61737465722f706970656c696e652e737667\"\
    \ alt=\"pipeline status\" data-canonical-src=\"https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape/badges/master/pipeline.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fd551ba4b042d89480347a0e74e31af63b356b2cac1116c7b80038f41b04a581/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667\"\
    \ alt=\"License: MIT\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-green.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p align=\"center\">\n   <a href=\"https://camo.githubusercontent.com/64f9054d866a78f16e1451647c19b22fc159a58191e004612257ce5277bd6db9/68747470733a2f2f63646e2e65736f2e6f72672f696d616765732f6c617267652f616e6e3138303834612e6a7067\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/64f9054d866a78f16e1451647c19b22fc159a58191e004612257ce5277bd6db9/68747470733a2f2f63646e2e65736f2e6f72672f696d616765732f6c617267652f616e6e3138303834612e6a7067\"\
    \ width=\"640\" height=\"453\" data-canonical-src=\"https://cdn.eso.org/images/large/ann18084a.jpg\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p>A simple template project to provide\
    \ software to ESCAPE.</p>\n<p>This repository shows the <strong>basic content</strong>\
    \ that should be included in a project (following the\n<a href=\"https://opensource.guide/starting-a-project/\"\
    \ rel=\"nofollow\">opensource guide</a>):</p>\n<ul>\n<li>An <a href=\"https://help.github.com/en/github/creating-cloning-and-archiving-repositories/licensing-a-repository#where-does-the-license-live-on-my-repository\"\
    >open source</a>\n<strong>license</strong>.</li>\n<li>A <a href=\"https://help.github.com/en/github/getting-started-with-github/create-a-repo#commit-your-first-change\"\
    ><strong>README</strong> file</a>,\nsimilar to this one.</li>\n<li>Contributing\
    \ guidelines.\n<ul>\n<li>See below the general guidelines for the ESCAPE repository.</li>\n\
    </ul>\n</li>\n<li>A <a href=\"https://opensource.guide/code-of-conduct/\" rel=\"\
    nofollow\">code of conduct</a>.\n<ul>\n<li>Check why is a good idea to add one.</li>\n\
    </ul>\n</li>\n<li>The repository itself.</li>\n</ul>\n<p>It would be highly suitable\
    \ to include too:</p>\n<ul>\n<li>A setup file as well as the basic commands to\
    \ install the library (see below).</li>\n<li>A <code>.gitignore</code> file.</li>\n\
    <li>Unitary and integration tests, and ideally a CI pipeline.</li>\n</ul>\n<p><strong>Please\
    \ feel free to clone / fork / template this project!</strong> (For example, look\
    \ to left of the\n<code>Clone or download</code> button in the <a href=\"https://github.com/garciagenrique/template_project_escape\"\
    >GitHub</a> site).</p>\n<ul>\n<li>For a detailed explanation of how to submit\
    \ a contribution to a project / repository (Fork, create a branch, make\na pull\
    \ request...), you can have a look to the <a href=\"https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution\"\
    \ rel=\"nofollow\">opensource guide</a>\nand/or the <a href=\"https://git-scm.com/doc\"\
    \ rel=\"nofollow\">git's documentation</a>.</li>\n<li>Not that if you have login\
    \ GitLab by using the <code>[Shibbolenth]</code> service (eduGAIN, F\xE9d\xE9\
    ration d'Identit\xE9s\nRENATER), you will need to <a href=\"https://gitlab.in2p3.fr/help/ssh/README#generating-a-new-ssh-key-pair\"\
    \ rel=\"nofollow\">add a SSH key</a> to\nyour GitLab profile if you want to 'push'\
    \ your changes to the server.</li>\n</ul>\n<h1>\n<a id=\"user-content-contribute-to-the-escape-ossr\"\
    \ class=\"anchor\" href=\"#contribute-to-the-escape-ossr\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contribute\
    \ to the ESCAPE OSSR</h1>\n<p>If you want to provide software to the ESCAPE repository:</p>\n\
    <ul>\n<li>\n<p>Check the <a href=\"https://escape2020.pages.in2p3.fr/wp3/ossr-pages/page/contribute/contribute_ossr/\"\
    \ rel=\"nofollow\">ESCAPE OSSR guidelines</a>.</p>\n<ul>\n<li>For ESCAPE members,\
    \ follow the steps detailed in <a href=\"https://gitlab.in2p3.fr/escape2020/wp3/onboarding\"\
    \ rel=\"nofollow\">the onboarding project</a>\nto finalise your contribution and\
    \ the same onboarding process.</li>\n</ul>\n</li>\n<li>\n<p>All the code provided\
    \ should be uploaded to the <a href=\"https://zenodo.org/communities/escape2020/\"\
    \ rel=\"nofollow\">Zenodo ESCAPE community</a>.</p>\n</li>\n<li>\n<p>Check the\
    \ following <a href=\"https://escape2020.pages.in2p3.fr/wp3/ossr-pages/page/contribute/publish_tutorial/\"\
    \ rel=\"nofollow\">tutorial on how to publish content in Zenodo</a>,\nand how\
    \ to automatise the upload of each new release of your project.</p>\n</li>\n</ul>\n\
    <h1>\n<a id=\"user-content-this-project-also-includes\" class=\"anchor\" href=\"\
    #this-project-also-includes\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>This project also includes</h1>\n\
    <h2>\n<a id=\"user-content-1-how-to-automatise-the-building-of-a-singularity-image-and-upload-it-to-zenodo-using-the-gitlab-ci\"\
    \ class=\"anchor\" href=\"#1-how-to-automatise-the-building-of-a-singularity-image-and-upload-it-to-zenodo-using-the-gitlab-ci\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1. How to automatise the building of a Singularity image and upload\
    \ it to Zenodo using the GitLab-CI</h2>\n<p>A working example of how to automatise\
    \ the GitLab-CI to;</p>\n<ol>\n<li>create a Singularity image / container of your\
    \ code,</li>\n<li>make it available as a downloadable artifact within your project\
    \ and</li>\n<li>upload it to the <a href=\"https://zenodo.org/communities/escape2020\"\
    \ rel=\"nofollow\">ESCAPE OSSR</a>,</li>\n</ol>\n<p>can be found in the <code>.singularityci</code>,\
    \ and <code>Singularity</code> directories and in the <code>.gitlab-ci.yml</code>\
    \ file - the\n<code>build_singularity_image</code> stage. Please read carefully\
    \ all the README files.</p>\n<p>For an easy example of how to create a Singularity\
    \ receipt from scratch (and its corresponding container when executed),\nplease\
    \ have a look to the <code>singularity_utils</code> directory.</p>\n<h2>\n<a id=\"\
    user-content-2-how-to-automatise-the-building-of-a-docker-container-and-upload-it-to-the-gitlab-container-registry\"\
    \ class=\"anchor\" href=\"#2-how-to-automatise-the-building-of-a-docker-container-and-upload-it-to-the-gitlab-container-registry\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2. How to automatise the building of a Docker container and upload\
    \ it to the GitLab Container Registry</h2>\n<p>An example can be found in the\
    \ <code>Docker</code> directory and in the <code>.gitlab-ci.yml</code> file -\
    \  the\n<code>build_docker_image</code> stage.</p>\n<h1>\n<a id=\"user-content-install\"\
    \ class=\"anchor\" href=\"#install\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install</h1>\n<p>Example of how\
    \ to show installing instructions (and indeed the way to install this project).</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  $ git clone https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape.git\n\
    \  $ <span class=\"pl-c1\">cd</span> template_project_escape\n  $ pip install\
    \ <span class=\"pl-c1\">.</span></pre></div>\n<h1>\n<a id=\"user-content-citing\"\
    \ class=\"anchor\" href=\"#citing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Citing</h1>\n<p>Example of citing\
    \ (as well as the DOI to cite this project),</p>\n<p>In case of citing this repository,\
    \ use the following DOI:</p>\n<ul>\n<li>v2.2 <a href=\"https://doi.org/10.5281/zenodo.4923992\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/869b462bf3a319fe4fc2ffa52fb6b0f7c8e42eb4e0ed4bc2482306b9fd5aafab/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343932333939322e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4923992.svg\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>v2.1 <a href=\"https://doi.org/10.5281/zenodo.4790629\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c8f54e2f50cdf0c4d1bd5183d6472cae8b708efacd6a1319b443d8e456f41b7f/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343739303632392e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4790629.svg\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>v2.0 <a href=\"https://doi.org/10.5281/zenodo.3884963\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c27201272a77fc3ab3029c8c2c452e02a71736b7adaa0926bc45d3ac825598ed/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333838343936332e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3884963.svg\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>v1.1 <a href=\"https://doi.org/10.5281/zenodo.3743490\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/cbfe31d3a9bd48ef4554b414c3c2325276269a476a79ec0217aa9feccc87cecd/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333734333439302e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3743490.svg\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>v1.0 <a href=\"https://doi.org/10.5281/zenodo.3572655\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5ec734ee4c1c793eaa8f9c2b189a6eae6d7d2ccb5f2a92adeb8af479409cc7bb/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333537323635352e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3572655.svg\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n</ul>\n<p>Do not forget to include your\
    \ code / container into the <a href=\"https://zenodo.org/communities/escape2020/\"\
    \ rel=\"nofollow\">Zenodo ESCAPE community</a>.</p>\n<ul>\n<li>\n<em><strong>Note\
    \ that</strong></em> a DOI will be assigned in the moment create a new record/entry\
    \ in Zenodo.</li>\n</ul>\n<h1>\n<a id=\"user-content-license\" class=\"anchor\"\
    \ href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>License</h1>\n<p>Please check the licenses of the\
    \ code within the <code>.singularityci</code> directory before adding this template\n\
    to your project.</p>\n<h1>\n<a id=\"user-content-report-an-issue--ask-a-question\"\
    \ class=\"anchor\" href=\"#report-an-issue--ask-a-question\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Report an\
    \ issue / Ask a question</h1>\n<p>Use the <a href=\"https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape/-/issues\"\
    \ rel=\"nofollow\">GitLab repository Issues</a>.</p>\n<h1>\n<a id=\"user-content-contact\"\
    \ class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contact</h1>\n<p>Email to vuillaume\
    \ [at] lapp.in2p3.fr / garcia [at] lapp.in2p3.fr.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623346169.0
geoschem/Singularity_GC:
  data_format: 2
  description: Singularity Recipe for GEOS-Chem
  filenames:
  - Singularity
  full_name: geoschem/Singularity_GC
  latest_release: null
  readme: '<h1>

    <a id="user-content-multi_atlas_app" class="anchor" href="#multi_atlas_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Multi_Atlas_app</h1>

    <p>This includes everything required (except for the "full-multi-atlas" directory)
    to build a docker and corresponding singularity container for the Multi Atlas
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/multi_atlas/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://singularity-hub.org/collections/734" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Multi_Atlas_app.git

    cd Multi_Atlas_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have full-multi-atlas directory which contains atlases.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/multi_atlas

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Multi_Atlas_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics:
  - geos-chem
  - singularity-container
  - docker-image
  updated_at: 1564082038.0
geoschem/Singularity_GCHP:
  data_format: 2
  description: Singularity Recipe for High-Performance GEOS-Chem (GCHP)
  filenames:
  - Singularity
  full_name: geoschem/Singularity_GCHP
  latest_release: null
  readme: '<h1>

    <a id="user-content-apsim" class="anchor" href="#apsim" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>APSIM</h1>

    <p>The Agricultural Production Systems sIMulator (APSIM) is internationally recognised
    as a highly advanced simulator of agricultural systems. It contains a suite of
    modules which enable the simulation of systems that cover a range of plant, animal,
    soil, climate and management interactions. APSIM is undergoing continual development,
    with new capability added to regular releases of official versions. Its development
    and maintenance is underpinned by rigorous science and software engineering standards.
    The APSIM Initiative has been established to promote the development and use of
    the science modules and infrastructure software of APSIM.</p>

    <p>CI builds of this repository can be found <a href="https://apsimdev.apsim.info/APSIM.Builds.Portal/Bob.aspx"
    rel="nofollow">Here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1531155094.0
gipert/Singularity.def:
  data_format: 2
  description: My Singularity recipe files
  filenames:
  - asciinema/Singularity.def
  - julia/Singularity.def
  - root-cern/Singularity.def
  - gerda-tgsend/Singularity.def
  - lilypond/Singularity.def
  - arch-base/Singularity.def
  - bat/Singularity.def
  - centos-base/Singularity.def
  - texlive/Singularity.def
  - itunes/Singularity.def
  full_name: gipert/Singularity.def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - singularity
  - containers
  updated_at: 1587858477.0
gnperdue/singularity_imgs:
  data_format: 2
  description: Singularity images for deep learning software
  filenames:
  - Singularity.py2_tf17
  - Singularity.py3_trch
  - Singularity.py3_dmda
  - Singularity.py3_tf2gnt
  - Singularity.py3_tf
  - Singularity.py2_tf110
  - Singularity.py3_tf1gnt
  - Singularity.py3_fast2
  full_name: gnperdue/singularity_imgs
  latest_release: null
  readme: '<p>Singularity containers (with inspiration from J. Simone, <a href="https://github.com/TomaszGolan/mlmpr">T.
    Golan</a>, and <a href="https://github.com/DeepLearnPhysics/larcv2-singularity">K.
    Terao</a>).</p>

    <p><a href="https://singularity-hub.org/collections/998" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ul>

    <li>Pull, e.g. <code>$ singularity pull shub://gnperdue/singularity_imgs:py2_tf17</code>

    </li>

    </ul>

    <h2>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <ul>

    <li>

    <code>Singularity.py2_tf110</code> - See <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu">TF</a>
    for base package definition.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - singularity-hub
  - singularity-container
  updated_at: 1593117348.0
gpuhackathons-org/gpubootcamp:
  data_format: 2
  description: This repository consists for gpu bootcamp material for HPC and AI
  filenames:
  - ai/RAPIDS/Singularity
  - ai/DeepStream_Perf_Lab/Singularity
  - ai/DeepStream/Singularity
  - hpc_ai/ai_science_cfd/Singularity
  - hpc_ai/ai_science_climate/Singularity
  - misc/jupyter_lab_template/appName/Singularity
  - hpc/openacc/Singularity
  - hpc/miniprofiler/Singularity
  - hpc/nways/Singularity
  full_name: gpuhackathons-org/gpubootcamp
  latest_release: null
  readme: '<h1>

    <a id="user-content-gpubootcamp-official-training-materials" class="anchor" href="#gpubootcamp-official-training-materials"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GPUBootcamp
    Official Training Materials</h1>

    <p>GPU Bootcamps are designed to help build confidence in Accelerated Computing
    and eventually prepare developers to enroll for <a href="http://gpuhackathons.org/"
    rel="nofollow">Hackathons</a></p>

    <p>This repository consists of GPU bootcamp material for HPC, AI and convergence
    of both:</p>

    <ul>

    <li>

    <a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc">HPC</a>
    ::

    The bootcamp content focuses on how to follow the Analyze, Parallelize and Optimize
    Cycle to write parallel codes using different parallel programming models accelerating
    HPC simulations.</li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Lab</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc/nways">N-Ways</a></td>

    <td>This lab will cover multiple GPU programming models and choose the one that
    best fits your needs. The material supports different programmin glangauges including
    C ( CUDA C, OpenACC C, OpenMP C, C++ stdpar ),  Fortran ( CUDA Fortran, OpenACC
    Fortran, OpenMP Fortran, ISO DO CONCURRENT</td>

    </tr>

    <tr>

    <td><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc/openacc">OpenACC</a></td>

    <td>The lab will cover how to write portable parallel program that can run on
    multicore CPUs and accelerators like GPUs and how to apply incremental parallelization
    strategies using OpenACC</td>

    </tr>

    </tbody>

    </table>

    <ul>

    <li>

    <a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc_ai">Convergence
    of HPC and AI</a> ::

    The bootcamp content focuses on how AI can accelerate HPC simulations by introducing
    concepts of Deep Neural Networks, including data pre-processing, techniques on
    how to build, compare and improve accuracy of deep learning models.</li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Lab</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc_ai/ai_science_climate">Weather
    Pattern Recognition</a></td>

    <td>This Bootcamp will introduce developers to fundamentals of AI and how data
    driven approach can be applied to Climate/Weather domain</td>

    </tr>

    <tr>

    <td><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc_ai/ai_science_cfd">CFD
    Flow Prediction</a></td>

    <td>This Bootcamp will introduce developers to fundamentals of AI and how they
    can be applied to CFD (Computational Fluid Dynamics)</td>

    </tr>

    </tbody>

    </table>

    <ul>

    <li>

    <a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai">AI</a>::

    The bootcamp content focuses on using popular accelerated AI frameworks and using
    optimization techniques to get max performance from accelerators like GPU.</li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Lab</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai/DeepStream">Accelerated
    Intelligent Video Analytics</a></td>

    <td>Learn how Nvidia DeepStream SDK can be used to create optimized Intelligent
    Video Analytics (IVA) pipeline. Participants will be exposed to the building blocks
    for creating IVA pipeline followed by profiling exercise to identify hotspots
    in the pipeline and methods to optimize and get higher throughput</td>

    </tr>

    <tr>

    <td><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai/RAPIDS">Accelerated
    Data Science</a></td>

    <td>Learn how RAPIDS suite of open source software libraries gives you the freedom
    to execute end-to-end data science and analytics pipelines entirely on GPUs. Participants
    will be exposed to using libraries that can be easily integrated with the daily
    data science pipeline and accelerate computations for faster execution</td>

    </tr>

    </tbody>

    </table>

    <h1>

    <a id="user-content-system-requirements" class="anchor" href="#system-requirements"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>System
    Requirements</h1>

    <p>Each lab contains docker and singularity definition files. Follow the readme
    files inside each on how to build the container and run the labs inside it.</p>

    <h1>

    <a id="user-content-contribution" class="anchor" href="#contribution" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contribution</h1>

    <ul>

    <li>The repository uses Apache 2.0 license. For more details on folder structure
    developers may refer to CONTRIBUTING.md file.</li>

    <li>A project template for reference is located at <a href="https://github.com/bharatk-parallel/gpubootcamp-1/tree/nways_md_fortran/misc/jupyter_lab_template/appName">Template</a>

    </li>

    </ul>

    <h1>

    <a id="user-content-feature-request-or-filing-issues" class="anchor" href="#feature-request-or-filing-issues"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feature
    Request or filing issues</h1>

    <ul>

    <li>Bootcamp users may request for newer training material or file a bug by filing
    a github issues</li>

    <li>Please do go through the existing list of issues to get more details of upcoming
    features and bugs currently being fixed <a href="https://github.com/gpuhackathons-org/gpubootcamp/issues">Issues</a>

    </li>

    </ul>

    <h2>

    <a id="user-content-questions" class="anchor" href="#questions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Questions?</h2>

    <p>Please join <a href="https://openacclang.slack.com/messages/openaccusergroup"
    rel="nofollow">OpenACC Slack Channel</a> for questions.</p>

    '
  stargazers_count: 68
  subscribers_count: 6
  topics: []
  updated_at: 1625053775.0
granek/jupyter-MIC-2021:
  data_format: 2
  description: null
  filenames:
  - Singularity.def
  full_name: granek/jupyter-MIC-2021
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hts-jupyter-notebook-container\" class=\"anchor\"\
    \ href=\"#hts-jupyter-notebook-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>HTS Jupyter notebook container</h1>\n\
    <p>We are offering a series of 6 workshops on biological assays and data analysis\
    \ for HIV researchers.\nThis series is funded by an R25 grand from the National\
    \ Institute of Allergies and Infectious Disease (NIAID).\nOur goal is to provide\
    \ educational enrichment for HIV researchers on current assay technologies and\
    \ the statistical and bioinformatic analysis techniques necessary to process such\
    \ data.</p>\n<p>This is the source for the Docker container used to run the course\
    \ Jupyter\nnotebooks.</p>\n<h1>\n<a id=\"user-content-using-the-image\" class=\"\
    anchor\" href=\"#using-the-image\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Using the image</h1>\n<h2>\n<a id=\"\
    user-content-install-docker\" class=\"anchor\" href=\"#install-docker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Install\
    \ docker</h2>\n<p>To run a container on your local machine or laptop, download\
    \ the docker program from <a href=\"https://www.docker.com\" rel=\"nofollow\"\
    >https://www.docker.com</a>.</p>\n<h2>\n<a id=\"user-content-run-image-on-your-local-computer\"\
    \ class=\"anchor\" href=\"#run-image-on-your-local-computer\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run image\
    \ on your local computer</h2>\n<p>Once you have the docker program installed,\
    \ open the program (you should get a terminal screen with command line). Enter\
    \ the command:</p>\n<pre><code>docker pull dukehtscourse/jupyter-hts-2019\n</code></pre>\n\
    <p>This will pull down the course docker image from dockerhub. It may take a few\
    \ minutes. Next, run the command to start a container:</p>\n<pre><code>docker\
    \ run --name hts-course -v YOUR_DIRECTORY_WITH_COURSE_MATERIAL:/home/jovyan/work\
    \ \\\n-d -p 127.0.0.1\\:9999\\:8888 \\\n-e PASSWORD=\"YOUR_CHOSEN_NOTEBOOK_PASSWORD\"\
    \ \\\n-e NB_UID=1000 \\\n-t dukehtscourse/jupyter-hts-2019\n</code></pre>\n<p>The\
    \ most important parts of this verbiage are the <code>YOUR_DIRECTORY_WITH_COURSE_MATERIALS</code>\
    \ and <code>YOUR_CHOSEN_NOTEBOOK_PASSWORD</code>.</p>\n<ul>\n<li>\n<code>YOUR_DIRECTORY_WITH_COURSE_MATERIALS</code>\
    \ (Bind mounting): The directory name is the one you extracted your course materials\
    \ into. So, if you put them in your home directory, it might look something like:\
    \ <code>-v /home/janice/HTS2019-notebooks:/home/jovyan/work</code>\n</li>\n<li>\n\
    <code>YOUR_CHOSEN_NOTEBOOK_PASSWORD</code>: The password is whatever you want\
    \ to use to password protect your notebook. Now, this command is running the notebook\
    \ so that it is only 'seen' by your local computer - no one else on the internet\
    \ can access it, and you cannot access it remotely, so the password is a bit of\
    \ overkill. Use it anyway. An example might be: <code>-e PASSWORD=\"Pssst_this_is_Secret\"\
    </code> except that this is a terrible password and you should follow standard\
    \ rules of not using words, include a mix of capital and lowercase and special\
    \ symbols. etc.</li>\n<li>\n<code>-d -p 127.0.0.1\\:9999\\:8888</code> part of\
    \ the command is telling docker to run the notebook so that it is only visible\
    \ to the local machine. It is absolutely possible to run it as a server to be\
    \ accessed across the web - but there are some security risks associated, so if\
    \ you want to do this proceed with great caution and get help.</li>\n</ul>\n<p>Of\
    \ course, it would be better either configure HTTPS (see the options section below)\
    \ or run an Nginx proxy in front of the container instance so you get https (encryption)\
    \ instead of http.</p>\n<h3>\n<a id=\"user-content-open-the-jupyter-in-your-browser\"\
    \ class=\"anchor\" href=\"#open-the-jupyter-in-your-browser\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Open the\
    \ Jupyter in your browser</h3>\n<p>Open a browser and point it to <a href=\"http://127.0.0.1:9999\"\
    \ rel=\"nofollow\">http://127.0.0.1:9999</a>\nYou should get to a Jupyter screen\
    \ asking for a password. This is the password you created in the docker run command.\n\
    Now, you should be able to run anything you like from the course. Depending on\
    \ your laptop's resources (RAM, cores), this might be slow, so be aware and start\
    \ by testing only one file (vs the entire course data set).</p>\n<h3>\n<a id=\"\
    user-content-stopping-docker\" class=\"anchor\" href=\"#stopping-docker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Stopping\
    \ Docker</h3>\n<p>The container will continue running, even if you do not have\
    \ Jupyter open in a web browser.  If you don't plan to use it for a while, you\
    \ might want to shut it down so it isn't using resources on your computer.  Here\
    \ are two ways to do that:</p>\n<h4>\n<a id=\"user-content-kitematic\" class=\"\
    anchor\" href=\"#kitematic\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Kitematic</h4>\n<p>Included in the <a href=\"\
    https://docs.docker.com/docker-for-mac/\" rel=\"nofollow\">Docker for Mac</a>\
    \ and the <a href=\"https://docs.docker.com/docker-for-windows/\" rel=\"nofollow\"\
    >Docker for Windows</a> installations.</p>\n<h4>\n<a id=\"user-content-commandline\"\
    \ class=\"anchor\" href=\"#commandline\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Commandline</h4>\n<p>You may\
    \ want to familiarize yourself with the following Docker commands.</p>\n<ul>\n\
    <li><code>docker stop</code></li>\n<li><code>docker rm</code></li>\n<li><code>docker\
    \ ps -a</code></li>\n<li><code>docker images</code></li>\n<li><code>docker rmi</code></li>\n\
    </ul>\n<h3>\n<a id=\"user-content-windows-note\" class=\"anchor\" href=\"#windows-note\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Windows Note</h3>\n<p>These instructions have not been tested in a\
    \ Windows environment.  If you have problems with them, please give us feedback</p>\n\
    <h2>\n<a id=\"user-content-run-image-on-a-server\" class=\"anchor\" href=\"#run-image-on-a-server\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run image on a server</h2>\n<p>To run on a remote server you will\
    \ want to use a slightly different command from above, because you <em>will need\
    \ to connect remotely</em>:</p>\n<pre><code>docker run --name hts-course \\\n\
    -v YOUR_DIRECTORY_WITH_COURSE_MATERIAL:/home/jovyan/work \\\n-d -p 8888:8888 \\\
    \n-e USE_HTTPS=\"yes\" \\\n-e PASSWORD=\"YOUR_CHOSEN_NOTEBOOK_PASSWORD\" \\\n\
    -e NB_UID=1000 \\\n-t dukehtscourse/jupyter-hts-2019\n</code></pre>\n<h2>\n<a\
    \ id=\"user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Options</h2>\n\
    <p>You may customize the execution of the Docker container and the Notebook server\
    \ it contains with the following optional arguments.</p>\n<ul>\n<li>\n<code>-e\
    \ PASSWORD=\"YOURPASS\"</code> - Configures Jupyter Notebook to require the given\
    \ password. Should be conbined with <code>USE_HTTPS</code> on untrusted networks.</li>\n\
    <li>\n<code>-e USE_HTTPS=yes</code> - Configures Jupyter Notebook to accept encrypted\
    \ HTTPS connections. If a <code>pem</code> file containing a SSL certificate and\
    \ key is not provided (see below), the container will generate a self-signed certificate\
    \ for you.</li>\n<li>\n<strong>(v4.0.x)</strong> <code>-e NB_UID=1000</code> -\
    \ Specify the uid of the <code>jovyan</code> user. Useful to mount host volumes\
    \ with specific file ownership.</li>\n<li>\n<code>-e GRANT_SUDO=yes</code> - Gives\
    \ the <code>jovyan</code> user passwordless <code>sudo</code> capability. Useful\
    \ for installing OS packages. <strong>You should only enable <code>sudo</code>\
    \ if you trust the user or if the container is running on an isolated host.</strong>\n\
    </li>\n<li>\n<code>-v /some/host/folder/for/work:/home/jovyan/work</code> - Host\
    \ mounts the default working directory on the host to preserve work even when\
    \ the container is destroyed and recreated (e.g., during an upgrade).</li>\n<li>\n\
    <strong>(v3.2.x)</strong> <code>-v /some/host/folder/for/server.pem:/home/jovyan/.ipython/profile_default/security/notebook.pem</code>\
    \ - Mounts a SSL certificate plus key for <code>USE_HTTPS</code>. Useful if you\
    \ have a real certificate for the domain under which you are running the Notebook\
    \ server.</li>\n<li>\n<strong>(v4.0.x)</strong> <code>-v /some/host/folder/for/server.pem:/home/jovyan/.local/share/jupyter/notebook.pem</code>\
    \ - Mounts a SSL certificate plus key for <code>USE_HTTPS</code>. Useful if you\
    \ have a real certificate for the domain under which you are running the Notebook\
    \ server.</li>\n<li>\n<code>-e INTERFACE=10.10.10.10</code> - Configures Jupyter\
    \ Notebook to listen on the given interface. Defaults to '*', all interfaces,\
    \ which is appropriate when running using default bridged Docker networking. When\
    \ using Docker's <code>--net=host</code>, you may wish to use this option to specify\
    \ a particular network interface.</li>\n<li>\n<code>-e PORT=8888</code> - Configures\
    \ Jupyter Notebook to listen on the given port. Defaults to 8888, which is the\
    \ port exposed within the Dockerfile for the image. When using Docker's <code>--net=host</code>,\
    \ you may wish to use this option to specify a particular port.</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-running-the-course-image-with-singularity\" class=\"\
    anchor\" href=\"#running-the-course-image-with-singularity\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ the Course Image with Singularity</h2>\n<p>Docker requires root permissions\
    \ to run, so you are unlikely to be able to run Docker on a computer that you\
    \ are not fully in control of.  As an alternative you can run the course image\
    \ with <a href=\"https://sylabs.io/singularity/\" rel=\"nofollow\">Singularity</a>,\
    \ another container system. Singularity is similar to Docker, and can run Docker\
    \ images, but you do not need special permissions to run Singularity images <em>or</em>\
    \ Docker images with Singularity (as long as Singularity is actually installed\
    \ on the computer).</p>\n<p>The following command uses Singularity to start up\
    \ a container from the course Jupyter image.</p>\n<pre><code>singularity exec\
    \ docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook\
    \ --ip=0.0.0.0 --no-browser\n</code></pre>\n<h3>\n<a id=\"user-content-running-the-course-image-on-a-slurm-cluster\"\
    \ class=\"anchor\" href=\"#running-the-course-image-on-a-slurm-cluster\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ the Course Image on a SLURM cluster</h3>\n<p>We will use the example of the\
    \ Duke Computer Cluster, but these instructions should be easily adaptable to\
    \ other clusters</p>\n<ol>\n<li>From your computer run this to connect to DCC:</li>\n\
    </ol>\n<pre><code>ssh NetID@dcc-login-03.oit.duke.edu\n</code></pre>\n<ol start=\"\
    2\">\n<li>Once you are connected run this to start a tmux session:</li>\n</ol>\n\
    <pre><code>tmux new -s jupyter\n</code></pre>\n<ol start=\"3\">\n<li>Once you\
    \ have started a tmux session you can start up Jupyter with this command:</li>\n\
    </ol>\n<pre><code>srun singularity exec docker://dukehtscourse/jupyter-hts-2019\
    \ /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n</code></pre>\n\
    <blockquote>\n<p>Note: the first time you run this, it might take a VERY long\
    \ time to download the Docker image and build the Singularity image from it</p>\n\
    </blockquote>\n<p>Running this command will print a bunch of stuff. You can ignore\
    \ everything except the last two lines, which will say something like:</p>\n<pre><code>http://dcc-chsi-01:8889/?token=08172007896ad29bb5fbd92f6f3f516a8b2f7303ed7f1df3\n\
    or http://127.0.0.1:8889/?token=08172007896ad29bb5fbd92f6f3f516a8b2f7303ed7f1df3\n\
    </code></pre>\n<p>You need this information for the next few steps. For the next\
    \ step you need the \u201Cdcc-chsi-01:8889\u201D part.\n\u201Cdcc-chsi-01\u201D\
    \ is the compute node that Jupyter is running on and \u201C8889\u201D is the port\
    \ it is listening on. You may get a different value every time you start the container.</p>\n\
    <ol start=\"4\">\n<li>You want to run the following command in another terminal\
    \ on your computer to set up port forwarding.</li>\n</ol>\n<pre><code>ssh -L PORT:NODE.rc.duke.edu:PORT\
    \ NetID@dcc-login-03.oit.duke.edu\n</code></pre>\n<p>In this command you want\
    \ to replace \u201CPORT\u201D with the value you got for port from the srun command\
    \ and replace \u201CNODE\u201D with the compute node that was printed by the srun\
    \ command. So for the example above, the ssh port forwarding command would be:</p>\n\
    <pre><code>ssh -L 8889:dcc-chsi-01.rc.duke.edu:8889 NetID@dcc-login-03.oit.duke.edu\n\
    </code></pre>\n<ol start=\"5\">\n<li>Now you can put the last line that the srun\
    \ command printed in your web browser and it should open your Jupyter instance\
    \ running on DCC.</li>\n</ol>\n<h4>\n<a id=\"user-content-notes\" class=\"anchor\"\
    \ href=\"#notes\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Notes</h4>\n<ol>\n<li>\n<p>The Jupyter session keeps\
    \ running until you explicitly shut it down.  If the port forwarding SSH connection\
    \ drops you will need to restart SSH with the same command, but you don\u2019\
    t need to restart Jupyter.</p>\n</li>\n<li>\n<p>There are two ways to explicitly\
    \ shut down Jupyter:</p>\n<ol>\n<li>Within Jupyter, click on the <em>Jupyter</em>\
    \ logo in the top left to go to the main Jupyter page, then click \"Quit\" in\
    \ the top right</li>\n<li>Do control-C twice in the terminal where you started\
    \ Jupyter. If this connection dropped, you can reconnect to it with:</li>\n</ol>\n\
    <pre><code>ssh NetID@dcc-login-03.oit.duke.edu\ntmux a -t jupyter\n</code></pre>\n\
    <p>After shutting down the Jupyter session you can type <code>exit</code> at the\
    \ terminal to close the tmux session.</p>\n</li>\n<li>\n<p>If you need more memory\
    \ or more cpus you can use the <code>--mem</code> and/or <code>--cpus-per-task</code>\
    \ arguments to in the \u201Csrun\u201D, for example to request 4 CPUs and 10GB\
    \ of RAM:</p>\n</li>\n</ol>\n<pre><code>srun --cpus-per-task=4 --mem=10G singularity\
    \ exec docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter\
    \ notebook --ip=0.0.0.0 --no-browser\n</code></pre>\n<ol start=\"4\">\n<li>If\
    \ you have high priority access to a partition you can request that partition\
    \ be used with the <code>-A</code> and <code>-p</code> arguments to <code>srun</code>:</li>\n\
    </ol>\n<pre><code>srun -A chsi -p chsi singularity exec docker://dukehtscourse/jupyter-hts-2019\
    \ /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n</code></pre>\n\
    <ol start=\"5\">\n<li>You might want to access files that are outside of your\
    \ home directory. Within a singularity container your access to the host computer\
    \ is\nlimited: by default, from inside the container you can only access your\
    \ home directory. If you want to access directories that are outside your home\n\
    directory, you have to tell <em>Singularity</em> when you start the container\
    \ with the <code>--bind</code> command line argument. For example:</li>\n</ol>\n\
    <pre><code>srun singularity --bind /work/josh:/work/josh exec docker://dukehtscourse/jupyter-hts-2019\
    \ /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n</code></pre>\n\
    <ol start=\"6\">\n<li>You can combine several of these command line flags:</li>\n\
    </ol>\n<pre><code>srun -A chsi -p chsi --cpus-per-task=4 --mem=10G singularity\
    \  exec --bind /work/josh:/work/josh docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh\
    \ jupyter notebook --ip=0.0.0.0 --no-browser\n</code></pre>\n<ol start=\"6\">\n\
    <li>It is strongly recommended to set the <code>SINGULARITY_CACHEDIR</code> environment\
    \ variables in your .bashrc or when running <code>srun</code>. This environment\
    \ variable specifies where the Docker image (and the Singularity image built from\
    \ it) are saved. If this variable is not specified, singularity will cache images\
    \ in <code>$HOME/.singularity/cache</code>, which can fill up quickly. This is\
    \ discussed in the <a href=\"https://sylabs.io/guides/3.7/user-guide/build_env.html#cache-folders\"\
    \ rel=\"nofollow\">Singularity Documentation</a>\n</li>\n</ol>\n<pre><code>export\
    \ SINGULARITY_CACHEDIR=\"/work/josh/singularity_cache\"; srun -A chsi -p chsi\
    \ --cpus-per-task=4 --mem=10G singularity  exec --bind /work/josh:/work/josh docker://dukehtscourse/jupyter-hts-2019\
    \ /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n</code></pre>\n\
    <h3>\n<a id=\"user-content-install-singularity\" class=\"anchor\" href=\"#install-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Install Singularity</h3>\n<p>Here are instructions for installing:</p>\n\
    <ul>\n<li><a href=\"https://sylabs.io/guides/2.6/user-guide/quick_start.html#quick-installation-steps\"\
    \ rel=\"nofollow\">Singularity version 2.6</a></li>\n<li><a href=\"https://sylabs.io/guides/3.2/user-guide/quick_start.html#quick-installation-steps\"\
    \ rel=\"nofollow\">Singularity version 3.2</a></li>\n<li><a href=\"https://sylabs.io/singularity-desktop-macos/\"\
    \ rel=\"nofollow\">Singularity Desktop for macOS (Alpha Preview)</a></li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623704048.0
granek/singularity-rstudio-base:
  data_format: 2
  description: 'Singularity image to serve as base for all project images.  Defaults
    to starting up RStudio with an auto-selected port and password '
  filenames:
  - Singularity.3.6.0
  - Singularity.4.0.2
  - Singularity.3.6.1
  - Singularity.4.0.3
  - Singularity.mro.4.0.3
  full_name: granek/singularity-rstudio-base
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3197" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>This Singularity image is intended to serve as base for all project images.</p>

    <p>By default it starts up RStudio with an auto-selected port and password</p>

    <h1>

    <a id="user-content-running-singularity-image" class="anchor" href="#running-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    Singularity Image</h1>

    <p>Run a singularity-rstudio-base container with <code>singularity run shub://granek/singularity-rstudio-base</code></p>

    <h2>

    <a id="user-content-tmp-issues" class="anchor" href="#tmp-issues" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>/tmp issues</h2>

    <p>It is recommended to do one of the following when running this image. There
    is no need to do both:</p>

    <ol>

    <li>Set "mount tmp = no" in <code>/etc/singularity/singularity.conf</code>.</li>

    <li>If #1 is not an option, the following command can be used to bind mount <code>/tmp</code>
    in the container to a "private" tmp directory:</li>

    </ol>

    <pre><code>SINGTMP="/tmp/${USER}_$$_tmp"; mkdir -p $SINGTMP; singularity run --bind
    $SINGTMP:/tmp shub://granek/singularity-rstudio-base

    </code></pre>

    <h3>

    <a id="user-content-tmp-issues-tldr" class="anchor" href="#tmp-issues-tldr" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>/tmp issues TLDR</h3>

    <p>If a second user tries on the same server tries to run an RStudio container
    they will have permission issues with <code>/tmp/rstudio-server</code>, which
    will be owned by the user who first ran an RStudio container.</p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1624649294.0
grst/containerize-conda:
  data_format: 2
  description: Turn an existing conda environment into a Docker or Singularity container
  filenames:
  - Singularity
  full_name: grst/containerize-conda
  latest_release: null
  readme: '<h1>

    <a id="user-content-containerize-an-existing-conda-environment" class="anchor"
    href="#containerize-an-existing-conda-environment" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Containerize an existing conda environment</h1>

    <p>I use conda environments for working on data analysis projects.

    Sometimes I need to revert to install using <code>pip</code> or <code>R</code>''s

    <code>install.packages</code> if a package is not on bioconda or conda-forge.</p>

    <p>This makes it very hard to reproduce the environment, and therefore,

    the analysis, on another system. Even pure conda environments stored

    as an <code>environment.yml</code> file tend to <a href="https://github.com/conda/conda/issues/9257">break
    after a

    while</a>.</p>

    <p>Using the instructions below allows to package an existing environment

    into a Docker or Singularity container which should be more portable

    and can also easily be integrated into a <a href="https://grst.github.io/bioinformatics/2019/12/23/reportsrender.html"
    rel="nofollow">fully reproducible

    data analysis

    workflow</a>

    based on e.g. <a href="https://www.nextflow.io/" rel="nofollow">Nextflow</a>.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <ul>

    <li><a href="https://conda.github.io/conda-pack/" rel="nofollow">conda-pack</a></li>

    <li>either Docker, Podman or Singularity</li>

    <li>source conda environment needs to be on a linux x64 machine.</li>

    </ul>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ol>

    <li>Clone this repository (retrieve <code>Dockerfile</code>/<code>Singularity</code>)</li>

    </ol>

    <pre><code>git clone git@github.com:grst/containerize-conda.git

    cd containerize-conda

    </code></pre>

    <ol start="2">

    <li>Pack the environment</li>

    </ol>

    <pre><code>conda-pack -n &lt;MY_ENV&gt; -o packed_environment.tar.gz

    </code></pre>

    <ol start="3">

    <li>Build the container</li>

    </ol>

    <pre><code># With singularity

    singularity build --fakeroot &lt;OUTPUT_CONTAINER.sif&gt; Singularity


    # With Docker

    docker build . -t &lt;TAG&gt;


    # With Podman/Buildah

    podman build . -t &lt;TAG&gt;

    </code></pre>

    <h2>

    <a id="user-content-how-it-works" class="anchor" href="#how-it-works" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How it works</h2>

    <p>Conda environment can''t be just "moved" to another location, as some paths
    are

    hardcoded into the environment. <code>conda-pack</code> takes care of replacing
    these paths

    back to placeholders and creates a <code>.tar.gz</code> archive that contains
    the

    environment. This environment can be unpacked to another machine (or, in our

    case, a container). Running <code>conda-unpack</code> in the environment replaces
    the

    placeholders back to the actual paths matching the new location.</p>

    '
  stargazers_count: 11
  subscribers_count: 2
  topics: []
  updated_at: 1624680632.0
grst/rstudio-server-conda:
  data_format: 2
  description: Run Rstudio Server in a conda environment
  filenames:
  - singularity/Singularity
  full_name: grst/rstudio-server-conda
  latest_release: v0.3.0
  readme: "<h1>\n<a id=\"user-content-running-rstudio-server-in-a-conda-environment\"\
    \ class=\"anchor\" href=\"#running-rstudio-server-in-a-conda-environment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Rstudio Server in a Conda Environment</h1>\n<p>I usually rely on the <a href=\"\
    https://docs.conda.io/en/latest/\" rel=\"nofollow\">conda package manager</a>\
    \ to manage my environments during development. Thanks to <a href=\"https://conda-forge.org/\"\
    \ rel=\"nofollow\">conda-forge</a> and <a href=\"https://bioconda.github.io/\"\
    \ rel=\"nofollow\">bioconda</a> most R packages are now also available through\
    \ conda. For production,\nI <a href=\"https://github.com/grst/containerize-conda\"\
    >convert them to containers</a> as these are easier to share.</p>\n<p>Unfortunately,\
    \ there seems to be <a href=\"https://community.rstudio.com/t/start-rstudio-server-session-in-conda-environment/12516/15\"\
    \ rel=\"nofollow\">no straightforward way</a> to use conda envs in Rstudio server.\
    \ This repository provides three approaches to make rstudio server work with conda\
    \ envs.</p>\n<ul>\n<li><a href=\"#running-rstudio-server-with-singularity\">Running\
    \ Rstudio Server in a Singularity Container</a></li>\n<li><a href=\"#running-rstudio-server-with-podmandocker\"\
    >Running Rstudio Server in a Docker/Podman Container</a></li>\n<li><a href=\"\
    #running-locally\">Running Rstudio Server locally</a></li>\n</ul>\n<h2>\n<a id=\"\
    user-content-running-rstudio-server-with-singularity\" class=\"anchor\" href=\"\
    #running-rstudio-server-with-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Rstudio Server with Singularity</h2>\n\
    <p>With this approach Rstudio Server runs in a Singularity container (based on\
    \ <a href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\">rocker/rstudio</a>).<br>\n\
    The conda environment gets mounted into the container - like that there's no need\
    \ to rebuild the container to add a package and\n<code>install.packages</code>\
    \ can be used without issues. The container-based approach has the following benefits:</p>\n\
    <ul>\n<li>Authentication works (<a href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\
    >#3</a>)</li>\n<li>Several separate instances of Rstudio server can run in parallel,\
    \ even without the <em>Pro</em> version.</li>\n</ul>\n<h3>\n<a id=\"user-content-prerequisites\"\
    \ class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h3>\n<ul>\n<li><a\
    \ href=\"https://sylabs.io/guides/3.0/user-guide/quick_start.html\" rel=\"nofollow\"\
    >Singularity</a></li>\n<li>\n<a href=\"https://docs.conda.io/en/latest/miniconda.html\"\
    \ rel=\"nofollow\">conda</a> or <a href=\"https://github.com/conda-forge/miniforge#mambaforge\"\
    >mamba</a>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repository</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:grst/rstudio-server-conda.git\n\
    <span class=\"pl-c1\">cd</span> rstudio-server-conda/singularity</pre></div>\n\
    </li>\n<li>\n<p>Activate the target conda env or set the environment variable\
    \ <code>CONDA_PREFIX</code>\nto point to the location of the conda env.</p>\n\
    </li>\n<li>\n<p>Check the <code>run_singularity.sh</code> script. In particular,\
    \ you may need to add additional bind mounts\n(e.g. a global data directory).</p>\n\
    </li>\n<li>\n<p>Execute the <code>run_singularity.sh</code> script. It will automatically\
    \ build the container if it is not available.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>PORT=8787 PASSWORD=notsafe ./run_singularity.sh</pre></div>\n</li>\n<li>\n\
    <p>Log into Rstudio</p>\n<ul>\n<li>open rstudio server at <code>http://localhost:8787</code>\
    \ (or whatever port you specified)</li>\n<li>login with your default username\
    \ and the password you specified via the <code>PASSWORD</code> environment variable.</li>\n\
    </ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-running-rstudio-server-with-podmandocker\"\
    \ class=\"anchor\" href=\"#running-rstudio-server-with-podmandocker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Rstudio Server with Podman/Docker</h2>\n<p>This approach is similar to <a href=\"\
    #running-rstudio-server-with-singularity\">Singularity</a>, but uses\nDocker or\
    \ Podman and a <code>docker-compose.yml</code> file instead.</p>\n<h3>\n<a id=\"\
    user-content-known-limitations\" class=\"anchor\" href=\"#known-limitations\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Known limitations</h3>\n<ul>\n<li>No access to shared group directories\
    \ (<a href=\"https://github.com/grst/rstudio-server-conda/issues/14\">#14</a>)</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h3>\n<ul>\n<li>\n<a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> or <a href=\"https://podman.io/\" rel=\"nofollow\"\
    >Podman</a>\n</li>\n<li>\n<a href=\"https://github.com/docker/compose\">docker-compose</a>\
    \ or <a href=\"https://github.com/containers/podman-compose\">podman-compose</a>\n\
    </li>\n<li>\n<a href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"\
    nofollow\">conda</a> or <a href=\"https://github.com/conda-forge/miniforge#mambaforge\"\
    >mamba</a>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-usage-1\" class=\"anchor\"\
    \ href=\"#usage-1\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repository</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:grst/rstudio-server-conda.git</pre></div>\n\
    </li>\n<li>\n<p>Build the rstudio container (fetches the latest version of <a\
    \ href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\">rocker/rstudio</a>\
    \ and adds some custom scripts)</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> rstudio-server-conda/docker\ndocker-compose\
    \ build     <span class=\"pl-c\"><span class=\"pl-c\">#</span> or podman-compose</span></pre></div>\n\
    </li>\n<li>\n<p>Copy the docker-compose.yml file into your project directory and\
    \ adjust the paths.</p>\n<p>You may want to add additional volumes with your data.</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-s\">[...]</span>\n\
    \   <span class=\"pl-ent\">ports</span>:\n      <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> port on the host : port in the container (the latter is always\
    \ 8787)</span>\n      - <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>8889:8787<span\
    \ class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-ent\">volumes</span>:\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> mount conda env into\
    \ exactely the same path as on the host system - some paths are hardcoded in the\
    \ env.</span>\n      - <span class=\"pl-s\">/home/sturm/anaconda3/envs/R400:/home/sturm/anaconda3/envs/R400</span>\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> Share settings between\
    \ rstudio instances</span>\n      - <span class=\"pl-s\">/home/sturm/.local/share/rstudio/monitored/user-settings:/root/.local/share/rstudio/monitored/user-settings</span>\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> mount the working directory\
    \ containing your R project.</span>\n      - <span class=\"pl-s\">/home/sturm/projects:/projects</span>\n\
    \    <span class=\"pl-ent\">environment</span>:\n      <span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> password used for authentication</span>\n      - <span\
    \ class=\"pl-s\">PASSWORD=notsafe</span>\n      <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> repeat the path of the conda environment (must be identical to\
    \ the path in \"volumes\")</span>\n      - <span class=\"pl-s\">CONDAENV=/home/sturm/anaconda3/envs/R400</span></pre></div>\n\
    </li>\n<li>\n<p>Run your project-specific instance of Rstudio-server</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>docker-compose up </pre></div>\n\
    </li>\n<li>\n<p>Log into Rstudio</p>\n</li>\n</ol>\n<ul>\n<li>Open your server\
    \ at <code>http://localhost:8889</code> (or whatever port you specified)</li>\n\
    <li>Login with the user <code>rstudio</code> (when using Docker) or <code>root</code>\
    \ (when using Podman) and the password you specified\nin the <code>docker-compose.yml</code>.\
    \ If you are using Podman and login with <code>rstudio</code> you won't have permissions\
    \ to\naccess the mounted volumes.</li>\n</ul>\n<h2>\n<a id=\"user-content-running-locally\"\
    \ class=\"anchor\" href=\"#running-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Locally</h2>\n<p>With\
    \ this approach a locally installed Rstudio server is ran such that it uses the\
    \ conda env.</p>\n<h3>\n<a id=\"user-content-known-limitations-1\" class=\"anchor\"\
    \ href=\"#known-limitations-1\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Known limitations</h3>\n<ul>\n<li>no\
    \ authentication (<a href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\
    >#3</a>). Use this approach only in a secure network!</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-prerequisites-2\" class=\"anchor\" href=\"#prerequisites-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h3>\n\
    <ul>\n<li>\n<a href=\"https://www.rstudio.com/products/rstudio/download-server/\"\
    \ rel=\"nofollow\">rstudio server</a> installed locally</li>\n<li>\n<a href=\"\
    https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\">conda</a> or\
    \ <a href=\"https://github.com/conda-forge/miniforge#mambaforge\">mamba</a>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-usage-2\" class=\"anchor\" href=\"#usage-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repo</p>\n<pre><code>git clone\
    \ https://github.com/grst/rstudio-server-conda.git\n</code></pre>\n</li>\n<li>\n\
    <p>Run rstudio server in the conda env</p>\n<pre><code>cd rstudio-server-conda/local\n\
    conda activate my_project\n./start_rstudio_server.sh 8787  # use any free port\
    \ number here. \n</code></pre>\n</li>\n<li>\n<p>Connect to Rstudio</p>\n<p>You\
    \ should now be able to connect to rstudio server on the port you specify.\n<strong>If\
    \ an R Session has previously been running, you'll need to rstart the Rsession\
    \ now</strong>.</p>\n<p>Obviously, if your env does not have a version of <code>R</code>\
    \ installed, this will either not\nwork at all, or fall back to the system-wide\
    \ R installation.</p>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-how-it-works\"\
    \ class=\"anchor\" href=\"#how-it-works\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How it works</h3>\n<ul>\n<li>\n\
    <p>Rstudio server, can be started in non-daemonized mode by each user individually\
    \ on a custom port (similar to a jupyter notebook). This instance can then run\
    \ in a conda environment:</p>\n<pre><code>&gt; conda activate my_project\n&gt;\
    \ /usr/lib/rstudio-server/bin/rserver \\\n   --server-daemonize=0 \\\n   --www-port\
    \ 8787 \\\n   --rsession-which-r=$(which R) \\\n   --rsession-ld-library-path=$CONDA_PREFIX/lib\n\
    </code></pre>\n</li>\n<li>\n<p>To avoid additional problems with library paths,\
    \ also <code>rsession</code> needs to run within the conda environment. This is\
    \ achieved by wrapping <code>rsession</code> into the <a href=\"https://github.com/grst/rstudio-server-conda/blob/master/local/rsession.sh\"\
    >rsession.sh</a> script. The path to the wrapped <code>rsession</code> executable\
    \ can be passed to <code>rserver</code> as command line argument.</p>\n<pre><code>rserver\
    \ # ...\n    --rsession-path=rsession.sh\n</code></pre>\n</li>\n<li>\n<p>When\
    \ using multiple users a unique <code>secret-cookie-key</code> has to be generated\
    \ for each user. The path to the secret cookie key can be passed to <code>rserver</code>\
    \ as a command line parameter.</p>\n<pre><code>uuid &gt; /tmp/rstudio-server/${USER}_secure-cookie-key\n\
    rserver # ...\n  --secure-cookie-key-file /tmp/rstudio-server/${USER}_secure-cookie-key\n\
    </code></pre>\n</li>\n</ul>\n"
  stargazers_count: 76
  subscribers_count: 3
  topics: []
  updated_at: 1624696322.0
gwastro/pycbc_bench:
  data_format: 2
  description: Some benchmark singularity images for pycbc / pycbc inference
  filenames:
  - Singularity
  full_name: gwastro/pycbc_bench
  latest_release: null
  readme: '<h1>

    <a id="user-content-pycbc_bench" class="anchor" href="#pycbc_bench" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pycbc_bench</h1>

    <p>Some benchmark singularity images for pycbc / pycbc inference</p>

    <h1>

    <a id="user-content-build-singularity-image" class="anchor" href="#build-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>build
    singularity image</h1>

    <p>sudo singularity build pycbcb.img Singularity</p>

    <h1>

    <a id="user-content-run-pycbc-inspiral" class="anchor" href="#run-pycbc-inspiral"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>run
    pycbc inspiral</h1>

    <p>singularity run --cleanenv --app inspiral pycbcb.img</p>

    '
  stargazers_count: 1
  subscribers_count: 8
  topics: []
  updated_at: 1559569839.0
hariszaf/pema:
  data_format: 2
  description: 'PEMA: a flexible Pipeline for Environmental DNA Metabarcoding Analysis
    of the 16S/18S rRNA, ITS and COI marker genes'
  filenames:
  - Singularity
  - singularity/Singularity.latest
  - singularity/Singularity.v.2.0.2
  - singularity/Singularity.v.1.3
  - singularity/Singularity.v.2.0.3
  - singularity/Singularity.v.2.1.0
  - singularity/Singularity.v.1.1
  - singularity/Singularity.v.1.3.2
  - singularity/Singularity.v.2.1.3
  - singularity/Singularity.v.1.3.1
  full_name: hariszaf/pema
  latest_release: v.2.1.3
  readme: "<p align=\"center\">\n  <a href=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\"\
    \ width=\"300\" align=\"left\" data-canonical-src=\"https://i.paste.pics/870189fadf668a958c8aac83f38e799c.png\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h1>\n<a id=\"user-content-pema\" class=\"\
    anchor\" href=\"#pema\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>PEMA:</h1>\n<h2>\n<a id=\"user-content-a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\"\
    \ class=\"anchor\" href=\"#a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>a flexible Pipeline for Environmental DNA Metabarcoding Analysis of\
    \ the 16S/18S rRNA, ITS and COI marker genes</h2>\n<p><em>PEMA is reposited in</em>\
    \ <a href=\"https://hub.docker.com/r/hariszaf/pema\" rel=\"nofollow\"><em>Docker\
    \ Hub</em></a> <em>as well as in</em> <a href=\"https://singularity-hub.org/collections/2295\"\
    \ rel=\"nofollow\"><em>Singularity Hub</em></a></p>\n<h4>\n<a id=\"user-content-a-pema-tutorial-can-be-found-here\"\
    \ class=\"anchor\" href=\"#a-pema-tutorial-can-be-found-here\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>A\
    \ PEMA tutorial can be found <a href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\"\
    \ rel=\"nofollow\"><strong>here</strong></a>.</h4>\n<h4>\n<a id=\"user-content-for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\"\
    \ class=\"anchor\" href=\"#for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>For any troubles you may have when running PEMA or for any potential\
    \ improvevments you would like to suggest, please share on the <a href=\"https://gitter.im/pema-helpdesk/community\"\
    \ rel=\"nofollow\">PEMA Gitter community</a>.</h4>\n\n<p><a href=\"https://gitter.im/pema-helpdesk/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7385c04b449351f12fb57a4bd6f9791ebd68a483493399e50a8f096fadde4246/68747470733a2f2f6261646765732e6769747465722e696d2f70656d612d68656c706465736b2f636f6d6d756e6974792e737667\"\
    \ alt=\"Gitter\" data-canonical-src=\"https://badges.gitter.im/pema-helpdesk/community.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.gnu.org/licenses/gpl-3.0\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/400c4e52df43f6a0ab8a89b74b1a78d1a64da56a7848b9110c9d2991bb7c3105/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-table-of-contents\"\
    \ class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Table of Contents</h1>\n<ul>\n\
    <li><a href=\"#pema-biodiversity-in-all-its-different-levels\">PEMA: biodiversity\
    \ in all its different levels</a></li>\n<li><a href=\"#a-container-based-tool\"\
    > A container-based tool</a></li>\n<li>\n<a href=\"#how-to-run-pema\">How to run\
    \ PEMA</a>\n<ul>\n<li><a href=\"#parameters-file\">Parameters' file</a></li>\n\
    </ul>\n</li>\n<li>\n<a href=\"#pema-on-hpc\">PEMA on HPC</a>\n<ul>\n<li><a href=\"\
    #prerequisites-1\">Prerequisites</a></li>\n<li><a href=\"#installing-1\">Installing</a></li>\n\
    <li>\n<a href=\"#running-pema-1\">Running PEMA</a>\n<ul>\n<li><a href=\"#example\"\
    >Example</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<a href=\"#pema-on-a-simple-pc\"\
    >PEMA on a simple PC</a>\n<ul>\n<li><a href=\"#prerequisites\">Prerequisites</a></li>\n\
    <li><a href=\"#installing\">Installing</a></li>\n<li>\n<a href=\"#running-pema\"\
    >Running PEMA</a>\n<ul>\n<li><a href=\"#step-1---build-a-docker-container\">Step\
    \ 1 - Build a Docker container</a></li>\n<li><a href=\"#step-2---run-pema\">Step\
    \ 2 - Run PEMA</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#the-phyloseq-r-package\"\
    >phyloseq - for a downstream ecological analysis</a></li>\n<li><a href=\"#acknowledgments\"\
    >Acknowledgments</a></li>\n<li><a href=\"#license\">License</a></li>\n<li><a href=\"\
    #citation\">Citation</a></li>\n</ul>\n<div class=\"highlight highlight-source-diff\"\
    ><pre><span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> convertion of the\
    \ Illumina raw data is now implemented in the framework of PEMA</span>\n<span\
    \ class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> PEMA now supports 2 extra marker\
    \ genes, 18S rRNA and ITS. </span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\"\
    >+</span> PEMA is now available for macOS!</span>\n<span class=\"pl-mi1\"><span\
    \ class=\"pl-mi1\">+</span> for anything feel free to contact me at: haris-zaf@hcmr.gr</span></pre></div>\n\
    \n<h1>\n<a id=\"user-content-pema-biodiversity-in-all-its-different-levels\" class=\"\
    anchor\" href=\"#pema-biodiversity-in-all-its-different-levels\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>PEMA:\
    \ biodiversity in all its different levels</h1>\n<p>PEMA supports the metabarcoding\
    \ analysis of four marker genes, <strong>16S rRNA</strong> (Bacteria), <strong>ITS</strong>\
    \ (Fungi) as well as <strong>COI</strong> and <strong>18S rRNA</strong> (metazoa).\
    \ As input, PEMA accepts .fastq.gz files as returned by Illumina sequencing platforms.</p>\n\
    <p>PEMA processes the reads from each sample and <strong>returns an OTU- or an\
    \ ASV-table with the taxonomies</strong> of the taxa found and their abundances\
    \ in each sample. It also returns statistics and a FASTQC diagram about the quality\
    \ of the reads for each sample. Finally, PEMA supports <strong>downstream ecological\
    \ analysis</strong> of the profiles retrieved, facilitated by the <a href=\"http://joey711.github.io/phyloseq/index.html\"\
    \ rel=\"nofollow\">phyloseq</a> R package.</p>\n<p>PEMA supports both OTU clustering\
    \ (thanks to VSEARCH and CROP algorithms) and ASV inference (via SWARM) for all\
    \ four marker genes.</p>\n<p>For the case of the 16S rRNA marker gene, PEMA includes\
    \ two separate approaches for taxonomy assignment: alignment-based and phylogenetic-based.\
    \ For the latter, a reference tree of 1000 taxa was created using SILVA_132_SSURef,\
    \ EPA-ng and RaxML-ng.</p>\n<p>PEMA has been implemented in <a href=\"https://pcingola.github.io/BigDataScript/\"\
    \ rel=\"nofollow\">BigDataScript</a> programming language. BDS\u2019s ad hoc task\
    \ parallelism and task synchronization, supports heavyweight computation. Thus,\
    \ PEMA inherits such features and it also supports roll-back checkpoints and on-demand\
    \ partial pipeline execution. In addition, PEMA takes advantage of all the computational\
    \ power available on a specific machine; for example, if PEMA is executed on a\
    \ personal laptop with 4 cores, it is going to use all four of them.</p>\n<p>Finally,\
    \ container-based technologies such as Docker and Singularity, make PEMA easy\
    \ accessible for all operating systems.\nAs you can see in the <a href=\"https://github.com/hariszaf/pema/blob/master/help_files/GitHub%20tutorial.pdf\"\
    >PEMA_tutorial.pdf</a>, once you have either Docker or Singularity on your computational\
    \ environment (see below which suits your case better), running PEMA is cakewalk.\
    \ You can also find the <a href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?usp=sharing\"\
    \ rel=\"nofollow\"><strong>PEMA tutorial</strong></a> as a Google Slides file.</p>\n\
    \n<h1>\n<a id=\"user-content-a-container-based-tool\" class=\"anchor\" href=\"\
    #a-container-based-tool\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>A container-based tool</h1>\n<p>PEMA can run\
    \ either on a HPC environment (server, cluster etc) or on a simple PC. However,\
    \ we definitely suggest to run it on an HPC environment to exploit the full potential\
    \ of PEMA. Running on a powerful server or a cluster can be time-saving since\
    \ it would require significantly less computational time than in a common PC.\
    \ However, for analyses with a small number of samples, a common PC can suffice.</p>\n\
    <p>There is one <strong>major difference</strong> between running PEMA on a common\
    \ PC than running it on a HPC environment. In the first case, PEMA runs through\
    \ <a href=\"https://www.docker.com/\" rel=\"nofollow\"><strong>Docker</strong></a>,\
    \ while in the latter one, it runs through <a href=\"https://sylabs.io/singularity/\"\
    \ rel=\"nofollow\"><strong>Singularity</strong></a>.</p>\n<p>On the following\
    \ chapters, you can find how to install PEMA both in Docker and Singlularity including\
    \ examples.</p>\n<p>Running PEMA is exactly <strong>the same</strong> procedure\
    \ in both of those cases.</p>\n\n<h2>\n<a id=\"user-content-how-to-run-pema\"\
    \ class=\"anchor\" href=\"#how-to-run-pema\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How to run PEMA</h2>\n<p>Assuming\
    \ you have either Docker or Singularity on your system (see below how to get them).\n\
    You need to create a directory where you will have everything PEMA needs - we\
    \ will call it <em><strong>analysis directory</strong></em>.</p>\n<p>In this directory,\
    \ you need to add the following <strong>mandatory</strong> files:</p>\n<ul>\n\
    <li>the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\
    ><em><strong>parameters.tsv</strong></em></a> file (you can download it from this\
    \ repository and then <strong>complete it</strong> according to the needs of your\
    \ analysis)</li>\n<li>a subdirectory called <em><strong>mydata</strong></em> where\
    \ your .fastq.gz files will be located <br>\n</li>\n</ul>\n<p>If your need to\
    \ perform phyloseq, in the analysis directory you also need to add the following\
    \ <strong>optionally</strong> files:</p>\n<ul>\n<li>the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\
    ><em><strong>phyloseq_in_PEMA.R</strong></em></a> which you can also download\
    \ from this repository and set it the way you want (that is an R script which\
    \ we have implemented and has some main features that need to stay always the\
    \ same in order to be executed as part of PEMA and some parts where the user can\
    \ set what exactly needs to get from the phyloseq package)</li>\n<li>the <a href=\"\
    https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\"\
    \ rel=\"nofollow\"><em><strong>metadata.csv</strong></em></a> file which has to\
    \ be in a <strong>comma separated</strong> format (you can find an example of\
    \ this file on PEMA's GitHub repository).</li>\n</ul>\n<h3>\n<a id=\"user-content-attention--\"\
    \ class=\"anchor\" href=\"#attention--\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><strong>Attention!</strong> \
    \ <br>\n</h3>\n<p>PEMA will <strong>fail</strong> unless you name the aforementioned\
    \ files and directories <strong>exactly</strong> as described above.\n<br></p>\n\
    <p>Here is an example of how your <em>analysis directory</em> should be in case\
    \ you do want a phyloseq analysis:</p>\n<pre><code>user@home-PC:~/Desktop/analysis_directory$\
    \ ls\nmydata  parameters.tsv  phyloseq_in_PEMA.R  metadata.csv\n</code></pre>\n\
    <p>and in case you do not:</p>\n<pre><code>user@home-PC:~/Desktop/analysis_directory$\
    \ ls\nmydata  parameters.tsv \n</code></pre>\n<p><a href=\"https://github.com/hariszaf/pema/tree/master/analysis_directory\"\
    ><strong>Here</strong></a> you can find an example of an <em>analysis directory</em>.</p>\n\
    <p>After you have prepared this <em>analysis directory</em> you are ready to run\
    \ PEMA (see below).</p>\n<p><strong>An extended list with PEMA's ouput can be\
    \ found <a href=\"https://github.com/hariszaf/pema/blob/master/help_files/PEMA's%20output%20files.md\"\
    ><strong>here</strong></a>.</strong></p>\n\n<h1>\n<a id=\"user-content-parameters-file\"\
    \ class=\"anchor\" href=\"#parameters-file\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Parameters' file</h1>\n<p>The\
    \ most crucial component in running PEMA is the parameters file. This file must\
    \ be located <strong>in</strong> the <em>analysis directory</em> and the user\
    \ needs to fill it <strong>every time</strong> PEMA is about to be called. If\
    \ you need more than one analyses to run, then you need to make copies of the\
    \ parameters' file and have one of those in eah of the analysis directrories you\
    \ create.</p>\n<p>So, here is the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\
    ><em><strong>parameters.tsv</strong></em></a> file as it looks like, in a study\
    \ case of our own.</p>\n\n<h1>\n<a id=\"user-content-pema-on-hpc\" class=\"anchor\"\
    \ href=\"#pema-on-hpc\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>PEMA on HPC</h1>\n<p>PEMA is best to run on\
    \ HPC (server, cluster, cloud). Usually environmental data are quite large and\
    \ the whole process has huge computational demands. To get PEMA running on your\
    \ HPC you (actually your system administrator) need to install Singularity as\
    \ described below.</p>\n<h2>\n<a id=\"user-content-prerequisites\" class=\"anchor\"\
    \ href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Prerequisites</h2>\n<p><strong><a href=\"https://www.sylabs.io/guides/3.0/user-guide/quick_start.html#quick-installation-steps\"\
    \ rel=\"nofollow\">Singularity</a></strong>  is a free, cross-platform and open-source\
    \ computer program that performs operating-system-level virtualization also known\
    \ as containerization. One of the main uses of Singularity is to bring containers\
    \ and reproducibility to scientific computing and the high-performance computing\
    \ (HPC) world.</p>\n<p>Singularity needs a Linux/Unix system to run.</p>\n<h2>\n\
    <a id=\"user-content-installing\" class=\"anchor\" href=\"#installing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing</h2>\n\
    <p>After you install Singularity in your environment and open it, you need to\
    \ download PEMA's image from Singularity Hub, by running the command:</p>\n<pre><code>\
    \ singularity pull shub://hariszaf/pema:v.1.1\n</code></pre>\n<p>Now you have\
    \ PEMA on your environment. But there is still one really <strong>important</strong>\
    \ thing that you need to do! Please <strong>download</strong> the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\
    ><em>parameters.tsv</em></a> file and move it or copy it to the same directory\
    \ with your raw data.</p>\n<p>Now you are ready to go!</p>\n<h2>\n<a id=\"user-content-running-pema\"\
    \ class=\"anchor\" href=\"#running-pema\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running PEMA</h2>\n<p>Singularity\
    \ permits the use of a job scheduler that allocates computional resources on clusters\
    \ and at the same time, works as a queuing system, as <strong><a href=\"https://slurm.schedmd.com/overview.html\"\
    \ rel=\"nofollow\">Slurm</a></strong>. This way you are able to create a job as\
    \ you usually do in your system and after editing the parameters file as needed,\
    \ run PEMA as a job on your cluster.</p>\n<h3>\n<a id=\"user-content-example\"\
    \ class=\"anchor\" href=\"#example\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Example</h3>\n<pre><code>#SBATCH\
    \ --partition=batch\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20\n#SBATCH\
    \ --mem=\n# Memory per node specification is in MB. It is optional.\n# The default\
    \ limit is 3000MB per core.\n#SBATCH --job-name=\"testPema\"\n#SBATCH --output=PEMA.output\n\
    #SBATCH --mail-user=haris-zafr@hcmr.gr\n#SBATCH --mail-type=ALL\n#SBATCH --requeue\n\
    \n\nsingularity run -B /&lt;path&gt;/&lt;of&gt;/&lt;input&gt;/&lt;directory&gt;/:/mnt/analysis\
    \ /&lt;path&gt;/&lt;of&gt;/&lt;PEMA_container&gt;\n\n</code></pre>\n<p>In the\
    \ above example, we set the cluster \"Zorba\", to run PEMA in 1 node, with 20\
    \ cores.</p>\n<p>For further information, you can always check <a href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\"\
    \ rel=\"nofollow\">PEMA's tutorial</a>.</p>\n\n<h1>\n<a id=\"user-content-pema-on-a-simple-pc\"\
    \ class=\"anchor\" href=\"#pema-on-a-simple-pc\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>PEMA on a simple PC</h1>\n<h2>\n\
    <a id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h2>\n<p>To run PEMA in a simple PC on your own environment,\
    \ you first need to install <a href=\"https://docs.docker.com/install/\" rel=\"\
    nofollow\">Docker</a>, in case you do not already have it.</p>\n<p>You should\
    \ check your software version. A version of Docker is avalable for all Windows,\
    \ Mac and Linux. If you have Windows 10 Pro or your Mac's hardware in after 2010,\
    \ then you can insall Docker straightforward. Otherwise, you need to install the\
    \ <a href=\"https://docs.docker.com/toolbox/\" rel=\"nofollow\">Docker toolbox</a>\
    \ instead. You can check if your System Requirements are according to the ones\
    \ mentioned below in order to be sure what you need to do.</p>\n<p><strong>System\
    \ Requirements</strong></p>\n<pre><code>**__Windows 10 64bit__**:\nPro, Enterprise\
    \ or Education (1607 Anniversary Update, Build 14393 or later).\nVirtualization\
    \ is enabled in BIOS. Typically, virtualization is enabled by default.\nThis is\
    \ different from having Hyper-V enabled. For more detail see Virtualization must\
    \ be enabled in Troubleshooting.\nCPU SLAT-capable feature.\nAt least 4GB of RAM.\n\
    \n**__Mac__**\nMac hardware must be a 2010 or newer model, with Intel\u2019s hardware\
    \ support for memory management unit (MMU)\nvirtualization, including Extended\
    \ Page Tables (EPT) and Unrestricted Mode. You can check to see if your machine\n\
    has this support by running the following command in a terminal:\nsysctl kern.hv_support\
    \ macOS El Capitan 10.11 and newer macOS releases are supported.\nWe recommend\
    \ upgrading to the latest version of macOS.\nAt least 4GB of RAM\nVirtualBox prior\
    \ to version 4.3.30 must NOT be installed (it is incompatible with Docker for\
    \ Mac).\nIf you have a newer version of VirtualBox installed, it\u2019s fine.\n\
    </code></pre>\n<h2>\n<a id=\"user-content-installing-1\" class=\"anchor\" href=\"\
    #installing-1\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Installing</h2>\n<p>After you install Docker in your\
    \ environment and run it, the only thing you need to do, is to download PEMA's\
    \ image, by running the command:</p>\n<pre><code>docker pull hariszaf/pema\n</code></pre>\n\
    <p>The PEMA image file is a quite large (~3Gb), so it will take a while until\
    \ it is downloaded in your computer system.</p>\n<h2>\n<a id=\"user-content-running-pema-1\"\
    \ class=\"anchor\" href=\"#running-pema-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running PEMA</h2>\n<p>Running\
    \ PEMA has two discrete steps.</p>\n<h3>\n<a id=\"user-content-step-1---build-a-docker-container\"\
    \ class=\"anchor\" href=\"#step-1---build-a-docker-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 1 - Build a Docker container</h3>\n<p>At first, you need to let Docker have\
    \ access in your dataset. To provide access you need to run the following command\
    \ and specifying the path to where your data is stored, i.e. changing the &lt;path_to_analysis_directory&gt;\
    \ accordingly:</p>\n<pre><code>docker run -it -v /&lt;path_to_analysis_directory&gt;/:/mnt/analysis\
    \ hariszaf/pema\n</code></pre>\n<p>After you run the command above, you have now\
    \ built a Docker container, in which you can run PEMA!</p>\n<h3>\n<a id=\"user-content-step-2---run-pema\"\
    \ class=\"anchor\" href=\"#step-2---run-pema\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 2 - Run PEMA</h3>\n<p>Now,\
    \ being inside the PEMA container, the only thing remaining to do, is to run PEMA</p>\n\
    <pre><code>./PEMA_v1.bds\n</code></pre>\n<p>PEMA is now running. The runtime of\
    \ PEMA depends on the computational features of your environment, on the size\
    \ of your data, as well as the parameters you chose.</p>\n<p>Please, keep in mind\
    \ that when you need to copy a whole directory, then you always have to put \"\
    /\" in the end of the path that describes where the directory is located.</p>\n\
    <p>Finally, you will find the PEMA output in the analysis directory on your computer.\
    \ <br>\nAs the output directory is mounted into the built Docker container, you\
    \ can copy its contents wherever you want. However, in case you want to remove\
    \ it permanently, you need to do this as a sudo user.</p>\n\n<h1>\n<a id=\"user-content-the-phyloseq-r-package\"\
    \ class=\"anchor\" href=\"#the-phyloseq-r-package\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>The \"phyloseq\"\
    \ R package</h1>\n<p><strong>for a downstream ecological analysis of OTUs/ASVs\
    \ retrieved</strong></p>\n<p>PEMA performs all the basic functions of the \"phyloseq\"\
    \ R package. In addition, it performs certain functions of the <a href=\"https://cran.r-project.org/web/packages/vegan/index.html\"\
    \ rel=\"nofollow\"><em><strong>vegan</strong></em></a> R package.</p>\n<p>When\
    \ the user asks for a downstream analysis using the \"phyloseq\" R package, then\
    \ an extra input file called <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\
    ><em><strong>\"phyloseq_script.R\"</strong></em></a> needs to be imported in the\
    \ \"analysis_directory\". In PEMA's main repository, you can find a template of\
    \ this file; this file needs to be as it would run on your own computer, as you\
    \ would run <em>phyloseq</em> in any case. PEMA will create the <em>\"phyloseq\
    \ object\"</em> automatically and then it will perform the analysis as asked.\
    \ The output will be placed in an extra subfolder in the main output directory\
    \ of PEMA called <em>phyloseq_analysis</em>.</p>\n<p>In addition, the <em><strong>metadata.tsv</strong></em>\
    \ file is also required when the phyloseq option has been selected. An example\
    \ of this file you can find <a href=\"https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\"\
    \ rel=\"nofollow\">here</a>.</p>\n<h1>\n<a id=\"user-content-acknowledgments\"\
    \ class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgments</h1>\n<p>PEMA\
    \ uses a series of tools, datasets as well as Big Data Script language. We thank\
    \ all the groups that developed them.\nThe tools &amp; databases that PEMA uses\
    \ are:</p>\n<ul>\n<li>BigDataScript programming language - <a href=\"https://pcingola.github.io/BigDataScript/\"\
    \ rel=\"nofollow\">https://pcingola.github.io/BigDataScript/</a>\n</li>\n<li>FASTQC\
    \ - <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"\
    nofollow\">https://www.bioinformatics.babraham.ac.uk/projects/fastqc/</a>\n</li>\n\
    <li>\u03A4rimmomatic - <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\"\
    \ rel=\"nofollow\">http://www.usadellab.org/cms/?page=trimmomatic</a>\n</li>\n\
    <li>Cutadapt - <a href=\"https://cutadapt.readthedocs.io/en/stable/\" rel=\"nofollow\"\
    >https://cutadapt.readthedocs.io/en/stable/</a>\n</li>\n<li>BayesHammer - included\
    \ in SPAdes - <a href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\
    >http://cab.spbu.ru/software/spades/</a>\n</li>\n<li>PANDAseq - <a href=\"https://github.com/neufeld/pandaseq\"\
    >https://github.com/neufeld/pandaseq</a>\n</li>\n<li>OBITools - <a href=\"https://pythonhosted.org/OBITools/welcome.html\"\
    \ rel=\"nofollow\">https://pythonhosted.org/OBITools/welcome.html</a>\n</li>\n\
    <li>BLAST Command Line Applications - <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK52640/\"\
    \ rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/books/NBK52640/</a>\n</li>\n<li>VSEARCH-2.9.1\
    \ - <a href=\"https://github.com/torognes/vsearch/releases/tag/v2.9.1\">https://github.com/torognes/vsearch/releases/tag/v2.9.1</a>\n\
    </li>\n<li>SWARM - <a href=\"https://github.com/torognes/swarm\">https://github.com/torognes/swarm</a>\n\
    </li>\n<li>CROP - <a href=\"https://github.com/tingchenlab/CROP\">https://github.com/tingchenlab/CROP</a>\n\
    </li>\n<li>CREST - <a href=\"https://github.com/lanzen/CREST\">https://github.com/lanzen/CREST</a>\n\
    </li>\n<li>RDPClassifier - <a href=\"https://github.com/rdpstaff/classifier\"\
    >https://github.com/rdpstaff/classifier</a>\n(RPDtools are required in order to\
    \ execute RDPClassifier)</li>\n<li>SILVA db - <a href=\"https://www.arb-silva.de/no_cache/download/archive/current/Exports/\"\
    \ rel=\"nofollow\">https://www.arb-silva.de/no_cache/download/archive/current/Exports/</a>\n\
    </li>\n<li>MIDORI db - <a href=\"http://reference-midori.info/index.html\" rel=\"\
    nofollow\">http://reference-midori.info/index.html</a>\n</li>\n<li>\"phat\" algorithm,\
    \ from the \"gappa\" package - <a href=\"https://github.com/lczech/gappa/wiki/Subcommand:-phat\"\
    >https://github.com/lczech/gappa/wiki/Subcommand:-phat</a>\n</li>\n<li>MAFFT -\
    \ <a href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\">https://mafft.cbrc.jp/alignment/software/</a>\n\
    </li>\n<li>RAxML -ng - <a href=\"https://github.com/amkozlov/raxml-ng\">https://github.com/amkozlov/raxml-ng</a>\n\
    </li>\n<li>PaPaRa - <a href=\"https://cme.h-its.org/exelixis/web/software/papara/index.html\"\
    \ rel=\"nofollow\">https://cme.h-its.org/exelixis/web/software/papara/index.html</a>\n\
    </li>\n<li>EPA-ng - <a href=\"https://github.com/Pbdas/epa-ng\">https://github.com/Pbdas/epa-ng</a>\n\
    </li>\n<li>phyloseq R package - <a href=\"http://joey711.github.io/phyloseq/index.html\"\
    \ rel=\"nofollow\">http://joey711.github.io/phyloseq/index.html</a>\n</li>\n<li>vegan\
    \ R package - <a href=\"https://cran.r-project.org/web/packages/vegan/index.html\"\
    \ rel=\"nofollow\">https://cran.r-project.org/web/packages/vegan/index.html</a>\n\
    </li>\n</ul>\n<p>And of course the container-based technologies:</p>\n<ul>\n<li>Docker\
    \ - <a href=\"https://www.docker.com/\" rel=\"nofollow\">https://www.docker.com/</a>\n\
    </li>\n<li>Singularity - <a href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\
    >https://sylabs.io/singularity/</a>\n</li>\n</ul>\n<h1>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h1>\n<p>PEMA is under\
    \ the GNU GPLv3 license (for 3rd party components separate licenses apply).</p>\n\
    <h1>\n<a id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citation</h1>\n\
    <p>Haris Zafeiropoulos, Ha Quoc Viet, Katerina Vasileiadou, Antonis Potirakis,\
    \ Christos Arvanitidis, Pantelis Topalis, Christina Pavloudi, Evangelos Pafilis,\
    \ PEMA: a flexible Pipeline for Environmental DNA Metabarcoding Analysis of the\
    \ 16S/18S ribosomal RNA, ITS, and COI marker genes, GigaScience, Volume 9, Issue\
    \ 3, March 2020, giaa022, <a href=\"https://doi.org/10.1093/gigascience/giaa022\"\
    \ rel=\"nofollow\">https://doi.org/10.1093/gigascience/giaa022</a></p>\n"
  stargazers_count: 12
  subscribers_count: 3
  topics: []
  updated_at: 1623939477.0
heathsc/gemBS:
  data_format: 2
  description: gemBS is a bioinformatics pipeline designed for high throughput analysis
    of DNA methylation from Whole Genome Bisulfite Sequencing data (WGBS).
  filenames:
  - Singularity
  - IHEC/Singularity.ihec
  full_name: heathsc/gemBS
  latest_release: v3.5.1_IHEC
  readme: "<h1>\n<a id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>News</h1>\n\
    <p>First release of gemBS-rs, a complete rewrite of the gemBS pipeline (apart\
    \ from the mapper) in Rust bringing increased\nstability while maintaining the\
    \ high performance of gemBS: <a href=\"https://github.com/heathsc/gemBS-rs.git\"\
    >https://github.com/heathsc/gemBS-rs.git</a></p>\n<h1>\n<a id=\"user-content-gembs\"\
    \ class=\"anchor\" href=\"#gembs\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>gemBS</h1>\n<p>gemBS is a high performance\
    \ bioinformatic pipeline designed for highthroughput analysis\nof DNA methylation\
    \ data from whole genome bisulfites sequencing data\n(WGBS). It combines GEM3,\
    \ a high performance read aligner and\nbs_call, a high performance variant and\
    \ methyation caller, into a streamlined and efficient pipeline for\nbisulfite\
    \ sueqnce analysis.</p>\n<p>The manuscript describing the pipeline is available\
    \ <a href=\"https://www.biorxiv.org/content/early/2017/10/11/201988\" rel=\"nofollow\"\
    >here</a></p>\n<hr>\n<h2>\n<a id=\"user-content-licensing\" class=\"anchor\" href=\"\
    #licensing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Licensing</h2>\n<p>gemBS is licensed under GPL.</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Download</h2>\n\
    <p>Use <code>git clone --recursive</code> to retrieve the complete source code\
    \ including the code from external projects such as <code>bs_call</code> and <code>gem3-mapper</code>.</p>\n\
    <pre><code>git clone --recursive https://github.com/heathsc/gemBS.git\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<ol>\n<li>Before starting the installation of gemBS,\
    \ you will need to install\nor check the installation of several packages.</li>\n\
    </ol>\n<p>a) gcc with development libraries\nb) python3, pip3, matplotlib, multiprocess\n\
    c) zlib, lzma, openssl, libcurl, libncurses, wget, pigz</p>\n<p>If you are working\
    \ on a clean (fairly recent) Ubuntu installation, you\ncan install everything\
    \ required with the followiwg commands:</p>\n<pre><code>sudo apt-get update\n\
    sudo apt-get install -y python3 build-essential git python3-pip wget pigz\nsudo\
    \ apt-get install -y zlib1g-dev libbz2-dev\nsudo apt-get install -y libncurses5-dev\
    \ liblzma-dev libssl-dev libcurl4-openssl-dev\npip3 install matplotlib multiprocess\n\
    </code></pre>\n<ol start=\"2\">\n<li>\n<p>Download the gemBS distribution if you\
    \ haven't already done so:</p>\n<p><code>git clone --recursive https://github.com/heathsc/gemBS.git</code></p>\n\
    </li>\n<li>\n<p>Use python install command:</p>\n</li>\n</ol>\n<p>To install to\
    \ the standard system location (i.e., so that all users\ncan use gemBS):</p>\n\
    <pre><code>``python3 setup.py install``\n</code></pre>\n<p>To install to the user's\
    \ home directory:</p>\n<pre><code>``python3 setup.py install --user``\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-check-your-installation\" class=\"anchor\" href=\"\
    #check-your-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Check your installation</h2>\n<p>For checking\
    \ your installation follow this\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/example.html\"\
    \ rel=\"nofollow\">worked example</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Documentation\
    \ can be found at\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/index.html\"\
    \ rel=\"nofollow\">gemBS</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-changelog\"\
    \ class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Changelog:</h2>\n<pre><code>3.5.5\
    \ Fix logging bug caused by trimming change in 3.5.3\n3.5.4 Fix bug in the output\
    \ of strand specific cpg txt files (not\n      encode Bed files) where the 'C'\
    \ entry was not being printed\n3.5.3 Allow for read end specific trimming in bs_call\n\
    3.5.3 Enable range checks and asserts in bs_call all target; add bs_call debug\
    \ target\n3.5.2 Correct problems with gcc10.  Move to htslib/samtools/bcftools\
    \ version 1.11\n3.5.1 Check if C compiler requires --std=c99 flag for standards\
    \ conformant behaviour\n3.5.1 Make sure bgzip is copied correctly during installation\n\
    3.5.0 Make bs_call process contig pools from largest to smallest (this change\
    \ alters the sqlite db format so\n      if you have a previously started gemBS\
    \ run you should (a) remove the .gemBS directory, (b) redo the\n      'gemBS prepare'\
    \ step to recreate the db file and (3) run 'gemBS db-sync'. \n3.5.0 Switch bs_call\
    \ and snpxtr to use the new dbSNP index format\n3.5.0 Add ability of dbSNP to\
    \ read the new JSON and VCF  dbSNP format files\n      that are now used for human\
    \ and non-human species respectively\n3.5.0 Add multithreading to dbSNP_idx\n\
    3.5.0 Change format of dbSNP index to allow (a) efficient loading\n      of SNP\
    \ data for individual contigs and (b) parallel index creation \n3.5.0 Rewrite\
    \ mextr and snpxtr as standalone tools rather than\n      bcftools plugins.  Now\
    \ multithreaded and (relatively) memoryefficient\n3.5.0 Replace bedToBigBed and\
    \ wigToBigWig to reduce memory usage\n      and improve speed\n3.4.5 Fix crash\
    \ when using the -k (keep-mismatch) flag, and fix rare hangs at end of processing\n\
    3.4.4 Sort input bcf files to bcftools concat stage to ensure reproducibility.\n\
    3.4.4 Add extra sort keys when generating pools to ensure stability of pool membership\
    \ in the event of multiple contigs\n      having the same size\n3.4.3 Remove calculation\
    \ of the goodness of filter (GOF) as this is expensive, non-standard and unreliable.\
    \  Removing this\n      removes the dependency on GSL.\n3.4.3 Add autodetection\
    \ of output format to bs_call (unless explicitly specified on the command line)\n\
    3.4.2 Add CRAM support (via make_cram option in configuration file)\n3.4.1 Add\
    \ benchmark-mode that does not write date or program version numbers into SAM/BAM\
    \ or VCF/BCF files\n      Switch to samtools, bcftools and htslib v1.10\n3.4.0\
    \ Move to new bs_call version (2.1.0) which is more efficient\n      in memory\
    \ use and can read BAMs and write BCFs natively.\n      The new bs_call requires\
    \ a faidx indexed reference, so gemBS\n      no creates this during indexing.\n\
    3.4.0 Add switches to give more control to threads and memory\n      usage in\
    \ mapping and calling stages\n3.3.3 Remove legacy pathway for config files with\
    \ no header line (fix issue 'error in gemBS index #65)\n3.3.2 Fix error where\
    \ header line for wig files could be omitted\n3.3.2 Fix generation of non_cpg\
    \ files\n3.3.1 Fix Attribute error bug due to not checking if conversion is a\
    \ list\n3.3.0 Make new release for IHEC\n3.3.0 Switch conversion default in IHEC_standard\
    \ configuration to 0.01,0.05 rather than auto, which can give odd results if conversion\
    \ controls not present or not working correctly\n3.3.0 Fix bug where conversion\
    \ parameters could be ignored\n3.2.13 Fix formatting bug in mextr with multiple\
    \ samples (not triggered in normal gemBS use)\n3.2.12 Ensure that conversion statistics\
    \ are correctly calculated for non-stranded or reverse conversion protocols\n\
    3.2.11 Introduce reverse_conversion option for mapping where read 1 is G2A converted\
    \ and read 2 is C2T converted\n3.2.10 Correct regex patch for single end reads\n\
    3.2.9 Update Singularity and Dockerfile recipes to allow kemp utils to be built\
    \ correctly\n3.2.9 Make setup.py and gemBS/commands.py read the version information\
    \ from gemBS/version.py, so ensuring consistency\n3.2.9 Fix bug added in last\
    \ version where options in config file were not being taken into account\n3.2.8\
    \ Fix mis specification errors in long options for mextr. \n3.2.8 Fix bug where\
    \ mextr (methyl extract plugin for bcftools) would crash if cpg output  option\
    \ was not set.\n3.2.7 Apply patches for bugs in handling single end reads (suggested\
    \ by I. Moghul)\n3.2.7 Changed regex for filenames to make it more general (suggested\
    \ by I. Moghul)\n3.2.7 Fixed bug (reported by chhylp123) where zero arguments\
    \ to some options were being ignored\n3.2.6 Cleaned up compilation and cleaning\
    \ of gemBS tools\n3.2.6 Fixed python error if either the over conversion reference\
    \ sequence was not defined\n3.2.6 Added check in bs_call that conversion parameters\
    \ are valid (between 0 and 1)\n3.2.6 Perform more stringent sanity checking on\
    \ conversion vaalues when autocomputed by gemBS\n3.2.6 Use --diasble-lzma configuration\
    \ flag for samtools and bcftools as we don't need it and it removes an unneccesary\
    \ dependency\n3.2.6 Add install options --disable-cuda (on by default) and --enable-cuda\
    \ that affect GEM3 comppilation\n3.2.6 Bug fix with incorrect handling of duplicate\
    \ reads\n3.2.5 Minor bug fix - correct error with non-paired end non-bisulfite\
    \ reads\n3.2.4 Modify the bisulfite processing in gem-mapper to be more efficient\
    \ (in particular for the non-stranded option)\n3.2.4 Modify gemBS to use the new\
    \ conversion options for gem-mapper\n3.2.4 Switch gem-mapper to use option --underconversion-sequence\
    \ and --overconversion-sequence rather than --underconversion_sequence to be consistent\
    \ with other options\n3.2.3 Fixed bug if conversion parameters were not set\n\
    3.2.2 Rework non-stranded mode so that both possible conversions are tried and\
    \ the results merged\n3.2.2 Fix bug where non-stranded flag was not being passed\
    \ to mapper in paired end mode\n3.2.1 Move warning message from bscall from stdout\
    \ to stderr\n3.2.1 Switch Singularity build to use Ubuntu 16.04 rather than 18.04\
    \ to allow the image to work in CentOS 6 (Docker build changed as well to keep\
    \ the two in sync)\n3.2.1 Fix undeclared variable bugs and missing --ignore-deps\
    \ option in merge-bcfs\n3.2.1 Add default for dbSNP_index if dbSNP_files is set\n\
    3.2.1 Add gsl-path install option\n3.2.0 Make new release\n3.1.0 Make installation\
    \ process more modular.  Allow for sub-installs\n3.1.0 Add support for reading\
    \ config from ${index_dir}/gemBS.json if it exists\n3.1.0 Add --reference-bias\
    \ option to mextr and gemBS extract\n3.1.0 Add support for non-bisulfite mapping\
    \ of individual datasets\n3.1.0 Allow white space in variable values\n3.1.0 Allow\
    \ fallback to gzip if pigz not present\n3.1.0 Add --dry-run, --json, --ignore-db\
    \ and --ignore-dep to extract command\n3.1.0 Add --ignore-dep option to call and\
    \ merge-bcfs commands\n3.1.0 Add SNP extraction function to extract command\n\
    3.0 Make v3.0 release\n3.0 Merge with master branch.\n3.0 Bump samtools sort memory\
    \ limit to 2G\n3.0 Add extra_references option for reference generation\n3.0 Allow\
    \ input files to mapping to be shell commands\n3.0 Add links to documentation\n\
    3.0 Upload new yeast example and add documentation\n3.0 Add --dir option to gemBS\n\
    3.0 Add --ignore-db options for --dry-run / --json\n3.0 Add --json output option\
    \ for dry runs\n3.0 Update help text to match new functions\n3.0 Introduce standard\
    \ analysis configurations stored within distribution\n3.0 Switch gem3-mapper distribution\
    \ to gembs branch on official gem3-mapper repo\n3.0 Removal of incomplete files\
    \ and roll back of db in the event of pipeline failure\n3.0 Automatic removal\
    \ of individual BAMs and BCFs after successful merging\n3.0 Prevent pipelines\
    \ hanging in event of failure\n3.0 Generate ENCODE bed and bigbed files\n3.0 Switch\
    \ to python 3\n3.0 Switch to mextr for BCF filtering\n3.0 Include fetch and build\
    \ of samtools / bcftools during build process\n3.0 Add dry-run capability to map\
    \ and call commands\n3.0 Introduce contig pools to automatically group small contigs\n\
    3.0 Automatic generation of contig.size files from index build\n3.0 Allow use\
    \ of in memory sqlite3 db as an option\n3.0 Allow multiple instances of gemBS\
    \ (possible on different hosts) to work \n    simultaneously on the same analysis\n\
    3.0 Reduce and simply commands\n3.0 Add Dockerfile\n3.0 Add multi-threading and\
    \ multi-processing options for most commands\n3.0 Use sqlite3 to track progress\
    \ of analyses, file paths etc.\n3.0 Added more flexible configuration options\
    \ (new csv format + new configuration file)\n3.0 Remove test dataset from distribution\
    \ (distribute from web site)\n2.1.0 Ensure commands run during pipeline come from\
    \ installation\n2.1.0 Added Singularity build recipe\n2.1.0 Add new command gemBS\
    \ direct-mapping\n2.1.0 Fixed Makefile clean in tools\n2.0.2 Fixed bug related\
    \ with the percentage of High Quality Variant in Variants summary report.\n2.0.2\
    \ Check temporary directory existence.\n2.0.2 Fixed QualityNonRefCpg sample name\
    \ in png image.\n2.0.2 Fixed mapper issues related with aligning performace.\n\
    2.0.2 Fixed arguments for Under/Over Conversion sequence name in gem3-mapper\n\
    2.0.1 On bscall repository, fixed argument -k about discarded reads that do not\
    \ form proper pairs.\n2.0 Check tmp folder before starting mapping process.\n\
    2.0 Added Left and Right Trimming optional arguments to gemBS bscall.\n2.0 Added\
    \ GC Coverage correlation value to BS Call Stats Summary.\n2.0 Fixed error when\
    \ reporting complete path to not found bam files.\n2.0 Fixed iteration over sampleBams\
    \ dictionary in MergeAll method.\n2.0 Updated: Avoid redo indexing when merging\
    \ just one file.\n2.0 Changed conversion formula.\n2.0 Added parameter for dbSNP.\n\
    2.0 Added threads to bscall.\n2.0 Removed CpGs reports. Already done from bscall\
    \ report.\n2.0 Fixed bs_call makefile for the gcc to be used.\n2.0 New bscall\
    \ version. Generates JSON report.\n2.0 Removed gemBS options snp-stats,cpg-report,cpg-stats.\n\
    2.0 Added summary report from the bs_call json stats\n2.0 New BSCall Report. From\
    \ bscall son file generates three types of reports:\n    Mapping and Coverage\
    \ Report\n    Bs-Genotypes Calls Report\n    Methylation Statistics report\n1.7\
    \ Added non stranded read conversion parameter\n1.7 Fixed SE crash when estimating\
    \ overlapped bases.\n1.7 Fixed gem-index (gem3) to follow fastq and SAM specifications.\
    \ \n    Modified gem3-mapper repository external module.\n    New external module\
    \ https://github.com/heathsc/gem3-mapper.git\n1.7 Fixed threads parameter to samtools\
    \ merge\n1.7 Fixed threads parameter to gem-mapper\n1.7 Removed Indels Field on\
    \ Variants Report.\n1.7 Added Overlapping Bases at Mapping Report\n1.7 Modified\
    \ Base Counts Overall, removed Base Counts general and Base Counts Overall\n1.7\
    \ New Dinucleotide CpGs Report\n    New table dinucleotide stats\n    New plots\
    \ for Informative Reads and CpGs\n    Methylation levels plots for different types\
    \ of CpGs\n    Summary Table\n1.7 New Readme file to inform about report test\n\
    1.7 New basic statis table for Variants Report\n1.7 Removed parameter -r (reference\
    \ length) parameter for mapping reports command (gemBS bsMap).\n1.6 New CpGs Density\
    \ plot, include box plos, bar plot and fitting curve\n1.6 Change name at CpG report:\n\
    \    \"Heterozygous\" for \"Alternative CX\"\n    \"De Novo CpGs Methylation Status\"\
    \ for \"Non Reference CpGs\"\n    \"CpGs with SNP\" for \"SNPs (CX) at Reference\
    \ CpGs\"\n1.6 CpGs Report Simplified to Q&gt;20\n1.6 BigWig Default parameters\
    \ for filtering CpG per a given quality and a total number of supported informative\
    \ reads   \n1.5 Initial Release  \n</code></pre>\n<hr>\n<h2>\n<a id=\"user-content-developers\"\
    \ class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Developers</h2>\n<p>gemBS:</p>\n\
    <ul>\n<li>Marcos Fernandez-Callejo - <a href=\"mailto:marcos.fernandez@cnag.crg.eu\"\
    >marcos.fernandez@cnag.crg.eu</a>\n</li>\n<li>Simon Heath - <a href=\"mailto:simon.heath@gmail.com\"\
    >simon.heath@gmail.com</a>\n</li>\n</ul>\n<p>gem mapper:</p>\n<ul>\n<li>Santiago\
    \ Marco-Sola - <a href=\"mailto:santiagomsola@gmail.com\">santiagomsola@gmail.com</a>\n\
    </li>\n</ul>\n<p>bisulfite caller and filtering:</p>\n<ul>\n<li>Simon Heath -\
    \ <a href=\"mailto:simon.heath@gmail.com\">simon.heath@gmail.com</a>\n</li>\n\
    </ul>\n"
  stargazers_count: 24
  subscribers_count: 4
  topics: []
  updated_at: 1624379756.0
heathsc/gemBS-rs:
  data_format: 2
  description: A re-write of the gemBS pipeline framework in Rust
  filenames:
  - Singularity
  - texlive/Singularity.tex
  full_name: heathsc/gemBS-rs
  latest_release: v4.0
  readme: "<h1>\n<a id=\"user-content-gembs-rs\" class=\"anchor\" href=\"#gembs-rs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>gemBS-rs</h1>\n<p>A rewrite of the <a href=\"https://github.com/heathsc/gemBS\"\
    >gemBS</a> pipeline\nframework from Python/C into Rust.</p>\n<p>gemBS is a high\
    \ performance bioinformatic pipeline designed for highthroughput analysis\nof\
    \ DNA methylation data from whole genome bisulfites sequencing data\n(WGBS). It\
    \ combines GEM3, a high performance read aligner and\nbs_call, a high performance\
    \ variant and methyation caller, into a streamlined and efficient pipeline for\n\
    bisulfite sequence analysis.</p>\n<p>The manuscript describing the original gemBS\
    \ pipeline is available\n<a href=\"https://doi.org/10.1093/bioinformatics/bty690\"\
    \ rel=\"nofollow\">here</a></p>\n<p>The rewrite of the pipeline into Rust has\
    \ two aims: (1) to have a more\nrobust pipeline and (2) to provide a more flexible\
    \ platform for future\ndevelopments.  All of the tools developed for the pipeline\
    \ except for the GEM3 mapper (being an external project that is also very stable!)\
    \ have now been re-written in Rust. These include bs_call, the methylation and\
    \ SNV-variant caller, and the methylation and SNP extractions tools mextr and\
    \ snpxtr.  In all cases the running times are comparable to the original C versions.</p>\n\
    <hr>\n<h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<p>The pipeline uses samtools for generating sorted\
    \ BAM/CRAM files from GEM3 and bcftools for merging and indexing BCF files produced\
    \ by bs_call.  In addition, many of the tools link to htslb to enable reading\
    \ of BAM/CRAM and reading/writing of BCF files.  Samtools and htslib are automatically\
    \ installed during the installation of the gemBS pipeline.   There is also an\
    \ optional dependency on TeXLive which is used to produce pdf versions of the\
    \ QC reports.  If requested by the user this is also installed with the pipeline.</p>\n\
    <hr>\n<h2>\n<a id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Licensing</h2>\n<p>gemBS is licensed under the GPL.</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Download</h2>\n\
    <p>Use <code>git clone --recursive</code> to retrieve the complete source code\
    \ including the code from external projects such as <code>gem3-mapper</code>.</p>\n\
    <pre><code>git clone --recursive https://github.com/heathsc/gemBS-rs.git\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-configure--install\" class=\"anchor\" href=\"\
    #configure--install\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Configure &amp; Install</h2>\n<p>Before starting\
    \ the installation of gemBS, you will need to install\nor check the installation\
    \ of several packages.</p>\n<p>a) gcc with development libraries</p>\n<p>b) rust\
    \ (for installation instructions see <a href=\"https://www.rust-lang.org/learn/get-started\"\
    \ rel=\"nofollow\">here</a>).  Note that if you have rust already installed you\
    \ should update it using <code>rustup update</code> before trying to compile gemBS.</p>\n\
    <p>c) zlib, libz2, lzma, openssl, libcurl, libncurses, wget, expat, ncurses, openssl,\
    \ freetype, fontconfig</p>\n<p>If you are working on a clean (fairly recent) Ubuntu\
    \ installation, you\ncan install everything required with the following commands:</p>\n\
    <pre><code>apt-get install -y build-essential git autoconf wget lbzip2 pkg-config\
    \ cmake\napt-get install -y zlib1g-dev libbz2-dev libexpat1-dev\napt-get install\
    \ -y libncurses5-dev liblzma-dev libssl-dev libcurl4-openssl-dev curl\napt-get\
    \ install -y libfreetype6-dev libfontconfig1-dev\ncurl https://sh.rustup.rs -sSf\
    \ &gt; rust.sh &amp;&amp; sh rust.sh -y\n</code></pre>\n<p>Download the gemBS\
    \ distribution if you haven't already done so:</p>\n<pre><code>git clone --recursive\
    \ https://github.com/heathsc/gemBS-rs.git\n</code></pre>\n<p>From the main gemBS-rs\
    \ directory type the following to make the default config file:</p>\n<pre><code>make\
    \ gemBS_config.mk\n</code></pre>\n<p>Then look at the file gemBS_config.mk and\
    \ make any changes that are required.  When the file is OK the pipeline and components\
    \ can be built and installed by typing:</p>\n<pre><code>make install\n</code></pre>\n\
    <p>If the make and install process is successful, a shell script called gemBS\
    \ will be created in the main gemBS-rs directory.  This file should be copied\
    \ to a directory that is in your PATH so that gemBS can be invoked from anywhere.</p>\n\
    <hr>\n<h2>\n<a id=\"user-content-check-your-installation\" class=\"anchor\" href=\"\
    #check-your-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Check your installation</h2>\n<p>For checking\
    \ your installation follow this\n<a href=\"http://statgen.cnag.cat/gemBS/UserGuide/_build/html/example.html\"\
    \ rel=\"nofollow\">worked example</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Documentation\
    \ can be found at\n<a href=\"http://statgen.cnag.cat/gemBS/\" rel=\"nofollow\"\
    >gemBS</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-changelog\" class=\"anchor\"\
    \ href=\"#changelog\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Changelog:</h2>\n<pre><code>4.0 First release\
    \ of gemBS-rs (4th release of gemBS)\n4.0.1 Correct bug preventing that caused\
    \ non-stranded mapping to fail\n4.0.2 Move to versions 1.12 of htslib/samtools/bcftools\n\
    4.0.2 Change way we iterate over SAM/BAM/CRAM files to the same way used in samtools\
    \ \n      view (the old way did not always work with cram files)\n</code></pre>\n"
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1621670204.0
hejm37/sysu-planner:
  data_format: 2
  description: sparkle planning challenge
  filenames:
  - Singularity
  full_name: hejm37/sysu-planner
  latest_release: null
  readme: '<h1>

    <a id="user-content-sysu-planner" class="anchor" href="#sysu-planner" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>sysu-planner</h1>

    <p>The SYSU-Planner is a two-stage planner designed to solve classical planning
    problems. It first performs the 1-BFWS (<a href="https://people.eng.unimelb.edu.au/nlipovetzky/papers/aaai17-BFWS-novelty-exploration.pdf"
    rel="nofollow">Nir and Hector 2017</a>) with very fast speed. If it fails to find
    a solution, it will then perform a modified online refinement algorithm named
    <a href="http://ada.liacs.nl/events/sparkle-planning-19/documents/solver_description/SYSU-planner-description.pdf"
    rel="nofollow">Forward-RHC</a> (see also <a href="https://ipc2018-classical.bitbucket.io/planner-abstracts/team8.pdf"
    rel="nofollow">Maximilian and Jorg 2018</a>).</p>

    <h2>

    <a id="user-content-build-and-run-with-container" class="anchor" href="#build-and-run-with-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    and run with container</h2>

    <p>Using the planner with <a href="https://sylabs.io/docs/#singularity" rel="nofollow">Singularity</a>
    is rather simple. First install Singularity following <a href="https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">this guide</a>. Then run the following script in CLI and you will
    have the plan file <em>sas_plan</em> under <em>$RUNDIR</em>.</p>

    <pre><code>sudo singularity build planner.img sysu-planner/Singularity

    mkdir rundir

    cp path/to/domain.pddl rundir

    cp path/to/problem.pddl rundir

    RUNDIR="$(pwd)/rundir"

    DOMAIN="$RUNDIR/domain.pddl"

    PROBLEM="$RUNDIR/problem.pddl"

    PLANFILE="$RUNDIR/sas_plan"

    singularity run -C -H $RUNDIR planner.img $DOMAIN $PROBLEM $PLANFILE $COSTBOUND

    </code></pre>

    <h3>

    <a id="user-content-supported-problems" class="anchor" href="#supported-problems"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Supported
    problems</h3>

    <p>The formulation of supported problems is the same as <a href="https://ipc2018-classical.bitbucket.io/#pddl"
    rel="nofollow">IPC 2018</a>. We also provide a set of supported domains and problems
    in <a href="https://github.com/hejm37/benchmark-domains">benchmark-domains</a>.</p>

    <h2>

    <a id="user-content-notes-on-playing-with-the-source-code" class="anchor" href="#notes-on-playing-with-the-source-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Notes
    on playing with the source code</h2>

    <p>The source code of the planner contains two part:</p>

    <ul>

    <li>BFWS-public and its dependency, LAPKT-public</li>

    <li>fast-downward-conjunctions</li>

    </ul>

    <p>Then planner should be invoked in the fast-downward-conjunctions part (using
    --dual option and it will call BFWS-public/fd-version/bfws.py to perform 1-BFWS,
    see <a href="https://github.com/hejm37/sysu-planner/blob/master/Singularity">the
    Singularity script</a> for more details).</p>

    <h3>

    <a id="user-content-potential-failures" class="anchor" href="#potential-failures"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Potential
    Failures</h3>

    <p>If the above build has failed, it may appears to be a cmake cache fail. In
    this case, remove the <em>builds</em> (if it exists) directory under fast-downward-conjunctions
    and rerun the singularity command shall solve the problem.</p>

    <p>Or it may appears to be a scons build fail. In this case, remove all the <em>.sconsign.dblite</em>
    files under the directory shall solve the problem.</p>

    <p>Both cases would occur if the planner was built outside a container.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1563536767.0
hexmek/container:
  data_format: 2
  description: null
  filenames:
  - Singularity.iqtree
  - Singularity.ncbi-downloader
  - Singularity.BUSCO4
  - Singularity.mashmap
  - Singularity.snakemake
  - Singularity.bioconvert
  - Singularity.metawrap
  - Singularity.VAMB
  - Singularity.nanofilt
  - Singularity.biopython
  - Singularity.bwa
  - Singularity.antismash_standalone
  - Singularity.pysam
  - Singularity.puntseq
  - Singularity.eukcc_vanilla
  - Singularity.METAMVGL
  - Singularity.CAT_update
  - Singularity.dbcan
  - Singularity.metawap_docker
  - Singularity.minimap2
  - Singularity.kraken2
  - Singularity.BUSCO414
  - Singularity.bbmap
  - Singularity.trimal
  - Singularity.krona
  - Singularity.bioinfo
  - Singularity.sepp
  - Singularity.dRep
  - Singularity.cmseq
  - Singularity.VAMB_10.1
  - Singularity.mmseq2
  - Singularity.mummer
  - Singularity.BUSCO5
  - Singularity.repeatmasker
  - Singularity.ploidyNGS
  - Singularity.ete3
  - Singularity.CAT
  - Singularity.deeptools
  - Singularity.metaeuk
  - Singularity.megahit
  - Singularity.EukRep
  - Singularity.famsa
  - Singularity.fastani
  - Singularity.comparem
  - Singularity.nQuire
  - Singularity.qiime2
  - Singularity.euk_decide
  - Singularity.pasta
  - Singularity.sourmash
  - Singularity.spades
  - Singularity.raxml-ng
  - Singularity.metabat2
  - Singularity.tree
  - Singularity.spades_3.13
  - Singularity.bamm
  - Singularity.mafft
  - Singularity.mash
  - Singularity.dRep3
  - Singularity.art
  - Singularity.cmseq_conda
  - Singularity.spades_3.15
  - Singularity.seqtk
  - Singularity.VAMP
  - Singularity.R
  full_name: hexmek/container
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"\
    #singularity-rnaseq\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-rnaseq</h1>\n<h2>\n<a id=\"user-content-running-jupyter\"\
    \ class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Jupyter</h2>\n<p>Run\
    \ this to start Jupyter:</p>\n<pre><code>singularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\
    </code></pre>\n<p>Then follow the instructions that Jupyter printed to the terminal\
    \ when you started it up to access Jupyter in your web browser</p>\n<h3>\n<a id=\"\
    user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Accessing Jupyter on a remote server</h3>\n<p>If you are running the\
    \ container on a remote server, you will need to set up port forwarding with ssh\
    \ to be able to access Jupyter.  Run this command to forward the default Jupyter\
    \ port (8888)</p>\n<pre><code>ssh -L 8888:localhost:8888 bug\n</code></pre>\n\
    <blockquote>\n<p>Note if the default Jupyter port is not available, Jupyter will\
    \ choose a different port.  In this case you will need to substitute the port\
    \ that Jupyter outputs for 8888 in the ssh port forwarding command above.</p>\n\
    </blockquote>\n<h2>\n<a id=\"user-content-running-on-a-slurm-cluster\" class=\"\
    anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running on a SLURM Cluster</h2>\n\
    <p>You can use this image interactively on a SLURM-managed cluster by running\
    \ launching RStudio or Jupyter. The following instructions work on the Duke Compute\
    \ Cluster (DCC).  Doing this on other cluster will require some modification and\
    \ may not work, depending on how the cluster is configured.</p>\n<h3>\n<a id=\"\
    user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RStudio</h3>\n\
    <ol>\n<li>ssh to DCC login node: <code>ssh NETID@dcc-login-01.rc.duke.edu</code>\n\
    </li>\n<li>run tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n\
    <li>Run this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty\
    \ bash -i</code>\n</li>\n<li>Run <code>hostname -A</code> on compute node and\
    \ record results</li>\n<li>Run on the following on a compute node and note the\
    \ port, username, and password that the command prints:</li>\n</ol>\n<pre><code>mkdir\
    \ -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\n\
    singularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind\
    \ /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\
    </code></pre>\n<ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the port returned\
    \ but the \"singularity run\" commmand</li>\n<li>Where COMPUTE_HOSTNAME is the\
    \ hostname returned by running \"hostname -A\" on the compute node</li>\n<li>Where\
    \ NETID is your NetID</li>\n</ul>\n</li>\n<li>Go to \"localhost:PORT\" in a webrowser\
    \ and enter the username and password printed by the \"singularity run\" commmand</li>\n\
    <li>Have fun!!</li>\n<li>At the end of an analysis you will probably want to copy\
    \ results to your directory in <code>/work</code> or <code>/hpc/group</code>\n\
    </li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Jupyter</h3>\n<ol>\n<li>ssh to dcc-login-01.rc.duke.edu</li>\n<li>run\
    \ tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n<li>Run\
    \ this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty bash -i</code>\n\
    </li>\n<li>Run on compute node:</li>\n</ol>\n<pre><code>mkdir -p /scratch/josh/rnaseq_demo/rawdata\
    \ /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\
    \n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace\
    \ \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n</code></pre>\n\
    <ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the number after\
    \ <code>http://127.0.0.1:</code> in the URL given by Jupyter (defaults to 8888,\
    \ but Jupyter will use a different one if the default is in use, or if a different\
    \ port is supplied as an argument using <code>--port</code> when running the singularity\
    \ container</li>\n<li>Where COMPUTE_HOSTNAME is the hostname returned by running\
    \ \"hostname -A\" on the compute node</li>\n<li>Where NETID is your NetID</li>\n\
    </ul>\n</li>\n<li>Copy the URL supplied by jupyter that starts <code>http://127.0.0.1:</code>\
    \ and paste it in a webbrowser</li>\n<li>Have fun!!</li>\n<li>At the end of an\
    \ analysis you will probably want to copy results to your directory in <code>/work</code>\
    \ or <code>/hpc/group</code>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter-on-gpu-node\"\
    \ class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Jupyter on GPU node</h3>\n<p>Same\
    \ as above, but the srun command should use the <code>chsi-gpu</code> partition\
    \ and request a gpu, but less CPUs and Memory:</p>\n<p><code>srun -A chsi -p chsi-gpu\
    \ --gres=gpu:1 --mem=15866 -c 2 --pty bash -i</code></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1619700602.0
housw/BlendIt:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity.BlendIt.def
  full_name: housw/BlendIt
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corebedtools-intersect" class="anchor" href="#nf-corebedtools-intersect"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/bedtools-intersect</h1>

    <p><strong>Intersect lots of bed files with lots of other bed files</strong></p>

    <p><a href="https://travis-ci.org/nf-core/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/811368779316af4f70b4dd35fc2c24cebcc4dc194cd63234e130384ec38ac89f/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f626564746f6f6c732d696e746572736563742e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/bedtools-intersect.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ca7e06b0d2929a9cba14da1892e90c6d4673a695806cb07ea82e89a1cbecef92/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f626564746f6f6c732d696e746572736563742e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/bedtools-intersect.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/bedtools-intersect pipeline comes with documentation about the
    pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1623019661.0
hpcng/singularity:
  data_format: 2
  description: 'Singularity: Application containers for Linux'
  filenames:
  - e2e/testdata/Singularity
  - examples/apps/Singularity
  - examples/apps/Singularity.cowsay
  - examples/shub/Singularity
  - examples/instances/Singularity
  - examples/opensuse/Singularity
  - examples/docker/Singularity
  - examples/debian/Singularity
  - examples/self/Singularity
  - examples/asciinema/Singularity
  - examples/arch/Singularity
  - examples/centos/Singularity
  - examples/raspbian/Singularity
  - examples/library/Singularity
  - examples/scientific/Singularity
  - examples/ubuntu/Singularity
  - examples/sle/Singularity
  - examples/busybox/Singularity
  - examples/multistage/Singularity
  - examples/scratch/Singularity.busybox
  - examples/scratch/Singularity.alpine
  full_name: hpcng/singularity
  latest_release: v3.8.0
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <p><a href="https://github.com/hpcng/singularity/actions/workflows/ci.yml"><img
    src="https://github.com/hpcng/singularity/actions/workflows/ci.yml/badge.svg"
    alt="CI" style="max-width:100%;"></a></p>

    <ul>

    <li><a href="CONTRIBUTING.md">Guidelines for Contributing</a></li>

    <li><a href=".github/PULL_REQUEST_TEMPLATE.md">Pull Request Template</a></li>

    <li><a href="LICENSE.md">Project License</a></li>

    <li><a href="https://singularity.hpcng.org/docs/" rel="nofollow">Documentation</a></li>

    <li><a href="#support">Support</a></li>

    <li><a href="#citing-singularity">Citation</a></li>

    </ul>

    <p>Singularity is an open source container platform designed to be simple, fast,

    and secure. Singularity is optimized for compute focused enterprise and HPC

    workloads, allowing untrusted users to run untrusted containers in a trusted

    way.</p>

    <p>Check out <a href="https://singularity.hpcng.org/videos" rel="nofollow">talks
    about Singularity</a>

    and some <a href="https://singularity.hpcng.org/usecases" rel="nofollow">use cases
    of Singularity</a>

    on our website.</p>

    <h2>

    <a id="user-content-getting-started-with-singularity" class="anchor" href="#getting-started-with-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with Singularity</h2>

    <p>To install Singularity from source, see the <a href="INSTALL.md">installation

    instructions</a>. For other installation options, see <a href="https://singularity.hpcng.org/admin-docs/master/installation.html"
    rel="nofollow">our

    guide</a>.</p>

    <p>System administrators can learn how to configure Singularity, and get an

    overview of its architecture and security features in the <a href="https://singularity.hpcng.org/admin-docs/master/"
    rel="nofollow">administrator

    guide</a>.</p>

    <p>For users, see the <a href="https://singularity.hpcng.org/user-docs/master/"
    rel="nofollow">user

    guide</a> for details on how to use

    and build Singularity containers.</p>

    <h2>

    <a id="user-content-contributing-to-singularity" class="anchor" href="#contributing-to-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    to Singularity</h2>

    <p>Community contributions are always greatly appreciated. To start developing

    Singularity, check out the <a href="CONTRIBUTING.md">guidelines for contributing</a>.</p>

    <p>We also welcome contributions to our <a href="https://github.com/hpcng/singularity-userdocs">user

    guide</a> and <a href="https://github.com/hpcng/singularity-admindocs">admin

    guide</a>.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>To get help with Singularity, check out the <a href="https://singularity.hpcng.org/help"
    rel="nofollow">Singularity

    Help</a> web page.</p>

    <h2>

    <a id="user-content-citing-singularity" class="anchor" href="#citing-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Citing
    Singularity</h2>

    <pre><code>Kurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers
    for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459

    </code></pre>

    <p>We also have a Zenodo citation:</p>

    <pre><code>Kurtzer, Gregory M. et. al. Singularity - Linux application and environment

    containers for science. 10.5281/zenodo.1310023

    </code></pre>

    <p><a href="https://doi.org/10.5281/zenodo.1310023" rel="nofollow">https://doi.org/10.5281/zenodo.1310023</a></p>

    <p>This is an ''all versions'' DOI. Follow the link to Zenodo to obtain a DOI
    specific

    to a particular version of Singularity.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p><em>Unless otherwise noted, this project is licensed under a 3-clause BSD license

    found in the <a href="LICENSE.md">license file</a>.</em></p>

    '
  stargazers_count: 2124
  subscribers_count: 94
  topics:
  - containers
  - container
  - singularity-container
  - portable
  - linux
  - hpc
  - parallel
  - singularity
  - science
  - reproducible
  - reproducible-science
  - portability
  - rootless-containers
  - cloud-native
  updated_at: 1624725016.0
humanconnectome/hcp-pipelines-singularity:
  data_format: 2
  description: The definition files for creating singularity containers that can run
    in the WashU HPC
  filenames:
  - Singularity.def
  full_name: humanconnectome/hcp-pipelines-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-definitions-for-hcp-pipelines" class="anchor"
    href="#singularity-definitions-for-hcp-pipelines" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Singularity Definitions for HCP Pipelines</h1>

    <p>The definition files for creating singularity containers for the XNAT pipelines

    wrapper code so that it can run in the WashU HPC.</p>

    <h2>

    <a id="user-content-cloning-with-submodules" class="anchor" href="#cloning-with-submodules"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cloning
    with Submodules</h2>

    <p>Don''t forget to pull down the submodules as well, with the <code>--recursive</code>
    flag.</p>

    <pre><code>git clone https://github.com/humanconnectome/hcp-pipelines-singularity
    --recursive

    </code></pre>

    <h2>

    <a id="user-content-development" class="anchor" href="#development" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Development</h2>

    <table>

    <thead>

    <tr>

    <th>Command</th>

    <th>Task</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><code>make clean</code></td>

    <td>Remove previous container image.</td>

    </tr>

    <tr>

    <td><code>make update</code></td>

    <td>Update all the git submodule repos.</td>

    </tr>

    <tr>

    <td><code>make build</code></td>

    <td>Generate a container image from .def file</td>

    </tr>

    <tr>

    <td><code>make upload</code></td>

    <td>Upload the container to correct location in the HPC.</td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610395015.0
ianjamesx/DeepCAM_Singularity:
  data_format: 2
  description: Experimental Singularity container for MLCommons DeepCAM Climate Segmentation
    Benchmark
  filenames:
  - Singularity
  full_name: ianjamesx/DeepCAM_Singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-deepcam-singularity" class="anchor" href="#deepcam-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>DeepCAM
    Singularity</h1>

    <p>Singularity container for MLCommons DeepCAM Climate Segmentation Benchmark,
    reference implementation developed by Lawrence Berkeley National Laboratory</p>

    <p>Currently in progress</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1624406680.0
iapalm/lc-builds:
  data_format: 2
  description: singularity lc builds
  filenames:
  - Singularity
  full_name: iapalm/lc-builds
  latest_release: null
  readme: '<p>This repository provides some boiler plate scripts for running ''pangeo''
    python ecosystem using singularity containers.</p>

    <p>Steps are:</p>

    <ol>

    <li>

    <p>Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a></p>

    <pre><code>docker pull pangeo/pangeo-notebook

    </code></pre>

    <p>The pangeo-notebook has a pretty diverse set of libraries for most cloud,

    dask, zarr, netCDF, analysis type tasks.</p>

    <ul>

    <li>

    <p>(Optional) Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a>

    If you need to customise, see minimal example in Dockerfile and requirements.txt
    and description here:</p>

    <ul>

    <li>

    <p>(Deprecated) <a href="https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants">https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants</a></p>

    </li>

    <li>

    <p>(<strong>Use this since 27-07-2020</strong>) <a href="https://github.com/pangeo-data/pangeo-docker-images">https://github.com/pangeo-data/pangeo-docker-images</a></p>

    <p>Then you would build a custom image along the lines of:</p>

    <pre><code>make pangeo-notebook

    </code></pre>

    </li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Convert docker image to singularity with a command such as:</p>

    <pre><code>singularity -d build pangeo-latest.sif docker-daemon://pangeo/pangeo-notebook:master

    </code></pre>

    </li>

    <li>

    <p>Copy the created <code>pangeo-latest.sif</code> singularity image to somewhere
    accessible on the HPC filesystem and edit the <code>container=</code> and <code>scheduler_file=</code>
    variables in the <code>start_jupyter.slurm</code> and <code>start_worker.slurm</code>
    scripts to point to the singularity image and the shared filesystem location to
    write the scheduler details, respectively.</p>

    </li>

    <li>

    <p>Start the jupyter lab and dask-scheduler, the first parameter is the working
    path you want to use for jupyter lab:</p>

    <pre><code>sbatch start_jupyter.slurm $MYGROUP

    </code></pre>

    <p>This starts a scheduler and jupyterlab with 2 cores each and 8GB/core memory.
    These can be edited in the #SBATCH headers, also note you can set the default
    directory for jupyterlab with the notebook_dir which is the parameter passed to
    start_jupyter.slurm.</p>

    </li>

    <li>

    <p>Start dask-workers (where n is the number of workers you want - these are configures
    for &lt; 2 hour wall time limit so that they use the <code>h2</code> queue):</p>

    <pre><code>sbatch -n 10 start_worker.slurm

    </code></pre>

    <p>also note that this input argument to dask-worker <code>--local-directory $LOCALDIR</code>
    tells the worker the path to local disk storage on the node which can be used
    for spilling data, but not all HPC nodes/centres have attached local storage.
    Currently this is disabled.</p>

    </li>

    <li>

    <p>See instruction printed to the slurm-######.out log file for connecting to
    the jupyter session running on the compute node, something like:</p>

    <pre><code>ssh -N -l pbranson -L 8888:compute-node123:8888 hpc-login.host.com

    </code></pre>

    <p>and take note of the randomly generated token printed to the slurm-######.out
    log file. You will need that to login to Jupyterlab.</p>

    </li>

    <li>

    <p>To connect to the dask-scheduler from a notebook use the following snippet:</p>

    <pre><code>import os

    from distributed import Client

    client=Client(scheduler_file=os.environ[''MYSCRATCH''] + ''/scheduler.json'')

    client

    </code></pre>

    </li>

    <li>

    <p>View the scheduler bokeh dashboard at <a href="http://localhost:8888/proxy/8787/status"
    rel="nofollow">http://localhost:8888/proxy/8787/status</a>. This can also be entered
    into the Jupyterlab dask widget as <code>/proxy/8787/status</code></p>

    </li>

    <li>

    <p>As a little cheat in jupyter lab I open up a terminal and then do</p>

    <pre><code>ssh localhost

    </code></pre>

    <p>to connect to the host running the jupyter container - this gives you access
    to the slurm job scheduler from that terminal and you can start workers  in there
    with:</p>

    <pre><code>sbatch start_worker.slurm

    </code></pre>

    <p>Also note that the dask worker specifications used in the <code>start_worker.slurm</code>
    script are based of the slurm environment variables, so you can alter the worker
    specification using the <code>#SBATCH</code> directives:</p>

    <pre><code>#SBATCH --ntasks=20

    #SBATCH --cpus-per-task=2

    #SBATCH --mem-per-cpu=10G

    #SBATCH --time=0:30:00

    </code></pre>

    <p>or at the command line when you submit the script:</p>

    <pre><code>sbatch -n 4 -c 4 --mem-per-cpu=16G start_worker.slurm

    </code></pre>

    <p>which would start 4 workers with 4 cores per worker and 16*4 = 64GB memory
    per dask-worker.</p>

    </li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1560984106.0
icaoberg/singularity-abyss:
  data_format: 2
  description: Singularity recipe for ABySS
  filenames:
  - 2.1.5/Singularity
  full_name: icaoberg/singularity-abyss
  latest_release: null
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-hyperfine\"\
    \ class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>singularity-hyperfine</h1>\n\
    <p><a href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/sharkdp/hyperfine\">hyperfine</a>.</p>\n<h2>\n\
    <a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ <code>hyperfine</code> script</li>\n</ul>\n<p>to <code>/opt/packages/hyperfine/1.11.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/hyperfine</code>\
    \ as <code>1.11.0</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1622730467.0
icaoberg/singularity-bat:
  data_format: 2
  description: Singularity recipe for bat
  filenames:
  - 0.17.1/Singularity
  full_name: icaoberg/singularity-bat
  latest_release: v0.17.1
  readme: "<h1>\n<a id=\"user-content-singularity-bat\" class=\"anchor\" href=\"#singularity-bat\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>singularity-bat</h1>\n<p><a href=\"https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/sharkdp/bat\">bat</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>bat</code> script</li>\n</ul>\n<p>to <code>/opt/packages/bat/0.17.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/bat</code>\
    \ as <code>0.17.1</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1622870361.0
icaoberg/singularity-chalk-cli:
  data_format: 2
  description: Singularity recipe for chalk-cli.
  filenames:
  - 4.1.0/Singularity
  full_name: icaoberg/singularity-chalk-cli
  latest_release: v4.1.0
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-chalk-cli/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-chalk-cli/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/41b71df381c58acd22a2f008355de6880684d6fae2cf7bf65fe0a838346e984e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/41b71df381c58acd22a2f008355de6880684d6fae2cf7bf65fe0a838346e984e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-chalk-cli\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/ec759ebf35710187fc88f31b68ceb6932b976079e159288cffbbad8dee77d527/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ec759ebf35710187fc88f31b68ceb6932b976079e159288cffbbad8dee77d527/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-chalk-cli\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/27518b30fab8201fa0ff8f34fb9beab4d4c1fa9ca921199f8cadb070e8236575/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/27518b30fab8201fa0ff8f34fb9beab4d4c1fa9ca921199f8cadb070e8236575/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-chalk-cli\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/818fb7a40695878fd336b1ae1e1a55e90b2b70ed4dd5acb0e69e51ed019535e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/818fb7a40695878fd336b1ae1e1a55e90b2b70ed4dd5acb0e69e51ed019535e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-chalk-cli\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-chalk-cli\"\
    \ class=\"anchor\" href=\"#singularity-chalk-cli\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>singularity-chalk-cli</h1>\n\
    <p>Singularity recipe for <a href=\"https://github.com/chalk/chalk-cli\">chalk-cli</a>.</p>\n\
    <h2>\n<a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\"\
    \ href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-or-similar\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges (or similar)</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>chalk-cli</code> script</li>\n</ul>\n<p>to <code>/opt/packages/chalk-cli/4.1.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/chalk-cli</code>\
    \ as <code>4.1.0.lua</code>.</p>\n<h3>\n<a id=\"user-content-example\" class=\"\
    anchor\" href=\"#example\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Example</h3>\n<pre><code>singularity exec singularity-chalk-cli-4.1.0.sif\
    \ chalk -t '{red.bold Dungeons and Dragons {~bold.blue (with added fairies)}}'\n\
    </code></pre>\n<p><a href=\"images/screenshot.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"images/screenshot.png\" alt=\"Screenshot\" style=\"\
    max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-alternative-installation\"\
    \ class=\"anchor\" href=\"#alternative-installation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Alternative Installation</h2>\n\
    <pre><code>spack install npm\nspack load npm\nnpm install -g chalk-cli\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - cli-utilities
  - utilities
  updated_at: 1624059802.0
icaoberg/singularity-dust:
  data_format: 2
  description: Singularity recipe for dust
  filenames:
  - 0.5.4/Singularity
  full_name: icaoberg/singularity-dust
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-dust\" class=\"anchor\" href=\"\
    #singularity-dust\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>singularity-dust</h1>\n<p><a href=\"https://github.com/bootandy/dust/raw/master/media/snap.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/bootandy/dust/raw/master/media/snap.png\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/bootandy/dust\">dust</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>dust</code> script</li>\n</ul>\n<p>to <code>/opt/packages/dust/0.5.4</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/dust</code>\
    \ as <code>0.5.4</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1622860644.0
icaoberg/singularity-graphviz:
  data_format: 2
  description: Singularity recipe for graphviz
  filenames:
  - 2.44.0/Singularity
  full_name: icaoberg/singularity-graphviz
  latest_release: v2.44.0
  readme: "<h1>\n<a id=\"user-content-singularity-graphviz\" class=\"anchor\" href=\"\
    #singularity-graphviz\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-graphviz</h1>\n<p><a href=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\"\
    \ alt=\"Logo\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/en/4/48/GraphvizLogo.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for <a href=\"https://graphviz.org/\"\
    \ rel=\"nofollow\">graphviz </a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>graphviz </code> script</li>\n</ul>\n<p>to <code>/opt/packages/graphviz/2.44.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/graphviz </code>\
    \ as <code> 2.44.0</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1622859227.0
icaoberg/singularity-hisat2:
  data_format: 2
  description: null
  filenames:
  - 2.2.1/Singularity
  full_name: icaoberg/singularity-hisat2
  latest_release: null
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-hisat\"\
    \ class=\"anchor\" href=\"#singularity-hisat\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-hisat</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://daehwankimlab.github.io/hisat2/\" rel=\"nofollow\"\
    >hisat2</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the scripts <code>hisat2*</code>\n</li>\n</ul>\n<p>to <code>/opt/packages/hisat2/2.2.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/hisat2</code>\
    \ as <code>2.2.1.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-singularity-definition-file\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-singularity-definition-file\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building the image using the Singularity definition file</h2>\n<h3>\n\
    <a id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>To build the image locally</h3>\n<p>Run the script <code>build.sh</code>\
    \ to build image locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<h3>\n\
    <a id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>To build the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code>\
    \ to build image remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n\
    <p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624060173.0
icaoberg/singularity-hyperfine:
  data_format: 2
  description: Singularity recipe for hyperfine
  filenames:
  - 1.11.0/Singularity
  full_name: icaoberg/singularity-hyperfine
  latest_release: v1.11.0
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-hyperfine\"\
    \ class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>singularity-hyperfine</h1>\n\
    <p><a href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/sharkdp/hyperfine\">hyperfine</a>.</p>\n<h2>\n\
    <a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ <code>hyperfine</code> script</li>\n</ul>\n<p>to <code>/opt/packages/hyperfine/1.11.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/hyperfine</code>\
    \ as <code>1.11.0</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1624059711.0
icaoberg/singularity-methylpy:
  data_format: 2
  description: Singularity recipe for methylpy
  filenames:
  - 1.4.3/Singularity
  full_name: icaoberg/singularity-methylpy
  latest_release: null
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-hyperfine\"\
    \ class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>singularity-hyperfine</h1>\n\
    <p><a href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/sharkdp/hyperfine\">hyperfine</a>.</p>\n<h2>\n\
    <a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ <code>hyperfine</code> script</li>\n</ul>\n<p>to <code>/opt/packages/hyperfine/1.11.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/hyperfine</code>\
    \ as <code>1.11.0</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1622730662.0
icaoberg/singularity-shellcheck:
  data_format: 2
  description: Singularity recipe for shellcheck
  filenames:
  - 0.5.0/Singularity
  full_name: icaoberg/singularity-shellcheck
  latest_release: null
  readme: "<p align=\"center\">\n  <a href=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\"\
    \ width=\"300\" align=\"left\" data-canonical-src=\"https://i.paste.pics/870189fadf668a958c8aac83f38e799c.png\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h1>\n<a id=\"user-content-pema\" class=\"\
    anchor\" href=\"#pema\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>PEMA:</h1>\n<h2>\n<a id=\"user-content-a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\"\
    \ class=\"anchor\" href=\"#a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>a flexible Pipeline for Environmental DNA Metabarcoding Analysis of\
    \ the 16S/18S rRNA, ITS and COI marker genes</h2>\n<p><em>PEMA is reposited in</em>\
    \ <a href=\"https://hub.docker.com/r/hariszaf/pema\" rel=\"nofollow\"><em>Docker\
    \ Hub</em></a> <em>as well as in</em> <a href=\"https://singularity-hub.org/collections/2295\"\
    \ rel=\"nofollow\"><em>Singularity Hub</em></a></p>\n<h4>\n<a id=\"user-content-a-pema-tutorial-can-be-found-here\"\
    \ class=\"anchor\" href=\"#a-pema-tutorial-can-be-found-here\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>A\
    \ PEMA tutorial can be found <a href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\"\
    \ rel=\"nofollow\"><strong>here</strong></a>.</h4>\n<h4>\n<a id=\"user-content-for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\"\
    \ class=\"anchor\" href=\"#for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>For any troubles you may have when running PEMA or for any potential\
    \ improvevments you would like to suggest, please share on the <a href=\"https://gitter.im/pema-helpdesk/community\"\
    \ rel=\"nofollow\">PEMA Gitter community</a>.</h4>\n\n<p><a href=\"https://gitter.im/pema-helpdesk/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7385c04b449351f12fb57a4bd6f9791ebd68a483493399e50a8f096fadde4246/68747470733a2f2f6261646765732e6769747465722e696d2f70656d612d68656c706465736b2f636f6d6d756e6974792e737667\"\
    \ alt=\"Gitter\" data-canonical-src=\"https://badges.gitter.im/pema-helpdesk/community.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.gnu.org/licenses/gpl-3.0\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/400c4e52df43f6a0ab8a89b74b1a78d1a64da56a7848b9110c9d2991bb7c3105/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-table-of-contents\"\
    \ class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Table of Contents</h1>\n<ul>\n\
    <li><a href=\"#pema-biodiversity-in-all-its-different-levels\">PEMA: biodiversity\
    \ in all its different levels</a></li>\n<li><a href=\"#a-container-based-tool\"\
    > A container-based tool</a></li>\n<li>\n<a href=\"#how-to-run-pema\">How to run\
    \ PEMA</a>\n<ul>\n<li><a href=\"#parameters-file\">Parameters' file</a></li>\n\
    </ul>\n</li>\n<li>\n<a href=\"#pema-on-hpc\">PEMA on HPC</a>\n<ul>\n<li><a href=\"\
    #prerequisites-1\">Prerequisites</a></li>\n<li><a href=\"#installing-1\">Installing</a></li>\n\
    <li>\n<a href=\"#running-pema-1\">Running PEMA</a>\n<ul>\n<li><a href=\"#example\"\
    >Example</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<a href=\"#pema-on-a-simple-pc\"\
    >PEMA on a simple PC</a>\n<ul>\n<li><a href=\"#prerequisites\">Prerequisites</a></li>\n\
    <li><a href=\"#installing\">Installing</a></li>\n<li>\n<a href=\"#running-pema\"\
    >Running PEMA</a>\n<ul>\n<li><a href=\"#step-1---build-a-docker-container\">Step\
    \ 1 - Build a Docker container</a></li>\n<li><a href=\"#step-2---run-pema\">Step\
    \ 2 - Run PEMA</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#the-phyloseq-r-package\"\
    >phyloseq - for a downstream ecological analysis</a></li>\n<li><a href=\"#acknowledgments\"\
    >Acknowledgments</a></li>\n<li><a href=\"#license\">License</a></li>\n<li><a href=\"\
    #citation\">Citation</a></li>\n</ul>\n<div class=\"highlight highlight-source-diff\"\
    ><pre><span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> convertion of the\
    \ Illumina raw data is now implemented in the framework of PEMA</span>\n<span\
    \ class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> PEMA now supports 2 extra marker\
    \ genes, 18S rRNA and ITS. </span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\"\
    >+</span> PEMA is now available for macOS!</span>\n<span class=\"pl-mi1\"><span\
    \ class=\"pl-mi1\">+</span> for anything feel free to contact me at: haris-zaf@hcmr.gr</span></pre></div>\n\
    \n<h1>\n<a id=\"user-content-pema-biodiversity-in-all-its-different-levels\" class=\"\
    anchor\" href=\"#pema-biodiversity-in-all-its-different-levels\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>PEMA:\
    \ biodiversity in all its different levels</h1>\n<p>PEMA supports the metabarcoding\
    \ analysis of four marker genes, <strong>16S rRNA</strong> (Bacteria), <strong>ITS</strong>\
    \ (Fungi) as well as <strong>COI</strong> and <strong>18S rRNA</strong> (metazoa).\
    \ As input, PEMA accepts .fastq.gz files as returned by Illumina sequencing platforms.</p>\n\
    <p>PEMA processes the reads from each sample and <strong>returns an OTU- or an\
    \ ASV-table with the taxonomies</strong> of the taxa found and their abundances\
    \ in each sample. It also returns statistics and a FASTQC diagram about the quality\
    \ of the reads for each sample. Finally, PEMA supports <strong>downstream ecological\
    \ analysis</strong> of the profiles retrieved, facilitated by the <a href=\"http://joey711.github.io/phyloseq/index.html\"\
    \ rel=\"nofollow\">phyloseq</a> R package.</p>\n<p>PEMA supports both OTU clustering\
    \ (thanks to VSEARCH and CROP algorithms) and ASV inference (via SWARM) for all\
    \ four marker genes.</p>\n<p>For the case of the 16S rRNA marker gene, PEMA includes\
    \ two separate approaches for taxonomy assignment: alignment-based and phylogenetic-based.\
    \ For the latter, a reference tree of 1000 taxa was created using SILVA_132_SSURef,\
    \ EPA-ng and RaxML-ng.</p>\n<p>PEMA has been implemented in <a href=\"https://pcingola.github.io/BigDataScript/\"\
    \ rel=\"nofollow\">BigDataScript</a> programming language. BDS\u2019s ad hoc task\
    \ parallelism and task synchronization, supports heavyweight computation. Thus,\
    \ PEMA inherits such features and it also supports roll-back checkpoints and on-demand\
    \ partial pipeline execution. In addition, PEMA takes advantage of all the computational\
    \ power available on a specific machine; for example, if PEMA is executed on a\
    \ personal laptop with 4 cores, it is going to use all four of them.</p>\n<p>Finally,\
    \ container-based technologies such as Docker and Singularity, make PEMA easy\
    \ accessible for all operating systems.\nAs you can see in the <a href=\"https://github.com/hariszaf/pema/blob/master/help_files/GitHub%20tutorial.pdf\"\
    >PEMA_tutorial.pdf</a>, once you have either Docker or Singularity on your computational\
    \ environment (see below which suits your case better), running PEMA is cakewalk.\
    \ You can also find the <a href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?usp=sharing\"\
    \ rel=\"nofollow\"><strong>PEMA tutorial</strong></a> as a Google Slides file.</p>\n\
    \n<h1>\n<a id=\"user-content-a-container-based-tool\" class=\"anchor\" href=\"\
    #a-container-based-tool\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>A container-based tool</h1>\n<p>PEMA can run\
    \ either on a HPC environment (server, cluster etc) or on a simple PC. However,\
    \ we definitely suggest to run it on an HPC environment to exploit the full potential\
    \ of PEMA. Running on a powerful server or a cluster can be time-saving since\
    \ it would require significantly less computational time than in a common PC.\
    \ However, for analyses with a small number of samples, a common PC can suffice.</p>\n\
    <p>There is one <strong>major difference</strong> between running PEMA on a common\
    \ PC than running it on a HPC environment. In the first case, PEMA runs through\
    \ <a href=\"https://www.docker.com/\" rel=\"nofollow\"><strong>Docker</strong></a>,\
    \ while in the latter one, it runs through <a href=\"https://sylabs.io/singularity/\"\
    \ rel=\"nofollow\"><strong>Singularity</strong></a>.</p>\n<p>On the following\
    \ chapters, you can find how to install PEMA both in Docker and Singlularity including\
    \ examples.</p>\n<p>Running PEMA is exactly <strong>the same</strong> procedure\
    \ in both of those cases.</p>\n\n<h2>\n<a id=\"user-content-how-to-run-pema\"\
    \ class=\"anchor\" href=\"#how-to-run-pema\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How to run PEMA</h2>\n<p>Assuming\
    \ you have either Docker or Singularity on your system (see below how to get them).\n\
    You need to create a directory where you will have everything PEMA needs - we\
    \ will call it <em><strong>analysis directory</strong></em>.</p>\n<p>In this directory,\
    \ you need to add the following <strong>mandatory</strong> files:</p>\n<ul>\n\
    <li>the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\
    ><em><strong>parameters.tsv</strong></em></a> file (you can download it from this\
    \ repository and then <strong>complete it</strong> according to the needs of your\
    \ analysis)</li>\n<li>a subdirectory called <em><strong>mydata</strong></em> where\
    \ your .fastq.gz files will be located <br>\n</li>\n</ul>\n<p>If your need to\
    \ perform phyloseq, in the analysis directory you also need to add the following\
    \ <strong>optionally</strong> files:</p>\n<ul>\n<li>the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\
    ><em><strong>phyloseq_in_PEMA.R</strong></em></a> which you can also download\
    \ from this repository and set it the way you want (that is an R script which\
    \ we have implemented and has some main features that need to stay always the\
    \ same in order to be executed as part of PEMA and some parts where the user can\
    \ set what exactly needs to get from the phyloseq package)</li>\n<li>the <a href=\"\
    https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\"\
    \ rel=\"nofollow\"><em><strong>metadata.csv</strong></em></a> file which has to\
    \ be in a <strong>comma separated</strong> format (you can find an example of\
    \ this file on PEMA's GitHub repository).</li>\n</ul>\n<h3>\n<a id=\"user-content-attention--\"\
    \ class=\"anchor\" href=\"#attention--\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><strong>Attention!</strong> \
    \ <br>\n</h3>\n<p>PEMA will <strong>fail</strong> unless you name the aforementioned\
    \ files and directories <strong>exactly</strong> as described above.\n<br></p>\n\
    <p>Here is an example of how your <em>analysis directory</em> should be in case\
    \ you do want a phyloseq analysis:</p>\n<pre><code>user@home-PC:~/Desktop/analysis_directory$\
    \ ls\nmydata  parameters.tsv  phyloseq_in_PEMA.R  metadata.csv\n</code></pre>\n\
    <p>and in case you do not:</p>\n<pre><code>user@home-PC:~/Desktop/analysis_directory$\
    \ ls\nmydata  parameters.tsv \n</code></pre>\n<p><a href=\"https://github.com/hariszaf/pema/tree/master/analysis_directory\"\
    ><strong>Here</strong></a> you can find an example of an <em>analysis directory</em>.</p>\n\
    <p>After you have prepared this <em>analysis directory</em> you are ready to run\
    \ PEMA (see below).</p>\n<p><strong>An extended list with PEMA's ouput can be\
    \ found <a href=\"https://github.com/hariszaf/pema/blob/master/help_files/PEMA's%20output%20files.md\"\
    ><strong>here</strong></a>.</strong></p>\n\n<h1>\n<a id=\"user-content-parameters-file\"\
    \ class=\"anchor\" href=\"#parameters-file\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Parameters' file</h1>\n<p>The\
    \ most crucial component in running PEMA is the parameters file. This file must\
    \ be located <strong>in</strong> the <em>analysis directory</em> and the user\
    \ needs to fill it <strong>every time</strong> PEMA is about to be called. If\
    \ you need more than one analyses to run, then you need to make copies of the\
    \ parameters' file and have one of those in eah of the analysis directrories you\
    \ create.</p>\n<p>So, here is the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\
    ><em><strong>parameters.tsv</strong></em></a> file as it looks like, in a study\
    \ case of our own.</p>\n\n<h1>\n<a id=\"user-content-pema-on-hpc\" class=\"anchor\"\
    \ href=\"#pema-on-hpc\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>PEMA on HPC</h1>\n<p>PEMA is best to run on\
    \ HPC (server, cluster, cloud). Usually environmental data are quite large and\
    \ the whole process has huge computational demands. To get PEMA running on your\
    \ HPC you (actually your system administrator) need to install Singularity as\
    \ described below.</p>\n<h2>\n<a id=\"user-content-prerequisites\" class=\"anchor\"\
    \ href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Prerequisites</h2>\n<p><strong><a href=\"https://www.sylabs.io/guides/3.0/user-guide/quick_start.html#quick-installation-steps\"\
    \ rel=\"nofollow\">Singularity</a></strong>  is a free, cross-platform and open-source\
    \ computer program that performs operating-system-level virtualization also known\
    \ as containerization. One of the main uses of Singularity is to bring containers\
    \ and reproducibility to scientific computing and the high-performance computing\
    \ (HPC) world.</p>\n<p>Singularity needs a Linux/Unix system to run.</p>\n<h2>\n\
    <a id=\"user-content-installing\" class=\"anchor\" href=\"#installing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing</h2>\n\
    <p>After you install Singularity in your environment and open it, you need to\
    \ download PEMA's image from Singularity Hub, by running the command:</p>\n<pre><code>\
    \ singularity pull shub://hariszaf/pema:v.1.1\n</code></pre>\n<p>Now you have\
    \ PEMA on your environment. But there is still one really <strong>important</strong>\
    \ thing that you need to do! Please <strong>download</strong> the <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\
    ><em>parameters.tsv</em></a> file and move it or copy it to the same directory\
    \ with your raw data.</p>\n<p>Now you are ready to go!</p>\n<h2>\n<a id=\"user-content-running-pema\"\
    \ class=\"anchor\" href=\"#running-pema\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running PEMA</h2>\n<p>Singularity\
    \ permits the use of a job scheduler that allocates computional resources on clusters\
    \ and at the same time, works as a queuing system, as <strong><a href=\"https://slurm.schedmd.com/overview.html\"\
    \ rel=\"nofollow\">Slurm</a></strong>. This way you are able to create a job as\
    \ you usually do in your system and after editing the parameters file as needed,\
    \ run PEMA as a job on your cluster.</p>\n<h3>\n<a id=\"user-content-example\"\
    \ class=\"anchor\" href=\"#example\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Example</h3>\n<pre><code>#SBATCH\
    \ --partition=batch\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20\n#SBATCH\
    \ --mem=\n# Memory per node specification is in MB. It is optional.\n# The default\
    \ limit is 3000MB per core.\n#SBATCH --job-name=\"testPema\"\n#SBATCH --output=PEMA.output\n\
    #SBATCH --mail-user=haris-zafr@hcmr.gr\n#SBATCH --mail-type=ALL\n#SBATCH --requeue\n\
    \n\nsingularity run -B /&lt;path&gt;/&lt;of&gt;/&lt;input&gt;/&lt;directory&gt;/:/mnt/analysis\
    \ /&lt;path&gt;/&lt;of&gt;/&lt;PEMA_container&gt;\n\n</code></pre>\n<p>In the\
    \ above example, we set the cluster \"Zorba\", to run PEMA in 1 node, with 20\
    \ cores.</p>\n<p>For further information, you can always check <a href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\"\
    \ rel=\"nofollow\">PEMA's tutorial</a>.</p>\n\n<h1>\n<a id=\"user-content-pema-on-a-simple-pc\"\
    \ class=\"anchor\" href=\"#pema-on-a-simple-pc\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>PEMA on a simple PC</h1>\n<h2>\n\
    <a id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h2>\n<p>To run PEMA in a simple PC on your own environment,\
    \ you first need to install <a href=\"https://docs.docker.com/install/\" rel=\"\
    nofollow\">Docker</a>, in case you do not already have it.</p>\n<p>You should\
    \ check your software version. A version of Docker is avalable for all Windows,\
    \ Mac and Linux. If you have Windows 10 Pro or your Mac's hardware in after 2010,\
    \ then you can insall Docker straightforward. Otherwise, you need to install the\
    \ <a href=\"https://docs.docker.com/toolbox/\" rel=\"nofollow\">Docker toolbox</a>\
    \ instead. You can check if your System Requirements are according to the ones\
    \ mentioned below in order to be sure what you need to do.</p>\n<p><strong>System\
    \ Requirements</strong></p>\n<pre><code>**__Windows 10 64bit__**:\nPro, Enterprise\
    \ or Education (1607 Anniversary Update, Build 14393 or later).\nVirtualization\
    \ is enabled in BIOS. Typically, virtualization is enabled by default.\nThis is\
    \ different from having Hyper-V enabled. For more detail see Virtualization must\
    \ be enabled in Troubleshooting.\nCPU SLAT-capable feature.\nAt least 4GB of RAM.\n\
    \n**__Mac__**\nMac hardware must be a 2010 or newer model, with Intel\u2019s hardware\
    \ support for memory management unit (MMU)\nvirtualization, including Extended\
    \ Page Tables (EPT) and Unrestricted Mode. You can check to see if your machine\n\
    has this support by running the following command in a terminal:\nsysctl kern.hv_support\
    \ macOS El Capitan 10.11 and newer macOS releases are supported.\nWe recommend\
    \ upgrading to the latest version of macOS.\nAt least 4GB of RAM\nVirtualBox prior\
    \ to version 4.3.30 must NOT be installed (it is incompatible with Docker for\
    \ Mac).\nIf you have a newer version of VirtualBox installed, it\u2019s fine.\n\
    </code></pre>\n<h2>\n<a id=\"user-content-installing-1\" class=\"anchor\" href=\"\
    #installing-1\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Installing</h2>\n<p>After you install Docker in your\
    \ environment and run it, the only thing you need to do, is to download PEMA's\
    \ image, by running the command:</p>\n<pre><code>docker pull hariszaf/pema\n</code></pre>\n\
    <p>The PEMA image file is a quite large (~3Gb), so it will take a while until\
    \ it is downloaded in your computer system.</p>\n<h2>\n<a id=\"user-content-running-pema-1\"\
    \ class=\"anchor\" href=\"#running-pema-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running PEMA</h2>\n<p>Running\
    \ PEMA has two discrete steps.</p>\n<h3>\n<a id=\"user-content-step-1---build-a-docker-container\"\
    \ class=\"anchor\" href=\"#step-1---build-a-docker-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 1 - Build a Docker container</h3>\n<p>At first, you need to let Docker have\
    \ access in your dataset. To provide access you need to run the following command\
    \ and specifying the path to where your data is stored, i.e. changing the &lt;path_to_analysis_directory&gt;\
    \ accordingly:</p>\n<pre><code>docker run -it -v /&lt;path_to_analysis_directory&gt;/:/mnt/analysis\
    \ hariszaf/pema\n</code></pre>\n<p>After you run the command above, you have now\
    \ built a Docker container, in which you can run PEMA!</p>\n<h3>\n<a id=\"user-content-step-2---run-pema\"\
    \ class=\"anchor\" href=\"#step-2---run-pema\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 2 - Run PEMA</h3>\n<p>Now,\
    \ being inside the PEMA container, the only thing remaining to do, is to run PEMA</p>\n\
    <pre><code>./PEMA_v1.bds\n</code></pre>\n<p>PEMA is now running. The runtime of\
    \ PEMA depends on the computational features of your environment, on the size\
    \ of your data, as well as the parameters you chose.</p>\n<p>Please, keep in mind\
    \ that when you need to copy a whole directory, then you always have to put \"\
    /\" in the end of the path that describes where the directory is located.</p>\n\
    <p>Finally, you will find the PEMA output in the analysis directory on your computer.\
    \ <br>\nAs the output directory is mounted into the built Docker container, you\
    \ can copy its contents wherever you want. However, in case you want to remove\
    \ it permanently, you need to do this as a sudo user.</p>\n\n<h1>\n<a id=\"user-content-the-phyloseq-r-package\"\
    \ class=\"anchor\" href=\"#the-phyloseq-r-package\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>The \"phyloseq\"\
    \ R package</h1>\n<p><strong>for a downstream ecological analysis of OTUs/ASVs\
    \ retrieved</strong></p>\n<p>PEMA performs all the basic functions of the \"phyloseq\"\
    \ R package. In addition, it performs certain functions of the <a href=\"https://cran.r-project.org/web/packages/vegan/index.html\"\
    \ rel=\"nofollow\"><em><strong>vegan</strong></em></a> R package.</p>\n<p>When\
    \ the user asks for a downstream analysis using the \"phyloseq\" R package, then\
    \ an extra input file called <a href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\
    ><em><strong>\"phyloseq_script.R\"</strong></em></a> needs to be imported in the\
    \ \"analysis_directory\". In PEMA's main repository, you can find a template of\
    \ this file; this file needs to be as it would run on your own computer, as you\
    \ would run <em>phyloseq</em> in any case. PEMA will create the <em>\"phyloseq\
    \ object\"</em> automatically and then it will perform the analysis as asked.\
    \ The output will be placed in an extra subfolder in the main output directory\
    \ of PEMA called <em>phyloseq_analysis</em>.</p>\n<p>In addition, the <em><strong>metadata.tsv</strong></em>\
    \ file is also required when the phyloseq option has been selected. An example\
    \ of this file you can find <a href=\"https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\"\
    \ rel=\"nofollow\">here</a>.</p>\n<h1>\n<a id=\"user-content-acknowledgments\"\
    \ class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgments</h1>\n<p>PEMA\
    \ uses a series of tools, datasets as well as Big Data Script language. We thank\
    \ all the groups that developed them.\nThe tools &amp; databases that PEMA uses\
    \ are:</p>\n<ul>\n<li>BigDataScript programming language - <a href=\"https://pcingola.github.io/BigDataScript/\"\
    \ rel=\"nofollow\">https://pcingola.github.io/BigDataScript/</a>\n</li>\n<li>FASTQC\
    \ - <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"\
    nofollow\">https://www.bioinformatics.babraham.ac.uk/projects/fastqc/</a>\n</li>\n\
    <li>\u03A4rimmomatic - <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\"\
    \ rel=\"nofollow\">http://www.usadellab.org/cms/?page=trimmomatic</a>\n</li>\n\
    <li>Cutadapt - <a href=\"https://cutadapt.readthedocs.io/en/stable/\" rel=\"nofollow\"\
    >https://cutadapt.readthedocs.io/en/stable/</a>\n</li>\n<li>BayesHammer - included\
    \ in SPAdes - <a href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\
    >http://cab.spbu.ru/software/spades/</a>\n</li>\n<li>PANDAseq - <a href=\"https://github.com/neufeld/pandaseq\"\
    >https://github.com/neufeld/pandaseq</a>\n</li>\n<li>OBITools - <a href=\"https://pythonhosted.org/OBITools/welcome.html\"\
    \ rel=\"nofollow\">https://pythonhosted.org/OBITools/welcome.html</a>\n</li>\n\
    <li>BLAST Command Line Applications - <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK52640/\"\
    \ rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/books/NBK52640/</a>\n</li>\n<li>VSEARCH-2.9.1\
    \ - <a href=\"https://github.com/torognes/vsearch/releases/tag/v2.9.1\">https://github.com/torognes/vsearch/releases/tag/v2.9.1</a>\n\
    </li>\n<li>SWARM - <a href=\"https://github.com/torognes/swarm\">https://github.com/torognes/swarm</a>\n\
    </li>\n<li>CROP - <a href=\"https://github.com/tingchenlab/CROP\">https://github.com/tingchenlab/CROP</a>\n\
    </li>\n<li>CREST - <a href=\"https://github.com/lanzen/CREST\">https://github.com/lanzen/CREST</a>\n\
    </li>\n<li>RDPClassifier - <a href=\"https://github.com/rdpstaff/classifier\"\
    >https://github.com/rdpstaff/classifier</a>\n(RPDtools are required in order to\
    \ execute RDPClassifier)</li>\n<li>SILVA db - <a href=\"https://www.arb-silva.de/no_cache/download/archive/current/Exports/\"\
    \ rel=\"nofollow\">https://www.arb-silva.de/no_cache/download/archive/current/Exports/</a>\n\
    </li>\n<li>MIDORI db - <a href=\"http://reference-midori.info/index.html\" rel=\"\
    nofollow\">http://reference-midori.info/index.html</a>\n</li>\n<li>\"phat\" algorithm,\
    \ from the \"gappa\" package - <a href=\"https://github.com/lczech/gappa/wiki/Subcommand:-phat\"\
    >https://github.com/lczech/gappa/wiki/Subcommand:-phat</a>\n</li>\n<li>MAFFT -\
    \ <a href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\">https://mafft.cbrc.jp/alignment/software/</a>\n\
    </li>\n<li>RAxML -ng - <a href=\"https://github.com/amkozlov/raxml-ng\">https://github.com/amkozlov/raxml-ng</a>\n\
    </li>\n<li>PaPaRa - <a href=\"https://cme.h-its.org/exelixis/web/software/papara/index.html\"\
    \ rel=\"nofollow\">https://cme.h-its.org/exelixis/web/software/papara/index.html</a>\n\
    </li>\n<li>EPA-ng - <a href=\"https://github.com/Pbdas/epa-ng\">https://github.com/Pbdas/epa-ng</a>\n\
    </li>\n<li>phyloseq R package - <a href=\"http://joey711.github.io/phyloseq/index.html\"\
    \ rel=\"nofollow\">http://joey711.github.io/phyloseq/index.html</a>\n</li>\n<li>vegan\
    \ R package - <a href=\"https://cran.r-project.org/web/packages/vegan/index.html\"\
    \ rel=\"nofollow\">https://cran.r-project.org/web/packages/vegan/index.html</a>\n\
    </li>\n</ul>\n<p>And of course the container-based technologies:</p>\n<ul>\n<li>Docker\
    \ - <a href=\"https://www.docker.com/\" rel=\"nofollow\">https://www.docker.com/</a>\n\
    </li>\n<li>Singularity - <a href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\
    >https://sylabs.io/singularity/</a>\n</li>\n</ul>\n<h1>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h1>\n<p>PEMA is under\
    \ the GNU GPLv3 license (for 3rd party components separate licenses apply).</p>\n\
    <h1>\n<a id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citation</h1>\n\
    <p>Haris Zafeiropoulos, Ha Quoc Viet, Katerina Vasileiadou, Antonis Potirakis,\
    \ Christos Arvanitidis, Pantelis Topalis, Christina Pavloudi, Evangelos Pafilis,\
    \ PEMA: a flexible Pipeline for Environmental DNA Metabarcoding Analysis of the\
    \ 16S/18S ribosomal RNA, ITS, and COI marker genes, GigaScience, Volume 9, Issue\
    \ 3, March 2020, giaa022, <a href=\"https://doi.org/10.1093/gigascience/giaa022\"\
    \ rel=\"nofollow\">https://doi.org/10.1093/gigascience/giaa022</a></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1622859451.0
icaoberg/singularity-term-img-cli:
  data_format: 2
  description: Singularity recipe for singularity-term-img-cli
  filenames:
  - 4.1.0/Singularity
  full_name: icaoberg/singularity-term-img-cli
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-term-img-cli\" class=\"anchor\"\
    \ href=\"#singularity-term-img-cli\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-term-img-cli</h1>\n\
    <p><a href=\"https://github.com/sindresorhus/term-img-cli/raw/main/screenshot.jpg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/sindresorhus/term-img-cli/raw/main/screenshot.jpg\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/sindresorhus/term-img-cli\">term-img</a>.</p>\n\
    <h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ <code>term-img</code> script</li>\n</ul>\n<p>to <code>/opt/packages/term-img/3.0.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/term-img</code>\
    \ as <code>3.0.0</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1622923859.0
ifurther/flair-def:
  data_format: 2
  description: singularity def file for flair(fluka)
  filenames:
  - flair-cern.def
  - flair.def
  full_name: ifurther/flair-def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619686613.0
intel/HPC-containers-from-Intel:
  data_format: 2
  description: Intel HPC Containers using Singularity
  filenames:
  - definitionFiles/namd/namdRun.def
  - definitionFiles/namd/namdBuild.def
  - definitionFiles/lammps/lammpsBuild.def
  - definitionFiles/lammps/lammpsRun.def
  - definitionFiles/gromacs/gromacsBuild.def
  - definitionFiles/gromacs/gromacsRun.def
  - definitionFiles/WRF/wrfBuild.def
  - definitionFiles/WRF/wrfRun.def
  - definitionFiles/base/base.def
  full_name: intel/HPC-containers-from-Intel
  latest_release: null
  readme: '<h1>

    <a id="user-content-goal" class="anchor" href="#goal" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Goal:</h1>

    <p>Create containers using Singularity definition file for HPC apps and run them
    on the cloud or bare metal for Single and Cluster runs.</p>

    <p>This repo should have definition files only for few HPC applications. Users
    can utilize them to generate containers.</p>

    <h2>

    <a id="user-content-get-help" class="anchor" href="#get-help" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Get Help</h2>

    <ul>

    <li>

    <a href="https://github.com/intel/HPC-containers-from-Intel/issues">Post an issue</a>
    if you face any problem building or running a container</li>

    </ul>

    '
  stargazers_count: 16
  subscribers_count: 9
  topics:
  - hpc
  - cluster
  - singularity-containers
  - cloud
  updated_at: 1619711561.0
j-andrews7/singularity-rstudio:
  data_format: 2
  description: A Singularity image definition file built on top of the Ubuntu 20.04
    docker image with R (4.1), RStudio Server (1.4.1717), and additional linux dependencies
    for common R packages installed.
  filenames:
  - Singularity
  full_name: j-andrews7/singularity-rstudio
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rstudio-server\" class=\"anchor\"\
    \ href=\"#singularity-rstudio-server\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity RStudio Server</h1>\n\
    <p>This repo contains a Singularity file that contains R 4.1 and RStudio 1.4.1717.\
    \ It has several additional linux dependencies installed that are required for\
    \ common bioinformatics packages (openssl, libproj, libbz2, etc). If you have\
    \ others you'd like added, feel free to open a PR (or make your own fork and add\
    \ whatever you need).</p>\n<p>The Singularity image for this can be pulled via\
    \ <code>singularity pull library://j-andrews7/default/rstudio:4.1.0</code>.</p>\n\
    <p>This was mostly configured to run on HPCs in interactive jobs where users likely\
    \ don't have the appropriate permissions for RStudio server to work properly.\
    \ This requires a number of bindings to be made to the image and a secure cookie\
    \ file to be provided. The cookie file can be produced with:</p>\n<pre><code>#\
    \ Only needs to be run once.\nmkdir -p \"$HOME/rstudio-tmp/tmp/rstudio-server\"\
    \nuuidgen &gt; \"$HOME/rstudio-tmp/tmp/rstudio-server/secure-cookie-key\"\nchmod\
    \ 0600 \"$HOME/rstudio-tmp/tmp/rstudio-server/secure-cookie-key\"\n</code></pre>\n\
    <p>In general, you can launch a script similar to the following from within an\
    \ interactive job on your respective HPC to get it running, and it will print\
    \ the IP address and port the server is running on that you can pop into your\
    \ browser:</p>\n<pre><code>#!/bin/sh\n\nworkdir=${HOME}/rstudio-tmp\n\nmkdir -p\
    \ -m 700 ${workdir}/run ${workdir}/tmp ${workdir}/var/lib/rstudio-server \ncat\
    \ &gt; ${workdir}/database.conf &lt;&lt;END\nprovider=sqlite\ndirectory=/var/lib/rstudio-server\n\
    END\n\n# Set R_LIBS_USER to a path specific to rocker/rstudio to avoid conflicts\
    \ with\n# personal libraries from any R installation in the host environment\n\
    cat &gt; ${workdir}/rsession.sh &lt;&lt;END\n#!/bin/sh\nexport R_LIBS_USER=${HOME}/R/rstudio/4.1\n\
    exec rsession \"\\${@}\"\nEND\n\nchmod +x ${workdir}/rsession.sh\n\nexport SINGULARITY_BIND=\"\
    ${workdir}/run:/run,${workdir}/tmp:/tmp,${workdir}/database.conf:/etc/rstudio/database.conf,${workdir}/rsession.sh:/etc/rstudio/rsession.sh,${workdir}/var/lib/rstudio-server:/var/lib/rstudio-server\"\
    \n\n# Do not suspend idle sessions.\n# Alternative to setting session-timeout-minutes=0\
    \ in /etc/rstudio/rsession.conf\n# https://github.com/rstudio/rstudio/blob/v1.4.1106/src/cpp/server/ServerSessionManager.cpp#L126\n\
    export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT=0\n\n# Get unused socket per https://unix.stackexchange.com/a/132524\n\
    # Tiny race condition between the python &amp; singularity commands\nreadonly\
    \ PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]);\
    \ s.close()')\n# Get node IP address.\nreadonly ADD=$(nslookup `hostname` | grep\
    \ -i address | awk -F\" \" '{print $2}' | awk -F# '{print $1}' | tail -n 1)\n\n\
    cat 1&gt;&amp;2 &lt;&lt;END\n\"Running RStudio at $ADD:$PORT\"\nEND\n\nsingularity\
    \ exec --cleanenv rstudio_4.1.0.sif \\\n    rserver --www-port ${PORT} \\\n  \
    \          --rsession-path=/etc/rstudio/rsession.sh\n            --secure-cookie-key-file\
    \ ${workdir}/tmp/rstudio-server/secure-cookie-key\n</code></pre>\n<h2>\n<a id=\"\
    user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>This repo is distributed under the GNU-GPL3 license. See the LICENSE file for\
    \ more details.</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1624628445.0
jendrikseipp/scorpion:
  data_format: 2
  description: Optimal classical planner based on saturated cost partitioning
  filenames:
  - misc/releases/19.12/Singularity.19.12
  - misc/releases/20.06/Singularity.20.06
  - misc/releases/19.06/Singularity.19.06
  - misc/releases/latest/Singularity
  full_name: jendrikseipp/scorpion
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-scorpion\" class=\"anchor\" href=\"#scorpion\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Scorpion</h1>\n<p>Scorpion is an optimal classical planner that uses\
    \ saturated cost\npartitioning to combine multiple abstraction heuristics. It\
    \ also contains\nimplementations of many other cost partitioning algorithms over\n\
    abstraction and landmark heuristics. Scorpion is based on the <a href=\"https://github.com/aibasel/downward\"\
    >Fast\nDownward planning system</a>, which is\ndescribed below. We regularly port\
    \ the latest changes from Fast Downward\nto Scorpion and also try to port Scorpion\
    \ features back to Fast Downward.</p>\n<p>Please use the following reference when\
    \ citing Scorpion:\nJendrik Seipp, Thomas Keller and Malte Helmert.\n<a href=\"\
    https://www.jair.org/index.php/jair/article/view/11673\" rel=\"nofollow\">Saturated\
    \ Cost Partitioning for Optimal Classical Planning</a>.\nJournal of Artificial\
    \ Intelligence Research 67, pp. 129-167. 2020.</p>\n<h2>\n<a id=\"user-content-instructions\"\
    \ class=\"anchor\" href=\"#instructions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Instructions</h2>\n<p>After installing\
    \ the requirements (see below), compile the planner with</p>\n<pre><code>./build.py\n\
    </code></pre>\n<p>and see the available options with</p>\n<pre><code>./fast-downward.py\
    \ --help  # driver\n./fast-downward.py --search -- --help  # search component\n\
    </code></pre>\n<p>For more details (including build instructions for Windows),\
    \ see the\ndocumentation about\n<a href=\"http://www.fast-downward.org/ObtainingAndRunningFastDownward\"\
    \ rel=\"nofollow\">compiling</a>\nand <a href=\"http://www.fast-downward.org/PlannerUsage\"\
    \ rel=\"nofollow\">running</a> the planner.</p>\n<h3>\n<a id=\"user-content-recommended-configuration\"\
    \ class=\"anchor\" href=\"#recommended-configuration\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Recommended Configuration</h3>\n\
    <p>We recommend the following configuration, which is similar to the one\nScorpion\
    \ used in the IPC 2018. It prunes irrelevant operators in a\npreprocessing step,\
    \ uses partial order reduction, and maximizes over\nmultiple diverse SCP heuristics\
    \ computed for projections and Cartesian\nabstractions:</p>\n<pre><code>./fast-downward.py\
    \ --transform-task preprocess-h2\n  ../benchmarks/gripper/prob01.pddl\n  --search\
    \ \"astar(scp([\n    projections(hillclimbing(max_time=100, random_seed=0)),\n\
    \    projections(systematic(2)),\n    cartesian()],\n    max_orders=infinity,\
    \ max_time=200, max_optimization_time=2, diversify=true,\n    orders=greedy_orders(random_seed=0),\
    \ random_seed=0),\n    pruning=atom_centric_stubborn_sets(min_required_pruning_ratio=0.2))\"\
    \n</code></pre>\n<p>(In <a href=\"https://lab.readthedocs.io/\" rel=\"nofollow\"\
    >Downward Lab</a> you can use the\n<code>driver_options</code> argument of <code>add_algorithm</code>\
    \ to specify the\n<code>--transform-task</code> argument.)</p>\n<p>If you want\
    \ to run exactly the same Scorpion version as in IPC 2018, we\nrecommend using\
    \ the <a href=\"https://bitbucket.org/ipc2018-classical/team44/src/ipc-2018-seq-opt/\"\
    \ rel=\"nofollow\">Scorpion IPC\nrepo</a>.\nIt also includes a <a href=\"https://github.com/hpcng/singularity\"\
    >Singularity</a>\nimage.</p>\n<h2>\n<a id=\"user-content-differences-between-scorpion-and-fast-downward\"\
    \ class=\"anchor\" href=\"#differences-between-scorpion-and-fast-downward\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Differences\
    \ between Scorpion and Fast Downward</h2>\n<ul>\n<li>Scorpion comes with the\n\
    <a href=\"https://ojs.aaai.org/index.php/ICAPS/article/view/13708\" rel=\"nofollow\"\
    >h\xB2-preprocessor</a>\nby Vidal Alc\xE1zar and \xC1lvaro Torralba that prunes\
    \ irrelevant operators.\nPass <code>--transform-task builds/release/bin/preprocess-h2</code>\
    \ to use it.</li>\n<li>The <code>--transform-task</code> command allows you to\
    \ run arbitrary preprocessing\ncommands that transform the SAS+ output from the\
    \ translator before\npassing it to the search.</li>\n<li>If <a href=\"https://ccache.dev/\"\
    \ rel=\"nofollow\">ccache</a> is installed, Scorpion uses it to cache\ncompilation\
    \ files.</li>\n</ul>\n<h3>\n<a id=\"user-content-new-plugin-options\" class=\"\
    anchor\" href=\"#new-plugin-options\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>New plugin options</h3>\n<ul>\n\
    <li>\n<p><code>cegar(..., search_strategy=incremental)</code>: use <a href=\"\
    https://ojs.aaai.org/index.php/ICAPS/article/view/6667\" rel=\"nofollow\">incremental\
    \ search for\nCartesian abstraction\nrefinement</a>\n(default).</p>\n</li>\n<li>\n\
    <p><code>hillclimbing(..., max_generated_patterns=200)</code>: limit the number\
    \ of\npatterns generated by hill climbing.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-new-cost-partitioning-algorithms-for-abstraction-heuristics\"\
    \ class=\"anchor\" href=\"#new-cost-partitioning-algorithms-for-abstraction-heuristics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>New cost partitioning algorithms for abstraction heuristics</h3>\n\
    <p>We use Cartesian abstractions in the example configurations below\n(<code>[cartesian()]</code>).\
    \ You can also use pattern database heuristics, e.g.,\n<code>[projections(systematic(2))]</code>,\
    \ or mix abstractions, e.g.,\n<code>[projections(systematic(3)), cartesian()]</code>.\
    \ Some of the algorithms are\nalso part of vanilla Fast Downward, but only for\
    \ PDB heuristics.</p>\n<ul>\n<li>Optimal cost partitioning:\n<code>optimal_cost_partitioning([cartesian()])</code>\n\
    </li>\n<li>Canonical heuristic:\n<code>canonical_heuristic([cartesian()])</code>\n\
    </li>\n<li>Post-hoc optimization:\n<code>operatorcounting([pho_abstraction_constraints([cartesian()],\
    \ saturated=false)])</code>\n</li>\n<li>Uniform cost partitioning:\n<code>uniform_cost_partitioning([cartesian()],\
    \ opportunistic=false)</code>\n</li>\n<li>Opportunistic uniform cost partitioning:\n\
    <code>uniform_cost_partitioning([cartesian()], ..., opportunistic=true)</code>\n\
    </li>\n<li>Greedy zero-one cost partitioning:\n<code>zero_one_cost_partitioning([cartesian()],\
    \ ...)</code>\n</li>\n<li>Saturated post-hoc optimization:\n<code>operatorcounting([pho_abstraction_constraints([cartesian()],\
    \ saturated=true)])</code>\n</li>\n</ul>\n<p>You can also compute the maximum\
    \ over abstraction heuristics:</p>\n<ul>\n<li><code>maximize([cartesian()])</code></li>\n\
    </ul>\n<h3>\n<a id=\"user-content-new-cost-partitioning-algorithms-for-landmark-heuristics\"\
    \ class=\"anchor\" href=\"#new-cost-partitioning-algorithms-for-landmark-heuristics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>New cost partitioning algorithms for landmark heuristics</h3>\n<p>Example\
    \ using A* search and saturated cost partitioning over BJOLP\nlandmarks:</p>\n\
    <pre><code>--evaluator\n  \"lmc=lmcount(lm_merged([lm_rhw(), lm_hm(m=1)]),\n \
    \ admissible=true, cost_partitioning=suboptimal, greedy=true,\n  reuse_costs=true,\
    \ scoring_function=max_heuristic_per_stolen_costs)\"\n--search\n  \"astar(lmc,\
    \ lazy_evaluator=lmc)\"\n</code></pre>\n<p>Different cost partitioning algorithms\
    \ (all need <code>admissible=true</code>):</p>\n<ul>\n<li>Optimal cost partitioning\
    \ (part of vanilla Fast Downward):\n<code>lmcount(..., cost_partitioning=optimal)</code>\n\
    </li>\n<li>Canonical heuristic:\n<code>lmcount(..., cost_partitioning=canonical)</code>\n\
    </li>\n<li>Post-hoc optimization:\n<code>lmcount(..., cost_partitioning=pho)</code>\n\
    </li>\n<li>Uniform cost partitioning:\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=false, reuse_costs=false)</code>\n</li>\n<li>Opportunistic uniform cost\
    \ partitioning (part of vanilla Fast Downward):\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=false, reuse_costs=true, scoring_function=min_stolen_costs)</code>\n\
    </li>\n<li>Greedy zero-one cost partitioning:\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=true, reuse_costs=false, scoring_function=max_heuristic)</code>\n</li>\n\
    <li>Saturated cost partitioning:\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=true, reuse_costs=true, scoring_function=max_heuristic_per_stolen_costs)</code>\n\
    </li>\n</ul>\n<h1>\n<a id=\"user-content-fast-downward\" class=\"anchor\" href=\"\
    #fast-downward\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Fast Downward</h1>\n<p>Fast Downward is a domain-independent\
    \ classical planning system.</p>\n<p>Copyright 2003-2020 Fast Downward contributors\
    \ (see below).</p>\n<p>For further information:</p>\n<ul>\n<li>Fast Downward website:\
    \ <a href=\"http://www.fast-downward.org\" rel=\"nofollow\">http://www.fast-downward.org</a>\n\
    </li>\n<li>Report a bug or file an issue: <a href=\"http://issues.fast-downward.org\"\
    \ rel=\"nofollow\">http://issues.fast-downward.org</a>\n</li>\n<li>Fast Downward\
    \ mailing list: <a href=\"https://groups.google.com/forum/#!forum/fast-downward\"\
    \ rel=\"nofollow\">https://groups.google.com/forum/#!forum/fast-downward</a>\n\
    </li>\n<li>Fast Downward main repository: <a href=\"https://github.com/aibasel/downward\"\
    >https://github.com/aibasel/downward</a>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-tested-software-versions\"\
    \ class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Tested software\
    \ versions</h2>\n<p>This version of Fast Downward has been tested with the following\
    \ software versions:</p>\n<table>\n<thead>\n<tr>\n<th>OS</th>\n<th>Python</th>\n\
    <th>C++ compiler</th>\n<th>CMake</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Ubuntu\
    \ 20.04</td>\n<td>3.8</td>\n<td>GCC 9, GCC 10, Clang 10, Clang 11</td>\n<td>3.16</td>\n\
    </tr>\n<tr>\n<td>Ubuntu 18.04</td>\n<td>3.6</td>\n<td>GCC 7, Clang 6</td>\n<td>3.10</td>\n\
    </tr>\n<tr>\n<td>macOS 10.15</td>\n<td>3.6</td>\n<td>AppleClang 12</td>\n<td>3.19</td>\n\
    </tr>\n<tr>\n<td>Windows 10</td>\n<td>3.6</td>\n<td>Visual Studio Enterprise 2017\
    \ (MSVC 19.16) and 2019 (MSVC 19.28)</td>\n<td>3.19</td>\n</tr>\n</tbody>\n</table>\n\
    <p>We test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu,\
    \ we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and\
    \ on macOS, we do not test LP solvers (yet).</p>\n<h2>\n<a id=\"user-content-contributors\"\
    \ class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors</h2>\n<p>The following\
    \ list includes all people that actively contributed to\nFast Downward, i.e. all\
    \ people that appear in some commits in Fast\nDownward's history (see below for\
    \ a history on how Fast Downward\nemerged) or people that influenced the development\
    \ of such commits.\nCurrently, this list is sorted by the last year the person\
    \ has been\nactive, and in case of ties, by the earliest year the person started\n\
    contributing, and finally by last name.</p>\n<ul>\n<li>2003-2020 Malte Helmert</li>\n\
    <li>2008-2016, 2018-2020 Gabriele Roeger</li>\n<li>2010-2020 Jendrik Seipp</li>\n\
    <li>2010-2011, 2013-2020 Silvan Sievers</li>\n<li>2012-2020 Florian Pommerening</li>\n\
    <li>2013, 2015-2020 Salome Eriksson</li>\n<li>2016-2020 Cedric Geissmann</li>\n\
    <li>2017-2020 Guillem Franc\xE8s</li>\n<li>2018-2020 Augusto B. Corr\xEAa</li>\n\
    <li>2018-2020 Patrick Ferber</li>\n<li>2015-2019 Manuel Heusner</li>\n<li>2017\
    \ Daniel Killenberger</li>\n<li>2016 Yusra Alkhazraji</li>\n<li>2016 Martin Wehrle</li>\n\
    <li>2014-2015 Patrick von Reth</li>\n<li>2015 Thomas Keller</li>\n<li>2009-2014\
    \ Erez Karpas</li>\n<li>2014 Robert P. Goldman</li>\n<li>2010-2012 Andrew Coles</li>\n\
    <li>2010, 2012 Patrik Haslum</li>\n<li>2003-2011 Silvia Richter</li>\n<li>2009-2011\
    \ Emil Keyder</li>\n<li>2010-2011 Moritz Gronbach</li>\n<li>2010-2011 Manuela\
    \ Ortlieb</li>\n<li>2011 Vidal Alc\xE1zar Saiz</li>\n<li>2011 Michael Katz</li>\n\
    <li>2011 Raz Nissim</li>\n<li>2010 Moritz Goebelbecker</li>\n<li>2007-2009 Matthias\
    \ Westphal</li>\n<li>2009 Christian Muise</li>\n</ul>\n<h2>\n<a id=\"user-content-history\"\
    \ class=\"anchor\" href=\"#history\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>History</h2>\n<p>The current\
    \ version of Fast Downward is the merger of three different\nprojects:</p>\n<ul>\n\
    <li>the original version of Fast Downward developed by Malte Helmert\nand Silvia\
    \ Richter</li>\n<li>LAMA, developed by Silvia Richter and Matthias Westphal based\
    \ on\nthe original Fast Downward</li>\n<li>FD-Tech, a modified version of Fast\
    \ Downward developed by Erez\nKarpas and Michael Katz based on the original code</li>\n\
    </ul>\n<p>In addition to these three main sources, the codebase incorporates\n\
    code and features from numerous branches of the Fast Downward codebase\ndeveloped\
    \ for various research papers. The main contributors to these\nbranches are Malte\
    \ Helmert, Gabi R\xF6ger and Silvia Richter.</p>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>The following\
    \ directory is not part of Fast Downward as covered by\nthis license:</p>\n<ul>\n\
    <li>./src/search/ext</li>\n</ul>\n<p>For the rest, the following license applies:</p>\n\
    <pre><code>Fast Downward is free software: you can redistribute it and/or modify\n\
    it under the terms of the GNU General Public License as published by\nthe Free\
    \ Software Foundation, either version 3 of the License, or (at\nyour option) any\
    \ later version.\n\nFast Downward is distributed in the hope that it will be useful,\
    \ but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY\
    \ or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for\
    \ more details.\n\nYou should have received a copy of the GNU General Public License\n\
    along with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;.\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621274654.0
jengelmann/FastqPuri:
  data_format: 2
  description: fastq quality assessment and filtering tool
  filenames:
  - Singularity
  - Singularity-Test
  full_name: jengelmann/FastqPuri
  latest_release: v1.0.6
  readme: "<h1>\n<a id=\"user-content-fastqpuri-an-fq-quality-control-and-filter-tool\"\
    \ class=\"anchor\" href=\"#fastqpuri-an-fq-quality-control-and-filter-tool\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>FastqPuri,\
    \ an fq quality control and filter tool</h1>\n<p>Software and source code of <code>FastqPuri</code>.\
    \ It creates quality reports of\n<code>fastq</code> files and filters them removing\
    \ low quality reads, reads\ncontaining too many N's or contamination reads (unwanted\
    \ rRNA reads,\nimpurities coming from another organism, ...).</p>\n<h2>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p>Clone the repository, or download the source. Make sure that\nyour system supplies\
    \ the following dependencies for FastqPuri.</p>\n<ul>\n<li>OS: Linux (clang, gcc),\
    \ Mac OS (clang, gcc), OpenBSD (clang)</li>\n<li>\n<code>cmake</code> (at least\
    \ version 2.8),</li>\n<li>a <code>C</code> compiler supporting the <code>c11</code>\
    \ standard\n(change the compiler flags otherwise),</li>\n<li>pandoc (optional,\
    \ see documentation in <code>PANDOC.md</code>),</li>\n<li>\n<code>Rscript</code>\
    \ (optional),</li>\n<li>Following <code>R</code> packages installed (optional):\n\
    <ul>\n<li><code>pheatmap</code></li>\n<li><code>knitr</code></li>\n<li><code>rmarkdown</code></li>\n\
    </ul>\n</li>\n</ul>\n<p><strong>NOTE:</strong>  FastqPuri will work without the\
    \ optional dependencies\nbut will skip creating html reports if they are not available.</p>\n\
    <pre><code>$ cmake -H. -Bbuild/ [-DRSCRIPT=/path/to/my/R/bin/Rscript] [-DCMAKE_INSTALL_PREFIX=/path/to/my/root]\
    \ ... \n$ cd build \n$ make \n$ sudo make install  \n</code></pre>\n<p>When running\
    \ <code>cmake</code>, there are some variables you can set\nusing the option -D\
    \ followed by the variable name. These variables are:</p>\n<ul>\n<li>\n<code>CMAKE_C_COMPILER</code>:\
    \ <code>C</code> compiler (default <code>gcc</code>)</li>\n<li>\n<code>CMAKE_C_FLAGS</code>:\
    \ compiler flags (default <code>-Wall -O3 -march=native -std=c11</code>).</li>\n\
    <li>\n<code>CMAKE_INSTALL_PREFIX</code>: root path for <code>make install</code>,\
    \ e.g. to\nredirect to a directory with user access (default /usr/local),</li>\n\
    <li>\n<code>PANDOC</code>: <code>pandoc</code> executable (default <code>pandoc</code>),</li>\n\
    <li>\n<code>RSCRIPT</code>: <code>Rscript</code> executable (default <code>Rscript</code>),</li>\n\
    <li>\n<code>READ_MAXLEN</code>: Maximum Illumina read length</li>\n<li>(default\
    \ 400),</li>\n</ul>\n<p>The executables will be created in the folder <code>bin</code>\
    \ and installed in <code>/usr/local/bin</code>.\n<code>R</code> scripts will be\
    \ installed in <code>/usr/local/share/FastqPuri/R</code>.</p>\n<p><strong>WARNING:</strong>\
    \ do not move the executables that depend on <code>R</code> scripts,\nanywhere\
    \ else, unless you also move the corresponding <code>R</code> scripts respecting\n\
    the local folder structure.</p>\n<h2>\n<a id=\"user-content-executables\" class=\"\
    anchor\" href=\"#executables\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Executables</h2>\n<ul>\n<li>\n<code>Qreport</code>:\
    \ creates a quality report in html format (see <code>README_Qreport.md</code>),</li>\n\
    <li>\n<code>Sreport</code>: creates a summary report in html format on a set of\
    \ samples,\nregarding either the original files or the filtering process\n(see\
    \ <code>README_Sreport.md</code>),</li>\n<li>\n<code>makeBloom</code>: creates\
    \ a  bloom filter from a fasta file of a certain size,\nand stores it in a file\
    \ (see <code>README_makeBloom.md</code>)</li>\n<li>\n<code>makeTree</code>: creates\
    \ a tree of a certain depth from a fasta file and stores\nit in a file (see <code>README_makeTree.md</code>),</li>\n\
    <li>\n<code>trimFilter</code>: performs the filtering process for single-end data\n\
    (see <code>README_trimFilter.md</code>).</li>\n<li>\n<code>trimFilterPE</code>:\
    \ performs the filtering process for double stranded data\n(see <code>README_trimFilterPE.md</code>).</li>\n\
    </ul>\n<p>An exemplar work flow could be:</p>\n<ul>\n<li><code>Qreport</code></li>\n\
    <li><code>Sreport</code></li>\n<li><code>makeBloom</code></li>\n<li>\n<code>trimFilter</code>\
    \ or <code>trimFilterPE</code>\n</li>\n<li><code>Qreport</code></li>\n<li><code>Sreport</code></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-documentation-of-the-code\" class=\"anchor\"\
    \ href=\"#documentation-of-the-code\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation of the code</h2>\n\
    <p>A Doxygen documentation of the code is available:</p>\n<ul>\n<li>\n<code>html</code>\
    \ version under the folder <code>html</code> (open <code>index.html</code> with\
    \ a browser).</li>\n<li>\n<code>pdf</code> version: <code>latex/refman.pdf</code>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-use-a-docker-container-to-run-fastqpuri\"\
    \ class=\"anchor\" href=\"#use-a-docker-container-to-run-fastqpuri\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use\
    \ a docker container to run FastqPuri</h2>\n<p>The file 'Dockerfile' documents\
    \ the exact linux installation we used\nfor testing. If you have a docker installation\
    \ ready on your machine,\nyou may want to use a docker container for easy installation\
    \ and\ncapsulated usage of FastqPuri. After cloning this project from github\n\
    and change to its main directory, you may install a docker container\nas follows:</p>\n\
    <pre><code>$ docker build -t fastqpuri .\n</code></pre>\n<p>This will create a\
    \ container based on the debian linux distribution\ncovering all dependencies\
    \ including R and pandoc.  As soon as such a\ncontainer is installed, you can\
    \ use it either interactively:</p>\n<pre><code>$ docker run -v $PWD:/tmp -it fastqpuri\n\
    </code></pre>\n<p>or by running a pipeline implemented in an executable bash script:</p>\n\
    <pre><code>$ docker run -v $PWD:/tmp fastqpuri ./pipeline.sh\n</code></pre>\n\
    <p>Note that this call generates results in the docker container\ndirectory <code>/tmp</code>\
    \ but also keeps them after closing the docker container\nlocally where the container\
    \ was started.</p>\n<p>Instead of generating the docker container yourself with\
    \ 'docker\nbuild', you can also pull a pre-built image from the docker hub as\n\
    follows:</p>\n<pre><code>$ docker pull clottaz/fastqpuri\n</code></pre>\n<p>You\
    \ can run such a pre-built image with 'docker run' by indicating the\nimages as\
    \ 'clottaz/fastqpuri'.</p>\n<h2>\n<a id=\"user-content-use-a-singularity-container-to-run-fastqpuri\"\
    \ class=\"anchor\" href=\"#use-a-singularity-container-to-run-fastqpuri\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use\
    \ a singularity container to run FastqPuri</h2>\n<p>Alternativly, if you have\
    \ singularity installed on your machine, you\ncan call our docker container for\
    \ FastqPuri as follows:</p>\n<pre><code>$ singularity shell --bind .:/tmp docker://clottaz/fastqpuri\n\
    </code></pre>\n<p>This call opens a shell within the container.\nWith <code>--bind</code>\
    \ we  mount the current directory also in the container.\nThe syntax is as follows:\
    \ --bind src:dest; src is the source path on\nthe host and dest is the destination\
    \ path in the container, i.e. where\nyou would like to make the source path available\
    \ in your container.\nNote that this destination path in your container should\
    \ be an existing\ndirectory, the operation will fail if you do not create the\
    \ directory first.\nHence, when we call <code>singularity shell</code> like this,\
    \ the working directory\nin the container is <code>/tmp</code>.</p>\n<p>Alternatively,\
    \ in order to execute a script from the current\ndirectory, call singularity as\
    \ follows:</p>\n<pre><code>$ singularity run --bind .:/tmp docker://clottaz/fastqpuri\
    \ /tmp/pipeline.sh\n</code></pre>\n<p>Note that <code>/tmp/pipeline.sh</code>\
    \ relates to the call within the\ncontainer. Thus, <code>pipeline.sh</code> is\
    \ located in the directory where singularity\nrun is executed, but will be made\
    \ available to the container via the <code>--bind</code>\nparameter.</p>\n<p>If\
    \ you want to invoke a function of FastqPuri, you can use the 'exec'\ncommand\
    \ like so:</p>\n<pre><code>singularity exec docker://clottaz/fastqpuri Qreport\
    \ -h\n</code></pre>\n<p>or invoke a script located in your home directory (assuming\
    \ that\nrun_ex_TREE.sh is located in your home directory):</p>\n<pre><code>$ singularity\
    \ exec docker://clottaz/fastqpuri $HOME/run_ex_TREE.sh\n</code></pre>\n<p>Singularity\
    \ documentation can be found here: <a href=\"https://www.sylabs.io/docs/\" rel=\"\
    nofollow\">https://www.sylabs.io/docs/</a></p>\n<h2>\n<a id=\"user-content-installation-via-bioconda--under-construction\"\
    \ class=\"anchor\" href=\"#installation-via-bioconda--under-construction\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation\
    \ via bioconda <strong>-under construction</strong>.</h2>\n<p><em>We are currently\
    \ working on a bioconda environment for FastqPuri.\nIf you follow the instructions\
    \ below, it is quite likely that\nFastqPuri will not yet properly run from the\
    \ bioconda environment.\nSorry about that and please stay tuned!</em></p>\n<p>Bioconda\
    \ is a channel for the conda package manager specializing in\nbioinformatics software.\
    \ Have a look at the reference:</p>\n<ul>\n<li>Bjoern Gruening, Ryan Dale, Andreas\
    \ Sjoedin, Brad A. Chapman, Jillian\nRowe, Christopher H. Tomkins-Tinch, Renan\
    \ Valieris, the Bioconda\nTeam, and Johannes Koester. 2018. Bioconda: Sustainable\
    \ and\nComprehensive Software Distribution for the Life Sciences. Nature\nMethods,\
    \ 2018.</li>\n</ul>\n<p>To find out how to use bioconda, see <a href=\"https://bioconda.github.io\"\
    \ rel=\"nofollow\">https://bioconda.github.io</a>.\nFor installing FastqPuri in\
    \ a bioconda environment, you have to install\neither <code>miniconda</code> or\
    \ <code>anaconda</code> and register channels as follows:</p>\n<pre><code>$ conda\
    \ config --add channels defaults\n$ conda config --add channels bioconda\n$ conda\
    \ config --add channels conda-forge\n</code></pre>\n<p>Then you can install <code>fastqpuri</code>:</p>\n\
    <pre><code>$ conda install fastqpuri\n</code></pre>\n<p>Actually, you may also\
    \ want to use a specific environment for the\nsequencing quality control:</p>\n\
    <pre><code>$ conda create -n qc fastqpuri\n</code></pre>\n<p>This call installs\
    \ <code>FastqPuri</code> directly in a separate environment.</p>\n<h2>\n<a id=\"\
    user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributors</h2>\n\
    <p>Paula P\xE9rez Rubio,\nClaudio Lottaz,\nJulia Engelmann</p>\n<h2>\n<a id=\"\
    user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>GPL v3 (see LICENSE.txt)</p>\n"
  stargazers_count: 16
  subscribers_count: 4
  topics: []
  updated_at: 1622626216.0
jganong/singularity-test:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jganong/singularity-test
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606934860.0
jganong/ubuntu-bionic-R-4.0.3-foieGras:
  data_format: 2
  description: singularity container to run Ian Jonsen's foieGras package
  filenames:
  - Singularity
  full_name: jganong/ubuntu-bionic-R-4.0.3-foieGras
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607375064.0
jganong/ubuntu-focal-foiegras:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jganong/ubuntu-focal-foiegras
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-create-study-specific-roi-image-in-mni-space\"\
    \ class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Create\
    \ study-specific ROI image in MNI space</h1>\n<p>PMAT resting state connectivity\
    \ study.</p>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Inputs:</h2>\n<p>All should be matched to the same T1 image.</p>\n\
    <ul>\n<li>T1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)</li>\n\
    <li>Deformation from T1 subject space to atlas space (typically DEF_FWD resource\
    \ of cat12 assessor)</li>\n<li>SUBJECT directory of Freesurfer output (typically\
    \ SUBJECT resource of freesurfer_dev assessor)</li>\n<li>Temporal lobe segmentation\
    \ (typically SEG resource of Temporal_Lobe assessor)</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n\
    <pre><code>rois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv\
    \        Region labels and volumes\nmakerois-PMAT.pdf           Visual report\
    \ of final ROI image\n</code></pre>\n<h2>\n<a id=\"user-content-regions-of-interest\"\
    \ class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Regions of interest</h2>\n<h3>\n\
    <a id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spheres (atlas space)</h3>\n<p>Source: <em>Libby LA, Ekstrom AD, Ragland\
    \ JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal\
    \ cortices within human hippocampal subregions revealed by high-resolution functional\
    \ imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012</em></p>\n\
    <p>Method: <em>Schr\xF6der TN, Haak K V., Jimenez NIZ, et al. Functional topography\
    \ of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"\
    #entorhinal-cortex-atlas-space\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Entorhinal cortex (atlas space)</h3>\n\
    <p>Anterior lateral and posterior medial sections. Source and method: <em>Schr\xF6\
    der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal\
    \ cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\"\
    \ href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Temporal lobe (Subject space,\
    \ warped)</h3>\n<p>Head for anterior hippocampus; body and tail combined for posterior\
    \ hippocampus. Method: <em>Plassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale\
    \ Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE\
    \ Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017\
    \ Feb 24. PMID: 28781411; PMCID: PMC5544133.</em></p>\n<h3>\n<a id=\"user-content-parahippocampal-perirhinal-subject-space-warped\"\
    \ class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parahippocampal,\
    \ perirhinal (Subject space, warped)</h3>\n<p>Generated by Freesurfer 6. Parahippocampal\
    \ (1016,2016) and perirhinal (surface patch resampled to volume, overlap with\
    \ parahippocampus was assigned to perirhinal). Method: <em>Bruce Fischl, Andre\
    \ van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H.\
    \ Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne\
    \ Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating\
    \ the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.</em></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607375887.0
jintonic/gears:
  data_format: 2
  description: Geant4 Example Application with Rich features and Small footprints
  filenames:
  - INSTALL/Singularity
  full_name: jintonic/gears
  latest_release: v1.4.2
  readme: '<p><a href="https://codedocs.xyz/jintonic/gears/annotated.html" rel="nofollow"><img
    src="https://camo.githubusercontent.com/69e499a9ac348bd92b62e3eac1391967c594141c2855f42783e17f07527553c4/68747470733a2f2f636f6465646f63732e78797a2f6a696e746f6e69632f67656172732e737667"
    alt="Doxygen" data-canonical-src="https://codedocs.xyz/jintonic/gears.svg" style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667"
    alt="License" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"
    style="max-width:100%;"></a>

    <a href="examples"><img src="https://camo.githubusercontent.com/11e2b1b7847229fdbb3430cf4a5d908964287c64b59e0119936bce5359e4efb9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f67656172732d6578616d706c65732d626c75653f7374796c653d666c6174"
    alt="Examples" data-canonical-src="https://img.shields.io/badge/gears-examples-blue?style=flat"
    style="max-width:100%;"></a>

    <a href="INSTALL"><img src="https://camo.githubusercontent.com/04a8d93a22dbf7fc5499c5c388432c900fac24a8e4061f7794fa5c2260199c46/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6765742d737461727465642d6f72616e67653f7374796c653d666c6174"
    alt="Get Started" data-canonical-src="https://img.shields.io/badge/get-started-orange?style=flat"
    style="max-width:100%;"></a>

    <a href="#how-to-contribute"><img src="https://camo.githubusercontent.com/b02b9d3708d187a71e102cc95c8a8ad5fcdffb8241ba9b4a102a4272862f1216/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6765742d696e766f6c7665642d6666363962343f7374796c653d666c6174"
    alt="Get Involved" data-canonical-src="https://img.shields.io/badge/get-involved-ff69b4?style=flat"
    style="max-width:100%;"></a></p>

    <p><a href="examples/detector/visualization/gearsX3D.html"><img align="right"
    width="120px" src="examples/detector/visualization/gears.png" style="max-width:100%;"></a></p>

    <p><a href="https://github.com/jintonic/gears">GEARS</a> is a <a href="http://geant4.cern.ch"
    rel="nofollow">Geant4</a> <a href="http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Examples/examples.html"
    rel="nofollow">Example</a> Application with <a href="#features">Rich features</a>
    yet Small footprint. The entire C++ coding is minimized down to a single file
    with about 550 <a href="https://en.wikipedia.org/wiki/Source_lines_of_code" rel="nofollow">SLOC</a>.
    This is achieved mainly by utilizing <a href="http://geant4.cern.ch" rel="nofollow">Geant4</a>
    plain <a href="http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Detector/Geometry/geomASCII.html"
    rel="nofollow">text geometry description</a>, <a href="http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Control/commands.html"
    rel="nofollow">built-in UI commands</a> (macros), and C++ inheritance. It is ideal
    for student training and fast implementation of small to medium-sized experiments.</p>

    <h2>

    <a id="user-content-features" class="anchor" href="#features" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Features</h2>

    <ul>

    <li>

    <a href="gears.cc">Single small C++ file</a>, easy to manage, fast to <a href="INSTALL#compile-gears">compile</a>
    (a few second on a regular PC)</li>

    <li>

    <a href="examples/physics">Easy switching between well maintained Geant4 reference
    physics lists without recompilation</a>

    <ul>

    <li><a href="examples/physics#physics-processes">Individual processes can be turned
    on/off without recompilation</a></li>

    <li><a href="examples/physics#optical-properties-of-materials-and-surfaces">Fast
    implementation of optical properties without recompilation</a></li>

    <li>

    <a href="examples/physics#radioactive-decay">Optional radioactive decay simulation</a>
    with the possibility to <a href="examples/physics#split-decay-chain">save the
    parent and daughter decays into different events if the later happens after a
    user specified time interval</a>

    </li>

    </ul>

    </li>

    <li>

    <a href="examples/sources#common-sources">Frequently used source spectra (AmBe,
    Am-241, etc.)</a> in addition to <a href="examples/sources">GPS</a>

    </li>

    <li>

    <a href="examples/output">Output in multiple data format</a>

    <ul>

    <li>

    <a href="examples/output#root">ROOT</a> TTree format (default, no <a href="https://root.cern.ch"
    rel="nofollow">ROOT</a> installation is needed)

    <ul>

    <li>Build-in data compression, well suitable for large data processing</li>

    <li>Fast access to independent data members</li>

    <li>Flat tree (no nested branches or arrays) with short leaf names

    <ul>

    <li>Easy to use in TTree::Draw</li>

    <li>No need to load extra library to open</li>

    <li>Can be easily analyzed in <a href="https://www.python.org/" rel="nofollow">Python</a>
    through <a href="https://github.com/scikit-hep/uproot4">Uproot</a>.</li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <a href="https://www.hdfgroup.org/downloads/hdf5/" rel="nofollow">HDF5</a>, universal
    data format, easy to read by different tools</li>

    <li>CSV or XML, Human readable ASCII file, capable of dealing with multiple dimensional
    arrays</li>

    </ul>

    </li>

    <li>

    <a href="examples/output#record-information-of-step-0">Record information of step
    0</a> (initStep), which is not available through <a href="http://www-geant4.kek.jp/lxr/source/tracking/include/G4UserSteppingAction.hh"
    rel="nofollow">G4UserSteppingAction</a>

    </li>

    <li>

    <a href="http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Detector/Geometry/geomASCII.html"
    rel="nofollow">simple text</a> or <a href="https://gdml.web.cern.ch/GDML/" rel="nofollow">GDML</a>
    geometry I/O

    <ul>

    <li>

    <a href="examples/detector">Fast implementation of detector geometry</a> without
    C++ programming</li>

    <li>Create/Change geometry without re-compilation</li>

    <li>Turn off data saving in a volume by assigning it a non-positive copy number</li>

    <li>Turn any volume to a <a href="examples/detector#sensitive-volume">sensitive
    detector</a> by adding "(S)" in its name</li>

    <li>

    <a href="examples/detector/optical">Assign optical properties in Geant4 plain
    text geometry description</a>, which is not available in the official <a href="http://geant4.cern.ch"
    rel="nofollow">Geant4</a> release</li>

    <li>

    <a href="examples/detector/syntax">Syntax highlighting of the simple text geometry
    description files</a> in <a href="examples/detector/syntax#emacs">Emacs</a>, <a
    href="examples/detector/syntax#vim">Vim</a>, <a href="examples/detector/syntax#micro">Micro</a>,
    and <a href="examples/detector/syntax#sublime-text">Sublime Text</a>

    </li>

    </ul>

    </li>

    <li><a href="http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Detector/commandScore.html"
    rel="nofollow">Creating 3D mesh to record and visualize physical variables in
    it without any change of the C++ code</a></li>

    <li><a href="https://codedocs.xyz/jintonic/gears/" rel="nofollow">Doxygen documentation</a></li>

    <li>Many <a href="examples">sample macros</a> and <a href="examples/detector">geometry
    descriptions</a> for feature demonstration</li>

    </ul>

    <h2>

    <a id="user-content-how-to-contribute" class="anchor" href="#how-to-contribute"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How
    to contribute</h2>

    <p>Please <a href="https://help.github.com/en/github/getting-started-with-github/fork-a-repo">fork
    GEARS on GitHub</a>. Run the following to get a local copy of the forked repository
    and link it to the <a href="https://github.com/jintonic/gears">original GEARS
    repository</a>:</p>

    <div class="highlight highlight-source-shell"><pre>$ git clone git@github.com:yourGitHubAccount/gears.git
    <span class="pl-c"><span class="pl-c">#</span> get forked repository</span>

    $ git remote add upstream git@github.com:jintonic/gears.git <span class="pl-c"><span
    class="pl-c">#</span> link to original repository</span>

    $ git remote -v <span class="pl-c"><span class="pl-c">#</span> run a check</span></pre></div>

    <p>Run the following to keep your local repository updated with the <a href="https://github.com/jintonic/gears">original
    GEARS repository</a>:</p>

    <div class="highlight highlight-source-shell"><pre>$ git fetch upstream <span
    class="pl-c"><span class="pl-c">#</span> updates are saved in a new branch upstream/master</span>

    $ git merge upstream/master <span class="pl-c"><span class="pl-c">#</span> merge
    2 branches: upstream/master and master</span></pre></div>

    <p>If the merge is successful, run <code>git push</code> to update your forked
    GEARS repository on GitHub.</p>

    <p>You can initiate a <a href="https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests">pull
    request on GitHub</a> if you''d like to have your update being absorbed in <a
    href="https://github.com/jintonic/gears">the original GEARS repository</a>.</p>

    <h3>

    <a id="user-content-coding-convention" class="anchor" href="#coding-convention"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Coding
    convention</h3>

    <h4>

    <a id="user-content-g4cout-vs-stdcout" class="anchor" href="#g4cout-vs-stdcout"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>G4cout
    VS std::cout</h4>

    <p><code>G4cout</code> and <code>G4endl</code> is preferred over <code>std:cout</code>
    and <code>std:endl</code> because the former handle the output in <a href="http://geant4.cern.ch"
    rel="nofollow">Geant4</a> GUI correctly, while the later can only output to terminal.</p>

    <h4>

    <a id="user-content-indentation" class="anchor" href="#indentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Indentation</h4>

    <p>Two spaces instead of a tab are used to indent a line in <a href="gears.cc">gears.cc</a>
    to insure a consistent appearance in different text editors, and to avoid wasting
    space in front of deeply nested code blocks. The following mode lines are added
    to the end of <a href="gears.cc">gears.cc</a> to insure that in <a href="https://www.vim.org/"
    rel="nofollow">Vim</a> and <a href="https://www.gnu.org/software/emacs/" rel="nofollow">Emacs</a>:</p>

    <div class="highlight highlight-source-c++"><pre><span class="pl-c"><span class="pl-c">//</span>
    -*- C++; indent-tabs-mode:nil; tab-width:2 -*-</span>

    <span class="pl-c"><span class="pl-c">//</span> vim: ft=cpp:ts=2:sts=2:sw=2:et</span></pre></div>

    <h3>

    <a id="user-content-to-dos" class="anchor" href="#to-dos" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>To-do''s</h3>

    <ul>

    <li>examples

    <ul>

    <li>add an example to show how QE can be implemented</li>

    <li>add examples to show how one can distribute source in a volume or surface</li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 23
  subscribers_count: 11
  topics:
  - geant4
  - detector
  - physics
  - monte-carlo-simulation
  updated_at: 1623916729.0
jkulhanek/a2cat-vn-pytorch:
  data_format: 2
  description: Target driven visual navigation using deep reinforcement learning implemented
    in Pytorch
  filenames:
  - Singularity
  full_name: jkulhanek/a2cat-vn-pytorch
  latest_release: null
  readme: '<h1>

    <a id="user-content-target-driven-visual-navigation" class="anchor" href="#target-driven-visual-navigation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>target-driven-visual-navigation</h1>

    <p>Target driven visual navigation using deep reinforcement learning implemented
    in Pytorch</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1607415822.0
jkulhanek/deep-rl-pytorch:
  data_format: 2
  description: Popular Deep RL algorithms implemented in PyTorch
  filenames:
  - Singularity
  full_name: jkulhanek/deep-rl-pytorch
  latest_release: null
  readme: '<h1>

    <a id="user-content-deep-rl-pytorch" class="anchor" href="#deep-rl-pytorch" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deep RL PyTorch</h1>

    <p><a href="https://singularity-hub.org/collections/2581" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>This repo contains implementation of popular Deep RL algorithms. Furthermore
    it contains unified interface for training and evaluation with unified model saving
    and visualization. It can be used as a good starting point when implementing new
    RL algorithm in PyTorch.</p>

    <h2>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h2>

    <p>If you want to base your algorithm on this repository, start by installing
    it as a package</p>

    <pre><code>pip install git+https://github.com/jkulhanek/deep-rl-pytorch.git

    </code></pre>

    <p>If you want to run attached experiments yourself, feel free to clone this repository.</p>

    <pre><code>git clone https://github.com/jkulhanek/deep-rl-pytorch.git

    </code></pre>

    <p>All dependencies are prepared in a docker container. If you have nvidia-docker
    enabled, you can use this image. To pull and start the image just run:</p>

    <pre><code>docker run --runtime=nvidia --net=host -it kulhanek/deep-rl-pytorch:latest
    bash

    </code></pre>

    <p>From there, you can either clone your own repository containing your experiments
    or clone this one.</p>

    <h2>

    <a id="user-content-concepts" class="anchor" href="#concepts" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Concepts</h2>

    <p>All algorithms are implemented as base classes. In your experiment your need
    to subclass from those base classes. The <code>deep_rl.core.AbstractTrainer</code>
    class is used for all trainers and all algorithms inherit this class. Each trainer
    can be wrapped in several wrappers (classes extending <code>deep_rl.core.AbstractWrapper</code>).
    Those wrappers are used for saving, logging, terminating the experiment and etc.
    All experiments should be registered using <code>@deep_rl.register_trainer</code>
    decorator. This decorator than wraps the trainer with default wrappers. This can
    be controlled by passing arguments to the decorator. All registered trainers (experiments)
    can be run by calling <code>deep_rl.make_trainer(&lt;&lt;name&gt;&gt;).run()</code>.</p>

    <h2>

    <a id="user-content-implemented-algorithms" class="anchor" href="#implemented-algorithms"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Implemented
    algorithms</h2>

    <h3>

    <a id="user-content-a2c" class="anchor" href="#a2c" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>A2C</h3>

    <p>A2C is a synchronous, deterministic variant of Asynchronous Advantage Actor
    Critic (A3C) [2] which according to OpenAI [1] gives equal performance. It is
    however more efficient for GPU utilization.</p>

    <p>Start your experiment by subclassing <code>deep_rl.a2c.A2CTrainer</code>.

    Several models are included in <code>deep_rl.a2c.model</code>. You may want to
    use at least some helper modules contained in this package when designing your
    own experiment.</p>

    <p>In most of the models, initialization is done according to [3].</p>

    <h3>

    <a id="user-content-asynchronous-advantage-actor-critic-a3c-2" class="anchor"
    href="#asynchronous-advantage-actor-critic-a3c-2" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Asynchronous Advantage Actor Critic (A3C)
    [2]</h3>

    <p>This implementation uses multiprocessing. It comes with two optimizers - RMSprop
    and Adam.</p>

    <h3>

    <a id="user-content-actor-critic-using-kronecker-factored-trust-region-acktr-1"
    class="anchor" href="#actor-critic-using-kronecker-factored-trust-region-acktr-1"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Actor
    Critic using Kronecker-Factored Trust Region (ACKTR) [1]</h3>

    <p>This is an improvement of A2C described in [1].</p>

    <h2>

    <a id="user-content-experiments" class="anchor" href="#experiments" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Experiments</h2>

    <blockquote>

    <p>Comming soon</p>

    </blockquote>

    <h2>

    <a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Requirements</h2>

    <p>Those packages must be installed before using the framework for your own algorithm:</p>

    <ul>

    <li>OpenAI baselines (can be installed by running <code>pip install git+https://github.com/openai/baselines.git</code>)</li>

    <li>PyTorch</li>

    <li>Visdom (<code>pip install visdom</code>)</li>

    <li>Gym (<code>pip install gym</code>)</li>

    <li>MatPlotLib</li>

    </ul>

    <p>Those packages must be installed prior running experiments:</p>

    <ul>

    <li>DeepMind Lab</li>

    <li>Gym[atari]</li>

    </ul>

    <h2>

    <a id="user-content-sources" class="anchor" href="#sources" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Sources</h2>

    <p>This repository is based on work of several other authors. We would like to
    express our thanks.</p>

    <ul>

    <li><a href="https://github.com/openai/baselines/tree/master/baselines">https://github.com/openai/baselines/tree/master/baselines</a></li>

    <li><a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/a2c_ppo_acktr">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/a2c_ppo_acktr</a></li>

    <li><a href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></li>

    <li><a href="https://github.com/openai/gym">https://github.com/openai/gym</a></li>

    </ul>

    <h2>

    <a id="user-content-references" class="anchor" href="#references" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

    <p>[1] Wu, Y., Mansimov, E., Grosse, R.B., Liao, S. and Ba, J., 2017. Scalable
    trust-region method for deep reinforcement learning using kronecker-factored approximation.
    In Advances in neural information processing systems (pp. 5279-5288).</p>

    <p>[2] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
    Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement
    learning. In International conference on machine learning (pp. 1928-1937).</p>

    <p>[3] Saxe, A.M., McClelland, J.L. and Ganguli, S., 2013. Exact solutions to
    the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint
    arXiv:1312.6120.</p>

    '
  stargazers_count: 4
  subscribers_count: 1
  topics: []
  updated_at: 1615406783.0
jmurga/bgd-pic:
  data_format: 2
  description: null
  filenames:
  - Singularity.abcmk
  - Singularity.isafe
  - Singularity.pophuman
  - Singularity.breakseq
  full_name: jmurga/bgd-pic
  latest_release: null
  readme: '<h1>

    <a id="user-content-naked-singularity" class="anchor" href="#naked-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>naked-singularity</h1>

    <p>A repository of definition files for building

    <a href="https://sylabs.io/guides/latest/user-guide" rel="nofollow">Singularity</a>
    containers

    around the software applications, frameworks, and libraries you need to

    run on high-performance computing systems.</p>

    <h2>

    <a id="user-content-install-singularity" class="anchor" href="#install-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Singularity</h2>

    <p>Install Singularity on your Linux desktop, laptop, or virtual machine.</p>

    <div class="highlight highlight-source-shell"><pre>sudo ./naked-singularity.sh
    install</pre></div>

    <h2>

    <a id="user-content-build-a-singularity-container-from-a-definition-file" class="anchor"
    href="#build-a-singularity-container-from-a-definition-file" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build a Singularity
    container from a definition file</h2>

    <p>Build an Ubuntu Singularity container from one of the definition files

    available in this repository.</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build ubuntu.sif
    definition-files/ubuntu/Singularity.ubuntu-18.04</pre></div>

    <h2>

    <a id="user-content-download-an-existing-singularity-container" class="anchor"
    href="#download-an-existing-singularity-container" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Download an existing Singularity container</h2>

    <p>A number of pre-built containers from this repository are also now

    hosted at Singularity Hub.</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull shub://mkandes/naked-singularity:ubuntu-18.04</pre></div>

    <p><a href="https://vsoch.github.io/2021/singularity-hub-archive" rel="nofollow">Singularity
    Hub has been archived</a>. At least for the time being, naked-singularity definition

    files that rely on containers that were built and hosted on Singularity

    Hub prior to it being archived will continue to pull in these container

    dependencies and build properly. Alternative container build and hosting

    options for all future work are still under consideration.</p>

    <h2>

    <a id="user-content-status" class="anchor" href="#status" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Status</h2>

    <p>A work in progress.</p>

    <h2>

    <a id="user-content-contribute" class="anchor" href="#contribute" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contribute</h2>

    <p>If you would like to contribute one of your own Singularity container

    definition files for a specific application OR request a modification to

    an existing container definition, then please submit a pull request.</p>

    <h2>

    <a id="user-content-author" class="anchor" href="#author" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Author</h2>

    <p>Marty Kandes, Ph.D.<br>

    Computational &amp; Data Science Research Specialist<br>

    High-Performance Computing User Services Group<br>

    San Diego Supercomputer Center<br>

    University of California, San Diego</p>

    <h2>

    <a id="user-content-version" class="anchor" href="#version" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version</h2>

    <p>1.7.2</p>

    <h2>

    <a id="user-content-last-updated" class="anchor" href="#last-updated" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Last Updated</h2>

    <p>Sunday, June 20th, 2021</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624197047.0
jncc/s2-ard-processor:
  data_format: 2
  description: Sentinel 2 ARD processor
  filenames:
  - mpi-base/Singularity
  - base/Singularity
  full_name: jncc/s2-ard-processor
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-s2-ard-processor\" class=\"anchor\" href=\"\
    #s2-ard-processor\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>S2 ARD Processor</h1>\n<p>Docker based sentinel 2\
    \ Analysis ready production system.</p>\n<h2>\n<a id=\"user-content-base\" class=\"\
    anchor\" href=\"#base\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Base</h2>\n<p>A base docker image packaging\
    \ Dr Pete Buntings Python Atmospheric and Radiometric Correction of Satellite\
    \ Imagery (ARCSI) software (<a href=\"https://www.arcsi.remotesensing.info/\"\
    \ rel=\"nofollow\">https://www.arcsi.remotesensing.info/</a>).</p>\n<p>Based on\
    \ the official ContinuumIO Miniconda3 release with python 3.5, base package contains\
    \ a minimal installaition of ARCSI and its dependencies using the conda package\
    \ manger, correct as of version 3.1.6 (conda reporting 3.6.1).</p>\n<h3>\n<a id=\"\
    user-content-build-or-pull-arcsi-base\" class=\"anchor\" href=\"#build-or-pull-arcsi-base\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Build or Pull arcsi-base</h3>\n<h4>\n<a id=\"user-content-build-image\"\
    \ class=\"anchor\" href=\"#build-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build image</h4>\n<p><code>docker\
    \ build -t jncc/arcsi-base ./base/</code></p>\n<p><strong>OR</strong></p>\n<h4>\n\
    <a id=\"user-content-pull-image-direction-from-docker-hub\" class=\"anchor\" href=\"\
    #pull-image-direction-from-docker-hub\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pull Image direction from docker\
    \ hub</h4>\n<p><code>docker pull jncc/arcsi-base</code></p>\n<h3>\n<a id=\"user-content-usage\"\
    \ class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h3>\n<h4>\n<a id=\"user-content-run-image-interactively\"\
    \ class=\"anchor\" href=\"#run-image-interactively\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run image interactively</h4>\n\
    <p><code>docker run -i -v &lt;local mount point&gt;:/data -t jncc/arcsi-base /bin/bash</code></p>\n\
    <p>To run a container and get help on ARCSI commandline options do:</p>\n<p><code>docker\
    \ run -t jncc/arcsi-base arcsi.py -h</code></p>\n<p>See below under \"Docker example\"\
    \ for a more detailed Sentinel-2 example.</p>\n<h3>\n<a id=\"user-content-docker-example\"\
    \ class=\"anchor\" href=\"#docker-example\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Docker example</h3>\n<div class=\"\
    highlight highlight-source-shell\"><pre>docker run -i -t -v <span class=\"pl-smi\"\
    >${local_data}</span>:/data jncc/arcsi-base \\\n    arcsi.py -s sen2 --stats -f\
    \ KEA --fullimgouts -p RAD SHARP SATURATE CLOUDS TOPOSHADOW STDSREF DOSAOTSGL\
    \ METADATA FOOTPRINT \\\n    --interp near --outwkt /data/<span class=\"pl-smi\"\
    >${PATH_TO_OUTPUT_PROJ_WKT}</span> --projabbv <span class=\"pl-smi\">${PROJ_ABBREVIATION}</span>\
    \ -t /data/tmp/ -o /data/output/ \\\n    --dem /data/<span class=\"pl-smi\">${PATH_TO_DEM}</span>\
    \ -i /data/inputs/<span class=\"pl-smi\">${SINGLE_INPUT_FILE}</span></pre></div>\n\
    <h3>\n<a id=\"user-content-see-also\" class=\"anchor\" href=\"#see-also\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>See\
    \ also</h3>\n<p>Thanks to Markus Neteler (<a href=\"https://github.com/mundialis/docker-arcsi\"\
    >https://github.com/mundialis/docker-arcsi</a>), Edward P. Morris and Angelos\
    \ Tzotsos for their work on the orignal ARCSI Dockerfile.</p>\n"
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1604315244.0
joaocaldeira/singularity_imgs:
  data_format: 2
  description: Singularity images to run on the cluster
  filenames:
  - Singularity.py3_tf112
  - Singularity.py3_tf114
  - Singularity.py3_tf112_plus
  - Singularity.py3_astro
  - Singularity.py3_tf114_lls
  - Singularity.py3_tf115
  - Singularity.py3_tf113
  full_name: joaocaldeira/singularity_imgs
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_imgs" class="anchor" href="#singularity_imgs"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_imgs</h1>

    <p><a href="https://singularity-hub.org/collections/2968" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity images to run on the cluster</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1590440777.0
josephwkania/radio_transients:
  data_format: 2
  description: 'Singularity containers with common radio transient search software.  '
  filenames:
  - Singularity.gpu
  - Singularity
  - Singularity.cpu
  - Singularity.arm
  full_name: josephwkania/radio_transients
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-radio_transients\" class=\"anchor\" href=\"\
    #radio_transients\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>radio_transients</h1>\n<p><a href=\"\"><img src=\"\
    https://camo.githubusercontent.com/88694793dd1e428a0aab6788e9bbd21141580f62cbc4d6310bbcc74fde83ab22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"\"><img src=\"https://camo.githubusercontent.com/ff5d91d6824296a5d7ffaad36635c5ef0688d2c0e21e50ccc94735fe8387faa7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"Forks\" data-canonical-src=\"https://img.shields.io/github/forks/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"\"><img src=\"https://camo.githubusercontent.com/6e6ccae69f7f4df8c46d4d56b7e36d27fd932cc463a486a3111796543c271ab9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"\"><img src=\"https://camo.githubusercontent.com/d5708d5c2bcd6f7d5f3565d9e75135e1eaa086ff847e198c14d187c5612f8203/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://cloud.sylabs.io/library/josephwkania/radio_transients/radio_transients\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5a9c47f4c4e2f587278d94eea8fe905a930df7dd78bc5258f5d15cc1981348f8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f486f737465642d53796c6162732d477265656e2e737667\"\
    \ alt=\"Sylabs\" data-canonical-src=\"https://img.shields.io/badge/Hosted-Sylabs-Green.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-overview\" class=\"\
    anchor\" href=\"#overview\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Overview</h2>\n<p>These are my Singularity Recipes\
    \ for common radio transient software.\nThere are three containers</p>\n<h3>\n\
    <a id=\"user-content-radio_transients-1\" class=\"anchor\" href=\"#radio_transients-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>radio_transients</h3>\n<p>Contains everything (CPU+GPU)</p>\n<pre><code>CUDA\
    \ 10.2\nFETCH          https://github.com/devanshkv/fetch  -- In Conda environment\
    \ `FE`\nheimdall       https://sourceforge.net/p/heimdall-astro/wiki/Use/\n- dedisp\
    \       https://github.com/ajameson/dedisp\nhtop           https://htop.dev/\n\
    iqrm_apollo    https://gitlab.com/kmrajwade/iqrm_apollo\njupyterlab     https://jupyter.org/\n\
    PRESTO         https://www.cv.nrao.edu/~sransom/presto/\npsrdada        http://psrdada.sourceforge.net/\n\
    psrdada-python https://github.com/TRASAL/psrdada-python\npsrcat         https://www.atnf.csiro.au/people/pulsar/psrcat/download.html\n\
    pysigproc      https://github.com/devanshkv/pysigproc\nriptide        https://github.com/v-morello/riptide\n\
    sigproc        https://github.com/SixByNine/sigproc\nTempo          http://tempo.sourceforge.net/\n\
    RFIClean       https://github.com/ymaan4/RFIClean\nYAPP           https://github.com/jayanthc/yapp\n\
    your           https://github.com/thepetabyteproject/your\n</code></pre>\n<p>Get\
    \ with\n<code>singularity pull --arch amd64 library://josephwkania/radio_transients/radio_transients:latest</code></p>\n\
    <p>*One of FETCH's dependencies causes PRESTO's Python scripts to fail.\nThis\
    \ necessitated putting them in different environments.\nEverything except for\
    \ PRESTO is in <code>RT</code>, which is loaded by default.\nPRESTO is in <code>PE</code>,\
    \ in the shell you can activate this\nwith <code>conda activate PE</code>. If\
    \ you need access outside the container,\nyou should use radio_transients:cpu,\
    \ which has PRESTO in the default environment.</p>\n<h3>\n<a id=\"user-content-radio_transients_cpu\"\
    \ class=\"anchor\" href=\"#radio_transients_cpu\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>radio_transients_cpu</h3>\n<p>Contains\
    \ CPU based programs</p>\n<pre><code>htop\niqrm_apollo\njupyterlab   \nPRESTO\n\
    psrcat\npysigproc\nriptide\nsigproc\nTempo \nRFIClean\nYAPP  \nyour\n</code></pre>\n\
    <p>Get with\n<code>singularity pull --arch amd64 library://josephwkania/radio_transients/radio_transients:cpu</code><br>\n\
    There is an arm version <code>Singularity.arm</code>,\n<code>singularity pull\
    \ --arch arm library://josephwkania/radio_transients/radio_transients:arm</code></p>\n\
    <h3>\n<a id=\"user-content-radio_transients_gpu\" class=\"anchor\" href=\"#radio_transients_gpu\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>radio_transients_gpu</h3>\n<p>Contains gpu based programs</p>\n<pre><code>CUDA\
    \ 10.2\nFETCH      \njupyterlab\nheimdall\n- dedisp\nhtop \npsrdada \npsrdada-python\n\
    your\n</code></pre>\n<p>Get with\n<code>singularity pull --arch amd64 library://josephwkania/radio_transients/radio_transients:gpu</code></p>\n\
    <h3>\n<a id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to use</h3>\n<p>Your <code>$HOME</code> automatically gets mounted.\n\
    You can mount a directory with <code>-B /dir/on/host:/mnt</code>, which will mount\
    \ <code>/dir/on/host</code> to <code>/mnt</code> in the container.</p>\n<p>For\
    \ the gpu processes, you must pass <code>--nv</code> when running singularity.</p>\n\
    <p><code>singularity shell --nv -B /data:/mnt radio_transients_gpu.sif</code>\n\
    will mount <code>/data</code> to <code>/mnt</code>, give you GPU access, and drop\
    \ you into the interactive shell.</p>\n<p><code>singularity exec --nv -B /data:/mnt\
    \ radio_transients_gpu.sif your_heimdall.py -f /mnt/data.fil</code>\nwill mount\
    \ <code>/data</code> to <code>/mnt</code>, give you GPU access, and run your_heimdall.py\
    \ without entering the container.</p>\n<p>All the Python scripts are installed\
    \ in a Conda environment <code>RT</code>, this environment is automatically loaded.</p>\n\
    <p>You can see the commits and corresponding dates by running <code>singularity\
    \ inspect radio_transients.sif</code></p>\n<h3>\n<a id=\"user-content-sylabs-cloud\"\
    \ class=\"anchor\" href=\"#sylabs-cloud\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sylabs Cloud</h3>\n<p>These are\
    \ built on a E5 v3 family machine and uploaded to Sylabs Cloud at\n<a href=\"\
    https://cloud.sylabs.io/library/josephwkania/radio_transients/radio_transients\"\
    \ rel=\"nofollow\">https://cloud.sylabs.io/library/josephwkania/radio_transients/radio_transients</a>\n\
    They where last built on 21-Jun-2021</p>\n<p>If your processor your processor\
    \ is significantly older than this, you may run into problems with\nthe older\
    \ processor not having the whole instruction set needed. In this case, you should\
    \ build\nuse singularity to build the image locally.</p>\n<p>An archival version\
    \ of these (built 25-April-2021) are on Singularity Hub at:\n<a href=\"https://singularity-hub.org/collections/5231\"\
    \ rel=\"nofollow\">https://singularity-hub.org/collections/5231</a>\n<a href=\"\
    https://singularity-hub.org/collections/5231\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h3>\n<a id=\"user-content-improvements\"\
    \ class=\"anchor\" href=\"#improvements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Improvements</h3>\n<p>If you\
    \ come across bug or have suggestions for improvements, let me know or submit\
    \ a pull request.</p>\n<h3>\n<a id=\"user-content-thanks\" class=\"anchor\" href=\"\
    #thanks\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Thanks</h3>\n<p>To Kshitij Aggarwal for bug reports and suggestions.</p>\n"
  stargazers_count: 4
  subscribers_count: 2
  topics:
  - pulsars
  - fast-radio-bursts
  - psrdada
  - radio-astronomy
  updated_at: 1624333236.0
kalibera/rchk:
  data_format: 2
  description: null
  filenames:
  - image/Singularity.def
  - image/Singularity.bionic
  full_name: kalibera/rchk
  latest_release: null
  readme: '<p>This project consists of several bug-finding tools that look for memory

    protection errors in C source code using R API, that is in the source code

    of <a href="http://www.r-project.org/" rel="nofollow">R</a> itself and packages.  The
    tools perform

    whole-program static analysis on LLVM bitcode and run on Linux.  About

    200-300 memory protection bugs have been found using rchk and fixed in R.

    rchk is now regularly used to check <a href="https://github.com/kalibera/cran-checks/tree/master/rchk">CRAN

    packages</a>.</p>

    <p>To use the tool, one needs to build R from source using a special compiler

    wrapper, which builds LLVM bitcode in addition to native code (both shared

    libraries and executables). R packages are then installed using this version

    of R, providing LLVM bitcode for their shared libraries as well. The core of

    rchk is implemented in C++ and analyzes the LLVM bitcode of R packages and R

    itself. Several installation options are provided, including containers.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>The tool is available in pre-built containers, Docker and Singularity, for

    <em>non-interactive</em> use. The container is invoked as a command to check a

    particular package:</p>

    <pre><code>docker pull kalibera/rchk:latest

    docker run kalibera/rchk:latest audio

    </code></pre>

    <pre><code>singularity pull shub://kalibera/rchk:def

    singularity run kalibera-rchk-master-def.simg jpeg

    </code></pre>

    <p>For more details, see <a href="doc/DOCKER.md">Docker rchk container</a> and

    <a href="doc/SINGULARITY.md">Singularity rchk container</a>. This setup is good
    for

    occasional checking of a single package. Docker clients are

    available for Linux, macOS and Windows. Singularity only for Linux.</p>

    <p>The tool can also be used interactively in a virtual machine running Ubuntu,

    which can be automatically installed using Vagrant scripts. This setup is

    good for Linux, Windows and macOS users and makes it faster to repeatedly

    check the same package and easier to customize the process. See

    <a href="doc/INSTALLATION.md">Automated installation (Docker/Virtualbox) for interactive
    use</a>.</p>

    <p>Finally, the tool can be installed natively on Linux, compiled from source.

    This setup is good for interactive use and reduces disk space overhead. The

    setup is not automated, but only requires several steps described for recent

    Linux distributions. See <a href="doc/INSTALLATION.md">Native installation on
    Linux for interactive use</a>.</p>

    <p>An alternative docker image is also available from third parties on R-hub

    (<code>rhub/ubuntu-rchk</code>,

    <a href="https://github.com/r-hub/rhub-linux-builders/tree/master/ubuntu-rchk">source</a>).</p>

    <h2>

    <a id="user-content-checking-the-first-package-interactive-use" class="anchor"
    href="#checking-the-first-package-interactive-use" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Checking the first package (interactive
    use)</h2>

    <p>This part applies to interactive installation of rchk (natively or automated

    install in Docker/Virtualbox).  For this that one also needs to install

    <code>subversion</code>, <code>rsync</code> (<code>apt-get install subversion
    rsync</code>, but already

    available in the automated install).  More importantly, one also needs any

    dependencies needed by that package.</p>

    <ol>

    <li>Build R producing also LLVM bitcode

    <ul>

    <li><code>svn checkout https://svn.r-project.org/R/trunk</code></li>

    <li><code>cd trunk</code></li>

    <li>

    <code>. ../scripts/config.inc</code> (<em>in automated install</em>, <code>. /opt/rchk/scripts/config.inc</code>)</li>

    <li>

    <code>. ../scripts/cmpconfig.inc</code> (<em>in automated install</em>, <code>.
    /opt/rchk/scripts/cmpconfig.inc</code>)</li>

    <li>

    <code>../scripts/build_r.sh</code> (<em>in automated install</em>, <code>/opt/rchk/scripts/build_r.sh</code>)</li>

    </ul>

    </li>

    <li>Install and check the package

    <ul>

    <li><code>echo ''install.packages("jpeg",repos="http://cloud.r-project.org")''
    |  ./bin/R --no-echo</code></li>

    <li>

    <code>../scripts/check_package.sh jpeg</code> (in VM install, <code>/opt/rchk/scripts/check_package.sh
    jpeg</code>)</li>

    </ul>

    </li>

    </ol>

    <p>The output of the checking is in files

    <code>packages/lib/jpeg/libs/jpeg.so.*check</code>. For version 0.1-8 of the package,

    <code>jpeg.so.maacheck</code> includes</p>

    <pre><code>WARNING Suspicious call (two or more unprotected arguments) to Rf_setAttrib
    at read_jpeg /rchk/trunk/packages/build/IsnsJjDm/jpeg/src/read.c:131

    </code></pre>

    <p>which is a true error. <code>bcheck</code> does not find any errors, <code>jpeg.so.bcheck</code>

    only contains something like</p>

    <pre><code>Analyzed 15 functions, traversed 1938 states.

    </code></pre>

    <p>To check the next package, just follow the same steps, installing it into

    this customized version of R.  When checking a tarball, one would typically

    first install the CRAN/BIOC version of the package to get all dependencies

    in, and then use <code>R CMD INSTALL</code> to install the newest version to check
    from

    the tarball.</p>

    <p>One can reduce the number of required R package dependencies by only

    installing LinkingTo dependencies of the package and then installing the

    package with <code>--libs-only</code> option (only shared libraries are built
    and

    installed). This is enough to build shared libraries of most but not all

    packages. Docker and singularity rchk containers for non-interactive use do

    this, see <code>scripts/utils.r</code> and definitions of the containers for more

    details.</p>

    <p>Further information:</p>

    <ul>

    <li>

    <a href="doc/INSTALLATION.md">Installation</a> - installation instructions.</li>

    <li>

    <a href="doc/USAGE.md">User documentation</a> - how to use the tools and what
    they check.</li>

    <li>

    <a href="doc/INTERNALS.md">Internals</a> - how the tools work internally.</li>

    <li>

    <a href="doc/BUILDING.md">Building</a> - how to get the necessary bitcode files
    for R/packages; this is now encapsulated in scripts, but the background is here</li>

    </ul>

    <p><a href="https://singularity-hub.org/collections/2534" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 96
  subscribers_count: 10
  topics: []
  updated_at: 1624654368.0
kapsakcj/singularities:
  data_format: 2
  description: Singularity recipes for bioinformatics software
  filenames:
  - lyveset/1.1.4f/Singularity.lyveset.1.1.4f
  - spades/3.13.0/Singularity.spades.3.13.0
  - seqsero2/1.0.0/Singularity.seqsero2.1.0.0
  - seqsero2/0.1/Singularity.seqsero2-0.1
  - quast/5.0.0/Singularity.quast.5.0.0
  full_name: kapsakcj/singularities
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularities\" class=\"anchor\" href=\"#singularities\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>singularities</h1>\n<p>Singularity recipes for bioinformatics software.\
    \ Build singularity images with these recipes (sudo required) or download/pull\
    \ the images from <a href=\"https://singularity-hub.org/collections/2778\" rel=\"\
    nofollow\">singularity-hub.org</a></p>\n<p>This repo is <strong>WORK IN PROGRESS</strong>.\
    \ Feel free to try the recipes/Singularity builds, but they are <strong>not tested\
    \ deeply and are in no way guaranteed to work. Proceed at your own risk</strong>.</p>\n\
    <p>It is somewhat modeled after <a href=\"https://github.com/StaPH-B/docker-builds\"\
    >https://github.com/StaPH-B/docker-builds</a> , but with Singularity recipes instead.</p>\n\
    <p>Sysadmins for High Performance Cluster computers almost always favor Sinularity\
    \ over Docker :) so I'm starting to learn the ways of Singularity.</p>\n<h2>\n\
    <a id=\"user-content-available-singularity-images\" class=\"anchor\" href=\"#available-singularity-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Available Singularity images</h2>\n<table>\n<thead>\n<tr>\n<th align=\"\
    center\">Software</th>\n<th align=\"center\">Version</th>\n<th align=\"center\"\
    >Link</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">SPAdes</td>\n\
    <td align=\"center\">3.13.0</td>\n<td align=\"center\"><a href=\"http://cab.spbu.ru/software/spades/\"\
    \ rel=\"nofollow\">http://cab.spbu.ru/software/spades/</a></td>\n</tr>\n<tr>\n\
    <td align=\"center\">QUAST</td>\n<td align=\"center\">5.0.0</td>\n<td align=\"\
    center\"><a href=\"https://github.com/ablab/quast\">https://github.com/ablab/quast</a></td>\n\
    </tr>\n<tr>\n<td align=\"center\">Lyve-SET</td>\n<td align=\"center\">1.1.4f</td>\n\
    <td align=\"center\"><a href=\"https://github.com/lskatz/lyve-SET\">https://github.com/lskatz/lyve-SET</a></td>\n\
    </tr>\n<tr>\n<td align=\"center\">SeqSero2</td>\n<td align=\"center\">0.1, 1.0.0</td>\n\
    <td align=\"center\"><a href=\"https://github.com/denglab/SeqSero2\">https://github.com/denglab/SeqSero2</a></td>\n\
    </tr>\n</tbody>\n</table>\n<p>These Singularity images can be built if you have\
    \ Singularity installed and <strong>have sudo/admin priveleges</strong></p>\n\
    <pre><code># build an image using a recipe (called Singularity in this example)\n\
    sudo singularity build my-new-singularity-image.simg /path/to/Singularity\n\n\
    # download the repo\ngit clone https://github.com/kapsakcj/singularities.git\n\
    # another example using the SPAdes recipe\nsudo singularity build my-new-spades-3.13.0-image.simg\
    \ /path/to/Singularity.spades.3.13.0\n</code></pre>\n<p>These Singularity images\
    \ are also available to download from singularity-hub.org if you <strong>don't\
    \ have sudo priveleges</strong> (no build necessary!). The badge below is a link\
    \ to the singularity-hub.org collection.</p>\n<p><a href=\"https://singularity-hub.org/collections/2778\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>The name of the Singularity hub collection\
    \ is <code>kapsakcj/singularities</code> and the tag is specified by the extenion\
    \ of the Singularity recipe file. For example the recipe, <code>/spades/3.13.0/Singularity.spades.3.13.0</code>,\
    \ can be downloaded like so:</p>\n<pre><code># download an image like so, and\
    \ name it whatever you want with the --name flag\nsingularity pull --name my-new-spades-3.13.0-image\
    \ shub://kapsakcj/singularities:spades.3.13.0\n</code></pre>\n<h2>\n<a id=\"user-content-useful-links-and-resources\"\
    \ class=\"anchor\" href=\"#useful-links-and-resources\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Useful links\
    \ and resources</h2>\n<ul>\n<li>Singularity v2.6 User guide <a href=\"https://www.sylabs.io/guides/2.6/user-guide/index.html\"\
    \ rel=\"nofollow\">https://www.sylabs.io/guides/2.6/user-guide/index.html</a>\n\
    </li>\n<li>SingularityHub <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >https://singularity-hub.org/</a>\n</li>\n<li>Excellent tutorial on Singularity\
    \ (using v2.5) from Sylabs, many other links within <a href=\"https://github.com/Singularity-tutorial/Singularity-tutorial.github.io\"\
    >https://github.com/Singularity-tutorial/Singularity-tutorial.github.io</a>\n\
    </li>\n<li>How to build a container using Singularity Hub, linked to a github\
    \ repo with Singularity recipes <a href=\"https://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container\"\
    >https://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container</a>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-tipstricksthings-to-remember-about-singularity--many-from-jake-garfin--\"\
    \ class=\"anchor\" href=\"#tipstricksthings-to-remember-about-singularity--many-from-jake-garfin--\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Tips/Tricks/Things-to-remember about Singularity [ many from Jake\
    \ Garfin :) ]</h3>\n<ul>\n<li>Singularity automatically brings your user &amp;\
    \ group into the container with you (ie. no <code>-u $(id -u):$(id -g)</code>\
    \ needed like in Docker)</li>\n<li>Singularity (by default) wants to mount your\
    \ entire home directory inside the container as well. Use <code>--cleanenv</code>\
    \ and <code>--containall</code> to keep things separate and bring in specific\
    \ directories you want with <code>-B /local-dir:/dir-in-container</code>\n</li>\n\
    <li>Docker images converted to Singularity that want to write to system directories\
    \ owned by root aren't going to work out of the box.</li>\n<li>If you are making\
    \ a container with something that uses perl, add this to the recipe in the <code>%environment</code>\
    \ section to prevent locale settings errors (see lyveset recipe)</li>\n</ul>\n\
    <pre><code>%environment\n    export LC_ALL=C\n</code></pre>\n<p>TO-DO</p>\n<ul>\n\
    <li>\n<p>How to: Singularity</p>\n<ul>\n<li>Links to docs for installing</li>\n\
    <li>How to download an image from singularity hub</li>\n<li>How to download an\
    \ image fromm dockerhub\n<ul>\n<li><code>singularity pull docker://staphb/skesa</code></li>\n\
    </ul>\n</li>\n<li>How to take a recipe and build locally (sudo required)</li>\n\
    <li>How to run a Singularity container\n<ul>\n<li>Different ways to run - <code>singularity\
    \ exec [...]</code> or ./name-of-singularity.simg [...]</li>\n<li>Mounting DIRs\
    \ - default way (mount entire <code>$HOME</code> DIR), or way to mount a specific\
    \ DIR and not entire <code>$HOME</code> DIR</li>\n</ul>\n</li>\n</ul>\n</li>\n\
    <li>\n<p>Create SingularityHub account</p>\n<ul>\n<li>link to this repo</li>\n\
    <li>create autobuilds for each recipe</li>\n</ul>\n</li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1583250575.0
kavonrtep/SeqGrapheR:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: kavonrtep/SeqGrapheR
  latest_release: v0.5.0.2.4
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server_pitzer.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Pitzer batch job.</p>

    <h2>

    <a id="user-content-deprecated-application-warning" class="anchor" href="#deprecated-application-warning"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deprecated
    application warning</h2>

    <p>This application no longer works.  It raises an exception when users attempt
    to submit jobs.

    This is because we now have functionality to submit to multiple clusters and

    <a href="https://github.com/OSC/bc_osc_rstudio_server">the generic application</a>
    now submits

    to pitzer rendering this application useless.</p>

    <p>For historic versions, see the last released you can still view

    <a href="https://github.com/OSC/bc_osc_rstudio_server_pitzer/tree/v0.3.0">v0.3.0</a>
    as it was the last

    working version of this application.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1617867243.0
kbronik2017/nicpython36:
  data_format: 2
  description: modified version of nicMSlesions
  filenames:
  - Singularity
  full_name: kbronik2017/nicpython36
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-ms_cnn\" class=\"anchor\" href=\"#ms_cnn\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>MS_CNN</h1>\n\
    <p>[This is a modified version of nicMSlesions (<a href=\"https://github.com/NIC-VICOROB/nicMSlesions\"\
    >https://github.com/NIC-VICOROB/nicMSlesions</a>)]\n<br>\n<a href=\"CNN.jpeg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img height=\"300\" src=\"CNN.jpeg\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h1>\n<a id=\"user-content-this--version-support-additionally-the-following-functionalities\"\
    \ class=\"anchor\" href=\"#this--version-support-additionally-the-following-functionalities\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>This  version support additionally the following functionalities:</h1>\n\
    <dl>\n  <dt>(1) Runnable on a Mac system/computer</dt>\n  <dt>(2) Cold start and\
    \ warm start support:</dt>\n  <dd>- Allowing to re-create the architecture of\
    \ the model</dd>\n  <dd>- Allowing to use the saved weights of the model</dd>\n\
    \  <dd>- Allowing to use  the training configuration and avoiding to run preprocessing\
    \ again</dd>\n  <dd>- Allowing to resume training exactly where it left off(interrupting\
    \ the training is     \n    allowed throughout the training process)</dd>\n  <dd>-\
    \ Allowing to use pretrained model</dd>\n  <dt>(3) Supporting Python 3</dt>\n\
    \  <dt>(4) Integrated Tensorborad [to provide the measurements and visualisations\
    \ of TensorFlow execution (to understand, debug, and optimisation of  the TensorFlow\
    \ programs)]</dt>\n  <dt>(5) Checking whether a file or directory is relevant\
    \ for Training and Testing</dt> \n  <dt>(6) Easy HPC (High Performance Computing)\
    \ support</dt> \n  <dt>(7) Bias correction of masks using FSL</dt>\n  <dt>(8)\
    \ Registration, moving all images to the Flair, T1 or Standard space</dt>\n</dl>\n\
    <br>\n <p><a href=\"BR.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"><img\
    \ height=\"500\" src=\"BR.jpg\" style=\"max-width:100%;\"></a></p>\n \n<br>\n\
    \ <p><a href=\"note.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"><img height=\"\
    100\" src=\"note.jpg\" style=\"max-width:100%;\"></a></p>\n \n# Running the Program!\n\
    <p>This modified version can be run with or without a GUI (similar to original\
    \ version)</p>\n<p>After lunching the graphical user interface, user will need\
    \ to provide necessary information to start training/testing as follows:</p>\n\
    <br>\n <p><a href=\"GUI_NM.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img height=\"500\" src=\"GUI_NM.jpg\" style=\"max-width:100%;\"></a></p>\n \n\
    <h1></h1>\n<h1>\n<a id=\"user-content-running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\"\
    \ class=\"anchor\" href=\"#running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running the Program on the HPC cluster using NVIDIA GPUs(without any\
    \ additional library/dependency installation):</h1>\n<br>\n <p><a href=\"hpc.jpeg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img height=\"200\" src=\"hpc.jpeg\"\
    \ style=\"max-width:100%;\"></a></p>\n \n<p>First, user will need to be sure that\
    \ \"singularity\"\n<a href=\"https://singularity.lbl.gov/\" rel=\"nofollow\">https://singularity.lbl.gov/</a>\n\
    is available on local or remote machine.</p>\n<p>Then:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  - singularity pull docker://kbronik/ms_cnn_ucl:latest\
    \  </pre></div>\n<p>After running the above, a singularity image using docker\
    \ hub (docker://kbronik/ms_cnn_ucl:latest) will be generated:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>  - path to singularity//..///ms_cnn_ucl_latest.sif\
    \  </pre></div>\n<p>Finally:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  - singularity run --nv   (path to singularity)//..///ms_cnn_ucl_latest.sif\
    \  python  (path to nicpython36)/nic_train_network_batch.py (or other nic-python\
    \ code)</pre></div>\n<br>\n <p><a href=\"note_HPC.jpg\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img height=\"120\" src=\"note_HPC.jpg\" style=\"max-width:100%;\"\
    ></a></p>\n \n<h1>\n<a id=\"user-content-for-an-interactive-session\" class=\"\
    anchor\" href=\"#for-an-interactive-session\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>For an interactive session:</h1>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  - singularity shell   (path\
    \ to singularity)//..///ms_cnn_ucl_latest.sif </pre></div>\n<p>Then:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>  - <span class=\"pl-c1\">source</span>\
    \ activate idp\n  - python (path to nicpython36)/app.py</pre></div>\n<h1>\n<a\
    \ id=\"user-content-for-an-interactive-session-tensorflow-on-cpu-only\" class=\"\
    anchor\" href=\"#for-an-interactive-session-tensorflow-on-cpu-only\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>For\
    \ an interactive session (TensorFlow on CPU only):</h1>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  - singularity <span class=\"pl-c1\">exec</span>\
    \  docker://kbronik/ms-ucl-cnn-cpu:CPU_Latest  python  (path to nicpython36)/app.py\
    \ </pre></div>\n"
  stargazers_count: 1
  subscribers_count: 0
  topics:
  - convolutional-neural-networks
  - nicmslesions
  - nicmslesions-python3
  updated_at: 1587563855.0
kernsuite-debian/singularity-container:
  data_format: 2
  description: null
  filenames:
  - examples/arch/Singularity
  - examples/asciinema/Singularity
  - examples/busybox/Singularity
  - examples/shub/Singularity
  - examples/apps/Singularity
  - examples/apps/Singularity.cowsay
  - examples/ubuntu/Singularity
  - examples/docker/Singularity
  - examples/raspbian/Singularity
  - examples/centos/Singularity
  - examples/opensuse/Singularity
  - examples/scientific/Singularity
  - examples/self/Singularity
  full_name: kernsuite-debian/singularity-container
  latest_release: null
  readme: "<p>_Please note recent changes in the github repo branch structure.  If\
    \ you want\nto install a stable release of Singularity, please use a tag or a\
    \ <a href=\"https://github.com/singularityware/singularity/releases\">release\n\
    tarball</a>.  If you are\na developer who would like to contribute to Singularity\
    \ and you want to know\nwhich branch to submit your pull request to, please see\
    \ notes on the branch\nreorganization <a href=\"https://www.sylabs.io/2018/03/managing-singularity-branches/\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>Please also note that 2.6.0 is expected to\
    \ be the final feature release in the\n2.x series. While bug fixes may be added\
    \ via point releases (for example 2.6.1)\nno new features releases (for example\
    \ 2.7.0) are planned.</p>\n<p>Pull requests adding features to the 2.x series\
    \ will no longer be reviewed.<br>\nAny new features should be targeted to the\
    \ master branch (which used to be\ncalled development-3.0)._</p>\n<p><a href=\"\
    https://travis-ci.org/singularityware/singularity\" rel=\"nofollow\"><img src=\"\
    https://camo.githubusercontent.com/f9a86612d918b5d7b8615b4f1203222f491b2a672958652856370704a30742f9/68747470733a2f2f7472617669732d63692e6f72672f73696e67756c6172697479776172652f73696e67756c61726974792e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/singularityware/singularity.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<ul>\n<li><a href=\"CONTRIBUTING.md\">Guidelines\
    \ for Contributing</a></li>\n<li><a href=\".github/PULL_REQUEST_TEMPLATE.md\"\
    >Pull Request Template</a></li>\n<li><a href=\"LICENSE.md\">Project License</a></li>\n\
    <li><a href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\">Documentation</a></li>\n\
    <li><a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459\"\
    \ rel=\"nofollow\">Citation</a></li>\n</ul>\n<h1>\n<a id=\"user-content-singularity---enabling-users-to-have-full-control-of-their-environment\"\
    \ class=\"anchor\" href=\"#singularity---enabling-users-to-have-full-control-of-their-environment\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity - Enabling users to have full control of their environment.</h1>\n\
    <p>Starting a Singularity container \"swaps\" out the host\noperating system environment\
    \ for one the user controls!</p>\n<p>Let's say you are running Ubuntu on your\
    \ workstation or server, but you\nhave an application which only runs on Red Hat\
    \ Enterprise Linux 6.3.\nSingularity can instantly virtualize the operating system,\
    \ without having\nroot access, and allow you to run that application in its native\
    \ environment!</p>\n<h1>\n<a id=\"user-content-about\" class=\"anchor\" href=\"\
    #about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>About</h1>\n<p>Singularity is a container platform focused on supporting\
    \ \"Mobility of\nCompute\".</p>\n<p>Mobility of Compute encapsulates the development\
    \ to compute model where\ndevelopers can work in an environment of their choosing\
    \ and creation, and\nwhen the developer needs additional compute resources, this\
    \ environment\ncan easily be copied and executed on other platforms. Additionally,\
    \ as the\nprimary use case for Singularity is targeted towards computational portability.\n\
    Many of the barriers to entry of other container solutions do not apply to\nSingularity,\
    \ making it an ideal solution for users (both computational and\nnon-computational)\
    \ and HPC centers.</p>\n<h2>\n<a id=\"user-content-the-container\" class=\"anchor\"\
    \ href=\"#the-container\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>The Container</h2>\n<p>Singularity utilizes\
    \ container images, which means when you enter and\nwork within the Singularity\
    \ container, you are physically located inside\nof this image. The image grows\
    \ and shrinks in real time as you install\nor delete files within the container.\
    \ If you want to copy a container,\nyou copy the image.</p>\n<p>Using a single\
    \ image for the container format has added advantages\nespecially within the context\
    \ of HPC with large parallel file systems\nbecause all metadata operations within\
    \ the container occur within the\ncontainer image (and not on the metadata server!).</p>\n\
    <h2>\n<a id=\"user-content-mobility-of-compute\" class=\"anchor\" href=\"#mobility-of-compute\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Mobility of Compute</h2>\n<p>With Singularity, developers who like\
    \ to be able to easily control their\nown environment will love Singularity's\
    \ flexibility. Singularity does not\nprovide a pathway for escalation of privilege\
    \ (as do other container\nplatforms which are thus not appropriate for multi-tenant\
    \ resources) so\nyou must be able to become root on the host system (or virtual\
    \ machine)\nin order to modify the container.</p>\n<p>A Singularity container\
    \ can be launched in a variety of different ways\ndepending on what you wanted\
    \ to do with it. A simple method might be to\nlaunch an interactive shell within\
    \ the container image as follows:</p>\n<pre><code>[gmk@centos7-x64 demo]$ singularity\
    \ shell /tmp/Centos-7.img \ngmk@Centos-7.img demo&gt; echo \"Hello from within\
    \ the container\"\nHello from within the container\ngmk@Centos-7.img demo&gt;\
    \ whoami\ngmk\ngmk@Centos-7.img demo&gt; \n</code></pre>\n<p>And if you want to\
    \ do the same thing as root:</p>\n<pre><code>[gmk@centos7-x64 demo]$ sudo singularity\
    \ shell -w /tmp/Centos-7.img \nroot@Centos-7.img demo&gt; whoami\nroot\nroot@Centos-7.img\
    \ demo&gt; \n</code></pre>\n<p><em>note: By default, Singularity launches the\
    \ container image in read-only\nmode (so it can be easily launched in parallel).\
    \ The <code>-w</code> option used above\ntells Singularity to mount the image\
    \ in read/write mode, such that root\ncan now make changes to the container.</em></p>\n\
    <p>Additionally, relevant file systems on your host are shared, automatically,\n\
    within the context of your container. This can be demonstrated as\nfollows:</p>\n\
    <pre><code>[gmk@centos7-x64 demo]$ pwd\n/home/gmk/demo\n[gmk@centos7-x64 demo]$\
    \ echo \"world\" &gt; hello\n[gmk@centos7-x64 demo]$ singularity shell /tmp/Centos-7.img\
    \ \ngmk@Centos-7.img demo&gt; pwd\n/home/gmk/demo\ngmk@Centos-7.img demo&gt; cat\
    \ hello\nworld\n</code></pre>\n<p>Once the developer has completed their environment,\
    \ the image file can\nbe compressed and copied to any other system that has Singularity\
    \ installed.\nIf you do not have root on that system, you will not be able to\
    \ make any\nchanges to the image once on that system. But you will be able to\
    \ use the\ncontainer and access the data and files outside the container as\n\
    easily as you would on your development system or virtual machine.</p>\n<h2>\n\
    <a id=\"user-content-portability-of-singularity-container-images\" class=\"anchor\"\
    \ href=\"#portability-of-singularity-container-images\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Portability of\
    \ Singularity container images</h2>\n<p>Singularity images are highly portable\
    \ between Linux distributions (as\nlong as the binary format is the same). You\
    \ can generate your image on\nDebian or CentOS, and run it on Mint or Slackware.</p>\n\
    <p>Within a particular container, one can include their programs, data,\nscripts\
    \ and pipelines and thus port a workflow to any other architecture\ncompatible\
    \ Linux system or distribution.</p>\n<h2>\n<a id=\"user-content-bootstrapping-new-images\"\
    \ class=\"anchor\" href=\"#bootstrapping-new-images\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Bootstrapping\
    \ new images</h2>\n<p>Generally, when bootstrapping an image from scratch, you\
    \ must build it from\na compatible host. This is because you must use the distribution\
    \ specific\ntools it comes with (e.g. Red Hat does not provide Debian's debootstrap\
    \ by\ndefault). But once the image has been bootstrapped and includes the necessary\n\
    bits to be self-hosting (e.g. YUM on CentOS and apt-get on Debian/Ubuntu) then\n\
    the process of managing the container can be implemented from within the\ncontainer.</p>\n\
    <p>The process of building a bootstrap starts with a definition\nspecification.\
    \ The definition file describes how you want the operating\nsystem to be built,\
    \ what should go inside it and any additional\nmodifications necessary.</p>\n\
    <p>Here is an example of a very simple bootstrap definition file for CentOS:</p>\n\
    <pre><code>BootStrap: yum\nOSVersion: 7\nMirrorURL: http://mirror.centos.org/centos-%{OSVERSION}/%{OSVERSION}/os/$basearch/\n\
    Include: yum\n</code></pre>\n<p>Once you have created your bootstrap definition,\
    \ you can build your\nSingularity container image by first creating a blank image,\
    \ and then\nbootstrapping using your definition file:</p>\n<pre><code>[gmk@centos7-x64\
    \ demo]$ sudo singularity create /tmp/Centos-7.img\n[gmk@centos7-x64 demo]$ sudo\
    \ singularity bootstrap /tmp/Centos-7.img centos.def\n</code></pre>\n<p>From there\
    \ we can immediately start using the container:</p>\n<pre><code>[gmk@centos7-x64\
    \ demo]$ singularity exec /tmp/Centos-7.img cat /etc/redhat-release \nCentOS Linux\
    \ release 7.2.1511 (Core) \n[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-7.img\
    \ python --version\nPython 2.7.5\n[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-7.img\
    \ python hello.py \nhello world\n[gmk@centos7-x64 demo]$ \n</code></pre>\n<p>And\
    \ if I do this same process again, while changing the <strong>OSVersion</strong>\n\
    variable in the bootstrap definition to <strong>6</strong> (where previously it\
    \ was\nautomatically ascertained by querying the RPM database), we can\nessentially\
    \ build a CentOS-6 image in exactly the same manner as\nabove. Doing so reveals\
    \ this:</p>\n<pre><code>[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-6.img\
    \ cat /etc/redhat-release \nCentOS release 6.7 (Final)\n[gmk@centos7-x64 demo]$\
    \ singularity exec /tmp/Centos-6.img python --version\nPython 2.6.6\n[gmk@centos7-x64\
    \ demo]$ \n</code></pre>\n<p>And as expected, the Python version we now see is\
    \ what comes from by\ndefault in CentOS-6.</p>\n<h1>\n<a id=\"user-content-cite-as\"\
    \ class=\"anchor\" href=\"#cite-as\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Cite as:</h1>\n<pre><code>Kurtzer\
    \ GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for mobility\
    \ of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459\n\
    </code></pre>\n<p>We also have a Zenodo citation:</p>\n<pre><code>Kurtzer, Gregory\
    \ M.. (2016). Singularity 2.1.2 - Linux application and environment\ncontainers\
    \ for science. 10.5281/zenodo.60736\n\nhttp://dx.doi.org/10.5281/zenodo.60736\n\
    </code></pre>\n<h1>\n<a id=\"user-content-webpage\" class=\"anchor\" href=\"#webpage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Webpage</h1>\n<p>We have full documentation at <a href=\"https://www.sylabs.io/docs/\"\
    \ rel=\"nofollow\">https://www.sylabs.io/docs/</a>, and <a href=\"http://www.github.com/singularityware/singularityware.github.io\"\
    >welcome contributions</a>.</p>\n"
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1589975545.0
khanlab/tar2bids:
  data_format: 2
  description: null
  filenames:
  - Singularity.0.0.1b
  - Singularity.v0.0.4
  - Singularity.v0.0.5f
  - Singularity.v0.0.5e
  - Singularity.v0.0.4c
  - Singularity.v0.0.5g
  - Singularity.0.0.2d
  - Singularity.0.0.2e
  - Singularity.0.0.2h
  - Singularity.v0.0.5c
  - Singularity.0.0.1d
  - Singularity
  - Singularity.v0.0.5h
  - Singularity.v0.0.5
  - Singularity.v0.0.5a
  - Singularity.0.0.2c
  - Singularity.0.0.2g
  - Singularity.0.0.2a
  - Singularity.v0.0.3c
  - Singularity.v0.0.3b
  - Singularity.0.0.2f
  - Singularity.v0.0.5d
  - Singularity.v0.0.5b
  - Singularity.0.0.1c
  - Singularity.0.0.2
  - Singularity.v0.0.3d
  - Singularity.0.0.2b
  - Singularity.v0.0.3a
  - Singularity.v0.0.3
  - Singularity.v0.05a
  full_name: khanlab/tar2bids
  latest_release: v0.0.5g
  readme: "<h1>\n<a id=\"user-content-tar2bids\" class=\"anchor\" href=\"#tar2bids\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>tar2bids</h1>\n<pre><code>Runs dicom tarball(s) to BIDS conversion\
    \ using heudiconv\nUsage tar2bids  &lt;optional flags&gt;   &lt;in tar file(s)&gt;\n\
    \n Assumes tar files are generated by cfmm2tar or dicom2tar (to find PatientName)\n\
    \ If this is not the case, please use the -T option.\n\n Optional flags (must\
    \ appear before required arguments):\n\t-P &lt;patient name search string&gt;\
    \ :  default:  *_{subject}\n\t-T &lt;tar name search string&gt; :  uses tarfile\
    \ name instead of PatientName to search, e.g. '{subject}' if &lt;subject&gt;.tar\n\
    \t-o &lt;output_dir&gt; : default=./bids\n\t-N &lt;num parallel cores&gt; : default=0\
    \  (max cores)\n\t-h &lt;heuristic.py&gt; : default=cfmm_bold_rest.py\n\t-w &lt;tempdir&gt;\
    \  (--tempdir in heudiconv)\n\t-O \"&lt;additional heudiconv options&gt;\" : default=\n\
    \n Available heuristic files:\n\tcfmm_base.py\n\tcfmm_bold_rest.py\n\tcfmm_PS_PRC_3T.py\n\
    \tEPL14A_GE_3T.py\n\tEPL14B_3T.py\n\tGEvSE.py\n</code></pre>\n"
  stargazers_count: 4
  subscribers_count: 2
  topics: []
  updated_at: 1624334230.0
klm122/w2l:
  data_format: 2
  description: w2l
  filenames:
  - Singularity.gpu
  - Singularity
  full_name: klm122/w2l
  latest_release: null
  readme: '<h1>

    <a id="user-content-w2l" class="anchor" href="#w2l" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>w2l</h1>

    <p>w2l</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583870496.0
lamps24/neural_network_project:
  data_format: 2
  description: null
  filenames:
  - Singularity.gpu
  - Singularity
  - Singularity.devel
  full_name: lamps24/neural_network_project
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-csci5980\" class=\"anchor\" href=\"#csci5980\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>csci5980</h1>\n<p>Final project for CSci 5980: deep learning for automatic\
    \ music translation.</p>\n<p>Follow theses steps to install all package dependencies\
    \ for running the model:</p>\n<p>We first install software dependencies for manipulating\
    \ raw audio (<code>ffmpeg</code>):</p>\n<ol>\n<li>\n<p>Create a local software\
    \ directory\n<code>mkdir ~/software</code></p>\n</li>\n<li>\n<p>Install the NASM\
    \ assembler (dependency of ffmpeg):</p>\n</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span> <span class=\"\
    pl-k\">~</span>/software\nwget https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2\n\
    tar -xvf nasm-2.14.02.tar.bz2\n<span class=\"pl-c1\">cd</span> nasm-2.14.02\n\
    ./configure --prefix=<span class=\"pl-k\">~</span>/software/nasm/\nmake install\n\
    <span class=\"pl-k\">export</span> PATH=<span class=\"pl-smi\">$PATH</span>:<span\
    \ class=\"pl-k\">~</span>/software/nasm/bin/</pre></div>\n<ol start=\"3\">\n<li>Make\
    \ sure that NASM assembler installed correctly:</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>nasm -v</pre></div>\n<p>The output should look\
    \ something like:\n<code>NASM version 2.14.02 compiled on Mar 11 2020</code></p>\n\
    <ol start=\"4\">\n<li>Install ffmpeg:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/software\n\
    wget https://ffmpeg.org/releases/ffmpeg-4.2.2.tar.bz2\ntar -xvf ffmpeg-4.2.2.tar.bz2\n\
    <span class=\"pl-c1\">cd</span> ffmpeg-4.2.2\n./configure --prefix=<span class=\"\
    pl-k\">~</span>/software/ffmpeg/\nmake install\n<span class=\"pl-k\">export</span>\
    \ PATH=<span class=\"pl-smi\">$PATH</span>:<span class=\"pl-k\">~</span>/software/ffmpeg/bin/</pre></div>\n\
    <ol start=\"5\">\n<li>Make sure that ffmpeg installed correctly:</li>\n</ol>\n\
    <div class=\"highlight highlight-source-shell\"><pre>ffmpeg -version</pre></div>\n\
    <p>The output should look something like:</p>\n<pre><code>ffmpeg version 4.2.2\
    \ Copyright (c) 2000-2019 the FFmpeg developers\nbuilt with gcc 4.4.7 (GCC) 20120313\
    \ (Red Hat 4.4.7-23)\nconfiguration: --prefix=/home/csci5980/piehl008/software/ffmpeg/\n\
    libavutil      56. 31.100 / 56. 31.100\nlibavcodec     58. 54.100 / 58. 54.100\n\
    libavformat    58. 29.100 / 58. 29.100\nlibavdevice    58.  8.100 / 58.  8.100\n\
    libavfilter     7. 57.100 /  7. 57.100\nlibswscale      5.  5.100 /  5.  5.100\n\
    libswresample   3.  5.100 /  3.  5.100\n</code></pre>\n<ol start=\"6\">\n<li>Now,\
    \ we can make the virtual environment and install python packages.  First, create\
    \ the virtual environment by running:</li>\n</ol>\n<p><code>conda create --name\
    \ audio-proj python=3.7</code></p>\n<ol start=\"7\">\n<li>Next, install packages\
    \ by running</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/csci5980\nconda install\
    \ --name audio-proj --file requirements.txt --channel defaults --channel conda-forge</pre></div>\n\
    <p>(Note: this can take a while - and you need to say yes to installing everything\
    \ after it solves the environment)</p>\n<ol start=\"8\">\n<li>To activate the\
    \ virtual environment, you can now run <code>source activate audio-proj</code>.\
    \ Note: you should do this to test that you can activate the virtual evironment,\
    \ but you probably shouldn't run a lot unless you are submitting jobs to the queue.\
    \  If you want to use this virtual environment through the MSI notebooks, check\
    \ out the tutorial at <a href=\"https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\"\
    \ rel=\"nofollow\">https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html</a>.</li>\n\
    </ol>\n<h3>\n<a id=\"user-content-adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ class=\"anchor\" href=\"#adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Adding the Virtual Environment to Jupyter Notebooks</h3>\n<p>Now that\
    \ we have created the virtual environment, we can add it to the Jupyter notebook\
    \ kernels so that we can use the virtual environment through MSI's notebook server.\
    \ To do this, we have to add the kernel specifications to the known Jupyter kernels\
    \ for our user:</p>\n<ol start=\"9\">\n<li>If you haven't already, activate your\
    \ virtual environment by running <code>source activate audio-proj</code>. Then\
    \ enter</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>which\
    \ python</pre></div>\n<p>Your output should tell you where the python executable\
    \ for this virtual environment lives - the output for me displays <code>~/.conda/envs/audio-proj/bin/python</code>.\
    \  If you see something that looks like <code>/panfs/roc/msisoft/anaconda/anaconda3-2018.12/bin/python</code>,\
    \ go back and make sure that you have the virtual environment active and try again.\
    \ After you have an ouput that clearly has the name of the virtual environment\
    \ in the directory path (i.e. contains audio-proj in it), continue to the next\
    \ step.</p>\n<ol start=\"10\">\n<li>Now, we need to create the kernel configuration.\
    \ To do this run</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>mkdir <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj\n\
    nano <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj/kernel.json</pre></div>\n\
    <p>The nano command will open a very basic text editor that you can navigate with\
    \ the arrow keys. Enter the following:</p>\n<pre lang=\"text\"><code>{\n \"argv\"\
    : [\n  \"~/.conda/envs/audio-proj/bin/python\", #replace this with your path from\
    \ step 9 above! (and delete this comment)\n  \"-m\",\n  \"ipykernel_launcher\"\
    ,\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Audio Project\
    \ Kernel\",\n \"language\": \"python\"\n}\n</code></pre>\n<p>where you replace\
    \ the first line of the argv array with whatever executable path was output from\
    \ step 9 above (it likely will be identical to this). To exit the nano text editor,\
    \ type <code>Ctrl-x &lt;RETURN&gt;</code> and then type <code>Y &lt;RETURN&gt;</code>\
    \ to save the file.</p>\n<ol start=\"11\">\n<li>Now that you have saved the kernel\
    \ file, you should be able to go to <code>https://notebooks.msi.umn.edu/</code>\
    \ and when you click on the <code>New</code> tab to create a new file, you should\
    \ be able to select <code>Audio Project Kernel</code> as an available kernel to\
    \ run your newly created file in.</li>\n</ol>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1588946712.0
larosap/hackathon_intel_genci:
  data_format: 2
  description: hackathon_intel_genci
  filenames:
  - Sarek/Singularity
  - Sarek/ScLifeLab/Singularity
  full_name: larosap/hackathon_intel_genci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CNTdocker</h1>\n<h2>\n<a id=\"user-content-about\" class=\"anchor\"\
    \ href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>About</h2>\n<p>Dockerfiles to create Docker images\
    \ used by the CNT at the university of Pennsylvania</p>\n<h2>\n<a id=\"user-content-directory-contents-explanation\"\
    \ class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directory\
    \ contents explanation</h2>\n<h3>\n<a id=\"user-content-eeg\" class=\"anchor\"\
    \ href=\"#eeg\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>EEG</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common EEG analysis tools. Usually python 3</p>\n<p><strong>echobase</strong>:\
    \ Dockerfiles used to create images that can calculate functional connectivity\
    \ of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase\
    \ code is from <a href=\"https://github.com/andyrevell/paper001\">https://github.com/andyrevell/paper001</a></p>\n\
    <pre><code>Ubuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\n\
    scipy 1.4.1\n</code></pre>\n<h3>\n<a id=\"user-content-imaging\" class=\"anchor\"\
    \ href=\"#imaging\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>Imaging</strong>:</h3>\n<p>Dockerfiles used\
    \ to create images with common MRI analysis tools.</p>\n<pre><code>  Ubuntu 18.04\n\
    \  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n\
    \  FSL 6.0.1\n</code></pre>\n<h3>\n<a id=\"user-content-ml\" class=\"anchor\"\
    \ href=\"#ml\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>ml</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common machine learning tools.</p>\n<p><strong>wavenet</strong>:\
    \ Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet\
    \ paper\n<a href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\"\
    \ rel=\"nofollow\">Wavenet blog</a>\n<a href=\"https://arxiv.org/pdf/1609.03499.pdf\"\
    \ rel=\"nofollow\">Wavenet paper</a></p>\n<pre><code>  Ubuntu 18.04\n  tensorflow\
    \ 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n</code></pre>\n<p><strong>Tensorflow_2.1</strong>:\
    \ Dockerfile to create compatible dependencies to with tensorflow 2.1</p>\n<pre><code>\
    \  Ubuntu 18.04\n  tensorflow 2.1\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1573750055.0
lehtiolab/nf-deqms:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: lehtiolab/nf-deqms
  latest_release: null
  readme: '<h1>

    <a id="user-content-lehtiolabnf-deqms" class="anchor" href="#lehtiolabnf-deqms"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>lehtiolab/nf-deqms</h1>

    <p><strong>A small pipeline to re-run DEqMS on existing results</strong></p>

    <p><a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/0fcfc6847f4944e0c46cb62bb190c0110bafa56ce455c12dd23051df8d710a4a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413532302e30312e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A520.01.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/lehtiolab/nf-deqms" rel="nofollow"><img src="https://camo.githubusercontent.com/4068dc15ebffdfaa7d220510750dd7bcde75393d91d3fe2d05dc15190c515246/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6c656874696f6c61622f6e662d6465716d732e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/lehtiolab/nf-deqms.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>This workflow reruns DEqMS analysis on existing results, e.g. from the <a href="https://github.com/lehtiolab/ddamsproteomics">lehtiolab/ddamsproteomics</a>
    pipeline. It exists so one can use orthogonal sample groups (CTRL vs TREAT, old
    vs young) and rerun, or perhaps correct a mistake in the sample annotation, without
    having to re-search an entire set of spectra against a protein sequence database.</p>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h2>

    <a id="user-content-how-to-run" class="anchor" href="#how-to-run" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to run</h2>

    <ul>

    <li>install <a href="https://nextflow.io" rel="nofollow">Nextflow</a>

    </li>

    <li>install <a href="https://docs.docker.com/engine/installation/" rel="nofollow">Docker</a>,
    <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow">Singularity</a>,
    or <a href="https://conda.io/miniconda.html" rel="nofollow">Conda</a>

    </li>

    <li>run pipeline:</li>

    </ul>

    <pre><code>nextflow run lehtiolab/nf-deqms --proteins proteins.txt --peptides
    peptides.txt --genes genes.txt --ensg ensg.txt --sampletable samples.txt -profile
    standard,docker

    </code></pre>

    <p>You can leave out any accession that you do not have or are not interested
    in (e.g. <code>--ensg</code> in a Swissprot analysis).</p>

    <p>The lehtiolab/nf-deqms pipeline comes with documentation about the pipeline,
    found in the <code>docs/</code> directory:</p>

    <ul>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="https://nf-co.re/usage/troubleshooting" rel="nofollow">Troubleshooting</a></li>

    </ul>

    <p>There is more extensive documentation on the options inside the main.nf file.</p>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>lehtiolab/nf-deqms was originally written by Jorrit Boekel and tries to follow
    the <a href="https://nf-co.re" rel="nofollow">nf-core</a> best practices and templates.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1605692054.0
letaylor/docker-letaylor:
  data_format: 2
  description: Docker images
  filenames:
  - images/sc_qc_cluster/Singularity.sc_qc_cluster
  full_name: letaylor/docker-letaylor
  latest_release: null
  readme: '<h1>

    <a id="user-content-docker-letaylor" class="anchor" href="#docker-letaylor" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>docker-letaylor</h1>

    <p>This repo contains Docker images that are automatically built using Travis
    CI. It is not designed to scale to many images as each image is updated if any
    one image changes.</p>

    <h1>

    <a id="user-content-automatically-push-images-to-docker-hub-using-travis-ci" class="anchor"
    href="#automatically-push-images-to-docker-hub-using-travis-ci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Automatically push
    images to Docker Hub using Travis CI</h1>

    <h2>

    <a id="user-content-1-edit-config-files" class="anchor" href="#1-edit-config-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.
    Edit config files</h2>

    <p>Edit the following files:</p>

    <ul>

    <li>

    <code>.travis.yml</code> : alter <code>$IMAGE_NAME</code>.</li>

    </ul>

    <h2>

    <a id="user-content-2-give-travis-ci-access-to-upload-to-docer-hub" class="anchor"
    href="#2-give-travis-ci-access-to-upload-to-docer-hub" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>2. Give Travis CI access
    to upload to Docer Hub</h2>

    <p>Store both <code>$DOCKER_PASSWORD</code> and <code>$DOCKER_USERNAME</code>
    securely in on Travis CI. These are used for authentication.</p>

    <ol>

    <li>Login to the account you want Travis to use to upload on <a href="https://hub.docker.com/"
    rel="nofollow">hub.docker.com</a>.</li>

    <li>Click on your username on the top left and go to ''Account Settings''.</li>

    <li>On the left hand panel, go to ''Security'' and enter your password as requested.</li>

    <li>Now we''ll create an API token. Name it Travis CI.</li>

    <li>Create the token and copy it.</li>

    <li>Login to your account on <a href="https://travis-ci.org" rel="nofollow">travis-ci.org</a>
    and go to the repository that you want to add this automatic functionality to.</li>

    <li>On the right next to ''More options'' go to ''Settings'' in the hamburger
    menu.</li>

    <li>Add an environment variable with the name <code>DOCKER_PASSWORD</code> and
    give it the value of the API token that you copied from <a href="https://hub.docker.com/"
    rel="nofollow">hub.docker.com</a>.</li>

    <li>Add an environment variable with the name <code>DOCKER_USERNAME</code> and
    give it your <a href="https://hub.docker.com/" rel="nofollow">hub.docker.com</a>
    user name.</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611328575.0
lkirk/nb-env:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: lkirk/nb-env
  latest_release: null
  readme: '<h1>

    <a id="user-content-nb-env" class="anchor" href="#nb-env" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nb-env</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623793048.0
lxwgcool/singularity:
  data_format: 2
  description: singularity-recipe-share
  filenames:
  - Singularity.tensorflow-gpu-1.12.0
  full_name: lxwgcool/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-canopy" class="anchor" href="#coesra-singularity-canopy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-canopy</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 2
  subscribers_count: 0
  topics: []
  updated_at: 1574278904.0
maplesond/portcullis:
  data_format: 2
  description: Splice junction analysis and filtering from BAM files
  filenames:
  - Singularity
  full_name: maplesond/portcullis
  latest_release: 1.2.2
  readme: "<p><a href=\"doc/source/images/portcullis_logo.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"doc/source/images/portcullis_logo.png\"\
    \ alt=\"alt text\" title=\"Portcullis\" style=\"max-width:100%;\"></a></p>\n<h1>\n\
    <a id=\"user-content-portcullis\" class=\"anchor\" href=\"#portcullis\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Portcullis</h1>\n\
    <p><a href=\"https://github.com/maplesond/portcullis/releases\"><img src=\"https://camo.githubusercontent.com/b9c31b04d2671e6317cdfd9e4fdf893512936091302d1b1b56c99cb89ab43df7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7461672f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Version\" data-canonical-src=\"https://img.shields.io/github/tag/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://jenkins.sdlmapleson.net/job/portcullis/job/develop/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f696e3e0136cfb90f0e05f4f4e0a257ece7cd1e52ff19a0c8963b32df756d3a7/68747470733a2f2f6a656e6b696e732e73646c6d61706c65736f6e2e6e65742f6275696c645374617475732f69636f6e3f6a6f623d706f727463756c6c6973253246646576656c6f70\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://jenkins.sdlmapleson.net/buildStatus/icon?job=portcullis%2Fdevelop\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/ad4d6f3e16da4f0dddcd142fa3b6088042b13242787f5ad939d2db28282d3eb5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d627269676874677265656e2e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPL%20v3-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/issues\"\
    ><img src=\"https://camo.githubusercontent.com/d3bedf8e24750956939d66108f9ba197e72b83d1de8fc7305708ab2d67c20c17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues-raw/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Portcullis stands for PORTable CULLing\
    \ of Invalid Splice junctions from pre-aligned RNA-seq data.  It is known that\
    \ RNAseq mapping tools generate many invalid junction predictions, particularly\
    \ in deep datasets with high coverage over splice sites.  In order to address\
    \ this, instead for creating a new RNAseq mapper, with a focus on SJ accuracy\
    \ we created a tool that takes in a BAM file generated by an RNAseq mapper of\
    \ the user's own choice (e.g. Tophat2, Gsnap, STAR2 or HISAT2) as input (i.e.\
    \ it's portable).  It then, analyses and quantifies all splice junctions in the\
    \ file before, filtering (culling) those which are unlikely to be genuine.  Portcullis\
    \ output's junctions in a variety of formats making it suitable for downstream\
    \ analysis (such as differential splicing analysis and gene modelling) without\
    \ additional work.  Portcullis can also filter the original BAM file removing\
    \ alignments associated with <em>bad</em> junctions.  Both the filtered junctions\
    \ and BAM files are cleaner and more usable resources which can more effectively\
    \ be used to assist in downstream analyses such as gene prediction and genome\
    \ annotation.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h2>\n<p>We support multiple methods\
    \ for installing and running portcullis.  Hopefully your favourite container or\
    \ package manager is supported below.  If not let us know and we'll try to work\
    \ to get it integrated there.</p>\n<p><strong>Docker</strong></p>\n<p><a href=\"\
    https://hub.docker.com/r/maplesond/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/de966674ebe7a3dec2fed423683dd2c64e3630527fab6a691add53421292e384/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code># Keep in mind you need to mount\
    \ in any working directories to the container with the `-v` option.\n# Ideally,\
    \ mount these into the /data directory which is the container's working directory.\n\
    docker run --it --rm -v /abspath/to/data/on/host:/data maplesond/portcullis:stable\
    \ portcullis --help\n</code></pre>\n<p><strong>Singularity</strong></p>\n<pre><code>#\
    \ First download the container:\nsingularity pull --name portcullis.img shub://maplesond/portcullis:master\n\
    \n# Then to execute commands in the container:\nsingularity exec portcullis.img\
    \ portcullis --help\n</code></pre>\n<p><strong>Conda</strong></p>\n<p><a href=\"\
    https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/381a7739b713a2bae02343a6ac934de39148a7866dbf4e52b597391b2a07fd4b/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f6c61746573745f72656c656173655f646174652e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/latest_release_date.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/77a7c650d2675de3588df907d8e8aec11957abc95bcfd87d3b1b07f78a2bc4ec/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f706c6174666f726d732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/platforms.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/83781f462972e76ba4f2d046533fd48deb7cb72a0512481ff304f79c51bc01e3/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f646f776e6c6f6164732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/downloads.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code>conda install portcullis --channel=bioconda\n\
    </code></pre>\n<p><strong>Brew</strong></p>\n<pre><code>brew install brewsci/bio/portcullis\n\
    </code></pre>\n<p><strong>From source</strong></p>\n<p><a href=\"https://github.com/maplesond/portcullis/releases\"\
    ><img src=\"https://camo.githubusercontent.com/3885a69f4777ec0c98cf3d0bee17eb7ca3d3eb69bbf850df2f36895b80168ade/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d61706c65736f6e642f706f727463756c6c69732f746f74616c2e737667\"\
    \ alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/github/downloads/maplesond/portcullis/total.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>If you wish to install from source please\
    \ first confirm that first you have these dependencies are installed and configured:</p>\n\
    <ul>\n<li>\n<strong>GCC</strong> V4.8+</li>\n<li>\n<strong>autoconf</strong> V2.53+</li>\n\
    <li>\n<strong>automake</strong> V1.11+</li>\n<li><strong>make</strong></li>\n\
    <li>\n<strong>libtool</strong> V2.4.2+</li>\n<li><strong>zlib-dev</strong></li>\n\
    <li><strong>pthreads</strong></li>\n<li>\n<strong>boost-dev</strong> V1.52+</li>\n\
    <li>\n<strong>samtools</strong> V1.2+</li>\n<li>\n<strong>Python3-dev</strong>\
    \ V3.5+ (Make sure the following packages are installed: <em>pandas</em>, <em>matplotlib</em>,\
    \ <em>setuptools</em>, <em>sphinx</em>, <em>tabulate</em>)</li>\n</ul>\n<p>Then\
    \ proceed with the following steps:</p>\n<pre><code># Clone the repo\ngit clone\
    \ git@github.com:maplesond/portcullis.git\n\n# Move into repo directory\ncd portcullis\n\
    \n# Generate configure script\n./autogen.sh\n\n# Confirm dependencies and generate\
    \ makefiles\n# Adding --prefix &lt;dir&gt; will tell make install to put everything\
    \ in a \n# particular directory.  Default is /usr/local.\n./configure\n\n# Compile\
    \ (increasing -j will make it go faster!\nmake -j 2\n\n# Run some unit tests (you\
    \ can increase -j here too)\nmake -j 2 check\n\n# Install to prefix dir\nmake\
    \ install\n</code></pre>\n<p><strong>Common problems</strong></p>\n<ul>\n<li>\n\
    <p>Many system python installations do not come with the C API immediately available,\
    \ which prevents Portcullis from embedding python code.  We typically would recommend\
    \ installing anaconda3 as this would include the latest version of python, all\
    \ required python packages as well as the C API.  If you are running a debian\
    \ system and the C libraries are not available by default and you wish to use\
    \ the system python installation the you can install them using: <code>sudo apt-get\
    \ install python-dev</code>.  Also, if you have installed python to a custom location\
    \ please verify that the <em>bin</em> directors on the <em>PATH</em> environment\
    \ variable, and the lib (or lib64) directory is on the <em>LD_LIBRARY_PATH</em>\
    \ or <em>LD_RUN_PATH</em> as appropriate.</p>\n</li>\n<li>\n<p>If Portcullis is\
    \ failing at the <code>./autogen.sh</code> step you will likely need to install\
    \ autotools.  The following command should do this on MacOS: <code>brew install\
    \ autoconf automake libtool</code>.  On a debian system this can be done with:\
    \ <code>sudo apt-get install autoconf automake libtool</code>.</p>\n</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quickstart</h2>\n<p>After portcullis has been installed, the <code>portcullis</code>\
    \ executable should be available.  Typing <code>portcullis</code> or <code>portcullis\
    \ --help</code> at the command line will present you with the portcullis help\
    \ message.</p>\n<p>These modes are available:</p>\n<ul>\n<li>\n<strong>prep</strong>\
    \    - Prepares input data so that it is suitable for junction analysis</li>\n\
    <li>\n<strong>junc</strong>    - Calculates junction metrics for the prepared\
    \ data</li>\n<li>\n<strong>filter</strong>  - Separates alignments based on whether\
    \ they are likely to represent genuine splice junctions or not</li>\n<li>\n<strong>bamfilt</strong>\
    \ - Filters a BAM to remove any reads associated with invalid junctions</li>\n\
    <li>\n<strong>full</strong>    - Runs prep, junc, filter and optionally bamfilt\
    \ as a complete pipeline</li>\n</ul>\n<p>Typing <code>portcullis &lt;mode&gt;\
    \ --help</code> will bring up help and usage information specific to that mode.</p>\n\
    <p>In addition to portcullis, we provide a tool-suite for manipulating junction\
    \ files called junctools.  Typing <code>junctools --help</code> will provide you\
    \ with the program options.</p>\n<p>For much more information about portcullis'\
    \ capabilities and how to configure and run it, an online version of the manual\
    \ can be found here: <a href=\"https://portcullis.readthedocs.org/en/latest/\"\
    \ rel=\"nofollow\">https://portcullis.readthedocs.org/en/latest/</a>.</p>\n<h2>\n\
    <a id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Licensing</h2>\n\
    <p>GNU GPL V3.  See COPYING file for more details.</p>\n<h2>\n<a id=\"user-content-authors\"\
    \ class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Authors</h2>\n<ul>\n<li>Daniel\
    \ Mapleson</li>\n<li>Luca Venturini</li>\n<li>David Swarbreck</li>\n</ul>\n<p>See\
    \ AUTHORS file for more details.</p>\n<h2>\n<a id=\"user-content-acknowledgements\"\
    \ class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgements</h2>\n<p>Affiliation:\
    \ The Earlham Institute (EI)\nFunding: The Biotechnology and Biological Sciences\
    \ Research Council (BBSRC)</p>\n"
  stargazers_count: 23
  subscribers_count: 3
  topics:
  - portcullis
  - junction
  - splice-junctions
  - bam-files
  - filter
  updated_at: 1620067152.0
marcjwilliams1/rstudio_julia:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: marcjwilliams1/rstudio_julia
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <p><a href="https://singularity-hub.org/collections/5054" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for R studio server with Rv4.0 + julia v1.5.3</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611507934.0
marcjwilliams1/rstudiosrvrV4:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: marcjwilliams1/rstudiosrvrV4
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <p><a href="https://singularity-hub.org/collections/4911" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for R studio server with Rv4.0.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605122458.0
markxiao/freesurfer:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: markxiao/freesurfer
  latest_release: null
  readme: '<h1>

    <a id="user-content-freesurfer" class="anchor" href="#freesurfer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>freesurfer</h1>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1618603672.0
markxiao/fsl:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: markxiao/fsl
  latest_release: null
  readme: '<h1>

    <a id="user-content-fsl" class="anchor" href="#fsl" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>fsl</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618603672.0
masoudrezai/Singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity.13
  - Singularity.6
  - Singularity.3
  - Singularity.111
  - Singularity.10
  - Singularity.12
  - Singularity.4
  - Singularity.14
  - Singularity.7
  - Singularity.121
  - Singularity.8
  - Singularity.11
  - Singularity.5
  - Singularity.15
  - Singularity.9
  full_name: masoudrezai/Singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-octopussingularity-container\" class=\"anchor\"\
    \ href=\"#octopussingularity-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"https://github.com/luntergroup/octopus\"\
    >Octopus</a>\n<a href=\"https://github.com/hpcng/singularity\">Singularity</a>\
    \ container</h1>\n<p>Sylvain Schmitt\nApril 28, 2021</p>\n<p><strong>Bionformatics\
    \ software Octopus</strong></p>\n<p>Octopus is a mapping-based variant caller\
    \ that implements several\ncalling models within a unified haplotype-aware framework.\
    \ Octopus takes\ninspiration from particle filtering by constructing a tree of\
    \ haplotypes\nand dynamically pruning and extending the tree based on haplotype\n\
    posterior probabilities in a sequential manner. This allows octopus to\nimplicitly\
    \ consider all possible haplotypes at a given loci in\nreasonable time.</p>\n\
    <p>Octopus Version: 0.7.4</p>\n<p>[<a href=\"https://github.com/luntergroup/octopus\"\
    >https://github.com/luntergroup/octopus</a>]</p>\n<p>Singularity container based\
    \ on the recipe: Singularity.template.def</p>\n<p>Package installation using Miniconda3\
    \ V4.7.12</p>\n<p>Image singularity (V&gt;=3.3) is automatically test and built\
    \ and pushed\non the registry using\n<a href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\
    >test.yml</a>\n&amp;\n<a href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\
    >builder.yml</a></p>\n<p><strong>build</strong>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity build octopus.sif Singularity\nsingularity run octopus.sif\n\
    singularity <span class=\"pl-c1\">exec</span> octopus.sif octopus -h</pre></div>\n\
    <p><strong>pull</strong>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif</pre></div>\n\
    <p><strong>snakemake</strong>:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre>    <span class=\"pl-s1\">singularity</span>: \n        <span class=\"pl-s\"\
    >\"https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif\"\
    </span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623238419.0
matmu/vep:
  data_format: 2
  description: Recipe for VEP + Cache
  filenames:
  - Singularityfiles/Singularity.99-GRCh38-merged
  - Singularityfiles/Singularity.99-GRCh37-merged
  full_name: matmu/vep
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-containerized-variant-effect-predictor-vep--cache\"\
    \ class=\"anchor\" href=\"#containerized-variant-effect-predictor-vep--cache\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Containerized Variant Effect Predictor (VEP) + Cache</h1>\n<p><a href=\"\
    https://twitter.com/intent/tweet?hashtags=Ensembl,VEP,Singularity,Docker&amp;url=https://github.com/matmu/vep\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/90bc908826728c0e4261acfff5619fd732c7be2b2a00624fce6363c9a3623c90/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c\"\
    \ alt=\"Twitter\" data-canonical-src=\"https://img.shields.io/twitter/url/http/shields.io.svg?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>\_+ <a href=\"#Introduction\">Introduction</a>\
    \ <br>\n\_+ <a href=\"#Building-image-with-Singularity\">Building image with Singularity</a>\
    \ <br>\n\_+ <a href=\"#Run-VEP\">Run VEP</a> <br>\n\_\_\_\_|-- <a href=\"#More-options\"\
    >More options</a> <br>\n\_\_\_\_|-- <a href=\"#Examples\">Examples</a> <br>\n\_\
    + <a href=\"#Post-processing\">Post-processing</a> <br>\n\_\_\_\_|-- <a href=\"\
    #Split-VEP\">Split VEP</a> <br>\n\_\_\_\_|-- <a href=\"#Filtering-by-VEP-annotations\"\
    >Filtering by VEP annotations</a> <br>\n\_+ <a href=\"#VEP-plugins\">VEP plugins</a>\
    \ <br>\n\_+ <a href=\"#Build-and-run-VEP-with-Docker\">Build &amp; run VEP with\
    \ Docker</a> <br>\n\_+ <a href=\"#Acknowledgments\">Acknowledgements</a></p>\n\
    <h2>\n<a id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Introduction</h2>\n<p>This documentation describes the usage of the\
    \ Docker image at <a href=\"https://hub.docker.com/r/matmu/vep\" rel=\"nofollow\"\
    >https://hub.docker.com/r/matmu/vep</a> which contains the bioinformatics tool\
    \ <strong>Ensembl Variant effect predictor (VEP)</strong> for annotating genetic\
    \ variants. The image comes with</p>\n<ul>\n<li>Merged cache including RefSeq\
    \ and Ensembl transcripts (VEP parameter --merged required)</li>\n<li>Reference\
    \ genome and index</li>\n<li>Plugins (annotation data is not included)</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-available-versions\" class=\"anchor\" href=\"\
    #available-versions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Available versions</h2>\n<p><strong>Human:</strong>\
    \ <a href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38-merged.yml\"\
    ><img src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38-merged.yml/badge.svg\"\
    \ alt=\"103-GRCh38-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38.yml\"\
    ><img src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38.yml/badge.svg\"\
    \ alt=\"103-GRCh38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/101-GRCh38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/101-GRCh38/badge.svg\"\
    \ alt=\"101-GRCh38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh38/badge.svg\"\
    \ alt=\"100-GRCh38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh38-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh38-merged/badge.svg\"\
    \ alt=\"100-GRCh38-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh37/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh37/badge.svg\"\
    \ alt=\"100-GRCh37\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh37-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh37-merged/badge.svg\"\
    \ alt=\"100-GRCh37-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/99-GRCh38-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/99-GRCh38-merged/badge.svg\"\
    \ alt=\"99-GRCh38-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/99-GRCh37-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/99-GRCh37-merged/badge.svg\"\
    \ alt=\"99-GRCh37-merged\" style=\"max-width:100%;\"></a><br>\n<strong>Mouse:</strong>\
    \ <a href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCm39.yml\"\
    ><img src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCm39.yml/badge.svg\"\
    \ alt=\"103-GRCm39\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/101-GRCm38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/101-GRCm38/badge.svg\"\
    \ alt=\"101-GRCm38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCm38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCm38/badge.svg\"\
    \ alt=\"100-GRCm38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCm38-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCm38-merged/badge.svg\"\
    \ alt=\"100-GRCm38-merged\" style=\"max-width:100%;\"></a></p>\n<p>The term <code>merged</code>\
    \ refers to the merged Ensembl/RefSeq cache. To be consistent with the Ensembl\
    \ website, chose Ensembl cache only (i.e. without the term <code>merged</code>).\
    \ Examples for available versions are <strong>99-GRCh38</strong> (VEP 99 with\
    \ Ensembl cache for reference GRCh38) or <strong>99-GRh37-merged</strong> (VEP\
    \ 99 with Ensembl/Refseq cache for reference GRCh37).</p>\n<p>You can also visit\
    \ <a href=\"https://hub.docker.com/r/matmu/vep/tags\" rel=\"nofollow\">https://hub.docker.com/r/matmu/vep/tags</a>\
    \ to get a list of available versions.</p>\n<p><strong>Note:</strong> If you require\
    \ a container for a species not mentioned above, feel free to contact us or even\
    \ better, create an issue.</p>\n<h2>\n<a id=\"user-content-build-image-with-singularity\"\
    \ class=\"anchor\" href=\"#build-image-with-singularity\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Build image\
    \ with Singularity</h2>\n<div class=\"highlight highlight-source-shell\"><pre>singularity\
    \ build vep.<span class=\"pl-k\">&lt;</span>version<span class=\"pl-k\">&gt;</span>.simg\
    \ docker://matmu/vep:<span class=\"pl-k\">&lt;</span>version<span class=\"pl-k\"\
    >&gt;</span></pre></div>\n<p><code>&lt;version&gt;</code> is a tag representing\
    \ the Ensembl version and the species + version of the reference genome.</p>\n\
    <h2>\n<a id=\"user-content-run-vep\" class=\"anchor\" href=\"#run-vep\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run\
    \ VEP</h2>\n<p>To run VEP execute</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.<span class=\"pl-k\">&lt;</span>version<span\
    \ class=\"pl-k\">&gt;</span>.simg vep [options]</pre></div>\n<p>whereby <code>&lt;version&gt;</code>\
    \ is replaced by a respective version (see above), e.g. <code>99-CRCh38</code>.\
    \ It is essential to add the VEP option <code>--merged</code> when using an image\
    \ with merged Ensembl/Refseq cache. For species except homo sapiens, also the\
    \ parameter <code>--species</code> (e.g. <code>--species mus_musculus</code>),\
    \ has to be set as well.</p>\n<h3>\n<a id=\"user-content-more-options\" class=\"\
    anchor\" href=\"#more-options\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>More options</h3>\n<p>The options\
    \ for base cache/plugin directories, species and assembly are set to the right\
    \ values by default and do not need to be set by the user.</p>\n<p>Visit <a href=\"\
    http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html\" rel=\"nofollow\"\
    >http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html</a> for detailed\
    \ information about all VEP options. Detailed information about <strong>input/output\
    \ formats</strong> can be found at <a href=\"https://www.ensembl.org/info/docs/tools/vep/vep_formats.html#defaultout\"\
    \ rel=\"nofollow\">https://www.ensembl.org/info/docs/tools/vep/vep_formats.html#defaultout</a>.</p>\n\
    <h3>\n<a id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Examples</h3>\n\
    <h4>\n<a id=\"user-content-minimum-output-format-compressed-tab-delimited\" class=\"\
    anchor\" href=\"#minimum-output-format-compressed-tab-delimited\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Minimum\
    \ (output format: compressed tab delimited)</h4>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.100-GRCh38-merged.simg\
    \ vep --dir /opt/vep/.vep --merged --offline --cache --input_file <span class=\"\
    pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz] --output_file\
    \ <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.txt.gz\
    \ --tab --compress_output bgzip</pre></div>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.100-GRCh38.simg vep --dir\
    \ /opt/vep/.vep --offline --cache --input_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.vcf[.gz] --output_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.txt.gz --tab --compress_output bgzip</pre></div>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity <span class=\"\
    pl-c1\">exec</span> vep.100-GRCm38.simg vep --dir /opt/vep/.vep --offline --cache\
    \ --input_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz]\
    \ --output_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.txt.gz\
    \ --tab --compress_output bgzip -species mus_musculus</pre></div>\n<h4>\n<a id=\"\
    user-content-minimum-output-format-compressed-vcf\" class=\"anchor\" href=\"#minimum-output-format-compressed-vcf\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Minimum (output format: compressed vcf)</h4>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ vep.100-GRCh38.simg vep --dir /opt/vep/.vep --offline --cache --input_file <span\
    \ class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz]\
    \ --output_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf.gz\
    \ --vcf --compress_output bgzip</pre></div>\n<h4>\n<a id=\"user-content-full-annotation\"\
    \ class=\"anchor\" href=\"#full-annotation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Full annotation</h4>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ vep.100-GRCh38.simg vep --dir /opt/vep/.vep --offline --cache --input_file <span\
    \ class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz]\
    \ --output_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf.gz\
    \ --vcf --compress_output bgzip --everything --nearest symbol        </pre></div>\n\
    <h2>\n<a id=\"user-content-post-processing\" class=\"anchor\" href=\"#post-processing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Post-processing</h2>\n<h3>\n<a id=\"user-content-split-vep\" class=\"\
    anchor\" href=\"#split-vep\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Split VEP</h3>\n<p>There is a plugin for <code>bcftools</code>\
    \ that allows to split VEP annotations as well as sample information in a VCF\
    \ file and convert it to a text file: <a href=\"http://samtools.github.io/bcftools/howtos/plugin.split-vep.html\"\
    \ rel=\"nofollow\">http://samtools.github.io/bcftools/howtos/plugin.split-vep.html</a>.</p>\n\
    <h3>\n<a id=\"user-content-filtering-by-vep-annotations\" class=\"anchor\" href=\"\
    #filtering-by-vep-annotations\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Filtering by VEP annotations</h3>\n\
    <p>If you chose to output the VEP annotations as text file, any command line tool\
    \ (e.g. <code>awk</code>) or even <code>Excel</code> can be used for filtering\
    \ the results. For VCF files, the image includes a VEP filtering script which\
    \ can be executed by</p>\n<div class=\"highlight highlight-source-shell\"><pre>singularity\
    \ <span class=\"pl-c1\">exec</span> vep.<span class=\"pl-k\">&lt;</span>version<span\
    \ class=\"pl-k\">&gt;</span>.simg filter_vep [options]</pre></div>\n<h4>\n<a id=\"\
    user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Options</h4>\n\
    <p>Visit <a href=\"https://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html\"\
    \ rel=\"nofollow\">https://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html</a>\
    \ for detailed info about available options.</p>\n<h4>\n<a id=\"user-content-filtering-examples\"\
    \ class=\"anchor\" href=\"#filtering-examples\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Filtering examples</h4>\n<h5>\n\
    <a id=\"user-content-filter-for-rare-variants\" class=\"anchor\" href=\"#filter-for-rare-variants\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Filter for rare variants</h5>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.<span class=\"pl-k\">&lt;</span>version<span\
    \ class=\"pl-k\">&gt;</span>.simg filter_vep --input_file <span class=\"pl-k\"\
    >&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf --output_file <span class=\"\
    pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.filtered.vcf --only_matched\
    \ --filter <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>(IMPACT is HIGH\
    \ or IMPACT is MODERATE or IMPACT is LOW) and (BIOTYPE is protein_coding) and\
    \ ((PolyPhen &gt; 0.446) or (SIFT &lt; 0.05)) and (EUR_AF &lt; 0.001 or gnomAD_NFE_AF\
    \ &lt; 0.001 or (not EUR_AF and not gnomAD_NFE_AF))<span class=\"pl-pds\">\"</span></span>\
    \ </pre></div>\n<h2>\n<a id=\"user-content-vep-plugins\" class=\"anchor\" href=\"\
    #vep-plugins\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>VEP plugins</h2>\n<p>VEP allows several other annotations\
    \ sources (aka Plugins). Their respective Perl modules are included in the image,\
    \ the annotation files have to be added seperately, however. The list of plugins\
    \ as well as instructions on how to download and pre-process the annotation files\
    \ can be found at: <a href=\"http://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html\"\
    \ rel=\"nofollow\">http://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html</a>.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity <span class=\"\
    pl-c1\">exec</span> vep.100-GRCh38-merged.simg vep --dir /opt/vep/.vep --merged\
    \ --offline --cache --input_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.vcf[.gz] --output_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.txt.gz --tab --compress_output bgzip --plugin CADD,/path/to/ALL.TOPMed_freeze5_hg38_dbSNP.tsv.gz</pre></div>\n\
    <h2>\n<a id=\"user-content-build-and-run-vep-with-docker\" class=\"anchor\" href=\"\
    #build-and-run-vep-with-docker\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Build and run VEP with Docker</h2>\n\
    <p>To pull the image and run the container with Docker use</p>\n<pre><code>docker\
    \ run matmu/vep:&lt;version&gt; vep [options]\n</code></pre>\n<p>Unlike Singularity,\
    \ the directories of <strong>Plugin</strong> annotation files (e.g. <code>/path/to/dir</code>)\
    \ have to be explicitely bound to a target directory (e.g. <code>/opt/data</code>)\
    \ within the container with option <code>-v</code>:</p>\n<pre><code>docker run\
    \ -v /path/to/dir:/opt/data matmu/vep:&lt;version&gt; vep [options]\n</code></pre>\n\
    <h2>\n<a id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Acknowledgments</h2>\n<p>This document has been created by <strong>Julia\
    \ Remes</strong> &amp; <strong>Matthias Munz</strong>, <strong>University of L\xFC\
    beck</strong>, <strong>Germany</strong>.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1614861605.0
maxpkatz/singularity_image_files:
  data_format: 2
  description: null
  filenames:
  - Singularity.cosmic_tagging_tf_2010
  full_name: maxpkatz/singularity_image_files
  latest_release: null
  readme: ''
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1609779903.0
mbhall88/Longitude_pipeline:
  data_format: 2
  description: Pipeline for analysing M. tuberculosis nanopore reads and getting drug
    susceptibility information.
  filenames:
  - containers/recipes/Singularity.mykrobe
  - containers/recipes/Singularity.nanoporeqc
  full_name: mbhall88/Longitude_pipeline
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>containers</h1>

    <p>recipes of Singularity</p>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics:
  - nanopore
  - tuberculosis
  - bioinformatics-pipeline
  updated_at: 1581419079.0
mbhall88/Singularity_recipes:
  data_format: 2
  description: Repository with all my Singularity recipes
  filenames:
  - Singularity.template
  - nanopore/Singularity.guppygpu
  - nanopore/Singularity.puntseq
  - nanopore/Singularity.guppycpu
  - nanopore/Singularity.nanoporeqc
  - nanopore/Singularity.taiyaki
  - assembly/Singularity.racon
  - assembly/Singularity.canu
  - assembly/Singularity.polish
  - assembly/Singularity.haslr
  - recipes/Singularity.bracken
  - recipes/Singularity.mccortex
  - recipes/Singularity.krakenuniq
  - recipes/Singularity.nanopolish
  - recipes/Singularity.f5pack
  - recipes/Singularity.clustalo
  - recipes/Singularity.porechop
  - recipes/Singularity.deepbinnergpu
  - recipes/Singularity.mykrobe
  - recipes/Singularity.deepbinnercpu
  - recipes/Singularity.vcftools
  - recipes/Singularity.samtools
  - recipes/Singularity.minimap2
  - recipes/Singularity.kraken2
  - recipes/Singularity.ngm
  - recipes/Singularity.pistis
  - recipes/Singularity.filtlong
  - recipes/Singularity.mummer
  - recipes/Singularity.bcftools
  - recipes/Singularity.centrifuge
  - recipes/Singularity.spades
  - recipes/Singularity.pandora
  - recipes/Singularity.medaka
  - recipes/Singularity.pilon
  full_name: mbhall88/Singularity_recipes
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipes" class="anchor" href="#singularity-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipes</h1>

    <hr>

    <p><strong>NOTE</strong>: I am in the process of porting these recipes and images
    to

    singularity version 3 and <a href="https://cloud.sylabs.io/library" rel="nofollow">Singularity
    Library</a>. Any recipes that

    have been ported will have their images now hosted <a href="https://cloud.sylabs.io/library/mbhall88/default"
    rel="nofollow">here</a>.</p>

    <hr>

    <p><a href="https://singularity-hub.org/collections/685" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a><br>

    This repository is for all of my <a href="https://www.sylabs.io/singularity/"
    rel="nofollow">Singularity</a> recipes and is linked to Singularity

    Hub where they are built and available.</p>

    <p>You will find all the programs I have made Singularity recipes for inside the

    <a href="https://github.com/mbhall88/Singularity_recipes/tree/master/recipes"><code>recipes</code>
    directory</a>.</p>

    <p>I try to keep them up-to-date in terms of versions, but if you notice anything

    out of date, feel free to put in a pull request. Additionally, feel free to

    contribute recipes for programs I don''t already have in here, or request one
    and

    if I have time I will try and make one.</p>

    <h2>

    <a id="user-content-getting-a-pre-built-container" class="anchor" href="#getting-a-pre-built-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    a pre-built container</h2>

    <p>If you would like a pre-built version of any of the containers you can pull
    it

    down from <a href="https://www.singularity-hub.org/collections/685/usage" rel="nofollow">Singularity
    Hub</a>

    like so</p>

    <div class="highlight highlight-source-shell"><pre>tool=<span class="pl-s"><span
    class="pl-pds">"</span>samtools<span class="pl-pds">"</span></span>

    singularity pull --name <span class="pl-s"><span class="pl-pds">"</span><span
    class="pl-smi">$tool</span><span class="pl-pds">"</span></span>.simg shub://mbhall88/Singularity_recipes:<span
    class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$tool</span><span
    class="pl-pds">"</span></span></pre></div>

    <p>Note: The <code>--force</code> option will override any container that exists
    in that

    location with the same name. If you are trying to pull a container to update a

    pre-existing one, then the <code>--force</code> flag is necessary.</p>

    <p>For a full list of usage examples, check out the <a href="https://www.singularity-hub.org/collections/685/usage"
    rel="nofollow">Singularity Hub usage docs</a>.</p>

    <h2>

    <a id="user-content-template" class="anchor" href="#template" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Template</h2>

    <p>I have also included a template <a href="https://github.com/mbhall88/Singularity_recipes/blob/master/Singularity.template"><code>Singularity.template</code></a>

    which can be used for building up your own recipes.</p>

    '
  stargazers_count: 13
  subscribers_count: 1
  topics:
  - singularity-containers
  - singularity-hub
  - singularity
  - nanopore
  - bioinformatics
  updated_at: 1614220433.0
mbhall88/eipp-2019-singularity:
  data_format: 2
  description: Singularity group project for EIPP 2019
  filenames:
  - recipes/Singularity.snakemake
  - recipes/Singularity.nanopolish
  - recipes/Singularity.shellcheck
  - recipes/Singularity.flye
  - recipes/Singularity.fun
  - recipes/Singularity.template
  - recipes/Singularity.jupyter
  - recipes/sandbox-dev/Singularity.nanopolish
  full_name: mbhall88/eipp-2019-singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-eipp-2019-singularity-group-project\" class=\"\
    anchor\" href=\"#eipp-2019-singularity-group-project\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>EIPP 2019 Singularity\
    \ group project</h1>\n<p><a href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/4457c96be6834fd67756b9c0eab298334a5b948ab2234fbea89648e221e66af1/68747470733a2f2f73796c6162732e696f2f6775696465732f322e362f61646d696e2d67756964652f5f7374617469632f6c6f676f2e706e67\"\
    \ height=\"100\" data-canonical-src=\"https://sylabs.io/guides/2.6/admin-guide/_static/logo.png\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/0d44589c34845e74b1d32ae082d1f190828469fdc700fd026f3e4935eba669d2/68747470733a2f2f736369656e63652e736369656e63656d61672e6f72672f636f6e74656e742f7363692f3238372f353435372f313430312f46312e6d656469756d2e676966\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0d44589c34845e74b1d32ae082d1f190828469fdc700fd026f3e4935eba669d2/68747470733a2f2f736369656e63652e736369656e63656d61672e6f72672f636f6e74656e742f7363692f3238372f353435372f313430312f46312e6d656469756d2e676966\"\
    \ height=\"100\" data-canonical-src=\"https://science.sciencemag.org/content/sci/287/5457/1401/F1.medium.gif\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/3751\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<ul>\n<li>\n<a href=\"#introduction-to-containers\"\
    >Introduction to containers</a>\n<ul>\n<li><a href=\"#tldr\">tl;dr</a></li>\n\
    </ul>\n</li>\n<li><a href=\"#what-can-i-do-with-a-container\">What can I do with\
    \ a container?</a></li>\n<li>\n<a href=\"#how-do-i-get-a-container\">How do I\
    \ get a container?</a>\n<ul>\n<li>\n<a href=\"#remote\">Remote</a>\n<ul>\n<li><a\
    \ href=\"#docker-hub\">Docker Hub</a></li>\n<li><a href=\"#singularity-hub\">Singularity\
    \ Hub</a></li>\n<li><a href=\"#singularity-library\">Singularity Library</a></li>\n\
    <li><a href=\"#quay-and-biocontainers\">Quay and BioContainers</a></li>\n</ul>\n\
    </li>\n<li><a href=\"#build-locally\">Build locally</a></li>\n</ul>\n</li>\n<li>\n\
    <a href=\"#exercise-1\">Exercise 1</a>\n<ul>\n<li><a href=\"#task-1\">Task 1</a></li>\n\
    <li><a href=\"#task-2\">Task 2</a></li>\n<li><a href=\"#task-3\">Task 3</a></li>\n\
    </ul>\n</li>\n<li><a href=\"#sandbox-development\">Sandbox development</a></li>\n\
    <li><a href=\"#exercise-2\">Exercise 2</a></li>\n<li>\n<a href=\"#run-and-serving-applications\"\
    ><code>run</code> and serving applications</a>\n<ul>\n<li><a href=\"#singularity-run\"\
    ><code>singularity run</code></a></li>\n<li><a href=\"#serving-applications\"\
    >Serving applications</a></li>\n</ul>\n</li>\n<li><a href=\"#workflow-management-systems\"\
    >Workflow management systems</a></li>\n<li><a href=\"#programs-requiring-gpus\"\
    >Programs requiring GPUs</a></li>\n<li><a href=\"#bonus\">Bonus</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-introduction-to-containers\" class=\"anchor\" href=\"\
    #introduction-to-containers\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Introduction to containers</h2>\n\
    <h3>\n<a id=\"user-content-tldr\" class=\"anchor\" href=\"#tldr\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>tl;dr</h3>\n\
    <p>A container is a standard unit of software that packages up code and all its\n\
    dependencies, so the application runs quickly and reliably from one computing\n\
    environment to another. That includes files, environment variables, dependencies\
    \ and\nlibraries.</p>\n<p>For those who would like more detailed information about\
    \ what containers are, please\nrefer to <a href=\"https://github.com/titansmc/singularity-training-2019/raw/master/1.-singularity-training-what-are-containers.odp\"\
    >this fantastic slide deck from Josep Moscardo</a>.</p>\n<h2>\n<a id=\"user-content-what-can-i-do-with-a-container\"\
    \ class=\"anchor\" href=\"#what-can-i-do-with-a-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>What can\
    \ I do with a container?</h2>\n<p>In it's most basic form, you can execute a software\
    \ program, via a container, even\nthough you may not have that program installed\
    \ on the system you are running it on.</p>\n<p>Examples are the best teachers!</p>\n\
    <p>Firstly, let's clone this repository (and call it <code>eipp-singularity</code>)\
    \ as we will use some files from it throughout this\nproject.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>project=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>eipp-singularity<span class=\"pl-pds\">\"</span></span>\ngit\
    \ clone https://github.com/mbhall88/eipp-2019-singularity.git <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$project</span><span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-c1\">cd</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$project</span><span class=\"\
    pl-pds\">\"</span></span></pre></div>\n<p>Now, there is a <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\"\
    \ rel=\"nofollow\">BAM</a> file in the repository that we sadly can't view as\
    \ we do not have <a href=\"https://github.com/samtools/samtools\"><code>samtools</code></a>\
    \ installed (let's pretend). Thanks to Singularity we\ndon't have to worry about\
    \ trying to install <code>samtools</code> and can instead use a pre-built container\
    \ to view our BAM file with <code>samtools</code>.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>img=<span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>docker://quay.io/biocontainers/samtools:1.9--h10a08f8_12<span class=\"\
    pl-pds\">\"</span></span>\nsingularity <span class=\"pl-c1\">exec</span> <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$img</span><span\
    \ class=\"pl-pds\">\"</span></span> samtools view data/toy.bam</pre></div>\n<p>Magic\
    \ <g-emoji class=\"g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\
    >\u2728</g-emoji></p>\n<p>So what's going on here?</p>\n<p>Let's work our way\
    \ through the command.</p>\n<ol>\n<li>\n<a href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#executing-commands\"\
    \ rel=\"nofollow\"><code>singularity exec</code></a> tells Singularity to execute\
    \ a given command inside a\ngiven container.</li>\n<li>\n<code>\"$img\"</code>\
    \ specifies the container for Singularity to operate on. We will look at this\
    \ component in more detail later.</li>\n<li>\n<code>samtools view data/toy.bam</code>\
    \ This is the command we want Singularity to execute inside the container. Notice\
    \ how we can specify files that exist on our local file system?!</li>\n</ol>\n\
    <h2>\n<a id=\"user-content-how-do-i-get-a-container\" class=\"anchor\" href=\"\
    #how-do-i-get-a-container\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>How do I get a container?</h2>\n<h3>\n<a id=\"\
    user-content-remote\" class=\"anchor\" href=\"#remote\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Remote</h3>\n\
    <p>In the above example, the container we used for <code>samtools</code> was remote.</p>\n\
    <p>Remote containers are containers that have been pre-built and stored in \"\
    the cloud\".\nThere are many benefits to this kind of set up. Firstly, it makes\
    \ sharing containers\neasy. Secondly, it saves users (and yourself) a lot of time\
    \ in the future. As the\ncontainer is pre-built, we don't need to spend time waiting\
    \ for the build to happen (more on this later). The only wait time we have is\
    \ for the download of the remote\ncontainer to finish. Lastly, remote services\
    \ are convenient for building images if we\ndon't have <code>sudo</code> access\
    \ on the machine we are using. We will look at building containers\nlocally very\
    \ soon, but for now, it suffices to know that to build them locally, you need\n\
    <code>sudo</code> access.</p>\n<p>Now you might have noticed in the example above\
    \ that the <a href=\"https://en.wikipedia.org/wiki/Uniform_Resource_Identifier\"\
    \ rel=\"nofollow\">URI</a> for the <code>samtools</code>\ncontainer has the work\
    \ 'docker' in it. This is one of the coolest things about Singularity: <a href=\"\
    https://sylabs.io/guides/3.4/user-guide/singularity_and_docker.html\" rel=\"nofollow\"\
    >it can convert Docker containers into Singularity containers</a>! We now have\n\
    access to any Docker container <em>plus</em> any Singularity container.</p>\n\
    <p>Let's take a look at some remote container registries in a little more detail\
    \ and see\nhow we can use containers from them.</p>\n<h4>\n<a id=\"user-content-docker-hub\"\
    \ class=\"anchor\" href=\"#docker-hub\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"https://hub.docker.com/\"\
    \ rel=\"nofollow\">Docker Hub</a>\n</h4>\n<p>The official registry for Docker\
    \ containers. Let's search for <a href=\"http://conda.pydata.org/miniconda.html\"\
    \ rel=\"nofollow\"><code>miniconda3</code></a> on <a href=\"https://hub.docker.com/\"\
    \ rel=\"nofollow\">Docker Hub</a> and select the option <a href=\"https://hub.docker.com/r/continuumio/miniconda3\"\
    \ rel=\"nofollow\"><code>continuumio/miniconda3</code></a>. On the right, there\
    \ is a section <strong>Docker Pull Command</strong>. It\nsays <code>docker pull\
    \ continuumio/miniconda3</code>. If we were using Docker, this would be the\n\
    command we would use to pull that container to our local machine. To use it in\
    \ Singularity\nwe need to tweak it just a little. For <code>miniconda3</code>\
    \ we would use the URI <code>docker://continuumio/miniconda3</code>. As we can\
    \ see, you need to add <code>docker://</code> to the\nbeginning of the <code>repository/tag</code>.<br>\n\
    We can go one step further and unlock another great benefit of using remote containers.\
    \ We're reproducibility warriors, right?! Of course, we are. So let's be specific\n\
    about the version of <code>miniconda3</code> we want to use. On the <a href=\"\
    https://hub.docker.com/r/continuumio/miniconda3\" rel=\"nofollow\"><code>miniconda3</code>\
    \ Docker Hub page</a>, select the <a href=\"https://hub.docker.com/r/continuumio/miniconda3/tags\"\
    \ rel=\"nofollow\"><strong>Tags</strong></a> heading. On this\npage, we see a\
    \ whole bunch of different versions of <code>miniconda3</code> we can choose from.\
    \ Any\nversion of this container that has been built is kept. If we wanted to\
    \ use version <code>4.6.14</code>, then all we have to do is append this, with\
    \ a colon, to our original URI</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker://continuumio/miniconda3:4.6.14</pre></div>\n<p>Now, as we saw earlier,\
    \ we can directly execute a container from it's URI. However, it\nis likely you\
    \ may want to use a container multiple times. In these circumstances, it is\n\
    more \"economical\" to pull a copy of the container onto our local machine, so\
    \ we don't\nhave to try and retrieve it from the registry each time (images are\
    \ usually cached though). To pull the <code>miniconda3</code> container from Docker\
    \ Hub, we use Singularity's <a href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#download-pre-built-images\"\
    \ rel=\"nofollow\"><code>pull</code></a>\ncommand and optionally specify a name.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity pull docker://continuumio/miniconda3:4.6.14</pre></div>\n\
    <p>The above command will pull the container into the current directory and name\
    \ it <code>miniconda3-4.6.14.sif</code>. If we wanted to call it instead <code>miniconda3.sif</code>\
    \ we would use the <code>--name</code> argument</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull --name miniconda3.sif docker://continuumio/miniconda3:4.6.14</pre></div>\n\
    <p>When we want to use this image again in the future, rather than specifying\
    \ the URI we\njust point Singularity at our local copy</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ miniconda3.sif <span class=\"pl-k\">&lt;</span>command<span class=\"pl-k\">&gt;</span></pre></div>\n\
    <h4>\n<a id=\"user-content-singularity-hub\" class=\"anchor\" href=\"#singularity-hub\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity\
    \ Hub</a>\n</h4>\n<p>Set up and maintained by a collaboration between Stanford\
    \ University and Singularity,\nSingularity Hub is Singularity's \"semi-official\"\
    \ version of Docker Hub. We will dig\ninto how to set this up for yourself a little\
    \ later in <a href=\"#Exercise-1\">Exercise 1</a>.</p>\n<p>As with Docker Hub,\
    \ we can search for containers uploaded by users and then use them in\nthe same\
    \ way. However, it will ask us to log in using GitHub first. Login with your\n\
    GitHub account and then search for <a href=\"https://github.com/DaehwanKimLab/centrifuge\"\
    ><code>centrifuge</code></a>. The first result should\nbe for <a href=\"https://singularity-hub.org/collections/685\"\
    \ rel=\"nofollow\"><code>mbhall88/Singularity_recipes</code></a> - click on this.\
    \ This will take\nyou to a page listing all of the Singularity containers I maintain\
    \ in a <a href=\"https://github.com/mbhall88/Singularity_recipes\">recipes repository\
    \ on GitHub</a>. Scroll through these and look for the\n<a href=\"https://singularity-hub.org/containers/5461\"\
    \ rel=\"nofollow\"><code>centrifuge</code></a> one and then click on the green\
    \ <strong>Complete</strong> button.\nThe resulting screen will have the Build\
    \ Specs (more on this soon) plus a bunch of\nbuild metrics. Additionally, at the\
    \ top of this screen, you will see the core piece of\nthe URI that we need: <code>mbhall88/Singularity_recipes:centrifuge</code>.\
    \ So to use this container,\nwe add the <code>shub://</code> scheme to the front.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>uri=<span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>shub://mbhall88/Singularity_recipes:centrifuge<span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity pull --name centrifuge.sif <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$uri</span><span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity <span class=\"pl-c1\">exec</span>\
    \ centrifuge.sif centrifuge --help</pre></div>\n<p>Due to Singularity Hub be generously\
    \ hosted as no charge by Google Cloud, and also due\nto a recent malicious attack,\
    \ it is <a href=\"https://singularityhub.github.io/singularityhub-docs/docs/interact\"\
    \ rel=\"nofollow\">recommended</a> to <code>pull</code> containers from Singularity\
    \ and\nthen execute them, rather than running directly from the URI.</p>\n<p>Again,\
    \ we can go one step further and specify a particular build of the container we\n\
    want to use. In the <strong>Build Metrics</strong> section, there is a field called\
    \ 'Version (file hash)'. For reproducibility purposes, it is advisable to use\
    \ this hash as it makes it\nclear to others who may read your code exactly which\
    \ container you used. So to pull the\nlatest centrifuge container, we would do\
    \ the following (<strong>don't run this if you already\npulled the container above</strong>).</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>hash=<span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>13bc12f41b20001f17e6f8811dc3eeea<span class=\"\
    pl-pds\">\"</span></span>\nuri=<span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>shub://mbhall88/Singularity_recipes:centrifuge@<span class=\"pl-smi\">${hash}</span><span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity pull --name centrifuge.sif <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$uri</span><span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity <span class=\"pl-c1\">exec</span>\
    \ centrifuge.sif centrifuge --help</pre></div>\n<h4>\n<a id=\"user-content-singularity-library\"\
    \ class=\"anchor\" href=\"#singularity-library\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"https://cloud.sylabs.io/library\"\
    \ rel=\"nofollow\">Singularity Library</a>\n</h4>\n<p>This is the official container\
    \ registry for Singularity. However, all images built on\nthis service are Singularity\
    \ v3+ compatible. At EBI we only have Singularity v2.6, but\nEMBL Heidelberg's\
    \ cluster does use Singularity v3+. This service works similarly to Singularity\
    \ and Docker Hubs, using the scheme <code>library://</code> for its URIs.</p>\n\
    <p>One additional feature that Singularity Library has is a <a href=\"https://cloud.sylabs.io/builder\"\
    \ rel=\"nofollow\">remote builder</a>. This builder allows you to dump a recipe\
    \ for a container, it will build the\ncontainer for you, and then you can download\
    \ it on to your local machine. Very handy\nwhen working on a computer you do not\
    \ have <code>sudo</code> access on.</p>\n<p>See the slides <em>below</em> <a href=\"\
    https://slides.com/mbhall88/remote-container-systems#/2/1\" rel=\"nofollow\">this</a>\
    \ for more information about Singularity\nLibrary.</p>\n<h4>\n<a id=\"user-content-quay-and-biocontainers\"\
    \ class=\"anchor\" href=\"#quay-and-biocontainers\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://quay.io/\"\
    \ rel=\"nofollow\">Quay</a> and <a href=\"https://biocontainers.pro/\" rel=\"\
    nofollow\">BioContainers</a>\n</h4>\n<p>Quay is a container registry for Docker\
    \ and <a href=\"https://coreos.com/rkt/\" rel=\"nofollow\">rkt</a> containers.\
    \ We won't talk much\nabout this service outside how to use the BioContainers\
    \ builds hosted on it.</p>\n<p>BioContainers is an open-source and community-driven\
    \ framework for reproducibility in\nbioinformatics<a href=\"https://doi.org/10.1093/bioinformatics/btx192\"\
    \ rel=\"nofollow\"><sup>1</sup></a>. They build and maintain containers for a\
    \ large suite of bioinformatics\ntools. In particular, any tool that has a <a\
    \ href=\"https://bioconda.github.io/\" rel=\"nofollow\">Bioconda</a> recipe automatically\
    \ has\na BioContainers image built and stored on Quay.</p>\n<p>To see an example\
    \ of how to find and use these BioContainers images check out the slides\nbelow\
    \ <a href=\"https://slides.com/mbhall88/remote-container-systems#/4/1i\" rel=\"\
    nofollow\">here</a>.</p>\n<hr>\n<p>For more details on remote container systems,\
    \ refer to <a href=\"https://slides.com/mbhall88/remote-container-systems\" rel=\"\
    nofollow\">my slides</a> from a one-day\n<a href=\"https://git.embl.de/grp-bio-it/singularity-training-2019\"\
    \ rel=\"nofollow\">Singularity course</a> I was involved in running at EMBL in\
    \ early 2019.</p>\n<h3>\n<a id=\"user-content-build-locally\" class=\"anchor\"\
    \ href=\"#build-locally\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Build locally</h3>\n<p>We've talked a lot about\
    \ how to use containers that others have been kind enough to\nconstruct for us.\
    \ But what happens if an image doesn't exist for the software tool you\nwant to\
    \ use? Or if you want to combine multiple programs into a single container? You\n\
    guessed it; we can build containers locally from definition/recipe files.</p>\n\
    <p>Rather than reinvent the wheel, please refer to (and work your way through)\
    \ <a href=\"https://slides.com/mbhall88/making-containers#/\" rel=\"nofollow\"\
    >these slides</a> from the <a href=\"https://git.embl.de/grp-bio-it/singularity-training-2019\"\
    \ rel=\"nofollow\">Singularity course</a> I was involved in running at EMBL in\
    \ early 2019. Once you get to slide titled <a href=\"https://slides.com/mbhall88/making-containers#/2/4\"\
    \ rel=\"nofollow\">\"Playing in a sandbox with a shell\"</a> you can move on to\
    \ <a href=\"#Exercise-1\">Exercise 1</a>.</p>\n<p><strong>Note:</strong> As the\
    \ course was aimed at users of Singularity v3+ you will see the container\nextension\
    \ <code>.sif</code> used. This was a new container file format introduced in v3\
    \ that is\nnot usable with v2. The container extension for v2 was <code>.simg</code>,\
    \ so you may see this sometimes.\nFor instance, the cluster at EBI is still on\
    \ v2 (the training VMs are v3). For those using\nthe Heidelberg cluster, your\
    \ cluster has v3. Singularity v2 containers, with the <code>.simg</code> extension,\n\
    can be executed by Singularity v3. You will also find all of the recipe\nfiles\
    \ in that presentation in the <a href=\"https://github.com/mbhall88/eipp-2019-singularity/tree/master/recipes\"\
    ><code>recipes/</code></a> directory of this repository.</p>\n<h2>\n<a id=\"user-content-exercise-1\"\
    \ class=\"anchor\" href=\"#exercise-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Exercise 1</h2>\n<p>Form two\
    \ groups and complete the following tasks.</p>\n<h3>\n<a id=\"user-content-task-1\"\
    \ class=\"anchor\" href=\"#task-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Task 1</h3>\n<p><a href=\"https://help.github.com/en/github/getting-started-with-github/fork-a-repo\"\
    >Fork</a> this repository on GitHub.</p>\n<h3>\n<a id=\"user-content-task-2\"\
    \ class=\"anchor\" href=\"#task-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Task 2</h3>\n<p><a href=\"https://slides.com/mbhall88/remote-container-systems#/1/6\"\
    \ rel=\"nofollow\">Enable Singularity Hub</a> on your fork of this repository.</p>\n\
    <h3>\n<a id=\"user-content-task-3\" class=\"anchor\" href=\"#task-3\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Task\
    \ 3</h3>\n<p>Each group should choose one of the following two GitHub issues to\
    \ close:</p>\n<ul>\n<li>\n<code>snakemake</code> recipe: <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/1\"\
    ><img src=\"https://camo.githubusercontent.com/26d3e148ca179ea5b34cb0255936905ed487432faa4027a512640b8f92a68ea7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f31\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/1\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>\n<code>shellcheck</code> recipe:\
    \ <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/2\"><img\
    \ src=\"https://camo.githubusercontent.com/a43a776c8cd5a471e5293ecd213c14f9452745fe9c75b850bd1986cf79d0d70a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f32\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/2\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-sandbox-development\"\
    \ class=\"anchor\" href=\"#sandbox-development\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sandbox development</h2>\n<p>During\
    \ the previous exercise, you may have noticed that errors in your build recipe\
    \ require you to rerun the build all over again. When installing simple programs,\
    \ this isn't too costly. However, when we want to build more complicated containers,\
    \ it becomes time-consuming to rerun the entire build continually. In this section,\
    \ we will look at how we can use Singularity's <a href=\"https://sylabs.io/guides/3.4/user-guide/build_a_container.html#creating-writable-images-and-sandbox-directories\"\
    \ rel=\"nofollow\"><code>--sandbox</code></a> option to speed up the container\
    \ recipe development cycle.</p>\n<p>So what is a sandbox? Think of it as a directory\
    \ that mimics the inside of a container. You can then start an interactive shell\
    \ session in this sandbox and run commands in the same environment that they will\
    \ run in when building the container. In this way, you can test out what commands\
    \ you need to run to get your program(s) installed and executing correctly. This\
    \ massively reduces your turnaround time for creating containers. In addition,\
    \ as we make the sandbox writeable, any changes we make will stay saved.</p>\n\
    <p>Let's get into the sandbox and play!</p>\n<p>Create a new directory where we\
    \ will do our sandbox development.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>mkdir sandbox-dev\n<span class=\"pl-c1\">cd</span> sandbox-dev</pre></div>\n\
    <p>Next, we will use the <a href=\"https://github.com/mbhall88/eipp-2019-singularity/blob/master/recipes/Singularity.template\"\
    >template recipe</a> in this repository to build our sandbox from.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sudo singularity build --sandbox playground\
    \ ../recipes/Singularity.template</pre></div>\n<p>You should now see a directory\
    \ called <code>playground</code>. I've named the sandbox <code>playground</code>,\
    \ but you can name it whatever you want.</p>\n<p>Now we will start an interactive\
    \ shell within the sandbox/container image.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity shell --writable playground</pre></div>\n<p><em>Note: If\
    \ you don't use <a href=\"https://sylabs.io/guides/3.4/user-guide/build_a_container.html#writable\"\
    \ rel=\"nofollow\"><code>--writable</code></a> you won't be able to install anything\
    \ or do anything that changes the size of the container.</em></p>\n<p>You should\
    \ now see the prompt change to something like</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>Singularity playground:<span class=\"pl-k\">~</span><span class=\"pl-k\"\
    >&gt;</span></pre></div>\n<p><strong>IMPORTANT:<br>\nThe directory <code>/root</code>\
    \ from your local machine will be mounted in the sandbox. So anything you do in\
    \ the sandbox in that directory will also be reflected in the <code>/root</code>\
    \ directory locally.\nEnsure you move out of <code>/root</code> within the sandbox\
    \ and do all of your work there. I tend to use <code>/usr/local</code>, but you\
    \ could create a new directory altogether (but outside <code>/root</code>) e.g.\
    \ <code>/sandbox</code>.</strong></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> /usr/local</pre></div>\n<p>Now we'll try\
    \ and <a href=\"https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent\"\
    \ rel=\"nofollow\">install <code>conda</code></a> inside the sandbox.</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\
    \ -O miniconda3.sh</pre></div>\n<p>This will give us: <code>bash: wget: command\
    \ not found</code>. A perfect example of why these sandboxes\nare so useful. The\
    \ OS installation is <em>very</em> minimal and doesn't include a lot of programs.</p>\n\
    <p>Let's install <code>wget</code> in our sandbox and try again.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>apt install -y wget\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\
    \ -O miniconda3.sh</pre></div>\n<p>Now we'll install <code>conda</code>, specifying\
    \ the prefix (<code>-p</code>) as a directory in <code>/usr/local</code>\ncalled\
    \ <code>miniconda</code>.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>bash miniconda3.sh -b -p <span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pwd<span\
    \ class=\"pl-pds\">)</span></span>/miniconda</pre></div>\n<p>In order to run <code>conda</code>\
    \ now, we need to ensure it's binary is in our <code>PATH</code>.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> PATH=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-s\"><span class=\"\
    pl-pds\">$(</span>realpath miniconda/bin<span class=\"pl-pds\">)</span></span>:<span\
    \ class=\"pl-smi\">${PATH}</span><span class=\"pl-pds\">\"</span></span></pre></div>\n\
    <p><em>Remember from the <a href=\"https://slides.com/mbhall88/making-containers#/1/7\"\
    \ rel=\"nofollow\"><code>%environment</code> slide</a> that when writing the recipe\
    \ for\nthis <code>conda</code> installation we would need to write the <code>export</code>\
    \ line as:</em></p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>export\
    \ PATH=<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>realpath miniconda/bin<span\
    \ class=\"pl-pds\">)</span></span>:<span class=\"pl-smi\">${PATH}</span><span\
    \ class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">&gt;&gt;</span> <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$SINGULARITY_ENVIRONMENT</span><span\
    \ class=\"pl-pds\">\"</span></span></pre></div>\n<p>Lastly, we need to test <code>conda</code>\
    \ is executable.</p>\n<div class=\"highlight highlight-source-shell\"><pre>conda\
    \ list</pre></div>\n<p>In order to convert these commands into a recipe I generally\
    \ keep a text file open where\nI paste (successful) commands into as I go so I\
    \ don't have to search back through my\nshell history later.</p>\n<h2>\n<a id=\"\
    user-content-exercise-2\" class=\"anchor\" href=\"#exercise-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Exercise\
    \ 2</h2>\n<p>Similar to <a href=\"#exercise-1\">Exercise 1</a>, form two groups\
    \ (can be different groups) and put\nin a pull request each to close the following\
    \ two issues:</p>\n<ul>\n<li>\n<code>flye</code> recipe: <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/3\"\
    ><img src=\"https://camo.githubusercontent.com/5bdb30d6ea7dece3a9a4cfc16de03ce988f6197b0363cb987ad5506c879a57eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f33\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/3\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>\n<code>nanopolish</code> recipe:\
    \ <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/4\"><img\
    \ src=\"https://camo.githubusercontent.com/b1fb67d7045f5f2f26d02ed8c2d5b5423da330dfc0b19efa70dbfd53ca698f5f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f34\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/4\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n</ul>\n<p>I chose more complicated programs\
    \ this time so you can get some experience using a sandbox.</p>\n<h2>\n<a id=\"\
    user-content-run-and-serving-applications\" class=\"anchor\" href=\"#run-and-serving-applications\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><code>run</code> and serving applications</h2>\n<h3>\n<a id=\"user-content-singularity-run\"\
    \ class=\"anchor\" href=\"#singularity-run\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><code>singularity run</code>\n\
    </h3>\n<p>The <a href=\"https://sylabs.io/guides/3.4/user-guide/cli/singularity_run.html\"\
    \ rel=\"nofollow\"><code>run</code></a> directive will execute the <a href=\"\
    https://slides.com/mbhall88/making-containers#/1/10\" rel=\"nofollow\"><code>%runscript</code></a>\
    \ and\npass along all arguments to this script. The <code>run</code> directive\
    \ is handy for when you want\nto automate some common tasks using the programs\
    \ installed within the container and be\nable to handle user options. Refer to\
    \ <a href=\"https://slides.com/mbhall88/making-containers#/1/10\" rel=\"nofollow\"\
    >the slide on <code>%runscript</code></a>,\nfrom the earlier section on <a href=\"\
    #build-locally\">buiding containers locally</a>, for\nan example of using <code>singularity\
    \ run</code>.</p>\n<h3>\n<a id=\"user-content-serving-applications\" class=\"\
    anchor\" href=\"#serving-applications\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Serving applications</h3>\n<p>It\
    \ is also possible to serve applications through a port from a container. As an\
    \ example\nwe will build a container to run a <a href=\"https://jupyter.org/\"\
    \ rel=\"nofollow\"><code>jupyter notebook</code></a> that we can access on\nour\
    \ local machine.</p>\n<p>The recipe to do this can be found in the <code>recipe/</code>\
    \ directory as <a href=\"https://github.com/mbhall88/eipp-2019-singularity/blob/master/recipes/Singularity.jupyter\"\
    ><code>Singularity.jupyter</code></a>.\nOf particular interest for this example,\
    \ see the <code>%runscript</code> section.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>%runscript\n    PORT=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span\
    \ class=\"pl-smi\">${1<span class=\"pl-k\">:-</span>8888}</span><span class=\"\
    pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">echo</span> <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>Starting notebook...<span class=\"pl-pds\"\
    >\"</span></span>\n    <span class=\"pl-c1\">echo</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>Open browser to localhost:<span class=\"pl-smi\"\
    >${PORT}</span><span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\"\
    >exec</span> /usr/local/bin/jupyter notebook  --ip=<span class=\"pl-s\"><span\
    \ class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span> --port=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$PORT</span><span\
    \ class=\"pl-pds\">\"</span></span> --no-browser</pre></div>\n<p>We take the first\
    \ option passed by the user and store it in a variable <code>PORT</code>, or use\
    \ <code>8888</code>\nif nothing is given. We print some logging to the screen\
    \ with <code>echo</code> and then start\na <code>jupyter</code> session, passing\
    \ the <code>PORT</code> to <code>jupyter</code>.</p>\n<p>Let's build this image\
    \ and then fire it up.</p>\n<div class=\"highlight highlight-source-shell\"><pre>sudo\
    \ singularity build jupyter.sif recipes/Singularity.jupyter\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> we will use the default port 8888</span>\nsingularity\
    \ run jupyter.sif  </pre></div>\n<p>You should get some output from <code>jupyter</code>\
    \ indicating it has started running the notebook\nand providing a location, which\
    \ should look something like:</p>\n<pre><code>[I 11:40:28.948 NotebookApp] Serving\
    \ notebooks from local directory: /home/vagrant/container-dev\n[I 11:40:28.949\
    \ NotebookApp] The Jupyter Notebook is running at:\n[I 11:40:28.949 NotebookApp]\
    \ http://dev-vm:8888/?token=c8fe88de778120e5ccd42850d6d13712e27b125b0481d5b0\n\
    [I 11:40:28.949 NotebookApp]  or http://127.0.0.1:8888/?token=c8fe88de778120e5ccd42850d6d13712e27b125b0481d5b0\n\
    [I 11:40:28.949 NotebookApp] Use Control-C to stop this server and shut down all\
    \ kernels (twice to skip confirmation).\n[C 11:40:28.953 NotebookApp]\n</code></pre>\n\
    <p>Copy the URL (either one), and paste it into a web browser. You should now\
    \ see the home\npage for the notebook. Select the example notebook at <code>notebooks/plot.ipynb</code>.</p>\n\
    <p>Run the two cells in the notebook, and you should see some toy data plotted.</p>\n\
    <p>This is quite a simple use case for serving applications. You can do far more\
    \ complicated\nthings like <a href=\"https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/\"\
    \ rel=\"nofollow\">running an RStudio server</a> from a container and access it\
    \ locally.</p>\n<h2>\n<a id=\"user-content-workflow-management-systems\" class=\"\
    anchor\" href=\"#workflow-management-systems\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Workflow management systems</h2>\n\
    <p>Containers and workflow management systems (WMSs), such as <code>snakemake</code>\
    \ and <a href=\"https://www.nextflow.io/\" rel=\"nofollow\"><code>nextflow</code></a>,\n\
    are a match made in heaven. Containers add a crucial layer of reproducibility\
    \ to these systems.</p>\n<p>Though this is not a project to teach you how to use\
    \ WMSs, I would\nencourage you to take a look at <a href=\"https://slides.com/mbhall88/singularity-and-workflow-management-systems#/\"\
    \ rel=\"nofollow\">this short slide deck</a> from the Singularity course I ran\n\
    as it shows you how easy it is to integrate Singularity containers into WMSs.</p>\n\
    <h2>\n<a id=\"user-content-programs-requiring-gpus\" class=\"anchor\" href=\"\
    #programs-requiring-gpus\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Programs requiring GPUs</h2>\n<p>Singularity\
    \ also provides the ability to utilise GPU cards, without needing to install\n\
    the GPU drivers into your container. Currently, it can only use NVIDIA GPUs. To\
    \ allow a\ncontainer to use the local GPU card and drivers all you need to do\
    \ it pass the\n<a href=\"https://sylabs.io/guides/2.6/user-guide/appendix.html#a-gpu-example\"\
    \ rel=\"nofollow\"><code>--nv</code></a> option. For example, to get a python\
    \ shell with the GPU version of <code>tensorflow</code>\navailable, you would\
    \ run the following (on a machine with an NVIDIA GPU).</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ --nv docker://tensorflow/tensorflow:latest-gpu python</pre></div>\n<h2>\n<a\
    \ id=\"user-content-bonus\" class=\"anchor\" href=\"#bonus\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Bonus</h2>\n\
    <p>If you have gotten to this point, then have a go at creating a container for\
    \ a piece of\nsoftware you have had difficulties installing in the past. Alternatively,\
    \ you could try\nand reduce the size of the containers we have already produced\
    \ by using <a href=\"https://www.alpinelinux.org/\" rel=\"nofollow\">Alpine</a>\
    \ as the\nbase OS.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics:
  - singularity
  - containers
  - bioinformatics
  - phd
  - embl-ebi
  - embl
  updated_at: 1573137725.0
mbhall88/head_to_head_pipeline:
  data_format: 2
  description: Snakemake pipeline to run analysis for the Illumina vs. Nanopore comparison.
  filenames:
  - analysis/assembly/containers/Singularity.canu
  full_name: mbhall88/head_to_head_pipeline
  latest_release: null
  readme: '<p>This repository holds the pipelines/scripts used for our project analysing
    Illumina

    and Nanopore for Mtb drug resistance calling and epidemiological clustering.</p>

    <p>It is currently in progress.</p>

    <p>See subdirectories for more specific information about different pipelines.</p>

    <ul>

    <li><a href="analysis/assembly">Assembly</a></li>

    <li><a href="https://github.com/mbhall88/tubby">Basecall training</a></li>

    <li><a href="data/H37Rv_PRG">H37Rv PRG construction</a></li>

    <li><a href="data/QC">Quality Control</a></li>

    <li><a href="analysis/baseline_variants">Baseline variant analysis</a></li>

    <li><a href="analysis/pandora_variants">Pandora variant analysis</a></li>

    <li><a href="analysis/transmission_clustering">Transmission clustering</a></li>

    <li><a href="analysis/resistance_prediction">Drug Resistance Prediction</a></li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1624928660.0
mbhall88/pandora_analysis_pipeline:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity.subsample
  - containers/Singularity.make_prg_dependencies
  full_name: mbhall88/pandora_analysis_pipeline
  latest_release: null
  readme: "<p><a href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\"\
    \ alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HEMnet - Haematoxylin &amp; Eosin and Molecular neural network</h1>\n\
    <h2>\n<a id=\"user-content-description\" class=\"anchor\" href=\"#description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Description</h2>\n<p>A deep learning automated cancer diagnosis software\
    \ using molecular labelling to improve pathological annotation of\nHaematoxylin\
    \ and Eosin (H&amp;E) stained tissue.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<ol>\n<li>\n\
    <p>Docker</p>\n<p>You can download and run the docker image using the following\
    \ commands:</p>\n<pre><code>docker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\
    </code></pre>\n</li>\n<li>\n<p>Conda</p>\n<p>Install Openslide (this is necessary\
    \ to open whole slide images) - download it <a href=\"https://openslide.org/download/\"\
    \ rel=\"nofollow\">here</a></p>\n<p>Create a conda environment from the <code>environment.yml</code>\
    \ file</p>\n<pre><code>conda env create -f environment.yml\nconda activate HEMnet\n\
    </code></pre>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-slide-preparation\"\
    \ class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Slide Preparation</h3>\n<p>Name\
    \ slides in the format: <code>slide_id_TP53</code> for TP53 slides and <code>slide_id_HandE</code>\
    \ for H&amp;E slides\nThe <code>TP53</code> and <code>HandE</code> suffix is used\
    \ by HEMnet to identify the stain used.</p>\n<h3>\n<a id=\"user-content-1-generate-training-and-testing-datasets\"\
    \ class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.\
    \ Generate training and testing datasets</h3>\n<p>a. Generate train dataset</p>\n\
    <p><code>python HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides\
    \ -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs\
    \ -v</code></p>\n<p>b. Generate test dataset</p>\n<p><code>python HEMnet_test_dataset.py\
    \ -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory\
    \ -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh\
    \ -n non_cancer_thresh</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is the relative path to the template slide from which all other slides will\
    \ be normalised against. The template\nslide should be the same for each step.</li>\n\
    <li>\n<code>-m</code> is the tile magnification. e.g. if  the input is <code>10</code>\
    \ then the tiles will be output at 10x</li>\n<li>\n<code>-a</code> is the align\
    \ magnification. Paired TP53 and H&amp;E slides will be registered at this magnification.\n\
    To reduce computation time we recommend this be less than the tile magnification\
    \ - a five times downscale generally works well.</li>\n<li>\n<code>-c</code> cancer\
    \ threshold to apply to the DAB channel. DAB intensities less than this threshold\
    \ indicate cancer.</li>\n<li>\n<code>-n</code> non-cancer threshold to apply to\
    \ the DAB channel. DAB intensities greater than this threshold indicate no cancer.</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\"\
    \ href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2. Train and evaluate model</h3>\n\
    <p>a. Training model</p>\n<p><code>python train.py -b /path/to/base/directory\
    \ -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory\
    \ -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size\
    \ -s -w -f -v</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-m</code>\
    \ is CNN base model. eg. <code>resnet50</code>, <code>vgg16</code>, <code>vgg19</code>,\
    \ <code>inception_v3</code> and <code>xception</code>.</li>\n<li>\n<code>-g</code>\
    \ is number of GPUs for training.</li>\n<li>\n<code>-e</code> is training epochs.\
    \ Default is <code>100</code> epochs.</li>\n<li>\n<code>-a</code> is batch size.\
    \ Default is <code>32</code>\n</li>\n<li>\n<code>-s</code> is option to save the\
    \ trained model weights.</li>\n<li>\n<code>-w</code> is option to used transfer\
    \ learning. Model will used pre-trained weights from ImageNet at the initial stage.</li>\n\
    <li>\n<code>-f</code> is fine-tuning option. Model will re-train CNN base.</li>\n\
    </ul>\n<p>b. Test model prediction</p>\n<p><code>python test.py  -b /path/to/base/directory\
    \ -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory\
    \ -w model_weights -m cnn_base -g num_gpus -v</code></p>\n<p>Other parameters:</p>\n\
    <ul>\n<li>\n<code>-w</code> is path to trained model. eg. <code>trained_model.h5</code>.</li>\n\
    <li>\n<code>-m</code> is CNN base model (same to training step).</li>\n<li>\n\
    <code>-g</code> is number of GPUs for prediction.</li>\n</ul>\n<p>c. Evaluate\
    \ model performance and visualise model prediction</p>\n<p><code>python visualisation.py\
    \ -b /path/to/base/directory -t /relative/path/to/training_output_directory -p\
    \ /relative/path/to/test_output_directory  -o /relative/path/to/output/directory\
    \ -i sample</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is path to training outputs.</li>\n<li>\n<code>-p</code> is path to test outputs.</li>\n\
    <li>\n<code>-i</code> is name of Whole Slide Image for visualisation.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\"\
    \ href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3. Apply model to diagnose new\
    \ images</h3>\n<p><code>python HEMnet_inference.py -s '/path/to/new/HE/Slides/'\
    \ -o '/path/to/output/directory/' -t '/path/to/template/slide/' -nn '/path/to/trained/model/'\
    \ -v</code></p>\n<p>Predict on TCGA images with our pretrained model for colorectal\
    \ cancer using <a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\">google colab</a></p>\n<h2>\n<a id=\"user-content-results\"\
    \ class=\"anchor\" href=\"#results\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Results</h2>\n<h2>\n<a id=\"\
    user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citing\
    \ HEMnet</h2>\n<h2>\n<a id=\"user-content-the-team\" class=\"anchor\" href=\"\
    #the-team\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>The Team</h2>\n<p>Please contact Dr Quan Nguyen (<a href=\"mailto:quan.nguyen@uq.edu.au\"\
    >quan.nguyen@uq.edu.au</a>), Andrew Su (<a href=\"mailto:a.su@uqconnect.edu.au\"\
    >a.su@uqconnect.edu.au</a>),\nand Xiao Tan (<a href=\"mailto:xiao.tan@uqconnect.edu.au\"\
    >xiao.tan@uqconnect.edu.au</a>) for issues, suggestions,\nand we are very welcome\
    \ to collaboration opportunities.</p>\n"
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1604591459.0
mbhall88/pistis:
  data_format: 2
  description: Quality control plotting for long reads
  filenames:
  - Singularity
  full_name: mbhall88/pistis
  latest_release: v0.3.0
  readme: "<h1>\n<a id=\"user-content-pistis\" class=\"anchor\" href=\"#pistis\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Pistis</h1>\n\
    <h3>\n<a id=\"user-content-quality-control-plotting-for-long-reads\" class=\"\
    anchor\" href=\"#quality-control-plotting-for-long-reads\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Quality\
    \ control plotting for long reads.</h3>\n<p><a href=\"https://pypi.python.org/pypi/pistis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0d377fd7c4560ba9ce5e50da718cfcda6af8bfe6e63362d9c8741335e20fec6c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7069737469732e737667\"\
    \ alt=\"PyPI status\" data-canonical-src=\"https://img.shields.io/pypi/v/pistis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.org/mbhall88/pistis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/460505d13dbbc44006c446a195f753c22160192229624c04b719693986845945/68747470733a2f2f7472617669732d63692e6f72672f6d6268616c6c38382f7069737469732e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/mbhall88/pistis.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/a5606fdcd10a7afc202cdcc307f242a27a106834bebba2be192225e4315fb774/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6268616c6c38382f7069737469732e737667\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/mbhall88/pistis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://twitter.com/mbhall88\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/899e87a3d856d3491f29644236afe87260be498a45240bd9acde07d48634d9fd/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6d6268616c6c38382e7376673f7374796c653d736f6369616c266c6f676f3d74776974746572266c6162656c3d466f6c6c6f77\"\
    \ alt=\"Twitter Follow\" data-canonical-src=\"https://img.shields.io/twitter/follow/mbhall88.svg?style=social&amp;logo=twitter&amp;label=Follow\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/2402\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>This package provides plotting designed\
    \ to give you an idea of how your long read\nsequencing data looks. It was conceived\
    \ of and developed with nanopore reads in\nmind, but there is no reason why PacBio\
    \ reads can't be used.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"\
    anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Installation</h2>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>pip3 install pistis</pre></div>\n<p>You can also\
    \ use <code>pip</code> if you are running with python2.<br>\nOr using a virtual\n\
    environment manager such as <a href=\"https://conda.io/docs/\" rel=\"nofollow\"\
    >conda</a> or\n<a href=\"https://docs.pipenv.org/\" rel=\"nofollow\">pipenv</a>.</p>\n\
    <p>You should now be able to run <code>pistis</code> from the command line</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis --help</pre></div>\n\
    <h3>\n<a id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity</h3>\n<p>There is a built image maintained with this repository\
    \ that can be used. For the latest release you can use the URI <code>shub://mbhall88/pistis</code><br>\n\
    For example</p>\n<div class=\"highlight highlight-source-shell\"><pre>singularity\
    \ <span class=\"pl-c1\">exec</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>shub://mbhall88/pistis<span class=\"pl-pds\">\"</span></span> pistis\
    \ --help\nsingularity pull --name pistis.simg <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>shub://mbhall88/pistis<span class=\"pl-pds\">\"</span></span></pre></div>\n\
    <h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <p>The main use case for <code>pistis</code> is as a command-line interface (CLI),\
    \ but it can also be\nused in an interactive way, such as with a <a href=\"https://jupyter.org/\"\
    \ rel=\"nofollow\">Jupyter Notebook</a>.</p>\n<h4>\n<a id=\"user-content-cli-usage\"\
    \ class=\"anchor\" href=\"#cli-usage\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CLI Usage</h4>\n<p>After installing\
    \ and running the help menu you should see the following usage\noptions</p>\n\
    <pre><code>pistis -h\n\nUsage: pistis [OPTIONS]\n\n  A package for sanity checking\
    \ (quality control) your long read data.\n  Feed it a fastq file and in return\
    \ you will receive a PDF with four plots:\n\n          1. GC content histogram\
    \ with distribution curve for sample.\n\n          2. Jointplot showing the read\
    \ length vs. phred quality score for\n          each         read. The interior\
    \ representation of this plot can be\n          altered with the         --kind\
    \ option.\n\n          3. Box plot of the phred quality score at positional bins\
    \ across\n          all reads. The reads are binned into read positions 1, 2,\
    \ 3, 4, 5,\n          6, 7, 8, 9, 10, 11-20, 21-50, 51-100, 101-200, 201-300.\
    \ Plots from\n          the start of reads.\n\n          4. Same as 3, but plots\
    \ from the end of the read.\n\n  Additionally, if you provide a BAM/SAM file a\
    \ histogram of the read\n  percent identity will be added to the report.\n\nOptions:\n\
    \  -f, --fastq PATH                Fastq file to plot. This can be gzipped.\n\
    \  -o, --output PATH               Path to save the plot PDF as. If name is not\n\
    \                                  specified, will use the name of the fastq\n\
    \                                  (or bam) file with .pdf extension.\n  -k, --kind\
    \ [kde|scatter|hex]    The kind of representation to use for the\n           \
    \                       jointplot of quality score vs read length.\n         \
    \                         Accepted kinds are 'scatter', 'kde'\n              \
    \                    (default), or 'hex'. For examples refer to h\n          \
    \                        ttps://seaborn.pydata.org/generated/seaborn.\n      \
    \                            jointplot.html\n  --log_length / --no_log_length\
    \  Plot the read length as a log10\n                                  transformation\
    \ on the quality vs read length\n                                  plot\n  -b,\
    \ --bam PATH                  SAM/BAM file to produce read percent\n         \
    \                         identity histogram from.\n  -d, --downsample INTEGER\
    \        Down-sample the sequence files to a given\n                         \
    \         number of reads. Set to 0 for no\n                                 \
    \ subsampling. Default: 50000\n  -h, --help                      Show this message\
    \ and exit.\n</code></pre>\n<p>Note the <code>--downsample</code> option is set\
    \ to 50000 by default. That is, <code>pistis</code> will\nonly plot 50000 reads\
    \ (sampled from a uniform distribution). You can set this to\n0 if you want to\
    \ plot every read, or select another number of your choosing. Be aware\nthat if\
    \ you try to plot too many reads you may run into memory issues, so try\ndownsampling\
    \ if this happens.</p>\n<p>There are three different use cases - currently - for\
    \ producing plots:</p>\n<p><strong>Fastq only</strong> - This will return four\
    \ plots:</p>\n<ul>\n<li>A distribution plot of the GC content for each read.</li>\n\
    <li>A bivariate jointplot with read length on the y-axis and mean read quality\n\
    score on the x-axis.</li>\n<li>Two boxplots that show the distribution of quality\
    \ scores at select positions\nand positional ranges. One plot shows the scores\
    \ from the beginning of the\nread and the other from the end of the read.</li>\n\
    </ul>\n<p>To use <code>pistis</code> in this way you just need a fastq file.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis -f /path/to/my.fastq\
    \ -o /save/as/report.pdf</pre></div>\n<p>This will save the four plots to a file\
    \ called <code>report.pdf</code> in directory <code>/save/as/</code>.\nIf you\
    \ don't provide a <code>--output/-o</code> option the file will be saved in the\
    \ current\ndirectory with the basename of the fastq file. So in the above example\
    \ it would be\nsaved as <code>my.pdf</code>.<br>\nIf you would prefer the read\
    \ lengths in the bivariate plot of read length vs.\nmean quality score then you\
    \ can indicate this like so</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pistis -f /path/to/my.fastq -o /save/as/report.pdf --no_log_length</pre></div>\n\
    <p>Additionally, you can change the way the data is represented in the bivariate\
    \ plot.\nThe default is a kernel density estimation plot (as in the below image),\
    \ however you can\nchoose to use a <a href=\"https://seaborn.pydata.org/generated/seaborn.jointplot.html\"\
    \ rel=\"nofollow\">hex bin or scatter plot version instead</a>.\nIn the running\
    \ example, to use a scatter plot you would run the following</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>pistis -f /path/to/my.fastq -o /save/as/report.pdf\
    \ --kind scatter</pre></div>\n<p>You can also provide a <code>gzip</code>ed fastq\
    \ file without any extra steps</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pistis -f /path/to/my.fastq.gz -o /save/as/report.pdf</pre></div>\n<p><strong>Examples</strong><br>\n\
    GC content:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_gc_plot.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_gc_plot.png\"\
    \ alt=\"gc content plot\" style=\"max-width:100%;\"></a></p>\n<p>Read length vs.\
    \ mean read quality score:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_v_len.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_v_len.png\"\
    \ alt=\"read length vs quality plot\" style=\"max-width:100%;\"></a></p>\n<p>Base\
    \ quality from the start of each read:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_start.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_start.png\"\
    \ alt=\"base quality from start plot\" style=\"max-width:100%;\"></a></p>\n<p>Base\
    \ quality from the end of each read:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_end.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_end.png\"\
    \ alt=\"base quality from end plot\" style=\"max-width:100%;\"></a></p>\n<hr>\n\
    <p><strong>Fastq and BAM/SAM</strong> - This will return the above four plots,\
    \ plus a distribution\nplot of each read's percent identity with the reference\
    \ it is aligned to in the\n[BS]AM file. Reads which are flagged as supplementary\
    \ or secondary are not included.\nThe plot also includes a dashed vertical red\
    \ line indicating the median\npercent identity.<br>\nNote: If using a BAM file,\
    \ it must be sorted and indexed (i.e <code>.bai</code> file). See <a href=\"http://www.htslib.org/doc/samtools.html\"\
    \ rel=\"nofollow\"><code>samtools</code></a>\nfor instructions on how to do this.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis -f /path/to/my.fastq\
    \  -b /path/to/my.bam -o /save/as/report.pdf\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> or</span>\npistis -f /path/to/my.fastq  -b /path/to/my.sam -o\
    \ /save/as/report.pdf</pre></div>\n<p><strong>Example</strong><br>\nDistribution\
    \ of aligned read percent identity:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_perc_id.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_perc_id.png\"\
    \ alt=\"percent identity plot\" style=\"max-width:100%;\"></a></p>\n<hr>\n<p><strong>BAM/SAM\
    \ only</strong> - At this stage you will receive only the distribution\nplot of\
    \ each read's percent identity with the reference it is aligned to. In a\nfuture\
    \ release I aim to allow you to also get the other four fastq-only plots.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis -b /path/to/my.bam\
    \ -o /save/as/report.pdf</pre></div>\n<p>As with the fastq-only method, if you\
    \ don't provide a <code>--output/-o</code> option the file will be saved in the\
    \ current\ndirectory with the basename of the [BS]AM file. So in the above example\
    \ it would be\nsaved as <code>my.pdf</code>.</p>\n<h4>\n<a id=\"user-content-usage-in-a-development-environment\"\
    \ class=\"anchor\" href=\"#usage-in-a-development-environment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage\
    \ in a development environment</h4>\n<p>If you would like to use <code>pistis</code>\
    \ within a development environment such as a\n<code>jupyter notebook</code> or\
    \ just a plain ol' python shell then take a look at <a href=\"https://github.com/mbhall88/pistis/blob/master/examples/example_usage.ipynb\"\
    >this example notebook</a>\nfor all the details.</p>\n<h2>\n<a id=\"user-content-credits\"\
    \ class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Credits</h2>\n<ul>\n<li>This\
    \ package was created with <a href=\"https://github.com/audreyr/cookiecutter\"\
    >Cookiecutter</a> and the <a href=\"https://github.com/audreyr/cookiecutter-pypackage\"\
    ><code>audreyr/cookiecutter-pypackage</code> project template</a>.</li>\n<li>The\
    \ two test data files (fastq and BAM) that I have used in this repository were\n\
    taken from <a href=\"https://github.com/wdecoster/nanotest\">Wouter De Coster's\
    \ <code>nanotest</code> repository</a>.</li>\n<li>Which in turn comes from <a\
    \ href=\"http://lab.loman.net/2017/03/09/ultrareads-for-nanopore/\" rel=\"nofollow\"\
    >Nick Loman and Josh Quick</a>.</li>\n<li>The example plots in this <code>README</code>\
    \ were made using the entire fastq of basecalled\nreads from the experiment in\
    \ that <a href=\"http://lab.loman.net/2017/03/09/ultrareads-for-nanopore/\" rel=\"\
    nofollow\">blog on \"whale hunting\"</a>.</li>\n<li>The plot for the BAM file\
    \ was obtained by running <code>pistis</code> on a BAM file generated\nby mapping\
    \ the fastq file to <em>E. coli</em> reference <a href=\"https://www.ncbi.nlm.nih.gov/nuccore/NC_000913.3\"\
    \ rel=\"nofollow\">NC_000913.3</a>\nusing Heng Li's <a href=\"https://github.com/lh3/minimap2\"\
    ><code>minimap2</code></a> and <code>-x map-ont</code> option.</li>\n</ul>\n<h1>\n\
    <a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing</h1>\n\
    <p>If you would like to contribute to this package you are more than welcome.<br>\n\
    <strong>Please read through the <a href=\"https://github.com/mbhall88/pistis/blob/master/CONTRIBUTING.rst\"\
    >contributing guidelines</a> first</strong>.</p>\n"
  stargazers_count: 5
  subscribers_count: 2
  topics:
  - nanopore
  - oxford-nanopore
  - bioinformatics
  - bioinformatics-analysis
  - plotting
  - quality-control
  - pacbio
  updated_at: 1614735802.0
melnel000/Sarek_CBIO:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: melnel000/Sarek_CBIO
  latest_release: null
  readme: "<h1>\n<a id=\"\" class=\"anchor\" href=\"#\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"http://sarek.scilifelab.se/\"\
    \ rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/Sarek_logo.png\"\
    \ alt=\"Sarek\" title=\"Sarek\" style=\"max-width:100%;\"></a>\n</h1>\n<h4>\n\
    <a id=\"user-content-an-open-source-analysis-pipeline-to-detect-germline-or-somatic-variants-from-whole-genome-or-targeted-sequencing\"\
    \ class=\"anchor\" href=\"#an-open-source-analysis-pipeline-to-detect-germline-or-somatic-variants-from-whole-genome-or-targeted-sequencing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>An open-source analysis pipeline to detect germline or somatic variants\
    \ from whole genome or targeted sequencing</h4>\n<p><a href=\"https://www.nextflow.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.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\"\
    \ alt=\"Nextflow version\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg?logo=data:image/svg+xml;base64,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\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.org/SciLifeLab/Sarek\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/63575514d0146dcd6226c111d1293fade62aa93f4946f2afcbb55718f3be5d86/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d747261766973\"\
    \ alt=\"Travis build status\" data-canonical-src=\"https://img.shields.io/travis/SciLifeLab/Sarek.svg?logo=travis\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://gitter.im/SciLifeLab/Sarek\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/419eea0b1c13bbf50717b250a81b0de7616141e86a5fc88eee2360f003f4c4e6/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d676974746572266c6f676f436f6c6f723d776869746526636f6c6f72423d346662393961\"\
    \ alt=\"Join the chat on https://gitter.im/SciLifeLab/Sarek\" data-canonical-src=\"\
    https://img.shields.io/gitter/room/SciLifeLab/Sarek.svg?logo=gitter&amp;logoColor=white&amp;colorB=4fb99a\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/e041bce107aca31eca6dd63a962556bf70d6cef2508c777604eedb99ca049c4b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f5363694c6966654c61622f536172656b2e737667\"\
    \ alt=\"MIT License\" data-canonical-src=\"https://img.shields.io/github/license/SciLifeLab/Sarek.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/SciLifeLab/Sarek/releases/latest\"\
    ><img src=\"https://camo.githubusercontent.com/720a0b93892db5c772d24eb7dc2fd6fefb2b556eff92ee7ae6a2963a40a8dd5a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465\"\
    \ alt=\"Sarek version\" data-canonical-src=\"https://img.shields.io/github/release/SciLifeLab/Sarek.svg?logo=github&amp;logoColor=white\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://zenodo.org/badge/latestdoi/54024046\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2794ec0225017cde71e3ed51dd8393510fe23a950955ef03f7439d7c0f288f83/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f35343032343034362e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/54024046.svg\" style=\"\
    max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.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\"\
    \ alt=\"Install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?logo=data:image/png;base64,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\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/maxulysse/sarek\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/bc3bec2ef3bf857d42e0bff8df09f0e81595bbd7dbc2681d0feadd729acb4bc0/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6d6178756c797373652f736172656b2e7376673f6c6f676f3d646f636b6572\"\
    \ alt=\"Docker Container available\" data-canonical-src=\"https://img.shields.io/docker/automated/maxulysse/sarek.svg?logo=docker\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h2>\n<p><a href=\"\
    https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/CAW_logo.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img align=\"right\" title=\"CAW\" src=\"\
    https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/CAW_logo.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Previously known as the Cancer Analysis\
    \ Workflow (CAW),\nSarek is a workflow designed to run analyses on WGS data from\
    \ regular samples or tumour / normal pairs, including relapse samples if required.</p>\n\
    <p>It's built using <a href=\"https://www.nextflow.io/\" rel=\"nofollow\">Nextflow</a>,\
    \ a domain specific language for workflow building.\nSoftware dependencies are\
    \ handled using <a href=\"https://www.docker.com\" rel=\"nofollow\">Docker</a>\
    \ or <a href=\"https://www.sylabs.io/singularity/\" rel=\"nofollow\">Singularity</a>\
    \ - container technologies that provide excellent reproducibility and ease of\
    \ use.\nSingularity has been designed specifically for high-performance computing\
    \ environments.\nThis means that although Sarek has been primarily designed for\
    \ use with the Swedish <a href=\"https://www.uppmax.uu.se\" rel=\"nofollow\">UPPMAX\
    \ HPC systems</a>, it should be able to run on any system that supports these\
    \ two tools.</p>\n<p>Sarek was developed at the <a href=\"https://ngisweden.scilifelab.se/\"\
    \ rel=\"nofollow\">National Genomics Infastructure</a> and <a href=\"https://www.nbis.se/\"\
    \ rel=\"nofollow\">National Bioinformatics Infastructure Sweden</a> which are\
    \ both platforms at <a href=\"https://www.scilifelab.se/\" rel=\"nofollow\">SciLifeLab</a>.\n\
    It is listed on the <a href=\"https://bio.tools/Sarek\" rel=\"nofollow\">Elixir\
    \ - Tools and Data Services Registry</a>.</p>\n<h2>\n<a id=\"user-content-workflow-steps\"\
    \ class=\"anchor\" href=\"#workflow-steps\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Workflow steps</h2>\n<p>Sarek\
    \ is built with several workflow scripts.\nA wrapper script contained within the\
    \ repository makes it easy to run the different workflow scripts as a single job.\n\
    To test your installation, follow the <a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/TESTS.md\"\
    >tests documentation.</a></p>\n<p>Raw FastQ files or aligned BAM files (with or\
    \ without realignment &amp; recalibration) can be used as inputs.\nYou can choose\
    \ which variant callers to use, plus the pipeline is capable of accommodating\
    \ additional variant calling software or CNV callers if required.</p>\n<p>The\
    \ worflow steps and tools used are as follows:</p>\n<ol>\n<li>\n<strong>Preprocessing</strong>\
    \ - <code>main.nf</code> <em>(based on <a href=\"https://software.broadinstitute.org/gatk/best-practices/\"\
    \ rel=\"nofollow\">GATK best practices</a>)</em>\n<ul>\n<li>Map reads to Reference\n\
    <ul>\n<li><a href=\"http://bio-bwa.sourceforge.net/\" rel=\"nofollow\">BWA</a></li>\n\
    </ul>\n</li>\n<li>Mark Duplicates\n<ul>\n<li><a href=\"https://github.com/broadinstitute/gatk\"\
    >GATK MarkDuplicates</a></li>\n</ul>\n</li>\n<li>Base (Quality Score) Recalibration\n\
    <ul>\n<li><a href=\"https://github.com/broadinstitute/gatk\">GATK BaseRecalibrator</a></li>\n\
    <li><a href=\"https://github.com/broadinstitute/gatk\">GATK ApplyBQSR</a></li>\n\
    </ul>\n</li>\n</ul>\n</li>\n<li>\n<strong>Germline variant calling</strong> -\
    \ <code>germlineVC.nf</code>\n<ul>\n<li>SNVs and small indels\n<ul>\n<li><a href=\"\
    https://github.com/broadinstitute/gatk\">GATK HaplotyeCaller</a></li>\n<li><a\
    \ href=\"https://github.com/Illumina/strelka\">Strelka2</a></li>\n</ul>\n</li>\n\
    <li>Structural variants\n<ul>\n<li><a href=\"https://github.com/Illumina/manta\"\
    >Manta</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<strong>Somatic variant calling</strong>\
    \ - <code>somaticVC.nf</code> <em>(optional)</em>\n<ul>\n<li>SNVs and small indels\n\
    <ul>\n<li><a href=\"https://github.com/broadinstitute/gatk\">MuTect2</a></li>\n\
    <li><a href=\"https://github.com/ekg/freebayes\">Freebayes</a></li>\n<li><a href=\"\
    https://github.com/Illumina/strelka\">Strelka2</a></li>\n</ul>\n</li>\n<li>Structural\
    \ variants\n<ul>\n<li><a href=\"https://github.com/Illumina/manta\">Manta</a></li>\n\
    </ul>\n</li>\n<li>Sample heterogeneity, ploidy and CNVs\n<ul>\n<li><a href=\"\
    https://github.com/Crick-CancerGenomics/ascat\">ASCAT</a></li>\n</ul>\n</li>\n\
    </ul>\n</li>\n<li>\n<strong>Annotation</strong> - <code>annotate.nf</code> <em>(optional)</em>\n\
    <ul>\n<li>Variant annotation\n<ul>\n<li><a href=\"http://snpeff.sourceforge.net/\"\
    \ rel=\"nofollow\">SnpEff</a></li>\n<li><a href=\"https://www.ensembl.org/info/docs/tools/vep/index.html\"\
    \ rel=\"nofollow\">VEP (Variant Effect Predictor)</a></li>\n</ul>\n</li>\n</ul>\n\
    </li>\n<li>\n<strong>Reporting</strong> - <code>runMultiQC.nf</code>\n<ul>\n<li>Reporting\n\
    <ul>\n<li><a href=\"http://multiqc.info\" rel=\"nofollow\">MultiQC</a></li>\n\
    </ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>The Sarek\
    \ pipeline comes with documentation in the <code>docs/</code> directory:</p>\n\
    <ol>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL.md\"\
    >Installation documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL_RACKHAM.md\"\
    >Installation documentation specific for UPPMAX <code>rackham</code></a></li>\n\
    <li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL_BIANCA.md\"\
    >Installation documentation specific for UPPMAX <code>bianca</code></a></li>\n\
    <li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/TESTS.md\"\
    >Tests documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/REFERENCES.md\"\
    >Reference files documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/CONFIG.md\"\
    >Configuration and profiles documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INTERVALS.md\"\
    >Intervals documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/USAGE.md\"\
    >Running the pipeline</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/PARAMETERS.md\"\
    >Command line parameters</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/USE_CASES.md\"\
    >Examples</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INPUT.md\"\
    >Input files documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/PROCESS.md\"\
    >Processes documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/CONTAINERS.md\"\
    >Documentation about containers</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/ASCAT.md\"\
    >More information about ASCAT</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/OUTPUT.md\"\
    >Output documentation structure</a></li>\n</ol>\n<h2>\n<a id=\"user-content-contributions--support\"\
    \ class=\"anchor\" href=\"#contributions--support\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributions\
    \ &amp; Support</h2>\n<p>If you would like to contribute to this pipeline, please\
    \ see the <a href=\"https://github.com/SciLifeLab/Sarek/blob/master/.github/CONTRIBUTING.md\"\
    >contributing guidelines</a>.</p>\n<p>For further information or help, don't hesitate\
    \ to get in touch on <a href=\"https://gitter.im/SciLifeLab/Sarek\" rel=\"nofollow\"\
    >Gitter</a> or contact us: <a href=\"mailto:maxime.garcia@scilifelab.se\">maxime.garcia@scilifelab.se</a>,\
    \ <a href=\"mailto:szilveszter.juhos@scilifelab.se\">szilveszter.juhos@scilifelab.se</a></p>\n\
    <h2>\n<a id=\"user-content-changelog\" class=\"anchor\" href=\"#changelog\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CHANGELOG</h2>\n\
    <ul>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/CHANGELOG.md\"\
    >CHANGELOG</a></li>\n</ul>\n<h2>\n<a id=\"user-content-credits\" class=\"anchor\"\
    \ href=\"#credits\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Credits</h2>\n<p>Main authors:</p>\n<ul>\n<li><a href=\"\
    https://github.com/MaxUlysse\">Maxime Garcia</a></li>\n<li><a href=\"https://github.com/szilvajuhos\"\
    >Szilveszter Juhos</a></li>\n</ul>\n<p>Helpful contributors:</p>\n<ul>\n<li><a\
    \ href=\"https://github.com/alneberg\">Johannes Alneberg</a></li>\n<li><a href=\"\
    https://github.com/Sebastian-D\">Sebastian DiLorenzo</a></li>\n<li><a href=\"\
    https://github.com/J35P312\">Jesper Eisfeldt</a></li>\n<li><a href=\"https://github.com/ewels\"\
    >Phil Ewels</a></li>\n<li><a href=\"https://github.com/gulfshores\">Max K\xE4\
    ller</a></li>\n<li><a href=\"https://github.com/malinlarsson\">Malin Larsson</a></li>\n\
    <li><a href=\"https://github.com/marcelm\">Marcel Martin</a></li>\n<li><a href=\"\
    https://github.com/bjornnystedt\">Bj\xF6rn Nystedt</a></li>\n<li><a href=\"https://github.com/pallolason\"\
    >Pall Olason</a></li>\n<li><a href=\"https://github.com/arontommi\">Aron Skaftason</a></li>\n\
    </ul>\n<hr>\n<p><a href=\"https://www.scilifelab.se/\" rel=\"nofollow\"><img src=\"\
    https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/SciLifeLab_logo.png\"\
    \ alt=\"SciLifeLab\" title=\"SciLifeLab\" style=\"max-width:100%;\"></a>\n<a href=\"\
    https://ngisweden.scilifelab.se/\" rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/NGI_logo.png\"\
    \ alt=\"NGI\" title=\"NGI\" style=\"max-width:100%;\"></a>\n<a href=\"https://www.nbis.se/\"\
    \ rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/NBIS_logo.png\"\
    \ alt=\"NBIS\" title=\"NBIS\" style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1541579046.0
mesnardo/petibm-decoupledibpm:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity.petibm0.4.2-xenial
  - singularity/Singularity.petibm0.5-xenial
  - singularity/Singularity.petibm0.5.1-xenial
  full_name: mesnardo/petibm-decoupledibpm
  latest_release: null
  readme: '<h1>

    <a id="user-content-decoupled-immersed-boundary-projection-method-with-petibm"
    class="anchor" href="#decoupled-immersed-boundary-projection-method-with-petibm"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decoupled
    Immersed Boundary Projection Method with PetIBM</h1>

    <p><a href="https://github.com/mesnardo/petibm-decoupledibpm/raw/master/LICENSE"><img
    src="https://camo.githubusercontent.com/8ccf186e7288af6d88a1f6a930c0fcc4e7a8a9936b34e07629d815d1eab4d977/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667"
    alt="License" data-canonical-src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://cloud.docker.com/u/mesnardo/repository/docker/mesnardo/petibm-decoupledibpm"
    rel="nofollow"><img src="https://camo.githubusercontent.com/b8d9674ae17bb539afa71ecc4169a1ee5a6a9242d8f9e12a10f4583093ba57c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65722d2d6875622d696e666f726d6174696f6e616c2e737667"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/badge/hosted-docker--hub-informational.svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3171" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="Singularity Hub" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-flow-over-a-stationary-circular-cylinder-re40-and-100" class="anchor"
    href="#flow-over-a-stationary-circular-cylinder-re40-and-100" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Flow over a stationary
    circular cylinder ($Re=40$ and $100$)</h2>

    <p><a href="runs/cylinder2dRe40/189_markers/figures/wz_0005000.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/cylinder2dRe40/189_markers/figures/wz_0005000.png"
    alt="cylinderRe40_vorticity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Vorticity contours around the cylinder at Reynolds number
    $40$. (Contour levels between $-3D/U_\infty$ and $3D/U_\infty$ with increments
    of $0.4$.)</p>

    <p><a href="runs/cylinder2dRe100/189_markers/figures/wz_0020000.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/cylinder2dRe100/189_markers/figures/wz_0020000.png"
    alt="cylinderRe100_vorticity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Vorticity contours around the cylinder at Reynolds number
    $100$ after $200$ time units of flow simulation. (Contour levels between $-3D/U_\infty$
    and $3D/U_\infty$ with increments of $0.4$.)</p>

    <p><a href="runs/cylinder2dRe40/189_markers/figures/cp_0005000.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/cylinder2dRe40/189_markers/figures/cp_0005000.png"
    alt="cylinderRe40_pressure_coefficient" style="max-width:100%;"></a>

    <strong>Figure:</strong> Pressure coefficient along the upper and lower surfaces
    of the cylinder at Reynolds number $40$. We compare with the results from Li et
    al. (2016).</p>

    <p><a href="runs/cylinder2dRe100/189_markers/figures/pressure_coefficient.png"
    target="_blank" rel="noopener noreferrer"><img src="runs/cylinder2dRe100/189_markers/figures/pressure_coefficient.png"
    alt="cylinderRe100_pressure_coefficient" style="max-width:100%;"></a>

    <strong>Figure:</strong> Pressure coefficient along the upper and lower surfaces
    of the cylinder at Reynolds number $100$. We compare with the results from Li
    et al. (2016).</p>

    <h2>

    <a id="user-content-flow-around-an-inline-oscillating-circular-cylinder-re100"
    class="anchor" href="#flow-around-an-inline-oscillating-circular-cylinder-re100"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Flow
    around an inline oscillating circular cylinder ($Re=100$)</h2>

    <p><a href="runs/oscillatingcylinderRe100/algo1/figures/vorticity.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/algo1/figures/vorticity.png"
    alt="oscillatingcylinderRe100_vorticity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Contours of the vorticity field around an inline oscillating
    cylinder at different phase angles ($\phi = 2 \pi f t$): $\phi = 0^o$ (left) and
    $\phi = 288^o$ (right). (Contour levels between $-20 U_m / D$ and $20 U_m / D$
    using $30$ increments.)</p>

    <p><a href="runs/oscillatingcylinderRe100/algo1/figures/pressure.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/algo1/figures/pressure.png"
    alt="oscillatingcylinderRe100_pressure" style="max-width:100%;"></a>

    <strong>Figure:</strong> Contours of the pressure field around an inline oscillating
    cylinder at different phase angles ($\phi = 2 \pi f t$): $\phi = 0^o$ (left) and
    $\phi = 288^o$ (right). (Contour levels between $-1 \rho U_m^2$ and $1 \rho U_m^2$
    using $50$ increments.)</p>

    <p><a href="runs/oscillatingcylinderRe100/algo1/figures/velocity_profiles.png"
    target="_blank" rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/algo1/figures/velocity_profiles.png"
    alt="oscillatingcylinderRe100_velocity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Profile of the velocity components ($u$: left, $v$: right)
    at four locations along the centerline for various phase angles $\phi$.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/drag_coefficient.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient.png"
    alt="oscillatingcylinderRe100_drag_coefficient" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient of the inline oscillating
    cylinder obtained using different algorithms. We also show zooms at early and
    developed stages.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/drag_coefficient_dt.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient_dt.png"
    alt="oscillatingcylinderRe100_drag_coefficient_dt" style="max-width:100%;"></a>

    <a href="runs/oscillatingcylinderRe100/figures/drag_coefficient_dx.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient_dx.png"
    alt="oscillatingcylinderRe100_drag_coefficient_dx" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient obtained with Algorithm
    1 for different time-step sizes and different grid sizes.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/temporal_error.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/temporal_error.png"
    alt="oscillatingcylinderRe100_temporal_error" style="max-width:100%;"></a>

    <strong>Figure:</strong> Variations of the $L_\infty$ and $L_2$ norm errors of
    the streamwise velocity as a function of the computational time-step size.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/spatial_error.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/spatial_error.png"
    alt="oscillatingcylinderRe100_temporal_error" style="max-width:100%;"></a>

    <strong>Figure:</strong> Variations of the $L_\infty$ and $L_2$ norm errors of
    the streamwise velocity as a function of the computational grid spacing.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/drag_coefficient_lag.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient_lag.png"
    alt="oscillatingcylinderRe100_cd_lag" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient using Algorithm 3 with
    force-prediction scheme 3. We compared the history obtained with different Lagrangian
    mesh resolutions: $N_b = 500$ Lagrangian markers on the boundary and $N_b = 202$
    markers (the latter one corresponding to the same resolution as the Eulerian background
    grid).</p>

    <h2>

    <a id="user-content-flow-around-an-impulsively-started-circular-cylinder-re40"
    class="anchor" href="#flow-around-an-impulsively-started-circular-cylinder-re40"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Flow
    around an impulsively started circular cylinder (Re=40)</h2>

    <p><a href="runs/translatingcylinder2dRe40/figures/drag_coefficients.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/translatingcylinder2dRe40/figures/drag_coefficients.png"
    alt="translatingcylinder2dRe40_cd" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient of the impulsively started
    cylinder. Comparison with the analytical solution of Bar-Lev &amp; Yang (1997)
    and the numerical results from Taira &amp; Colonius (2007).</p>

    <p><a href="runs/translatingcylinder2dRe40/dt=0.0005/figures/vorticity.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/translatingcylinder2dRe40/dt=0.0005/figures/vorticity.png"
    alt="translatingcylinder2dRe40_wz" style="max-width:100%;"></a>

    <strong>Figure:</strong> Vorticity contours around the impulsively started circular
    cylinder at $t=1.0$ (left) and $t=3.5$ (right). Contour levels between $-3 \omega_z
    D / U_o$ and $3 \omega_z D / U_o$ with increments of $0.4$.</p>

    <p><a href="runs/translatingcylinder2dRe40/figures/recirculation_lengths.png"
    target="_blank" rel="noopener noreferrer"><img src="runs/translatingcylinder2dRe40/figures/recirculation_lengths.png"
    alt="translatingcylinder2dRe40_lw" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the recirculation length measured in the reference
    frame of the impulsively start cylinder at Reynolds number 40 and for different
    time-step sizes.</p>

    <h2>

    <a id="user-content-three-dimensional-flow-around-an-inline-oscillating-sphere-re7854"
    class="anchor" href="#three-dimensional-flow-around-an-inline-oscillating-sphere-re7854"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Three-dimensional
    flow around an inline oscillating sphere ($Re=78.54$)</h2>

    <p><a href="runs/oscillatingsphere/figures/pressure.png" target="_blank" rel="noopener
    noreferrer"><img src="runs/oscillatingsphere/figures/pressure.png" alt="sphere_pressure"
    style="max-width:100%;"></a>

    <strong>Figure:</strong> Contours of the pressure field in the $x$/$y$ at $z=0$
    at three phase angles. Contour levels between $-2 p / \rho U_m^2$ and $2 p / \rho
    U_m^2$ with $30$ increments.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1581529613.0
miquelramirez/hybrid-fs:
  data_format: 2
  description: 'Hybrid-FS: A planner for controlling hybrid systems specified in Functional
    STRIPS'
  filenames:
  - Singularity
  full_name: miquelramirez/hybrid-fs
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-the-fs-functional-strips-planner\" class=\"\
    anchor\" href=\"#the-fs-functional-strips-planner\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>The FS Functional\
    \ STRIPS planner</h1>\n<p><code>FS</code> is a classical planner that works with\
    \ the Functional STRIPS planning language <a href=\"#ref-geffner-fstrips-2000\"\
    >[Geffner, 2000]</a>,\na modeling language based on the quantifier-free\nfragment\
    \ of first-order logic that includes constant, function and predicate symbols,\
    \ but no variable symbols. The increased expressiveness\nof the Functional STRIPS\
    \ language with respect to propositional languages such as standard STRIPS (which\
    \ is indeed subsumed by Functional STRIPS)\noften results in problem encodings\
    \ which are more compact, more readable, have fewer ground actions\nand preserve\
    \ the structural properties of the problem in a manner which allows the derivation\
    \ of more effective heuristics.</p>\n<p>Along with the core of the Functional\
    \ STRIPS language, the <code>FS</code> planner supports certain extensions which\
    \ are useful both\nfrom the expressive <em>and</em> the computational point of\
    \ view. These include <em>existential quantification</em>,\n<em>state constraints</em>,\
    \ a fairly large library of <em>global constraints</em>, and the possibility of\
    \ using <em>externally-defined symbols</em>\nand <em>built-in arithmetic symbols</em>.</p>\n\
    <p>This documentation covers a number of practical issues related to the use of\
    \ the <code>FS</code> planner. The planner, however, has\nbeen used and described\
    \ in a number of academic publications that <a href=\"http://gfrances.github.io/pubs/\"\
    \ rel=\"nofollow\">can be found here</a>,\nthe most recent of which are <a href=\"\
    #ref-frances-modeling-2015\">[Franc\xE8s and Geffner, 2015]</a> and <a href=\"\
    #ref-frances-existential-2016\">[Franc\xE8s and Geffner, 2016a]</a>\nand <a href=\"\
    #ref-frances-effective-2016\">[Franc\xE8s and Geffner, 2016b]</a>.</p>\n<ol>\n\
    <li><a href=\"#installation\">Installation</a></li>\n<li><a href=\"#usage\">Usage</a></li>\n\
    <li><a href=\"#credits\">Credits</a></li>\n<li><a href=\"#references\">References</a></li>\n\
    </ol>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a name=\"user-content-references\"></a>Installation</h2>\n<p>The\
    \ easiest way to use the planner is by <a href=\"doc/installation.md\">manually\
    \ compiling the planner source code</a>.\nThis procedure is complemented by the\
    \ <a href=\"doc/hybrid.md\">instructions specific for setting up the hybrid module</a>\
    \ of <code>FS</code>.</p>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><a name=\"user-content-usage\"></a>Usage</h2>\n<p>You\
    \ can find a high-level overview of the planner usage options <a href=\"doc/usage.md\"\
    >here</a></p>\n<h2>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a name=\"user-content-credits\"></a>Credits</h2>\n<p>The <code>FS</code>\
    \ planner is partially built upon the <a href=\"http://www.lapkt.org\" rel=\"\
    nofollow\">Lightweight Automated Planning Toolkit</a>\nand the PDDL parser from\
    \ the <a href=\"http://www.fast-downward.org\" rel=\"nofollow\">Fast Downward</a>\
    \ distribution.</p>\n<h2>\n<a id=\"user-content-references\" class=\"anchor\"\
    \ href=\"#references\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a name=\"user-content-references\"></a>References</h2>\n\
    <ul>\n<li>\n<p><a name=\"user-content-ref-frances-modeling-2015\">Franc\xE8s,\
    \ G., and Geffner, H. (2015)</a>,\n<a href=\"http://gfrances.github.io/pubs/2015-icaps-better-heuristics-more-expressive-languages/\"\
    \ rel=\"nofollow\"><em>Modeling and Computation in Planning: Better Heuristics\
    \ from More Expressive Languages</em></a>, ICAPS 2015.</p>\n</li>\n<li>\n<p><a\
    \ name=\"user-content-ref-frances-existential-2016\">Franc\xE8s, G., and Geffner,\
    \ H. (2016a)</a>,\n<a href=\"http://gfrances.github.io/pubs/2016-ijcai-existential-quantification-planning-csp/\"\
    \ rel=\"nofollow\"><em>E-STRIPS: Existential Quantification in Planning and Constraint\
    \ Satisfaction</em></a>, IJCAI 2016.</p>\n</li>\n<li>\n<p><a name=\"user-content-ref-frances-effective-2016\"\
    >Franc\xE8s, G., and Geffner, H. (2016b)</a>,\n<a href=\"http://gfrances.github.io/pubs/2016-ijcai-effective-planning-more-expressive-languages/\"\
    \ rel=\"nofollow\"><em>Effective Planning with More Expressive Languages</em></a>,\
    \ IJCAI 2016.</p>\n</li>\n<li>\n<p><a name=\"user-content-ref-geffner-fstrips-2000\"\
    >Geffner, H. (2000)</a>,\n<a href=\"http://www.tecn.upf.es/~hgeffner/\" rel=\"\
    nofollow\"><em>Functional STRIPS: A more flexible lan-\nguage for planning and\
    \ problem solving</em></a>.\nIn Minker, J., ed., Logic-Based Artificial Intelligence.\
    \ Kluwer. 187\u2013205.</p>\n</li>\n</ul>\n"
  stargazers_count: 2
  subscribers_count: 3
  topics:
  - cplusplus-14
  - ai
  - planning
  - hybrid-systems
  - kinodynamic-planning
  updated_at: 1570694552.0
mkandes/naked-singularity:
  data_format: 2
  description: A repository of definition files for bootstrapping Singularity containers
    around the software applications, frameworks, and libraries you need to run on
    high-performance computing systems.
  filenames:
  - definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.6-openblas-0.3.10
  - definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.4-openblas-0.3.10
  - definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-4.0.5-openblas-0.3.14
  - definition-files/anaconda/Singularity.anaconda3-py38-2020.11-ubuntu-18.04
  - definition-files/anaconda/Singularity.anaconda2-py27-2019.10-ubuntu-18.04
  - definition-files/miniconda/Singularity.miniconda3-py39-4.9.2-ubuntu-18.04
  - definition-files/miniconda/Singularity.miniconda2-py27-4.8.3-ubuntu-18.04
  - definition-files/miniconda/Singularity.miniconda3-py38-4.9.2-ubuntu-18.04
  - definition-files/miniconda/Singularity.miniconda3-py37-4.9.2-ubuntu-18.04
  - definition-files/tensorflow/Singularity.tensorflow-2.3.0-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4
  - definition-files/tensorflow/Singularity.tensorflow-2.5.0-ubuntu-18.04-cuda-11.2-openmpi-4.0.5
  - definition-files/visit/Singularity.visit-3.1.4-ubuntu-18.04-openmpi-3.1.6
  - definition-files/singularity/Singularity.singularity-3.5.3
  - definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.4
  - definition-files/ior/Singularity.ior-3.3.0-ubuntu-18.04-openmpi-4.0.5
  - definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.6
  - definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.4-osmesa-20.1.5
  - definition-files/paraview/Singularity.paraview-5.9.0-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5
  - definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5
  - definition-files/centos/Singularity.centos-7.9.2009
  - definition-files/centos/Singularity.centos-7.9.2009-cuda-10.1.168
  - definition-files/centos/Singularity.centos-7.9.2009-openmpi-3.1.4
  - definition-files/centos/Singularity.centos-7.9.2009-mvapich-2.3.2
  - definition-files/spark/Singularity.spark-2.3.1-hadoop-2.7-ubuntu-18.04
  - definition-files/spark/Singularity.spark-3.1.2-hadoop-3.2-ubuntu-18.04
  - definition-files/stream/Singularity.stream-5.10-ubuntu-18.04
  - definition-files/mxnet/Singularity.mxnet-1.7.0-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4
  - definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.6
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2
  - definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.4
  - definition-files/ubuntu/Singularity.ubuntu-18.04
  - definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-4.0.5
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2-openmpi-4.0.5
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4
  - definition-files/ubuntu/Singularity.ubuntu-18.04-mvapich-2.3.2
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2-openmpi-3.1.6
  - definition-files/ubuntu/Singularity.ubuntu-20.04
  - definition-files/beast/Singularity.beast-1.10.4-ubuntu-18.04-cuda-10.1.168
  - definition-files/beast/Singularity.beast-2.6.1-ubuntu-18.04-cuda-10.1.168
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.6
  - definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-cuda-11.2-openmpi-4.0.5
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.4
  - definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-openmpi-3.1.4
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4
  - definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-openmpi-4.0.5
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-mvapich-2.3.2
  - definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-mvapich-2.3.2
  - definition-files/gromacs/Singularity.gromacs-2020.3-ubuntu-18.04-cuda-10.1.168-tmpi-avx-512-cuda-70
  - definition-files/pytorch/Singularity.pytorch-1.8.1-ubuntu-18.04-cuda-11.2-openmpi-4.0.5
  - definition-files/pytorch/Singularity.pytorch-1.5.0-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4
  - definition-files/ciml/Singularity.tape-0.4
  - definition-files/ciml/Singularity.r-3.6.1
  - definition-files/ciml/Singularity.sparkr-2.3.1
  - definition-files/ciml/Singularity.esm-0.3.1
  - definition-files/ciml/Singularity.pyspark-3.1.2
  - definition-files/fenics/Singularity.fenics-2019.1.0-ubuntu-18.04-openmpi-3.1.4
  - definition-files/xcrysden/Singularity.xcrysden-1.6.2-ubuntu-18.04
  full_name: mkandes/naked-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-naked-singularity" class="anchor" href="#naked-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>naked-singularity</h1>

    <p>A repository of definition files for building

    <a href="https://sylabs.io/guides/latest/user-guide" rel="nofollow">Singularity</a>
    containers

    around the software applications, frameworks, and libraries you need to

    run on high-performance computing systems.</p>

    <h2>

    <a id="user-content-install-singularity" class="anchor" href="#install-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Singularity</h2>

    <p>Install Singularity on your Linux desktop, laptop, or virtual machine.</p>

    <div class="highlight highlight-source-shell"><pre>sudo ./naked-singularity.sh
    install</pre></div>

    <h2>

    <a id="user-content-build-a-singularity-container-from-a-definition-file" class="anchor"
    href="#build-a-singularity-container-from-a-definition-file" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build a Singularity
    container from a definition file</h2>

    <p>Build an Ubuntu Singularity container from one of the definition files

    available in this repository.</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build ubuntu.sif
    definition-files/ubuntu/Singularity.ubuntu-18.04</pre></div>

    <h2>

    <a id="user-content-download-an-existing-singularity-container" class="anchor"
    href="#download-an-existing-singularity-container" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Download an existing Singularity container</h2>

    <p>A number of pre-built containers from this repository are also now

    hosted at Singularity Hub.</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull shub://mkandes/naked-singularity:ubuntu-18.04</pre></div>

    <p><a href="https://vsoch.github.io/2021/singularity-hub-archive" rel="nofollow">Singularity
    Hub has been archived</a>. At least for the time being, naked-singularity definition

    files that rely on containers that were built and hosted on Singularity

    Hub prior to it being archived will continue to pull in these container

    dependencies and build properly. Alternative container build and hosting

    options for all future work are still under consideration.</p>

    <h2>

    <a id="user-content-status" class="anchor" href="#status" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Status</h2>

    <p>A work in progress.</p>

    <h2>

    <a id="user-content-contribute" class="anchor" href="#contribute" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contribute</h2>

    <p>If you would like to contribute one of your own Singularity container

    definition files for a specific application OR request a modification to

    an existing container definition, then please submit a pull request.</p>

    <h2>

    <a id="user-content-author" class="anchor" href="#author" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Author</h2>

    <p>Marty Kandes, Ph.D.<br>

    Computational &amp; Data Science Research Specialist<br>

    High-Performance Computing User Services Group<br>

    San Diego Supercomputer Center<br>

    University of California, San Diego</p>

    <h2>

    <a id="user-content-version" class="anchor" href="#version" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version</h2>

    <p>1.7.2</p>

    <h2>

    <a id="user-content-last-updated" class="anchor" href="#last-updated" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Last Updated</h2>

    <p>Sunday, June 20th, 2021</p>

    '
  stargazers_count: 32
  subscribers_count: 2
  topics: []
  updated_at: 1624390505.0
ml4ai/UA-hpc-containers:
  data_format: 2
  description: Container recipes for use with the U of A HPC resources
  filenames:
  - Singularity.numba
  - Singularity.keras
  - Singularity.pytorch
  - Singularity.dynet
  - Singularity.im2markup
  - Singularity.cuda9_py36
  - Singularity.tensorflow
  - Singularity.pytorch_skimage
  - Singularity.caffe2
  - Singularity.delphi
  - Singularity.nvidia_docker
  - Singularity.torch
  - word2vec/Singularity.w2v
  - openmnt/Singularity.opennmt
  full_name: ml4ai/UA-hpc-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-ml4ai-lab-singularity-container-repository" class="anchor"
    href="#ml4ai-lab-singularity-container-repository" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>ML4AI Lab Singularity Container Repository</h1>

    <p><a href="https://singularity-hub.org/collections/2086" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>This repository holds singularity containers that are commonly used by members
    of the ML4AI lab to run projects on the University of Arizona''s high-performance
    computing environment.</p>

    <p><strong>See Also</strong>:

    <a href="https://github.com/clulab/hpc-ml">https://github.com/clulab/hpc-ml</a></p>

    '
  stargazers_count: 2
  subscribers_count: 20
  topics: []
  updated_at: 1617312549.0
ml4ai/automates:
  data_format: 2
  description: 'AutoMATES: Automated Model Assembly from Text, Equations, and Software'
  filenames:
  - automates/equation_reading/equation_extraction/containers/Singularity.im2markup
  - automates/equation_reading/equation_extraction/containers/Singularity.pytorch_skimage
  full_name: ml4ai/automates
  latest_release: v0.1.0
  readme: "<h1 align=\"center\">\n<a id=\"user-content-automated-model-assemblyfrom-text-equations-and-software\"\
    \ class=\"anchor\" href=\"#automated-model-assemblyfrom-text-equations-and-software\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Automated Model Assembly<br>from Text, Equations, and Software</h1>\n\
    <p align=\"center\">\n  \n  \n  <a href=\"https://github.com/ml4ai/automates/actions\"\
    >\n    <img src=\"https://camo.githubusercontent.com/aca4ff329fcbc8477eb69ee3754d654a8f27d1b798178530242c11421121ca79/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f6d6c3461692f6175746f6d617465732f436f6e74696e756f7573253230496e746567726174696f6e3f6c6162656c3d7465737473\"\
    \ alt=\"GH Actions build status\" data-canonical-src=\"https://img.shields.io/github/workflow/status/ml4ai/automates/Continuous%20Integration?label=tests\"\
    \ style=\"max-width:100%;\">\n  </a>\n  <a href=\"https://codecov.io/gh/ml4ai/automates\"\
    \ rel=\"nofollow\">\n   <img src=\"https://camo.githubusercontent.com/f7328d200bac8b4c5c0473f8c3e3d1b870c0811e5802f20ef544cfc68259abbd/68747470733a2f2f636f6465636f762e696f2f67682f6d6c3461692f6175746f6d617465732f6272616e63682f6d61737465722f67726170682f62616467652e737667\"\
    \ data-canonical-src=\"https://codecov.io/gh/ml4ai/automates/branch/master/graph/badge.svg\"\
    \ style=\"max-width:100%;\">\n  </a>\n  <a href=\"https://www.codefactor.io/repository/github/ml4ai/automates\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ce091de26720098ad92a2ee617c9d8975b7bf53366140d5119b22f0025f8e740/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6d6c3461692f6175746f6d617465732f6261646765\"\
    \ alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/ml4ai/automates/badge\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p>This repository holds the source code\
    \ for the AutoMATES documentation\nand several component pipelines.</p>\n<p>For\
    \ documentation: <a href=\"https://ml4ai.github.io/automates\" rel=\"nofollow\"\
    >https://ml4ai.github.io/automates</a></p>\n<h2>\n<a id=\"user-content-installation-instructions\"\
    \ class=\"anchor\" href=\"#installation-instructions\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation\
    \ instructions</h2>\n<p>For all operating systems, the first step of the installation\
    \ process is to clone the AutoMATES repository.</p>\n<h3>\n<a id=\"user-content-linux-and-macos\"\
    \ class=\"anchor\" href=\"#linux-and-macos\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linux and macOS</h3>\n<ul>\n\
    <li>Create a new <a href=\"https://docs.python.org/3/library/venv.html\" rel=\"\
    nofollow\">Python virtualenv</a>\n</li>\n<li>Activate your new Python virtualenv</li>\n\
    <li>Install Graphviz as defined below</li>\n<li>Run <code>pip install -e .</code>\
    \ from the root of the AutoMATES directory</li>\n</ul>\n<h4>\n<a id=\"user-content-graphviz-installation\"\
    \ class=\"anchor\" href=\"#graphviz-installation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>GraphViz installation</h4>\n\
    <h5>\n<a id=\"user-content-debian-flavored-linux\" class=\"anchor\" href=\"#debian-flavored-linux\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Debian flavored linux</h5>\n<ul>\n<li>Use the command: <code>sudo\
    \ apt-get install graphviz libgraphviz-dev pkg-config</code>\n</li>\n</ul>\n<h5>\n\
    <a id=\"user-content-macos-with-homebrew\" class=\"anchor\" href=\"#macos-with-homebrew\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>macOS with Homebrew</h5>\n<ul>\n<li>Use the command: <code>brew install\
    \ graphviz</code>\n</li>\n<li>Install PyGraphviz to your virtualenv with: <code>pip\
    \ install --install-option=\"--include-path=/usr/local/include/\" --install-option=\"\
    --library-path=/usr/local/lib\" pygraphviz</code>\n</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-windows\" class=\"anchor\" href=\"#windows\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Windows</h3>\n\
    <ul>\n<li>Download and install <a href=\"https://www.anaconda.com/products/individual\"\
    \ rel=\"nofollow\">Anaconda</a>\n</li>\n<li>Edit the <code>PYTHONPATH</code> variable\
    \ in <code>environment.yml</code> to be your local path to your checkout of the\
    \ AutoMATES repo</li>\n<li>Run <code>conda env create --file environment.yml</code>\
    \ from the root of the AutoMATES directory</li>\n</ul>\n"
  stargazers_count: 17
  subscribers_count: 10
  topics: []
  updated_at: 1623935131.0
mmirko/singularitytest:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: mmirko/singularitytest
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularitytest" class="anchor" href="#singularitytest" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>singularitytest</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605257877.0
mosoriob/pegasus_montage-workflow-v2:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: mosoriob/pegasus_montage-workflow-v2
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-montage-workflow-v2\" class=\"anchor\" href=\"\
    #montage-workflow-v2\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>montage-workflow-v2</h1>\n<p>A new Python DAX\
    \ generator version of the classic Montage workflow. This workflow uses the <a\
    \ href=\"http://montage.ipac.caltech.edu\" rel=\"nofollow\">Montage\ntoolkit</a>\
    \ to re-project, background correct and add astronomical\nimages into custom mosaics.</p>\n\
    <h2>\n<a id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h2>\n<ul>\n<li>\n<a href=\"http://montage.ipac.caltech.edu\"\
    \ rel=\"nofollow\">Montage</a> - version 4.0 or later</li>\n<li>\n<a href=\"http://www.astropy.org/\"\
    \ rel=\"nofollow\">AstroPy</a> - version 1.0 or later</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-plan-a-montage-workflow\" class=\"anchor\" href=\"#plan-a-montage-workflow\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Plan a Montage Workflow</h2>\n<p>The <em>./montage-workflow.py</em>\
    \ Python script sets up a <em>data/</em> directory with a Pegasus DAX,\nimage\
    \ tables and region headers. For example:</p>\n<pre><code>./montage-workflow.py\
    \ --center \"56.7 24.0\" --degrees 2.0 \\\n          --band dss:DSS2B:blue --band\
    \ dss:DSS2R:green --band dss:DSS2IR:red\n</code></pre>\n<p>This will create a\
    \ 2x2 degree mosaic centered on 56.7 24.0, with 3 bands making up the\nred, green,\
    \ and blue channels for the final JPEG output. A 2 degree workflow has a lot\n\
    of input images and thus the workflow becomes wide. I simplified version of the\
    \ workflow\nlooks like:</p>\n<p><a href=\"docs/images/dax1.png?raw=true\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"docs/images/dax1.png?raw=true\"\
    \ alt=\"DAX 1\" title=\"DAX 1\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"\
    user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Examples</h2>\n\
    <p>The quickest way to get started is to use the <em>./example-dss.sh</em>\nscript.\
    \ It shows how to use the <em>montage-workflow.py</em> DAX generator to set up\
    \ and plan\n2 degree workflows as described above. Example:</p>\n<pre><code>$\
    \ ./example-dss.sh \n\nAdding band 1 (dss DSS2B -&gt; blue)\nRunning sub command:\
    \ mArchiveList dss DSS2B \"56.7 24.00\" 2.2 2.2 data/1-images.tbl\n[struct stat=\"\
    OK\", count=\"16\"]\nRunning sub command: cd data &amp;&amp; mDAGTbls 1-images.tbl\
    \ region-oversized.hdr 1-raw.tbl 1-projected.tbl 1-corrected.tbl\n[struct stat=\"\
    OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data &amp;&amp; mOverlaps\
    \ 1-raw.tbl 1-diffs.tbl\n[struct stat=\"OK\", count=120]\n\nAdding band 2 (dss\
    \ DSS2R -&gt; green)\nRunning sub command: mArchiveList dss DSS2R \"56.7 24.00\"\
    \ 2.2 2.2 data/2-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command:\
    \ cd data &amp;&amp; mDAGTbls 2-images.tbl region-oversized.hdr 2-raw.tbl 2-projected.tbl\
    \ 2-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub\
    \ command: cd data &amp;&amp; mOverlaps 2-raw.tbl 2-diffs.tbl\n[struct stat=\"\
    OK\", count=120]\n\nAdding band 3 (dss DSS2IR -&gt; red)\nRunning sub command:\
    \ mArchiveList dss DSS2IR \"56.7 24.00\" 2.2 2.2 data/3-images.tbl\n[struct stat=\"\
    OK\", count=\"16\"]\nRunning sub command: cd data &amp;&amp; mDAGTbls 3-images.tbl\
    \ region-oversized.hdr 3-raw.tbl 3-projected.tbl 3-corrected.tbl\n[struct stat=\"\
    OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data &amp;&amp; mOverlaps\
    \ 3-raw.tbl 3-diffs.tbl\n[struct stat=\"OK\", count=120]\n2016.06.02 21:46:32.455\
    \ PDT:    \n2016.06.02 21:46:32.461 PDT:   -----------------------------------------------------------------------\
    \ \n2016.06.02 21:46:32.466 PDT:   File for submitting this DAG to HTCondor  \
    \         : montage-0.dag.condor.sub \n2016.06.02 21:46:32.471 PDT:   Log of DAGMan\
    \ debugging messages                 : montage-0.dag.dagman.out \n2016.06.02 21:46:32.476\
    \ PDT:   Log of HTCondor library output                     : montage-0.dag.lib.out\
    \ \n2016.06.02 21:46:32.481 PDT:   Log of HTCondor library error messages    \
    \         : montage-0.dag.lib.err \n2016.06.02 21:46:32.487 PDT:   Log of the\
    \ life of condor_dagman itself          : montage-0.dag.dagman.log \n2016.06.02\
    \ 21:46:32.492 PDT:    \n2016.06.02 21:46:32.497 PDT:   -no_submit given, not\
    \ submitting DAG to HTCondor.  You can do this with: \n2016.06.02 21:46:32.507\
    \ PDT:   -----------------------------------------------------------------------\
    \ \n2016.06.02 21:46:33.387 PDT:   Your database is compatible with Pegasus version:\
    \ 4.6.1 \n2016.06.02 21:46:33.392 PDT:   \n\nI have concretized your abstract\
    \ workflow. The workflow has been entered \ninto the workflow database with a\
    \ state of \"planned\". The next step is \nto start or execute your workflow.\
    \ The invocation required is\n\npegasus-run  /data/scratch/rynge/montage2/montage-workflow-v2/work/1464929190\n\
    \n2016.06.02 21:46:33.419 PDT:   Time taken to execute is 2.961 seconds \n</code></pre>\n\
    <p>Running the workflow produces fits and jpeg mosaics for each band, as well\
    \ as a combined color one:</p>\n<p><a href=\"docs/images/pleiades.jpg?raw=true\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"docs/images/pleiades.jpg?raw=true\"\
    \ alt=\"Pleiades\" title=\"Pleiades\" style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1535257330.0
mvdoc/singularity-def:
  data_format: 2
  description: Singularity definition files for various projects
  filenames:
  - miniconda/Singularity
  - hauntedhouse/Singularity
  - hauntedhouse_freesurfer/Singularity
  full_name: mvdoc/singularity-def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1495630055.0
nbcrrolls/electrostatics-singularity:
  data_format: 2
  description: Molecular electrostatics singularity image
  filenames:
  - Singularity
  full_name: nbcrrolls/electrostatics-singularity
  latest_release: v2.1
  readme: "<h1>\n<a id=\"user-content-singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ class=\"anchor\" href=\"#singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity container for molecular electrostatic calculations using\
    \ PDB2PQR/APBS and Brownian dynamics with BrownDye.</h1>\n<p>This singularity\
    \ image contains a complete software environment for running <a href=\"http://browndye.ucsd.edu/\"\
    \ rel=\"nofollow\">BrownDye (version 1 and 2)</a> simulations. It also includes\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">PDB2PQR</a> and\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">APBS</a>.</p>\n\
    <p>Please <a href=\"http://eepurl.com/by4eQr\" rel=\"nofollow\">register</a> your\
    \ use of APBS and PDB2PQR.</p>\n<p>The image has been verified to work on XSEDE\
    \ <a href=\"https://portal.xsede.org/sdsc-comet\" rel=\"nofollow\">comet</a> and\
    \ <a href=\"https://www.sdsc.edu/support/user_guides/tscc-quick-start.html\" rel=\"\
    nofollow\">TSCC</a> shared cluster at SDSC. It will automatically bind <code>/cvmfs</code>\
    \ <code>/oasis</code> <code>/projects</code> <code>/scratch</code> directories,\
    \ if available on the host.</p>\n<h2>\n<a id=\"user-content-using-the-container\"\
    \ class=\"anchor\" href=\"#using-the-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using the container</h2>\n<p>Pull\
    \ the singularity image:</p>\n<pre><code>singularity pull shub://nbcrrolls/electrostatics-singularity\n\
    </code></pre>\n<p>Start bash shell in the container:</p>\n<pre><code>singularity\
    \ shell nbcrrolls-electrostatics-singularity-master-latest.simg\n</code></pre>\n\
    <p>Now the container is running and we can start a BrownDye2 job (using the Thrombin\
    \ example):</p>\n<pre><code>module load browndye2\ncp -ai $BD2_PATH/examples/thrombin\
    \ .\ncd thrombin\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n_trajectories&gt; 10000\
    \ /&lt;n_trajectories&gt; 1000 /' t_m_simulation.xml.bak\nmake all # takes about\
    \ min to run\nmodule unload browndye2\n</code></pre>\n<p>And if you want to use\
    \ BrownDye version 1:</p>\n<pre><code>module load browndye1\ncp -ai $BD1_PATH/thrombin-example\
    \ .\ncd thrombin-example\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n-trajectories&gt;\
    \ 10000 /&lt;n-trajectories&gt; 1000 /' input.xml.bak # limit the number of calculated\
    \ trajectories\nmake all\nbd_top input.xml\nnam_simulation t-m-simulation.xml\
    \ # this takes about 3 min to run\ncat results.xml\nmodule unload browndye1\n\
    </code></pre>\n<p>After we are finished we can quit the container:</p>\n<pre><code>exit\n\
    </code></pre>\n<p>You can also access individual applications from the electrostatics\
    \ container.</p>\n<p>To list available applications:</p>\n<pre><code>$ singularity\
    \ apps nbcrrolls-electrostatics-singularity-master-latest.simg \napbs\npdb2pqr\n\
    nam_simulation\nwe_simulation\n</code></pre>\n<p>To run, for example, apbs calculation:</p>\n\
    <pre><code>singularity exec nbcrrolls-electrostatics-singularity-master-latest.simg\
    \ apbs input.in\n</code></pre>\n<p>or</p>\n<pre><code>singularity run --app apbs\
    \ nbcrrolls-electrostatics-singularity-master-latest.simg input.in\n</code></pre>\n\
    <p>This Singularity image is hosted on Singularity Hub: <a href=\"https://singularity-hub.org/collections/2497\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h6>\n<a id=\"user-content-this-project-is-supported-by-nbcr\"\
    \ class=\"anchor\" href=\"#this-project-is-supported-by-nbcr\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>This\
    \ project is supported by <a href=\"http://nbcr.ucsd.edu\" rel=\"nofollow\">NBCR</a>.</h6>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1556048171.0
nealplatt/sH_hybridization:
  data_format: 2
  description: null
  filenames:
  - config/Singularity
  - scripts/unused/Singularity
  - scripts/unused/Singularity_newhybrids
  full_name: nealplatt/sH_hybridization
  latest_release: v1.0
  readme: '<p><a href="https://zenodo.org/badge/latestdoi/124456755" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4641d504e79e577f2add43b190e60f3910a1688ac8f26f972d799fd6f3f4b213/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3132343435363735352e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/124456755.svg" style="max-width:100%;"></a>
    <a href="https://github.com/ambv/black"><img src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667"
    alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    class="anchor" href="#ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ancient
    hybridization and adaptive introgression of an invadolysin gene in <em>Schistosoma
    haematobium</em>.</h1>

    <p>Roy N. Platt II, Marina McDew-White, Winka Le Clec''h, Frederic D. Chevalier,
    Fiona Allan, Aidan M. Emery, Amadou Garba, Shaali M. Ame, Joanne P. Webster, David
    Rollinson, Bonnie L. Webster, Timothy J. C. Anderson.</p>

    <p>The parasitic blood fluke <em>Schistosoma</em> <em>haematobium</em> causes
    urogenital schistosomiasis in humans and is a major cause of morbidity and mortality
    across sub-Saharan Africa. <em>S</em>. <em>haematobium</em> can hybridize with
    closely-related livestock schistosomes, including <em>S</em>. <em>bovis</em>,
    however the frequency, direction, age and genomic consequences of hybridization
    in nature are unknown. We sequenced 96 <em>S</em>. <em>haematobium</em> exomes
    from Niger and the Zanzibar archipelago. We found evidence of an ancient, adaptive
    introgression event between Nigerien <em>S</em>. <em>haematobium</em> and <em>S</em>.
    <em>bovis</em> occurring 108-613 generations ago. Introgressed S. bovis alleles
    constitute 3.3-8.2% of Nigerien <em>S</em>. <em>haematobium</em> genomes. Some
    <em>S</em>. <em>bovis</em> alleles have reached high frequency and show signatures
    of directional selection; the strongest signal spans a single gene in the invadolysin
    gene family, an M8 metalloprotease associated with parasitic life-history traits.</p>

    <h4>

    <a id="user-content-biorxiv-pre-print" class="anchor" href="#biorxiv-pre-print"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="https://doi.org/10.1101/539353" rel="nofollow">bioRxiv pre-print</a>

    </h4>

    <hr>

    <h3>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTES:</h3>

    <p>All analyses were conducted on a HPCC in a <code>singularity</code> container
    or in a <code>conda</code> managed environment. The singularity recipe and conda
    environmental yaml are in the <code>config</code> dir.</p>

    <p>Raw code is found in the <code>scripts</code> dir</p>

    <p>Data that is not readily available through the SRA is in the <code>data</code>
    dir.  These will be housed in an online repository (ex. Dryad), but provided here
    for documentation purposes.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1618309153.0
nextgenusfs/funannotate:
  data_format: 2
  description: Eukaryotic Genome Annotation Pipeline
  filenames:
  - Singularity
  full_name: nextgenusfs/funannotate
  latest_release: v1.8.7
  readme: '<p><a href="https://github.com/nextgenusfs/funannotate/releases/latest"><img
    src="https://camo.githubusercontent.com/77ba51f3f201259675cc5dd53ada0a361559748add45bf62437f21ab6ba102a6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6e65787467656e757366732f66756e616e6e6f746174652e737667"
    alt="Latest Github release" data-canonical-src="https://img.shields.io/github/release/nextgenusfs/funannotate.svg"
    style="max-width:100%;"></a>

    <a href="https://zenodo.org/badge/latestdoi/48254740" rel="nofollow"><img src="https://camo.githubusercontent.com/60682e17fa8054e0d7609e1eccfc1226867f83f55d9f19a2d9ddf6aadb09ba70/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f34383235343734302e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/48254740.svg" style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/6dfb6879b08d70d126676bad7d9ba75f1600e35f7ffbe85772b833121a2d0897/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f62696f636f6e64612f66756e616e6e6f74617465"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/6dfb6879b08d70d126676bad7d9ba75f1600e35f7ffbe85772b833121a2d0897/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f62696f636f6e64612f66756e616e6e6f74617465"
    alt="Conda" data-canonical-src="https://img.shields.io/conda/dn/bioconda/funannotate"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/025770f53027c0679a57f4dcce8795b4fdc998bc94e79ea907db5b3e44cc321c/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f6e65787467656e757366732f66756e616e6e6f746174652f6c6174657374"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/025770f53027c0679a57f4dcce8795b4fdc998bc94e79ea907db5b3e44cc321c/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f6e65787467656e757366732f66756e616e6e6f746174652f6c6174657374"
    alt="Docker Image Size (tag)" data-canonical-src="https://img.shields.io/docker/image-size/nextgenusfs/funannotate/latest"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/528f0577834915a1e28ec7423f521cf14aac5496340487bf67cf4abacf31aad3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6e65787467656e757366732f66756e616e6e6f74617465"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/528f0577834915a1e28ec7423f521cf14aac5496340487bf67cf4abacf31aad3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6e65787467656e757366732f66756e616e6e6f74617465"
    alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/nextgenusfs/funannotate"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/5068" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p><a href="funannotate-logo.png?raw=true" target="_blank" rel="noopener noreferrer"><img
    src="funannotate-logo.png?raw=true" alt="Alt text" title="Funannotate" style="max-width:100%;"></a></p>

    <p>funannotate is a pipeline for genome annotation (built specifically for fungi,
    but will also work with higher eukaryotes). Installation, usage, and more information
    can be found at <a href="http://funannotate.readthedocs.io" rel="nofollow">http://funannotate.readthedocs.io</a></p>

    <h4>

    <a id="user-content-quickest-start-docker" class="anchor" href="#quickest-start-docker"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Quickest
    start Docker:</h4>

    <p>You can use docker to run <code>funannotate</code>. Caveats are that GeneMark
    is not included in the docker image (see licensing below and you can complain
    to the developers for making it difficult to distribute/use). I''ve also written
    a bash script that can run the docker image and auto-detect/include the proper
    user/volume bindings.  This docker image is built off of the latest code in master,
    so it will be ahead of the tagged releases. The image includes the required databases
    as well, if you want just funannotate without the databases then that is located
    on docker hub as well <code>nextgenusfs/funannotate-slim</code>. So this route
    can be achieved with:</p>

    <pre><code># download/pull the image from docker hub

    $ docker pull nextgenusfs/funannotate


    # download bash wrapper script (optional)

    $ wget -O funannotate-docker https://raw.githubusercontent.com/nextgenusfs/funannotate/master/funannotate-docker


    # might need to make this executable on your system

    $ chmod +x /path/to/funannotate-docker


    # assuming it is in your PATH, now you can run this script as if it were the funannotate
    executable script

    $ funannotate-docker test -t predict --cpus 12

    </code></pre>

    <h4>

    <a id="user-content-quickstart-bioconda-install" class="anchor" href="#quickstart-bioconda-install"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Quickstart
    Bioconda install:</h4>

    <p>The pipeline can be installed with conda (via <a href="https://bioconda.github.io/"
    rel="nofollow">bioconda</a>):</p>

    <pre><code>#add appropriate channels

    conda config --add channels defaults

    conda config --add channels bioconda

    conda config --add channels conda-forge


    #then create environment

    conda create -n funannotate funannotate

    </code></pre>

    <p>If <code>conda</code> is taking forever to solve the environment, I would recommend
    giving <a href="https://github.com/mamba-org/mamba">mamba</a> a try:</p>

    <pre><code>#install mamba into base environment

    conda install -n base mamba


    #then use mamba as drop in replacmeent

    mamba create -n funannotate funannotate

    </code></pre>

    <p>If you want to use GeneMark-ES/ET you will need to install that manually following
    developers instructions:

    <a href="http://topaz.gatech.edu/GeneMark/license_download.cgi" rel="nofollow">http://topaz.gatech.edu/GeneMark/license_download.cgi</a></p>

    <p>Note that you will need to change the shebang line for all perl scripts in
    GeneMark to use <code>/usr/bin/env perl</code>.

    You will then also need to add <code>gmes_petap.pl</code> to the $PATH or set
    the environmental variable $GENEMARK_PATH to the gmes_petap directory.</p>

    <p>To install just the python funannotate package, you can do this with pip:</p>

    <pre><code>python -m pip install funannotate

    </code></pre>

    <p>To install the most updated code in master you can run:</p>

    <pre><code>python -m pip install git+https://github.com/nextgenusfs/funannotate.git

    </code></pre>

    '
  stargazers_count: 161
  subscribers_count: 13
  topics:
  - genome-annotation
  - gene-models
  - comparative-genomics
  - ncbi-submission
  updated_at: 1624262530.0
nhoffman/dada2-nf:
  data_format: 2
  description: A Nextflow pipeline for processing 16S rRNA sequences using dada2
  filenames:
  - singularity/Singularity
  full_name: nhoffman/dada2-nf
  latest_release: null
  readme: '<h1>

    <a id="user-content-dada2-nextflow-pipeline" class="anchor" href="#dada2-nextflow-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dada2
    Nextflow pipeline</h1>

    <h2>

    <a id="user-content-local-execution-quickstart-for-the-truly-impatient" class="anchor"
    href="#local-execution-quickstart-for-the-truly-impatient" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Local execution quickstart
    for the truly impatient</h2>

    <p>Install Docker and make sure that the Docker daemon is running.</p>

    <p>Install the nextflow binary in this directory</p>

    <pre><code>wget -qO- https://get.nextflow.io | bash

    </code></pre>

    <p>Execute locally, using the minimal data set.</p>

    <pre><code>./nextflow run main.nf -params-file params-minimal.json

    </code></pre>

    <h2>

    <a id="user-content-execution-on-aws-batch" class="anchor" href="#execution-on-aws-batch"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execution
    on AWS Batch</h2>

    <p>Details will depend on your AWS batch configuration. General instructions TBD.</p>

    <h3>

    <a id="user-content-infernal-16s-filtering" class="anchor" href="#infernal-16s-filtering"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Infernal
    16s filtering</h3>

    <p>Coveriance model used for Infernal sequence filtering obtained from the Rfam
    database:</p>

    <p><a href="https://rfam.xfam.org/family/RF00177" rel="nofollow">https://rfam.xfam.org/family/RF00177</a></p>

    <p>To cite Rfam see latest web site instructions:</p>

    <p><a href="https://rfam.xfam.org/" rel="nofollow">https://rfam.xfam.org/</a></p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1623959326.0
nicspalla/openmpi_centos_x86_64:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: nicspalla/openmpi_centos_x86_64
  latest_release: null
  readme: '<h1>

    <a id="user-content-openmpi_centos_x86_64" class="anchor" href="#openmpi_centos_x86_64"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>openmpi_centos_x86_64</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605260984.0
nservant/HiC-Pro:
  data_format: 2
  description: 'HiC-Pro: An optimized and flexible pipeline for Hi-C data processing'
  filenames:
  - Singularity
  full_name: nservant/HiC-Pro
  latest_release: v3.0.0
  readme: "<h1>\n<a id=\"user-content-hic-pro\" class=\"anchor\" href=\"#hic-pro\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HiC-Pro</h1>\n<h3>\n<a id=\"user-content-an-optimized-and-flexible-pipeline-for-hi-c-data-processing\"\
    \ class=\"anchor\" href=\"#an-optimized-and-flexible-pipeline-for-hi-c-data-processing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>An optimized and flexible pipeline for Hi-C data processing</h3>\n\
    <p><a href=\"https://travis-ci.com/nservant/HiC-Pro\" rel=\"nofollow\"><img src=\"\
    https://camo.githubusercontent.com/73a86528f082d212ef4ce88c8e2cf0c58768f77ffcca0a20820f724928dd04f1/68747470733a2f2f7472617669732d63692e636f6d2f6e73657276616e742f4869432d50726f2e7376673f6272616e63683d646576656c5f707933\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/nservant/HiC-Pro.svg?branch=devel_py3\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://camo.githubusercontent.com/fa5e0167fc052efefc929acb2c1c877388fffa8a486163becac51ff00b7ccadc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e64612d6275696c642d627269676874677265656e2e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fa5e0167fc052efefc929acb2c1c877388fffa8a486163becac51ff00b7ccadc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e64612d6275696c642d627269676874677265656e2e737667\"\
    \ alt=\"Conda\" data-canonical-src=\"https://img.shields.io/badge/Conda-build-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a0a5d2fc20aeae0c34909c3cb3c096a3f2c0592c8e47dc0c0e9c6bb1f0014110/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53696e67756c61726974792d6275696c642d627269676874677265656e2e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a0a5d2fc20aeae0c34909c3cb3c096a3f2c0592c8e47dc0c0e9c6bb1f0014110/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53696e67756c61726974792d6275696c642d627269676874677265656e2e737667\"\
    \ alt=\"Singularity\" data-canonical-src=\"https://img.shields.io/badge/Singularity-build-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/repository/docker/nservant/hicpro\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a946c73292a5c2f469ea28505b0533d98efa33c02af4137ce82a41ad0a9d96c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f636b65722d6d616e75616c2d79656c6c6f772e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/badge/Docker-manual-yellow.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://camo.githubusercontent.com/5224ef8b916118d25eaf98ae2a4b258283c163a2354a008c7eed94dcba0635e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d756c746951432d312e382d626c75652e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5224ef8b916118d25eaf98ae2a4b258283c163a2354a008c7eed94dcba0635e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d756c746951432d312e382d626c75652e737667\"\
    \ alt=\"MultiQC\" data-canonical-src=\"https://img.shields.io/badge/MultiQC-1.8-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://groups.google.com/forum/#!forum/hic-pro\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1b9d743d52863763181e70393d3998508fea708ce52d4e2c6cb4c9d8d43f3d50/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47726f7570732d2532306a6f696e253230636861742532302545322538362539322d3466623939612e7376673f7374796c653d666c61742d737175617265\"\
    \ alt=\"Forum\" data-canonical-src=\"https://img.shields.io/badge/Groups-%20join%20chat%20%E2%86%92-4fb99a.svg?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://doi.org/10.1186/s13059-015-0831-x\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/27e7387e5cf05b69efa15a349127425e83ef45fd07d906c440f0b14d68df2d4e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f492d31302e313138362532467331333035392d2d3031352d2d303833312d2d782d6c69676874677265792e7376673f7374796c653d666c61742d737175617265\"\
    \ alt=\"DOI\" data-canonical-src=\"https://img.shields.io/badge/DOI-10.1186%2Fs13059--015--0831--x-lightgrey.svg?style=flat-square\"\
    \ style=\"max-width:100%;\"></a></p>\n<hr>\n<p>Find documentation and examples\
    \ at <a href=\"http://nservant.github.io/HiC-Pro/\" rel=\"nofollow\">http://nservant.github.io/HiC-Pro/</a></p>\n\
    <p>For any question about HiC-Pro, please contact <a href=\"mailto:nicolas.servant@curie.fr\"\
    >nicolas.servant@curie.fr</a> or use the <a href=\"https://groups.google.com/forum/#!forum/hic-pro\"\
    \ rel=\"nofollow\">HiC-Pro forum</a></p>\n<h2>\n<a id=\"user-content-what-is-hic-pro-\"\
    \ class=\"anchor\" href=\"#what-is-hic-pro-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>What is HiC-Pro ?</h2>\n<p>HiC-Pro\
    \ was designed to process Hi-C data, from raw fastq files (paired-end Illumina\
    \ data) to normalized contact maps. It supports the main Hi-C protocols, including\
    \ digestion protocols as well as protocols that do not require restriction enzymes\
    \ such as DNase Hi-C. In practice, HiC-Pro was successfully applied to many data-sets\
    \ including dilution Hi-C, in situ Hi-C, DNase Hi-C, Micro-C, capture-C, capture\
    \ Hi-C or HiChip data.<br>\nThe pipeline is flexible, scalable and optimized.\
    \ It can operate either on a single laptop or on a computational cluster. HiC-Pro\
    \ is sequential and each step of the workflow can be run independantly.<br>\n\
    HiC-Pro includes a fast implementatation of the iterative correction method (see\
    \ the <a href=\"https://github.com/hiclib/iced\">iced python package</a> for more\
    \ information).\nFinally, HiC-Pro can use phasing data to build <a href=\"doc/AS.md\"\
    >allele-specific contact maps</a>.</p>\n<p>If you use HiC-Pro, please cite :</p>\n\
    <p><em>Servant N., Varoquaux N., Lajoie BR., Viara E., Chen CJ., Vert JP., Dekker\
    \ J., Heard E., Barillot E.</em> HiC-Pro: An optimized and flexible pipeline for\
    \ Hi-C processing. Genome Biology 2015, 16:259 <a href=\"https://doi.org/10.1186/s13059-015-0831-x\"\
    \ rel=\"nofollow\">doi:10.1186/s13059-015-0831-x</a></p>\n<h2>\n<a id=\"user-content-containers\"\
    \ class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Containers</h2>\n<h3>\n<a id=\"\
    user-content-using-hic-pro-through-conda\" class=\"anchor\" href=\"#using-hic-pro-through-conda\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using HiC-Pro through <code>conda</code>\n</h3>\n<p>In order to ease\
    \ the installation of HiC-Pro dependancies, we provide a <code>yml</code> file\
    \ for conda with all required tools.\nIn order to build your conda environment,\
    \ first install <a href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"\
    nofollow\">miniconda</a> and use :</p>\n<pre><code>conda env create -f MY_INSTALL_PATH/HiC-Pro/environment.yml\
    \ -p WHERE_TO_INSTALL_MY_ENV\nconda activate WHERE_TO_INSTALL_MY_ENV\n</code></pre>\n\
    <h3>\n<a id=\"user-content-using-the-hic-pro-docker-image\" class=\"anchor\" href=\"\
    #using-the-hic-pro-docker-image\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Using the HiC-Pro <code>Docker</code>\
    \ image</h3>\n<p>A docker image is automatically build and available on <a href=\"\
    https://hub.docker.com/repository/docker/nservant/hicpro\" rel=\"nofollow\">Docker\
    \ Hub</a>\nTo pull a Docker image, simply use :</p>\n<pre><code>docker pull nservant/hicpro:latest\n\
    </code></pre>\n<p>Note that the <code>tag</code> may depend on the HiC-Pro version.</p>\n\
    <p>You can also build your own image from the root folder using</p>\n<pre><code>docker\
    \ build -t hicpro:3.0.0 .\n</code></pre>\n<h3>\n<a id=\"user-content-using-hic-pro-through-singularity\"\
    \ class=\"anchor\" href=\"#using-hic-pro-through-singularity\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using\
    \ HiC-Pro through <code>Singularity</code>\n</h3>\n<p>HiC-Pro provides a Singularity\
    \ container to ease its installation process.\nA ready-to-use container is available\
    \ <a href=\"https://zerkalo.curie.fr/partage/HiC-Pro/singularity_images/hicpro_latest_ubuntu.img\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>In order to build you own Singularity image;</p>\n\
    <p>1- Install singularity</p>\n<ul>\n<li>Linux : <a href=\"http://singularity.lbl.gov/install-linux\"\
    \ rel=\"nofollow\">http://singularity.lbl.gov/install-linux</a>\n</li>\n<li>MAC\
    \ : <a href=\"http://singularity.lbl.gov/install-mac\" rel=\"nofollow\">http://singularity.lbl.gov/install-mac</a>\n\
    </li>\n<li>Windows : <a href=\"http://singularity.lbl.gov/install-windows\" rel=\"\
    nofollow\">http://singularity.lbl.gov/install-windows</a>\n</li>\n</ul>\n<p>2-\
    \ Build the singularity HiC-Pro image using the 'Singularity' file available in\
    \ the HiC-Pro root directory.</p>\n<pre><code>sudo singularity build hicpro_latest_ubuntu.img\
    \ MY_INSTALL_PATH/HiC-Pro/envs/Singularity\n</code></pre>\n<p>3- Run HiC-pro</p>\n\
    <p>You can then either use HiC-Pro using the 'exec' command ;</p>\n<pre><code>singularity\
    \ exec hicpro_latest_ubuntu.img HiC-Pro -h\n</code></pre>\n<p>Or directly use\
    \ HiC-Pro within the Singularity shell</p>\n<pre><code>singularity shell hicpro_latest_ubuntu.img\n\
    HiC-Pro -h\n</code></pre>\n<h2>\n<a id=\"user-content-how-to-install-it-\" class=\"\
    anchor\" href=\"#how-to-install-it-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How to install it ?</h2>\n<p>The\
    \ HiC-Pro pipeline requires the following dependencies :</p>\n<ul>\n<li>The <a\
    \ href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\" rel=\"nofollow\"\
    >bowtie2</a> mapper</li>\n<li>Python (&gt;3.7) with <em>pysam (&gt;=0.15.4)</em>,\
    \ <em>bx-python(&gt;=0.8.8)</em>, <em>numpy(&gt;=1.18.1)</em>, and <em>scipy(&gt;=1.4.1)</em>\
    \ libraries.<br>\n<strong>Note that the current version no longer supports python\
    \ 2</strong>\n</li>\n<li>R with the <em>RColorBrewer</em> and <em>ggplot2 (&gt;2.2.1)</em>\
    \ packages</li>\n<li>g++ compiler</li>\n<li>samtools (&gt;1.9)</li>\n<li>Unix\
    \ sort (<strong>which support -V option</strong>) is required ! For Mac OS user,\
    \ please install the GNU core utilities !</li>\n</ul>\n<p>Note that Bowtie &gt;2.2.2\
    \ is strongly recommanded for allele specific analysis.</p>\n<p>To install HiC-Pro,\
    \ be sure to have the appropriate rights and run :</p>\n<pre><code>tar -zxvf HiC-Pro-master.tar.gz\n\
    cd HiC-Pro-master\n## Edit config-install.txt file if necessary\nmake configure\n\
    make install\n</code></pre>\n<p>Note that if some of these dependencies are not\
    \ installed (i.e. not detected in the $PATH), HiC-Pro will try to install them.<br>\n\
    You can also edit the <em>config-install.txt</em> file and manually defined the\
    \ paths to dependencies.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>SYSTEM CONFIGURATION</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>PREFIX</td>\n<td>Path to installation folder</td>\n\
    </tr>\n<tr>\n<td>BOWTIE2_PATH</td>\n<td>Full path the bowtie2 installation directory</td>\n\
    </tr>\n<tr>\n<td>SAMTOOLS_PATH</td>\n<td>Full path to the samtools installation\
    \ directory</td>\n</tr>\n<tr>\n<td>R_PATH</td>\n<td>Full path to the R installation\
    \ directory</td>\n</tr>\n<tr>\n<td>PYTHON_PATH</td>\n<td>Full path to the python\
    \ installation directory</td>\n</tr>\n<tr>\n<td>CLUSTER_SYS</td>\n<td>Scheduler\
    \ to use for cluster submission. Must be TORQUE, SGE, SLURM or LSF</td>\n</tr>\n\
    </tbody>\n</table>\n<h2>\n<a id=\"user-content-annotation-files\" class=\"anchor\"\
    \ href=\"#annotation-files\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Annotation Files</h2>\n<p>In order to process\
    \ the raw data, HiC-Pro requires three annotation files. Note that the pipeline\
    \ is provided with some Human and Mouse annotation files.<br>\n<strong>Please\
    \ be sure that the chromosome names are the same than the ones used in your bowtie\
    \ indexes !</strong></p>\n<ul>\n<li>\n<strong>A BED file</strong> of the restriction\
    \ fragments after digestion. This file depends both of the restriction enzyme\
    \ and the reference genome. See the <a href=\"doc/FAQ.md\">FAQ</a> and the <a\
    \ href=\"doc/UTILS.md\">HiC-Pro utilities</a> for details about how to generate\
    \ this file. A few annotation files are provided with the HiC-Pro sources as examples.</li>\n\
    </ul>\n<pre><code>   chr1   0       16007   HIC_chr1_1    0   +\n   chr1   16007\
    \   24571   HIC_chr1_2    0   +\n   chr1   24571   27981   HIC_chr1_3    0   +\n\
    \   chr1   27981   30429   HIC_chr1_4    0   +\n   chr1   30429   32153   HIC_chr1_5\
    \    0   +\n   chr1   32153   32774   HIC_chr1_6    0   +\n   chr1   32774   37752\
    \   HIC_chr1_7    0   +\n   chr1   37752   38369   HIC_chr1_8    0   +\n   chr1\
    \   38369   38791   HIC_chr1_9    0   +\n   chr1   38791   39255   HIC_chr1_10\
    \   0   +\n   (...)\n</code></pre>\n<ul>\n<li>\n<strong>A table file</strong>\
    \ of chromosomes' size. This file can be easily find on the UCSC genome browser.\
    \ Of note, pay attention to the contigs or scaffolds, and be aware that HiC-pro\
    \ will generate a map per chromosomes pair. For model organisms such as Human\
    \ or Mouse, which are well annotated, we usually recommand to remove all scaffolds.</li>\n\
    </ul>\n<pre><code>   chr1    249250621\n   chr2    243199373\n   chr3    198022430\n\
    \   chr4    191154276\n   chr5    180915260\n   chr6    171115067\n   chr7   \
    \ 159138663\n   chr8    146364022\n   chr9    141213431\n   chr10   135534747\n\
    \   (...)\n</code></pre>\n<ul>\n<li>\n<strong>The bowtie2 indexes</strong>. See\
    \ the <a href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\" rel=\"\
    nofollow\">bowtie2 manual page</a> for details about how to create such indexes.</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-how-to-use-it-\" class=\"anchor\" href=\"#how-to-use-it-\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to use it ?</h2>\n<p>First have a look at the help message !</p>\n\
    <pre><code>  HiC-Pro --help\n  usage : HiC-Pro -i INPUT -o OUTPUT -c CONFIG [-s\
    \ ANALYSIS_STEP] [-p] [-h] [-v]\n  Use option -h|--help for more information\n\
    \n  HiC-Pro 3.0.0\n  ---------------\n  OPTIONS\n\n   -i|--input INPUT : input\
    \ data folder; Must contains a folder per sample with input files\n   -o|--output\
    \ OUTPUT : output folder\n   -c|--conf CONFIG : configuration file for Hi-C processing\n\
    \   [-p|--parallel] : if specified run HiC-Pro on a cluster\n   [-s|--step ANALYSIS_STEP]\
    \ : run only a subset of the HiC-Pro workflow; if not specified the complete workflow\
    \ is run\n      mapping: perform reads alignment - require fast files\n      proc_hic:\
    \ perform Hi-C filtering - require BAM files\n      quality_checks: run Hi-C quality\
    \ control plots\n      merge_persample: merge multiple inputs and remove duplicates\
    \ if specified - require .validPairs files\n      build_contact_maps: Build raw\
    \ inter/intrachromosomal contact maps - require .allValidPairs files\n      ice_norm\
    \ : run ICE normalization on contact maps - require .matrix files\n   [-h|--help]:\
    \ help\n   [-v|--version]: version\n</code></pre>\n<ul>\n<li>\n<p>Copy and edit\
    \ the configuration file <em>'config-hicpro.txt'</em> in your local folder. See\
    \ the <a href=\"doc/MANUAL.md\">manual</a> for details about the configuration\
    \ file</p>\n</li>\n<li>\n<p>Put all input files in a rawdata folder. The input\
    \ files have to be organized with <strong>one folder per sample</strong>, such\
    \ as;</p>\n</li>\n</ul>\n<pre><code>   + PATH_TO_MY_DATA\n     + sample1\n   \
    \    ++ file1_R1.fastq.gz\n       ++ file1_R2.fastq.gz\n       ++ ...\n     +\
    \ sample2\n       ++ file1_R1.fastq.gz\n       ++ file1_R2.fastq.gz\n     *...\n\
    </code></pre>\n<ul>\n<li>Run HiC-Pro on your laptop in standalone model</li>\n\
    </ul>\n<pre><code>    MY_INSTALL_PATH/bin/HiC-Pro -i FULL_PATH_TO_DATA_FOLDER\
    \ -o FULL_PATH_TO_OUTPUTS -c MY_LOCAL_CONFIG_FILE\n</code></pre>\n<ul>\n<li>Run\
    \ HiC-Pro on a cluster (TORQUE/SGE/SLURM/LSF)</li>\n</ul>\n<pre><code>   MY_INSTALL_PATH/bin/HiC-Pro\
    \ -i FULL_PATH_TO_DATA_FOLDER -o FULL_PATH_TO_OUTPUTS -c MY_LOCAL_CONFIG_FILE\
    \ -p\n</code></pre>\n<p>In the latter case, you will have the following message\
    \ :</p>\n<pre><code>  Please run HiC-Pro in two steps :\n  1- The following command\
    \ will launch the parallel workflow through 12 torque jobs:\n  qsub HiCPro_step1.sh\n\
    \  2- The second command will merge all outputs to generate the contact maps:\n\
    \  qsub HiCPro_step2.sh\n</code></pre>\n<p>Execute the displayed command from\
    \ the output folder:</p>\n<pre><code>  qsub HiCPro_step1.sh\n</code></pre>\n<p>Once\
    \ executed succesfully (may take several hours), run the step using:</p>\n<pre><code>\
    \  qsub HiCPro_step2.sh\n</code></pre>\n<h2>\n<a id=\"user-content-test-dataset\"\
    \ class=\"anchor\" href=\"#test-dataset\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Test Dataset</h2>\n<p>The test\
    \ dataset and associated results are available <a href=\"https://zerkalo.curie.fr/partage/HiC-Pro/\"\
    \ rel=\"nofollow\">here</a>.\nSmall fastq files (2M reads) extracted from the\
    \ Dixon et al. 2012 paper are available for test.</p>\n<pre><code> ## Get the\
    \ data. Will download a test_data folder and a configuration file\n wget https://zerkalo.curie.fr/partage/HiC-Pro/HiCPro_testdata.tar.gz\
    \ &amp;&amp; tar -zxvf HiCPro_testdata.tar.gz\n\n ## Edit the configuration file\
    \ and set the path to Human bowtie2 indexes\n\n ## Run HiC-Pro\n time HICPRO_INSTALL_DIR/bin/HiC-Pro\
    \ -c config_test_latest.txt -i test_data -o hicpro_latest_test\n\nRun HiC-Pro\
    \ 3.0.0\n--------------------------------------------\nThu Mar 19, 12:18:10 (UTC+0100)\n\
    Bowtie2 alignment step1 ...\nLogs: logs/dixon_2M_2/mapping_step1.log\nLogs: logs/dixon_2M/mapping_step1.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:18:57 (UTC+0100)\n\
    Bowtie2 alignment step2 ...\nLogs: logs/dixon_2M_2/mapping_step2.log\nLogs: logs/dixon_2M/mapping_step2.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:19:08 (UTC+0100)\n\
    Combine R1/R2 alignment files ...\nLogs: logs/dixon_2M_2/mapping_combine.log\n\
    Logs: logs/dixon_2M/mapping_combine.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:19:13 (UTC+0100)\nMapping statistics for R1 and R2 tags ...\nLogs:\
    \ logs/dixon_2M_2/mapping_stats.log\nLogs: logs/dixon_2M/mapping_stats.log\n\n\
    --------------------------------------------\nThu Mar 19, 12:19:15 (UTC+0100)\n\
    Pairing of R1 and R2 tags ...\nLogs: logs/dixon_2M_2/mergeSAM.log\nLogs: logs/dixon_2M/mergeSAM.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:19:25 (UTC+0100)\n\
    Assign alignments to restriction fragments ...\nLogs: logs/dixon_2M_2/mapped_2hic_fragments.log\n\
    Logs: logs/dixon_2M/mapped_2hic_fragments.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:20:10 (UTC+0100)\nMerge chunks from the same sample ...\nLogs:\
    \ logs/dixon_2M/merge_valid_interactions.log\nLogs: logs/dixon_2M_2/merge_valid_interactions.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:20:11 (UTC+0100)\n\
    Merge stat files per sample ...\nLogs: logs/dixon_2M/merge_stats.log\nLogs: logs/dixon_2M_2/merge_stats.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:20:11 (UTC+0100)\n\
    Run quality checks for all samples ...\nLogs: logs/dixon_2M/make_Rplots.log\n\
    Logs: logs/dixon_2M_2/make_Rplots.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:20:22 (UTC+0100)\nGenerate binned matrix files ...\nLogs: logs/dixon_2M/build_raw_maps.log\n\
    Logs: logs/dixon_2M_2/build_raw_maps.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:20:22 (UTC+0100)\nRun ICE Normalization ...\nLogs: logs/dixon_2M/ice_500000.log\n\
    Logs: logs/dixon_2M/ice_1000000.log\nLogs: logs/dixon_2M_2/ice_500000.log\nLogs:\
    \ logs/dixon_2M_2/ice_1000000.log\n\nreal\t2m15,736s\nuser\t4m3,277s\nsys\t0m24,423s\n\
    \n</code></pre>\n"
  stargazers_count: 245
  subscribers_count: 20
  topics: []
  updated_at: 1624371447.0
onuryukselen/singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity.3.0
  full_name: onuryukselen/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>singularity</h1>

    <p>Development Branch</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1623903591.0
openPMD/openPMD-api:
  data_format: 2
  description: ':floppy_disk: C++ & Python API for Scientific I/O'
  filenames:
  - Singularity
  full_name: openPMD/openPMD-api
  latest_release: 0.13.4
  readme: "<h1>\n<a id=\"user-content-c--python-api-for-scientific-io-with-openpmd\"\
    \ class=\"anchor\" href=\"#c--python-api-for-scientific-io-with-openpmd\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>C++\
    \ &amp; Python API for Scientific I/O with openPMD</h1>\n<p><a href=\"https://github.com/openPMD/openPMD-standard/releases\"\
    ><img src=\"https://camo.githubusercontent.com/5957dc11cb68f6705ef9ef6683152c6c4fcc057a24dff340e1e8bada1bee0d01/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e504d442d312e302e302d2d312e312e302d626c7565\"\
    \ alt=\"Supported openPMD Standard\" data-canonical-src=\"https://img.shields.io/badge/openPMD-1.0.0--1.1.0-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.openpmd.org/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2d54fa5adcf7ca50d2cdb45823be5064379b613d526fa06f6e193ba2ce616318/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4150492d446f787967656e2d626c7565\"\
    \ alt=\"Doxygen\" data-canonical-src=\"https://img.shields.io/badge/API-Doxygen-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://gitter.im/openPMD/API\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/f551141eea2176c34aa163092549d6fdde9a220d605d6efe9128b024c6e5932b/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6f70656e504d442f415049\"\
    \ alt=\"Gitter chat\" data-canonical-src=\"https://img.shields.io/gitter/room/openPMD/API\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"Supported Platforms\" title=\"Supported Platforms\" data-canonical-src=\"\
    https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://www.gnu.org/licenses/lgpl-3.0.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4fc8091716dbd967fa2a82eef3d29227110e5081f3128aaa38663e547c24f812/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4c47504c76332d626c7565\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/license-LGPLv3-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://doi.org/10.14278/rodare.27\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/104f6901ae04bff8385acc8273bb276ab84656a927a418108b940d09fd6e5d7c/68747470733a2f2f726f646172652e687a64722e64652f62616467652f444f492f31302e31343237382f726f646172652e32372e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://rodare.hzdr.de/badge/DOI/10.14278/rodare.27.svg\"\
    \ style=\"max-width:100%;\"></a><br>\n<a href=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0d568047fbbd300af56b766d9a4235a20534485f5827194e859d4f5c20e58334/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6f70656e706d642f6f70656e706d642d6170692f6261646765\"\
    \ alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api/badge\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:cpp\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/63a7f9e783999e3afc03ef38ee82e2048017e4e6d279ff4120ad8b8718480ccd/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6370702f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: C/C++\" data-canonical-src=\"https://img.shields.io/lgtm/grade/cpp/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:python\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5046bf66a4612476a030d38de817c23fa03990183d2d74fa92c5f1379feb5d09/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: Python\" data-canonical-src=\"https://img.shields.io/lgtm/grade/python/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/alerts/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/85e32deb8face392eea9bfa2be4da4c11ca7c0f834fa069223fbc63758b68c4f/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f616c657274732f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: Total alerts\" data-canonical-src=\"https://img.shields.io/lgtm/alerts/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://coveralls.io/github/openPMD/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f0487a8fc14d269210ae8ce80b556d4afcd93684f02e61386d2d02daa4423cbd/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6f70656e504d442f6f70656e504d442d6170692f6261646765\"\
    \ alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/openPMD/openPMD-api/badge\"\
    \ style=\"max-width:100%;\"></a><br>\n<a href=\"https://openpmd-api.readthedocs.io/en/latest/?badge=latest\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8f438ca4eaa308f982d0a28c48f05bf63ca1a86e52c3d2817f6d7559d1dee68e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6f70656e706d642d6170692f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/openpmd-api/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.com/openPMD/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8278d89e3634e25a818a2d673fe997c2aa5a09fd5995f716aeffa743388030ee/68747470733a2f2f7472617669732d63692e636f6d2f6f70656e504d442f6f70656e504d442d6170692e7376673f6272616e63683d646576\"\
    \ alt=\"Linux/OSX Build Status dev\" data-canonical-src=\"https://travis-ci.com/openPMD/openPMD-api.svg?branch=dev\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://ci.appveyor.com/project/ax3l/openpmd-api/branch/dev\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/30679331f3c6a5ae54970eda85f36a30347dee8a9111448d7aa48587fd524a1d/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f78393571346e36323070716b306530742f6272616e63682f6465763f7376673d74727565\"\
    \ alt=\"Windows Build Status dev\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/x95q4n620pqk0e0t/branch/dev?svg=true\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/openPMD/openPMD-api/actions?query=workflow%3Awheels\"\
    ><img src=\"https://github.com/openPMD/openPMD-api/workflows/wheels/badge.svg?branch=wheels&amp;event=push\"\
    \ alt=\"PyPI Wheel Release\" style=\"max-width:100%;\"></a>\n<a href=\"https://dev.azure.com/axelhuebl/openPMD-api/_build/latest?definitionId=1&amp;branchName=azure_install\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1fc9c860bb1fc8e7e145ea9dd6723b9ac6ac2743c195e5e899f6cfa3d38a5a69/68747470733a2f2f6465762e617a7572652e636f6d2f6178656c687565626c2f6f70656e504d442d6170692f5f617069732f6275696c642f7374617475732f6f70656e504d442e6f70656e504d442d6170693f6272616e63684e616d653d617a7572655f696e7374616c6c266c6162656c3d6e696768746c792532307061636b61676573\"\
    \ alt=\"Nightly Packages Status\" data-canonical-src=\"https://dev.azure.com/axelhuebl/openPMD-api/_apis/build/status/openPMD.openPMD-api?branchName=azure_install&amp;label=nightly%20packages\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://scan.coverity.com/projects/openpmd-openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0b068d7b5718aafccb46e2ee35f8249938ef189a36ba353c15222ed5e6f65e49/68747470733a2f2f7363616e2e636f7665726974792e636f6d2f70726f6a656374732f31373630322f62616467652e737667\"\
    \ alt=\"Coverity Scan Build Status\" data-canonical-src=\"https://scan.coverity.com/projects/17602/badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>openPMD is an open meta-data schema that\
    \ provides meaning and self-description for data sets in science and engineering.\n\
    See <a href=\"https://github.com/openPMD/openPMD-standard\">the openPMD standard</a>\
    \ for details of this schema.</p>\n<p>This library provides a reference API for\
    \ openPMD data handling.\nSince openPMD is a schema (or markup) on top of portable,\
    \ hierarchical file formats, this library implements various backends such as\
    \ HDF5, ADIOS1, ADIOS2 and JSON.\nWriting &amp; reading through those backends\
    \ and their associated files is supported for serial and <a href=\"https://www.mpi-forum.org/docs/\"\
    \ rel=\"nofollow\">MPI-parallel</a> workflows.</p>\n<h2>\n<a id=\"user-content-usage\"\
    \ class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-c\"\
    \ class=\"anchor\" href=\"#c\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>C++</h3>\n<p><a href=\"https://isocpp.org/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/042b5af19c304a2d4f876865d00baa90a284f2d35056ed9728c944befbb07733/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231342d79656c6c6f77677265656e\"\
    \ alt=\"C++14\" title=\"C++14 API\" data-canonical-src=\"https://img.shields.io/badge/language-C%2B%2B14-yellowgreen\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ alt=\"C++14 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-c++\"\
    ><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"\
    pl-pds\">&lt;</span>openPMD/openPMD.hpp<span class=\"pl-pds\">&gt;</span></span>\n\
    #<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">//</span> ...</span>\n\n<span class=\"pl-k\">auto</span>\
    \ s = openPMD::Series(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>samples/git-sample/data%T.h5<span\
    \ class=\"pl-pds\">\"</span></span>, openPMD::Access::READ_ONLY);\n\n<span class=\"\
    pl-k\">for</span>( <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp;\
    \ i : s.iterations ) {\n    std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>Iteration: <span class=\"pl-pds\">\"</span></span> &lt;&lt;\
    \ i.<span class=\"pl-smi\">first</span> &lt;&lt; <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>;\n\
    \n    <span class=\"pl-k\">for</span>( <span class=\"pl-k\">auto</span> <span\
    \ class=\"pl-k\">const</span>&amp; m : i.<span class=\"pl-smi\">second</span>.<span\
    \ class=\"pl-smi\">meshes</span> ) {\n        std::cout &lt;&lt; <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>  Mesh '<span class=\"pl-pds\">\"</span></span>\
    \ &lt;&lt; m.<span class=\"pl-smi\">first</span> &lt;&lt; <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>' attributes:<span class=\"pl-cce\">\\n</span><span\
    \ class=\"pl-pds\">\"</span></span>;\n        <span class=\"pl-k\">for</span>(\
    \ <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp; val\
    \ : m.<span class=\"pl-smi\">second</span>.<span class=\"pl-c1\">attributes</span>()\
    \ )\n            std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>    <span class=\"pl-pds\">\"</span></span> &lt;&lt; val &lt;&lt; <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span\
    \ class=\"pl-pds\">'</span></span>;\n    }\n\n    <span class=\"pl-k\">for</span>(\
    \ <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp; p :\
    \ i.<span class=\"pl-smi\">second</span>.<span class=\"pl-smi\">particles</span>\
    \ ) {\n        std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>  Particle species '<span class=\"pl-pds\">\"</span></span> &lt;&lt;\
    \ p.<span class=\"pl-smi\">first</span> &lt;&lt; <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>' attributes:<span class=\"pl-cce\">\\n</span><span class=\"\
    pl-pds\">\"</span></span>;\n        <span class=\"pl-k\">for</span>( <span class=\"\
    pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp; val : p.<span class=\"\
    pl-smi\">second</span>.<span class=\"pl-c1\">attributes</span>() )\n         \
    \   std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> \
    \   <span class=\"pl-pds\">\"</span></span> &lt;&lt; val &lt;&lt; <span class=\"\
    pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"\
    pl-pds\">'</span></span>;\n    }\n}</pre></div>\n<h3>\n<a id=\"user-content-python\"\
    \ class=\"anchor\" href=\"#python\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Python</h3>\n<p><a href=\"https://www.python.org/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/acd285afab5d7ddd4942e5215ade53e84551c9d7d635642ba92c19fde7d4345b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e332d79656c6c6f77677265656e\"\
    \ alt=\"Python3\" title=\"Python3 API\" data-canonical-src=\"https://img.shields.io/badge/language-Python3-yellowgreen\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ alt=\"Python3 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">openpmd_api</span>\
    \ <span class=\"pl-k\">as</span> <span class=\"pl-s1\">io</span>\n\n<span class=\"\
    pl-c\"># ...</span>\n\n<span class=\"pl-s1\">series</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-s1\">io</span>.<span class=\"pl-v\">Series</span>(<span\
    \ class=\"pl-s\">\"samples/git-sample/data%T.h5\"</span>, <span class=\"pl-s1\"\
    >io</span>.<span class=\"pl-v\">Access</span>.<span class=\"pl-s1\">read_only</span>)\n\
    \n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">k_i</span>, <span class=\"\
    pl-s1\">i</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">series</span>.<span\
    \ class=\"pl-s1\">iterations</span>.<span class=\"pl-en\">items</span>():\n  \
    \  <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"Iteration: {0}\"\
    </span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">k_i</span>))\n\
    \n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">k_m</span>, <span\
    \ class=\"pl-s1\">m</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\"\
    >i</span>.<span class=\"pl-s1\">meshes</span>.<span class=\"pl-en\">items</span>():\n\
    \        <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"  Mesh '{0}'\
    \ attributes:\"</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\"\
    >k_m</span>))\n        <span class=\"pl-k\">for</span> <span class=\"pl-s1\">a</span>\
    \ <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">m</span>.<span class=\"\
    pl-s1\">attributes</span>:\n            <span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s\">\"    {0}\"</span>.<span class=\"pl-en\">format</span>(<span\
    \ class=\"pl-s1\">a</span>))\n\n    <span class=\"pl-k\">for</span> <span class=\"\
    pl-s1\">k_p</span>, <span class=\"pl-s1\">p</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">i</span>.<span class=\"pl-s1\">particles</span>.<span\
    \ class=\"pl-en\">items</span>():\n        <span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s\">\"  Particle species '{0}' attributes:\"</span>.<span class=\"\
    pl-en\">format</span>(<span class=\"pl-s1\">k_p</span>))\n        <span class=\"\
    pl-k\">for</span> <span class=\"pl-s1\">a</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">p</span>.<span class=\"pl-s1\">attributes</span>:\n  \
    \          <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"    {0}\"\
    </span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">a</span>))</pre></div>\n\
    <h3>\n<a id=\"user-content-more\" class=\"anchor\" href=\"#more\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>More!</h3>\n\
    <p>Curious?\nOur manual shows full <a href=\"https://openpmd-api.readthedocs.io/en/latest/usage/firstwrite.html\"\
    \ rel=\"nofollow\">read &amp; write examples</a>, both serial and MPI-parallel!</p>\n\
    <h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<p>Required:</p>\n<ul>\n<li>CMake 3.15.0+</li>\n\
    <li>C++14 capable compiler, e.g. g++ 5.0+, clang 5.0+, VS 2017+</li>\n</ul>\n\
    <p>Shipped internally in <code>share/openPMD/thirdParty/</code>:</p>\n<ul>\n<li>\n\
    <a href=\"https://github.com/mpark/variant\">MPark.Variant</a> 1.4.0+ (<a href=\"\
    https://github.com/mpark/variant/blob/master/LICENSE.md\">BSL-1.0</a>)</li>\n\
    <li>\n<a href=\"https://github.com/catchorg/Catch2\">Catch2</a> 2.13.4+ (<a href=\"\
    https://github.com/catchorg/Catch2/blob/master/LICENSE.txt\">BSL-1.0</a>)</li>\n\
    <li>\n<a href=\"https://github.com/pybind/pybind11\">pybind11</a> 2.6.2+ (<a href=\"\
    https://github.com/pybind/pybind11/blob/master/LICENSE\">new BSD</a>)</li>\n<li>\n\
    <a href=\"https://github.com/nlohmann/json\">NLohmann-JSON</a> 3.9.1+ (<a href=\"\
    https://github.com/nlohmann/json/blob/develop/LICENSE.MIT\">MIT</a>)</li>\n</ul>\n\
    <p>I/O backends:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/JSON\"\
    \ rel=\"nofollow\">JSON</a></li>\n<li>\n<a href=\"https://support.hdfgroup.org/HDF5\"\
    \ rel=\"nofollow\">HDF5</a> 1.8.13+ (optional)</li>\n<li>\n<a href=\"https://www.olcf.ornl.gov/center-projects/adios\"\
    \ rel=\"nofollow\">ADIOS1</a> 1.13.1+ (optional)</li>\n<li>\n<a href=\"https://github.com/ornladios/ADIOS2\"\
    >ADIOS2</a> 2.7.0+ (optional)</li>\n</ul>\n<p>while those can be built either\
    \ with or without:</p>\n<ul>\n<li>MPI 2.1+, e.g. OpenMPI 1.6.5+ or MPICH2</li>\n\
    </ul>\n<p>Optional language bindings:</p>\n<ul>\n<li>Python:\n<ul>\n<li>Python\
    \ 3.6 - 3.9</li>\n<li>pybind11 2.6.2+</li>\n<li>numpy 1.15+</li>\n<li>mpi4py 2.1+\
    \ (optional, for MPI)</li>\n<li>pandas 1.0+ (optional, for dataframes)</li>\n\
    <li>dask 2021+ (optional, for dask dataframes)</li>\n</ul>\n</li>\n</ul>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p><a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/580cdb6331254f66680bd967326d9630f5124291efd5ace18549fbb93cbae6db/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737061636b2e696f2d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Spack Package\" data-canonical-src=\"https://img.shields.io/badge/spack.io-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3c3728308a9e416589fa8b3b8168967287c515037bfbd4b40f1b14f266de56fb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612e696f2d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Conda Package\" data-canonical-src=\"https://img.shields.io/badge/conda.io-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/openPMD/homebrew-openPMD\"\
    ><img src=\"https://camo.githubusercontent.com/92b7b7bb69b2b17d1981ae5fdcdb32f971ee57f54a98ea4804e93fd0a2eb58ef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772e73682d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Brew Package\" data-canonical-src=\"https://img.shields.io/badge/brew.sh-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4eeb2c48c0e554ea8dbe8e90f73a6f2d217e775e0bd5e5cb90ddf4619ef3a6a0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707970692e6f72672d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"PyPI Package\" data-canonical-src=\"https://img.shields.io/badge/pypi.org-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://cmake.org\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/2babf79954ea524c24f2f9b1cfa05728fdaccbe0a80c2da6a7a7595cc131894c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66726f6d5f736f757263652d434d616b652d627269676874677265656e\"\
    \ alt=\"From Source\" data-canonical-src=\"https://img.shields.io/badge/from_source-CMake-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Our community loves to help each other.\n\
    Please <a href=\"https://github.com/openPMD/openPMD-api/issues/new?labels=install&amp;template=install_problem.md\"\
    >report installation problems</a> in case you should get stuck.</p>\n<p>Choose\
    \ <em>one</em> of the install methods below to get started:</p>\n<h3>\n<a id=\"\
    user-content-spack\" class=\"anchor\" href=\"#spack\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://spack.io\"\
    \ rel=\"nofollow\">Spack</a>\n</h3>\n<p><a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/becffbecb6a50502286f02ec86c4e62f239f3671148d11341152e0dc695a2fc9/68747470733a2f2f696d672e736869656c64732e696f2f737061636b2f762f6f70656e706d642d617069\"\
    \ alt=\"Spack Version\" data-canonical-src=\"https://img.shields.io/spack/v/openpmd-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://spack.io\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\"\
    \ alt=\"Spack Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b29fc54abf92a2a6d1d2c70159eb276df53b67a1294ae3f82b1b0bf22a3c586b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392c5f646576656c6f706d656e742c5f4850432d627269676874677265656e\"\
    \ alt=\"Spack Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29,_development,_HPC-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:           \
    \    +python +adios1 -adios2 -hdf5 -mpi</span>\nspack install openpmd-api\nspack\
    \ load -r openpmd-api</pre></div>\n<h3>\n<a id=\"user-content-conda\" class=\"\
    anchor\" href=\"#conda\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"https://conda.io\" rel=\"nofollow\"\
    >Conda</a>\n</h3>\n<p><a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3ad2b345bb3589aa2d733ca20df72938ed54a48342b69f7cbe9eacab43d84549/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f6f70656e706d642d617069\"\
    \ alt=\"Conda Version\" data-canonical-src=\"https://img.shields.io/conda/vn/conda-forge/openpmd-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"Conda Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\"\
    \ alt=\"Conda Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a51d3cd2bfa3c6b01bd0f2e36abc206fb9db5700105fac2acd2c2105fced2ec4/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f636f6e64612d666f7267652f6f70656e706d642d617069\"\
    \ alt=\"Conda Downloads\" data-canonical-src=\"https://img.shields.io/conda/dn/conda-forge/openpmd-api\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:           \
    \           OpenMPI support  =*=mpi_openmpi*</span>\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> optional:                        MPICH support  =*=mpi_mpich*</span>\n\
    conda create -n openpmd -c conda-forge openpmd-api\nconda activate openpmd</pre></div>\n\
    <h3>\n<a id=\"user-content-brew\" class=\"anchor\" href=\"#brew\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"https://brew.sh\" rel=\"nofollow\">Brew</a>\n</h3>\n<p><a href=\"https://github.com/openPMD/homebrew-openPMD\"\
    ><img src=\"https://camo.githubusercontent.com/d873c8c473fece8abf1c34a8299a50b7ee0da9772eb50cce8649e7ca32468a6c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772d6c61746573745f76657273696f6e2d6f72616e6765\"\
    \ alt=\"Brew Version\" data-canonical-src=\"https://img.shields.io/badge/brew-latest_version-orange\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://docs.brew.sh/Homebrew-on-Linux\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\"\
    \ alt=\"Brew Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://brew.sh\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/7b767b67facc26db7d850ae5c3155fa949048991dde7a0f5471c1e6862f0a586/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392d627269676874677265656e\"\
    \ alt=\"Brew Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>brew tap openpmd/openpmd\nbrew install openpmd-api</pre></div>\n<h3>\n<a\
    \ id=\"user-content-pypi\" class=\"anchor\" href=\"#pypi\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"\
    https://pypi.org\" rel=\"nofollow\">PyPI</a>\n</h3>\n<p><a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fc28bd368523d0b8adab5f4b414aebb304743130eef42bca203f4e9eed428ae7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f70656e504d442d617069\"\
    \ alt=\"PyPI Version\" data-canonical-src=\"https://img.shields.io/pypi/v/openPMD-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api/#files\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"PyPI Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\"\
    \ alt=\"PyPI Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7cef25e887c028f65d5d7e20f3e7abe394ab328ecb499e34e47460afd2c78558/68747470733a2f2f696d672e736869656c64732e696f2f707970692f666f726d61742f6f70656e504d442d617069\"\
    \ alt=\"PyPI Format\" data-canonical-src=\"https://img.shields.io/pypi/format/openPMD-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/82acdd95515851a5ba4a3006871151f76cfe4d6583f6c24e80bd0fd29d391845/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6f70656e504d442d617069\"\
    \ alt=\"PyPI Downloads\" data-canonical-src=\"https://img.shields.io/pypi/dm/openPMD-api\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>On very old macOS versions (&lt;10.9)\
    \ or on exotic processor architectures, this install method <em>compiles from\
    \ source</em> against the found installations of HDF5, ADIOS1, ADIOS2, and/or\
    \ MPI (in system paths, from other package managers, or loaded via a module system,\
    \ ...).</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> we need pip 19 or newer</span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> optional:                   --user</span>\n\
    python3 -m pip install -U pip\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ optional:                        --user</span>\npython3 -m pip install openpmd-api</pre></div>\n\
    <p>If MPI-support shall be enabled, we always have to recompile:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> optional:                                    --user</span>\npython3\
    \ -m pip install -U pip setuptools wheel\npython3 -m pip install -U cmake\n\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:                 \
    \                                                  --user</span>\nopenPMD_USE_MPI=ON\
    \ python3 -m pip install openpmd-api --no-binary openpmd-api</pre></div>\n<p>For\
    \ some exotic architectures and compilers, you might need to disable a compiler\
    \ feature called <a href=\"https://en.wikipedia.org/wiki/Interprocedural_optimization\"\
    \ rel=\"nofollow\">link-time/interprocedural optimization</a> if you encounter\
    \ linking problems:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-k\">export</span> CMAKE_INTERPROCEDURAL_OPTIMIZATION=OFF\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> optional:                               \
    \                 --user</span>\npython3 -m pip install openpmd-api --no-binary\
    \ openpmd-api</pre></div>\n<h3>\n<a id=\"user-content-from-source\" class=\"anchor\"\
    \ href=\"#from-source\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>From Source</h3>\n<p><a href=\"https://cmake.org\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0a40953107090abff3b564ec3436668756b873cd4d9e021738a046781a5a0e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d646576656c6f706d656e742d627269676874677265656e\"\
    \ alt=\"Source Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-development-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>openPMD-api can also be built and installed\
    \ from source using <a href=\"https://cmake.org/\" rel=\"nofollow\">CMake</a>:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/openPMD/openPMD-api.git\n\
    \nmkdir openPMD-api-build\n<span class=\"pl-c1\">cd</span> openPMD-api-build\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optional: for full tests,\
    \ with unzip</span>\n../openPMD-api/share/openPMD/download_samples.sh\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> for own install prefix append:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DCMAKE_INSTALL_PREFIX=$HOME/somepath</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for options append:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DopenPMD_USE_...=...</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> e.g. for python support add:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DopenPMD_USE_PYTHON=ON</span>\n\
    cmake ../openPMD-api\n\ncmake --build <span class=\"pl-c1\">.</span>\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> optional</span>\nctest\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> sudo might be required for system\
    \ paths</span>\ncmake --build <span class=\"pl-c1\">.</span> --target install</pre></div>\n\
    <p>The following options can be added to the <code>cmake</code> call to control\
    \ features.\nCMake controls options with prefixed <code>-D</code>, e.g. <code>-DopenPMD_USE_MPI=OFF</code>:</p>\n\
    <table>\n<thead>\n<tr>\n<th>CMake Option</th>\n<th>Values</th>\n<th>Description</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td><code>openPMD_USE_MPI</code></td>\n<td>\n\
    <strong>AUTO</strong>/ON/OFF</td>\n<td>Parallel, Multi-Node I/O for clusters</td>\n\
    </tr>\n<tr>\n<td><code>openPMD_USE_HDF5</code></td>\n<td>\n<strong>AUTO</strong>/ON/OFF</td>\n\
    <td>HDF5 backend (<code>.h5</code> files)</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_ADIOS1</code></td>\n\
    <td>\n<strong>AUTO</strong>/ON/OFF</td>\n<td>ADIOS1 backend (<code>.bp</code>\
    \ files up to version BP3)</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_ADIOS2</code></td>\n\
    <td>\n<strong>AUTO</strong>/ON/OFF</td>\n<td>ADIOS2 backend (<code>.bp</code>\
    \ files in BP3, BP4 or higher)</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_PYTHON</code></td>\n\
    <td>\n<strong>AUTO</strong>/ON/OFF</td>\n<td>Enable Python bindings</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_INVASIVE_TESTS</code></td>\n<td>ON/<strong>OFF</strong>\n\
    </td>\n<td>Enable unit tests that modify source code <sup>1</sup>\n</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_VERIFY</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Enable internal VERIFY (assert) macro independent of build type <sup>2</sup>\n\
    </td>\n</tr>\n<tr>\n<td><code>openPMD_INSTALL</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Add installation targets</td>\n</tr>\n<tr>\n<td><code>Python_EXECUTABLE</code></td>\n\
    <td>(newest found)</td>\n<td>Path to Python executable</td>\n</tr>\n</tbody>\n\
    </table>\n<p><sup>1</sup> <em>e.g. changes C++ visibility keywords, breaks MSVC</em>\n\
    <sup>2</sup> <em>this includes most pre-/post-condition checks, disabling without\
    \ specific cause is highly discouraged</em></p>\n<p>Additionally, the following\
    \ libraries are shipped internally.\nThe following options allow to switch to\
    \ external installs:</p>\n<table>\n<thead>\n<tr>\n<th>CMake Option</th>\n<th>Values</th>\n\
    <th>Library</th>\n<th>Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>openPMD_USE_INTERNAL_VARIANT</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>MPark.Variant</td>\n<td>1.4.0+</td>\n\
    </tr>\n<tr>\n<td><code>openPMD_USE_INTERNAL_CATCH</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Catch2</td>\n<td>2.13.4+</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_INTERNAL_PYBIND11</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>pybind11</td>\n<td>2.6.2+</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_INTERNAL_JSON</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>NLohmann-JSON</td>\n<td>3.9.1+</td>\n</tr>\n</tbody>\n</table>\n<p>By default,\
    \ this will build as a shared library (<code>libopenPMD.[so|dylib|dll]</code>)\
    \ and installs also its headers.\nIn order to build a static library, append <code>-DBUILD_SHARED_LIBS=OFF</code>\
    \ to the <code>cmake</code> command.\nYou can only build a static or a shared\
    \ library at a time.</p>\n<p>By default, the <code>Release</code> version is built.\n\
    In order to build with debug symbols, pass <code>-DCMAKE_BUILD_TYPE=Debug</code>\
    \ to your <code>cmake</code> command.</p>\n<p>By default, tests, examples and\
    \ command line tools are built.\nIn order to skip building those, pass <code>OFF</code>\
    \ to these <code>cmake</code> options:</p>\n<table>\n<thead>\n<tr>\n<th>CMake\
    \ Option</th>\n<th>Values</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td><code>openPMD_BUILD_TESTING</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Build tests</td>\n</tr>\n<tr>\n<td><code>openPMD_BUILD_EXAMPLES</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Build examples</td>\n</tr>\n<tr>\n<td><code>openPMD_BUILD_CLI_TOOLS</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Build command-line tools</td>\n</tr>\n\
    </tbody>\n</table>\n<h2>\n<a id=\"user-content-linking-to-your-project\" class=\"\
    anchor\" href=\"#linking-to-your-project\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linking to your project</h2>\n\
    <p>The install will contain header files and libraries in the path set with <code>-DCMAKE_INSTALL_PREFIX</code>.</p>\n\
    <h3>\n<a id=\"user-content-cmake\" class=\"anchor\" href=\"#cmake\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CMake</h3>\n\
    <p>If your project is using CMake for its build, one can conveniently use our\
    \ provided <code>openPMDConfig.cmake</code> package which is installed alongside\
    \ the library.</p>\n<p>First set the following environment hint if openPMD-api\
    \ was <em>not</em> installed in a system path:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional: only needed\
    \ if installed outside of system paths</span>\n<span class=\"pl-k\">export</span>\
    \ CMAKE_PREFIX_PATH=<span class=\"pl-smi\">$HOME</span>/somepath:<span class=\"\
    pl-smi\">$CMAKE_PREFIX_PATH</span></pre></div>\n<p>Use the following lines in\
    \ your project's <code>CMakeLists.txt</code>:</p>\n<div class=\"highlight highlight-source-cmake\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> supports:           \
    \            COMPONENTS MPI NOMPI HDF5 ADIOS1 ADIOS2</span>\n<span class=\"pl-c1\"\
    >find_package</span>(openPMD 0.9.0 <span class=\"pl-k\">CONFIG</span>)\n\n<span\
    \ class=\"pl-k\">if</span>(openPMD_FOUND)\n    <span class=\"pl-c1\">target_link_libraries</span>(YourTarget\
    \ <span class=\"pl-k\">PRIVATE</span> openPMD::openPMD)\n<span class=\"pl-k\"\
    >endif</span>()</pre></div>\n<p><em>Alternatively</em>, add the openPMD-api repository\
    \ source directly to your project and use it via:</p>\n<div class=\"highlight\
    \ highlight-source-cmake\"><pre><span class=\"pl-c1\">add_subdirectory</span>(<span\
    \ class=\"pl-s\">\"path/to/source/of/openPMD-api\"</span>)\n\n<span class=\"pl-c1\"\
    >target_link_libraries</span>(YourTarget <span class=\"pl-k\">PRIVATE</span> openPMD::openPMD)</pre></div>\n\
    <p>For development workflows, you can even automatically download and build openPMD-api\
    \ from within a depending CMake project.\nJust replace the <code>add_subdirectory</code>\
    \ call with:</p>\n<div class=\"highlight highlight-source-cmake\"><pre><span class=\"\
    pl-c1\">include</span>(FetchContent)\n<span class=\"pl-c1\">set</span>(CMAKE_POLICY_DEFAULT_CMP0077\
    \ <span class=\"pl-k\">NEW</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_CLI_TOOLS\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_EXAMPLES\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_TESTING\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> set(openPMD_BUILD_SHARED_LIBS OFF)  # precedence over BUILD_SHARED_LIBS\
    \ if needed; or:</span>\n<span class=\"pl-c1\">set</span>(openPMD_INSTALL <span\
    \ class=\"pl-smi\">${BUILD_SHARED_LIBS}</span>)  <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> only install if used as shared a library</span>\n<span class=\"\
    pl-c1\">set</span>(openPMD_USE_PYTHON <span class=\"pl-k\">OFF</span>)\nFetchContent_Declare(openPMD\n\
    \  GIT_REPOSITORY <span class=\"pl-s\">\"https://github.com/openPMD/openPMD-api.git\"\
    </span>\n  GIT_TAG        <span class=\"pl-s\">\"dev\"</span>)\nFetchContent_MakeAvailable(openPMD)</pre></div>\n\
    <h3>\n<a id=\"user-content-manually\" class=\"anchor\" href=\"#manually\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Manually</h3>\n\
    <p>If your (Linux/OSX) project is build by calling the compiler directly or uses\
    \ a manually written <code>Makefile</code>, consider using our <code>openPMD.pc</code>\
    \ helper file for <code>pkg-config</code> which are installed alongside the library.</p>\n\
    <p>First set the following environment hint if openPMD-api was <em>not</em> installed\
    \ in a system path:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> optional: only needed if installed\
    \ outside of system paths</span>\n<span class=\"pl-k\">export</span> PKG_CONFIG_PATH=<span\
    \ class=\"pl-smi\">$HOME</span>/somepath/lib/pkgconfig:<span class=\"pl-smi\"\
    >$PKG_CONFIG_PATH</span></pre></div>\n<p>Additional linker and compiler flags\
    \ for your project are available via:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> switch to check if openPMD-api\
    \ was build as static library</span>\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> (via BUILD_SHARED_LIBS=OFF) or as shared library (default)</span>\n\
    <span class=\"pl-k\">if</span> [ <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span><span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pkg-config --variable=static\
    \ openPMD<span class=\"pl-pds\">)</span></span><span class=\"pl-pds\">\"</span></span>\
    \ <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>true<span class=\"pl-pds\">\"</span></span> ]\n<span class=\"pl-k\">then</span>\n\
    \    pkg-config --libs --static openPMD\n    <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lopenPMD\
    \ -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so\
    \ /usr/lib/libmpi.so /usr/lib/x86_64-linux-gnu/hdf5/openmpi/libhdf5.so /usr/lib/x86_64-linux-gnu/libsz.so\
    \ /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libdl.so /usr/lib/x86_64-linux-gnu/libm.so\
    \ -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so\
    \ /usr/lib/libmpi.so</span>\n<span class=\"pl-k\">else</span>\n    pkg-config\
    \ --libs openPMD\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> -L${HOME}/somepath/lib\
    \ -lopenPMD</span>\n<span class=\"pl-k\">fi</span>\n\npkg-config --cflags openPMD\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> -I${HOME}/somepath/include</span></pre></div>\n\
    <h2>\n<a id=\"user-content-author-contributions\" class=\"anchor\" href=\"#author-contributions\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Author Contributions</h2>\n<p>openPMD-api is developed by many people.\n\
    It was initially started by the <a href=\"https://hzdr.de/crp\" rel=\"nofollow\"\
    >Computational Radiation Physics Group</a> at <a href=\"https://www.hzdr.de/\"\
    \ rel=\"nofollow\">HZDR</a> as successor to <a href=\"https://github.com/ComputationalRadiationPhysics/libSplash/\"\
    >libSplash</a>, generalizing the <a href=\"https://arxiv.org/abs/1706.00522\"\
    \ rel=\"nofollow\">successful HDF5 &amp; ADIOS1 implementations</a> in <a href=\"\
    https://github.com/ComputationalRadiationPhysics/picongpu\">PIConGPU</a>.\nThe\
    \ following people and institutions <a href=\"https://github.com/openPMD/openPMD-api/graphs/contributors\"\
    >contributed</a> to openPMD-api:</p>\n<ul>\n<li>\n<a href=\"https://github.com/ax3l\"\
    >Axel Huebl (HZDR, now LBNL)</a>:\nproject lead, releases, documentation, automated\
    \ CI/CD, Python bindings, Dask, installation &amp; packaging, prior reference\
    \ implementations</li>\n<li>\n<a href=\"https://github.com/franzpoeschel\">Franz\
    \ Poeschel (CASUS)</a>:\nJSON &amp; ADIOS2 backend, data staging/streaming, reworked\
    \ class design</li>\n<li>\n<a href=\"https://github.com/C0nsultant\">Fabian Koller\
    \ (HZDR)</a>:\ninitial library design and implementation with HDF5 &amp; ADIOS1\
    \ backend</li>\n<li>\n<a href=\"https://github.com/guj\">Junmin Gu (LBNL)</a>:\n\
    non-collective parallel I/O fixes, ADIOS improvements, benchmarks</li>\n</ul>\n\
    <p>Further thanks go to improvements and contributions from:</p>\n<ul>\n<li>\n\
    <a href=\"https://github.com/CFGrote\">Carsten Fortmann-Grote (EU XFEL GmbH, now\
    \ MPI-EvolBio)</a>:\ndraft of our Python unit tests</li>\n<li>\n<a href=\"https://github.com/StanczakDominik\"\
    >Dominik Sta\u0144czak (Warsaw University of Technology)</a>:\ndocumentation improvements</li>\n\
    <li>\n<a href=\"https://github.com/mingwandroid\">Ray Donnelly (Anaconda, Inc.)</a>:\n\
    support on conda packaging and libc++ quirks</li>\n<li>\n<a href=\"https://github.com/amundson\"\
    >James Amundson (FNAL)</a>:\ncompile fix for newer compilers</li>\n<li>\n<a href=\"\
    https://github.com/psychocoderHPC\">Ren\xE9 Widera (HZDR)</a>:\ndesign improvements\
    \ for initial API design</li>\n<li>\n<a href=\"https://github.com/erikzenker\"\
    >Erik Zenker (HZDR)</a>:\ndesign improvements for initial API design</li>\n<li>\n\
    <a href=\"https://github.com/sbastrakov\">Sergei Bastrakov (HZDR)</a>:\ndocumentation\
    \ improvements (windows)</li>\n<li>\n<a href=\"https://github.com/RemiLehe\">R\xE9\
    mi Lehe (LBNL)</a>:\npackage integration testing on macOS and Linux</li>\n<li>\n\
    <a href=\"https://github.com/LDAmorim\">L\xEDgia Diana Amorim (LBNL)</a>:\npackage\
    \ integration testing on macOS</li>\n<li>\n<a href=\"https://github.com/KseniaBastrakova\"\
    >Kseniia Bastrakova (HZDR)</a>:\ncompatibility testing</li>\n<li>\n<a href=\"\
    https://github.com/PrometheusPi\">Richard Pausch (HZDR)</a>:\ncompatibility testing,\
    \ documentation improvements</li>\n<li>\n<a href=\"https://github.com/pordyna\"\
    >Pawe\u0142 Ordyna (HZDR)</a>:\nreport on NVCC warnings</li>\n<li>\n<a href=\"\
    https://github.com/dmitry-ganyushin\">Dmitry Ganyushin (ORNL)</a>:\nDask dataframe\
    \ support</li>\n<li>\n<a href=\"https://github.com/jakirkham\">John Kirkham (NVIDIA)</a>:\n\
    Dask guidance &amp; reviews</li>\n</ul>\n<h3>\n<a id=\"user-content-grants\" class=\"\
    anchor\" href=\"#grants\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Grants</h3>\n<p>The openPMD-api authors acknowledge\
    \ support via the following programs.\nThis project has received funding from\
    \ the European Unions Horizon 2020 research and innovation programme under grant\
    \ agreement No 654220.\nSupported by the Consortium for Advanced Modeling of Particles\
    \ Accelerators (CAMPA), funded by the U.S. DOE Office of Science under Contract\
    \ No. DE-AC02-05CH11231.\nSupported by the Exascale Computing Project (17-SC-20-SC),\
    \ a collaborative effort of two U.S. Department of Energy organizations (Office\
    \ of Science and the National Nuclear Security Administration).\nThis work was\
    \ partially funded by the Center of Advanced Systems Understanding (CASUS), which\
    \ is financed by Germany's Federal Ministry of Education and Research (BMBF) and\
    \ by the Saxon Ministry for Science, Culture and Tourism (SMWK) with tax funds\
    \ on the basis of the budget approved by the Saxon State Parliament.</p>\n<h3>\n\
    <a id=\"user-content-transitive-contributions\" class=\"anchor\" href=\"#transitive-contributions\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Transitive Contributions</h3>\n<p>openPMD-api stands on the shoulders\
    \ of giants and we are grateful for the following projects included as direct\
    \ dependencies:</p>\n<ul>\n<li>\n<a href=\"https://github.com/ornladios/ADIOS\"\
    >ADIOS1</a> and <a href=\"https://github.com/ornladios/ADIOS2\">ADIOS2</a> by\
    \ <a href=\"https://csmd.ornl.gov/adios\" rel=\"nofollow\">S. Klasky (ORNL), team,\
    \ collaborators</a> and <a href=\"https://github.com/ornladios/ADIOS2/graphs/contributors\"\
    >contributors</a>\n</li>\n<li>\n<a href=\"https://github.com/catchorg/Catch2\"\
    >Catch2</a> by <a href=\"https://github.com/philsquared\">Phil Nash</a>, <a href=\"\
    https://github.com/horenmar\">Martin Ho\u0159e\u0148ovsk\xFD</a> and <a href=\"\
    https://github.com/catchorg/Catch2/graphs/contributors\">contributors</a>\n</li>\n\
    <li>HDF5 by <a href=\"https://www.hdfgroup.org\" rel=\"nofollow\">the HDF group</a>\
    \ and community</li>\n<li>\n<a href=\"https://github.com/nlohmann/json\">json</a>\
    \ by <a href=\"https://github.com/nlohmann\">Niels Lohmann</a> and <a href=\"\
    https://github.com/nlohmann/json/graphs/contributors\">contributors</a>\n</li>\n\
    <li>\n<a href=\"https://github.com/pybind/pybind11\">pybind11</a> by <a href=\"\
    https://github.com/wjakob\">Wenzel Jakob (EPFL)</a> and <a href=\"https://github.com/pybind/pybind11/graphs/contributors\"\
    >contributors</a>\n</li>\n<li>all contributors to the evolution of modern C++\
    \ and early library preview developers, e.g. <a href=\"https://github.com/mpark\"\
    >Michael Park (Facebook)</a>\n</li>\n<li>the <a href=\"https://cmake.org\" rel=\"\
    nofollow\">CMake build system</a> and <a href=\"https://github.com/Kitware/CMake/blob/master/Copyright.txt\"\
    >contributors</a>\n</li>\n<li>packaging support by the <a href=\"https://conda-forge.org\"\
    \ rel=\"nofollow\">conda-forge</a>, <a href=\"https://pypi.org\" rel=\"nofollow\"\
    >PyPI</a> and <a href=\"https://spack.io\" rel=\"nofollow\">Spack</a> communities,\
    \ among others</li>\n<li>the <a href=\"https://github.com/openPMD/openPMD-standard\"\
    >openPMD-standard</a> by <a href=\"https://github.com/ax3l\">Axel Huebl (HZDR,\
    \ now LBNL)</a> and <a href=\"https://github.com/openPMD/openPMD-standard/blob/latest/AUTHORS.md\"\
    >contributors</a>\n</li>\n</ul>\n"
  stargazers_count: 68
  subscribers_count: 9
  topics:
  - openpmd
  - openscience
  - hdf5
  - adios
  - mpi
  - hpc
  - research
  - file-handling
  - python3
  - meta-data
  - opendata
  - cpp14
  updated_at: 1624648282.0
openhpc/ohpc:
  data_format: 2
  description: OpenHPC Integration, Packaging, and Test Repo
  filenames:
  - containers/Singularity.1.3.3.el7
  full_name: openhpc/ohpc
  latest_release: v2.3.GA
  readme: '<h3>

    <a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon
    octicon-link"></span></a><a href="https://github.com/openhpc/ohpc/blob/master/docs/recipes/install/common/figures/ohpc_logo.png"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/openhpc/ohpc/raw/master/docs/recipes/install/common/figures/ohpc_logo.png"
    width="170" valign="middle" hspace="5" alt="OpenHPC" style="max-width:100%;"></a>

    </h3>

    <h3>

    <a id="user-content-community-building-blocks-for-hpc-systems" class="anchor"
    href="#community-building-blocks-for-hpc-systems" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Community building blocks for HPC systems</h3>

    <h4>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h4>

    <p>This stack provides a variety of common, pre-built ingredients required to

    deploy and manage an HPC Linux cluster including provisioning tools, resource

    management, I/O clients, runtimes, development tools, containers, and a variety
    of

    scientific libraries.</p>

    <p>There are currently two release series:

    <a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> and

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>, which target different
    major

    Linux OS distributions. The 1.3.x series targets CentOS7 and SLES12 while the

    2.x series targets CentOS8 and Leap15.</p>

    <h4>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h4>

    <p>OpenHPC provides pre-built binaries via repositories for use with standard

    Linux package manager tools (e.g. <code>yum</code> or <code>zypper</code>). To
    get started,

    you can enable an OpenHPC repository locally through installation of an

    <code>ohpc-release</code> RPM which includes gpg keys for package signing and
    defines

    the URL locations for [base] and [update] package repositories. Installation

    guides tailored for each supported provisioning system and resource manager

    with detailed example instructions for installaing a cluster are also available.

    Copies of the <code>ohpc-release</code> package and installation guides along
    with

    more information is available on the relevant release series pages

    (<a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> or

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>).</p>

    <hr>

    <h4>

    <a id="user-content-questions-comments-or-bug-reports" class="anchor" href="#questions-comments-or-bug-reports"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Questions,
    Comments, or Bug Reports?</h4>

    <p>Subscribe to the users email list at <a href="https://groups.io/g/openhpc-users"
    rel="nofollow">https://groups.io/g/openhpc-users</a> or see

    the <a href="http://openhpc.community" rel="nofollow">http://openhpc.community</a>
    page for more pointers.</p>

    <h4>

    <a id="user-content-additional-software-requests" class="anchor" href="#additional-software-requests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Additional
    Software Requests?</h4>

    <p>Please see the component submission page at

    <a href="https://github.com/openhpc/submissions">https://github.com/openhpc/submissions</a>
    for more information regarding new

    software inclusion requests.</p>

    <h4>

    <a id="user-content-register-your-system" class="anchor" href="#register-your-system"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Register
    your system</h4>

    <p>If you are using elements of OpenHPC, please consider registering your

    system(s) using the <a href="https://drive.google.com/open?id=1KvFM5DONJigVhOlmDpafNTDDRNTYVdolaYYzfrHkOWI"
    rel="nofollow">System Registration

    Form</a>.</p>

    '
  stargazers_count: 571
  subscribers_count: 94
  topics:
  - hpc
  - scientific-computing
  - package-repository
  - clusters-management
  - devtools
  - mpi
  - linuxfoundation
  updated_at: 1624990757.0
pbranson/pangeo-hpc-singularity:
  data_format: 2
  description: Scripts to run dask and jupyter lab on Singularity using the pangeo-notebook
    image
  filenames:
  - Singularity.pangeo-notebook
  full_name: pbranson/pangeo-hpc-singularity
  latest_release: null
  readme: '<p>This repository provides some boiler plate scripts for running ''pangeo''
    python ecosystem using singularity containers.</p>

    <p>Steps are:</p>

    <ol>

    <li>

    <p>Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a></p>

    <pre><code>docker pull pangeo/pangeo-notebook

    </code></pre>

    <p>The pangeo-notebook has a pretty diverse set of libraries for most cloud,

    dask, zarr, netCDF, analysis type tasks.</p>

    <ul>

    <li>

    <p>(Optional) Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a>

    If you need to customise, see minimal example in Dockerfile and requirements.txt
    and description here:</p>

    <ul>

    <li>

    <p>(Deprecated) <a href="https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants">https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants</a></p>

    </li>

    <li>

    <p>(<strong>Use this since 27-07-2020</strong>) <a href="https://github.com/pangeo-data/pangeo-docker-images">https://github.com/pangeo-data/pangeo-docker-images</a></p>

    <p>Then you would build a custom image along the lines of:</p>

    <pre><code>make pangeo-notebook

    </code></pre>

    </li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Convert docker image to singularity with a command such as:</p>

    <pre><code>singularity -d build pangeo-latest.sif docker-daemon://pangeo/pangeo-notebook:master

    </code></pre>

    </li>

    <li>

    <p>Copy the created <code>pangeo-latest.sif</code> singularity image to somewhere
    accessible on the HPC filesystem and edit the <code>container=</code> and <code>scheduler_file=</code>
    variables in the <code>start_jupyter.slurm</code> and <code>start_worker.slurm</code>
    scripts to point to the singularity image and the shared filesystem location to
    write the scheduler details, respectively.</p>

    </li>

    <li>

    <p>Start the jupyter lab and dask-scheduler, the first parameter is the working
    path you want to use for jupyter lab:</p>

    <pre><code>sbatch start_jupyter.slurm $MYGROUP

    </code></pre>

    <p>This starts a scheduler and jupyterlab with 2 cores each and 8GB/core memory.
    These can be edited in the #SBATCH headers, also note you can set the default
    directory for jupyterlab with the notebook_dir which is the parameter passed to
    start_jupyter.slurm.</p>

    </li>

    <li>

    <p>Start dask-workers (where n is the number of workers you want - these are configures
    for &lt; 2 hour wall time limit so that they use the <code>h2</code> queue):</p>

    <pre><code>sbatch -n 10 start_worker.slurm

    </code></pre>

    <p>also note that this input argument to dask-worker <code>--local-directory $LOCALDIR</code>
    tells the worker the path to local disk storage on the node which can be used
    for spilling data, but not all HPC nodes/centres have attached local storage.
    Currently this is disabled.</p>

    </li>

    <li>

    <p>See instruction printed to the slurm-######.out log file for connecting to
    the jupyter session running on the compute node, something like:</p>

    <pre><code>ssh -N -l pbranson -L 8888:compute-node123:8888 hpc-login.host.com

    </code></pre>

    <p>and take note of the randomly generated token printed to the slurm-######.out
    log file. You will need that to login to Jupyterlab.</p>

    </li>

    <li>

    <p>To connect to the dask-scheduler from a notebook use the following snippet:</p>

    <pre><code>import os

    from distributed import Client

    client=Client(scheduler_file=os.environ[''MYSCRATCH''] + ''/scheduler.json'')

    client

    </code></pre>

    </li>

    <li>

    <p>View the scheduler bokeh dashboard at <a href="http://localhost:8888/proxy/8787/status"
    rel="nofollow">http://localhost:8888/proxy/8787/status</a>. This can also be entered
    into the Jupyterlab dask widget as <code>/proxy/8787/status</code></p>

    </li>

    <li>

    <p>As a little cheat in jupyter lab I open up a terminal and then do</p>

    <pre><code>ssh localhost

    </code></pre>

    <p>to connect to the host running the jupyter container - this gives you access
    to the slurm job scheduler from that terminal and you can start workers  in there
    with:</p>

    <pre><code>sbatch start_worker.slurm

    </code></pre>

    <p>Also note that the dask worker specifications used in the <code>start_worker.slurm</code>
    script are based of the slurm environment variables, so you can alter the worker
    specification using the <code>#SBATCH</code> directives:</p>

    <pre><code>#SBATCH --ntasks=20

    #SBATCH --cpus-per-task=2

    #SBATCH --mem-per-cpu=10G

    #SBATCH --time=0:30:00

    </code></pre>

    <p>or at the command line when you submit the script:</p>

    <pre><code>sbatch -n 4 -c 4 --mem-per-cpu=16G start_worker.slurm

    </code></pre>

    <p>which would start 4 workers with 4 cores per worker and 16*4 = 64GB memory
    per dask-worker.</p>

    </li>

    </ol>

    '
  stargazers_count: 7
  subscribers_count: 2
  topics: []
  updated_at: 1624890978.0
pchengi/cmorfixer_env:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: pchengi/cmorfixer_env
  latest_release: null
  readme: '<h1>

    <a id="user-content-environment-for-cmor-fixer" class="anchor" href="#environment-for-cmor-fixer"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment
    for cmor-fixer</h1>

    <ul>

    <li>

    <p>A tool to create an environment to allow easy use of the <a href="https://github.com/EC-Earth/cmor-fixer">cmor-fixer
    tool</a></p>

    </li>

    <li>

    <p>cmorfixer_env is a singularity container which comes with preinstalled miniconda3</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h1>

    <p>You need the singularity program installed. Follow the instructions here, to
    install singularity on your machine.</p>

    <p><a href="https://singularity.lbl.gov/install-linux" rel="nofollow">https://singularity.lbl.gov/install-linux</a></p>

    <h1>

    <a id="user-content-to-download-a-prebuilt-singularity-image" class="anchor" href="#to-download-a-prebuilt-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>To
    download a prebuilt singularity image:</h1>

    <ul>

    <li>If you''d like to use a prebuilt image, you could download from the link below;
    if you''d rather build the container yourself, follow the build instructing in
    the To build section.</li>

    <li><a href="https://esg-dn2.nsc.liu.se/virtualtestbed/cmorfixerenv.simg" rel="nofollow">Link
    to prebuilt image</a></li>

    </ul>

    <h1>

    <a id="user-content-to-build" class="anchor" href="#to-build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>To build</h1>

    <pre><code>sudo singularity build cmorfixerenv.simg Singularity

    </code></pre>

    <h1>

    <a id="user-content-to-initialize-container-and-optionally-mount-external-filesystems"
    class="anchor" href="#to-initialize-container-and-optionally-mount-external-filesystems"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>To
    initialize container (and optionally mount external filesystems)</h1>

    <ul>

    <li>If you don''t have to mount any non-root filesystems, you could start the
    container like this:</li>

    </ul>

    <pre><code> singularity shell cmorfixerenv.simg

    </code></pre>

    <ul>

    <li>If you don''t see on the container the filesystem which is accessible on the
    host machine, you could try this, and once inside the container, you''ll be able
    to see the filesystem mounted on /mnt.</li>

    </ul>

    <pre><code> singularity shell --bind &lt;path to filesystem you want mounted on
    the container&gt;:/mnt cmorfixerenv.simg

    </code></pre>

    <ul>

    <li>Inside the container, do the following</li>

    </ul>

    <pre><code>source /etc/bashrc

    activateminiconda3

    conda activate cmorfixer

    </code></pre>

    <ul>

    <li>Execute cmorfixer (present in /opt/cmor_fixer/cmor-fixer/cmor-fixer.py, in
    the container)</li>

    </ul>

    <pre><code>cd /root

    script -c ''/opt/cmor_fixer/cmor-fixer/cmor-fixer.py --verbose --forceid --olist
    --npp 1 --dry /mnt/CMIP6/ScenarioMIP/EC-Earth-Consortium/EC-Earth3/ssp126/'' scriptout_cmorfix_dryrun

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611252471.0
perambluate/singularity-definition-files-for-HPC:
  data_format: 2
  description: To build hpc benchmark and mpi with cuda support sif
  filenames:
  - hpcc_intel.def
  - hpl_intel_cuda.def
  - hpc_mpi_cuda.def
  - bert.def
  full_name: perambluate/singularity-definition-files-for-HPC
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc_mpi_cuda_singu_def_file" class="anchor" href="#hpc_mpi_cuda_singu_def_file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>hpc_mpi_cuda_singu_def_file</h1>

    <p>A collect of definition files to build images for singularity containers, which
    includes hpc benchmarks and mpis with cuda support.</p>

    <p><a href="https://singularity-hub.org/collections/4181" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1588998487.0
perminaa/SingularityHPC:
  data_format: 2
  description: 'This repository will hold: the build script to install singularity
    and other dependencies for it, and a definition file for the singularity container
    for HPC.'
  filenames:
  - SingularitySC
  - Singularity
  full_name: perminaa/SingularityHPC
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularityhpc" class="anchor" href="#singularityhpc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SingularityHPC</h1>

    <p>This repository will hold: the build script to install singularity and other
    dependencies for it, and a definition file for the singularity container for HPC.</p>

    <p>To install, run <code>git clone https://github.com/perminaa/SingularityHPC.git
    &amp;&amp; cd SingularityHPC &amp;&amp; bash buildscript.sh</code>. This will
    install and configure singularity

    and build a container called <code>Container.sif</code></p>

    <p>To run the container, you can use <code>singularity shell Container.sif</code>
    to run in the singularity shell or <code>singularity exec Container.sif &lt;command&gt;</code>.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1623989129.0
peterk87/nf-villumina:
  data_format: 2
  description: Generic viral Illumina sequence analysis pipeline
  filenames:
  - Singularity
  - singularity/Singularity.2.0.0
  full_name: peterk87/nf-villumina
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-peterk87nf-villumina\" class=\"anchor\" href=\"\
    #peterk87nf-villumina\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>peterk87/nf-villumina</h1>\n<p><strong>Generic\
    \ viral Illumina sequence analysis pipeline</strong></p>\n<p><a href=\"https://travis-ci.org/peterk87/nf-villumina\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c95912a5b97ffebe518b92d2612faba172f193f3aec7d85e0b8ee6a88db89b94/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f6e662d76696c6c756d696e612e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/nf-villumina.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.nextflow.io/\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\"\
    \ alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\"\
    \ alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/peterk87/nf-villumina\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/14da47af1d6b7d4d6e7909986afbce060794df560644ce6b565495a43df45b94/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f70657465726b38372f6e662d76696c6c756d696e612e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/peterk87/nf-villumina.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/2925\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h3>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h3>\n<p>The pipeline\
    \ is built using <a href=\"https://www.nextflow.io\" rel=\"nofollow\">Nextflow</a>,\
    \ a workflow tool to run tasks across multiple compute infrastructures in a very\
    \ portable manner. It comes with a <a href=\"https://sylabs.io/\" rel=\"nofollow\"\
    >Singularity</a> container making installation trivial and results highly reproducible.</p>\n\
    <p><code>nf-villumina</code> will</p>\n<ul>\n<li>remove low quality reads (<a\
    \ href=\"https://github.com/OpenGene/fastp\">fastp</a>)</li>\n<li>filter for reads\
    \ from a taxonomic group of interest (by default superkingdom <a href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;id=10239&amp;lvl=3&amp;lin=f&amp;keep=1&amp;srchmode=1&amp;unlock\"\
    \ rel=\"nofollow\">Viruses</a> (taxid=10239)) using <a href=\"https://ccb.jhu.edu/software/kraken2/\"\
    \ rel=\"nofollow\">Kraken2</a> and <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\"\
    \ rel=\"nofollow\">Centrifuge</a> classification results</li>\n<li>perform <em>de\
    \ novo</em> assembly with [Unicycler] and [Shovill] on the taxonomic classification\
    \ filtered reads</li>\n<li>search all contig sequences using NCBI nucleotide <a\
    \ href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\">BLAST</a>\
    \ against a database of your choice (we recommend the version 5 NCBI nt DB)</li>\n\
    </ul>\n<p><strong>NOTE:</strong> You will need to create/download databases for\
    \ <a href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\">Kraken2</a>,\
    \ <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\"\
    >Centrifuge</a> and <a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"\
    nofollow\">BLAST</a> in order to get the most out of this workflow!</p>\n<h3>\n\
    <a id=\"user-content-pre-requisites\" class=\"anchor\" href=\"#pre-requisites\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pre-requisites</h3>\n<h4>\n<a id=\"user-content-taxonomic-classification-for-kraken2-and-centrifuge\"\
    \ class=\"anchor\" href=\"#taxonomic-classification-for-kraken2-and-centrifuge\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Taxonomic Classification for <a href=\"https://ccb.jhu.edu/software/kraken2/\"\
    \ rel=\"nofollow\">Kraken2</a> and <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\"\
    \ rel=\"nofollow\">Centrifuge</a>\n</h4>\n<p>For taxonomic classification with\
    \ <a href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\">Kraken2</a>\
    \ and <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"\
    nofollow\">Centrifuge</a>, you will need to download (or build) databases for\
    \ these programs so that you may use them within the <code>nf-villumina</code>\
    \ workflow.</p>\n<p>You can point to the Kraken2 and Centrifuge database with\
    \ <code>export KRAKEN2_DB=/path/to/kraken2/database</code> and <code>export CENTRIFUGE_DB=/path/to/centrifuge/database/prefix</code>\
    \ in your <code>~/.bashrc</code> so you don't need to specify it each time you\
    \ run the workflow with <code>--kraken2_db /path/to/kraken2/standard2 --centrifuge_db\
    \ /path/to/centrifuge/nt-2018-03-03/nt</code></p>\n<h4>\n<a id=\"user-content-kraken2-dbs\"\
    \ class=\"anchor\" href=\"#kraken2-dbs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Kraken2 DBs</h4>\n<ul>\n<li>MiniKraken2_v2_8GB:\
    \ (5.5GB) 8GB Kraken 2 Database built from the Refseq bacteria, archaea, and viral\
    \ libraries and the GRCh38 human genome</li>\n<li>\n<a href=\"https://monash.figshare.com/articles/GTDB_r89_54k/8956970\"\
    \ rel=\"nofollow\">GTDB_r89_54k Kraken2 DBs</a>: There are multiple Kraken2 DBs\
    \ of various sizes available for download. For more info, see <a href=\"https://github.com/rrwick/Metagenomics-Index-Correction\"\
    >https://github.com/rrwick/Metagenomics-Index-Correction</a> and the manuscript:\
    \ M\xE9ric, Wick et al. (2019) Correcting index databases improves metagenomic\
    \ studies. doi: <a href=\"https://doi.org/10.1101/712166\" rel=\"nofollow\">https://doi.org/10.1101/712166</a>\n\
    </li>\n</ul>\n<h4>\n<a id=\"user-content-centrifuge-dbs\" class=\"anchor\" href=\"\
    #centrifuge-dbs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Centrifuge DBs</h4>\n<ul>\n<li>NCBI nucleotide non-redundant\
    \ sequences (2018-03-03) (64 GB)</li>\n<li>\n<a href=\"https://monash.figshare.com/ndownloader/files/16378439\"\
    \ rel=\"nofollow\">GTDB_r89_54k Centrifuge DB (108 GB tar file)</a>: For more\
    \ info, see <a href=\"https://github.com/rrwick/Metagenomics-Index-Correction\"\
    >https://github.com/rrwick/Metagenomics-Index-Correction</a> and the manuscript:\
    \ M\xE9ric, Wick et al. (2019) Correcting index databases improves metagenomic\
    \ studies. doi: <a href=\"https://doi.org/10.1101/712166\" rel=\"nofollow\">https://doi.org/10.1101/712166</a>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-blast-dbs\" class=\"anchor\" href=\"\
    #blast-dbs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\"\
    >BLAST</a> DBs</h3>\n<p>For nf-villumina, you must have a version 5 BLAST DB with\
    \ embedded taxonomic information installed, e.g. version 5 <code>nt</code> DB\
    \ (see <a href=\"https://ftp.ncbi.nlm.nih.gov/blast/db/v5/\" rel=\"nofollow\"\
    >https://ftp.ncbi.nlm.nih.gov/blast/db/v5/</a>)</p>\n<p>You can download pre-built\
    \ <a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\">BLAST</a>\
    \ DBs like <code>nt</code> and <code>nr</code> from <a href=\"https://ftp.ncbi.nlm.nih.gov/blast/db/\"\
    \ rel=\"nofollow\">the NCBI FTP site</a> using the <code>update_blastdb.pl</code>\
    \ script included with your install of BLAST+ to download and/or update your local\
    \ BLAST databases.</p>\n<p>Show all available databases:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ update_blastdb.pl --showall</pre></div>\n<p>Download\
    \ the BLASTDB version 5 \"nt\" database to your current directory decompressing\
    \ files and deleting original compressed archives:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>update_blastdb.pl --blastdb_version 5 nt --decompress</pre></div>\n\
    <p><strong>NOTE:</strong> For ease of use, all databases should be downloaded\
    \ to the same directory (e.g. <code>/opt/DB/blast</code> set in <code>$BLASTDB</code>\
    \ environment variable in your <code>~/.bashrc</code>)</p>\n<p>Check that your\
    \ database has been downloaded properly and has taxids associated with the sequences\
    \ contained within it:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$\
    \ blastdbcheck -db nt -must_have_taxids -verbosity 3</pre></div>\n<h3>\n<a id=\"\
    user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Documentation</h3>\n\
    <p>The peterk87/nf-villumina pipeline comes with documentation about the pipeline,\
    \ found in the <code>docs/</code> directory:</p>\n<ol>\n<li><a href=\"docs/installation.md\"\
    >Installation</a></li>\n<li>Pipeline configuration\n<ul>\n<li><a href=\"docs/configuration/local.md\"\
    >Local installation</a></li>\n<li><a href=\"docs/configuration/adding_your_own.md\"\
    >Adding your own system</a></li>\n</ul>\n</li>\n<li><a href=\"docs/usage.md\"\
    >Running the pipeline</a></li>\n<li><a href=\"docs/output.md\">Output and how\
    \ to interpret the results</a></li>\n<li><a href=\"docs/troubleshooting.md\">Troubleshooting</a></li>\n\
    </ol>\n\n<h3>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Credits</h3>\n<p>peterk87/nf-villumina was originally written by Peter\
    \ Kruczkiewicz.</p>\n<p>Bootstrapped with <a href=\"https://github.com/nf-core/tools\"\
    >nf-core/tools</a> <code>nf-core create</code>.</p>\n<p>Thank you to the <a href=\"\
    https://github.com/nf-core/tools\">nf-core/tools</a> team for a great tool for\
    \ bootstrapping creation of a production ready Nextflow workflows.</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1603317815.0
peterk87/nf-virontus:
  data_format: 2
  description: Oxford Nanopore reference mapping, taxonomic classification, de novo
    assembly workflow primarily for viral sequence data
  filenames:
  - singularity/Singularity.1.1.0
  - singularity/Singularity.1.0.0
  full_name: peterk87/nf-virontus
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-peterk87nf-virontus\" class=\"anchor\" href=\"\
    #peterk87nf-virontus\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>peterk87/nf-virontus</h1>\n<p><strong>Virontus\
    \ viral Oxford Nanopore sequence analysis pipeline</strong></p>\n<p><a href=\"\
    https://travis-ci.org/peterk87/nf-virontus\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ca2382eaedc143481936b2847287dfadcc9737054d5f078007bb7dcc0f5474bb/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f6e662d7669726f6e7475732e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/nf-virontus.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.nextflow.io/\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\"\
    \ alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\"\
    \ alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/peterk87/nf-virontus\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/701f9cda36830e80f60b8cb5e6108a4b9fdfcb6f09698b97e11a87e15dd71a93/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f70657465726b38372f6e662d7669726f6e7475732e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/peterk87/nf-virontus.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/4297\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>Table of Contents</strong></p>\n\
    <ul>\n<li>\n<a href=\"#peterk87nf-virontus\">peterk87/nf-virontus</a>\n<ul>\n\
    <li><a href=\"#introduction\">Introduction</a></li>\n<li>\n<a href=\"#installation\"\
    >Installation</a>\n<ul>\n<li>\n<a href=\"#1-install-nextflow\">1) Install </a><a\
    \ href=\"https://www.nextflow.io\" rel=\"nofollow\">Nextflow</a>\n</li>\n<li>\n\
    <a href=\"#2-install-singularity\">2) Install </a><a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a>\n</li>\n<li><a href=\"#3-install-virontus\"\
    >3) Install Virontus</a></li>\n</ul>\n</li>\n<li>\n<a href=\"#usage\">Usage</a>\n\
    <ul>\n<li><a href=\"#preparing-your-data\">Preparing your data</a></li>\n<li><a\
    \ href=\"#recommended-steps\">Recommended Steps</a></li>\n<li><a href=\"#example\"\
    >Example</a></li>\n</ul>\n</li>\n<li><a href=\"#credits\">Credits</a></li>\n</ul>\n\
    </li>\n</ul>\n<blockquote>\n<p>TOC created by <a href=\"https://github.com/ekalinin/github-markdown-toc\"\
    >gh-md-toc</a></p>\n</blockquote>\n<h3>\n<a id=\"user-content-introduction\" class=\"\
    anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Introduction</h3>\n<p>The Virontus\
    \ pipeline is for the analysis of viral shotgun and amplicon  Oxford Nanopore\
    \ sequence data. Given basecalled (and demultiplexed) Nanopore reads, Virontus\
    \ produces one or more consensus sequences from read mapping with <a href=\"https://github.com/lh3/minimap2\"\
    >Minimap2</a> and variant calling with <a href=\"https://github.com/nanoporetech/medaka\"\
    >Medaka</a> and <a href=\"https://www.nature.com/articles/s41467-019-12493-y\"\
    \ rel=\"nofollow\">Longshot</a> results with respect to one or more reference\
    \ sequences. For amplicon sequencing, the user should provide a BED file containing\
    \ primer coordinates with respect to a reference sequence so that the primer sequences\
    \ can be trimmed using <a href=\"https://github.com/andersen-lab/ivar\">iVar</a>.</p>\n\
    <p>Optionally, Virontus will perform taxonomic classification with <a href=\"\
    https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\">Kraken2</a> and <a href=\"\
    https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\">Centrifuge</a>\
    \ if index paths are provided. Reads can be filtered by taxonomic classification.\
    \ By default viral and unclassified reads are filtered.</p>\n<p>De novo assembly\
    \ with <a href=\"https://github.com/rrwick/Unicycler\">Unicycler</a> can be optionally\
    \ performed if desired (specify <code>--do_unicycler_assembly</code> when running\
    \ Virontus). If taxonomic classification is performed then taxonomically filtered\
    \ reads will be assembled, otherwise all reads will be used for assembly.</p>\n\
    <p>The Virontus pipeline is built using <a href=\"https://www.nextflow.io\" rel=\"\
    nofollow\">Nextflow</a>, a workflow tool to run tasks across multiple compute\
    \ infrastructures in a very portable manner. It comes with <a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> and <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a> containers making installation trivial and\
    \ results highly reproducible.</p>\n<h3>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h3>\n<p>You will\
    \ need to install <a href=\"https://www.nextflow.io\" rel=\"nofollow\">Nextflow</a>\
    \ in order to run the Virontus pipeline.</p>\n<p><a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a> is recommended for portable and reproducible\
    \ execution of the pipeline with the <code>-profile singularity</code> command-line\
    \ argument.</p>\n<h4>\n<a id=\"user-content-1-install-nextflow\" class=\"anchor\"\
    \ href=\"#1-install-nextflow\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>1) Install <a href=\"https://www.nextflow.io\"\
    \ rel=\"nofollow\">Nextflow</a>\n</h4>\n<p>If you have <a href=\"https://conda.io/\"\
    \ rel=\"nofollow\">Conda</a> installed, you can install <a href=\"https://www.nextflow.io\"\
    \ rel=\"nofollow\">Nextflow</a> with the following command:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>conda install -c bioconda -c conda-forge\
    \ nextflow</pre></div>\n<h4>\n<a id=\"user-content-2-install-singularity\" class=\"\
    anchor\" href=\"#2-install-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2) Install <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a>\n</h4>\n<p>Installing <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a> is optional but recommended for portability\
    \ and reproducibility of results.</p>\n<h4>\n<a id=\"user-content-3-install-virontus\"\
    \ class=\"anchor\" href=\"#3-install-virontus\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3) Install Virontus</h4>\n<p>Nextflow\
    \ will automatically download the latest version of Virontus. You can show the\
    \ Virontus help message with usage information with:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>nextflow run peterk87/nf-virontus --help</pre></div>\n\
    <h3>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h3>\n\
    <p>Show usage information with</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>nextflow run peterk87/nf-virontus --help</pre></div>\n<p>You should see\
    \ the following</p>\n<pre><code>N E X T F L O W  ~  version 20.01.0\nLaunching\
    \ `main.nf` [awesome_pauling] - revision: 9aeb19496b\nWARN: DSL 2 IS AN EXPERIMENTAL\
    \ FEATURE UNDER DEVELOPMENT -- SYNTAX MAY CHANGE IN FUTURE RELEASE\n==================================================================\n\
    peterk87/nf-virontus   ~  version 1.1.0\n==================================================================\n\
    \n  Git info: null - null [null]\n\nUsage:\nGiven some barcoded and demultiplexed\
    \ reads, the typical command for running the pipeline is as follows:\n\n  nextflow\
    \ run peterk87/nf-virontus \\\n    --reads \"reads/*.fastq\" \\\n    --outdir\
    \ results \\\n    --ref_fasta refs.fa \\\n    -profile singularity # recommended\
    \ to run with Singularity\n\nThe above assumes that you have a Centrifuge DB and\
    \ Kraken2 DB located at\n/opt/DB/centrifuge/nt-2018-03-03/nt and /opt/DB/kraken2/standard2,\n\
    respectively, OR that you have set $CENTRIFUGE_DB and $KRAKEN2_DB env\nvariables.\
    \ It also assumes that you have Singularity installed on your\nlocal machine and\
    \ will automatically pull and use the Singularity image for\nthis workflow from\
    \ Singularity-Hub.org.\n\nNOTE: For best results, please ensure you have Singularity\
    \ installed prior to running this workflow.(https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-installation-steps)\n\
    \nNote:\nThe argument supplied to \"--reads\" must be quoted if using \"*\" and\
    \ other\ncharacters and symbols that could be shell expanded!\n\nMandatory Options:\n\
    \  --reads   Input reads directory and pattern (default: \"reads/*.fastq\")\n\
    \  --ref_fasta      Reference genomes multiFASTA file (one or more references\n\
    \                   in a single file) (default: \"./refs.fasta\")\nAmplicon Sequencing\
    \ Options:\n  --bedfile        BED format file with amplicon sequencing primers\
    \ info (optional).\n                   Produced as output from PrimalScheme.\n\
    Consensus Generation Options:\n  --low_coverage   Low coverage threshold (default=3).\n\
    \                   Replace consensus sequence positions below this depth\n  \
    \                 threshold with a low coverage character\n                  \
    \ (see --low_cov_char)\n  --no_coverage    No coverage threshold (default=0).\n\
    \                   Replace consensus sequence positions with less than or\n \
    \                  equal this depth with a no coverage character\n           \
    \        (see --no_cov_char)\n  --low_cov_char   Low coverage character (default=\"\
    N\")\n  --no_cov_char    No coverage character (default=\"-\")\n\nCluster Options:\n\
    \  --slurm_queue     Name of SLURM queue to run workflow on; use with -profile\
    \ slurm\n\n\nTaxonomic Classification Options:\n  --centrifuge_db   Path to Centrifuge\
    \ DB and prefix. If not specified, will\n                    try to get from $CENTRIFUGE_DB\
    \ env variable or see if\n                    \"/opt/DB/centrifuge/nt-2018-03-03/nt\"\
    \ exists.\n                    (default: null)\n  --kraken2_db      Path to Kraken2\
    \ DB directory. . If not specified, will\n                    try to get from\
    \ $KRAKEN2_DB env variable or see if\n                    \"/opt/DB/kraken2/standard2\"\
    \ exists.\n                    (default: null)\n  --taxids          Taxonomic\
    \ IDs to filter reads by. Multiple taxids should\n                    be delimited\
    \ by commas (`--taxids 1,2,3`). To disable\n                    filtering of reads\
    \ based on taxids, do not provide a\n                    value for the `--taxids`\
    \ argument:\n                    `nextflow run ... --taxids --reads ...`\n   \
    \                 (default: 10239 (Viruses))\n  --exclude_unclassified_reads \
    \ Exclude unclassified reads from taxonomic\n                                classification\
    \ filtered reads (default: false)\n\nDe Novo Assembly Options:\n  --do_unicycler_assembly\
    \       Assemble filtered reads using Unicycler? (default: false)\n\nOther Options:\n\
    \  --outdir          The output directory where the results will be saved\n  \
    \                  (default: results)\n  -w/--work-dir     The temporary directory\
    \ where intermediate data will be\n                    saved (default: ./work)\n\
    \  -profile          Configuration profile to use. [standard, singularity,\n \
    \                   conda, slurm] (default 'standard')\n  --tracedir        Pipeline\
    \ run info output directory (default:\n                    results/pipeline_info)\n\
    \nNote:\nIt is recommended that this workflow be executed with Singularity using\
    \ the\nSingularity profile (`-profile singularity`) for maximum reproducibility\
    \ and\nease of execution on different platforms.\n</code></pre>\n<h4>\n<a id=\"\
    user-content-preparing-your-data\" class=\"anchor\" href=\"#preparing-your-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Preparing your data</h4>\n<p>It is assumed that your data has been\
    \ basecalled using the latest version of ONT Guppy (<code>guppy_basecaller</code>/<code>guppy_basecall_server</code>)\
    \ and barcode demultiplexed using <code>guppy_barcoder</code> with the appropriate\
    \ settings for the kits used.</p>\n<p>After basecalling and demultiplexing, it\
    \ is recommended that all reads belonging to a particular barcode be concatenated\
    \ together and optionally renamed to represent the sample to which the reads belong.\
    \ Virontus will extract the sample name for each input reads FASTQ file from the\
    \ base filename of the FASTQ file (e.g. sample name will be <code>sample</code>\
    \ from filename <code>sample1.fastq</code>).</p>\n<p>Below is an example <code>guppy_barcoder</code>\
    \ command for more lenient barcode demultiplexing:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>guppy_barcoder \\\n  -q 0 \\\n  --min_score 30\
    \ \\\n  --detect_mid_strand_barcodes \\\n  --allow_inferior_barcodes \\\n  --trim_barcodes\
    \ \\\n  -i basecalled-reads/ \\\n  -s demuxed-reads \\\n  --arrangements_files\
    \ barcode_arrs_nb12.cfg</pre></div>\n<ul>\n<li>\n<code>-q 0</code> to output less\
    \ files per barcode</li>\n<li>\n<code>--min_score 30</code> for a lower barcode\
    \ score threshold (default: 60)</li>\n<li>\n<code>--detect_mid_strand_barcodes</code>\
    \ to detect mid strand barcodes</li>\n<li>\n<code>--trim_barcodes</code> to trim\
    \ barcodes from read sequences</li>\n<li>\n<code>--arrangements_files</code> to\
    \ specify the barcodes used</li>\n</ul>\n<p><strong>NOTE:</strong> It's recommended\
    \ to use the default setting if possible to avoid misassigning reads into the\
    \ incorrect barcodes.</p>\n<h5>\n<a id=\"user-content-recommended-steps\" class=\"\
    anchor\" href=\"#recommended-steps\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recommended Steps</h5>\n<ol>\n\
    <li>Basecall reads using Guppy</li>\n<li>Demultiplex reads using <code>guppy_barcoder</code>\n\
    </li>\n<li>Concatenate reads belonging to the same barcode into a single file\
    \ (<code>cat barcode01/*.fastq &gt; concat-reads/barcode01.fastq</code>)</li>\n\
    <li>[Optionally] rename concatenated barcoded reads with appropriate sample name\
    \ (<code>mv concat-reads/barcode01.fastq concat-reads/sample1.fastq</code>)</li>\n\
    </ol>\n<h4>\n<a id=\"user-content-example\" class=\"anchor\" href=\"#example\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Example</h4>\n<p>Example command</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ nextflow run peterk87/nf-virontus \\\n    -resume \\\n    -profile singularity\
    \ \\\n    --reads <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>reads/*.fq<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --ref_fasta MN908947.3.fa \\\n   \
    \ --low_coverage 3 \\\n    --bedfile nCoV-2019.bed</pre></div>\n<p>What you will\
    \ see in the terminal:</p>\n<pre><code>N E X T F L O W  ~  version 20.01.0\nLaunching\
    \ `../main.nf` [ecstatic_davinci] - revision: 9aeb19496b\nWARN: DSL 2 IS AN EXPERIMENTAL\
    \ FEATURE UNDER DEVELOPMENT -- SYNTAX MAY CHANGE IN FUTURE RELEASE\n=======================================================\n\
    peterk87/nf-virontus v1.1.0\n=======================================================\n\
    Pipeline Name         : peterk87/nf-virontus\nPipeline Version      : 1.1.0\n\
    Run Name              : ecstatic_davinci\nReads                 : reads/*.fq\n\
    Ref Sequences FASTA   : MN908947.3.fa\nPrimer Scheme         : nCoV-2019.bed\n\
    Consensus No Coverage : &lt;=0X positions replaced with '-'\nConsensus Low Coverage:\
    \ &lt;3X positions replaced with 'N'\nCentrifuge DB         : null\nKraken2 DB\
    \            : null\nTaxids                : Filtering for taxids belonging to\
    \ 10239\nUnicycler Assembly?   : No\nMax Memory            : 256 GB\nMax CPUs\
    \              : 48\nMax Time              : 10d\nOutput dir            : results\n\
    Working dir           : ./work\nContainer Engine      : singularity\nContainer\
    \             : virontus.simg\nCurrent home          : /home/pkruczkiewicz\nCurrent\
    \ user          : pkruczkiewicz\nCurrent path          : ./\nScript dir      \
    \      : ./nf-virontus\nConfig Profile        : standard\nCommand-Line       \
    \   : nextflow run peterk87/nf-virontus -profile singularity -resume --reads 'reads/*.fq'\
    \ --ref_fasta MN908947.3.fa --low_coverage 3 --bedfile nCoV-2019.bed\nNextflow\
    \ version      : 20.01.0\n=========================================\nexecutor\
    \ &gt;  local (18)\n[0a/142458] process &gt; REC2FASTA  [100%] 1 of 1 \u2714\n\
    [a3/3168c5] process &gt; MAP        [100%] 3 of 3 \u2714\n[0d/8a698f] process\
    \ &gt; IVAR_TRIM  [100%] 3 of 3 \u2714\n[76/f82320] process &gt; MAP_STATS  [100%]\
    \ 3 of 3 \u2714\n[cc/de6b36] process &gt; MEDAKA     [100%] 3 of 3 \u2714\n[74/058b57]\
    \ process &gt; LONGSHOT   [100%] 3 of 3 \u2714\n[b4/5ed366] process &gt; BCF_FILTER\
    \ [100%] 3 of 3 \u2714\n[a3/ae8e3a] process &gt; CONSENSUS  [ 100%] 3 of 3 \u2714\
    \n[e3/f75ddb] process &gt; COVERAGE_PLOT [100%] 3 of 3 \u2714\n\nPipeline execution\
    \ summary\nCompleted at: 30-Apr-2020 14:00:11\nDuration    : 1m 40s\nCPU hours\
    \   : 0.1 (58.9% cached)\nSucceeded   : 18\nCached      : 4\n</code></pre>\n<p>Example\
    \ output file tree structure:</p>\n<pre><code>results/\n\u251C\u2500\u2500 consensus\n\
    \u2502\_\_ \u251C\u2500\u2500 NB02-MN908947.3.consensus.fasta\n\u2502\_\_ \u251C\
    \u2500\u2500 NB04-MN908947.3.consensus.fasta\n\u2502\_\_ \u2514\u2500\u2500 unclassified-MN908947.3.consensus.fasta\n\
    \u251C\u2500\u2500 mapping\n\u2502\_\_ \u251C\u2500\u2500 NB02\n\u2502\_\_ \u2502\
    \_\_ \u251C\u2500\u2500 bamfiles\n\u2502\_\_ \u2502\_\_ \u2502\_\_ \u251C\u2500\
    \u2500 NB02-MN908947.3.bam \n\u2502\_\_ \u2502\_\_ \u2502\_\_ \u2514\u2500\u2500\
    \ NB02-MN908947.3.trim.bam \n\u2502\_\_ \u2502\_\_ \u251C\u2500\u2500 NB02-MN908947.3-depths.tsv\n\
    \u2502\_\_ \u2502\_\_ \u251C\u2500\u2500 NB02-MN908947.3.flagstat\n\u2502\_\_\
    \ \u2502\_\_ \u2514\u2500\u2500 NB02-MN908947.3.idxstats\n\u2502\_\_ \u251C\u2500\
    \u2500 NB04\n\u2502\_\_ \u2502\_\_ \u251C\u2500\u2500 bamfiles\n\u2502\_\_ \u2502\
    \_\_ \u2502\_\_ \u251C\u2500\u2500 NB04-MN908947.3.bam \n\u2502\_\_ \u2502\_\_\
    \ \u2502\_\_ \u2514\u2500\u2500 NB04-MN908947.3.trim.bam \n\u2502\_\_ \u2502\_\
    \_ \u251C\u2500\u2500 NB04-MN908947.3-depths.tsv\n\u2502\_\_ \u2502\_\_ \u251C\
    \u2500\u2500 NB04-MN908947.3.flagstat\n\u2502\_\_ \u2502\_\_ \u2514\u2500\u2500\
    \ NB04-MN908947.3.idxstats\n\u2502\_\_ \u2514\u2500\u2500 unclassified\n\u2502\
    \_\_     \u251C\u2500\u2500 bamfiles\n\u2502\_\_     \u2502\_\_ \u251C\u2500\u2500\
    \ unclassified-MN908947.3.bam \n\u2502\_\_     \u2502\_\_ \u2514\u2500\u2500 unclassified-MN908947.3.trim.bam\
    \ \n\u2502\_\_     \u251C\u2500\u2500 unclassified-MN908947.3-depths.tsv\n\u2502\
    \_\_     \u251C\u2500\u2500 unclassified-MN908947.3.flagstat\n\u2502\_\_     \u2514\
    \u2500\u2500 unclassified-MN908947.3.idxstats\n\u251C\u2500\u2500 pipeline_info\n\
    \u2502\_\_ \u251C\u2500\u2500 execution_dag.dot\n\u2502\_\_ \u251C\u2500\u2500\
    \ execution_report.html\n\u2502\_\_ \u251C\u2500\u2500 execution_timeline.html\n\
    \u2502\_\_ \u2514\u2500\u2500 execution_trace.txt\n\u251C\u2500\u2500 plots\n\u2502\
    \_\_ \u251C\u2500\u2500 coverage_plot-NB02-VS-MN908947.3-log_scale.pdf\n\u2502\
    \_\_ \u251C\u2500\u2500 coverage_plot-NB02-VS-MN908947.3.pdf\n\u2502\_\_ \u251C\
    \u2500\u2500 coverage_plot-NB04-VS-MN908947.3-log_scale.pdf\n\u2502\_\_ \u2514\
    \u2500\u2500 coverage_plot-NB04-VS-MN908947.3.pdf\n\u251C\u2500\u2500 refs\n\u2502\
    \_\_ \u2514\u2500\u2500 MN908947.3.fa\n\u2514\u2500\u2500 vcf\n    \u251C\u2500\
    \u2500 NB02-MN908947.3.longshot.filt.vcf\n    \u251C\u2500\u2500 NB02-MN908947.3.longshot.vcf\n\
    \    \u251C\u2500\u2500 NB02-MN908947.3.medaka.vcf\n    \u251C\u2500\u2500 NB04-MN908947.3.longshot.filt.vcf\n\
    \    \u251C\u2500\u2500 NB04-MN908947.3.longshot.vcf\n    \u251C\u2500\u2500 NB04-MN908947.3.medaka.vcf\n\
    \    \u251C\u2500\u2500 unclassified-MN908947.3.longshot.filt.vcf\n    \u251C\u2500\
    \u2500 unclassified-MN908947.3.longshot.vcf\n    \u2514\u2500\u2500 unclassified-MN908947.3.medaka.vcf\n\
    </code></pre>\n<h3>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Credits</h3>\n<p>peterk87/nf-virontus was originally written by Peter\
    \ Kruczkiewicz.</p>\n<p>Bootstrapped with <a href=\"https://github.com/nf-core/tools\"\
    >nf-core/tools</a> <code>nf-core create</code>.</p>\n<p>Thank you to the <a href=\"\
    https://github.com/nf-core/tools\">nf-core/tools</a> team for a great tool for\
    \ bootstrapping creation of a production ready Nextflow workflows.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1603317652.0
peterk87/viral-ampliseq-assembly:
  data_format: 2
  description: Snakemake workflow for analysis and assembly of viral genomes from
    IonTorrent AmpliSeq data.
  filenames:
  - Singularity
  full_name: peterk87/viral-ampliseq-assembly
  latest_release: v1.0.0
  readme: "<h1>\n<a id=\"user-content-snakemake-workflow-viral-ampliseq-assembly\"\
    \ class=\"anchor\" href=\"#snakemake-workflow-viral-ampliseq-assembly\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Snakemake\
    \ workflow: viral-ampliseq-assembly</h1>\n<p><a href=\"https://snakemake.bitbucket.io\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/de7b3ae9d2ddd7970750ed14a267d738217987e5635a19380de6f3b2ec3216e6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d254532253839254135352e352e342d627269676874677265656e2e737667\"\
    \ alt=\"Snakemake\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%E2%89%A55.5.4-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.com/peterk87/viral-ampliseq-assembly\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9ca62ba99cb6a38032432759aa450c99bf81b9671bab9e21e2492c47bf7cf065/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f766972616c2d616d706c697365712d617373656d626c792e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/viral-ampliseq-assembly.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/3359\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://snakemake.readthedocs.io/en/stable/\"\
    \ rel=\"nofollow\">Snakemake</a> workflow for analysis and assembly of viral genomes\
    \ such as Classical Swine Fever Virus (<a href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=11096\"\
    \ rel=\"nofollow\">CSFV</a>) from IonTorrent AmpliSeq data.</p>\n<h2>\n<a id=\"\
    user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Overview</h2>\n\
    <ul>\n<li>\n<p>Preprocessing</p>\n<ul>\n<li>Duplicate reads were removed using\
    \ <a href=\"https://broadinstitute.github.io/picard/\" rel=\"nofollow\">Picard</a>\n\
    </li>\n<li>Reads were trimmed with <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\"\
    \ rel=\"nofollow\">Trimmomatic</a> prior to <a href=\"http://cab.spbu.ru/software/spades/\"\
    \ rel=\"nofollow\">SPAdes</a> assembly</li>\n<li>BAM file stats computed using\
    \ <a href=\"https://samtools.github.io/\" rel=\"nofollow\">Samtools</a> (coverage\
    \ depth, extent, extent per genome, # of reads mapped)</li>\n</ul>\n</li>\n<li>\n\
    <p>Reference Genome Selection</p>\n<ul>\n<li>Downloading of all Classical swine\
    \ fever virus (<a href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=11096\"\
    \ rel=\"nofollow\">CSFV</a>) (or FMDV, Ebola, Zika) virus genomes from <a href=\"\
    https://www.ncbi.nlm.nih.gov/books/NBK25501/\" rel=\"nofollow\">NCBI Entrez API</a>\
    \ using <a href=\"https://biopython.org/\" rel=\"nofollow\">BioPython</a>\n</li>\n\
    <li>\n<a href=\"https://mash.readthedocs.io/en/latest/\" rel=\"nofollow\">Mash</a>\
    \ screen of deduplicated reads against all reference genomes with sketch size\
    \ of 10000 and sketch k-mer size of 16, sorting by Mash screen identity to find\
    \ top reference genome for read mapping and variant calling</li>\n</ul>\n</li>\n\
    <li>\n<p>Read Mapping &amp; Variant Calling</p>\n<ul>\n<li>Read mapping with <a\
    \ href=\"https://github.com/lh3/bwa\">BWA MEM</a>\n</li>\n<li>Removal of duplicate\
    \ reads with <a href=\"https://samtools.github.io/\" rel=\"nofollow\">Samtools</a>\n\
    </li>\n<li>Variant calling with <a href=\"https://github.com/ekg/freebayes\">FreeBayes</a>\n\
    </li>\n<li>\n<a href=\"http://snpeff.sourceforge.net/SnpEff.html\" rel=\"nofollow\"\
    >SnpEff</a> was used to predict and report variant effects using reference genome\
    \ annotation</li>\n</ul>\n</li>\n<li>\n<p>De Novo Assembly</p>\n<ul>\n<li>\n<a\
    \ href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\">SPAdes</a> de\
    \ novo assembly of trimmed deduplicated reads.</li>\n<li>\n<a href=\"http://quast.sourceforge.net/quast.html\"\
    \ rel=\"nofollow\">QUAST</a> quality assessment of assemblies</li>\n</ul>\n</li>\n\
    <li>\n<p>Quality Control</p>\n<ul>\n<li>\n<a href=\"https://multiqc.info/\" rel=\"\
    nofollow\">MultiQC</a> interactive report of <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\"\
    \ rel=\"nofollow\">FastQC</a>, <a href=\"https://samtools.github.io/\" rel=\"\
    nofollow\">Samtools</a>, <a href=\"http://quast.sourceforge.net/quast.html\" rel=\"\
    nofollow\">QUAST</a>, <a href=\"http://snpeff.sourceforge.net/SnpEff.html\" rel=\"\
    nofollow\">SnpEff</a>\n</li>\n</ul>\n</li>\n<li>\n<p>Phylogenetic Tree</p>\n<ul>\n\
    <li>Phylogenetic tree constructed with <a href=\"http://www.iqtree.org/\" rel=\"\
    nofollow\">IQ-TREE</a> (or <a href=\"http://bioinformatics.hungry.com/clearcut/\"\
    \ rel=\"nofollow\">Clearcut</a> if a quick and dirty tree is okay)</li>\n<li>Interactive\
    \ HTML phylogenetic tree visualization with <a href=\"http://phylocanvas.org/\"\
    \ rel=\"nofollow\">PhyloCanvas</a> using <a href=\"https://github.com/peterk87/shiptv\"\
    >shiptv</a>\n</li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-authors\"\
    \ class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Authors</h2>\n<ul>\n<li>Peter\
    \ Kruczkiewicz (@peterk87)</li>\n</ul>\n<h2>\n<a id=\"user-content-usage\" class=\"\
    anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-step-0-install-pre-requisites\"\
    \ class=\"anchor\" href=\"#step-0-install-pre-requisites\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 0:\
    \ Install pre-requisites</h3>\n<p>Running this workflow with <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> is recommended, but you can use <a href=\"\
    https://conda.io/en/latest/\" rel=\"nofollow\">Conda</a> if you prefer. The Singularity\
    \ image will come with all the dependencies bundled together in a single file.</p>\n\
    <h4>\n<a id=\"user-content-install-singularity-recommended\" class=\"anchor\"\
    \ href=\"#install-singularity-recommended\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> (recommended)</h4>\n<p>Follow the instructions\
    \ for installing Singularity <a href=\"https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-start\"\
    \ rel=\"nofollow\">here</a></p>\n<h4>\n<a id=\"user-content-setup-and-activate-the-conda-environment-if-not-using-singularity-optional\"\
    \ class=\"anchor\" href=\"#setup-and-activate-the-conda-environment-if-not-using-singularity-optional\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Setup and activate the <a href=\"https://conda.io/en/latest/\" rel=\"\
    nofollow\">Conda</a> environment if not using <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> (optional)</h4>\n<p>Install <a href=\"https://conda.io/en/latest/\"\
    \ rel=\"nofollow\">Conda</a> if you haven't already following <a href=\"https://conda.io/en/latest/miniconda.html\"\
    \ rel=\"nofollow\">these instructions</a> and setup the <a href=\"https://bioconda.github.io/user/install.html#set-up-channels\"\
    \ rel=\"nofollow\">BioConda channel</a>.</p>\n<p>Download or <code>git clone</code>\
    \ this repo</p>\n<div class=\"highlight highlight-source-shell\"><pre>git clone\
    \ https://github.com/peterk87/viral-ampliseq-assembly.git\n<span class=\"pl-c1\"\
    >cd</span> viral-ampliseq-assembly\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> create a conda environment named \"viral-ampliseq-assembly-1.0.0\"</span>\n\
    conda env create -f environment.yml\nconda activate viral-ampliseq-assembly-1.0.0\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> install snakemake into this\
    \ env</span>\nconda install -y snakemake\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> run Snakemake on the test directory</span>\nsnakemake --directory\
    \ test/</pre></div>\n<h3>\n<a id=\"user-content-step-1-install-workflow\" class=\"\
    anchor\" href=\"#step-1-install-workflow\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 1: Install workflow</h3>\n\
    <p>If you simply want to use this workflow, download and extract the <a href=\"\
    https://github.com/peterk87/viral-ampliseq-assembly/releases\">latest release</a>.\n\
    If you intend to modify and further develop this workflow, fork this repository.\
    \ Please consider providing any generally applicable modifications via a pull\
    \ request.</p>\n<p>In any case, if you use this workflow in a paper, don't forget\
    \ to give credits to the authors by citing the URL of this repository and, if\
    \ available, its DOI (see above).</p>\n<h3>\n<a id=\"user-content-step-2-configure-workflow\"\
    \ class=\"anchor\" href=\"#step-2-configure-workflow\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 2: Configure\
    \ workflow</h3>\n<p>Create an analysis directory, copy and modify the example\
    \ <code>config.yaml</code> and <code>samples.tsv</code> files to suit your needs.</p>\n\
    <p>e.g.</p>\n<pre><code>mkdir ~/my-ampliseq-analysis\ncp viral-ampliseq-assembly/config.yaml\
    \ ~/my-ampliseq-analysis/\ncp viral-ampliseq-assembly/samples.tsv ~/my-ampliseq-analysis/\n\
    </code></pre>\n<p>Edit your <code>config.yaml</code> as needed.</p>\n<p>Add sample\
    \ entries to your <code>samples.tsv</code></p>\n<pre><code>sample  bam_file\n\
    Sample1 bams/Sample1.bam\nSample2 bams/Sample2.bam\nSample3 bams/Sample3.bam\n\
    ... &lt;more sample entries&gt;\n</code></pre>\n<p>where <code>bam_file</code>\
    \ can be the relative or absolute path to a sample's BAM file.</p>\n<h4>\n<a id=\"\
    user-content-iq-tree-maximum-likelihood-or-clearcut-rnj-tree\" class=\"anchor\"\
    \ href=\"#iq-tree-maximum-likelihood-or-clearcut-rnj-tree\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"\
    http://www.iqtree.org/\" rel=\"nofollow\">IQ-TREE</a> maximum-likelihood or <a\
    \ href=\"http://bioinformatics.hungry.com/clearcut/\" rel=\"nofollow\">Clearcut</a>\
    \ RNJ tree</h4>\n<p>In your <code>config.yaml</code> the <code>fast_tree</code>\
    \ parameter controls which method (ML or RNJ) is used for phylogenetic tree construction.</p>\n\
    <p>If you want a quick and dirty tree, set</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre><span class=\"pl-ent\">fast_tree</span>: <span class=\"pl-c1\">true</span></pre></div>\n\
    <p>in your <code>config.yaml</code> to generate a Relaxed Neighbor Joining (RNJ)\
    \ tree.</p>\n<p>Otherwise, if you want a high accuracy phylogenetic tree and are\
    \ willing to wait for it, then set</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre><span class=\"pl-ent\">fast_tree</span>: <span class=\"pl-c1\">false</span></pre></div>\n\
    <p>to use <a href=\"http://www.iqtree.org/\" rel=\"nofollow\">IQ-TREE</a> to generate\
    \ a maximum-likelihood phylogenetic tree with 1000 ultrafast bootstraps (UFBoot)\
    \ (see <a href=\"http://dx.doi.org/10.1093/molbev/mst024\" rel=\"nofollow\">Minh\
    \ et al., 2016</a> for more info on UFBoot).</p>\n<h3>\n<a id=\"user-content-step-3-execute-workflow\"\
    \ class=\"anchor\" href=\"#step-3-execute-workflow\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 3: Execute\
    \ workflow</h3>\n<p><em>If you do not have <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> installed then remove the <code>--use-singularity</code>\
    \ flag</em></p>\n<p>Test your configuration by performing a dry-run via</p>\n\
    <pre><code>snakemake --use-singularity -n\n</code></pre>\n<p>Execute the workflow\
    \ locally via</p>\n<pre><code>snakemake --use-singularity --cores $N\n</code></pre>\n\
    <p>using <code>$N</code> cores.</p>\n<h4>\n<a id=\"user-content-cluster-execution\"\
    \ class=\"anchor\" href=\"#cluster-execution\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Cluster execution</h4>\n<p><em>Note:\
    \ You may need to install the <code>drmaa</code> Python library (<code>pip install\
    \ drmaa</code>)</em></p>\n<p>You can execute the workflow on a SLURM/DRMAA cluster\
    \ environment with</p>\n<pre><code>snakemake --directory test --use-singularity\
    \ --drmaa \" -c 4 -p YourClusterQueueName --mem=4096 \" -j 8 -w 60\n</code></pre>\n\
    <p>This will run the workflow on the test data in the <code>test/</code> directory\
    \ with 4 CPUs and 4G memory per job and 8 jobs at once (<code>-j 8</code>) while\
    \ waiting 60 seconds for output files to appear on the shared filesystem (<code>-w\
    \ 60</code>).</p>\n<p>The cluster partition or queue to schedule jobs to is specified\
    \ with <code>-p YourClusterQueueName</code>.</p>\n<p>The above will run each rule\
    \ or job with 4 CPUs and 4GB memory each, which may be way more than needed or\
    \ not enough so you could create a YAML (or JSON) file to specify default and\
    \ specific resource requirements for some steps:</p>\n<p>Example <code>cluster-config.yaml</code>:</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">__default__</span>:\n\
    \    <span class=\"pl-ent\">cpu</span>: <span class=\"pl-c1\">1</span>\n    <span\
    \ class=\"pl-ent\">partition</span>: <span class=\"pl-s\">YourClusterQueueName</span>\n\
    \    <span class=\"pl-ent\">memory</span>: <span class=\"pl-c1\">1024</span>\n\
    <span class=\"pl-ent\">samtools_index_bam_initial</span>:\n    <span class=\"\
    pl-ent\">cpu</span>: <span class=\"pl-c1\">32</span>\n    <span class=\"pl-ent\"\
    >memory</span>: <span class=\"pl-c1\">16384</span>\n<span class=\"pl-ent\">spades_assembly</span>:\n\
    \    <span class=\"pl-ent\">cpu</span>: <span class=\"pl-c1\">32</span>\n    <span\
    \ class=\"pl-ent\">memory</span>: <span class=\"pl-c1\">16384</span>\n<span class=\"\
    pl-ent\">bwa_mem</span>:\n    <span class=\"pl-ent\">cpu</span>: <span class=\"\
    pl-c1\">32</span>\n    <span class=\"pl-ent\">memory</span>: <span class=\"pl-c1\"\
    >4096</span>\n<span class=\"pl-ent\">mafft_msa</span>:\n    <span class=\"pl-ent\"\
    >cpu</span>: <span class=\"pl-c1\">32</span>\n    <span class=\"pl-ent\">memory</span>:\
    \ <span class=\"pl-c1\">4096</span>\n<span class=\"pl-ent\">iqtree</span>:\n \
    \   <span class=\"pl-ent\">cpu</span>: <span class=\"pl-c1\">8</span>\n    <span\
    \ class=\"pl-ent\">memory</span>: <span class=\"pl-c1\">4096</span>\n<span class=\"\
    pl-ent\">snpeff</span>:\n    <span class=\"pl-ent\">memory</span>: <span class=\"\
    pl-c1\">4096</span></pre></div>\n<p>With the <code>cluster-config.yaml</code>,\
    \ run the workflow in a cluster environment via</p>\n<pre><code>snakemake --directory\
    \ test --use-singularity --cluster-config cluster-config.yaml --drmaa \" -c {cluster.cpu}\
    \ -p {cluster.partition} --mem={cluster.memory} \" -j 8 -w 60\n</code></pre>\n\
    <p>With the above command and <code>cluster-config.yaml</code>, by default, a\
    \ rule or step in the workflow will only use 1 CPU and request 1G of memory, while\
    \ the rules like <code>iqtree</code> or <code>spades_assembly</code> will request\
    \ more CPUs and memory from the SLURM/DRMAA scheduler.</p>\n<p>See the <a href=\"\
    https://snakemake.readthedocs.io\" rel=\"nofollow\">Snakemake documentation</a>\
    \ for further details.</p>\n<h2>\n<a id=\"user-content-testing\" class=\"anchor\"\
    \ href=\"#testing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Testing</h2>\n<p>Tests cases are in the subfolder\
    \ <code>test</code>. They should be executed via continuous integration with Travis\
    \ CI.</p>\n<h2>\n<a id=\"user-content-output\" class=\"anchor\" href=\"#output\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Output</h2>\n<p>If you were to copy the files in <code>test</code>\
    \ (<code>samples.tsv</code>, <code>bam/</code> and <code>config.yaml</code>) to\
    \ a new directory <code>my-analysis-directory</code> and run the workflow on that\
    \ directory, i.e.</p>\n<div class=\"highlight highlight-source-shell\"><pre>snakemake\
    \ --directory my-analysis-directory/ <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> other args</span></pre></div>\n<p>The contents of <code>my-analysis-directory</code>\
    \ should look like:</p>\n<div class=\"highlight highlight-source-shell\"><pre>my-analysis-directory\n\
    \u251C\u2500\u2500 phylogeny <span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Phylogenetic Tree Output</span>\n\u2502   \u251C\u2500\u2500 genome-metadata.tsv\n\
    \u2502   \u2514\u2500\u2500 tree.html\n\u251C\u2500\u2500 config.yaml <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> INPUT: Workflow Execution Config File </span>\n\
    \u251C\u2500\u2500 qc <span class=\"pl-c\"><span class=\"pl-c\">#</span> Quality\
    \ Control Output</span>\n\u2502   \u251C\u2500\u2500 multiqc.html <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> MultiQC report file</span>\n\u2502   \u251C\
    \u2500\u2500 fastqc <span class=\"pl-c\"><span class=\"pl-c\">#</span> FastQC\
    \ Output</span>\n\u2502   \u2502   \u251C\u2500\u2500 Sample1.html\n\u2502   \u2502\
    \   \u2514\u2500\u2500 Sample1_fastqc.zip\n\u2502   \u251C\u2500\u2500 multiqc_data\n\
    \u2502   \u2502   \u251C\u2500\u2500 [Text files]\n\u2502   \u2514\u2500\u2500\
    \ quast <span class=\"pl-c\"><span class=\"pl-c\">#</span> QUAST Output</span>\n\
    \u2502       \u251C\u2500\u2500 report.tex\n\u2502       \u251C\u2500\u2500 icarus_viewers\n\
    \u2502       \u2502   \u2514\u2500\u2500 contig_size_viewer.html\n\u2502     \
    \  \u251C\u2500\u2500 report.html\n\u2502       \u251C\u2500\u2500 basic_stats\n\
    \u2502       \u2502   \u251C\u2500\u2500 [QUAST PDFs]\n\u2502       \u251C\u2500\
    \u2500 icarus.html\n\u2502       \u251C\u2500\u2500 transposed_report.tex\n\u2502\
    \       \u251C\u2500\u2500 quast.log\n\u2502       \u251C\u2500\u2500 report.pdf\n\
    \u2502       \u251C\u2500\u2500 report.txt\n\u2502       \u251C\u2500\u2500 .snakemake_timestamp\n\
    \u2502       \u251C\u2500\u2500 report.tsv\n\u2502       \u251C\u2500\u2500 transposed_report.tsv\n\
    \u2502       \u2514\u2500\u2500 transposed_report.txt\n\u251C\u2500\u2500 variant_calling\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> Variant Calling Output</span>\n\
    \u2502   \u251C\u2500\u2500 Sample1-filtered.vcf <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Filtered variants for Sample1 in VCF format</span>\n\u2502   \u251C\
    \u2500\u2500 Sample1.vcf <span class=\"pl-c\"><span class=\"pl-c\">#</span> Unfiltered\
    \ variants for Sample1 in VCF format</span>\n\u2502   \u251C\u2500\u2500 snpeff\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> SnpEff Output</span>\n\u2502\
    \   \u2502   \u251C\u2500\u2500 Sample1\n\u2502   \u2502   \u2502   \u251C\u2500\
    \u2500 [SnpEff specific files]\n\u2502   \u2502   \u251C\u2500\u2500 Sample1.vcf\n\
    \u2502   \u2502   \u251C\u2500\u2500 Sample1.csv\n\u2502   \u2502   \u251C\u2500\
    \u2500 Sample1.html <span class=\"pl-c\"><span class=\"pl-c\">#</span> SnpEff\
    \ report for Sample1</span>\n\u2502   \u2502   \u2514\u2500\u2500 Sample1.genes.txt\n\
    \u2502   \u2514\u2500\u2500 Sample1-vcf.tsv <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> SnpEff annotated variants in a tab-delimited table</span>\n\u251C\
    \u2500\u2500 mapping <span class=\"pl-c\"><span class=\"pl-c\">#</span> Read Mapping\
    \ Output</span>\n\u2502   \u2514\u2500\u2500 Sample1 <span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> Read mapping output and summary files for Sample1</span>\n\
    \u2502       \u251C\u2500\u2500 Sample1-extent.tsv\n\u2502       \u251C\u2500\u2500\
    \ Sample1-genome_extent.tsv\n\u2502       \u251C\u2500\u2500 Sample1-idxstats.tsv\n\
    \u2502       \u251C\u2500\u2500 Sample1.bam\n\u2502       \u251C\u2500\u2500 Sample1-depth.tsv\n\
    \u2502       \u251C\u2500\u2500 Sample1-idxstats-sorted.tsv\n\u2502       \u251C\
    \u2500\u2500 Sample1-idxstats-top_mapped.txt\n\u2502       \u2514\u2500\u2500\
    \ Sample1.bam.bai\n\u251C\u2500\u2500 bam <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Input directory with Sample1 BAM file specified in config.yaml</span>\n\
    \u2502   \u2514\u2500\u2500 a.bam\n\u251C\u2500\u2500 consensus <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Consensus Sequence Output</span>\n\u2502\
    \   \u2514\u2500\u2500 Sample1.fasta <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Consensus sequence for Sample1 from reference mapping and variant calling</span>\n\
    \u251C\u2500\u2500 logs <span class=\"pl-c\"><span class=\"pl-c\">#</span> Log\
    \ files for various tools</span>\n\u2502   \u251C\u2500\u2500 <span class=\"pl-k\"\
    >&lt;</span>tool name<span class=\"pl-k\">&gt;</span>\n\u2502   \u2502   \u2514\
    \u2500\u2500 Sample1.log\n\u251C\u2500\u2500 samples.tsv <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> INPUT: tab-delimited table with 2 fields: \"sample\"\
    \ and \"bam_file\"</span>\n\u251C\u2500\u2500 references <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Reference Genomes Downloaded From NCBI</span>\n\
    \u2502   \u251C\u2500\u2500 Sample1 <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Top Reference Genome</span>\n\u2502   \u2502   \u251C\u2500\u2500 reference.gff\n\
    \u2502   \u2502   \u251C\u2500\u2500 reference-no_ambig.fasta.bwt\n\u2502   \u2502\
    \   \u251C\u2500\u2500 reference-no_ambig.fasta.pac\n\u2502   \u2502   \u251C\u2500\
    \u2500 reference.genbank\n\u2502   \u2502   \u251C\u2500\u2500 reference-no_ambig.fasta.amb\n\
    \u2502   \u2502   \u251C\u2500\u2500 reference-no_ambig.fasta.ann\n\u2502   \u2502\
    \   \u251C\u2500\u2500 reference-no_ambig.fasta\n\u2502   \u2502   \u251C\u2500\
    \u2500 reference-no_ambig.fasta.sa\n\u2502   \u2502   \u251C\u2500\u2500 reference.fasta\n\
    \u2502   \u2502   \u2514\u2500\u2500 reference-no_ambig.fasta.fai\n\u2502   \u251C\
    \u2500\u2500 csf.msh <span class=\"pl-c\"><span class=\"pl-c\">#</span> Mash sketch\
    \ database from \"csf.fasta\"</span>\n\u2502   \u251C\u2500\u2500 csf.genbank\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> CSFV genomes downloaded from\
    \ NCBI in GenBank format</span>\n\u2502   \u2514\u2500\u2500 csf.fasta <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> CSFV genomes downloaded from NCBI in FASTA\
    \ format</span>\n\u251C\u2500\u2500 assembly <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Assembly Output</span>\n\u2502   \u251C\u2500\u2500 spades <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> SPAdes assembly outputs for each\
    \ input sample</span>\n\u2502   \u2502   \u2514\u2500\u2500 Sample1 <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> SPAdes assembly output for Sample1</span>\n\
    \u2502   \u2502       \u251C\u2500\u2500 before_rr.fasta\n\u2502   \u2502    \
    \   \u251C\u2500\u2500 params.txt\n\u2502   \u2502       \u251C\u2500\u2500 contigs.paths\n\
    \u2502   \u2502       \u251C\u2500\u2500 input_dataset.yaml\n\u2502   \u2502 \
    \      \u251C\u2500\u2500 <span class=\"pl-k\">&lt;</span>SPAdes specific output\
    \ directories<span class=\"pl-k\">&gt;</span>\n\u2502   \u2502       \u251C\u2500\
    \u2500 scaffolds.paths\n\u2502   \u2502       \u251C\u2500\u2500 contigs.fasta\n\
    \u2502   \u2502       \u251C\u2500\u2500 spades.log\n\u2502   \u2502       \u251C\
    \u2500\u2500 assembly_graph.fastg\n\u2502   \u2502       \u251C\u2500\u2500 dataset.info\n\
    \u2502   \u2502       \u251C\u2500\u2500 scaffolds.fasta\n\u2502   \u2502    \
    \   \u2514\u2500\u2500 assembly_graph_with_scaffolds.gfa\n\u2502   \u2514\u2500\
    \u2500 spades-Sample1.fasta\n\u251C\u2500\u2500 benchmarks <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Benchmark runtime info for tools in workflow</span>\n\
    \u2502   \u251C\u2500\u2500 <span class=\"pl-k\">&lt;</span>benchmark tab-delimited\
    \ files <span class=\"pl-k\">for</span> <span class=\"pl-smi\">various tools</span>\
    \ <span class=\"pl-k\">in</span> workflow<span class=\"pl-k\">&gt;</span>\n\u251C\
    \u2500\u2500 msa <span class=\"pl-c\"><span class=\"pl-c\">#</span> Multiple sequence\
    \ alignment (MSA) output and IQ-TREE/Clearcut phylogenetic tree</span>\n\u2502\
    \   \u251C\u2500\u2500 alignment.fasta\n\u2502   \u251C\u2500\u2500 samples-pre-aln.fasta\n\
    \u2502   \u2514\u2500\u2500 alignment.fasta.treefile\n\u2514\u2500\u2500 preprocess\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> Preprocessing Output of Input\
    \ BAM Files </span>\n    \u251C\u2500\u2500 samtools <span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> Initial BAM file stats output</span>\n    \u2502   \u251C\
    \u2500\u2500 depth\n    \u2502   \u2502   \u251C\u2500\u2500 Sample1-extent.tsv\n\
    \    \u2502   \u2502   \u251C\u2500\u2500 Sample1-genome_extent.tsv\n    \u2502\
    \   \u2502   \u2514\u2500\u2500 Sample1.tsv\n    \u2502   \u251C\u2500\u2500 flagstat\n\
    \    \u2502   \u2502   \u2514\u2500\u2500 Sample1.flagstat\n    \u2502   \u251C\
    \u2500\u2500 index\n    \u2502   \u2502   \u2514\u2500\u2500 Sample1.done\n  \
    \  \u2502   \u2514\u2500\u2500 idxstats\n    \u2502       \u251C\u2500\u2500 Sample1-top_mapped.txt\n\
    \    \u2502       \u251C\u2500\u2500 Sample1.tsv\n    \u2502       \u2514\u2500\
    \u2500 Sample1-sorted.tsv\n    \u251C\u2500\u2500 fastqs <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Deduplicated reads in FASTQ format</span>\n   \
    \ \u2502   \u2514\u2500\u2500 Sample1.fastq\n    \u251C\u2500\u2500 mash <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Mash Screen results</span>\n  \
    \  \u2502   \u251C\u2500\u2500 Sample1-screen_references-sorted.tsv\n    \u2502\
    \   \u2514\u2500\u2500 Sample1-screen_references.tsv\n    \u251C\u2500\u2500 trimmed_fastqs\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> Trimmomatic trimmed reads</span>\n\
    \    \u2502   \u2514\u2500\u2500 Sample1.fastq\n    \u2514\u2500\u2500 dedup <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Deduplicated BAM files</span>\n\
    \        \u251C\u2500\u2500 Sample1.bam\n        \u251C\u2500\u2500 Sample1.metrics.txt\n\
    \        \u2514\u2500\u2500 Sample1.bam.bai</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1566573045.0
piyanatk/singularity-containers:
  data_format: 2
  description: Definition files for singularity container
  filenames:
  - Singularity.reach
  - Singularity.one-point-stats
  full_name: piyanatk/singularity-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-containers" class="anchor" href="#singularity-containers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-containers</h1>

    <p>Definition files for singularity container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624371927.0
pmitev/Teoroo-singularity:
  data_format: 2
  description: Singularity images and recipes
  filenames:
  - Atom/Singularity.atom
  - rstudio-server/Singularity.rstudio-server
  - acroread/Singularity.acroread
  - ubuntu/Singularity.1804
  - ubuntu/Singularity.2004
  - VESTA/Singularity.vesta
  - emacs/Singularity.emacs
  - xcrysden/Singularity.xcrysden
  - xcrysden/Singularity.xcrysden_1.5.60
  - lammps/Singularity.lammps_prophet
  - lammps/Singularity.lammps_ase
  - lammps/Singularity.lammps
  - lammps/Singularity.lammps_ase_kim
  - AMPE/Singularity.ampe
  - kmos/Singularity.kmos3_9
  - kmos/Singularity.kmos
  - pp/Singularity.pp2
  - gdis/Singularity.gdis
  - deal.II/Singularity.deal
  - MD2-lab/Singularity.md2-lab
  - mongodb/Singularity.mongodb
  - jmol/Singularity.jmol
  - jupyter/Singularity.jupyter
  - gromacs/Singularity.gromacs
  - obabel/Singularity.obabel
  - graphics/Singularity.gnuplot_alpine
  - graphics/Singularity.graphics
  - graphics/Singularity.gnuplot_5.4
  - graphics/Singularity.gnuplot_4.6
  - graphics/Singularity.gnuplot_4.6a
  - graphics/Singularity.gnuplot_5.4a
  - clease/Singularity.clease
  - tools/Singularity.mc
  - tools/Singularity.gawk
  - tools/Singularity.vim
  - tools/Singularity.meld
  - tools/Singularity.gnuplot
  - cuda/Singularity.u18.04_cuda9.2
  - tesseract/Singularity.tesseract
  - texlive/Singularity.texlive
  - ase-twistd/Singularity.ase-twistd
  - mkdocs-serve/Singularity.mkdocs-serve
  full_name: pmitev/Teoroo-singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2338" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-teoroo-singularity" class="anchor" href="#teoroo-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Teoroo-singularity</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - singularity-containers
  updated_at: 1623074518.0
pmitev/UPPMAX-Singularity:
  data_format: 2
  description: UPPMAX Singularity builds
  filenames:
  - UniteM/Singularity.UniteM
  - gapseq/Singularity.gapseq
  - metaWRAP/Singularity.metaWRAP-deps-only-ubuntu
  - metaWRAP/Singularity.metaWRAP-deps-only
  - metaWRAP/Singularity.metaWRAP
  - bonito/Singularity.bonito
  - MitoZ/Singularity.v2.3-pm
  - gromacs/Singularity.gromacs
  full_name: pmitev/UPPMAX-Singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-uppmax-singularity" class="anchor" href="#uppmax-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>UPPMAX-Singularity</h1>

    <p>UPPMAX Singularity builds</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623764674.0
pmonnahan/AncInf:
  data_format: 2
  description: Local Ancestry Inference
  filenames:
  - singularity/Singularity_defs.def
  full_name: pmonnahan/AncInf
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-ancestry-inference\" class=\"anchor\" href=\"\
    #ancestry-inference\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Ancestry Inference</h1>\n<p>Human local ancestry\
    \ inference using <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3738819/\"\
    \ rel=\"nofollow\">RFmix</a>.  The basic workflow is to parse the input PLINK\
    \ file by chromosome, perform reference-based haplotype phasing on the data using\
    \ <a href=\"https://odelaneau.github.io/shapeit4/\" rel=\"nofollow\">ShapeIt4</a>,\
    \ and, finally, perform local ancestry inference with RFMix.  More information\
    \ is provided in the <em>Pipeline Overview</em> below.  With the RFMix output,\
    \ admixture mapping (i.e. associating local ancestry with phenotype) can be accomplished\
    \ via a separate pipeline found <a href=\"https://github.com/pmonnahan/admixMap\"\
    >here</a>.</p>\n\n\n<p><strong>Table of Contents</strong></p>\n<ul>\n<li>\n<a\
    \ href=\"#requirements\">Requirements</a>\n<ul>\n<li><a href=\"#snakemake\">Snakemake</a></li>\n\
    <li><a href=\"#singularity\">Singularity</a></li>\n</ul>\n</li>\n<li>\n<a href=\"\
    #running-the-workflow\">Running the workflow</a>\n<ul>\n<li><a href=\"#other-notes\"\
    >Other Notes</a></li>\n<li><a href=\"#debugging-and-error-reports\">Debugging\
    \ and error reports</a></li>\n</ul>\n</li>\n<li>\n<a href=\"#pipeline-overview\"\
    >Pipeline Overview</a>\n<ul>\n<li><a href=\"#input-data\">Input Data</a></li>\n\
    <li><a href=\"#output\">Output</a></li>\n<li><a href=\"#reference-population\"\
    >Reference population</a></li>\n<li><a href=\"#phasing\">Phasing</a></li>\n<li><a\
    \ href=\"#local-ancestry-inference\">Local Ancestry Inference</a></li>\n</ul>\n\
    </li>\n</ul>\n\n<p><a href=\"https://github.com/pmonnahan/AncInf/blob/master/Pipeline_DAG.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pmonnahan/AncInf/raw/master/Pipeline_DAG.png\"\
    \ alt=\"Pipeline DAG\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<h3>\n<a id=\"\
    user-content-snakemake\" class=\"anchor\" href=\"#snakemake\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Snakemake</h3>\n\
    <p>The pipeline is coordinated and run on an HPC (or locally) using <em>Snakemake</em>.\
    \  To install snakemake, first create a virtual environment via:</p>\n<pre><code>module\
    \ load python3/3.6.3_anaconda5.0.1\nconda install -c conda-forge mamba\nmamba\
    \ create -c conda-forge -c bioconda -n &lt;your_environment_name&gt; snakemake\n\
    </code></pre>\n<p>This will create a new virtual environment and install <code>snakemake</code>.\
    \  Then, activate this environment and perform following installations:</p>\n\
    <pre><code>conda activate &lt;your_environment_name&gt;\nconda install numpy yaml\
    \ pandas\n</code></pre>\n<p>Anytime you need to run the pipeline, activate this\
    \ environment beforehand via:</p>\n<pre><code>conda activate &lt;environment_name&gt;\n\
    </code></pre>\n<p>If you choose not to create an environment, you must ensure\
    \ that these packages are installed and available for your python installation.</p>\n\
    <h3>\n<a id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity</h3>\n<p>The installation of the individual programs used\
    \ throughout this pipeline can be completely avoid by utilizing a Singularity\
    \ image.  This image is too large to be hosted on Github, although you can find\
    \ the definitions file used to create the image <a href=\"https://github.com/pmonnahan/AncInf/blob/master/singularity/Singularity_defs.def\"\
    >here</a>.  Building of images is still not currently supported at MSI, so I used\
    \ a Vagrant virtual machine, which comes with Singularity pre-configured/installed\
    \ (<a href=\"https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\"\
    \ rel=\"nofollow\">https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4</a>).\
    \  I can also share the img file directly upon request.</p>\n<p>However, in order\
    \ to utilize the singularity image, <em>singularity</em> must be installed on\
    \ the HPC.  Currently, the pipeline assumes that <em>singularity</em> will be\
    \ available as a module and can be loaded into the environment via the command\
    \ specified in the config.yml file, in the <code>module</code> entry under the\
    \  <code>singularity</code> section.  The default setting will work for MSI at\
    \ UMN.</p>\n<p>Singularity settings in config.yml</p>\n<pre><code>singularity:\n\
    \  use_singularity: 'true'\n  image: '/home/pmonnaha/pmonnaha/singularity/AncestryInference.sif\n\
    \  module: 'module load singularity'\n</code></pre>\n<h2>\n<a id=\"user-content-running-the-workflow\"\
    \ class=\"anchor\" href=\"#running-the-workflow\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running the workflow</h2>\n<p>Clone\
    \ this repository to the location where you want to store the output of the pipeline.</p>\n\
    <pre><code>git clone https://github.com/pmonnahan/AncInf.git rfmix_test\ncd rfmix_test\n\
    </code></pre>\n<p>The critical files responsible for executing the pipeline are\
    \ contained in the <em>./workflow</em> subdirectory contained within the cloned\
    \ repo.  They are:</p>\n<ul>\n<li>Snakefile</li>\n<li>config.yml</li>\n<li>cluster.yaml</li>\n\
    </ul>\n<p>The <em>Snakefile</em> is the primary workhouse of snakemake, which\
    \ specifies the dependencies of various parts of the pipeline and coordinates\
    \ execution.  No modifications to the <em>Snakefile</em> are necessary.</p>\n\
    <p>In order for the <em>Snakefile</em> to locate all of the necessary input and\
    \ correctly submit jobs to the cluster, <strong>both</strong> the <em>config.yaml</em>\
    \ and <em>cluster.yaml</em> need to be modified. Open these files and change the\
    \ required entries that are indicated with 'MODIFY'.  Other fields do not require\
    \ modification, although this may be desired given the particulars of the run\
    \ you wish to implement.  Details on each entry in the config file (e.g. what\
    \ the program expects in each entry as well as the purpose of the entry) are provided\
    \ in the <em>Pipeline Overview</em> at the bottom.</p>\n<p>The entire pipeline\
    \ can be executed on a local machine (not recommended) or on an HPC, and the <em>cluster.yaml</em>\
    \ file is required only for the latter.  For a local run, change the <code>local_run</code>\
    \ entry to <code>true</code> under the <code>run_settings</code> section of the\
    \ config file, and launch snakemake from within the parent directory by the simple\
    \ command:</p>\n<pre><code>snakemake\n</code></pre>\n<p>However, multiple steps\
    \ in the pipeline have high resource demands, and so are unlikely to be able to\
    \ be run locally.  This option exists primarily for testing and troubleshooting,\
    \ so the remainder of the  documentation assumes that the pipeline will be executed\
    \ on an HPC.  In order to coordinate the use of the HPC, the following modifications\
    \ to the snakemake command are required:</p>\n<pre><code>snakemake --cluster \"\
    sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 32\n</code></pre>\n<p>where -j specifies the number of jobs that can be submitted\
    \ at once.</p>\n<p>One additional setting in the <em>config.yml</em> is needed\
    \ in order to correctly submit jobs to the HPC.  The relevant entries are under\
    \ the <code>run_settings</code> section of the config file:</p>\n<pre><code>run_settings:\n\
    \  local_run: 'false'\n  cluster_config: 'workflow/cluster_slurm.yaml'\n  scheduler:\
    \ 'slurm'\n</code></pre>\n<p>Here, it is necessary that the <code>cluster_config</code>\
    \ entry is set to the path of the cluster_slurm.yaml file that will be used in\
    \ the snakemake command.  Also, the scheduler must correspond to the syntax used\
    \ in the snakemake command and cluster.yaml file.  I should point out that these\
    \ additional changes are needed for responsibly using PLINK within a snakemake\
    \ framework, and are not directly needed for snakemake.  PLINK will attempt to\
    \ auto-detect available resources upon running regardless of the resources that\
    \ were requested when the job was submitted.  Therefore, we have to read and parse\
    \ the requested resources in the cluster config file in order for them to be communicated\
    \ to PLINK from within the Snakefile.</p>\n<h3>\n<a id=\"user-content-other-notes\"\
    \ class=\"anchor\" href=\"#other-notes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Other notes</h3>\n<p>It is recommended\
    \ that <em>snakemake</em> is run as an interactive session on an HPC.  <em>Snakemake</em>\
    \ will launch the specified number (via the -j flag) of jobs, and then will hang\
    \ and wait for them to finish.  As jobs finish (and assuming no errors), <em>snakemake</em>\
    \ will launch additional jobs keeping the total running jobs at whatever -j is\
    \ set for.  Although <em>snakemake</em> should not use a lot of memory, it could\
    \ have long run times, which is generally not advisable on login nodes.</p>\n\
    <p>One attractive feature of <em>snakemake</em> is its ability to keep track of\
    \ the progress and dependencies of the different stages of the pipeline.  Specifically,\
    \ if an error is encountered or the pipeline otherwise stops before the final\
    \ step, <em>snakemake</em> can resume the pipeline where it left off, avoiding\
    \ redundant computation for previously completed tasks.  To do so, simply resubmit\
    \ the original <em>snakemake</em> command.</p>\n<p>To run a specific part of the\
    \ pipeline, do:</p>\n<pre><code>snakemake -R &lt;rule_name&gt; --cluster \"sbatch\
    \ --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 20 --rerun-incomplete\n</code></pre>\n<p>where <em>rule_name</em> indicates\
    \ the 'rule' (i.e. job) in the Snakefile that you wish to run.  Or, you can request\
    \ a specific file by providing the filename at the end of the command.  You may\
    \ need to include the -F (i.e. force) if the output file already exists and you\
    \ want to overwrite it.</p>\n<p>Also, it is often very helpful to do a 'dry-run'\
    \ of the pipeline in which the different steps and dependencies are printed to\
    \ screen, but no actual jobs are executed.  This can be helpful to ensure that\
    \ config entries are correct, etc.  To perform a dry-run, do:</p>\n<pre><code>snakemake\
    \ -nrp\n</code></pre>\n<p>NOTE: It is convenient to make an alias in your ~/.bashrc\
    \ file to run snakemake on the cluster without having to type the --cluster...\
    \ part of the command every time.  For me, it looked like this:</p>\n<pre><code>alias\
    \ snakeslurm=\"snakemake -k --cluster 'sbatch --no-requeue --partition={cluster.p}\
    \ --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --job-name={cluster.job-name}\
    \ --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}' --cluster-config workflow/cluster_slurm.yaml\"\
    \n</code></pre>\n<p>This way, I can just do:</p>\n<pre><code>snakeslurm -j 25\n\
    </code></pre>\n<p>To launch snakemake on the cluster.</p>\n<h4>\n<a id=\"user-content-unlocking-the-working-directory\"\
    \ class=\"anchor\" href=\"#unlocking-the-working-directory\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Unlocking\
    \ the working directory</h4>\n<p>When <em>snakemake</em> is launched it will place\
    \ a lock on the working directory, such that other <em>snakemake</em> runs are\
    \ prohibited from starting.  When <em>snakemake</em> finishes or errors out, it\
    \ will remove this lock.  However, sometimes this lock is not correctly removed.\
    \  This can occur, for example, if the VPN drops connection while <em>snakemake</em>\
    \ is running.  If you receive a \"Directory cannot be locked...\" error message\
    \ from <em>snakemake</em> and you are sure that no other <em>snakemake</em> processes\
    \ are currently running, you can unlock the directory by:</p>\n<pre><code>snakemake\
    \ --unlock\n</code></pre>\n<p>Then, you can run the usual <em>snakemake</em> command\
    \ to restart the pipeline.</p>\n<h4>\n<a id=\"user-content-debugging-and-error-reports\"\
    \ class=\"anchor\" href=\"#debugging-and-error-reports\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Debugging\
    \ and error reports</h4>\n<p>Should an error be encountered in a job, snakemake\
    \ will halt the pipeline and indicate in the terminal that an error has occurred.\
    \  The offending job will also be printed in red in the terminal window.  More\
    \ information on why the job failed can be found in the 'stdout' and 'stderr'\
    \ files that are output to the <em>'OandE'</em> directory and will be labelled\
    \ with the jobname.</p>\n<h2>\n<a id=\"user-content-pipeline-overview\" class=\"\
    anchor\" href=\"#pipeline-overview\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Overview</h2>\n<h3>\n\
    <a id=\"user-content-input-data\" class=\"anchor\" href=\"#input-data\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Input\
    \ Data</h3>\n<p>The pipeline expects as input a single set of PLINK files (.bed,\
    \ .fam, .bim) that has gone through basic QC steps (missingness, hwe, maf, etc).\
    \  I have written QC pipelines for non-imputed and imputed data, which are available\
    \ <a href=\"https://github.com/pmonnahan/DataPrep\">here</a> and <a href=\"https://github.com/pmonnahan/DataPrep/tree/master/postImpute\"\
    >here</a>, respectively.  It is technically possible to use imputed data in ancestry\
    \ inference, although this is not widely seen throughout the literature.</p>\n\
    <p>The input PLINK files are specified in the <code>query</code> entry within\
    \ the config file.</p>\n<pre><code>query: \"PATH_TO_PLINK_PREFIX\" \nsamples:\
    \ \"all\"  \n</code></pre>\n<p>The user can also provide a path to a file in the\
    \ <code>samples</code> entry, in which case the program will subset the <code>query</code>\
    \ dataset to include only the samples in the file (one sample per line).</p>\n\
    <p>It is assumed that the query coordinates and chromosome names are consistent\
    \ with those used in the reference VCF (see below).</p>\n<h3>\n<a id=\"user-content-output\"\
    \ class=\"anchor\" href=\"#output\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Output</h3>\n<p>All output is\
    \ labelled using the prefix specified in the <code>outname</code> entry in the\
    \ config file.</p>\n<pre><code>outname: \"AncInf\"\n</code></pre>\n<p>The RFMix\
    \ results will be output to the <em>rfmix</em> directory that is automatically\
    \ created. RFMix outputs a number of files, but the most relevant files are those\
    \ ending in <em>.Q</em> (which contain the global ancestry percentage estimates\
    \ for each individual) and the files ending in <em>.msp.tsv</em> (which contain\
    \ the maximum-likelihood ancestry state in each window analyzed; i.e. local ancestry).\
    \  The <em>.Q</em> files can be easily filtered to isolate individuals of a given\
    \ ethnicity, based on user-provided thresholds.</p>\n<p>A set of phased BCF files\
    \ (separated by chromosome) are generated as an intermediate step and are saved\
    \ to the <em>phased</em> directory.  This directory will also contain the phased\
    \ BCF of the individuals from the reference population.</p>\n<p>A good initial\
    \ check that the results make sense is to simply look at the average local ancestry\
    \ along a chromosome.  A full collection of these images (one for each chromosome)\
    \ will be created and output into the <em>chrom_plots</em> folder within the master\
    \ run directory.  These averages should remain fairly stable across the chromosome.\
    \  Any large, sudden changes in the dominant ancestral component are indicative\
    \ of issues in phasing or ancestry inference.  Furthermore, these chromosome plots\
    \ should be inspected to identify areas of suspect inference.  For example, drastic\
    \ changes in average ancestry is often observed near centromeres or telomeres.\
    \  These can also likely be flagged by low SNP counts in the inferred windows\
    \ (which is reported in the <em>.msp.tsv</em> files).</p>\n<h3>\n<a id=\"user-content-reference-population\"\
    \ class=\"anchor\" href=\"#reference-population\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Reference population</h3>\n<p>The\
    \ reference VCF to be used for phasing as well as for ancestry inference is provided\
    \ under the <code>reference</code> section of the config file.  The pipeline is\
    \ currently set up to use the 1000Genomes VCF (available <a href=\"https://www.internationalgenome.org/\"\
    \ rel=\"nofollow\">here</a> or by request) for the reference population.  However,\
    \ any VCF should work in theory as long as the necessary accessory files are provided.</p>\n\
    <pre><code>reference:\n  vcf: \"PATH_TO_REFERENCE_VCF\"\n  subpops: \"accessory/1000G_PopLabels.txt\"\
    \n  genmap: \"PATH_TO_DATA_SUBDIRECTORY/genetic_map_hg19.txt\"\n  phased_bcf:\
    \ 'none`  \n</code></pre>\n<p>There are two required files that need to accompany\
    \ the reference VCF, and these are provided at the <code>subpops</code> and <code>genmap</code>\
    \ entries.  The <code>subpops</code> file should be a text file with two columns:\
    \ sample ID as it appears in the VCF in the first column and the subpopulation\
    \ label for that sample in the second column.  If using the 1000Genomes VCF, then\
    \ the <code>subpop</code> file was automatically downloaded to the <em>accessory</em>\
    \ subdirectory.  The <code>genmap</code> file specifies the genetic map for the\
    \ reference genome and is too large to be hosted on GitHub.  However, the hg19\
    \ genetic map is available <a href=\"https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.html\"\
    \ rel=\"nofollow\">here</a> or by request.  The file contains 3 space-delimited\
    \ columns: chromosome, base position, genetic position.</p>\n<p>It is assumed\
    \ that the reference VCF file has been filtered, phased, and indexed. The VCF\
    \ does NOT need to be subsetted to include only the individuals from the desired\
    \ reference subpopulations.  This is accomplished by the initial steps of the\
    \ pipeline, using the <code>subpops</code> file described above along with the\
    \ comma-separated lists (no spaces!) in the <code>ref_pops</code> and <code>pop_names</code>\
    \ entries under the <code>rfmix</code> section of the config file.</p>\n<pre><code>rfmix:\n\
    \  ref_pops: \"YRI,GWD,ESN,CEU,IBS,TSI\" # No spaces!!\n  pop_names: \"AFR,AFR,AFR,EUR,EUR,EUR\"\
    \ \n</code></pre>\n<p>Based on the information contained in the <code>subpops</code>\
    \ file described above, individuals corresponding to the subpopulation names provided\
    \ in <code>ref_pops</code> entry are extracted from the reference VCF.  In addition,\
    \ a new file is created at:</p>\n<pre><code> accessory/Population_Map_File.txt\n\
    </code></pre>\n<p>, which re-labels the subsetted individuals with the corresponding\
    \ value in the <code>pop_names</code>.</p>\n<p>There is expected to be a 1:1 ordered\
    \ correspondence between the subpopulation labels <code>ref_pops</code> and the\
    \ superpopulation names in <code>pop_names</code>.  In this example where we are\
    \ interesting in inferring 2-way admixture between AFR and EUR populations, all\
    \ YRI, GWD, and ESN individuals would be extracted and re-labelled as AFR individuals,\
    \ while the CEU, IBS, and TSI individuals would be labelled as EUR individuals.\
    \  This scheme was developed to allow for flexibility in the inclusion/exclusion\
    \ of particular subpopulations.</p>\n<p>RFMix will sample randomly from within\
    \ these superpopulations to generate the training/test sets needed for the machine\
    \ learning algorithm.  It is best if the reference individuals from a superpopulation\
    \ are evenly distributed across subpopulations, so that a single subpopulation\
    \ does not dominate during the resampling.</p>\n<h3>\n<a id=\"user-content-phasing\"\
    \ class=\"anchor\" href=\"#phasing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Phasing</h3>\n<p>The config file\
    \ has the following options for modifying the behavior of haplotype phasing in\
    \ ShapeIt4:</p>\n<pre><code>phase:\n  threads: \"12\"\n  pbwt_depth: \"4\"\n \
    \ sequence_data: 'true'\n</code></pre>\n<p>Increasing the <code>pbwt_depth</code>\
    \ may increase the phasing accuracy, but comes at a substantial computational\
    \ cost.  The <code>sequence_data</code> entry should be set to false if the data\
    \ comes from an array.</p>\n<h3>\n<a id=\"user-content-local-ancestry-inference\"\
    \ class=\"anchor\" href=\"#local-ancestry-inference\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Local Ancestry\
    \ Inference</h3>\n<p>In addition to the <code>ref_pops</code> and <code>pop_names</code>,\
    \ the <code>rfmix</code> section of the config file provides a number of options\
    \ for modifying the behavior of RFMix.</p>\n<pre><code>rfmix:\n  ref_pops: \"\
    YRI,GWD,ESN,CEU,IBS,TSI\" # No spaces!!\n  pop_names: \"AFR,AFR,AFR,EUR,EUR,EUR\"\
    \ \n  generations: \"8\"\n  reanalyze_reference: \"true\" \n  window_size: \"\
    0.02\" \n  threads: \"12\"\n</code></pre>\n<p>The <code>generations</code> entry\
    \ specifies the number of generations in the past when admixture between the superpopulations\
    \ is assumed to have begun.  Values used in the literature are typically approximations\
    \ based off of historical events or genomic dating methods.  <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4289685/#:~:text=Patterns%20of%20Genetic%20Ancestry%20of%20Self%2DReported%20Latinos&amp;text=On%20average%2C%20we%20estimate%20that,%2C%20and%206.2%25%20African%20ancestry\"\
    \ rel=\"nofollow\">Bryc et al 2015</a> provide a good reference for African American\
    \ and Latinx ancestry inference.  For both scenarios, they modelled admixture\
    \ between Europeans and Native Americans at 11-12 generations ago and subsequent\
    \ admixture with Africans 6-8 generations ago.  Unfortunately, RFMix only allows\
    \ the user to specify a single value, so I have used '8' for African Americans\
    \ (modelling 2-way admixture between AFR and EUR) and '12' for Latinx individuals\
    \ (modelling 3-way admixture between AFR, EUR, and AMR)</p>\n<p>In the case that\
    \ a set of reference haplotypes may not be of \"pure\" ancestry and may themselves\
    \ be somewhat admixed, the option --reanalyze-reference will cause the program\
    \ to iteratively analyze the reference haplotypes as if they were query haplotypes,\
    \ in addition to analyzing the query input (see the <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3738819/\"\
    \ rel=\"nofollow\">RFmix</a> paper for a more thorough explanation of this procedure).\
    \  This is often advised for inferring local ancestry in Latinx populations, where\
    \ a 3-way AFR, EUR, and AMR admixture is modelled.  However, it is likely not\
    \ necessary for inferring ancestry in African American populations, where the\
    \ ancestral populations likely do not contain any admixed individuals.</p>\n<p>The\
    \ last relevant option is the window size in which ancestry is to be inferred.\
    \  This value is specified in centiMorgans (cM).  Default is 0.2 cM, which corresponds\
    \ to ~100 - 150 kb windows.  For a given window, there is a minimum requirement\
    \ on the number of SNPs, and windows will be expanded to meet this requirement\
    \ regardless of the specified window size.</p>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1616969120.0
pmonnahan/DataPrep:
  data_format: 2
  description: start with raw plink, end with standardized QCed plink
  filenames:
  - workflow/Singularity_defs.def
  full_name: pmonnahan/DataPrep
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-pre-imputation-qc-pipeline\" class=\"anchor\"\
    \ href=\"#pre-imputation-qc-pipeline\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pre-imputation QC pipeline</h1>\n\
    <p>The purpose of this pipeline is to perform the following for a set of input\
    \ PLINK datasets:</p>\n<ul>\n<li>basic QC (genotype/variant missingness, HWE,\
    \ and minor allele frequency)</li>\n<li>harmonize allele specifications with the\
    \ GRCh37 reference genome</li>\n<li>produce a set of VCF files (separated by chromosome)\
    \ for imputation</li>\n<li>merge filtered datasets into a single dataset consisting\
    \ only of overlapping sites.</li>\n</ul>\n<p>A companion pipeline, which performs\
    \ post-imputation QC, will download alongside the pre-imputation pipeline.  To\
    \ use the post-imputation pipeline, see the README in the postImpute directory.</p>\n\
    \n\n<p><strong>Table of Contents</strong></p>\n<ul>\n<li>\n<a href=\"#requirements\"\
    >Requirements</a>\n<ul>\n<li><a href=\"#snakemake\">Snakemake</a></li>\n<li><a\
    \ href=\"#singularity\">Singularity</a></li>\n</ul>\n</li>\n<li>\n<a href=\"#running-the-workflow\"\
    >Running the workflow</a>\n<ul>\n<li><a href=\"#other-notes\">Other Notes</a></li>\n\
    <li><a href=\"#debugging-and-error-reports\">Debugging and error reports</a></li>\n\
    </ul>\n</li>\n<li>\n<a href=\"#pipeline-overview\">Pipeline Overview</a>\n<ul>\n\
    <li><a href=\"#input-data\">Input Data</a></li>\n<li><a href=\"#output\">Output</a></li>\n\
    <li><a href=\"#dataset-harmonization\">Data Harmonization</a></li>\n<li><a href=\"\
    #reference-allele-fixing\">Reference allele fixing</a></li>\n<li><a href=\"#basic-qc\"\
    >Basic QC</a></li>\n<li><a href=\"#merging-inputs-optional\">Merging Inputs (Optional)</a></li>\n\
    <li><a href=\"#imputaton-preparation\">Imputation Preparation</a></li>\n</ul>\n\
    </li>\n</ul>\n\n<p><a href=\"https://github.com/pmonnahan/DataPrep/blob/master/Pipeline_DAG.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pmonnahan/DataPrep/raw/master/Pipeline_DAG.png\"\
    \ alt=\"Pipeline DAG\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<h3>\n<a id=\"\
    user-content-snakemake\" class=\"anchor\" href=\"#snakemake\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Snakemake</h3>\n\
    <p>The pipeline is coordinated and run on an HPC (or locally) using <em>Snakemake</em>.\
    \  To install snakemake, first create a virtual environment via:</p>\n<pre><code>module\
    \ load python3/3.6.3_anaconda5.0.1\nconda install -c conda-forge mamba\nmamba\
    \ create -c conda-forge -c bioconda -n &lt;your_environment_name&gt; snakemake\n\
    </code></pre>\n<p>This will create a new virtual environment and install <code>snakemake</code>.\
    \  Then, activate this environment and perform following installations:</p>\n\
    <pre><code>conda activate &lt;your_environment_name&gt;\nconda install numpy yaml\
    \ pandas\n</code></pre>\n<p>Anytime you need to run the pipeline, activate this\
    \ environment beforehand via:</p>\n<pre><code>conda activate &lt;environment_name&gt;\n\
    </code></pre>\n<p>If you choose not to create an environment, you must ensure\
    \ that these packages are installed and available for your python installation.</p>\n\
    <h3>\n<a id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity</h3>\n<p>The installation of the individual programs \
    \ used throughout this pipeline can be completely avoid by utilizing a Singularity\
    \ image.  This image is too large to be hosted on Github, although you can find\
    \ the definitions file used to create the image <a href=\"https://github.com/pmonnahan/AncInf/blob/master/singularity/Singularity_defs.def\"\
    >here</a>.  Building of images is still not currently supported at MSI, so I used\
    \ a Vagrant virtual machine, which comes with Singularity pre-configured/installed\
    \ (<a href=\"https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\"\
    \ rel=\"nofollow\">https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4</a>).\
    \  I can also share the img file directly upon request.</p>\n<p>However, in order\
    \ to utilize the singularity image, <em>Singularity</em> must be installed on\
    \ the HPC.  Currently, the pipeline assumes that <em>Singularity</em> will be\
    \ available as a module and can be loaded into the environment via the command\
    \ specified in the config.yml file, where it says 'singularity_module'.  The default\
    \ setting will work for MSI at UMN.</p>\n<p>Singularity settings in config.yml</p>\n\
    <pre><code>singularity:\n  use_singularity: 'true'\n  image: '/home/pmonnaha/pmonnaha/singularity/AncestryInference.sif\n\
    </code></pre>\n<h2>\n<a id=\"user-content-running-the-workflow\" class=\"anchor\"\
    \ href=\"#running-the-workflow\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Running the workflow</h2>\n<p>First,\
    \ activate the virtual environment into which snakemake was installed:</p>\n<pre><code>conda\
    \ activate &lt;environment_name&gt;\n</code></pre>\n<p>Clone the parent repository\
    \ to the location where you want to store the output of the pipeline.</p>\n<pre><code>git\
    \ clone https://github.com/pmonnahan/DataPrep.git preImputeQC\ncd preImputeQC\n\
    </code></pre>\n<p>The critical files responsible for executing the pipeline are\
    \ contained in the <em>./workflow</em> subdirectory contained within the cloned\
    \ repo.  They are:</p>\n<ul>\n<li>Snakefile</li>\n<li>config.yml</li>\n<li>cluster.yaml</li>\n\
    </ul>\n<p>The <em>Snakefile</em> is the primary workhouse of snakemake, which\
    \ specifies the dependencies of various parts of the pipeline and coordinates\
    \ execution.  No modifications to the <em>Snakefile</em> are necessary.</p>\n\
    <p>In order for the <em>Snakefile</em> to locate all of the necessary input and\
    \ correctly submit jobs to the cluster, <strong>both</strong> the <em>config.yaml</em>\
    \ and <em>cluster.yaml</em> need to be modified. Open these files and change the\
    \ required entries that are indicated with 'MODIFY'.  Other fields do not require\
    \ modification, although this may be desired given the particulars of the run\
    \ you wish to implement.  Details on each entry in the config file (e.g. what\
    \ the program expects in each entry as well as the purpose of the entry) are provided\
    \ in the <em>Pipeline Overview</em> at the bottom.  Note: Only use letters and\
    \ numbers when naming output files or datasets as this may cause issues with the\
    \ report creation.</p>\n<p>The entire pipeline can be executed on a local machine\
    \ (not recommended) or on an HPC, and the <em>cluster.yaml</em> file is required\
    \ only for the latter.  For a local run, change the <code>local_run</code> entry\
    \ to <code>true</code> under the <code>run_settings</code> section of the config\
    \ file, and launch snakemake from within the parent directory by the simple command:</p>\n\
    <pre><code>snakemake\n</code></pre>\n<p>However, multiple steps in the pipeline\
    \ have high resource demands, and so are unlikely to be able to be run locally.\
    \  This option exists primarily for testing and troubleshooting, so the remainder\
    \ of the  documentation assumes that the pipeline will be executed on an HPC.\
    \  In order to coordinate the use of the HPC, the following modifications to the\
    \ snakemake command are required:</p>\n<pre><code>snakemake --cluster \"sbatch\
    \ --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 32\n</code></pre>\n<p>where -j specifies the number of jobs that can be submitted\
    \ at once.</p>\n<p>One additional setting in the <em>config.yml</em> is needed\
    \ in order to correctly submit jobs to the HPC.  The relevant entries are under\
    \ the <code>run_settings</code> section of the config file:</p>\n<pre><code>run_settings:\n\
    \  local_run: 'false'\n  cluster_config: 'workflow/cluster_slurm.yaml'\n  scheduler:\
    \ 'slurm'\n</code></pre>\n<p>Here, it is necessary that the <code>cluster_config</code>\
    \ entry is set to the path of the cluster_slurm.yaml file that will be used in\
    \ the snakemake command.  Also, the scheduler must correspond to the syntax used\
    \ in the snakemake command and cluster.yaml file.  I should point out that these\
    \ additional changes are needed for responsibly using PLINK within a snakemake\
    \ framework, and are not directly needed for snakemake.  PLINK will attempt to\
    \ auto-detect available resources upon running regardless of the resources that\
    \ were requested when the job was submitted.  Therefore, we have to read and parse\
    \ the requested resources in the cluster config file in order for them to be communicated\
    \ to PLINK from within the Snakefile.</p>\n<h3>\n<a id=\"user-content-other-notes\"\
    \ class=\"anchor\" href=\"#other-notes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Other notes</h3>\n<p>It is recommended\
    \ that <em>snakemake</em> is run as an interactive session on an HPC.  <em>Snakemake</em>\
    \ will launch the specified number (via the -j flag) of jobs, and then will hang\
    \ and wait for them to finish.  As jobs finish (and assuming no errors), <em>snakemake</em>\
    \ will launch additional jobs keeping the total running jobs at whatever -j is\
    \ set for.  Although <em>snakemake</em> should not use a lot of memory, it could\
    \ have long run times, which is generally not advisable on login nodes.</p>\n\
    <p>One attractive feature of <em>snakemake</em> is its ability to keep track of\
    \ the progress and dependencies of the different stages of the pipeline.  Specifically,\
    \ if an error is encountered or the pipeline otherwise stops before the final\
    \ step, <em>snakemake</em> can resume the pipeline where it left off, avoiding\
    \ redundant computation for previously completed tasks.  To do so, simply resubmit\
    \ the original <em>snakemake</em> command.</p>\n<p>To run a specific part of the\
    \ pipeline, do:</p>\n<pre><code>snakemake -R &lt;rule_name&gt; --cluster \"sbatch\
    \ --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 20 --rerun-incomplete\n</code></pre>\n<p>where <em>rule_name</em> indicates\
    \ the 'rule' (i.e. job) in the Snakefile that you wish to run.  Or, you can request\
    \ a specific file by providing the filename at the end of the command.  You may\
    \ need to include the -F (i.e. force) if the output file already exists and you\
    \ want to overwrite it.</p>\n<p>Also, it is often very helpful to do a 'dry-run'\
    \ of the pipeline in which the different steps and dependencies are printed to\
    \ screen, but no actual jobs are executed.  This can be helpful to ensure that\
    \ config entries are correct, etc.  To perform a dry-run, do:</p>\n<pre><code>snakemake\
    \ -nrp\n</code></pre>\n<p>NOTE: It is convenient to make an alias in your ~/.bashrc\
    \ file to run snakemake on the cluster without having to type the --cluster...\
    \ part of the command every time.  For me, it looked like this:</p>\n<pre><code>alias\
    \ snakeslurm=\"snakemake -k --cluster 'sbatch --no-requeue --partition={cluster.p}\
    \ --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --job-name={cluster.job-name}\
    \ --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}' --cluster-config workflow/cluster_slurm.yaml\"\
    \n</code></pre>\n<p>This way, I can just do:</p>\n<pre><code>snakeslurm -j 25\n\
    </code></pre>\n<p>To launch snakemake on the cluster.</p>\n<h4>\n<a id=\"user-content-debugging-and-error-reports\"\
    \ class=\"anchor\" href=\"#debugging-and-error-reports\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Debugging\
    \ and error reports</h4>\n<p>Should an error be encountered in a job, snakemake\
    \ will halt the pipeline and indicate in the terminal that an error has occurred.\
    \  The offending job will also be printed in red in the terminal window.  More\
    \ information on why the job failed can be found in the 'stdout' and 'stderr'\
    \ files that are output to the <em>'OandE'</em> directory and will be labelled\
    \ with the jobname.</p>\n<h2>\n<a id=\"user-content-pipeline-overview\" class=\"\
    anchor\" href=\"#pipeline-overview\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Overview</h2>\n<h3>\n\
    <a id=\"user-content-input-data\" class=\"anchor\" href=\"#input-data\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Input\
    \ Data</h3>\n<p>Under the 'query' section, you can specify the inputs for one\
    \ or more datasets.  Each dataset should be uniquely named (Note: avoid using\
    \ periods or underscores when naming output files or datasets as this may cause\
    \ issues with the report creation.) with values specified for the following \"\
    keys\":</p>\n<ul>\n<li>\n<strong>data</strong>: path to the PLINK files (just\
    \ the PLINK prefix).</li>\n<li>\n<strong>chrom_key</strong>: tab-delimited text\
    \ file with 2 columns (no header).  The first column contains the old chromosome\
    \ names, and the second column contains the new names.\n<ul>\n<li>Used for converting\
    \ to numeric names. e.g chr10 to 10.</li>\n</ul>\n</li>\n<li>\n<strong>allele_key</strong>:\
    \ tab-delimited text file with 5 columns (no header).  First column is snpID and\
    \ following columns are: old_allele1 old_allele2 new_allele1 new_allele2.\n<ul>\n\
    <li>Used for converting alleles with A/B specification to ACGT.  Oftentimes provided\
    \ in the dbGaP download.  If alleles are already specified in ACGT format, this\
    \ field can be set to 'none'</li>\n</ul>\n</li>\n<li>\n<strong>ID_key</strong>:\
    \ tab-delimited text file with 2 columns (no header).  First column is old SNP\
    \ ID and second column is new SNP ID.\n<ul>\n<li>Used for converting to rsID format.\
    \  If SNP IDs are already in rs-format, this field can be set to 'none'</li>\n\
    </ul>\n</li>\n<li>\n<strong>flip_key</strong>: text file with single column containing\
    \ SNP rsIDs that need to be flipped in order to align strand to the hg19 reference\
    \ genome.\n<ul>\n<li>Used to harmonize strand across datasets to the hg19 reference\
    \ genome.  Set this field to 'none' if all alleles are already on the same strand\
    \ as the target reference genome.</li>\n</ul>\n</li>\n</ul>\n<p>Each of these\
    \ fields are optional and providing 'none' as the entry will disable the steps\
    \ associated with each key.  However, these fields should only be set to 'none'\
    \ if you are sure that they are not necessary (e.g. you have already fixed any\
    \ existing strand issues across datasets).</p>\n<p>Example of input specifications\
    \ in the config file:</p>\n<pre><code>query:\n  \"dataset1\":\n    data: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\
    \n    chrom_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    allele_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\
    \n    ID_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    flip_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\
    \n  \"dataset2\":\n    data: \"PATH/TO/PLINK/PREFIX/FOR/DATASET2\"\n    chrom_key:\
    \ \"none\"\n    allele_key: \"none\"\n    ID_key: \"none\"\n    flip_key: \"none\"\
    \n</code></pre>\n<p>Phenotypes of the samples must be specified by a tab-delimited\
    \ text file where the first column contains the sample IDs (as they appear in\
    \ the imputed VCF file) and the second column contains the phenotype. The path\
    \ to this file can be provided in the field labelled 'phenotype_file' under the\
    \ 'phenotype_data' field in the config.yml file.</p>\n<p>Sex of the samples must\
    \ also be specified in a tab-delimited text file where the first column is sample\
    \ ID and the second column is the sex specification according to PLINK.  The path\
    \ to this file can be provided in the field labelled 'sex_file' under the 'phenotype_data'\
    \ field in the config.yml file.</p>\n<pre><code>phenotype_data: \n  pheno_file:\
    \ \"none\"\n  sex_file: \"/path/to/sex/file\"\n</code></pre>\n<h3>\n<a id=\"user-content-output\"\
    \ class=\"anchor\" href=\"#output\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Output</h3>\n<p>The output is\
    \ a set of PLINK files in the parent directory labelled with the value provided\
    \ in the 'outname' entry of the config file.  However, if 'merge' is set to 'false'\
    \ in the config file, this final merge step is skipped, and the final output would\
    \ be the set of QC'ed plink files within each subdirectory labelled with the dataset\
    \ names.  Within each of these subdirectories, there will also be a set of VCF\
    \ files, which are suitable for use in either the Michigan or TOPMed imputation\
    \ servers.</p>\n<p>The other primary output is a PDF report containing a summary\
    \ of various steps in the pipeline.  It is <strong>highly recommended</strong>\
    \ that the user carefully review this report to confirm that everything seems\
    \ in order.  Particular attention should be paid to whether specific steps have\
    \ resulted in major loss of markers as well as whether there is a positive correlation\
    \ between allele frequencies in the 1000Genomes dataset and allele frequencies\
    \ in each of the query datasets.  These scatter plots are provided towards the\
    \ end of the report, and if a substantial subset of the points exhibit an anti-correlation,\
    \ this is indicative of a preponderance of strand errors that ought to be corrected\
    \ (via the 'flip_key') prior to proceeding.</p>\n<h3>\n<a id=\"user-content-dataset-harmonization\"\
    \ class=\"anchor\" href=\"#dataset-harmonization\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Dataset harmonization</h3>\n\
    <p>The first step(s) in the pipeline aims to harmonize the naming of chromosomes,\
    \ alleles, and variant IDs.  This is accomplished via the 4 keys described above.\
    \  While this pipeline generally attempts to simplify the QC process, it is extremely\
    \ important that the user is acquainted well enough with each individual dataset\
    \ to ensure that the appropriate keys are specified (or not specified).</p>\n\
    <h3>\n<a id=\"user-content-reference-allele-fixing\" class=\"anchor\" href=\"\
    #reference-allele-fixing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Reference allele fixing</h3>\n<p>In contrast\
    \ to a VCF, where alleles are specified with respect to a specified reference\
    \ genome (reference versus alternative alleles), PLINK-formatted files often specify\
    \ alleles as major/minor alleles based on the frequency in the dataset.  Furthermore,\
    \ many commonly used arrays will contain a mixture of SNPs genotyped on either\
    \ the + or - strand.  Lastly, the default behavior of PLINK is to automatically\
    \ set the minor to A1 and the major allele to A2, which can unintentionally generate\
    \ inconsistencies in allele specifications across datasets.</p>\n<p>With respect\
    \ to a reference genome, two possible types of errors can occur:</p>\n<ul>\n<li>Flipped\
    \ strand:  The genotype is specified with respect to the opposite strand relative\
    \ to the reference genome.</li>\n<li>Swapped allele:  The genotype is specified\
    \ on the same strand as the reference genome, but the A1 (minor) allele has been\
    \ set to equal the 'reference' allele when it ought to be set to equal the non-reference/'alternative'\
    \ allele</li>\n</ul>\n<p>To identify these errors, we use the bcftools plugin\
    \ '+fixref', which requires not only the reference sequence (fasta) file, but\
    \ also a VCF file containing variant sites that are used to identify mismatching\
    \ alleles in the query dataset.  Importantly, if the program determines that no\
    \ strand issues exist and that the reference/alternative alleles have simply been\
    \ swapped, then program will swap the major/minor alleles to match the reference.\
    \  It will not perform any strand flipping, where it converts genotypes to be\
    \ specified with respect to the nucleotide on the opposite strand.  Although the\
    \ program will attempt to identify these strand flips, it doesn't make the correction\
    \ as the authors consider this a risky move that should not be handled in an automated\
    \ fashion.  Thus, flip-strand mismatches are ultimately removed.  If there are\
    \ a large number of these, the user should attempt to understand and resolve the\
    \ source of the issue and rerun this pipeline.</p>\n<p>By default, the pipeline\
    \ will download the following files for the hg19 reference genome:</p>\n<p>Reference\
    \ fasta:\n<a href=\"ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz\"\
    \ rel=\"nofollow\">ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz</a></p>\n\
    <p>Reference VCF (1000Genomes):\n<a href=\"ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz\"\
    \ rel=\"nofollow\">ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz</a></p>\n\
    <p>An indication of whether alleles are now specified correctly is to plot frequency\
    \ of an allele in the query population against the frequency in the reference\
    \ population and look for an obviously positive correlation.  Such plots are automatically\
    \ produced in the PDF report as the final step in the pipeline.</p>\n<h3>\n<a\
    \ id=\"user-content-basic-qc\" class=\"anchor\" href=\"#basic-qc\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Basic\
    \ QC</h3>\n<p>After alleles have been fixed as described above, a series of basic\
    \ QC steps are performed on each dataset by the script <em>'scripts/QC.py'</em>,\
    \ with the filtering thresholds specified in the config file (see below).</p>\n\
    <pre><code>perform_QC: 'true'\nQC:\n  vm1: \"0.2\" # Initial variant missingness\
    \ filter\n  gm: \"0.1\" # Individual missingness filter\n  vm2: \"0.05\"  # Ultimate\
    \ call rate for variants after removing low-callrate samples\n  maf: \"0.01\"\
    \  # mimimum Minor allele frequency\n  hwe: \"0.0000001\"  # p-value threshold\
    \ for whether site follows hardy-weinberg\n  mbs: \"0.0000001\"  # p-value treshold\
    \ for test of whether missingness varies by sex\n</code></pre>\n<p>We first wish\
    \ to identify and remove individual samples that show high missingess across markers\
    \ (specified by 'gm').  However, to identify these individuals, we first need\
    \ to remove variants that imputed poorly across all individuals (specified by\
    \ 'vm1').  After removing these individuals, we then remove variants with high\
    \ missingness (specified by 'vm2').  Since poor imputation will result in missing\
    \ genotypes, this missingness filter indirectly filters for low quality imputation\
    \ sites.  Variants are also filtered based whether or not they show significant\
    \ departures from Hardy-Weinberg Equilibrium ('hwe' entry) and whether there is\
    \ a significant association between missingness and sex ('mbs' entry).  We also\
    \ remove rare variants based on the 'maf' value.  Lastly, we remove indels, duplicate\
    \ SNPs, and multi-allelic variants.</p>\n<p>Note that testing for missigness by\
    \ case/control status is generally recommended as well if the user wishes to proceed\
    \ straight to SNP-based analyses such as GWAS.  However, if the data is to be\
    \ used for ancestry inference, it may make more sense to retain these SNPs.</p>\n\
    <h3>\n<a id=\"user-content-merging-inputs-optional\" class=\"anchor\" href=\"\
    #merging-inputs-optional\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Merging inputs (Optional)</h3>\n<p>If multiple\
    \ input datasets were provided, an optional final step is to create a single merged\
    \ dataset consisting of only the sites that overlap (i.e. passed filters) across\
    \ all component datasets.  This behavior is controlled by the 'merge' entry in\
    \ the config file.  To enable the merging behavior, set this to:</p>\n<pre><code>merge:\
    \ 'true'\n</code></pre>\n<h3>\n<a id=\"user-content-imputaton-preparation\" class=\"\
    anchor\" href=\"#imputaton-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Imputaton preparation</h3>\n\
    <p>Another optional, final feature is to create a set of of VCF files (parsed\
    \ by chromosome) for each of the input datasets.  These VCFs can be used directly\
    \ as input into either the Michigan Imputation Server or the TOPMed Imputation\
    \ Server.  The output of the imputation servers can then be used as input into\
    \ the post-imputation QC pipeline (see README.md in the 'postImpute' directory).</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1617574369.0
powellgenomicslab/SingularityBaseImages:
  data_format: 2
  description: Singularity base images that will be build on singularity hub and can
    be used to build other images
  filenames:
  - Singularity.TxnDoubletDetection
  - Singularity.DemultiplexingSoftwares
  - Singularity.AllSoftwares
  - Singularity.R4_python368
  - Singularity.Anne_demultiplexing_test
  - Singularity.R363_python368
  - Singularity.DoubletDetection
  full_name: powellgenomicslab/SingularityBaseImages
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularitybaseimages" class="anchor" href="#singularitybaseimages"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SingularityBaseImages</h1>

    <p>A repo for singularity images. This is linked to singularity hub and all results
    can be pulled from there.</p>

    <h2>

    <a id="user-content-singularity-hub-images" class="anchor" href="#singularity-hub-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Hub Images</h2>

    <ul>

    <li>

    <p>Singularity.R363_python368</p>

    <ul>

    <li>To pull: <code>singularity pull shub://drneavin/SingularityBaseImages:r363_python368</code>

    </li>

    <li>Contains:

    <ul>

    <li>R 3.6.3</li>

    <li>python 3.6.8</li>

    <li>conda</li>

    <li>Some basic R packages (see the definition file to see all installed)</li>

    <li>Some basic python package (see the definition file to see all installed)</li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Singularity.R4_python368</p>

    <ul>

    <li>To pull: <code>singularity pull shub://drneavin/SingularityBaseImages:r4_python368</code>

    </li>

    <li>Contains:

    <ul>

    <li>R 4.0.3</li>

    <li>python 3.6.8</li>

    <li>conda</li>

    <li>Some basic R packages (see the definition file to see all installed)</li>

    <li>Some basic python package (see the definition file to see all installed)</li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Singularity.TxnDoubletDetection</p>

    <ul>

    <li>To pull: <code>singularity pull shub://drneavin/SingularityBaseImages:txndoubletdetection</code>

    </li>

    <li>Built on top of <code>Singularity.R4_python368</code> image</li>

    <li>Also contains:

    <ul>

    <li>DoubletDetection</li>

    <li>DoubletDecon</li>

    <li>DoubletFinder</li>

    <li>scds</li>

    <li>scrublet</li>

    <li>scDoubletFinder</li>

    <li>solo</li>

    </ul>

    </li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1611115640.0
powerPlant/3d-dna-srf:
  data_format: 2
  description: Singularity recipe files for 3D DNA (https://github.com/theaidenlab/3d-dna)
  filenames:
  - Singularity
  - Singularity.180922
  full_name: powerPlant/3d-dna-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2286" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the 3D de novo assembly (3D DNA) pipeline</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549335251.0
powerPlant/angsd-srf:
  data_format: 2
  description: Singularity recipe files for angsd (https://github.com/ANGSD/angsd)
  filenames:
  - Singularity.0.923
  - Singularity
  - Singularity.0.925
  - Singularity.0.919
  - Singularity.0.917
  - Singularity.0.921
  - Singularity.0.922
  - Singularity.0.918
  full_name: powerPlant/angsd-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2300" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the angsd program for analysing NGS data</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549507443.0
powerPlant/apsim-srf:
  data_format: 2
  description: Singularity recipe files for APSIM Classic (https://github.com/APSIMInitiative/APSIMClassic)
  filenames:
  - Singularity
  - Singularity.7.9-r4047
  - Singularity.7.10-r49ace54f9c8a670190aef9d8d0fb9d5477bb1534
  full_name: powerPlant/apsim-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the APSIM Classic version of the Agricultural
    Production Systems sIMulator</p>

    <h2>

    <a id="user-content-maintainer-notes" class="anchor" href="#maintainer-notes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Maintainer
    Notes</h2>

    <ul>

    <li>Recipes for APSIM 7.9 use the upstream SVN repository (no longer available)</li>

    <li>Please see comments inside the recipes for the reasons why some upstream files
    are overwritten during the build process</li>

    <li>The Cotton Model requires a password, which needs to be obtained by the model
    owner and placed under <code>files/CottonPassword.txt</code>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1586904956.0
powerPlant/apsimx-srf:
  data_format: 2
  description: Singularity recipe files for ApsimX (https://github.com/APSIMInitiative/ApsimX)
  filenames:
  - Singularity.2020.10.21.5755
  - Singularity.2020.11.27.5887
  - Singularity.2019.06.05.3920
  - Singularity
  - Singularity.2021.04.15.6139
  - Singularity.2020.04.09.5012
  - Singularity.2019.10.04.4236
  - Singularity.2020.09.17.5665
  - Singularity.2018.09.28.3099
  - Singularity.2019.01.08.3392
  - Singularity.2020.08.04.5350
  - Singularity.2019.01.30.3436
  - Singularity.2018.01.30.2253
  - Singularity.2019.04.03.3693
  - Singularity.2019.07.18.4025
  full_name: powerPlant/apsimx-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2271" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for ApsimX, the next generation of the Agricultural
    Production Systems sIMulator (APSIM)</p>

    <h2>

    <a id="user-content-maintainer-notes" class="anchor" href="#maintainer-notes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Maintainer
    notes</h2>

    <p>This container downloads the ApsimX <code>deb</code> file built by the <a href="https://github.com/APSIMInitiative/APSIM.Builds">BOB
    Build Service</a> at CSIRO and installs it on a Ubuntu:Bionic container.</p>

    <p>There are two useful endpoints from the Build Service:</p>

    <ul>

    <li>

    <a href="http://apsimdev.apsim.info/APSIM.Builds.Service/Builds.svc/GetLatestVersion"
    rel="nofollow">Get Latest Version</a>: shows the full version number of the latest
    release</li>

    <li>

    <a href="http://apsimdev.apsim.info/APSIM.Builds.Service/Builds.svc/GetURLOfLatestVersion?operatingSystem=Debian"
    rel="nofollow">Get URL of Latest Version</a>: shows the download URL for the latest
    version of the specified OS</li>

    </ul>

    <p><code>ApsimSetup*.deb</code> files are named using the <a href="https://github.com/APSIMInitiative/APSIM.Builds/blob/21a8ac85a1d868a45f60a994a35a5d07dc04562a/APSIM.Builds.Service/Builds.svc.cs#L278">build.issueNumber</a>
    instead of the full version number, so we must hardcode the URLs inside the recipe
    files, while using the full version number for the recipe file names.</p>

    <p>To facilitate running the container, a <code>/usr/local/bin/apsimmodels</code>
    file is created as the entrypoint using <code>/usr/local/bin/apsim</code> as a
    template.</p>

    <h2>

    <a id="user-content-container-specific-notes" class="anchor" href="#container-specific-notes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container-specific
    notes</h2>

    <ul>

    <li>ApsimX apparently needs <code>/etc/localtime</code> to run so we have to install
    <code>tzdata</code> as a dependency. We configure our local timezone, but this
    can be overridden at run time by exporting the <code>TZ</code> environment variable.</li>

    </ul>

    '
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1618541722.0
powerPlant/aws-cli-srf:
  data_format: 2
  description: Singularity recipe files for aws-cli (https://github.com/aws/aws-cli)
  filenames:
  - Singularity
  - Singularity.2.0.43
  full_name: powerPlant/aws-cli-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the AWS CLI v2 tool</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1598486009.0
powerPlant/bedops-srf:
  data_format: 2
  description: Singularity recipe files for bedops (https://github.com/bedops/bedops)
  filenames:
  - Singularity
  - Singularity.2.4.39
  full_name: powerPlant/bedops-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the BEDOPS open-source command-line toolkit
    that performs highly efficient and scalable Boolean and other set operations,
    statistical calculations, archiving, conversion and other management of genomic
    data of arbitrary scale.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1596773368.0
powerPlant/bismark-srf:
  data_format: 2
  description: Singularity recipe files for Bismark (https://github.com/FelixKrueger/Bismark)
  filenames:
  - Singularity
  - Singularity.0.20.0
  - Singularity.0.23.0
  - Singularity.0.19.1
  full_name: powerPlant/bismark-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2263" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Bismark bisulfite mapping and methylation
    calling program</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1623122485.0
powerPlant/bonito-srf:
  data_format: 2
  description: Singularity recipe files for bonito (https://github.com/nanoporetech/bonito)
  filenames:
  - Singularity.0.3.6
  - Singularity
  - Singularity.0.4.0
  full_name: powerPlant/bonito-srf
  latest_release: null
  readme: '<p>Singularity recipe files for bonito, a PyTorch Basecaller for Oxford
    Nanopore Reads

    <a href="https://github.com/nanoporetech/bonito">https://github.com/nanoporetech/bonito</a></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1623211756.0
powerPlant/busco-srf:
  data_format: 2
  description: Singularity recipe files for busco (https://gitlab.com/ezlab/busco)
  filenames:
  - Singularity.4.1.0
  - Singularity.4.1.1
  - Singularity.5.1.2
  - Singularity
  - Singularity.4.0.6
  - Singularity.4.0.4
  - Singularity.4.0.2
  - Singularity.4.0.5
  - Singularity.4.1.4
  - Singularity.4.1.2
  - Singularity.4.0.1
  - Singularity.4.0.0
  full_name: powerPlant/busco-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the BUSCO tool for Benchmarking Universal
    Single-Copy Ortholog assessment</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618269021.0
powerPlant/cdo-srf:
  data_format: 2
  description: Singularity recipe files for cdo (https://www.mpimet.mpg.de/cdo/)
  filenames:
  - Singularity
  - Singularity.1.7.0
  - Singularity.1.9.5
  - Singularity.1.9.3
  full_name: powerPlant/cdo-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2262" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Climate Data Operators toolset</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549335527.0
powerPlant/checkm-srf:
  data_format: 2
  description: Singularity recipe files for checkm (http://ecogenomics.github.io/CheckM)
  filenames:
  - Singularity.1.0.13
  - Singularity.1.0.11
  - Singularity
  - Singularity.1.1.3
  - Singularity.1.0.12
  - Singularity.1.0.10
  - Singularity.1.0.7
  - Singularity.1.0.8
  full_name: powerPlant/checkm-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2464" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the CheckM set of tools for assessing the quality
    of genomes recovered from isolates, single cells, or metagenomes</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1598504920.0
powerPlant/crema-srf:
  data_format: 2
  description: Singularity recipe files for crema (https://github.com/gbgolding/crema)
  filenames:
  - Singularity
  - Singularity.fe4cf7a
  full_name: powerPlant/crema-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2320" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the crema tool to classify RNAs by Ensemble Machine
    learning Algorithms</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550200930.0
powerPlant/diamond-srf:
  data_format: 2
  description: Singularity recipe files for DIAMOND (https://github.com/bbuchfink/diamond)
  filenames:
  - Singularity.v0.9.21
  - Singularity
  - Singularity.v0.9.20
  - Singularity.v0.9.22
  - Singularity.v0.9.17
  - Singularity.v0.9.15
  - Singularity.v0.9.19
  - Singularity.v0.9.18
  - Singularity.v0.9.24
  - Singularity.v0.9.16
  - Singularity.v0.9.23
  full_name: powerPlant/diamond-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2322" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the DIAMOND Accelerated BLAST compatible local
    sequence aligner</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549857110.0
powerPlant/eddypro-engine-srf:
  data_format: 2
  description: Singularity recipe files for EddyPro Engine (https://github.com/LI-COR/eddypro-engine)
  filenames:
  - Singularity
  - Singularity.5.2.1
  - Singularity.6.2.1
  - Singularity.5.1.1
  - Singularity.6.0.0
  - Singularity.6.1.0
  - Singularity.6.2.0
  - Singularity.5.2.0
  full_name: powerPlant/eddypro-engine-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2272" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the EddyPro eddy covariance data processing software</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549923689.0
powerPlant/edta-srf:
  data_format: 2
  description: Singularity recipe files for edta (https://github.com/oushujun/EDTA)
  filenames:
  - Singularity
  - Singularity.1.9.0
  - Singularity.1.8.3
  full_name: powerPlant/edta-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the Extensive de novo TE Annotator tool</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1603071842.0
powerPlant/entrez-direct-srf:
  data_format: 2
  description: Singularity recipe files for entrez-direct (https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/)
  filenames:
  - Singularity
  - Singularity.13.8.20200819
  full_name: powerPlant/entrez-direct-srf
  latest_release: null
  readme: '<p>Singularity recipe files for Entrez Direct: E-utilities on the Unix
    Command Line to provide access to the NCBI''s suite of interconnected databases</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1598244762.0
powerPlant/getorganelle-srf:
  data_format: 2
  description: Singularity recipe files for getorganelle (https://github.com/Kinggerm/GetOrganelle)
  filenames:
  - Singularity
  - Singularity.v1.6.2e
  full_name: powerPlant/getorganelle-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the GetOrganelle toolkit to assembly organelle
    genomes from genome skimming data</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1579837325.0
powerPlant/groimp-srf:
  data_format: 2
  description: Singularity recipe files for GroIMP (http://www.grogra.de/software/groimp)
  filenames:
  - Singularity
  - Singularity.1.6-jre8-cuda
  - Singularity.1.6-cuda
  full_name: powerPlant/groimp-srf
  latest_release: null
  readme: '<p>Singularity recipe files for GroIMP, a 3D-modelling platform</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583894106.0
powerPlant/gromacs-srf:
  data_format: 2
  description: Singularity recipe files for GROMACS (http://www.gromacs.org/)
  filenames:
  - Singularity.2019.2
  - Singularity.2018.3
  - Singularity.2020.2
  - Singularity.2019.4
  - Singularity.2020.1
  - Singularity.2018
  - Singularity
  - Singularity.2018.4
  - Singularity.2019.6
  - Singularity.2019.5
  - Singularity.2019.1
  - Singularity.2019.3
  - Singularity.2020
  - Singularity.2018.2
  - Singularity.2019
  - Singularity.2018.1
  - Singularity.2018.5
  full_name: powerPlant/gromacs-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2264" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the GROMACS molecular dynamics package</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1591670800.0
powerPlant/hapcol-srf:
  data_format: 2
  description: Singularity recipe files for hapcol (https://github.com/AlgoLab/HapCol)
  filenames:
  - Singularity.97d4a5e
  - Singularity
  full_name: powerPlant/hapcol-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the HapCol tool, a fast and memory-efficient
    method for haplotype assembly from long gapless reads</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1579837367.0
powerPlant/mandalorion-episode-ii-srf:
  data_format: 2
  description: Singularity recipe files for Mandalorion-Episode-II (https://github.com/rvolden/Mandalorion-Episode-II)
  filenames:
  - Singularity.6219d58
  - Singularity
  full_name: powerPlant/mandalorion-episode-ii-srf
  latest_release: null
  readme: '<p>Singularity recipe files for Mandalorion Episode II, Attack of the Isoforms</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583274107.0
powerPlant/mapgd-srf:
  data_format: 2
  description: Singularity recipe files for MAPGD (https://github.com/LynchLab/MAPGD)
  filenames:
  - Singularity
  - Singularity.0.4.38-d3edee2
  full_name: powerPlant/mapgd-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2319" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the MAPGD series of related programs for the analysis
    of low coverage population genomic data or for the analysis of pooled data</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549853299.0
powerPlant/metabat2-srf:
  data_format: 2
  description: Singularity recipe files for metabat2 (https://bitbucket.org/berkeleylab/metabat/src/master/)
  filenames:
  - Singularity
  - Singularity.2.15
  - Singularity.2.15-3-g367a7ef
  full_name: powerPlant/metabat2-srf
  latest_release: null
  readme: '<p>Singularity recipe files for MetaBAT: A robust statistical framework
    for reconstructing genomes from metagenomic data</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1618472328.0
powerPlant/mrbayes-srf:
  data_format: 2
  description: Singularity recipe files for MrBayes (http://nbisweden.github.io/MrBayes)
  filenames:
  - Singularity
  - Singularity.3.2.7a-gpu
  - Singularity.3.2.7a
  full_name: powerPlant/mrbayes-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3808" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the MrBayes program for Bayesian inference and
    model choice across a wide range of phylogenetic and evolutionary models</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1574325488.0
powerPlant/novograph-srf:
  data_format: 2
  description: Singularity recipe files for NovoGraph (https://github.com/NCBI-Hackathons/NovoGraph)
  filenames:
  - Singularity
  - Singularity.1.0.0
  full_name: powerPlant/novograph-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2342" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the NovoGraph tool to construct a genome graph
    representation of long-read-based de novo sequence assemblies</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550014659.0
powerPlant/opendronemap-srf:
  data_format: 2
  description: Singularity recipe files for OpenDroneMap (https://www.opendronemap.org/)
  filenames:
  - Singularity
  - Singularity.0.4.1
  - Singularity.0.4.0
  full_name: powerPlant/opendronemap-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2266" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the OpenDroneMap Drone Mapping Software</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549336324.0
powerPlant/paml-srf:
  data_format: 2
  description: Singularity recipe files for paml (http://abacus.gene.ucl.ac.uk/software/paml.html)
  filenames:
  - Singularity
  - Singularity.4.9i
  full_name: powerPlant/paml-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3399" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PAML tool for phylogenetic analyses of DNA
    or protein sequences using maximum likelihood.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1565742033.0
powerPlant/pblat-srf:
  data_format: 2
  description: Singularity recipe files for Pblat (http://icebert.github.io/pblat/)
  filenames:
  - Singularity
  - Singularity.2.0
  - Singularity.2.1
  full_name: powerPlant/pblat-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2380" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for Pblat, the parallelized blat with multi-threads
    support</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550562816.0
powerPlant/pcl-srf:
  data_format: 2
  description: Singularity recipe files for pcl (https://github.com/PointCloudLibrary/pcl)
  filenames:
  - Singularity.1.9.1
  - Singularity
  - Singularity.1.9.0
  - Singularity.1.8.1
  full_name: powerPlant/pcl-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2329" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the pcl Point Cloud Library</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550093426.0
powerPlant/pinfish-srf:
  data_format: 2
  description: Singularity recipe files for pinfish (https://github.com/nanoporetech/pinfish)
  filenames:
  - Singularity
  - Singularity.0.1.0
  full_name: powerPlant/pinfish-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the pinfish collection of tools helping
    to make sense of long transcriptomics data (long cDNA reads, direct RNA reads)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583274123.0
powerPlant/plink2-srf:
  data_format: 2
  description: Singularity recipe files for plink2 (https://www.cog-genomics.org/plink/2.0/)
  filenames:
  - Singularity.v2.00a2LM
  - Singularity
  full_name: powerPlant/plink2-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3722" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PLINK association analysis toolset</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1572401216.0
powerPlant/portcullis-srf:
  data_format: 2
  description: Singularity recipe files for Portcullis (https://github.com/maplesond/portcullis)
  filenames:
  - Singularity
  - Singularity.1.1.0
  - Singularity.1.1.1
  - Singularity.1.1.2
  full_name: powerPlant/portcullis-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2267" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for Portcullis, a program for PORTable CULLing of
    Invalid Splice junctions from pre-aligned RNA-seq data</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549336366.0
powerPlant/qiime2-srf:
  data_format: 2
  description: Singularity recipe files for QIIME 2 (https://docs.qiime2.org/)
  filenames:
  - Singularity.2019.10
  - Singularity.2020.8
  - Singularity.2020.2
  - Singularity.2019.4
  - Singularity.2018.11
  - Singularity.2019.7
  - Singularity
  - Singularity.2019.1-picrust2
  - Singularity.2019.7-picrust2
  - Singularity.2019.1
  - Singularity.2018.2
  - Singularity.2020.6
  full_name: powerPlant/qiime2-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2268" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the QIIME 2 microbiome analysis package</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1603334435.0
powerPlant/racon-srf:
  data_format: 2
  description: Singularity recipe files for Racon (https://github.com/isovic/racon/)
  filenames:
  - Singularity.1.3.0
  - Singularity
  - Singularity.1.4.7
  - Singularity.1.3.2
  - Singularity.1.4.3
  - Singularity.1.4.2
  - Singularity.1.3.1
  - Singularity.1.3.3
  - Singularity.1.4.0
  full_name: powerPlant/racon-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2269" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Racon consensus module for raw de novo DNA
    assembly of long uncorrected reads</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1590711591.0
powerPlant/ragoo-srf:
  data_format: 2
  description: Singularity recipe files for RaGOO (https://github.com/malonge/RaGOO)
  filenames:
  - Singularity.1.01
  - Singularity.1.02
  - Singularity
  full_name: powerPlant/ragoo-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2341" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the RaGOO tool to order and orient genome assembly
    contigs via Minimap2 alignments to a reference genome</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550774761.0
powerPlant/repet-srf:
  data_format: 2
  description: Singularity recipe files for REPET (https://urgi.versailles.inra.fr/Tools/REPET)
  filenames:
  - Singularity
  - Singularity.3.0
  full_name: powerPlant/repet-srf
  latest_release: null
  readme: '<p>Singularity recipe files for REPET

    (<a href="https://urgi.versailles.inra.fr/Tools/REPET" rel="nofollow">https://urgi.versailles.inra.fr/Tools/REPET</a>),
    used to detect, annotate and

    analyse repeats in genomic sequences, specifically designed for transposable

    elements (TEs).</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602104190.0
powerPlant/sex-detector-plusplus-srf:
  data_format: 2
  description: Singularity recipe files for sex-detector-plusplus (https://gitlab.in2p3.fr/sex-det-family/sex-detector-plusplus)
  filenames:
  - Singularity
  - Singularity.00f7d723
  full_name: powerPlant/sex-detector-plusplus-srf
  latest_release: null
  readme: '<p>Singularity recipe files for SEX-DETector, a tool for the statistical
    inferrence of sex-linked genes from RNA / DNA reads from a cross (parents and
    set of childrens)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1600917082.0
powerPlant/sga-srf:
  data_format: 2
  description: Singularity recipe files for sga (https://github.com/jts/sga)
  filenames:
  - Singularity
  - Singularity.0.10.15
  full_name: powerPlant/sga-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3984" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the SGA tool, a de novo genome assembler based
    on the concept of string graphs</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1579231330.0
powerPlant/slim-srf:
  data_format: 2
  description: Singularity recipe files for slim (https://github.com/MesserLab/SLiM)
  filenames:
  - Singularity
  - Singularity.3.4+1c85d00
  - Singularity.3.5
  full_name: powerPlant/slim-srf
  latest_release: null
  readme: '<p>Singularity recipe files for Selection on Linked Mutations: A forward
    population genetic simulation for studying linkage effects, such as hitchhiking,
    background selection, and Hill-Robertson interference</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607459916.0
powerPlant/sortmerna-srf:
  data_format: 2
  description: Singularity recipe files for sortmerna (https://github.com/biocore/sortmerna)
  filenames:
  - Singularity
  - Singularity.4.2.0
  - Singularity.4.3.2
  - Singularity.3.0.3
  full_name: powerPlant/sortmerna-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the SortMeRNA local sequence alignment
    tool for filtering, mapping and clustering.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618542140.0
powerPlant/spades-srf:
  data_format: 2
  description: Singularity recipe files for spades (git@github:powerPlant/spades-srf.git)
  filenames:
  - Singularity.v3.13.0
  - Singularity.v3.11.0
  - Singularity.v3.10.0
  - Singularity.cami2-submission
  - Singularity.v3.8.2
  - Singularity.template
  - Singularity.spaligner-paper
  - Singularity.v3.12.0
  - Singularity.metaplasmid-paper
  - Singularity.v3.11.1
  - Singularity.v3.8.0
  - Singularity.v3.9.0
  - Singularity.v3.8.1
  - Singularity.v3.13.1
  - Singularity.v3.14.0
  - Singularity.v3.10.1
  - Singularity.cloudspades-paper
  - Singularity.v0.5-recomb
  - templates/Singularity.template
  full_name: powerPlant/spades-srf
  latest_release: null
  readme: '<p>Singularity recipe files for spades</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1580700253.0
powerPlant/squeezemeta-srf:
  data_format: 2
  description: Singularity recipe files for SqueezeMeta (https://github.com/jtamames/SqueezeMeta)
  filenames:
  - Singularity
  - Singularity.0.4.4
  - Singularity.1.0.0-beta
  full_name: powerPlant/squeezemeta-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2930" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the SqueezeMeta fully automated metagenomics pipeline</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1557458055.0
powerPlant/stacks-srf:
  data_format: 2
  description: Singularity recipe files for Stacks (http://catchenlab.life.illinois.edu/stacks/)
  filenames:
  - Singularity
  - Singularity.2.0
  - Singularity.2.1
  - Singularity.2.2
  full_name: powerPlant/stacks-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2270" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Stacks software pipeline for building loci
    from short-read sequences</p>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1611661899.0
powerPlant/swan-srf:
  data_format: 2
  description: Singularity recipe files for SWAN (http://bitbucket.org/charade/swan)
  filenames:
  - Singularity.3516c2f
  full_name: powerPlant/swan-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2354" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the SWAN tool for SV detection</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550114140.0
powerPlant/trinityrnaseq-srf:
  data_format: 2
  description: Singularity recipe files for trinityrnaseq (https://github.com/trinityrnaseq/trinityrnaseq)
  filenames:
  - Singularity.2.9.1
  - Singularity
  - Singularity.2.8.6
  - Singularity.2.10.0
  - Singularity.2.9.0
  full_name: powerPlant/trinityrnaseq-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the Trinity RNA-Seq de novo transcriptome
    assembly</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1591576526.0
powerPlant/vg-srf:
  data_format: 2
  description: Singularity recipe files for vg (https://github.com/vgteam/vg)
  filenames:
  - Singularity.1.10.0
  - Singularity.1.13.0
  - Singularity
  - Singularity.1.12.1
  - Singularity.1.9.0
  - Singularity.1.11.0
  - Singularity.1.8.0
  - Singularity.1.12.0
  full_name: powerPlant/vg-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2311" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the vg tools for working with genome variation
    graphs</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549578706.0
pranithavangala/singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity.add_g2gtools
  - Singularity_recipev1.0
  - Singularity_recipe_R.3.4.1
  - Singularity_recipev1.0_addR.3.4.3
  - Singularity_hicpro_v1
  - Singularity_add.R_packages
  - Singularity_recipev1.R-3-4-3
  - Singularity.add_python_packages
  - Singularity_recipe0_part1
  - Singularity_recipe_MMARGE
  full_name: pranithavangala/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-multi_atlas_app" class="anchor" href="#multi_atlas_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Multi_Atlas_app</h1>

    <p>This includes everything required (except for the "full-multi-atlas" directory)
    to build a docker and corresponding singularity container for the Multi Atlas
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/multi_atlas/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://singularity-hub.org/collections/734" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Multi_Atlas_app.git

    cd Multi_Atlas_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have full-multi-atlas directory which contains atlases.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/multi_atlas

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Multi_Atlas_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1609299433.0
pscedu/singularity-abyss:
  data_format: 2
  description: Singularity recipe for ABySS
  filenames:
  - 2.1.5/Singularity
  full_name: pscedu/singularity-abyss
  latest_release: null
  readme: '<h1>

    <a id="user-content-r-training" class="anchor" href="#r-training" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>R training</h1>

    <p>Use this link to run the project in mybinder</p>

    <p><a href="https://mybinder.org/v2/gh/reisportela/R_Training/HEAD?urlpath=rstudio"
    rel="nofollow"><img src="https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667"
    alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"></a></p>

    <p>Or use the following MyBinder <a href="https://github.com/reisportela/R_plus_RStudio">repository</a>
    and clone the contents of the exercise:</p>

    <p>Direct link:</p>

    <p>RStudio: <a href="https://mybinder.org/v2/gh/reisportela/R_plus_RStudio/HEAD?urlpath=rstudio"
    rel="nofollow"><img src="https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667"
    alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"></a></p>

    <p>Go to:</p>

    <p>File &gt; New Project &gt; Version Control &gt; Git</p>

    <p>and paste the following link <code>https://github.com/reisportela/R_Training.git</code>
    in <strong>Repository URL</strong></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624580590.0
pscedu/singularity-asciinema:
  data_format: 2
  description: Singularity recipe for asciinema
  filenames:
  - 2.0.2/Singularity
  full_name: pscedu/singularity-asciinema
  latest_release: v2.0.2-r3
  readme: "<h1>\n<a id=\"user-content-singularity-asciinema\" class=\"anchor\" href=\"\
    #singularity-asciinema\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-asciinema</h1>\n<p><a href=\"https://www.travis-ci.com/icaoberg/singularity-asciinema\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7af8220113fb0c457eda3ba87d518d1a80a3bfc8eab5f60a14fac181d20e05c3/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d61736369696e656d612e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-asciinema.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for asciinema.</p>\n\
    <h2>\n<a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\"\
    \ href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-or-similar\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges (or similar)</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>asciinema</code> script</li>\n</ul>\n<p>to <code>/opt/packages/asciinema/2.0.2</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/asciinema</code>\
    \ as <code>2.0.2.lua</code>.</p>\n<hr>\n<p>Copyright \xA9 2021 Pittsburgh Supercomputing\
    \ Center. All Rights Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\"\
    \ rel=\"nofollow\">icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\"\
    >Pittsburgh Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - asciinema
  - singularity-recipe
  updated_at: 1624644150.0
pscedu/singularity-bamtools:
  data_format: 2
  description: C++ API & command-line toolkit for working with BAM data
  filenames:
  - 2.5.1/Singularity
  full_name: pscedu/singularity-bamtools
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-bamtools\" class=\"anchor\" href=\"\
    #singularity-bamtools\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-bamtools</h1>\n<p><a href=\"https://github.com/pscedu/singularity-bamtools/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-bamtools/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/pezmaster31/bamtools\">bamtools</a>.</p>\n<h2>\n\
    <a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ <code>bamtools</code> script</li>\n</ul>\n<p>to <code>/opt/packages/bamtools/2.5.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/bamtools</code>\
    \ as <code>2.5.1</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  - bamtools
  updated_at: 1624379350.0
pscedu/singularity-bat:
  data_format: 2
  description: A cat(1) clone with syntax highlighting and Git integration.
  filenames:
  - 0.17.1/Singularity
  full_name: pscedu/singularity-bat
  latest_release: v0.17.1
  readme: "<p><a href=\"https://github.com/pscedu/singularity-bat/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-bat/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/80ce266ea551486e532b8479474ece87011121c1b177e0e067f4d8022ad2f52c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626174\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/80ce266ea551486e532b8479474ece87011121c1b177e0e067f4d8022ad2f52c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626174\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bat\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/b6d53daf33b347fa4d1a74ab0a30fd631965db08d1237e10991c7e7230ed671f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626174\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b6d53daf33b347fa4d1a74ab0a30fd631965db08d1237e10991c7e7230ed671f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626174\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bat\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/d23d5a1f67b897b9eeaf3e805efa8c4c8562272babd67bc8d1da9c8ca70d6259/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626174\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/d23d5a1f67b897b9eeaf3e805efa8c4c8562272babd67bc8d1da9c8ca70d6259/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626174\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bat\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/bf0a53aac9fa2abb057c22f8fb00c5d07141c61f15c27983fd073a5445c15421/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626174\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/bf0a53aac9fa2abb057c22f8fb00c5d07141c61f15c27983fd073a5445c15421/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626174\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bat\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-bat\"\
    \ class=\"anchor\" href=\"#singularity-bat\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-bat</h1>\n<p><a href=\"\
    https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/sharkdp/bat\">bat</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>bat</code> script</li>\n</ul>\n<p>to <code>/opt/packages/bat/0.17.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/bat</code>\
    \ as <code>0.17.1.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p>The <a href=\"https://www.psc.edu/biomedical-applications/\"\
    \ rel=\"nofollow\">Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\"\
    \ rel=\"nofollow\">Pittsburgh Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1624570774.0
pscedu/singularity-blast:
  data_format: 2
  description: BLAST finds regions of similarity between biological sequences.
  filenames:
  - 2.9.0/Singularity
  full_name: pscedu/singularity-blast
  latest_release: v2.9.0
  readme: "<p><a href=\"https://github.com/pscedu/singularity-blast/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-blast/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/34775a544d028af1a76f53887556e0b47d8a40289aa78e79056c5e13dcbc48e4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626c617374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/34775a544d028af1a76f53887556e0b47d8a40289aa78e79056c5e13dcbc48e4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626c617374\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-blast\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/e33eb8d62dc7df0e7a911833138fefaed18e36044c13f7a3aa53c104c1ffc719/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626c617374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e33eb8d62dc7df0e7a911833138fefaed18e36044c13f7a3aa53c104c1ffc719/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626c617374\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-blast\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/c141231c49671d4a45e13d6d79f61b30ec62be1462b950fac124cfb21cc2d206/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626c617374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c141231c49671d4a45e13d6d79f61b30ec62be1462b950fac124cfb21cc2d206/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626c617374\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-blast\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/0ec9871992e72eb86a829ac86878026892749bddbb4623030eb745093184def6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626c617374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0ec9871992e72eb86a829ac86878026892749bddbb4623030eb745093184def6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626c617374\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-blast\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-blast\"\
    \ class=\"anchor\" href=\"#singularity-blast\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-blast</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=Download\"\
    \ rel=\"nofollow\">BLAST</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the Perl scripts</li>\n</ul>\n<p>to <code>/opt/packages/blast/2.9.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/blast</code>\
    \ as <code>2.9.0.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - bioinformatics
  - singularity
  updated_at: 1624643460.0
pscedu/singularity-bowtie2:
  data_format: 2
  description: Bowtie2 is an ultrafast and memory-efficient tool for aligning sequencing
    reads to long reference sequences.
  filenames:
  - 2.4.2/Singularity
  full_name: pscedu/singularity-bowtie2
  latest_release: null
  readme: ''
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - bioinformatics
  - singularity
  updated_at: 1624338730.0
pscedu/singularity-bwa:
  data_format: 2
  description: 'BWA is a program for aligning sequencing reads against a large reference
    genome (e.g. human genome). '
  filenames:
  - 0.7.3a/Singularity
  full_name: pscedu/singularity-bwa
  latest_release: v0.7.3a
  readme: "<p><a href=\"https://github.com/pscedu/singularity-bwa/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-bwa/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/dbf644fd78a6a349b822d2b6db36a3f1ab2e4155f5d67671d27202073ac6cb6a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d627761\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/dbf644fd78a6a349b822d2b6db36a3f1ab2e4155f5d67671d27202073ac6cb6a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d627761\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bwa\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/0f4fa8203031531a1e542e55475a3668dbdc3d10e36a3ce505f5f098b465b1ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d627761\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0f4fa8203031531a1e542e55475a3668dbdc3d10e36a3ce505f5f098b465b1ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d627761\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bwa\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5c10a09139792e339dfd18dbbab5079f2bf2027194d2dcfc6796e85d1bab3b88/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d627761\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5c10a09139792e339dfd18dbbab5079f2bf2027194d2dcfc6796e85d1bab3b88/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d627761\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bwa\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a1977c9c08b068aecc0940ff8e8665b3e38fd423b0e217273d28b8ada424e70a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d627761\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a1977c9c08b068aecc0940ff8e8665b3e38fd423b0e217273d28b8ada424e70a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d627761\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bwa\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-bwa\"\
    \ class=\"anchor\" href=\"#singularity-bwa\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-bwa</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://github.com/sandialabs/Bwa\">bwa</a>.</p>\n<h2>\n\
    <a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ Perl scripts</li>\n</ul>\n<p>to <code>/opt/packages/bwa/0.7.3a</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/bwa</code>\
    \ as <code>0.7.3a.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - bioinformatics
  - singularity
  updated_at: 1624570327.0
pscedu/singularity-dust:
  data_format: 2
  description: du + rust = dust. Like du but more intuitive.
  filenames:
  - 0.5.4/Singularity
  full_name: pscedu/singularity-dust
  latest_release: null
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-dust/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-dust/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5c03a3a0c39c4bcd2fd882586df6b4c94ef095b690cc8918ff9c7f7121b699f5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5c03a3a0c39c4bcd2fd882586df6b4c94ef095b690cc8918ff9c7f7121b699f5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-dust\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/b64d777f7ab79be22d2c6a3a092aa35845fe1fdd0043d607ace41468a89fcaae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b64d777f7ab79be22d2c6a3a092aa35845fe1fdd0043d607ace41468a89fcaae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-dust\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/3896c740167ff8ef43b82e7e352cb3540ecaa2bee029033a9be9bbbd7d91575a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3896c740167ff8ef43b82e7e352cb3540ecaa2bee029033a9be9bbbd7d91575a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-dust\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/4eb12296596ab94dcb1a488179c5b541b54d2a5559c9e67868f60876e6f1b6e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4eb12296596ab94dcb1a488179c5b541b54d2a5559c9e67868f60876e6f1b6e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d64757374\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-dust\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-dust\"\
    \ class=\"anchor\" href=\"#singularity-dust\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-dust</h1>\n<p><a\
    \ href=\"https://github.com/bootandy/dust/raw/master/media/snap.png\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/bootandy/dust/raw/master/media/snap.png\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/bootandy/dust\">dust</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>dust</code> script</li>\n</ul>\n<p>to <code>/opt/packages/dust/0.5.4</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/dust</code>\
    \ as <code>0.5.4.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\
    >icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh\
    \ Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"\
    nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"\
    nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1624568686.0
pscedu/singularity-fastani:
  data_format: 2
  description: FastANI is developed for fast alignment-free computation of whole-genome
    Average Nucleotide Identity (ANI)
  filenames:
  - 1.33/Singularity
  full_name: pscedu/singularity-fastani
  latest_release: v1.3.3
  readme: "<p><a href=\"https://github.com/pscedu/singularity-fastani/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-fastani/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/8dc34ff6e6f588edb478bbdd040070223d6309ed57ff692ec669a99a4ce3c044/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8dc34ff6e6f588edb478bbdd040070223d6309ed57ff692ec669a99a4ce3c044/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-fastani\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/eee7b443f78bf774c4336377ac5dee69d66644752fb8ba1f4c38931628d44fb4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/eee7b443f78bf774c4336377ac5dee69d66644752fb8ba1f4c38931628d44fb4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-fastani\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/27c0f180ef8bc2d2094b529129fd31169bf0e10b3a965f25fb6ef0d8b8311868/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/27c0f180ef8bc2d2094b529129fd31169bf0e10b3a965f25fb6ef0d8b8311868/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-fastani\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/86bc49a261c15fbea5000ea905a561248a21175c8d281a5949cf47e66c9eae71/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/86bc49a261c15fbea5000ea905a561248a21175c8d281a5949cf47e66c9eae71/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d66617374616e69\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-fastani\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-fastani\"\
    \ class=\"anchor\" href=\"#singularity-fastani\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-fastani</h1>\n<p>Singularity\
    \ recipe for <a href=\"github.com/parbliss/fastani\">fastANI</a>.</p>\n<h2>\n\
    <a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ <code>fastANI</code> script</li>\n</ul>\n<p>to <code>/opt/packages/fastANI/1.33</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/fastANI</code>\
    \ as <code>1.33.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p>The <a href=\"https://www.psc.edu/biomedical-applications/\"\
    \ rel=\"nofollow\">Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\"\
    \ rel=\"nofollow\">Pittsburgh Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  updated_at: 1624570170.0
pscedu/singularity-flac:
  data_format: 2
  description: "FLAC (/fl\xE6k/; Free Lossless Audio Codec) is an audio coding format\
    \ for lossless compression of digital audio."
  filenames:
  - 1.3.3/Singularity
  full_name: pscedu/singularity-flac
  latest_release: null
  readme: "<p><a href=\"https://github.com/pscedu/singularity-flac/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-flac/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/37fadb899e2280d332672dd4ff8c55c77b6d1da4314ad56fb36c15142413fbda/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d666c6163\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/37fadb899e2280d332672dd4ff8c55c77b6d1da4314ad56fb36c15142413fbda/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d666c6163\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-flac\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5ccca52861f2fb4e7486645f35b923f2cbc790f1e7ed709d7422b0c9f2dc19d7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d666c6163\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5ccca52861f2fb4e7486645f35b923f2cbc790f1e7ed709d7422b0c9f2dc19d7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d666c6163\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-flac\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/e8f82abbc9bacec399c48512a0390d8b4d29091355a9ce463d889ecb16cd4775/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d666c6163\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e8f82abbc9bacec399c48512a0390d8b4d29091355a9ce463d889ecb16cd4775/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d666c6163\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-flac\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5325f2a609a18c54057940e4962347ba4f1beeef88a23a92c7149e8016cf4e13/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d666c6163\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5325f2a609a18c54057940e4962347ba4f1beeef88a23a92c7149e8016cf4e13/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d666c6163\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-flac\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-flac\"\
    \ class=\"anchor\" href=\"#singularity-flac\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-flac</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://github.com/sandialabs/flac\">flac</a>.</p>\n<h2>\n\
    <a id=\"user-content-about-this-repository\" class=\"anchor\" href=\"#about-this-repository\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>About this repository</h2>\n<p><a href=\"https://camo.githubusercontent.com/4d9e61b063851d778ce9938e3f7bb848d158ebf589dbcd68bf21c5d5eeef6d40/68747470733a2f2f6d65646961322e67697068792e636f6d2f6d656469612f31334867774773584630616947592f67697068792e6769663f6369643d6563663035653437396d61316e736b74386d786278726c323076377375656868343931687532306b6973786878636265267269643d67697068792e6769662663743d67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4d9e61b063851d778ce9938e3f7bb848d158ebf589dbcd68bf21c5d5eeef6d40/68747470733a2f2f6d65646961322e67697068792e636f6d2f6d656469612f31334867774773584630616947592f67697068792e6769663f6369643d6563663035653437396d61316e736b74386d786278726c323076377375656868343931687532306b6973786878636265267269643d67697068792e6769662663743d67\"\
    \ alt=\"DANGER\" data-canonical-src=\"https://media2.giphy.com/media/13HgwGsXF0aiGY/giphy.gif?cid=ecf05e479ma1nskt8mxbxrl20v7suehh491hu20kisxhxcbe&amp;rid=giphy.gif&amp;ct=g\"\
    \ style=\"max-width:100%;\"></a></p>\n<ul>\n<li>The purpose of this repository\
    \ is to highlight how to deploy a Singularity and Spack together.</li>\n<li>At\
    \ this moment, the workflow is expected to fail as we have not found a good solution\
    \ to deploying the images (yet).</li>\n</ul>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the Perl scripts</li>\n</ul>\n<p>to <code>/opt/packages/flac/1.3.3</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/flac</code>\
    \ as <code>1.3.3.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - utilities
  - singularity
  updated_at: 1624547157.0
pscedu/singularity-gatk:
  data_format: 2
  description: Singularity recipe for GATK
  filenames:
  - 4.1.9.0/Singularity
  full_name: pscedu/singularity-gatk
  latest_release: null
  readme: "<p><a href=\"https://github.com/pscedu/singularity-gatk/actions/workflows/main.yml/badge.svg\"\
    >Status</a>\n<a href=\"https://camo.githubusercontent.com/93c4f349e621f15ffd8933b4c7d4ea8eea7b6f9156528a6b483b1b47d8064a91/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6761746b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/93c4f349e621f15ffd8933b4c7d4ea8eea7b6f9156528a6b483b1b47d8064a91/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6761746b\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-gatk\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/744613aa898037bbfe237a7943e118e4fe72355964b8726f785c33e44de131e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6761746b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/744613aa898037bbfe237a7943e118e4fe72355964b8726f785c33e44de131e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6761746b\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-gatk\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5d9f7edad1535dfc343a82ee05a1cee751f4185de5e88e9959f1e306baf3af56/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6761746b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5d9f7edad1535dfc343a82ee05a1cee751f4185de5e88e9959f1e306baf3af56/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6761746b\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-gatk\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/936434bf1d55721ba57e95e4bb99cfb8b78d330106b7ffebafb8911c75556071/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6761746b\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/936434bf1d55721ba57e95e4bb99cfb8b78d330106b7ffebafb8911c75556071/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6761746b\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-gatk\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-gatk\"\
    \ class=\"anchor\" href=\"#singularity-gatk\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-gatk</h1>\n<p><a\
    \ href=\"https://camo.githubusercontent.com/f0a6e98fde6f4e7dd338612de8a154b1169c4572acc8ca3ffed117a81e28d4be/68747470733a2f2f7468656d652e7a646173736574732e636f6d2f7468656d655f6173736574732f323337383336302f646630383566313534333231666161633931353964646135376635303130336238376134663734332e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f0a6e98fde6f4e7dd338612de8a154b1169c4572acc8ca3ffed117a81e28d4be/68747470733a2f2f7468656d652e7a646173736574732e636f6d2f7468656d655f6173736574732f323337383336302f646630383566313534333231666161633931353964646135376635303130336238376134663734332e706e67\"\
    \ alt=\"Logo\" data-canonical-src=\"https://theme.zdassets.com/theme_assets/2378360/df085f154321faac9159dda57f50103b87a4f743.png\"\
    \ style=\"max-width:100%;\"></a>\nSingularity recipe for <a href=\"https://gatk.broadinstitute.org/hc/en-us\"\
    \ rel=\"nofollow\">GATK</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>gatk</code> script</li>\n</ul>\n<p>to <code>/opt/packages/gatk/4.1.9.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/gatk</code>\
    \ as <code>4.1.9.0.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p>The <a href=\"https://www.psc.edu/biomedical-applications/\"\
    \ rel=\"nofollow\">Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\"\
    \ rel=\"nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  updated_at: 1624651042.0
pscedu/singularity-gent:
  data_format: 2
  description: 'This program computes the cross entropy for groups of sequences  that
    have been assigned to groups on the basis of biochemical,  physiological, or other
    biological property. '
  filenames:
  - 1.0.0/Singularity
  full_name: pscedu/singularity-gent
  latest_release: v1.0.0
  readme: "<p><a href=\"https://github.com/pscedu/singularity-gent/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-gent/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/0bc7e2953fe196a794842a90c0c691e61aae4d46a38b5ea94f97be5354c5563e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d67656e74\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0bc7e2953fe196a794842a90c0c691e61aae4d46a38b5ea94f97be5354c5563e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d67656e74\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-gent\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/736a01217414593b006aba14bc9a1c3d29361075a38dea7b579f6297408854ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d67656e74\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/736a01217414593b006aba14bc9a1c3d29361075a38dea7b579f6297408854ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d67656e74\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-gent\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/258cd3bbde4ace9b70ffb87058e2ec74b2af329db10c4be6571e9947e38b92b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d67656e74\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/258cd3bbde4ace9b70ffb87058e2ec74b2af329db10c4be6571e9947e38b92b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d67656e74\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-gent\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/9bd43b6e0fd124c9db72f1eae2f35f9f1a11833efa211af988ce1d994bf3b481/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d67656e74\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9bd43b6e0fd124c9db72f1eae2f35f9f1a11833efa211af988ce1d994bf3b481/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d67656e74\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-gent\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-gent\"\
    \ class=\"anchor\" href=\"#singularity-gent\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-gent</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://github.com/icaoberg/gent\">GeNT</a>.</p>\n<h2>\n\
    <a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"\
    #building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p>The <a href=\"https://www.psc.edu/biomedical-applications/\"\
    \ rel=\"nofollow\">Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\"\
    \ rel=\"nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  updated_at: 1624306567.0
pscedu/singularity-graphviz:
  data_format: 2
  description: Graphviz is a package of open-source tools initiated by AT&T Labs Research
    for drawing graphs specified in DOT language scripts.
  filenames:
  - 2.44.0/Singularity
  full_name: pscedu/singularity-graphviz
  latest_release: v2.44.0
  readme: "<p><a href=\"https://github.com/pscedu/singularity-graphviz/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-graphviz/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/4f27d7f7ac7b9a86a1f0f6c45d19b90496b7c8ce89d5004d3fe96d163fe99e73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4f27d7f7ac7b9a86a1f0f6c45d19b90496b7c8ce89d5004d3fe96d163fe99e73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-graphviz\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/e6789d316f02fdfe74574853fb2870a7e4b7b6cccca76d201ff81cb1ac2adfbe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e6789d316f02fdfe74574853fb2870a7e4b7b6cccca76d201ff81cb1ac2adfbe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-graphviz\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/b71cbcc295d522b3323d66e9141fdec85461c9d128011383fac4956c54d95d73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b71cbcc295d522b3323d66e9141fdec85461c9d128011383fac4956c54d95d73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-graphviz\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5227dbb6ba22ff0c43933a4284a416f0f8d311e9972bf97e5383fe334f545102/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5227dbb6ba22ff0c43933a4284a416f0f8d311e9972bf97e5383fe334f545102/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d677261706876697a\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-graphviz\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-graphviz\"\
    \ class=\"anchor\" href=\"#singularity-graphviz\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-graphviz</h1>\n<p><a\
    \ href=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\"\
    \ alt=\"Logo\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/en/4/48/GraphvizLogo.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for <a href=\"https://graphviz.org/\"\
    \ rel=\"nofollow\">graphviz</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>graphviz</code> script</li>\n</ul>\n<p>to <code>/opt/packages/graphviz/2.44.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/graphviz</code>\
    \ as <code> 2.44.0.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p>The <a href=\"https://www.psc.edu/biomedical-applications/\"\
    \ rel=\"nofollow\">Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\"\
    \ rel=\"nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1624570992.0
pscedu/singularity-hisat2:
  data_format: 2
  description: 'HISAT2 is a fast and sensitive alignment program for mapping next-generation
    sequencing reads (both DNA and RNA) to a population of human genomes as well as
    to a single reference genome. '
  filenames:
  - 2.2.1/Singularity
  full_name: pscedu/singularity-hisat2
  latest_release: null
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hisat2\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-hisat\"\
    \ class=\"anchor\" href=\"#singularity-hisat\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-hisat</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://daehwankimlab.github.io/hisat2/\" rel=\"nofollow\"\
    >hisat2</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the scripts <code>hisat2*</code>\n</li>\n</ul>\n<p>to <code>/opt/packages/hisat2/2.2.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/hisat2</code>\
    \ as <code>2.2.1.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-singularity-definition-file\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-singularity-definition-file\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building the image using the Singularity definition file</h2>\n<h3>\n\
    <a id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>To build the image locally</h3>\n<p>Run the script <code>build.sh</code>\
    \ to build image locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<h3>\n\
    <a id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>To build the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code>\
    \ to build image remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n\
    <p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - bioinformatics
  - singularity
  updated_at: 1624650733.0
pscedu/singularity-hyperfine:
  data_format: 2
  description: A command-line benchmarking tool.
  filenames:
  - 1.11.0/Singularity
  full_name: pscedu/singularity-hyperfine
  latest_release: v1.11.0
  readme: "<p><a href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-hyperfine\"\
    \ class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>singularity-hyperfine</h1>\n\
    <p><a href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\"\
    \ alt=\"Example\" style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for\
    \ <a href=\"https://github.com/sharkdp/hyperfine\">hyperfine</a>.</p>\n<h2>\n\
    <a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ <code>hyperfine</code> script</li>\n</ul>\n<p>to <code>/opt/packages/hyperfine/1.11.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/hyperfine</code>\
    \ as <code>1.11.0.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-tests\"\
    \ class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To run tests</h2>\n<p>To run\
    \ the available tests, run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights\
    \ Reserved.</p>\n<p>The <a href=\"https://www.psc.edu/biomedical-applications/\"\
    \ rel=\"nofollow\">Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\"\
    \ rel=\"nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  updated_at: 1624568894.0
pscedu/singularity-methylpy:
  data_format: 2
  description: Singularity recipe for methylpy
  filenames:
  - 1.4.3/Singularity
  full_name: pscedu/singularity-methylpy
  latest_release: null
  readme: '<h1>

    <a id="user-content-r-training" class="anchor" href="#r-training" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>R training</h1>

    <p>Use this link to run the project in mybinder</p>

    <p><a href="https://mybinder.org/v2/gh/reisportela/R_Training/HEAD?urlpath=rstudio"
    rel="nofollow"><img src="https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667"
    alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"></a></p>

    <p>Or use the following MyBinder <a href="https://github.com/reisportela/R_plus_RStudio">repository</a>
    and clone the contents of the exercise:</p>

    <p>Direct link:</p>

    <p>RStudio: <a href="https://mybinder.org/v2/gh/reisportela/R_plus_RStudio/HEAD?urlpath=rstudio"
    rel="nofollow"><img src="https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667"
    alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"></a></p>

    <p>Go to:</p>

    <p>File &gt; New Project &gt; Version Control &gt; Git</p>

    <p>and paste the following link <code>https://github.com/reisportela/R_Training.git</code>
    in <strong>Repository URL</strong></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624580745.0
pscedu/singularity-phylip-suite:
  data_format: 2
  description: PHYLIP is a free package of programs for inferring phylogenies.
  filenames:
  - 3.697/Singularity
  full_name: pscedu/singularity-phylip-suite
  latest_release: v3.697
  readme: "<p><a href=\"https://github.com/pscedu/singularity-phylip-suite/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-phylip-suite/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/c170f91fccaa5cf129589585cae304316c06a6c4926ef489d6ae6cec8639c69d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c170f91fccaa5cf129589585cae304316c06a6c4926ef489d6ae6cec8639c69d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-phylip-suite\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5e02750234489a3b6681aab63cc8c7dee07d7b579324265bc6a45b0d56dad6d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5e02750234489a3b6681aab63cc8c7dee07d7b579324265bc6a45b0d56dad6d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-phylip-suite\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a77ca9fdcc58325c1ddfe94710d8ad36e21b307dbe40570153f1e80be6cac559/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a77ca9fdcc58325c1ddfe94710d8ad36e21b307dbe40570153f1e80be6cac559/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-phylip-suite\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/50a0a1f8e6d29153411b82f9caaa4f006a72cf5b55af95c617d808d70a1588b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/50a0a1f8e6d29153411b82f9caaa4f006a72cf5b55af95c617d808d70a1588b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-phylip-suite\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-phylip-suite\"\
    \ class=\"anchor\" href=\"#singularity-phylip-suite\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>singularity-phylip-suite</h1>\n\
    <p><a href=\"https://camo.githubusercontent.com/b55efde725f14299bbb17335a6c6f17cde58bd9f451a128b97c0cfb8fe5b4edc/68747470733a2f2f65766f6c7574696f6e2e67656e65746963732e77617368696e67746f6e2e6564752f7068796c69702e676966\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b55efde725f14299bbb17335a6c6f17cde58bd9f451a128b97c0cfb8fe5b4edc/68747470733a2f2f65766f6c7574696f6e2e67656e65746963732e77617368696e67746f6e2e6564752f7068796c69702e676966\"\
    \ alt=\"Logo\" data-canonical-src=\"https://evolution.genetics.washington.edu/phylip.gif\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for <a href=\"https://evolution.genetics.washington.edu/phylip.html\"\
    \ rel=\"nofollow\">PHYLIP</a>.</p>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the scripts</li>\n</ul>\n<p>to <code>/opt/packages/phylip-suite/3.697</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/phylip-suite</code>\
    \ as <code>3.697.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - bioinformatics
  - singularity
  updated_at: 1624555284.0
pscedu/singularity-picard:
  data_format: 2
  description: 'Picard is a set of command line tools for manipulating high-throughput
    sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF. '
  filenames:
  - 2.23.2/Singularity
  full_name: pscedu/singularity-picard
  latest_release: v2.23.2
  readme: "<p><a href=\"https://github.com/pscedu/singularity-picard/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-picard/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/f2ee028725767bd1588c30ee90365dddfc357c08ce7b5a43ed492ec5987e19f9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d706963617264\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f2ee028725767bd1588c30ee90365dddfc357c08ce7b5a43ed492ec5987e19f9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d706963617264\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-picard\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/0011e731d3015546848fd1f5982cbad69352604dc45cd908e9ff27c2773e8107/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d706963617264\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0011e731d3015546848fd1f5982cbad69352604dc45cd908e9ff27c2773e8107/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d706963617264\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-picard\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a19ffb10b86582f80f0dd57f7696abb065ebaaab0d440703423ea0f2441278ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d706963617264\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a19ffb10b86582f80f0dd57f7696abb065ebaaab0d440703423ea0f2441278ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d706963617264\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-picard\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/b6afcdc9a6fce9707e6a449e49036b6136dd0335325684f4c8605d441b992e1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d706963617264\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b6afcdc9a6fce9707e6a449e49036b6136dd0335325684f4c8605d441b992e1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d706963617264\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-picard\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-picard\"\
    \ class=\"anchor\" href=\"#singularity-picard\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-picard</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://github.com/sandialabs/PIGER\">Picard</a>.</p>\n\
    <h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ Perl scripts</li>\n</ul>\n<p>to <code>/opt/packages/picard/2.23.2</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/picard</code>\
    \ as <code>2.23.2.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - bioinformatics
  - singularity
  updated_at: 1624643416.0
pscedu/singularity-tiger:
  data_format: 2
  description: Target/Integrative Genetic Element Retriever
  filenames:
  - 5.32.1/Singularity
  full_name: pscedu/singularity-tiger
  latest_release: v5.32.1
  readme: "<p><a href=\"https://github.com/pscedu/singularity-tiger/actions/workflows/main.yml/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pscedu/singularity-tiger/actions/workflows/main.yml/badge.svg\"\
    \ alt=\"Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/45b51fb9658ca6d66b992d887a9258c238463b14dc9fb58c4ed17ba4e75f2efc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7469676572\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/45b51fb9658ca6d66b992d887a9258c238463b14dc9fb58c4ed17ba4e75f2efc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7469676572\"\
    \ alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-tiger\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/266d33a01bfbc88af8ef713b4ab3f12e4207aea6a693420544c401bcc4687384/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7469676572\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/266d33a01bfbc88af8ef713b4ab3f12e4207aea6a693420544c401bcc4687384/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7469676572\"\
    \ alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-tiger\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/cd42640e858ab21849faf7ea1ae7b80b386fea232400bb8666ab226125727f09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7469676572\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/cd42640e858ab21849faf7ea1ae7b80b386fea232400bb8666ab226125727f09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7469676572\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-tiger\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/049f66af13d64dfd0fc9fb6af0dd2b62c6d9f524419d096508747447c1c661ac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7469676572\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/049f66af13d64dfd0fc9fb6af0dd2b62c6d9f524419d096508747447c1c661ac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7469676572\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-tiger\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-tiger\"\
    \ class=\"anchor\" href=\"#singularity-tiger\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-tiger</h1>\n<p>Singularity\
    \ recipe for <a href=\"https://github.com/sandialabs/TIGER\">TIGER</a>.</p>\n\
    <h2>\n<a id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\"\
    \ href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing the container on Bridges\
    \ 2</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code> file</li>\n<li>and the\
    \ Perl scripts</li>\n</ul>\n<p>to <code>/opt/packages/tiger/5.32.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modulefiles/tiger</code>\
    \ as <code>5.32.1.lua</code>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ remotely.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p>The <a href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\
    >Biomedical Applications Group</a> at the <a href=\"http://www.psc.edu\" rel=\"\
    nofollow\">Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - tiger
  updated_at: 1624334539.0
qbicsoftware-archive/microarray-qc-workflow:
  data_format: 2
  description: Quality Control plots and data normalisation for Microarray data
  filenames:
  - Singularity
  full_name: qbicsoftware-archive/microarray-qc-workflow
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-work-in-progress\" class=\"anchor\" href=\"\
    #work-in-progress\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Work in progress</h1>\n<h1>\n<a id=\"user-content-microarray-qc-workflow\"\
    \ class=\"anchor\" href=\"#microarray-qc-workflow\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>microarray-qc-workflow</h1>\n\
    <p>Takes .cel files and creates qc plots as well as normalising the data</p>\n\
    <p><a href=\"https://travis-ci.org/qbicsoftware/microarray-qc-workflow\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/16f4a8e65783fd7014126125a1c351cf1ac4d34d672b6cb4e6cab27706d0c85f/68747470733a2f2f7472617669732d63692e6f72672f71626963736f6674776172652f6d6963726f61727261792d71632d776f726b666c6f772e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/qbicsoftware/microarray-qc-workflow.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.nextflow.io/\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/17df893666bfa5cad487466d4476ab773ea5def560c04c50f45795017865e81c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33302e302d627269676874677265656e2e737667\"\
    \ alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.30.0-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\"\
    \ alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/microarray-qc-workflow\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8af3143d98534fd47758128af0ea9ed413467e1151e5ab095c16968f578fcdd3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6d6963726f61727261792d71632d776f726b666c6f772e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/microarray-qc-workflow.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\"\
    \ alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h3>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h3>\n<p>microarray-qc-workflow:\
    \ Takes .cel files and creates qc plots as well as normalising the data</p>\n\
    <p>The pipeline is built using <a href=\"https://www.nextflow.io\" rel=\"nofollow\"\
    >Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures\
    \ in a very portable manner. It comes with docker / singularity containers making\
    \ installation trivial and results highly reproducible.</p>\n<h3>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h3>\n<p>The microarray-qc-workflow\
    \ pipeline comes with documentation about the pipeline, found in the <code>docs/</code>\
    \ directory:</p>\n<ol>\n<li><a href=\"docs/installation.md\">Installation</a></li>\n\
    <li>Pipeline configuration\n<ul>\n<li><a href=\"docs/configuration/local.md\"\
    >Local installation</a></li>\n<li><a href=\"docs/configuration/adding_your_own.md\"\
    >Adding your own system</a></li>\n</ul>\n</li>\n<li><a href=\"docs/usage.md\"\
    >Running the pipeline</a></li>\n<li><a href=\"docs/output.md\">Output and how\
    \ to interpret the results</a></li>\n<li><a href=\"docs/troubleshooting.md\">Troubleshooting</a></li>\n\
    </ol>\n<h3>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Credits</h3>\n<p>This pipeline was written by Timo Lucas (<a href=\"\
    https://github.com/lucass122\">lucass122</a>) at <a href=\"http://www.qbic.uni-tuebingen.de/\"\
    \ rel=\"nofollow\">QBiC T\xFCbingen</a>.\nR script based on script by Stefan Czemmel\
    \ [qbicStefanC]:\n<a href=\"https://github.com/qbicsoftware/qbic-wf-microarrayQC\"\
    >https://github.com/qbicsoftware/qbic-wf-microarrayQC</a></p>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1600940492.0
radio1988/OneStopRNAseq:
  data_format: 2
  description: null
  filenames:
  - snakemake/workflow/envs/Singularity
  full_name: radio1988/OneStopRNAseq
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-csci5980\" class=\"anchor\" href=\"#csci5980\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>csci5980</h1>\n<p>Final project for CSci 5980: deep learning for automatic\
    \ music translation.</p>\n<p>Follow theses steps to install all package dependencies\
    \ for running the model:</p>\n<p>We first install software dependencies for manipulating\
    \ raw audio (<code>ffmpeg</code>):</p>\n<ol>\n<li>\n<p>Create a local software\
    \ directory\n<code>mkdir ~/software</code></p>\n</li>\n<li>\n<p>Install the NASM\
    \ assembler (dependency of ffmpeg):</p>\n</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span> <span class=\"\
    pl-k\">~</span>/software\nwget https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2\n\
    tar -xvf nasm-2.14.02.tar.bz2\n<span class=\"pl-c1\">cd</span> nasm-2.14.02\n\
    ./configure --prefix=<span class=\"pl-k\">~</span>/software/nasm/\nmake install\n\
    <span class=\"pl-k\">export</span> PATH=<span class=\"pl-smi\">$PATH</span>:<span\
    \ class=\"pl-k\">~</span>/software/nasm/bin/</pre></div>\n<ol start=\"3\">\n<li>Make\
    \ sure that NASM assembler installed correctly:</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>nasm -v</pre></div>\n<p>The output should look\
    \ something like:\n<code>NASM version 2.14.02 compiled on Mar 11 2020</code></p>\n\
    <ol start=\"4\">\n<li>Install ffmpeg:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/software\n\
    wget https://ffmpeg.org/releases/ffmpeg-4.2.2.tar.bz2\ntar -xvf ffmpeg-4.2.2.tar.bz2\n\
    <span class=\"pl-c1\">cd</span> ffmpeg-4.2.2\n./configure --prefix=<span class=\"\
    pl-k\">~</span>/software/ffmpeg/\nmake install\n<span class=\"pl-k\">export</span>\
    \ PATH=<span class=\"pl-smi\">$PATH</span>:<span class=\"pl-k\">~</span>/software/ffmpeg/bin/</pre></div>\n\
    <ol start=\"5\">\n<li>Make sure that ffmpeg installed correctly:</li>\n</ol>\n\
    <div class=\"highlight highlight-source-shell\"><pre>ffmpeg -version</pre></div>\n\
    <p>The output should look something like:</p>\n<pre><code>ffmpeg version 4.2.2\
    \ Copyright (c) 2000-2019 the FFmpeg developers\nbuilt with gcc 4.4.7 (GCC) 20120313\
    \ (Red Hat 4.4.7-23)\nconfiguration: --prefix=/home/csci5980/piehl008/software/ffmpeg/\n\
    libavutil      56. 31.100 / 56. 31.100\nlibavcodec     58. 54.100 / 58. 54.100\n\
    libavformat    58. 29.100 / 58. 29.100\nlibavdevice    58.  8.100 / 58.  8.100\n\
    libavfilter     7. 57.100 /  7. 57.100\nlibswscale      5.  5.100 /  5.  5.100\n\
    libswresample   3.  5.100 /  3.  5.100\n</code></pre>\n<ol start=\"6\">\n<li>Now,\
    \ we can make the virtual environment and install python packages.  First, create\
    \ the virtual environment by running:</li>\n</ol>\n<p><code>conda create --name\
    \ audio-proj python=3.7</code></p>\n<ol start=\"7\">\n<li>Next, install packages\
    \ by running</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/csci5980\nconda install\
    \ --name audio-proj --file requirements.txt --channel defaults --channel conda-forge</pre></div>\n\
    <p>(Note: this can take a while - and you need to say yes to installing everything\
    \ after it solves the environment)</p>\n<ol start=\"8\">\n<li>To activate the\
    \ virtual environment, you can now run <code>source activate audio-proj</code>.\
    \ Note: you should do this to test that you can activate the virtual evironment,\
    \ but you probably shouldn't run a lot unless you are submitting jobs to the queue.\
    \  If you want to use this virtual environment through the MSI notebooks, check\
    \ out the tutorial at <a href=\"https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\"\
    \ rel=\"nofollow\">https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html</a>.</li>\n\
    </ol>\n<h3>\n<a id=\"user-content-adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ class=\"anchor\" href=\"#adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Adding the Virtual Environment to Jupyter Notebooks</h3>\n<p>Now that\
    \ we have created the virtual environment, we can add it to the Jupyter notebook\
    \ kernels so that we can use the virtual environment through MSI's notebook server.\
    \ To do this, we have to add the kernel specifications to the known Jupyter kernels\
    \ for our user:</p>\n<ol start=\"9\">\n<li>If you haven't already, activate your\
    \ virtual environment by running <code>source activate audio-proj</code>. Then\
    \ enter</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>which\
    \ python</pre></div>\n<p>Your output should tell you where the python executable\
    \ for this virtual environment lives - the output for me displays <code>~/.conda/envs/audio-proj/bin/python</code>.\
    \  If you see something that looks like <code>/panfs/roc/msisoft/anaconda/anaconda3-2018.12/bin/python</code>,\
    \ go back and make sure that you have the virtual environment active and try again.\
    \ After you have an ouput that clearly has the name of the virtual environment\
    \ in the directory path (i.e. contains audio-proj in it), continue to the next\
    \ step.</p>\n<ol start=\"10\">\n<li>Now, we need to create the kernel configuration.\
    \ To do this run</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>mkdir <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj\n\
    nano <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj/kernel.json</pre></div>\n\
    <p>The nano command will open a very basic text editor that you can navigate with\
    \ the arrow keys. Enter the following:</p>\n<pre lang=\"text\"><code>{\n \"argv\"\
    : [\n  \"~/.conda/envs/audio-proj/bin/python\", #replace this with your path from\
    \ step 9 above! (and delete this comment)\n  \"-m\",\n  \"ipykernel_launcher\"\
    ,\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Audio Project\
    \ Kernel\",\n \"language\": \"python\"\n}\n</code></pre>\n<p>where you replace\
    \ the first line of the argv array with whatever executable path was output from\
    \ step 9 above (it likely will be identical to this). To exit the nano text editor,\
    \ type <code>Ctrl-x &lt;RETURN&gt;</code> and then type <code>Y &lt;RETURN&gt;</code>\
    \ to save the file.</p>\n<ol start=\"11\">\n<li>Now that you have saved the kernel\
    \ file, you should be able to go to <code>https://notebooks.msi.umn.edu/</code>\
    \ and when you click on the <code>New</code> tab to create a new file, you should\
    \ be able to select <code>Audio Project Kernel</code> as an available kernel to\
    \ run your newly created file in.</li>\n</ol>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1623730067.0
rickstaa/COR-robotics-lab-reference:
  data_format: 2
  description: A quick reference repository for using the robots in the COR lab
  filenames:
  - containers/singularity/Singularity.panda_ros_kinetic
  - containers/singularity/Singularity.husky_ros_melodic
  - containers/singularity/Singularity.panda_ros_melodic
  - containers/singularity/Singularity.panda_ros_noetic
  - containers/singularity/Singularity.ros_melodic
  full_name: rickstaa/COR-robotics-lab-reference
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-cor-robotics-lab-reference\" class=\"anchor\"\
    \ href=\"#cor-robotics-lab-reference\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>COR-robotics-lab-reference</h1>\n\
    <p>A quick reference repository for using the robots in the COR lab. This repository\
    \ contains several code examples, a <a href=\"https://github.com/rickstaa/COR-robotics-lab-reference/discussions\"\
    >discussion forum</a> with FAQ that you might have while working with the robot\
    \ and a <a href=\"https://github.com/rickstaa/COR-robotics-lab-reference/wiki\"\
    >wiki</a> with several helpfull documents</p>\n<h2>\n<a id=\"user-content-how-to-reserve-the-robots\"\
    \ class=\"anchor\" href=\"#how-to-reserve-the-robots\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How to reserve\
    \ the robots</h2>\n<p>Please check the <g-emoji class=\"g-emoji\" alias=\"spiral_calendar\"\
    \ fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f5d3.png\"\
    >\U0001F5D3\uFE0F</g-emoji> <a href=\"https://github.com/rickstaa/COR-robotics-lab-reference/wiki/%F0%9F%97%93%EF%B8%8F-Robot-reservation-forum\"\
    >robot reservation form</a> wiki page for more information.</p>\n<h2>\n<a id=\"\
    user-content-how-to-work-with-the-robots\" class=\"anchor\" href=\"#how-to-work-with-the-robots\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to work with the robots</h2>\n<p>Please check <g-emoji class=\"\
    g-emoji\" alias=\"safety_vest\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f9ba.png\"\
    >\U0001F9BA</g-emoji> <a href=\"https://github.com/rickstaa/COR-robotics-lab-reference/wiki/Panda-safety-guidelines\"\
    >safety-guidelines</a> in the wiki before working with the robot.</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1624697768.0
rohitfarmer/singularity-defs:
  data_format: 2
  description: Definition (recipe) files for singularity containers.
  filenames:
  - cite-seq/Singularity_AJM_COVID.def
  - cite-seq/Singularity_3.def
  - cite-seq/Singularity_xenial.def
  - cite-seq/Singularity_rocker.def
  - cite-seq/Singularity_publish.def
  - cytof-workflow-v3/Singularity.def
  - cytof-workflow-v3/Singularity_SCS.def
  - cytof-deep-cnn/Singularity.def
  - H5N1/Singularity.def
  - H5N1/Singularity_R_3.6.def
  - comet/Singularity.def
  - bittersweet/Singularity.def
  - tf-gpu-chemistry/Singularity
  - generic/Singularity.def
  full_name: rohitfarmer/singularity-defs
  latest_release: null
  readme: '<h1>

    <a id="user-content-definitionrecipe-files-for-singularity-containers" class="anchor"
    href="#definitionrecipe-files-for-singularity-containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Definition/Recipe Files
    for Singularity Containers</h1>

    <p>Some of the containers are available to download from <a href="https://cloud.sylabs.io/library/rohitfarmer"
    rel="nofollow">https://cloud.sylabs.io/library/rohitfarmer</a></p>

    <p>For feedback and collaboration write to me at <a href="mailto:rohit.farmer@gmail.com">rohit.farmer@gmail.com</a></p>

    <h1>

    <a id="user-content-install-singularity-on-linux" class="anchor" href="#install-singularity-on-linux"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Singularity on Linux</h1>

    <h2>

    <a id="user-content-singularity-version-34" class="anchor" href="#singularity-version-34"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Version 3.4</h2>

    <p>Follow the instructions on <a href="https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps</a></p>

    <h1>

    <a id="user-content-building-a-singularity-container" class="anchor" href="#building-a-singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    a Singularity Container</h1>

    <h2>

    <a id="user-content-readonly-container" class="anchor" href="#readonly-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Readonly
    Container</h2>

    <p>To build a read-only SquashFS Singularity container on a local machine using
    a recipe/definition file.</p>

    <p><code>sudo singularity build &lt;container-name.sif&gt; &lt;Singularity.def&gt;</code></p>

    <p>To set a different temporary directory than the default <code>/tmp</code>.<br>

    <code>sudo -E SINGULARITY_TMPDIR=/home/rohit/tmp singularity build &lt;container.sif&gt;
    &lt;container.def&gt;</code></p>

    <h2>

    <a id="user-content-writable-sandbox" class="anchor" href="#writable-sandbox"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Writable
    Sandbox.</h2>

    <p>To build a writable sandbox (essentially a folder) on a local machine using
    a recipe/definition file.</p>

    <p><code>sudo singularity build --sandbox  &lt;sandbox-folder-name/&gt; &lt;Singularity.def&gt;</code></p>

    <p><em>Note: The advantage of building a writable sandbox is that it can be used
    to install and configure packages as you go, and once you are satisfied with the
    requirements, the sandbox can be converted into a read-only SquashFS container.
    To build a sandbox quickly, it''s better to install a minimal set of packages
    via the definition file.</em></p>

    <h3>

    <a id="user-content-installconfigure-packages-in-a-writable-sandbox" class="anchor"
    href="#installconfigure-packages-in-a-writable-sandbox" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install/Configure Packages
    in a Writable Sandbox</h3>

    <p>Once a writable sandbox is created to execute it to invoke the shell of the
    operating installed in the container in the "writable" mode. If the shell is not
    invoked in the "writable" mode, all the changes will be lost once you exit from
    the container environment.</p>

    <p><code>sudo singularity shell --writable &lt;sandbox-folder-name/&gt;</code></p>

    <p>Install packages as you would, for example, in Ubuntu from the command line.</p>

    <h2>

    <a id="user-content-convert-a-writable-sandbox-to-a-readonly-container" class="anchor"
    href="#convert-a-writable-sandbox-to-a-readonly-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Convert a Writable
    Sandbox to a Readonly Container</h2>

    <p><code>sudo singularity build &lt;container-name.sif&gt; &lt;sandbox-folder-name/&gt;</code></p>

    <h1>

    <a id="user-content-execute-a-container" class="anchor" href="#execute-a-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execute
    a Container</h1>

    <h2>

    <a id="user-content-invoke-a-shell" class="anchor" href="#invoke-a-shell" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Invoke a shell</h2>

    <p>The command below can be used for both read-only/writable containers/sandbox.</p>

    <p><code>singularity shell &lt;container-name.sif&gt;</code></p>

    <p><em>Note: By default, Singularity binds to your current working and home directory.
    Therefore, you do not need to do anything else to execute a script that is in
    your current working directory. It can also pull, for example, Vim settings from
    the .vimrc file in your home directory. Therefore, if Vim installed in the container,
    it can be used with the same settings from inside the container as it would from
    outside.</em></p>

    <h2>

    <a id="user-content-execute-a-command-via-container" class="anchor" href="#execute-a-command-via-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execute
    a Command via Container</h2>

    <p><code>singularity exec &lt;container-name.sif&gt; &lt;command&gt;</code></p>

    <p>For example: <code>singularity exec &lt;container-name.sif&gt; Rscript --vanilla
    hello.R</code></p>

    <h1>

    <a id="user-content-running-jupyter-notebooks-from-within-a-container" class="anchor"
    href="#running-jupyter-notebooks-from-within-a-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running Jupyter Notebooks
    from Within a Container</h1>

    <p>This section is for containers that have Jupyter notebook installed (e.g. cite-seq).</p>

    <p>A generic command that should work on a personal computer. <code>singularity
    exec container-name.sif jupyter notebook --no-browser --ip=127.0.0.1 --port=8888</code><br>

    <em>Note: The IP address and the port number mentioned in the command are the
    jupyter defaults. They can be changed as per need.</em><br>

    Copy the URL generated by jupyter daemon and paste it in your browser; this should
    open Jupyter with the list of the files in your current working directory on the
    host computer.</p>

    <h2>

    <a id="user-content-running-with-r-as-a-kernel" class="anchor" href="#running-with-r-as-a-kernel"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    with R as a Kernel</h2>

    <p>Sometimes if you already have an R kernel installed in your home directory,
    it conflicts with what you have inside the container. Therefore, it would require
    you to re-install the kernel specs in your home directory via the container.</p>

    <pre><code>singularity exec container-name.sif R --quiet --slave -e ''IRkernel::installspec()''


    # Screen log.

    # [InstallKernelSpec] Removing existing kernelspec in /home/user/.local/share/jupyter/kernels/ir

    # [InstallKernelSpec] Installed kernelspec ir in /home/user/.local/share/jupyter/kernels/ir

    </code></pre>

    <h2>

    <a id="user-content-running-on-an-hpc" class="anchor" href="#running-on-an-hpc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    on an HPC</h2>

    <ol>

    <li>SSH to the HPC.</li>

    <li>Claim an interactive node.</li>

    <li>Navigate to your project directory. Singularity container should be in your
    project directory.</li>

    <li><code>singularity exec container-name.sif jupyter notebook --no-browser --ip=''0.0.0.0''</code></li>

    <li>Keep the SSH session and Jupyter notebook session running. Copy the URL on
    your local browser.</li>

    </ol>

    <p><em>Note: On some HPCs, you may have to initiate an additional SSH tunnel connecting
    your local machine to the interactive node on the HPC. In that case, follow some
    generic instructions here <a href="https://rohitfarmer.github.io/docs/docs/HPC/jupyter/"
    rel="nofollow">https://rohitfarmer.github.io/docs/docs/HPC/jupyter/</a> or ask
    your system administrator.</em></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1623100431.0
sailfish009/openstructure:
  data_format: 2
  description: Open-Source Computational Structural Biology Framework
  filenames:
  - singularity/Singularity
  full_name: sailfish009/openstructure
  latest_release: null
  readme: '<h1>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h1>

    <p>This README is pulled from a default template for workflows.</p>

    <h1>

    <a id="user-content-workflow-template-setup" class="anchor" href="#workflow-template-setup"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Workflow
    template setup</h1>

    <h2>

    <a id="user-content-lib" class="anchor" href="#lib" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>lib</h2>

    <ul>

    <li>The <code>lib</code> directory contains general libraries that may be referenced
    by multiple workflows, for instance cromwell configs and python configs. Currently
    nothing in this directory is used.</li>

    </ul>

    <h2>

    <a id="user-content-pipelines" class="anchor" href="#pipelines" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pipelines</h2>

    <ul>

    <li>

    <p>Each pipeline is a full analysis. Think of it like the heading of a methods
    section in a paper. For instance if this were genetic summary statistics workflow,
    a pipeline might be "fine-mapping" that does both conditional and credible set
    analysis. Another pipeline may be "colocalization".</p>

    </li>

    <li>

    <p>Pipelines may have numbers prior to their name (e.g., <code>example_pipeline_1</code>
    to <code>0025-example_pipeline_1</code>). These numbers do not mean anything,
    but merely used to keep pipelines in their general order of execution. These are
    optional.</p>

    </li>

    <li>

    <p>A pipeline consists of :</p>

    </li>

    </ul>

    <ol>

    <li>A workflow.</li>

    <li>A <code>scripts</code> directory with <em>all</em> scripts referenced by that
    workflow (unless a general lib script is called). Scripts may have numbers prior
    to their name. These numbers do not mean anything, but merely used to keep scripts
    in their general order of execution. These are optional.</li>

    <li>A <code>docs</code> directory that contains a documentation of the default
    parameters written in a style that is publishable as methods in a paper (including
    citations). Within the <code>docs</code> directory there may be a <code>reference</code>
    with any additional reference materials.</li>

    <li>An <code>example_runtime_setup</code> directory contains files that give an
    example of actual config files and any other files used to run the pipeline.</li>

    </ol>

    <h2>

    <a id="user-content-studies" class="anchor" href="#studies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>studies</h2>

    <ul>

    <li>A studies directory should either exist within the workflow repo or be a separate
    repo that has the same name as the workflow repo, but with <code>studies</code>
    appended to it (e.g. <code>template-workflow</code> becomes <code>template-workflow-studies</code>).</li>

    <li>If there is a standard set of plots that will always look the same way, a
    pipeline should generate such plots. Otherwise, all code to analyze the results
    of a pipeline run should be in the <code>studies</code> directory. For instance
    if this were genetic summary statistics workflow, <code>studies</code> may contain
    a <code>t2d</code> directory and a <code>weight</code> directory.</li>

    <li>Within a study is either an Jupyter notebook (either python or R kernel) or
    an R markdown file. Nearly all plots / analysis of the results of running the
    various pipelines should be done in the notebook / markdown file.</li>

    <li>A study may also contain a scripts directory with scripts to aggregate data
    for a one off analysis (if the analysis is going to be repeated, consider making
    a new pipeline or adding it to an existing pipeline) or for special plots that
    cannot be done in the notebook / markdown file.</li>

    </ul>

    <h1>

    <a id="user-content-new-workflow-reminders" class="anchor" href="#new-workflow-reminders"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>New
    workflow reminders</h1>

    <ul>

    <li>[ ] Documentation</li>

    <li>[ ] Environment version control</li>

    <li>[ ] Pipeline version control</li>

    <li>[ ] Git branches</li>

    <li>[ ] Code review</li>

    </ul>

    <h1>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h1>

    <p>Be sure to document your code!</p>

    <h1>

    <a id="user-content-environment-version-control" class="anchor" href="#environment-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment
    version control</h1>

    <p>Analysis environment is controlled using conda. Each pipeline should have an
    <code>environment.yml</code> file with all of the packages used. If a required
    package or library is missing from conda (and therefore not in the <code>environment.yml</code>),
    it should be noted in the <code>README.md</code> of the pipeline.</p>

    <div class="highlight highlight-source-shell"><pre>conda env <span class="pl-k">export</span>
    --no-builds <span class="pl-k">|</span> grep -v prefix <span class="pl-k">|</span>
    grep -v name <span class="pl-k">&gt;</span> environment.yml</pre></div>

    <h1>

    <a id="user-content-pipeline-version-control" class="anchor" href="#pipeline-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    version control</h1>

    <p>Each pipeline within this workflow uses <a href="https://pypi.org/project/bumpversion"
    rel="nofollow">bumpversion</a> for automatic <a href="https://semver.org" rel="nofollow">semantic
    versioning</a>.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    bump the appropriate increment</span>

    bumpversion patch --verbose --dry-run

    bumpversion minor --verbose --dry-run

    bumpversion major --verbose --dry-run


    <span class="pl-c"><span class="pl-c">#</span> commit with tags</span>

    git push --tags</pre></div>

    <h1>

    <a id="user-content-github-forks" class="anchor" href="#github-forks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GitHub forks</h1>

    <p>Forking the repository allows developers to work independently while retaining
    well-maintained code on the master fork. For instructions on how to fork, follow
    the <a href="https://help.github.com/en/articles/fork-a-repo">Fork a repo</a>
    instructions.</p>

    <p>After forking the repo, clone the repo to your local desktop:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    to use SSH</span>

    git clone git@github.com:<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git


    <span class="pl-c"><span class="pl-c">#</span> to use Https</span>

    git clone https://github.com/<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git</pre></div>

    <p>This creates a replica of the remote repository on your local desktop. <em>Note</em>:
    When you create your local repository, it will also make a local clone of the
    remote repository (typically as <code>origin</code>). So, your local master branch
    would simply be <code>master</code>. But, your remote master branch will be <code>origin/master</code>.
    You can also add multiple remote repositories. For instance, let us say our main
    repository is under the remote repository <code>my_repo</code>. We will want to
    add it as a remote repository, so we can fetch the most up-to-date code. You could
    add it by:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Add the my_repo remote repo to your local desktop -- this will allow you to pull
    and push to branches on the my_repo repository</span>

    git remote add my_repo git@github.com:my_repo/template-workflow.git</pre></div>

    <h1>

    <a id="user-content-git-branches" class="anchor" href="#git-branches" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Git branches</h1>

    <p>Branching is how git actually tracks code development. For more information,
    see the <a href="https://www.atlassian.com/git/tutorials/using-branches" rel="nofollow">Git
    Branch Tutorial</a> on Atlassian. If you want to add a new feature, pipeline,
    or fix a bug, a common work flow would look like this:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Update your local copy of the master branch to make sure you are getting the most
    up-to-date code</span>

    git pull


    <span class="pl-c"><span class="pl-c">#</span> Create the branch on your local
    machine and switch in this branch </span>

    git checkout -b [name_of_your_new_branch]


    <span class="pl-c"><span class="pl-c">#</span> Push the branch on github</span>

    git push origin [name_of_your_new_branch]</pre></div>

    <p>As you develop, you want to commit your work to your branch, so you don''t
    lose it all if something happens!</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Confirm we''re on the right branch</span>

    git branch -a


    <span class="pl-c"><span class="pl-c">#</span> Add all your work to be tracked
    (Note: there are many ways to add specific files, etc. See https://git-scm.com/docs/git-add
    for more information). The following command adds everything in your currently
    directory.</span>

    git add <span class="pl-c1">.</span>


    <span class="pl-c"><span class="pl-c">#</span> Commit your work to the branch
    with a message describing what''s in the commit</span>

    git commit -m <span class="pl-s"><span class="pl-pds">"</span>Created the scATAC-seq
    pipeline!<span class="pl-pds">"</span></span>


    <span class="pl-c"><span class="pl-c">#</span> You can add a -u parameter to set-upstream
    for a push</span>

    <span class="pl-c"><span class="pl-c">#</span> Alternatively, git will also automatically
    query you when you do your first push.</span>

    <span class="pl-c"><span class="pl-c">#</span> You can also set this manually
    by adding a new remote for your branch:</span>

    <span class="pl-c"><span class="pl-c">#</span>git remote add [name_of_your_remote]
    [name_of_your_new_branch]</span>


    <span class="pl-c"><span class="pl-c">#</span> Here is another push where we specify
    HEAD</span>

    <span class="pl-c"><span class="pl-c">#</span>git push origin HEAD # HEAD pushes
    everything up to the most recent commit</span></pre></div>

    <h1>

    <a id="user-content-code-review" class="anchor" href="#code-review" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Code review</h1>

    <p>Create a <a href="https://help.github.com/en/articles/creating-a-pull-request">GitHub
    Pull Request</a>. A PR allows other developers a chance to go through and comment
    on lines of code they believe can be improved. In addition, it will tell you if
    the code you are trying to merge into the <code>my_repo</code> branch actually
    conflicts with code that already exists in the branch, so you don''t overwrite
    someone else''s work.</p>

    <p>Once another developer approves the PR, you have the go-ahead to merge your
    code! Congrats, you finished your feature!</p>

    <p><em>Note</em>: There are some cases where you may just want to push directly
    to the my_repo fork, thereby avoiding code reviews. For instance, if you''re working
    on a one-off project that you want people to be able to see, but no one else is
    necessarily working on, you can always push directly to the branches on my_repo
    fork. Or, you could also still go through the steps of a PR, but simply merge
    your own code without CR.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1597367982.0
sajjadtorabian/singularity_recipes:
  data_format: 2
  description: null
  filenames:
  - Singularity.nipype-plus-jupyter-plus-seaborn
  full_name: sajjadtorabian/singularity_recipes
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-recipes\" class=\"anchor\" href=\"\
    #singularity-recipes\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Singularity Recipes</h1>\n<h2>\n<a id=\"user-content-nipype-plus-jupyter\"\
    \ class=\"anchor\" href=\"#nipype-plus-jupyter\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Nipype Plus Jupyter</h2>\n<p>We\
    \ first need to download the Docker layers. Normally this happens automatically\
    \ with <code>singularity build</code> or <code>singularity pull</code>, but if\
    \ you aren't using the development version 3.0 branch of Singularity that has\
    \ a fixed bug with respect to whiteout files, you will have issue when you do\
    \ these commands with nipype (note that it has whiteout files). What we did (because\
    \ didn't feel like installing another version of Singularity) was to do a pull\
    \ of nipype with the <a href=\"https://singularityhub.github.io/sregistry-cli\"\
    \ rel=\"nofollow\">Singularity Global Client</a> that will download the fixed\
    \ layers and then put them in the same cache that Singularity uses. Then we will\
    \ have what we need :)  Here is how you can install and use the client:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ pip install sregistry\n\
    $ sregistry pull nipype/nipype:latest</pre></div>\n<p>Now we want to debug the\
    \ build and find the missing path! To do this, you can build a completely empty\
    \ image to look around in. The recipe looks like this:</p>\n<pre><code># Singularity.nipype-plus-jupyter-empty\n\
    Bootstrap: docker\nFrom: nipype/nipype:latest\n</code></pre>\n<p>The above you\
    \ can save to whatever file you want, we're calling ours <code>Singularity.nipype-plus-jupyter-empty</code>\
    \ we can then build like this:</p>\n<pre><code>sudo singularity build npjup.simg\
    \ Singularity.nipype-plus-jupyter-empty\n</code></pre>\n<p>Then we can shell inside\
    \ and find locations of things.</p>\n<pre><code>singularity shell npjup.simg\n\
    </code></pre>\n<p>We are able to see what will be exported in the environment\
    \ at runtime, and then source it to add these locations to the path (so we can\
    \ find executables there!)</p>\n<pre><code>cat /environment\n$ cat /.singularity.d/env/10-docker.sh\
    \ \nexport PATH=\"/opt/conda/bin:/usr/lib/ants:/opt/afni:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\
    \nexport LANG=\"en_US.UTF-8\"\nexport LC_ALL=\"C.UTF-8\"\nexport ND_ENTRYPOINT=\"\
    /neurodocker/startup.sh\"\nexport MATLABCMD=\"/opt/mcr/v92/toolbox/matlab\"\n\
    export FORCE_SPMMCR=\"1\"\nexport LD_LIBRARY_PATH=\"/usr/lib/x86_64-linux-gnu:/opt/mcr/v92/runtime/glnxa64:/opt/mcr/v92/bin/glnxa64:/opt/mcr/v92/sys/os/glnxa64:\"\
    \nexport FREESURFER_HOME=\"/opt/freesurfer\"\nexport ANTSPATH=\"/usr/lib/ants\"\
    \nexport MKL_NUM_THREADS=\"1\"\nexport OMP_NUM_THREADS=\"1\"\nexport CONDA_DIR=\"\
    /opt/conda\"\n</code></pre>\n<p>There we see the location of conda! Strange that\
    \ it wasn't where we expected. Let's add to the path and then we have pip</p>\n\
    <pre><code>export PATH=/opt/conda/bin:$PATH\nwhich pip\n/opt/conda/bin/pip\n</code></pre>\n\
    <p>Now we can update the recipe <a href=\"Singularity.nipype-plus-jupyter\">Singularity.nipype-plus-jupyter</a>\
    \ with our found pip.</p>\n<pre><code>Bootstrap: docker\nFrom: nipype/nipype:latest\n\
    \n%labels\n  Maintainer Sajjad\n  Version v1.0\n\n%post\n    export PATH=/opt/conda/bin:$PATH\n\
    \    pip install --upgrade pip\n    pip install jupyter\n</code></pre>\n<p>And\
    \ build the image</p>\n<div class=\"highlight highlight-source-shell\"><pre>sudo\
    \ singularity build npjup.simg Singularity.nipype-plus-jupyter</pre></div>\n<p>:)</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1534761419.0
salome-eriksson/downward-unsolvability:
  data_format: 2
  description: null
  filenames:
  - misc/releases/latest/Singularity
  - misc/releases/20.06/Singularity.20.06
  - misc/releases/19.12/Singularity.19.12
  - misc/releases/19.06/Singularity.19.06
  full_name: salome-eriksson/downward-unsolvability
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-fast-downward\" class=\"anchor\" href=\"#fast-downward\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Fast Downward</h1>\n<p>Fast Downward is a domain-independent classical\
    \ planning system.</p>\n<p>Copyright 2003-2020 Fast Downward contributors (see\
    \ below).</p>\n<p>For further information:</p>\n<ul>\n<li>Fast Downward website:\
    \ <a href=\"http://www.fast-downward.org\" rel=\"nofollow\">http://www.fast-downward.org</a>\n\
    </li>\n<li>Report a bug or file an issue: <a href=\"http://issues.fast-downward.org\"\
    \ rel=\"nofollow\">http://issues.fast-downward.org</a>\n</li>\n<li>Fast Downward\
    \ mailing list: <a href=\"https://groups.google.com/forum/#!forum/fast-downward\"\
    \ rel=\"nofollow\">https://groups.google.com/forum/#!forum/fast-downward</a>\n\
    </li>\n<li>Fast Downward main repository: <a href=\"https://github.com/aibasel/downward\"\
    >https://github.com/aibasel/downward</a>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-tested-software-versions\"\
    \ class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Tested software\
    \ versions</h2>\n<p>This version of Fast Downward has been tested with the following\
    \ software versions:</p>\n<table>\n<thead>\n<tr>\n<th>OS</th>\n<th>Python</th>\n\
    <th>C++ compiler</th>\n<th>CMake</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Ubuntu\
    \ 20.04</td>\n<td>3.8</td>\n<td>GCC 9, GCC 10, Clang 10, Clang 11</td>\n<td>3.16</td>\n\
    </tr>\n<tr>\n<td>Ubuntu 18.04</td>\n<td>3.6</td>\n<td>GCC 7, Clang 6</td>\n<td>3.10</td>\n\
    </tr>\n<tr>\n<td>macOS 10.15</td>\n<td>3.6</td>\n<td>AppleClang 12</td>\n<td>3.19</td>\n\
    </tr>\n<tr>\n<td>Windows 10</td>\n<td>3.6</td>\n<td>Visual Studio Enterprise 2017\
    \ (MSVC 19.16) and 2019 (MSVC 19.28)</td>\n<td>3.19</td>\n</tr>\n</tbody>\n</table>\n\
    <p>We test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu,\
    \ we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and\
    \ on macOS, we do not test LP solvers (yet).</p>\n<h2>\n<a id=\"user-content-contributors\"\
    \ class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors</h2>\n<p>The following\
    \ list includes all people that actively contributed to\nFast Downward, i.e. all\
    \ people that appear in some commits in Fast\nDownward's history (see below for\
    \ a history on how Fast Downward\nemerged) or people that influenced the development\
    \ of such commits.\nCurrently, this list is sorted by the last year the person\
    \ has been\nactive, and in case of ties, by the earliest year the person started\n\
    contributing, and finally by last name.</p>\n<ul>\n<li>2003-2020 Malte Helmert</li>\n\
    <li>2008-2016, 2018-2020 Gabriele Roeger</li>\n<li>2010-2020 Jendrik Seipp</li>\n\
    <li>2010-2011, 2013-2020 Silvan Sievers</li>\n<li>2012-2020 Florian Pommerening</li>\n\
    <li>2013, 2015-2020 Salome Eriksson</li>\n<li>2016-2020 Cedric Geissmann</li>\n\
    <li>2017-2020 Guillem Franc\xE8s</li>\n<li>2018-2020 Augusto B. Corr\xEAa</li>\n\
    <li>2018-2020 Patrick Ferber</li>\n<li>2015-2019 Manuel Heusner</li>\n<li>2017\
    \ Daniel Killenberger</li>\n<li>2016 Yusra Alkhazraji</li>\n<li>2016 Martin Wehrle</li>\n\
    <li>2014-2015 Patrick von Reth</li>\n<li>2015 Thomas Keller</li>\n<li>2009-2014\
    \ Erez Karpas</li>\n<li>2014 Robert P. Goldman</li>\n<li>2010-2012 Andrew Coles</li>\n\
    <li>2010, 2012 Patrik Haslum</li>\n<li>2003-2011 Silvia Richter</li>\n<li>2009-2011\
    \ Emil Keyder</li>\n<li>2010-2011 Moritz Gronbach</li>\n<li>2010-2011 Manuela\
    \ Ortlieb</li>\n<li>2011 Vidal Alc\xE1zar Saiz</li>\n<li>2011 Michael Katz</li>\n\
    <li>2011 Raz Nissim</li>\n<li>2010 Moritz Goebelbecker</li>\n<li>2007-2009 Matthias\
    \ Westphal</li>\n<li>2009 Christian Muise</li>\n</ul>\n<h2>\n<a id=\"user-content-history\"\
    \ class=\"anchor\" href=\"#history\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>History</h2>\n<p>The current\
    \ version of Fast Downward is the merger of three different\nprojects:</p>\n<ul>\n\
    <li>the original version of Fast Downward developed by Malte Helmert\nand Silvia\
    \ Richter</li>\n<li>LAMA, developed by Silvia Richter and Matthias Westphal based\
    \ on\nthe original Fast Downward</li>\n<li>FD-Tech, a modified version of Fast\
    \ Downward developed by Erez\nKarpas and Michael Katz based on the original code</li>\n\
    </ul>\n<p>In addition to these three main sources, the codebase incorporates\n\
    code and features from numerous branches of the Fast Downward codebase\ndeveloped\
    \ for various research papers. The main contributors to these\nbranches are Malte\
    \ Helmert, Gabi R\xF6ger and Silvia Richter.</p>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>The following\
    \ directory is not part of Fast Downward as covered by\nthis license:</p>\n<ul>\n\
    <li>./src/search/ext</li>\n</ul>\n<p>For the rest, the following license applies:</p>\n\
    <pre><code>Fast Downward is free software: you can redistribute it and/or modify\n\
    it under the terms of the GNU General Public License as published by\nthe Free\
    \ Software Foundation, either version 3 of the License, or (at\nyour option) any\
    \ later version.\n\nFast Downward is distributed in the hope that it will be useful,\
    \ but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY\
    \ or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for\
    \ more details.\n\nYou should have received a copy of the GNU General Public License\n\
    along with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;.\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624374031.0
sassy-crick/Singularity-Easybuild:
  data_format: 2
  description: Collection of Singularity build files and scripts to create them for
    popular Linux Distributions
  filenames:
  - definitions/ruby/Singularity.Ruby-2.7.1-GCCcore-8.3.0-envmod-debian10
  - definitions/ruby/Singularity.Ruby-2.7.1-GCCcore-8.3.0-envmod-debian9
  - definitions/gemma/Singularity.GEMMA-0.98.1-foss-2018b-envmod-debian9
  - definitions/gemma/Singularity.GEMMA-0.98.1-foss-2018b-envmod-centos7
  - definitions/tabix/Singularity.tabix-0.2.6-GCCcore-7.3.0-envmod-debian9
  - definitions/HTSeq/Singularity.HTSeq-0.11.3-foss-2020b-centos-7-envmod
  - definitions/bcftools/Singularity.BCFtools-1.3-foss-2016b-envmod-centos7
  - definitions/bcftools/Singularity.BCFtools-1.10.2-GCC-8.3.0-envmod-debian10
  - definitions/bcftools/Singularity.BCFtools-1.10.2-GCC-8.3.0-envmod-debian9
  - definitions/fastqtl/Singularity.FastQTL-2.184-foss-2018b-envmod-centos7
  - definitions/plink/Singularity.PLINK-2.00a2.3LM-x86_64-lmod
  - definitions/plink/Singularity.PLINK-2.00-alpha2-x86_64-envmod-debian9
  - definitions/plink/Singularity.PLINK-2.00a2.3LM-x86_64-envmod-debian9
  - definitions/mirtk/Singularity.mirtk-2.0.0-foss-2020a-Python-3.8.2-envmod-centos7
  - definitions/bowtie/Singularity.Bowtie-1.2.3-foss-2018b-envmod-debian9
  - definitions/samtools/Singularity.SAMtools-1.10-GCC-9.3.0-envmod-debian10
  - definitions/gcc-core/Singularity.GCCcore-7.3.0-envmod-debian9
  - definitions/R/Singularity.R-3.6.3-foss-2020a-envmod-debian9
  - definitions/R/Singularity.R-3.6.2-foss-2019b-envmod-debian9
  - definitions/R/Singularity.R-4.0.0-foss-2020a-envmod-debian9
  - definitions/R/Singularity.R-3.6.2-foss-2020a-envmod-debian9
  - definitions/R/Singularity.R-3.6.0-foss-2018b-envmod-debian9
  - definitions/bwa/Singularity.BWA-0.7.17-foss-2018b-envmod-debian9
  - definitions/RSEM/Singularity.RSEM-1.3.3-foss-2019b-centos-7-envmod
  - definitions/salmon/Singularity.Salmon-1.3.0-gompi-2020a-envmod-debian9
  - definitions/salmon/Singularity.Salmon-1.1.0-gompi-2019b-envmod-debian9
  - definitions/salmon/Singularity.Salmon-1.0.0-gompi-2019a-envmod-debian9
  - definitions/salmon/Singularity.Salmon-1.2.1-gompi-2019b-envmod-debian9
  - definitions/meme/Singularity.MEME-5.1.1-foss-2019b-Perl-5.30.0-Python-3.7.4-envmod-debian10
  - definitions/steak/Singularity.STEAK-20190912-foss-2019b-Python-2.7.16-envmod-centos7
  - definitions/vcftools/Singularity.VCFtools-0.1.15-foss-2018a-Perl-5.26.1-envmod-centos7
  - definitions/gcc/Singularity.GCC-7.3.0-2.30-envmod-centos7
  - definitions/gcc/Singularity.GCC-9.2.0-2.32-envmod-centos7
  - definitions/gcc/Singularity.GCC-9.3.0-envmod-debian10
  - definitions/gcc/Singularity.GCC-9.3.0-envmod-centos7
  - definitions/gcc/Singularity.GCC-8.3.0-envmod-debian9
  - definitions/gcc/Singularity.GCC-9.3.0-envmod-debian9
  - definitions/foss/Singularity.foss-2018a-envmod-centos7
  - definitions/foss/Singularity.foss-2020a-envmod-debian9
  full_name: sassy-crick/Singularity-Easybuild
  latest_release: v1.0.0
  readme: "<h1>\n<a id=\"user-content-singularity-easybuild\" class=\"anchor\" href=\"\
    #singularity-easybuild\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Singularity-Easybuild</h1>\n<h2>\n<a id=\"user-content-description\"\
    \ class=\"anchor\" href=\"#description\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Description:</h2>\n<p>Collection\
    \ of Singularity definition files and scripts to create them for popular Linux\
    \ Distributions</p>\n<p>The definitions folder contains the successful Singularity\
    \ Definition files, tested with version 3.5.3 and 3.7.1, whereas the scripts folder\
    \ contains the scripts to create the Singularity definition files which are based\
    \ on EasyBuild. This version is using EasyBuild version 4.4.0.</p>\n<h2>\n<a id=\"\
    user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements:</h2>\n\
    <p>You will need to have a Linux environment and Singularity installed in it.\n\
    If you don't have Linux, please use Vagrant to set up a virtual Linux environment.</p>\n\
    <p>Furthermore, if you want to build the containers, you either need to have <code>fakeroot</code>\
    \ installed and configured so it can be used as normal user, or have <code>sudo</code>\
    \ installed. The latter is required if you want to open up containers and re-build\
    \ them again.</p>\n<p>As the software inside the containers is built using Easybuild,\
    \ you will need to know the names of the Easybuild Configuration files, e.g. GCC-9.3.0.eb.\n\
    Thus, it is probably best to install the easybuild-easyconfig files like this\
    \ in a separate folder:</p>\n<pre><code>$ git clone https://github.com/easybuilders/easybuild-easyconfigs.git\n\
    </code></pre>\n<p>and search the easybuild/easyconfigs folder for the name of\
    \ the EasyBuild Configuration files you want to use. You only need the name, not\
    \ the content of the files.</p>\n<p>The version of EasyBuild  is now fixed with\
    \ this release. If you require a specific version, simply change inside the Singularity\
    \ definition file this line:</p>\n<pre><code>pip3 install easybuild==4.4.0\n</code></pre>\n\
    <p>to this line:</p>\n<pre><code>pip3 install easybuild\n</code></pre>\n<p>which\
    \ will install the latest EasyBuild version.</p>\n<h2>\n<a id=\"user-content-depreciated-versions\"\
    \ class=\"anchor\" href=\"#depreciated-versions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Depreciated versions:</h2>\n\
    <p>As <code>Python2</code> is depreciated, the containers are using the <code>Python3</code>\
    \ version for as their default system version. Note:  This is different from the\
    \ Python versions EasyBuild will install and should not be mixed up.</p>\n<h2>\n\
    <a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage:</h2>\n\
    <p>Using the scripts is simple. Go into the <code>scripts</code> folder and run\
    \ the installation script <code>install.sh</code> This will <em>either</em> install\
    \ the scripts in your <code>~/bin</code> folder as sym-links, or create the sym-links\
    \ in the folder where you are running the script from. We advice you to install\
    \ the script in the <code>~/bin</code> folder so they are in your <code>PATH</code>\
    \ environment. If you don't want to do this, we recommend to install the sym-links\
    \ in a different folder from where you have downloaded the GitHub files from.\
    \ Please note the usage of sym-links. Thus, if you do any changes in the folder\
    \ where you downloaded the GitHub repository to, these changes will be carried\
    \ over. If, for example, you were to delete that folder, the installation is broken.</p>\n\
    <p>During the installation, you will given a number of choices regarding whether\
    \ you want to <em>build</em> or <em>create</em> the definition files, which Linux\
    \ distribution and version you want to use and if you want to use Lmod or the\
    \ environment-module system.</p>\n<p>You can then execute for example, assuming\
    \ the links are in your <code>PATH</code> environment:</p>\n<pre><code>$ container-create-debian11-envmodules.sh\
    \ GCC-9.3.0.eb\n</code></pre>\n<p>You can supply a second script as well, which\
    \ could be one you have created. This script will be\nread into the Singularity\
    \ Build file. So in our example we would get a file called <code>Singularity.GCC-9.3.0-envmod-debian11</code>.</p>\n\
    <p>If you want to build the container and additionally a sandbox, you could use\
    \ this instead:</p>\n<pre><code>$ container-build-debian11-envmodules.sh GCC-9.3.0.eb\n\
    </code></pre>\n<p>So in our example we would get a file called <code>Singularity.GCC-9.3.0-envmod-debian11</code>,\
    \ next to the Singularity container called <code>GCC-9.3.0-envmod-debian11.sif</code></p>\n\
    <p>If you don't want to or cannot use the automatic build process, you can build\
    \ the container like this:</p>\n<pre><code>$ sudo singularity build GCC-9.3.0-envmod-debian10.sif\
    \ Singularity.GCC-9.3.0-envmod-debian10\n</code></pre>\n<p>Equally, if you want\
    \ to install software on top of the existing container manually, simply do:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ sudo singularity build\
    \ --sandbox GCC-9.3.0-envmod-debian10.sif Singularity.GCC-9.3.0-envmod-debian10</pre></div>\n\
    <p>See the example below for a complete build of R-4.0.0 in two steps: We first\
    \ build the toolchain container (foss-2020a) and inside the container we build\
    \ R-4.0.0. This approach allows us to create our own complete environment for\
    \ building complete pipelines as well.</p>\n<p>If you want to have your name and\
    \ email address included in the Singularity definition file, just create this\
    \ file:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-k\">~</span>/.singularity/sing-eb.conf</pre></div>\n<p>An empty file will mean\
    \ these values are not included in the Singularity definition file. If you want\
    \ to include your name and email address, simply add it. Likewise, you can pin\
    \ the version of EasyBuild you want to use, like 4.4.0 in this example:</p>\n\
    <pre><code>name=\"Your Name\"\nemail=\"email@address\"\neb_version=\"EasyBuild-version\"\
    \n</code></pre>\n<p>and replace \"Your Name\" and \"email@address\" and the \"\
    EasyBuild-version\" you want to use accordingly.</p>\n<h2>\n<a id=\"user-content-example-build\"\
    \ class=\"anchor\" href=\"#example-build\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Example build:</h2>\n<p>This\
    \ would be the complete sequence to build a container with the FOSS-2020a tool\
    \ chain from EasyBuild, unpack the container and build for example R-4.0.0 inside\
    \ the container. Of course you could to that all in one go as well. We are using\
    \ the CentOS7 OS in this example:</p>\n<p>We first create the Singularity Definition\
    \ File. As we don't need to add a separate EasyBuild configuration file we say\
    \ 'n' here:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ container-create-centos8-envmodules.sh\
    \ foss-2020a.eb</pre></div>\n<p>Do we need a second Easybuild recipe (y/N)?: n</p>\n\
    <pre><code>$ ls\nSingularity.foss-2020a-centos-8-envmod\n</code></pre>\n<p>We\
    \ now build the Singularity container. Note that the command 'singularity' needs\
    \ to be in the\nPATH of root:</p>\n<pre><code>$ sudo singularity build foss-2020a-centos-8-envmod.sif\
    \ Singularity.foss-2020a-centos-8-envmod\n\n$ ls\nSingularity.foss-2020a-envmod-centos7\
    \ foss-2020a-envmod-centos7.sif \n</code></pre>\n<p>Now that we got a container,\
    \ we can unpack it so we can add software to it.\nFirst we unpack:</p>\n<pre><code>$\
    \ sudo singularity build --sandbox foss-2020a-centos-8-envmod foss-2020a-centos-8-envmod.sif\n\
    $ ls\nSingularity.foss-2020a-centos-8-envmod foss-2020a-centos-8-envmod.sif foss-2020a-centos-8-envmod\n\
    </code></pre>\n<p>Now we enter the container. The '-w' flag means we can write\
    \ to the pseudo-chroot environment:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ sudo singularity shell -w foss-2020a-centos-8-envmod </pre></div>\n<p>We\
    \ become the easybuild user and install the software. We first fetch all the source\
    \ files. This\nis sometimes a problem due to flaky Internet connections. We then,\
    \ in a second step, build the\nsoftware. This step can take some time but is done\
    \ fully automatic. Once build, we exit the\ncontainer again:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> su -l easybuild</span>\n[easybuild]$ eb --fetch R-4.0.0-foss-2020a.eb\n\
    [easybuild]$ eb  R-4.0.0-foss-2020a.eb\n[easybuild]$ <span class=\"pl-c1\">exit</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> exit</span></pre></div>\n<p>One\
    \ step we need to do here as root is, to change the environment file so the new\
    \ module will be loaded. This will be towards the bottom of this file:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>$ sudo vi foss-2020a-centos-8-envmod/environment</pre></div>\n\
    <p>Finally, we build the Singularity container:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ sudo singularity build  R-4.0.0-foss-2020a-centos-8-envmod .sif foss-2020a-centos-8-envmod\
    \ \n\n$ ls\nSingularity.foss-2020a-centos-8-envmod  foss-2020a-centos-8-envmod\
    \ .sif foss-2020a-centos-8-envmod  R-4.0.0-foss-2020a-centos-8-envmod .sif </pre></div>\n\
    <p>Now you can run R-4.0.0 on a different system like this:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>$ singularity <span class=\"pl-c1\">exec</span>\
    \ R-4.0.0-foss-2020a-centos-8-envmod.sif R\nR<span class=\"pl-k\">&gt;</span></pre></div>\n\
    <p>We have introduced a more automated way of building containers. Thus, instead\
    \ of doing all of the steps above manually, let the computer do it for you. Instead\
    \ of using a 'create' script, we are simply using a 'build' script, like this\
    \ for example:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ container-build-centos8-envmodules.sh\
    \ foss-2020a.eb</pre></div>\n<p>You will be asked whether or not to build the\
    \ sandbox in the same run.</p>\n<p>We would recommend to build a generic container\
    \ like <code>foss-2020b</code> for example and then use the provided sandbox to\
    \ add software to it.</p>\n<p>For more details about what you can do with Singularity\
    \ please refer to their home page.</p>\n<h2>\n<a id=\"user-content-acknowledgement\"\
    \ class=\"anchor\" href=\"#acknowledgement\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgement:</h2>\n<p>This\
    \ work would not be possible without EasyBuild,  I am greateful to the project\
    \ and the community for their help.</p>\n<h2>\n<a id=\"user-content-links\" class=\"\
    anchor\" href=\"#links\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Links:</h2>\n<p>Singularity: <a href=\"https://sylabs.io/guides/3.5/admin-guide/installation.html\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.5/admin-guide/installation.html</a><br>\n\
    Vagrant: <a href=\"https://www.vagrantup.com/intro/getting-started\" rel=\"nofollow\"\
    >https://www.vagrantup.com/intro/getting-started</a><br>\nEasybuild: <a href=\"\
    https://easybuild.readthedocs.io/en/latest\" rel=\"nofollow\">https://easybuild.readthedocs.io/en/latest</a></p>\n\
    <p>Updated: 7.6.2021</p>\n"
  stargazers_count: 5
  subscribers_count: 2
  topics: []
  updated_at: 1623235915.0
sbutcher/container-R:
  data_format: 2
  description: singularity container for use with singularity hub
  filenames:
  - Singularity
  full_name: sbutcher/container-R
  latest_release: null
  readme: '<h1>

    <a id="user-content-container-r" class="anchor" href="#container-r" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>container-R</h1>

    <p>singularity container for use with singularity hub</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1525440620.0
sbutcher/container-python:
  data_format: 2
  description: Python from source for use with singularity
  filenames:
  - Singularity
  full_name: sbutcher/container-python
  latest_release: null
  readme: '<h1>

    <a id="user-content-container-python" class="anchor" href="#container-python"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>container-python</h1>

    <p>Python from source for use with singularity</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1525427896.0
sbutcher/container-setc:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: sbutcher/container-setc
  latest_release: null
  readme: '<h1>

    <a id="user-content-container-setc" class="anchor" href="#container-setc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>container-setc</h1>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1538491698.0
sbutcher/game-container:
  data_format: 2
  description: Containers for game AI
  filenames:
  - Singularity
  full_name: sbutcher/game-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-game-container" class="anchor" href="#game-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>game-container</h1>

    <p>Containers for game AI</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1547647598.0
sbutcher/minigym-container:
  data_format: 2
  description: Container to run various Game AI workloads
  filenames:
  - Singularity
  full_name: sbutcher/minigym-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-minigym-container" class="anchor" href="#minigym-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>minigym-container</h1>

    <p>Container to run various Game AI workloads</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1548062559.0
sc-eQTLgen-consortium/WG1-pipeline-QC:
  data_format: 2
  description: Part of the sc-eQTLgen consortium pipeline. Step 1, where the QC is
    done.
  filenames:
  - Singularity.WGpipeline
  - Singularity.Imputation
  full_name: sc-eQTLgen-consortium/WG1-pipeline-QC
  latest_release: null
  readme: '<h1>

    <a id="user-content-wg1-pipeline-qc" class="anchor" href="#wg1-pipeline-qc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>WG1-pipeline-QC</h1>

    <p><a href="https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png"
    target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png"
    width="300" height="140" style="max-width:100%;"></a></p>

    <p>Part of the sceQTL-Gen consortium pipeline. Step 1, where the QC is done.</p>

    <p>Please see the <a href="https://github.com/sc-eQTLgen-consortium/WG1-pipeline-QC/wiki">Wiki</a>
    for information on running the QC pipeline.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1623743752.0
seasite-project/CGO21_YaskSite_AD:
  data_format: 2
  description: "This is the Artifact Description repository for the CGO21 paper: YaskSite\
    \ \u2013 Stencil Optimization Techniques Applied to Explicit ODE Methods on Modern\
    \ Architectures"
  filenames:
  - Singularity
  full_name: seasite-project/CGO21_YaskSite_AD
  latest_release: CGO21v0.3
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1609764345.0
seb-mueller/singularity_dropSeqPipe:
  data_format: 2
  description: Singularity container for dropSeqPipe
  filenames:
  - Singularity
  - Singularity.v04
  full_name: seb-mueller/singularity_dropSeqPipe
  latest_release: null
  readme: "<p><a href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\"\
    \ alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HEMnet - Haematoxylin &amp; Eosin and Molecular neural network</h1>\n\
    <h2>\n<a id=\"user-content-description\" class=\"anchor\" href=\"#description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Description</h2>\n<p>A deep learning automated cancer diagnosis software\
    \ using molecular labelling to improve pathological annotation of\nHaematoxylin\
    \ and Eosin (H&amp;E) stained tissue.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<ol>\n<li>\n\
    <p>Docker</p>\n<p>You can download and run the docker image using the following\
    \ commands:</p>\n<pre><code>docker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\
    </code></pre>\n</li>\n<li>\n<p>Conda</p>\n<p>Install Openslide (this is necessary\
    \ to open whole slide images) - download it <a href=\"https://openslide.org/download/\"\
    \ rel=\"nofollow\">here</a></p>\n<p>Create a conda environment from the <code>environment.yml</code>\
    \ file</p>\n<pre><code>conda env create -f environment.yml\nconda activate HEMnet\n\
    </code></pre>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-slide-preparation\"\
    \ class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Slide Preparation</h3>\n<p>Name\
    \ slides in the format: <code>slide_id_TP53</code> for TP53 slides and <code>slide_id_HandE</code>\
    \ for H&amp;E slides\nThe <code>TP53</code> and <code>HandE</code> suffix is used\
    \ by HEMnet to identify the stain used.</p>\n<h3>\n<a id=\"user-content-1-generate-training-and-testing-datasets\"\
    \ class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.\
    \ Generate training and testing datasets</h3>\n<p>a. Generate train dataset</p>\n\
    <p><code>python HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides\
    \ -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs\
    \ -v</code></p>\n<p>b. Generate test dataset</p>\n<p><code>python HEMnet_test_dataset.py\
    \ -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory\
    \ -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh\
    \ -n non_cancer_thresh</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is the relative path to the template slide from which all other slides will\
    \ be normalised against. The template\nslide should be the same for each step.</li>\n\
    <li>\n<code>-m</code> is the tile magnification. e.g. if  the input is <code>10</code>\
    \ then the tiles will be output at 10x</li>\n<li>\n<code>-a</code> is the align\
    \ magnification. Paired TP53 and H&amp;E slides will be registered at this magnification.\n\
    To reduce computation time we recommend this be less than the tile magnification\
    \ - a five times downscale generally works well.</li>\n<li>\n<code>-c</code> cancer\
    \ threshold to apply to the DAB channel. DAB intensities less than this threshold\
    \ indicate cancer.</li>\n<li>\n<code>-n</code> non-cancer threshold to apply to\
    \ the DAB channel. DAB intensities greater than this threshold indicate no cancer.</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\"\
    \ href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2. Train and evaluate model</h3>\n\
    <p>a. Training model</p>\n<p><code>python train.py -b /path/to/base/directory\
    \ -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory\
    \ -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size\
    \ -s -w -f -v</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-m</code>\
    \ is CNN base model. eg. <code>resnet50</code>, <code>vgg16</code>, <code>vgg19</code>,\
    \ <code>inception_v3</code> and <code>xception</code>.</li>\n<li>\n<code>-g</code>\
    \ is number of GPUs for training.</li>\n<li>\n<code>-e</code> is training epochs.\
    \ Default is <code>100</code> epochs.</li>\n<li>\n<code>-a</code> is batch size.\
    \ Default is <code>32</code>\n</li>\n<li>\n<code>-s</code> is option to save the\
    \ trained model weights.</li>\n<li>\n<code>-w</code> is option to used transfer\
    \ learning. Model will used pre-trained weights from ImageNet at the initial stage.</li>\n\
    <li>\n<code>-f</code> is fine-tuning option. Model will re-train CNN base.</li>\n\
    </ul>\n<p>b. Test model prediction</p>\n<p><code>python test.py  -b /path/to/base/directory\
    \ -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory\
    \ -w model_weights -m cnn_base -g num_gpus -v</code></p>\n<p>Other parameters:</p>\n\
    <ul>\n<li>\n<code>-w</code> is path to trained model. eg. <code>trained_model.h5</code>.</li>\n\
    <li>\n<code>-m</code> is CNN base model (same to training step).</li>\n<li>\n\
    <code>-g</code> is number of GPUs for prediction.</li>\n</ul>\n<p>c. Evaluate\
    \ model performance and visualise model prediction</p>\n<p><code>python visualisation.py\
    \ -b /path/to/base/directory -t /relative/path/to/training_output_directory -p\
    \ /relative/path/to/test_output_directory  -o /relative/path/to/output/directory\
    \ -i sample</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is path to training outputs.</li>\n<li>\n<code>-p</code> is path to test outputs.</li>\n\
    <li>\n<code>-i</code> is name of Whole Slide Image for visualisation.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\"\
    \ href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3. Apply model to diagnose new\
    \ images</h3>\n<p><code>python HEMnet_inference.py -s '/path/to/new/HE/Slides/'\
    \ -o '/path/to/output/directory/' -t '/path/to/template/slide/' -nn '/path/to/trained/model/'\
    \ -v</code></p>\n<p>Predict on TCGA images with our pretrained model for colorectal\
    \ cancer using <a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\">google colab</a></p>\n<h2>\n<a id=\"user-content-results\"\
    \ class=\"anchor\" href=\"#results\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Results</h2>\n<h2>\n<a id=\"\
    user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citing\
    \ HEMnet</h2>\n<h2>\n<a id=\"user-content-the-team\" class=\"anchor\" href=\"\
    #the-team\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>The Team</h2>\n<p>Please contact Dr Quan Nguyen (<a href=\"mailto:quan.nguyen@uq.edu.au\"\
    >quan.nguyen@uq.edu.au</a>), Andrew Su (<a href=\"mailto:a.su@uqconnect.edu.au\"\
    >a.su@uqconnect.edu.au</a>),\nand Xiao Tan (<a href=\"mailto:xiao.tan@uqconnect.edu.au\"\
    >xiao.tan@uqconnect.edu.au</a>) for issues, suggestions,\nand we are very welcome\
    \ to collaboration opportunities.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1569595505.0
sghignone/Rnnotator:
  data_format: 2
  description: Rnnotator is an automated software pipeline that generates transcript
    models by de novo assembly of RNA-Seq data without the need for a reference genome.
  filenames:
  - Singularity
  full_name: sghignone/Rnnotator
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-rnnotator\" class=\"anchor\" href=\"#rnnotator\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Rnnotator</h1>\n<p>Rnnotator is an automated software pipeline that\
    \ generates transcript models by de novo assembly of RNA-Seq data without the\
    \ need for a reference genome.</p>\n<p>Rnnotator must be run on a 64-bit Linux\
    \ architecture. Before running Rnnotator the\nfollowing prerequisites must be\
    \ installed:</p>\n<ul>\n<li>Blat v. 34 (<a href=\"http://genome.ucsc.edu/FAQ/FAQblat.html#blat3\"\
    \ rel=\"nofollow\">http://genome.ucsc.edu/FAQ/FAQblat.html#blat3</a>) -- DONE</li>\n\
    <li>Velvet 1.0.15 (<a href=\"http://www.ebi.ac.uk/~zerbino/velvet/\" rel=\"nofollow\"\
    >http://www.ebi.ac.uk/~zerbino/velvet/</a>) -- DONE</li>\n<li>AMOS (<a href=\"\
    http://sourceforge.net/apps/mediawiki/amos/index.php\" rel=\"nofollow\">http://sourceforge.net/apps/mediawiki/amos/index.php</a>)\
    \ -- DONE</li>\n<li>Vmatch 2.0 (<a href=\"http://www.vmatch.de/\" rel=\"nofollow\"\
    >http://www.vmatch.de/</a>) -- DONE</li>\n<li>bwa 0.5.8c (<a href=\"http://bio-bwa.sourceforge.net/\"\
    \ rel=\"nofollow\">http://bio-bwa.sourceforge.net/</a>) -- DONE</li>\n<li>MUMmer\
    \ (<a href=\"http://sourceforge.net/projects/mummer/\" rel=\"nofollow\">http://sourceforge.net/projects/mummer/</a>)\
    \ -- DONE</li>\n<li>BioPerl (<a href=\"http://www.bioperl.org\" rel=\"nofollow\"\
    >http://www.bioperl.org</a>) -- base system</li>\n<li>Perl modules: Parallel::ForkManager,\
    \ Tree (<a href=\"http://search.cpan.org/\" rel=\"nofollow\">http://search.cpan.org/</a>)\
    \ -- DONE</li>\n</ul>\n<p>Optional prerequisites are:</p>\n<ul>\n<li>Oases 0.1.18\
    \ (<a href=\"http://www.ebi.ac.uk/~zerbino/oases/\" rel=\"nofollow\">http://www.ebi.ac.uk/~zerbino/oases/</a>)\
    \ -- DONE</li>\n<li>Bambus 2.33 (<a href=\"http://www.cbcb.umd.edu/software/bambus/\"\
    \ rel=\"nofollow\">http://www.cbcb.umd.edu/software/bambus/</a>)</li>\n<li>Sopra\
    \ 1.0 (<a href=\"mailto:dayarian@physics.rutgers.edu\">dayarian@physics.rutgers.edu</a>)\
    \ x1 \u2013 x4 scripts</li>\n</ul>\n<p>sg</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - pipeline
  - singularity
  - singularity-recipe
  - rnaseq
  - docker
  - dockerfile
  updated_at: 1612716290.0
shreyaskamathkm/Cluster_Images:
  data_format: 2
  description: This contains the latest docker and singularity images
  filenames:
  - Singularity_Ubuntu_18_04_Cuda_11_1
  - Singularity_Ubuntu_20_04_Cuda_11_1
  - Singularity_Ubuntu_16_04
  - Singularity_Ubuntu_18_04_Cuda_10_2
  full_name: shreyaskamathkm/Cluster_Images
  latest_release: null
  readme: '<h1>

    <a id="user-content-examples-with-drake-and-ros-2" class="anchor" href="#examples-with-drake-and-ros-2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Examples
    with Drake and ROS 2</h1>

    <p>This repo shows examples of using <a href="https://drake.mit.edu/" rel="nofollow">Drake</a>
    and <a href="https://www.ros.org/" rel="nofollow">ROS 2</a> together.

    It uses the pydrake API and is of prototype quality.

    For a similar effort in ROS 1, see <a href="https://github.com/EricCousineau-TRI/repro/tree/master/ros/drake_ros1_hacks">EricCousineau-TRI/repro
    <code>drake_ros1_hacks</code></a>.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <ul>

    <li>Ubuntu Focal (20.04)</li>

    <li><a href="https://index.ros.org/doc/ros2/Installation/Rolling/" rel="nofollow">ROS
    2 Rolling</a></li>

    <li><a href="https://drake.mit.edu/from_binary.html" rel="nofollow">Some Drake
    binary installation from October 2020</a></li>

    </ul>

    <p>Install ROS 2 Rolling using the <a href="https://index.ros.org/doc/ros2/Installation/Rolling/Linux-Install-Debians/"
    rel="nofollow">Linux binary instructions</a> and <a href="https://index.ros.org/doc/ros2/Installation/Prerelease-Testing/"
    rel="nofollow">enable the ROS 2 testing apt repo</a>.

    Install the apt packages <code>ros-rolling-desktop</code> and <code>ros-rolling-sdformat-urdf</code>.

    Extract the Drake binary installation, install it''s prerequisites, and <a href="https://drake.mit.edu/python_bindings.html#inside-virtualenv"
    rel="nofollow">use this Python virutalenv trick</a>.</p>

    <h2>

    <a id="user-content-setup" class="anchor" href="#setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h2>

    <p>Once the prerequisites are met, install <code>drake_ros</code> into the Drake
    virtualenv.</p>

    <pre><code>. path/to/drake/bin/activate

    cd path/to/this/repo

    cd drake_ros/

    python setup.py develop

    </code></pre>

    <h2>

    <a id="user-content-ros-2-tf-and-robot-model-demo" class="anchor" href="#ros-2-tf-and-robot-model-demo"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ROS
    2 tf and Robot Model Demo</h2>

    <p>This demo shows RViz visualizing a single UR10 robot being simulated by Drake.

    Set up two terminals: one for launching RViz, and another for launching the Drake
    simulation.</p>

    <pre><code>. /opt/ros/rolling/setup.bash

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" rviz2 -d view.rviz

    </code></pre>

    <pre><code>. /opt/ros/rolling/setup.bash

    . path/to/drake/bin/activate

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" ./ros2_demo.py

    </code></pre>

    <p><a href="https://user-images.githubusercontent.com/4175662/90415417-e7976980-e065-11ea-9564-96c820f51680.gif"
    target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/4175662/90415417-e7976980-e065-11ea-9564-96c820f51680.gif"
    alt="ur10_rviz_drake" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-interactive-markers-demo" class="anchor" href="#interactive-markers-demo"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interactive
    Markers Demo</h2>

    <p>This demonstrates using interactive markers to control an iiwa14 being simulated
    by Drake.

    Set up two terminals: one for launching RViz, and another for launching the Drake
    simulation.</p>

    <pre><code>. /opt/ros/rolling/setup.bash

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" rviz2 -d interactive_demo.rviz

    </code></pre>

    <pre><code>. /opt/ros/rolling/setup.bash

    . path/to/drake/bin/activate

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" ./interactive_demo.py

    </code></pre>

    <p><a href="https://user-images.githubusercontent.com/4175662/96510753-dcea8380-1212-11eb-89ca-4a9019a8a9cd.gif"
    target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/4175662/96510753-dcea8380-1212-11eb-89ca-4a9019a8a9cd.gif"
    alt="iiwa14_interactive_drake" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-using-a-container" class="anchor" href="#using-a-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    a Container</h2>

    <p>There is a definition file for a <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>
    container.</p>

    <p>First <a href="https://sylabs.io/guides/3.7/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">build and install Singularity</a>.

    Afterwards, build a Singularity sandbox from the definition file.</p>

    <pre><code>singularity build --fakeroot --sandbox ~/drake-ros2-demos.sandbox demos.singularity.def

    </code></pre>

    <p>Create a shell with access to an NVidia graphics card and run one of the RViz
    configs for your chosen demo.</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    shell --nv --writable-tmpfs <span class="pl-k">~</span>/drake-ros2-demos.sandbox</span>

    <span class="pl-c1">Singularity&gt; rviz2 -d view.rviz</span></pre></div>

    <p>Create a shell into the sandbox and run one of the demos.</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    shell --writable <span class="pl-k">~</span>/drake-ros2-demos.sandbox</span>

    <span class="pl-c1">Singularity&gt; ./ros2_demo.py</span></pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616693700.0
shreyaskamathkm/singularity_meshroom:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: shreyaskamathkm/singularity_meshroom
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_meshroom" class="anchor" href="#singularity_meshroom"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_meshroom</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602807348.0
shrutir11/lolcow:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: shrutir11/lolcow
  latest_release: null
  readme: '<h1>

    <a id="user-content-lolcow" class="anchor" href="#lolcow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>lolcow</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1624383824.0
shub-fuzz/afl:
  data_format: 2
  description: Singularity Image for AFL (https://github.com/google/AFL)
  filenames:
  - Singularity.i386
  - Singularity.1604
  - Singularity.1804
  full_name: shub-fuzz/afl
  latest_release: 0.0.2
  readme: '<p>Singularity Image for AFL (<a href="https://github.com/google/AFL">https://github.com/google/AFL</a>)</p>

    <p><a href="https://github.com/shub-fuzz/afl/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/afl/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    <ul>

    <li>

    <p><strong>What</strong> is <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>?<br>

    A containerization system primarily used by the scientific community on high-performance
    computing (HPC).

    On many University HPC systems, docker is not allowed, but singularity is availble
    because it runs with

    user level permisions.</p>

    </li>

    <li>

    <p><strong>Why</strong>?<br>

    Fuzzing on HPC!<br>

    Universities have trememdous resources available in HPC clusters that can be used
    to support

    large-scale fuzzing evaluations.</p>

    </li>

    <li>

    <p>usage:</p>

    </li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name afl.sif
    https://github.com/shub-fuzz/afl/releases/download/0.0.2/shub-fuzz-afl.1604.sif


    singularity shell afl.sif</pre></div>

    <ul>

    <li>pull Ubuntu 18.04 container</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name afl.1804.sif
    https://github.com/shub-fuzz/afl/releases/download/0.0.2/shub-fuzz-afl.1804.sif</pre></div>

    <ul>

    <li>pull Ubuntu 16.04 i386 container</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name afl_i386.sif
    https://github.com/shub-fuzz/afl/releases/download/0.0.2/shub-fuzz-afl.i386.sif


    singularity pull --name afl_i386.sif shub://shub-fuzz/afl:i386</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623682579.0
shub-fuzz/aflpp:
  data_format: 2
  description: Singularity image for afl++ (https://github.com/AFLplusplus/AFLplusplus)
  filenames:
  - Singularity.i386
  - Singularity.2004
  - Singularity.1604
  - Singularity.1804
  full_name: shub-fuzz/aflpp
  latest_release: 0.0.2
  readme: '<p>Singularity image for AFL++ (<a href="https://github.com/AFLplusplus/AFLplusplus">https://github.com/AFLplusplus/AFLplusplus</a>)</p>

    <p><a href="https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    <ul>

    <li>

    <p><strong>What</strong> is <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>?<br>

    A containerization system primarily used by the scientific community on high-performance
    computing (HPC).

    On many University HPC systems, docker is not allowed, but singularity is availble
    because it runs with

    user level permisions.</p>

    </li>

    <li>

    <p><strong>Why</strong>?<br>

    Fuzzing on HPC!<br>

    Universities have trememdous resources available in HPC clusters that can be used
    to support

    large-scale fuzzing evaluations.</p>

    </li>

    <li>

    <p>usage:</p>

    </li>

    </ul>

    <pre><code>singularity pull --name aflpp.sif https://github.com/shub-fuzz/aflpp/releases/download/0.0.2/shub-fuzz-aflpp.1604.sif


    singularity shell aflpp.sif

    </code></pre>

    <ul>

    <li>pull Ubuntu 18.04 container</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name aflpp.1804.sif
    https://github.com/shub-fuzz/aflpp/releases/download/0.0.2/shub-fuzz-aflpp.1804.sif</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623682681.0
shub-fuzz/angora:
  data_format: 2
  description: Singularity image for Angora (https://github.com/AngoraFuzzer/Angora)
  filenames:
  - Singularity.1604
  - Singularity.1804
  full_name: shub-fuzz/angora
  latest_release: 0.0.2
  readme: "<p>Singularity image for Angora (<a href=\"https://github.com/AngoraFuzzer/Angora\"\
    >https://github.com/AngoraFuzzer/Angora</a>)</p>\n<p><a href=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml\"\
    ><img src=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml/badge.svg?branch=main\"\
    \ alt=\"singularity-deploy\" style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/3645\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<ul>\n<li>\n<p><strong>What</strong> is\
    \ <a href=\"https://sylabs.io/singularity/\" rel=\"nofollow\">Singularity</a>?<br>\n\
    A containerization system primarily used by the scientific community on high-performance\
    \ computing (HPC).\nOn many University HPC systems, docker is not allowed, but\
    \ singularity is availble because it runs with\nuser level permisions.</p>\n</li>\n\
    <li>\n<p><strong>Why</strong>?<br>\nFuzzing on HPC!<br>\nUniversities have trememdous\
    \ resources available in HPC clusters that can be used to support\nlarge-scale\
    \ fuzzing evaluations.</p>\n</li>\n<li>\n<p>usage:</p>\n</li>\n</ul>\n<pre><code>singularity\
    \ pull --name angora.sif https://github.com/shub-fuzz/angora/releases/download/0.0.2/shub-fuzz-angora.1604.sif\n\
    \nsingularity shell angora.sif\n</code></pre>\n<ul>\n<li>interactive session:</li>\n\
    </ul>\n<pre><code>singularity shell angora.sif \n</code></pre>\n<ul>\n<li>start\
    \ fuzzing</li>\n</ul>\n<pre><code>singularity exec angora.sif /start_fuzzing [[\
    \ -n &lt;# instances&gt; ]  -t ] &lt;target_path&gt; \n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623682691.0
shub-fuzz/ankou:
  data_format: 2
  description: Singularity image for Ankou (https://github.com/SoftSec-KAIST/Ankou)
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/ankou
  latest_release: 0.0.2
  readme: '<p>Singularity image for Ankou (<a href="https://github.com/SoftSec-KAIST/Ankou">https://github.com/SoftSec-KAIST/Ankou</a>)</p>

    <p><a href="https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/4173" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li>

    <p><strong>What</strong> is <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>?<br>

    A containerization system primarily used by the scientific community on high-performance
    computing (HPC).

    On many University HPC systems, docker is not allowed, but singularity is availble
    because it runs with

    user level permisions.</p>

    </li>

    <li>

    <p><strong>Why</strong>?<br>

    Fuzzing on HPC!<br>

    Universities have trememdous resources available in HPC clusters that can be used
    to support

    large-scale fuzzing evaluations.</p>

    </li>

    <li>

    <p>usage:</p>

    </li>

    </ul>

    <pre><code>singularity pull --name ankou.sif https://github.com/shub-fuzz/ankou/releases/download/0.0.2/shub-fuzz-ankou.1604.sif


    singularity shell ankou.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623682696.0
shub-fuzz/eclipser:
  data_format: 2
  description: Singularity image for Eclipser (https://github.com/SoftSec-KAIST/Eclipser)
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/eclipser
  latest_release: 0.0.2
  readme: '<p>Singularity image for Eclipser (<a href="https://github.com/SoftSec-KAIST/Eclipser">https://github.com/SoftSec-KAIST/Eclipser</a>)</p>

    <p><a href="https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    <ul>

    <li>

    <p><strong>What</strong> is <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>?<br>

    A containerization system primarily used by the scientific community on high-performance
    computing (HPC).

    On many University HPC systems, docker is not allowed, but singularity is availble
    because it runs with

    user level permisions.</p>

    </li>

    <li>

    <p><strong>Why</strong>?<br>

    Fuzzing on HPC!<br>

    Universities have trememdous resources available in HPC clusters that can be used
    to support

    large-scale fuzzing evaluations.</p>

    </li>

    <li>

    <p>usage:</p>

    </li>

    </ul>

    <pre><code>singularity pull --name eclipser.sif https://github.com/shub-fuzz/eclipser/releases/download/0.0.2/shub-fuzz-eclipser.1604.sif


    singularity shell eclipser.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623682705.0
shub-fuzz/honggfuzz:
  data_format: 2
  description: Singularity image for honggfuzz (https://github.com/google/honggfuzz)
  filenames:
  - Singularity.i386
  - Singularity.1604
  - Singularity.1804
  - v21/Singularity.v21
  full_name: shub-fuzz/honggfuzz
  latest_release: 0.0.2
  readme: '<p><a href="https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3641" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li>

    <strong>What</strong> is <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>?<br>

    A containerization system primarily used by the scientific community on high-performance
    computing (HPC).

    On many University HPC systems, docker is not allowed, but singularity is availble
    because it runs with

    user level permisions.</li>

    <li>

    <strong>Why</strong>?<br>

    Fuzzing on HPC!<br>

    Universities have trememdous resources available in HPC clusters that can be used
    to support

    large-scale fuzzing evaluations.</li>

    </ul>

    <p>Singularity image for honggfuzz (<a href="https://github.com/google/honggfuzz">https://github.com/google/honggfuzz</a>)</p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name honggfuzz.sif https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.2/shub-fuzz-honggfuzz.1604.sif


    singularity shell honggfuzz.sif

    </code></pre>

    <ul>

    <li>pull Ubuntu 18.04 container</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name honggfuzz.1804.sif
    https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.2/shub-fuzz-honggfuzz.1804.sif</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623682711.0
shub-fuzz/qsym:
  data_format: 2
  description: QSYM  - Concolic Execution Engine (https://github.com/sslab-gatech/qsym)
  filenames:
  - Singularity.1604
  - Singularity.1804
  full_name: shub-fuzz/qsym
  latest_release: 0.0.2
  readme: '<p>Singularity Image for QSYM (<a href="https://github.com/sslab-gatech/qsym">https://github.com/sslab-gatech/qsym</a>)</p>

    <p><a href="https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3625" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li>

    <strong>What</strong> is <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>?<br>

    A containerization system primarily used by the scientific community on high-performance
    computing (HPC).

    On many University HPC systems, docker is not allowed, but singularity is availble
    because it runs with

    user level permisions.</li>

    <li>

    <strong>Why</strong>?<br>

    Fuzzing on HPC!<br>

    Universities have trememdous resources available in HPC clusters that can be used
    to support

    large-scale fuzzing evaluations.</li>

    </ul>

    <p>QSYM  - Concolic Execution Engine (<a href="https://github.com/sslab-gatech/qsym">https://github.com/sslab-gatech/qsym</a>)</p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name qsym.sif https://github.com/shub-fuzz/qsym/releases/download/0.0.2/shub-fuzz-qsym.1604.sif


    singularity shell qsym.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623682731.0
simarocchi/openmpi_centos7_x86_64:
  data_format: 2
  description: a Singularity recipe with openmpi 2.1.1 on base centos 7 to run on
    the Cineca clusters x86_64 based
  filenames:
  - Singularity
  full_name: simarocchi/openmpi_centos7_x86_64
  latest_release: null
  readme: '<h1>

    <a id="user-content-openmpi_centos7_x86_64" class="anchor" href="#openmpi_centos7_x86_64"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>openmpi_centos7_x86_64</h1>

    <p>a Singularity recipe with openmpi 2.1.1 on base centos 7 to run on the Cineca
    clusters x86_64 based</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605098444.0
sina-ehsani/hpc-singularity:
  data_format: 2
  description: Singularity for HPC
  filenames:
  - Singularity.centos7-python3.7-transformers2.11.0-ImageCrawl
  - Singularity.centos7-python3.7-transformers3.0.2-ImageCrawl
  full_name: sina-ehsani/hpc-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc-singularity" class="anchor" href="#hpc-singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>hpc-singularity</h1>

    <p>Singularity for HPC</p>

    <p>Make sure the sigularity is built on <a href="https://singularity-hub.org"
    rel="nofollow">https://singularity-hub.org</a></p>

    <p>if ready use:</p>

    <p><code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers3.0.2-imagecrawl</code></p>

    <p>Transformer 2.11.0:

    <code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers2.11.0-imagecrawl</code></p>

    <p>Make sure the imagecrawl is updated (latest commit)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1601682764.0
singularityhub/circle-ci:
  data_format: 2
  description: example builder for Singularity containers using Circle Continuous
    Integration, circle-ci
  filenames:
  - Singularity.tag
  - Singularity
  full_name: singularityhub/circle-ci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-builder-circle-ci\" class=\"anchor\"\
    \ href=\"#singularity-builder-circle-ci\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity Builder Circle-CI</h1>\n\
    <p><a href=\".circleci/sregistry-circle.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\".circleci/sregistry-circle.png\" alt=\".circleci/sregistry-circle.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://circleci.com/gh/singularityhub/circle-ci\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/bb4face391e298efe97092a4f2484374be5bc661342e9575c47b82ada2df4772/68747470733a2f2f636972636c6563692e636f6d2f67682f73696e67756c61726974796875622f636972636c652d63692e7376673f7374796c653d737667\"\
    \ alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/singularityhub/circle-ci.svg?style=svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>This is a simple example of how you can\
    \ achieve:</p>\n<ul>\n<li>version control of your recipes</li>\n<li>versioning\
    \ to include image hash <em>and</em> commit id</li>\n<li>build of associated container\
    \ and</li>\n<li>push to a storage endpoint</li>\n</ul>\n<p>for a reproducible\
    \ build workflow.</p>\n<p><strong>Why should this be managed via Github?</strong></p>\n\
    <p>Github, by way of easy integration with continuous integration, is an easy\
    \ way\nto have a workflow set up where multiple people can collaborate on a container\
    \ recipe,\nthe recipe can be tested (with whatever testing you need), discussed\
    \ in pull requests,\nand then finally pushed to your storage of choice or Singularity\
    \ Registry.\nImportantly, you don't need to give your entire team manager permissions\n\
    to the registry. An encrypted credential that only is accessible to\nadministrators\
    \ can do the push upon merge of a discussed change.</p>\n<p><strong>Why should\
    \ I use this instead of a service?</strong></p>\n<p>You could use a remote builder,\
    \ but if you do the build in a continuous integration\nservice you get complete\
    \ control over it. This means everything from the version of\nSingularity to use,\
    \ to the tests that you run for your container. You have a lot more\nfreedom in\
    \ the rate of building, and organization of your repository, because it's you\n\
    that writes the configuration. Although the default would work for most, you can\n\
    edit the build, setup, and circle configuration file in the\n<a href=\".circleci\"\
    >.circleci</a> folder to fit your needs.</p>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick Start</h2>\n<p>Add your\
    \ Singularity recipes to this repository, and edit the build commands in\nthe\
    \ <a href=\".circleci/build.sh\">build.sh</a> file. This is where you can specify\
    \ endpoints\n(Singularity Registry, Dropbox, Google Storage, AWS) along with container\
    \ names\n(the uri) and tag. You can build as many recipes as you like, just add\
    \ another line!</p>\n<div class=\"highlight highlight-source-yaml\"><pre>    \
    \                           <span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ recipe relative to repository base</span>\n  - <span class=\"pl-s\">/bin/bash\
    \ .circleci/build.sh Singularity</span>\n  - <span class=\"pl-s\">/bin/bash .circleci/build.sh\
    \ --uri collection/container --tag tacos --cli google-storage Singularity</span>\n\
    \  - <span class=\"pl-s\">/bin/bash .circleci/build.sh --uri collection/container\
    \ --cli google-drive Singularity</span>\n  - <span class=\"pl-s\">/bin/bash .circleci/build.sh\
    \ --uri collection/container --cli globus Singularity</span>\n  - <span class=\"\
    pl-s\">/bin/bash .circleci/build.sh --uri collection/container --cli registry\
    \ Singularity</span></pre></div>\n<p>For each client that you use, required environment\
    \ variables (e.g., credentials to push,\nor interact with the API) must be defined\
    \ in the (encrypted) Travis environment. To\nknow what variables to define, along\
    \ with usage for the various clients, see\nthe <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">client specific pages</a></p>\n<h2>\n<a id=\"user-content-detailed-started\"\
    \ class=\"anchor\" href=\"#detailed-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Detailed Started</h2>\n<h3>\n\
    <a id=\"user-content-0-fork-this-repository\" class=\"anchor\" href=\"#0-fork-this-repository\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>0. Fork this repository</h3>\n<p>You can clone and tweak, but it's\
    \ easiest likely to get started with our example\nfiles and edit them as you need.</p>\n\
    <h3>\n<a id=\"user-content-1-get-to-know-circleci\" class=\"anchor\" href=\"#1-get-to-know-circleci\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1. Get to Know CircleCi</h3>\n<p>We will be working with <a href=\"\
    https://www.circleci.com\" rel=\"nofollow\">Circle CI</a>. You can see\nexample\
    \ builds for this <a href=\"https://circleci.com/gh/singularityhub/circle-ci\"\
    \ rel=\"nofollow\">repository here</a>.</p>\n<ul>\n<li>Circle offers <a href=\"\
    https://support.circleci.com/hc/en-us/articles/115015481128-Scheduling-jobs-cron-for-builds-\"\
    \ rel=\"nofollow\">scheduled builds</a>.</li>\n<li>CircleCI also offers <a href=\"\
    https://circleci.com/docs/enterprise/gpu-configuration/\" rel=\"nofollow\">GPU\
    \ Builders</a> if you want/need that sort of thing.</li>\n<li>If you don't want\
    \ to use the <a href=\"https://singularityhub.github.io/sregistry-cli\" rel=\"\
    nofollow\">sregistry</a> to push to Google Storage, Drive, Globus, Dropbox, or\
    \ your personal Singularity Registry, CircleCI will upload your artifacts directly\
    \ to your <a href=\"https://circleci.com/docs/2.0/deployment-integrations/#section=deployment\"\
    \ rel=\"nofollow\">deployment</a> location of choice.</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-2-add-your-recipes\" class=\"anchor\" href=\"#2-add-your-recipes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2. Add your Recipe(s)</h3>\n<p>For the example here, we have a single\
    \ recipe named \"Singularity\" that is provided\nas an input argument to the <a\
    \ href=\".circleci/build.sh\">build script</a>. You could add another\nrecipe,\
    \ and then of course call the build to happen more than once.\nThe build script\
    \ will name the image based on the recipe, and you of course\ncan change this.\
    \ Just write the path to it (relative to the repository base) in\nyour <a href=\"\
    .circleci/config.yml\">.circleci/config.yml</a>.</p>\n<h3>\n<a id=\"user-content-3-configure-singularity\"\
    \ class=\"anchor\" href=\"#3-configure-singularity\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3. Configure\
    \ Singularity</h3>\n<p>The recipe uses the <a href=\"https://circleci.com/orbs/registry/orb/singularity/singularity\"\
    \ rel=\"nofollow\">Singularity Orb</a> to install your chosen version of Singularity.\
    \ If you want to change the version, just adjust\nthe parameter here:</p>\n<div\
    \ class=\"highlight highlight-source-yaml\"><pre>  - <span class=\"pl-ent\">build</span>:\n\
    \      <span class=\"pl-ent\">name</span>: <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>Singularity 3.2.1 - Python 3<span class=\"pl-pds\">\"</span></span>\n\
    \      <span class=\"pl-ent\">singularity</span>: <span class=\"pl-s\">3.2.1</span>\n\
    \      <span class=\"pl-ent\">singularity-3</span>: <span class=\"pl-c1\">true</span></pre></div>\n\
    <p>The basic steps to <a href=\".circleci/setup.sh\">setup</a> the build are the\
    \ following:</p>\n<p>We also install the <a href=\"https://singularityhub.github.io/sregistry-cli\"\
    \ rel=\"nofollow\">sregistry client</a>\nthat allows you to issue a command like\
    \ \"sregistry push ...\" to upload a finished\nimage to one of your cloud / storage\
    \ endpoints. In this basic example, the push won't happen,\nand you will just\
    \ build an image using the CI.</p>\n<h3>\n<a id=\"user-content-4-configure-the-build\"\
    \ class=\"anchor\" href=\"#4-configure-the-build\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>4. Configure\
    \ the Build</h3>\n<p>The basic steps for the <a href=\".circleci/build.sh\">build</a>\
    \ are the following:</p>\n<ul>\n<li>Running build.sh with no inputs will default\
    \ to a recipe called \"Singularity\" in the base of the repository. You can provide\
    \ an argument to point to a different recipe path, always relative to the base\
    \ of your repository.</li>\n<li>If you want to define a particular unique resource\
    \ identifier for a finished container (to be uploaded to your storage endpoint)\
    \ you can do that with <code>--uri collection/container</code>. If you don't define\
    \ one, a robot name will be generated.</li>\n<li>You can add <code>--uri</code>\
    \ to specify a custom name, and this can include the tag, OR you can specify <code>--tag</code>\
    \ to go along with a name without one. It depends on which is easier for you.</li>\n\
    <li>If you add <code>--cli</code> then this is telling the build script that you\
    \ have defined the <a href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"\
    nofollow\">needed environment variables</a> for your <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">client of choice</a> and you want successful builds to be pushed\
    \ to your storage endpoint. See <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">here</a> for a list of current client endpoints, or roll your\
    \ own!</li>\n</ul>\n<p>See the <a href=\".circleci/config.yml\">config.yml</a>\
    \ for examples of this build.sh command (commented out). If there is some cloud\
    \ service that you'd like that is not provided, please <a href=\"https://www.github.com/singularityhub/sregistry-cli/issues\"\
    >open an issue</a>.</p>\n<h3>\n<a id=\"user-content-5-connect-to-ci\" class=\"\
    anchor\" href=\"#5-connect-to-ci\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>5. Connect to CI</h3>\n<p>If you go\
    \ to your <a href=\"https://circleci.com/dashboard\" rel=\"nofollow\">Circle Dashboard</a>\
    \ you can usually select a Github organization (or user) and then the repository,\
    \ and then click the toggle button to activate it to build on commit --&gt; push.</p>\n\
    <p>That's it for the basic setup! At this point, you will have a continuous integration\
    \ service that will build your container from a recipe each time that you push.\
    \ The next step is figuring out where you want to put the finished image(s), and\
    \ we will walk through this in more detail.</p>\n<h2>\n<a id=\"user-content-storage\"\
    \ class=\"anchor\" href=\"#storage\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Storage!</h2>\n<p>Once the image\
    \ is built, where can you put it? An easy answer is to use the\n<a href=\"https://singularityhub.github.io/sregistry-cli\"\
    \ rel=\"nofollow\">Singularity Global Client</a> and\nchoose <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">one of the many clients</a>\nto add a final step to push the\
    \ image. You then use the same client to pull the\ncontainer from your host. Once\
    \ you've decided which endpoints you want to push to,\nyou will need to:</p>\n\
    <ol>\n<li>Save the credentials / other environment variables that your client\
    \ needs (see the client settings page linked in the sregistry docs above) to your\
    \ <a href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"nofollow\">repository\
    \ settings</a> where they will be encrypted and in the environment.</li>\n<li>Add\
    \ a line to your <a href=\".circleci/config.yml\">.circleci/config.yml</a> to\
    \ do an sregistry push action to the endpoint(s) of choice. We have provided some\
    \ (commented out) examples to get you started.</li>\n</ol>\n<p>Remember that you\
    \ can also take advantage of deployment options that CircleCI offers, or do any\
    \ other action that you might want for the reproducibility or archive of metadata\
    \ of your builds. We save the build folder as an artifact to the repository, but\
    \ the containers might be too big to do this.</p>\n<h2>\n<a id=\"user-content-advanced-usage\"\
    \ class=\"anchor\" href=\"#advanced-usage\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Advanced Usage</h2>\n<ul>\n<li>This\
    \ setup can work as an analysis node as well! Try setting up a <a href=\"https://support.circleci.com/hc/en-us/articles/115015481128-Scheduling-jobs-cron-for-builds-\"\
    \ rel=\"nofollow\">scheduled build</a> to build a container that processes some\
    \ information feed, and you have a regularly scheduled task.</li>\n<li>run builds\
    \ in parallel and test different building environments. You could try building\
    \ the \"same\" container across different machine types and see if you really\
    \ do get the same thing :)</li>\n<li>You can also do other sanity checks like\
    \ testing if the container runs as you would expect, etc.</li>\n</ul>\n"
  stargazers_count: 5
  subscribers_count: 2
  topics:
  - singularity
  - builder
  - circle-ci
  - singularity-ci
  updated_at: 1561654292.0
singularityhub/github-ci:
  data_format: 2
  description: An example GitHub Action (CI) to build a Singularity container
  filenames:
  - Singularity
  full_name: singularityhub/github-ci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-builder-github-ci\" class=\"anchor\"\
    \ href=\"#singularity-builder-github-ci\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity Builder GitHub CI</h1>\n\
    <p><a href=\"img/sregistry-github-small.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"img/sregistry-github-small.png\" alt=\"img/sregistry-github-small.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>This is a simple example of how you can\
    \ achieve:</p>\n<ul>\n<li>version control of your recipes</li>\n<li>versioning\
    \ to include image hash <em>and</em> commit id</li>\n<li>build of associated container\
    \ and</li>\n<li>(optional) push to a storage endpoint</li>\n</ul>\n<p>for a reproducible\
    \ build workflow.</p>\n<p>There are two workflows configured on master that build\
    \ a container:</p>\n<ol>\n<li>\n<a href=\".github/workflows/native-install.yml\"\
    >native install</a> builds Singularity 3.x (with GoLang).</li>\n<li>\n<a href=\"\
    .github/workfolws/container.yml\">docker image</a> builds in a <a href=\"https://quay.io/repository/singularity/singularity\"\
    \ rel=\"nofollow\">docker image</a>.</li>\n</ol>\n<p>While the second option is\
    \ faster to complete and a more simple workflow, it should be noted that docker\
    \ runs with\n<code>--privileged</code> which may lead to issues with the resulting\
    \ container in a non privileged situation.</p>\n<p><strong>Why should this be\
    \ managed via Github?</strong></p>\n<p>Github, by way of easy integration with\
    \ <strong>native</strong> continuous integration, is an easy way\nto have a workflow\
    \ set up where multiple people can collaborate on a container recipe,\nthe recipe\
    \ can be tested (with whatever testing you need), discussed in pull requests,\n\
    and tested on merge to master. If you add additional steps in the <a href=\".github/workflows/native-install.yml\"\
    >build workflow</a>\nyou can also use <a href=\"http://singularityhub.github.io/sregistry-cli\"\
    \ rel=\"nofollow\">Singularity Registry Client</a> to push your container to a\n\
    <a href=\"https://singularityhub.github.io/sregistry\" rel=\"nofollow\">Singularity\
    \ Registry Server</a> or other\ncloud storage.</p>\n<p><strong>Why should I use\
    \ this instead of a service?</strong></p>\n<p>You could use a remote builder,\
    \ but if you do the build in a continuous integration\nservice you get complete\
    \ control over it. This means everything from the version of\nSingularity to use,\
    \ to the tests that you run for your container. You have a lot more\nfreedom in\
    \ the rate of building, and organization of your repository, because it's you\n\
    that writes the configuration.</p>\n<h2>\n<a id=\"user-content-quick-start\" class=\"\
    anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Quick Start</h2>\n<h3>\n<a id=\"user-content-1-add-your-recipes\"\
    \ class=\"anchor\" href=\"#1-add-your-recipes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>1. Add Your Recipes</h3>\n<p>Add\
    \ your Singularity recipes to this repository, and edit the <a href=\".github/workflows/native-install.yml\"\
    >build workflow</a>\nsection where the container is built. The default will look\
    \ for a recipe file called\n\"Singularity\" in the base of the respository, <a\
    \ href=\"Singularity\">as we have here</a>.\nFor example, here is the default:</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre>    - <span class=\"pl-ent\"\
    >name</span>: <span class=\"pl-s\">Build Container</span>\n      <span class=\"\
    pl-ent\">env</span>:\n        <span class=\"pl-ent\">SINGULARITY_RECIPE</span>:\
    \ <span class=\"pl-s\">Singularity</span>\n        <span class=\"pl-ent\">OUTPUT_CONTAINER</span>:\
    \ <span class=\"pl-s\">container.sif</span>\n      <span class=\"pl-ent\">run</span>:\
    \ <span class=\"pl-s\">|</span>\n<span class=\"pl-s\">       ls </span>\n<span\
    \ class=\"pl-s\">       if [ -f \"${SINGULARITY_RECIPE}\" ]; then</span>\n<span\
    \ class=\"pl-s\">            sudo -E singularity build ${OUTPUT_CONTAINER} ${SINGULARITY_RECIPE}</span>\n\
    <span class=\"pl-s\">       else</span>\n<span class=\"pl-s\">           echo\
    \ \"${SINGULARITY_RECIPE} is not found.\"</span>\n<span class=\"pl-s\">      \
    \     echo \"Present working directory: $PWD\"</span>\n<span class=\"pl-s\"> \
    \          ls</span>\n<span class=\"pl-s\">       fi</span></pre></div>\n<p>And\
    \ I could easily change that to build as many recipes as I like, and\neven disregard\
    \ the environment variable.</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre>    - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">Build Container</span>\n\
    \      <span class=\"pl-ent\">run</span>: <span class=\"pl-s\">|</span>\n<span\
    \ class=\"pl-s\">        sudo -E singularity build smokey.sif Singularity.smokey</span>\n\
    <span class=\"pl-s\">        sudo -E singularity build toasty.sif marshmallow/Singularity.toasty</span></pre></div>\n\
    <h3>\n<a id=\"user-content-2-test-your-container\" class=\"anchor\" href=\"#2-test-your-container\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2. Test your Container</h3>\n<p>Importantly, then you should test\
    \ your container! Whether that's running it,\nexec'ing a custom command, or invoking\
    \ the test command, there is more than\none way to eat a reeses:</p>\n<div class=\"\
    highlight highlight-source-yaml\"><pre>    - <span class=\"pl-ent\">name</span>:\
    \ <span class=\"pl-s\">Test Container</span>\n      <span class=\"pl-ent\">run</span>:\
    \ <span class=\"pl-s\">|</span>\n<span class=\"pl-s\">        singularity exec\
    \ smokey.sif python run_tests.py</span>\n<span class=\"pl-s\">        singularity\
    \ test smokey.sif</span>\n<span class=\"pl-s\">        singularity run toasty.sif</span></pre></div>\n\
    <h3>\n<a id=\"user-content-3-push-to-a-registry\" class=\"anchor\" href=\"#3-push-to-a-registry\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>3. Push to a registry</h3>\n<p>You might be done there. But if not,\
    \ you can install <a href=\"http://singularityhub.github.io/sregistry-cli\" rel=\"\
    nofollow\">Singularity Registry Client</a> and push to your cloud storage of choice!\
    \ You will want to add python and python-dev to the dependency\ninstall:</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre>    - <span class=\"pl-ent\"\
    >name</span>: <span class=\"pl-s\">Install Dependencies</span>\n      <span class=\"\
    pl-ent\">run</span>: <span class=\"pl-s\">|</span>\n<span class=\"pl-s\">    \
    \    sudo apt-get update &amp;&amp; sudo apt-get install -y \\</span>\n<span class=\"\
    pl-s\">          build-essential \\</span>\n<span class=\"pl-s\">          libssl-dev\
    \ \\</span>\n<span class=\"pl-s\">          uuid-dev \\</span>\n<span class=\"\
    pl-s\">          libgpgme11-dev \\</span>\n<span class=\"pl-s\">          squashfs-tools\
    \ \\</span>\n<span class=\"pl-s\">          libseccomp-dev \\</span>\n<span class=\"\
    pl-s\">          pkg-config \\</span>\n<span class=\"pl-s\">          python-dev\
    \ python python3-pip</span></pre></div>\n<p>And then install and use sregistry\
    \ client. Here are many examples:</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre>    - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">Deploy Container</span>\n\
    \      <span class=\"pl-ent\">run</span>: <span class=\"pl-s\">|</span>\n<span\
    \ class=\"pl-s\">        sudo pip3 install sregistry</span>\n<span class=\"pl-s\"\
    >        SREGISTRY_CLIENT=google-storage sregistry push --name username/reponame\
    \ smokey.sif</span>\n<span class=\"pl-s\">        SREGISTRY_CLIENT=s3 sregistry\
    \ push --name username/reponame smokey.sif</span>\n<span class=\"pl-s\">     \
    \   SREGISTRY_CLIENT=registry sregistry push --name username/reponame smokey.sif</span>\n\
    <span class=\"pl-s\">        SREGISTRY_CLIENT=dropbox sregistry push --name username/reponame\
    \ smokey.sif</span></pre></div>\n<p>See the <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">clients page</a> for all the options.\nRemember that the example\
    \ workflow is intended to run on push to master, so you might want to have\na\
    \ similar one (without deployment) that runs on pull_request, or other events.\n\
    See <a href=\"https://help.github.com/en/articles/about-github-actions#core-concepts-for-github-actions\"\
    >here</a>\nfor getting started with GitHub actions, and <a href=\"https://www.github.com/singularityhub/github-ci/issues\"\
    >please open an issue</a>\nif you need any help.</p>\n<h2>\n<a id=\"user-content-other-options\"\
    \ class=\"anchor\" href=\"#other-options\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Other Options</h2>\n<p>You can\
    \ customize this base recipe in so many ways! For example:</p>\n<ul>\n<li>If you\
    \ are building a Docker container, you can start with the docker base, build the\
    \ container, and then pull it down into Singularity and test it. Successful builds\
    \ can be pushed to Docker Hub, and then you know they will pull okay to a Singularity\
    \ container.</li>\n<li>The action can be configured with a Matrix to run builds\
    \ on multiple platforms.</li>\n<li>You can also do the same, but test multiple\
    \ versions of Singularity.</li>\n</ul>\n<p>Have fun!</p>\n"
  stargazers_count: 13
  subscribers_count: 2
  topics:
  - singularity-ci
  - github-ci
  - singularity
  - container
  updated_at: 1615502996.0
slaclab/folding-at-home-gpu:
  data_format: 2
  description: gpu image for folding at home
  filenames:
  - Singularity
  full_name: slaclab/folding-at-home-gpu
  latest_release: null
  readme: '<h1>

    <a id="user-content-folding-at-home-gpu" class="anchor" href="#folding-at-home-gpu"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>folding-at-home-gpu</h1>

    <p>gpu image for folding at home</p>

    <p>simple merge of nvidia cl image with folding at home v7.5.1 to enable gpu processing.</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1584940583.0
slaclab/singularity-modules:
  data_format: 2
  description: Singularity Container Recipes
  filenames:
  - images/Singularity.lsf
  - images/protomo/Singularity
  - images/resmap/Singularity
  - images/amira/6.7.0/Singularity
  - images/appion-protomo/Singularity
  - images/openmbir/2.3.5/Singularity
  - images/motioncor2/Singularity
  - images/motioncor2/1.2.2/Singularity
  - images/motioncor2/1.2.3-intpix/Singularity
  - images/motioncor2/1.2.1/Singularity
  - images/motioncor2/1.2.3/Singularity
  - images/motioncor2/1.2.6/Singularity
  - images/motioncor2/1.3.0/Singularity
  - images/motioncor2/1.3.2/Singularity
  - images/cdms-jupyterlab/Singularity
  - images/pymol/Singularity
  - images/git/Singularity
  - images/phenix/Singularity
  - images/cryosparc/2.14.2/Singularity
  - images/cryosparc/2.13.2/Singularity
  - images/cryosparc/2.12.4/Singularity
  - images/openmpi/Singularity
  - images/openmpi/Singularity.ubuntu1810
  - images/openmpi/Singularity.ubuntu1804
  - images/openmpi/Singularity.centos7
  - images/relion/Singularity
  - images/relion/Singularity.old
  - images/relion/Singularity.docker
  - images/relion/3.0.8/Singularity.docker
  - images/relion/3.0.2/Singularity
  - images/relion/3.0.2/Singularity.docker
  - images/relion/3.0.7/Singularity
  - images/relion/3.0.7/Singularity.orig
  - images/relion/3.0.6/Singularity
  - images/relion/3.0.6/Singularity.docker
  - images/relion/ver3.1/Singularity.docker
  - images/relion/3.1.0-beta/Singularity
  - images/relion/3.1.0-beta/Singularity.docker
  - images/relion/2.1/Singularity
  - images/relion/2.1/Singularity.docker
  - images/relion/3.0.4/Singularity
  - images/relion/3.0.4/Singularity.docker
  - images/cryolo/1.5.4/Singularity
  - images/rclone/Singularity
  - images/xds/Singularity
  - images/fah/7.5.1/Singularity
  - images/rosetta/Singularity
  - images/rosetta/2018.48/Singularity
  - images/emClarity/Singularity
  - images/slac-ml/Singularity
  - images/slac-ml/20190712.2/Singularity
  - images/slac-ml/20200618.0/Singularity
  - images/slac-ml/20200227.0/Singularity
  - images/slac-ml/20200211.0/Singularity
  - images/scipion/Singularity
  - images/matlab/R2020a/Singularityfile
  - images/chimera/Singularity
  - images/topaz/0.2.2/Singularity
  - images/topaz/0.2.4/Singularity
  - images/imagemagick/Singularity
  - images/icon-gpu/Singularity
  - images/ctffind/Singularity
  - images/ctffind/4.1.10/Singularity
  - images/ctffind/4.1.13/Singularity
  - images/ctffind/4.1.12/Singularity
  - images/eman2/Singularity
  - images/eman2/2.31/Singularity
  - images/eman2/20200330/Singularity
  - images/eman2/20190805/Singularity
  - images/eman2/20190324/Singularity
  - images/eman2/20200419/Singularity
  - images/eman2/20190917/Singularity
  - images/eman2/20200319.0/Singularity
  - images/eman2/20190603/Singularity
  - images/eman2/20190418/Singularity
  - images/imod/Singularity
  - images/imod/4.10.38/Singularity
  - images/imod/4.9.10/Singularity
  - images/imod/4.9.11/Singularity
  - images/imod/4.9.12/Singularity
  - images/imod/4.10.42/Singularity
  - images/tem-simulator/Singularity
  full_name: slaclab/singularity-modules
  latest_release: null
  readme: '<h3>

    <a id="user-content-simnibs-singularity-recipe" class="anchor" href="#simnibs-singularity-recipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SimNIBS
    singularity recipe</h3>

    <p>Before building, place the SimNIBS source tarball in the /tmp directory. (recipe
    version 2.1.1)</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1592808314.0
sleeepyjack/variant_calling:
  data_format: 2
  description: null
  filenames:
  - Singularity.gpu
  - Singularity.cpu
  full_name: sleeepyjack/variant_calling
  latest_release: null
  readme: '<h1>

    <a id="user-content-variant-calling" class="anchor" href="#variant-calling" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Variant Calling</h1>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies:</h2>

    <pre><code>- make

    - Singularity (&gt;= v3.2)

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1589243145.0
slhogle/singularity_def_files:
  data_format: 2
  description: Singularity definition files for building various software to run on
    HPC systems
  filenames:
  - octopus.def
  - instrain.def
  - checkm.def
  - torstyverse.def
  full_name: slhogle/singularity_def_files
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-definition-files" class="anchor" href="#singularity-definition-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    definition files</h1>

    <p>Collection of def files for building some bioinformatics software I commonly
    use.</p>

    <h2>

    <a id="user-content-instrain-v1214" class="anchor" href="#instrain-v1214" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>inStrain v1.2.14</h2>

    <p><a href="https://github.com/MrOlm/inStrain">https://github.com/MrOlm/inStrain</a></p>

    <p>Also contains these functioning binaries:</p>

    <ul>

    <li><a href="https://github.com/samtools/samtools">samtools v1.10</a></li>

    <li><a href="https://github.com/hyattpd/Prodigal">prodigal v2.6.3</a></li>

    <li><a href="https://github.com/lh3/bwa">bwa v0.7.17-r1198-dirty</a></li>

    <li><a href="https://github.com/lh3/minimap2">minimap2 v2.17 (r941)</a></li>

    <li><a href="https://github.com/dnbaker/dashing">Dashing v0.4.8-1-g47e6</a></li>

    <li><a href="https://github.com/ParBLiSS/FastANI">FastANI v1.3</a></li>

    <li><a href="https://github.com/wwood/CoverM">CoverM v0.4.0</a></li>

    </ul>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/instrain" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/instrain</code></p>

    <h2>

    <a id="user-content-octopus-development-branch-version-v070-develop-2bde0433"
    class="anchor" href="#octopus-development-branch-version-v070-develop-2bde0433"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Octopus
    development branch version v0.7.0 (develop 2bde0433)</h2>

    <p><a href="https://github.com/luntergroup/octopus">https://github.com/luntergroup/octopus</a></p>

    <p>Built with:</p>

    <ul>

    <li>patchelf v0.10</li>

    <li>openssl v1.1.1g</li>

    <li>pkg-config v0.29.2</li>

    <li>gpatch v2.7.6</li>

    <li>ncurses v6.2</li>

    <li>cmake v3.17.3</li>

    <li>htslib v1.10</li>

    <li>boost v1.72.0</li>

    <li>GNU C/C++ compiler v9.3.0</li>

    </ul>

    <p>Target: x86_64 Linux 5.3.0-7642-generic<br>

    SIMD extension: AVX2</p>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/octopus" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/octopus</code></p>

    <h2>

    <a id="user-content-torstyverse" class="anchor" href="#torstyverse" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Torstyverse</h2>

    <p>Bundle of useful packages from <a href="https://github.com/tseemann">Torsten
    Seeman</a></p>

    <ul>

    <li><a href="https://github.com/tseemann/samclip">sampclip v0.4.0</a></li>

    <li><a href="https://github.com/tseemann/any2fasta">any2fasta v0.4.2</a></li>

    <li><a href="https://github.com/tseemann/barrnap">barrnap v0.9</a></li>

    <li><a href="https://github.com/tseemann/prokka">prokka v1.14.6</a></li>

    <li><a href="https://github.com/tseemann/shovill">shovill v1.1.0</a></li>

    <li><a href="https://github.com/tseemann/abricate">abricate v1.0.1</a></li>

    <li><a href="https://github.com/tseemann/snippy">snippy v4.6.0</a></li>

    </ul>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/torstyverse" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/torstyverse</code></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1614942669.0
solvcon/solvcon:
  data_format: 2
  description: A software framework of conservation-law solvers that use the space-time
    Conservation Element and Solution Element (CESE) method.
  filenames:
  - contrib/singularity/Singularity
  - contrib/singularity/Singularity.0.1.0
  - contrib/singularity/Singularity.1.0.0-0.1.4+
  full_name: solvcon/solvcon
  latest_release: 0.1.4
  readme: '<h3>

    <a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon
    octicon-link"></span></a><a href="https://github.com/openhpc/ohpc/blob/master/docs/recipes/install/common/figures/ohpc_logo.png"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/openhpc/ohpc/raw/master/docs/recipes/install/common/figures/ohpc_logo.png"
    width="170" valign="middle" hspace="5" alt="OpenHPC" style="max-width:100%;"></a>

    </h3>

    <h3>

    <a id="user-content-community-building-blocks-for-hpc-systems" class="anchor"
    href="#community-building-blocks-for-hpc-systems" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Community building blocks for HPC systems</h3>

    <h4>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h4>

    <p>This stack provides a variety of common, pre-built ingredients required to

    deploy and manage an HPC Linux cluster including provisioning tools, resource

    management, I/O clients, runtimes, development tools, containers, and a variety
    of

    scientific libraries.</p>

    <p>There are currently two release series:

    <a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> and

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>, which target different
    major

    Linux OS distributions. The 1.3.x series targets CentOS7 and SLES12 while the

    2.x series targets CentOS8 and Leap15.</p>

    <h4>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h4>

    <p>OpenHPC provides pre-built binaries via repositories for use with standard

    Linux package manager tools (e.g. <code>yum</code> or <code>zypper</code>). To
    get started,

    you can enable an OpenHPC repository locally through installation of an

    <code>ohpc-release</code> RPM which includes gpg keys for package signing and
    defines

    the URL locations for [base] and [update] package repositories. Installation

    guides tailored for each supported provisioning system and resource manager

    with detailed example instructions for installaing a cluster are also available.

    Copies of the <code>ohpc-release</code> package and installation guides along
    with

    more information is available on the relevant release series pages

    (<a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> or

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>).</p>

    <hr>

    <h4>

    <a id="user-content-questions-comments-or-bug-reports" class="anchor" href="#questions-comments-or-bug-reports"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Questions,
    Comments, or Bug Reports?</h4>

    <p>Subscribe to the users email list at <a href="https://groups.io/g/openhpc-users"
    rel="nofollow">https://groups.io/g/openhpc-users</a> or see

    the <a href="http://openhpc.community" rel="nofollow">http://openhpc.community</a>
    page for more pointers.</p>

    <h4>

    <a id="user-content-additional-software-requests" class="anchor" href="#additional-software-requests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Additional
    Software Requests?</h4>

    <p>Please see the component submission page at

    <a href="https://github.com/openhpc/submissions">https://github.com/openhpc/submissions</a>
    for more information regarding new

    software inclusion requests.</p>

    <h4>

    <a id="user-content-register-your-system" class="anchor" href="#register-your-system"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Register
    your system</h4>

    <p>If you are using elements of OpenHPC, please consider registering your

    system(s) using the <a href="https://drive.google.com/open?id=1KvFM5DONJigVhOlmDpafNTDDRNTYVdolaYYzfrHkOWI"
    rel="nofollow">System Registration

    Form</a>.</p>

    '
  stargazers_count: 15
  subscribers_count: 8
  topics:
  - computational-science
  updated_at: 1622890170.0
soudabeh19/centos7-reprozip.fslbuild-centos5:
  data_format: 2
  description: PreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)
  filenames:
  - Singularity
  full_name: soudabeh19/centos7-reprozip.fslbuild-centos5
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7-reprozipfslbuild-centos5" class="anchor" href="#centos7-reprozipfslbuild-centos5"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>centos7-reprozip.fslbuild-centos5</h1>

    <p>PreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1521572666.0
soycoder/nemo:
  data_format: 2
  description: HPC-AI 2020 | Training Project NEMO - Nucleus for European Modelling
    of the Ocean
  filenames:
  - Slurm Script/Singularity.nemo.apps
  - Slurm Script/Singularity.CENTOS-7.7-NEMO-MOFED
  full_name: soycoder/nemo
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--nemo---ocean\" class=\"anchor\" href=\"#-nemo---ocean\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><g-emoji class=\"g-emoji\" alias=\"ocean\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png\"\
    >\U0001F30A</g-emoji> NEMO - ocean</h1>\n<p>HPC-AI 2020 | Training Project - NEMO:\
    \ Nucleus for European Modelling of the Ocean</p>\n<h2>\n<a id=\"user-content--docker-images---centos\"\
    \ class=\"anchor\" href=\"#-docker-images---centos\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><g-emoji class=\"\
    g-emoji\" alias=\"floppy_disk\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png\"\
    >\U0001F4BE</g-emoji> Docker Images - CentOS</h2>\n<p>Thank you for an image \
    \ (<a href=\"https://hub.docker.com/r/wangyoucao577/centos7-gcc7.4\" rel=\"nofollow\"\
    >wangyoucao577/centos7-gcc7.4</a>)</p>\n<h2>\n<a id=\"user-content--tag\" class=\"\
    anchor\" href=\"#-tag\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><g-emoji class=\"g-emoji\" alias=\"bookmark\"\
    \ fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f516.png\"\
    >\U0001F516</g-emoji> Tag</h2>\n<ul>\n<li><a href=\"https://hub.docker.com/layers/soycoder/centos7/nemo-ocean/images/sha256-c7bdaa3614e1fc1bbef31bdb05ac997e64b11abff716d00315807b1b79ad13c3\"\
    \ rel=\"nofollow\">:nemo-ocean</a></li>\n</ul>\n<h2>\n<a id=\"user-content--environment\"\
    \ class=\"anchor\" href=\"#-environment\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><g-emoji class=\"g-emoji\" alias=\"\
    sunrise_over_mountains\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f304.png\"\
    >\U0001F304</g-emoji> Environment</h2>\n<ol>\n<li>HPC-X to build an out-of-box\
    \ MPI environment</li>\n<li>Boost library</li>\n<li>HDF5 Parallellibrary</li>\n\
    <li>NETCDF Parallel library with HDF5</li>\n<li>NETCDF-FortranParallel library\
    \ with NETCDF Parallel</li>\n<li>XIOS</li>\n<li>GYREwith GNUgfortran + HPC-X OpenMPI</li>\n\
    </ol>\n<div class=\"highlight highlight-text-html-basic\"><pre>/usr/mpi/gcc/openmpi-3.1.1rc1/bin/mpirun\
    \ \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 \\\n-mca mpi_show_mca_params 1\
    \ -mca pml_ucx_verbose 9 \\\n/usr/mpi/gcc/openmpi-3.1.1rc1/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n\n/usr/mpi/gcc/openmpi-3.1.1rc1/bin/mpirun -n 2 \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1\
    \ \\\n/usr/mpi/gcc/openmpi-3.1.1rc1/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n/usr/bin/time -p mpirun -n 2 \\\n-mca pml ucx -x UCX_TLS=rc UCX_NET_DEVICES=mlx5_0:1\
    \ ./nemo\n\n/usr/bin/time -p mpirun -n 2 \\\n-mca -x UCX_TLS=rc -x UCX_NET_DEVICES=mlx5_0:1\
    \ ./nemo\n\n/usr/bin/time -p mpirun -n 2 \\\n-mca -x UCX_TLS=rc -x UCX_NET_DEVICES=ib0\
    \ \\\n/home/hpc/nemo/apps/hpcx-v2.6.0-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-redhat7.7-x86_64/ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \nibstat\n\n\nNow step into the container and install MOFED:\n\n$ sudo singularity\
    \ exec -w u16.04-sandbox/ bash\n(singularity)# cd MOFED/MLNX_OFED_LINUX-4.3-1.0.1.0-ubuntu16.04-x86_64\n\
    (singularity)# ./mlnxofedinstall\n\n\n! -- (nemo) singularity exec -w nemo.sif\
    \ bash\n\n\n## Run container\nTo use Singularity in Mellanox/HPCX need to load\
    \ env module: `module load tools/singularity`\n.\n\nRun `osu_latency` test:\n\
    ```sh\n$ mpirun -np 2 --map-by node -mca btl self singularity exec hpcx-u16.04.simg\
    \ /hpcx/ompi-a7df\nd94/tests/osu-micro-benchmarks-5.3.2/osu_latency\n# OSU MPI\
    \ Latency Test v5.3.2\n# Size          Latency (us)\n0                       1.55\n\
    1                       1.55\n2                       1.55\n4                \
    \       1.55\n8                       1.54\n16                      1.55\n32 \
    \                     1.55\n64                      1.65\n128                \
    \     2.19\n256                     2.23\n512                     2.35\n1024 \
    \                   2.64\n2048                    2.89\n4096                 \
    \   3.51\n8192                    5.00\n16384                   6.44\n32768  \
    \                 8.91\n65536                  14.12\n131072                 25.05\n\
    262144                 27.31\n524288                 49.03\n1048576          \
    \      92.53\n2097152               178.95\n4194304               351.24\n\n\n\
    \n$hpcx_mpi_dir/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\ncd /home/hpc/nemo/apps/hpcx-v2.6.0-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-redhat7.7-x86_64\n\
    \nmpirun \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 \\\n-mca mpi_show_mca_params\
    \ 1 -mca pml_ucx_verbose 9 \\\n./ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n\nmpirun \\\n-mca mpi_show_mca_params 1 -mca pml_ucx_verbose 9 \\\n./ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n\n\n/usr/bin/time -p mpirun -np 4 \\\n--map-by core -report-bindings \\\n-mca\
    \ io ompio -x UCX_NET_DEVICES=mlx5_0:1 ./nemo</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1603363757.0
stevekm/IGV-snapshot-nf:
  data_format: 2
  description: Nextflow workflow for automated IGV snapshots
  filenames:
  - containers/IGV/Singularity.IGV
  full_name: stevekm/IGV-snapshot-nf
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-igv-snapshot-nf\" class=\"anchor\" href=\"#igv-snapshot-nf\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>IGV-snapshot-nf</h1>\n<p>An example Nextflow workflow for creating\
    \ automated IGV snapshots of .bam files based on a list of target regions.</p>\n\
    <p>This workflow is designed to show how to integrate <a href=\"https://github.com/stevekm/IGV-snapshot-automator\"\
    >automated IGV snapshotting</a> into a Nextflow workflow.</p>\n<h2>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p>First, clone this repository:</p>\n<pre><code>git clone https://github.com/stevekm/IGV-snapshot-automator.git\n\
    cd IGV-snapshot-automator\n</code></pre>\n<h3>\n<a id=\"user-content-containers\"\
    \ class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Containers</h3>\n<p>Docker and/or\
    \ Singularity containers are used to package IGV, X11, and <code>xvfb</code> required\
    \ for functionality. Docker is required to build Singularity containers</p>\n\
    <p>To build the Docker container for IGV:</p>\n<pre><code>cd containers\nmake\
    \ docker-build VAR=IGV\n</code></pre>\n<p>To test out the IGV Docker container:</p>\n\
    <pre><code>make docker-test VAR=IGV\n</code></pre>\n<p>(optional) To build a Singuarity\
    \ container for IGV, first build the Singularity Docker container:</p>\n<pre><code>make\
    \ docker-build VAR=Singularity-2.4\n</code></pre>\n<ul>\n<li>This container is\
    \ used to build Singularity containers</li>\n</ul>\n<p>To build the Singularity\
    \ container for IGV:</p>\n<pre><code>make singularity-build VAR=IGV\n\n# test\
    \ the container:\nmake singularity-test VAR=IGV\n</code></pre>\n<ul>\n<li>The\
    \ Singularity container will be saved to <code>containers/IGV/IGV.simg</code>,\
    \ which you can upload to your remote server for usage</li>\n</ul>\n<h1>\n<a id=\"\
    user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h1>\n\
    <p>Run the included demo workflow (from the parent repo directory):</p>\n<pre><code>make\
    \ run\n</code></pre>\n<p>Should look something like this:</p>\n<pre><code>IGV-snapshot-nf$\
    \ make run\n./nextflow run main.nf -profile \"docker\"\nN E X T F L O W  ~  version\
    \ 19.04.1\nLaunching `main.nf` [kickass_cray] - revision: 1823b32e4f\n~~~~~~~\
    \ IGV Pipeline ~~~~~~~\n* Project dir:        /Users/steve/projects/IGV-snapshot-nf\n\
    * Launch dir:         /Users/steve/projects/IGV-snapshot-nf\n* Work dir:     \
    \      /Users/steve/projects/IGV-snapshot-nf/work\n* Profile:            docker\n\
    * Script name:        main.nf\n* Script ID:          1823b32e4f4fbc1caa63b0c12b2d4340\n\
    * Container engine:   docker\n* Workflow session:   843f9541-9cc2-46c8-9005-89659c67ed80\n\
    * Nextflow run name:  kickass_cray\n* Nextflow version:   19.04.1, build 5072\
    \ (03-05-2019 12:29 UTC)\n* Launch command:\nnextflow run main.nf -profile docker\n\
    [warm up] executor &gt; local\nexecutor &gt;  local (1)\n[91/852794] process &gt;\
    \ run_IGV [100%] 1 of 1 \u2714\nCompleted at: 22-May-2019 15:27:46\nDuration \
    \   : 1m 20s\nCPU hours   : (a few seconds)\nSucceeded   : 1\n</code></pre>\n\
    <p>Example snapshot output:</p>\n<p><a href=\"https://raw.githubusercontent.com/stevekm/IGV-snapshot-nf/output/output/snapshots/chr13_113976596_113976736.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/stevekm/IGV-snapshot-nf/output/output/snapshots/chr13_113976596_113976736.png\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-software\"\
    \ class=\"anchor\" href=\"#software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Software</h1>\n<ul>\n<li>\n<p>Tested\
    \ with macOS 10.12.6 and RHEL 7</p>\n</li>\n<li>\n<p>Nextflow (requires Java 8+\
    \ and <code>bash</code>)</p>\n</li>\n<li>\n<p>IGV 2.4.10</p>\n</li>\n<li>\n<p>Python</p>\n\
    </li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - igv
  - nextflow
  updated_at: 1558554996.0
stevekm/MuTect2_target_chunking:
  data_format: 2
  description: demo pipeline for testing different data chunking methods for MuTect2
  filenames:
  - containers/variant-calling-0.0.2/Singularity.variant-calling-0.0.2
  - containers/annovar-150617/Singularity.annovar-150617
  full_name: stevekm/MuTect2_target_chunking
  latest_release: null
  readme: '<h1>

    <a id="user-content-mutect2-target-chunking" class="anchor" href="#mutect2-target-chunking"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MuTect2
    Target Chunking</h1>

    <p>Demo pipeline for testing different data chunking methods for MuTect2.</p>

    <p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_cancer_m2_MuTect2.php"
    rel="nofollow">MuTect2</a> is a common tool used for variant calling of tumor-normal
    pairs. However, it is limited to running only in single-threaded mode, which can
    lead to extremely long execution times.</p>

    <p>This demo pipeline uses different techniques to chunk the included list of
    target regions (<code>targets.bed</code>) into smaller segments to run in parallel,
    then aggregate all results for comparison to ensure that variant calls are the
    same across all chunking methods.</p>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <p>This pipeline comes pre-configured for usage on NYULMC''s Big Purple HPC cluster
    using pre-built Singularity containers and pre-downloaded reference files.</p>

    <p>In order to use this pipeline on your system you will need to update the file
    paths saved in <code>nextflow.config</code> for your system.</p>

    <p>Singularity and Docker container recipes are included in the <code>containers</code>
    directory.</p>

    <p>Paths to input .bam files for tumor and normal samples are read from the file
    <code>samples.analysis.tsv</code>.</p>

    <p>Once correctly configured, the pipeline can be run with:</p>

    <pre><code>make run

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics:
  - nextflow
  - mutect2
  - variant-calling
  updated_at: 1562090008.0
stevekm/NYU-phoenix-docker-singularity-nextflow-demo:
  data_format: 2
  description: Nextflow + Singularity/Docker demo for CentOS 6.8 without OverlayFS
  filenames:
  - containers/demo1/Singularity.demo1
  - containers/base/Singularity.base
  full_name: stevekm/NYU-phoenix-docker-singularity-nextflow-demo
  latest_release: null
  readme: '<h1>

    <a id="user-content-nyu-phoenix-hpc-dockersingularity-nextflow-demo" class="anchor"
    href="#nyu-phoenix-hpc-dockersingularity-nextflow-demo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NYU phoenix HPC Docker/Singularity
    Nextflow Demo</h1>

    <p>Demo on how to run a Nextflow pipeline on the HPC using Singularity containers
    built from Docker.</p>

    <h1>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <p>Clone this repository</p>

    <pre><code>git clone https://github.com/stevekm/NYU-phoenix-docker-singularity-nextflow-demo.git

    cd NYU-phoenix-docker-singularity-nextflow-demo

    </code></pre>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <h2>

    <a id="user-content-remote-hpc-phoenix" class="anchor" href="#remote-hpc-phoenix"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Remote
    HPC (phoenix)</h2>

    <p>To run this workflow on the NYU phoenix HPC system, use the following command:</p>

    <pre><code>make run-p

    </code></pre>

    <p>This will:</p>

    <ul>

    <li>

    <p>install Nextflow to the current directory</p>

    </li>

    <li>

    <p>extract a pre-built demo Singularity image from this repository</p>

    </li>

    <li>

    <p>run the Nextflow pipeline using the Singularity image</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-local" class="anchor" href="#local" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Local</h2>

    <p>To run this workflow on your local computer (Docker required), use the following
    command:</p>

    <pre><code>make run-l

    </code></pre>

    <p>This will:</p>

    <ul>

    <li>

    <p>install Nextflow to the current directory</p>

    </li>

    <li>

    <p>build the Docker containers included in this repository</p>

    </li>

    <li>

    <p>run the Nextflow pipeline using the Docker containers</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h1>

    <ul>

    <li>

    <p><code>Makefile</code>: shortcuts to common actions used in the demo</p>

    </li>

    <li>

    <p><code>main.nf</code>: main Nextflow pipeline file</p>

    </li>

    <li>

    <p><code>nextflow.config</code>: Nextflow configuration file</p>

    </li>

    <li>

    <p><code>bin</code>: directory for scripts to use inside the Nextflow pipeline;
    its contents will be prepended to your <code>PATH</code> when pipeline tasks are
    executed</p>

    </li>

    <li>

    <p><code>containers</code>: directory containing Docker and Singularity container
    files, along with documentation on their setup &amp; usage</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-software-requirements" class="anchor" href="#software-requirements"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    Requirements</h1>

    <h2>

    <a id="user-content-local--remote-hpc-server" class="anchor" href="#local--remote-hpc-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>local
    &amp; remote HPC server</h2>

    <ul>

    <li>

    <p>Java 8 (for Nextflow)</p>

    </li>

    <li>

    <p>GraphViz Dot (to compile flowchart)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-local-only" class="anchor" href="#local-only" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>local only</h2>

    <ul>

    <li>

    <p>Docker version 17.12.0-ce, build c97c6d6</p>

    </li>

    <li>

    <p>Vagrant version 2.0.1 (for tesing Singularity containers)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-remote-hpc-server-only" class="anchor" href="#remote-hpc-server-only"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>remote
    HPC server only</h2>

    <ul>

    <li>Singularity version 2.4.2</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1521145930.0
stevekm/bwa-bench:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity
  full_name: stevekm/bwa-bench
  latest_release: null
  readme: ''
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549319905.0
stevekm/nextflow-demos:
  data_format: 2
  description: Example Nextflow pipelines and programming techniques
  filenames:
  - Singularity/Singularity
  full_name: stevekm/nextflow-demos
  latest_release: null
  readme: '<h1>

    <a id="user-content-nextflow-demos" class="anchor" href="#nextflow-demos" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nextflow-demos</h1>

    <p>Demonstrations of various programming techniques for use inside <a href="https://www.nextflow.io/"
    rel="nofollow">Nextflow</a> pipelines. This repository is meant to be a supplement
    to the <a href="https://www.nextflow.io/docs/latest/getstarted.html" rel="nofollow">official
    Nextflow documentation</a> (links below).</p>

    <ul>

    <li>

    <p>an overview presentation about Nextflow can be found <a href="https://github.com/stevekm/nextflow-demos/blob/docs/docs/Nextflow_presentation.pdf">here</a>
    (view <a href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/stevekm/nextflow-demos/docs/docs/Nextflow_presentation.pdf"
    rel="nofollow">here</a>)</p>

    </li>

    <li>

    <p>Nextflow HTML report examples can be found here:</p>

    <ul>

    <li>

    <p><a href="https://htmlpreview.github.io/?https://github.com/stevekm/nextflow-demos/blob/report-output/reporting/nextflow-report.html"
    rel="nofollow">pipeline report</a></p>

    </li>

    <li>

    <p><a href="https://htmlpreview.github.io/?https://github.com/stevekm/nextflow-demos/blob/report-output/reporting/timeline-report.html"
    rel="nofollow">timeline report</a></p>

    </li>

    </ul>

    </li>

    </ul>

    <p><em>NOTE</em>: Some of the techniques demonstrated here may be deprecated by
    the new <a href="https://www.nextflow.io/docs/latest/dsl2.html" rel="nofollow">DSL2</a>
    syntax offered by Nextflow. Be sure to check that out as well.</p>

    <h1>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h1>

    <p>Clone this repo:</p>

    <div class="highlight highlight-source-shell"><pre>git clone git@github.com:stevekm/nextflow-demos.git

    <span class="pl-c1">cd</span> nextflow-demos</pre></div>

    <h1>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h1>

    <p>Each subdirectory contains files to run sample Nextflow pipelines.</p>

    <h2>

    <a id="user-content-files" class="anchor" href="#files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Files</h2>

    <ul>

    <li>

    <p><code>Makefile</code>: shortcut to commands to install and clean up Nextflow
    and its pipeline output</p>

    </li>

    <li>

    <p><code>main.nf</code>: Nextflow pipeline file</p>

    </li>

    <li>

    <p><code>nextflow.config</code>: config file for Nextflow pipeline (optional)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-sample-pipeline-directories" class="anchor" href="#sample-pipeline-directories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sample
    Pipeline Directories</h2>

    <p>(listed in recommended order for new users)</p>

    <ul>

    <li>

    <p><code>print-samples</code>: Prints samples from a list to the terminal</p>

    </li>

    <li>

    <p><code>make-files</code>: Creates files based on sample ID inputs</p>

    </li>

    <li>

    <p><code>output-files</code>: Same as <code>make-files</code> but includes custom
    file output options</p>

    </li>

    <li>

    <p><code>async</code>: demonstration of asynchronous process execution</p>

    </li>

    <li>

    <p><code>custom-email-output</code>: Creates files from sample ID''s then sends
    the user an email with a pipeline summary and files attached</p>

    </li>

    <li>

    <p><code>output-variable-name</code>: Same as <code>output-files</code> but includes
    inline variable definition of output file names</p>

    </li>

    <li>

    <p><code>R-Python</code>: methods for using other scripting languages inside the
    Nextflow pipeline</p>

    </li>

    <li>

    <p><code>join-pairs</code>: joining pairs of samples based on ID across input
    channels</p>

    </li>

    <li>

    <p><code>parse-samplesheet</code>: parsing of a samplesheet as input for Nextflow
    pipeline</p>

    </li>

    <li>

    <p><code>reporting</code>: execution of Nextflow pipeline with reporting and config
    features enabled.</p>

    </li>

    <li>

    <p><code>profiles-Docker-module</code>: usage of ''profiles'' to change process
    execution behavior to use Docker or environment modules</p>

    </li>

    <li>

    <p><code>Groovy-code</code>: example of using inline Groovy code inside the Nextflow
    pipeline</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <p>You can use the following commands inside the provided demo subdirs to run
    the demo pipelines.</p>

    <h2>

    <a id="user-content-install-nextflow" class="anchor" href="#install-nextflow"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Nextflow</h2>

    <pre><code># in a subdir in this repo

    make

    </code></pre>

    <h2>

    <a id="user-content-run-pipeline" class="anchor" href="#run-pipeline" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run pipeline</h2>

    <pre><code>./nextflow run main.nf

    </code></pre>

    <p>or</p>

    <pre><code>make run

    </code></pre>

    <h2>

    <a id="user-content-cleanup" class="anchor" href="#cleanup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cleanup</h2>

    <pre><code>make clean

    </code></pre>

    <h1>

    <a id="user-content-resources" class="anchor" href="#resources" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h1>

    <ul>

    <li>

    <p>Nextflow Homepage: <a href="https://www.nextflow.io/" rel="nofollow">https://www.nextflow.io/</a></p>

    </li>

    <li>

    <p>Nextflow Docs: <a href="https://www.nextflow.io/docs/latest/getstarted.html"
    rel="nofollow">https://www.nextflow.io/docs/latest/getstarted.html</a></p>

    </li>

    <li>

    <p>Nextflow Patterns: <a href="https://nextflow-io.github.io/patterns/index.html"
    rel="nofollow">https://nextflow-io.github.io/patterns/index.html</a></p>

    </li>

    <li>

    <p>Nextflow GitHub: <a href="https://github.com/nextflow-io/nextflow">https://github.com/nextflow-io/nextflow</a></p>

    </li>

    <li>

    <p>Nextflow Google Group: <a href="https://groups.google.com/forum/#!forum/nextflow"
    rel="nofollow">https://groups.google.com/forum/#!forum/nextflow</a></p>

    </li>

    </ul>

    <h2>

    <a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Examples</h2>

    <ul>

    <li>

    <p>Nextflow tutorial: <a href="https://github.com/nextflow-io/hack17-tutorial">https://github.com/nextflow-io/hack17-tutorial</a></p>

    </li>

    <li>

    <p>Nextflow examples: <a href="https://github.com/nextflow-io/examples">https://github.com/nextflow-io/examples</a></p>

    </li>

    <li>

    <p>Pipeline examples: <a href="https://github.com/nextflow-io/awesome-nextflow">https://github.com/nextflow-io/awesome-nextflow</a></p>

    </li>

    <li>

    <p>Boilerplate example for writing pipelines: <a href="https://github.com/stevekm/nextflow-boilerplate">https://github.com/stevekm/nextflow-boilerplate</a></p>

    </li>

    <li>

    <p>NYU pipelines:</p>

    <ul>

    <li>

    <p>exome sequencing: <a href="https://github.com/NYU-Molecular-Pathology/NGS580-nf">https://github.com/NYU-Molecular-Pathology/NGS580-nf</a></p>

    </li>

    <li>

    <p>demultiplexing: <a href="https://github.com/NYU-Molecular-Pathology/demux-nf">https://github.com/NYU-Molecular-Pathology/demux-nf</a></p>

    </li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 59
  subscribers_count: 5
  topics:
  - nextflow
  updated_at: 1625030182.0
stevekm/singularity-samtools-demo:
  data_format: 2
  description: 'Singularity container for samtools '
  filenames:
  - Singularity
  - old/Singularity.v1.6
  full_name: stevekm/singularity-samtools-demo
  latest_release: null
  readme: '<h1>

    <a id="user-content-setup" class="anchor" href="#setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h1>

    <p>This assumes you are building a Singularity container locally on a Mac</p>

    <p>Make sure you''ve already installed Vagrant, since its needed to run Singularity
    on a Mac</p>

    <pre><code>brew cask install virtualbox

    brew cask install vagrant

    brew cask install vagrant-manager

    </code></pre>

    <p>If you have trouble install Vagrant with homebrew, try using <a href="https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg"
    rel="nofollow">this</a>.</p>

    <h1>

    <a id="user-content-creating-the-container" class="anchor" href="#creating-the-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Creating
    the Container</h1>

    <p>The workflow for creating a Singularity container on a Mac through Vagrant
    is saved in the included <code>Makefile</code>.</p>

    <p>Make the container by running:</p>

    <div class="highlight highlight-source-shell"><pre>make container</pre></div>

    <p>And run a test on the created container with</p>

    <div class="highlight highlight-source-shell"><pre>make <span class="pl-c1">test</span></pre></div>

    <h2>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h2>

    <p>If everything worked, the following files should be created:</p>

    <ul>

    <li>

    <p><code>singularity-vm/image/singularity-container-samtools</code>: the Singularity
    container file for samtools</p>

    </li>

    <li>

    <p><code>singularity-vm/image/samtools-version.txt</code>: the output from running
    samtools inside the container, should look like this:</p>

    </li>

    </ul>

    <pre><code>samtools 1.6

    Using htslib 1.6

    Copyright (C) 2017 Genome Research Ltd.

    </code></pre>

    <h1>

    <a id="user-content-resources" class="anchor" href="#resources" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h1>

    <p><a href="http://singularity.lbl.gov/install-mac" rel="nofollow">http://singularity.lbl.gov/install-mac</a></p>

    <p><a href="https://app.vagrantup.com/singularityware/boxes/singularity-2.4" rel="nofollow">https://app.vagrantup.com/singularityware/boxes/singularity-2.4</a></p>

    <p><a href="https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg"
    rel="nofollow">https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg</a></p>

    <p><a href="http://singularity.lbl.gov/docs-build-container" rel="nofollow">http://singularity.lbl.gov/docs-build-container</a></p>

    <p><a href="http://singularity.lbl.gov/docs-recipes" rel="nofollow">http://singularity.lbl.gov/docs-recipes</a></p>

    <p><a href="https://github.com/qbicsoftware/qbic-singularity-samtools">https://github.com/qbicsoftware/qbic-singularity-samtools</a></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - singularity
  - singularity-container
  updated_at: 1521728818.0
stevekm/vep-annotation-nf:
  data_format: 2
  description: variant annotation workflow with VEP
  filenames:
  - container/Singularity.vep-96.0
  full_name: stevekm/vep-annotation-nf
  latest_release: null
  readme: '<h1>

    <a id="user-content-vep-annotation-nf" class="anchor" href="#vep-annotation-nf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>vep-annotation-nf</h1>

    <p>Demo pipeline for annotating variants in .vcf files using <a href="https://useast.ensembl.org/info/docs/tools/vep/index.html"
    rel="nofollow">Variant Effect Predictor</a> (VEP).</p>

    <h1>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <p>Clone this repo:</p>

    <pre><code>git clone https://github.com/stevekm/vep-annotation-nf.git

    cd vep-annotation-nf

    </code></pre>

    <h2>

    <a id="user-content-nextflow" class="anchor" href="#nextflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Nextflow</h2>

    <p>Install <code>nextflow</code> in the current directory with the command in
    the Makefile.</p>

    <pre><code>make install

    </code></pre>

    <h2>

    <a id="user-content-vep-docker" class="anchor" href="#vep-docker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>VEP: Docker</h2>

    <p>To install VEP using Docker, run the Makefile command in the <code>container</code>
    directory.</p>

    <pre><code>cd container

    make docker-build

    </code></pre>

    <h2>

    <a id="user-content-vep-conda" class="anchor" href="#vep-conda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>VEP: conda</h2>

    <p>To install VEP using <code>conda</code> (for NYULMC Big Purple HPC), instead
    run the <code>conda-install</code> recipe from the Makefile in the parent repo
    directory.</p>

    <pre><code>make conda-install

    </code></pre>

    <h2>

    <a id="user-content-reference-files" class="anchor" href="#reference-files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Reference Files</h2>

    <p>VEP reference files will be downloaded automatically by the pipeline. However
    the hg19 genome fasta, fasta.fai, and fasta.dict files must also be obtained (not
    included; try <a href="https://support.illumina.com/sequencing/sequencing_software/igenome.html"
    rel="nofollow">these</a>). On NYULMC Big Purple, all required files are already
    cached and no download should be needed. On other systems, the command line arguments
    specifying the genome fasta files should be provided separately when running,
    or place the files <code>genome.fa</code>, <code>genome.fa.fai</code>, and <code>genome.dict</code>
    inside the included <code>ref</code> directory.</p>

    <h1>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h1>

    <p>The Makefile includes shortcuts to help run the pipeline easier on NYULMC Big
    Purple HPC.</p>

    <pre><code>make run

    </code></pre>

    <p>The command can also be used to run on other systems, it will simply invoke
    the command:</p>

    <pre><code>./nextflow run main.nf -resume

    </code></pre>

    <p>Nextflow <code>params</code> values can be passed on the command line:</p>

    <pre><code>./nextflow run main.nf -resume --ref_fa /path/to/genome.fa --ref_fai
    /path/to/genome.fa.fai --ref_dict /path/to/genome.dict

    </code></pre>

    <h1>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h1>

    <p>Output files will be collected in the <code>output</code> directory.</p>

    <h1>

    <a id="user-content-software" class="anchor" href="#software" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Software</h1>

    <p>Tested on RHEL 7, macOS 10.12</p>

    <ul>

    <li>

    <p>Nextflow (Java 8+)</p>

    </li>

    <li>

    <p><code>bash</code></p>

    </li>

    <li>

    <p>GNU <code>make</code></p>

    </li>

    <li>

    <p>Python 2.7+</p>

    </li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - vcf
  - annotation
  - vep
  - nextflow
  updated_at: 1617871975.0
sylabs/singularity:
  data_format: 2
  description: SingularityCE is the Community Edition of Singularity, an open source
    container platform designed to be simple, fast, and secure.
  filenames:
  - e2e/testdata/Docker_registry.def
  - e2e/testdata/sshfs.def
  - e2e/testdata/inspecter_container.def
  - e2e/testdata/regressions/issue_5399.def
  - e2e/testdata/regressions/issue_5315.def
  - e2e/testdata/regressions/issue_4967.def
  - e2e/testdata/regressions/issue_4969.def
  - e2e/testdata/regressions/issue_4203.def
  - e2e/testdata/regressions/issue_4583.def
  - e2e/testdata/regressions/issue_5250.def
  - e2e/testdata/regressions/issue_4820.def
  - examples/legacy/2.3/contrib/raspbian.def
  - examples/legacy/2.2/arch.def
  - examples/legacy/2.2/ubuntu.def
  - examples/legacy/2.2/busybox.def
  - examples/legacy/2.2/docker.def
  - examples/legacy/2.2/debian.def
  - examples/legacy/2.2/scientific.def
  - examples/legacy/2.2/centos.def
  - examples/legacy/2.2/contrib/ubuntu-root.def
  - examples/legacy/2.2/contrib/debian85-tensorflow-0.10.def
  - examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1-gpu.def
  - examples/legacy/2.2/contrib/linuxbrew_and_non-root_software_example.def
  - examples/legacy/2.2/contrib/ubuntu-openfoam.def
  - examples/legacy/2.2/contrib/centos7-ompi_master.def
  - examples/legacy/2.2/contrib/ubuntu-bio.def
  - examples/legacy/2.2/contrib/centos-minimal.def
  - examples/legacy/2.2/contrib/centos7-ompi_cuda.def
  - examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1.def
  - examples/legacy/2.2/contrib/r_python_julia.def
  - examples/legacy/2.2/contrib/fedora.def
  - examples/build-singularity/build-singularity.def
  full_name: sylabs/singularity
  latest_release: v3.8.0
  readme: '<h1>

    <a id="user-content-singularityce" class="anchor" href="#singularityce" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SingularityCE</h1>

    <p><a href="https://circleci.com/gh/sylabs/singularity/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li><a href="CONTRIBUTING.md">Guidelines for Contributing</a></li>

    <li><a href=".github/PULL_REQUEST_TEMPLATE.md">Pull Request Template</a></li>

    <li><a href="LICENSE.md">Project License</a></li>

    <li><a href="https://www.sylabs.io/docs/" rel="nofollow">Documentation</a></li>

    <li><a href="https://github.com/sylabs/singularityce-community">Community Meetings
    / Minutes / Roadmap</a></li>

    <li><a href="#support">Support</a></li>

    <li><a href="#citing-singularity">Citation</a></li>

    </ul>

    <p>SingularityCE is the Community Edition of Singularity, an open source container

    platform designed to be simple, fast, and secure. Singularity is optimized

    for compute focused enterprise and HPC workloads, allowing untrusted users

    to run untrusted containers in a trusted way.</p>

    <p>Check out <a href="https://www.sylabs.io/videos" rel="nofollow">talks about
    Singularity</a> and some <a href="https://sylabs.io/case-studies" rel="nofollow">use

    cases of Singularity</a> on our website.</p>

    <h2>

    <a id="user-content-getting-started-with-singularityce" class="anchor" href="#getting-started-with-singularityce"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with SingularityCE</h2>

    <p>To install SingularityCE from source, see the <a href="INSTALL.md">installation

    instructions</a>. For other installation options, see <a href="https://www.sylabs.io/guides/latest/admin-guide/"
    rel="nofollow">our

    guide</a>.</p>

    <p>System administrators can learn how to configure SingularityCE, and get an

    overview of its architecture and security features in the <a href="https://www.sylabs.io/guides/latest/admin-guide/"
    rel="nofollow">administrator

    guide</a>.</p>

    <p>For users, see the <a href="https://www.sylabs.io/guides/latest/user-guide/"
    rel="nofollow">user

    guide</a> for details on how to use

    and build Singularity containers.</p>

    <h2>

    <a id="user-content-contributing-to-singularityce" class="anchor" href="#contributing-to-singularityce"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    to SingularityCE</h2>

    <p>Community contributions are always greatly appreciated. To start developing

    SingularityCE, check out the <a href="CONTRIBUTING.md">guidelines for contributing</a>.</p>

    <p>Our roadmap, other documents, and user/developer meeting information can be

    found in the <a href="https://github.com/sylabs/singularityce-community">singularityce-community
    repository</a>.</p>

    <p>We also welcome contributions to our <a href="https://github.com/sylabs/singularity-userdocs">user

    guide</a> and <a href="https://github.com/sylabs/singularity-admindocs">admin

    guide</a>.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>To get help with SingularityCE, check out the community spaces

    detailed at our <a href="https://www.sylabs.io/singularity/community/" rel="nofollow">Community

    Portal</a>.</p>

    <p>See also our <a href="SUPPORT.md">Support Guidelines</a> for further

    information about the best place, and how, to raise different kinds of

    issues and questions.</p>

    <p>For additional support, <a href="https://www.sylabs.io/contact/" rel="nofollow">contact
    us</a> to receive

    more information.</p>

    <h2>

    <a id="user-content-citing-singularity" class="anchor" href="#citing-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Citing
    Singularity</h2>

    <pre><code>Kurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers
    for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459

    </code></pre>

    <p>We also have a Zenodo citation:</p>

    <pre><code>Kurtzer, Gregory M. et. al. Singularity - Linux application and environment

    containers for science. 10.5281/zenodo.1310023

    </code></pre>

    <p><a href="https://doi.org/10.5281/zenodo.1310023" rel="nofollow">https://doi.org/10.5281/zenodo.1310023</a></p>

    <p>This is an ''all versions'' DOI. Follow the link to Zenodo to obtain a DOI
    specific

    to a particular version of Singularity.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p><em>Unless otherwise noted, this project is licensed under a 3-clause BSD license

    found in the <a href="LICENSE.md">license file</a>.</em></p>

    '
  stargazers_count: 47
  subscribers_count: 9
  topics:
  - containers
  - hpc
  - linux
  updated_at: 1624765003.0
sylvainschmitt/singularity-octopus:
  data_format: 2
  description: 'octopus Singularity container '
  filenames:
  - Singularity
  full_name: sylvainschmitt/singularity-octopus
  latest_release: 0.0.1
  readme: "<h1>\n<a id=\"user-content-octopussingularity-container\" class=\"anchor\"\
    \ href=\"#octopussingularity-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"https://github.com/luntergroup/octopus\"\
    >Octopus</a>\n<a href=\"https://github.com/hpcng/singularity\">Singularity</a>\
    \ container</h1>\n<p>Sylvain Schmitt\nApril 28, 2021</p>\n<p><strong>Bionformatics\
    \ software Octopus</strong></p>\n<p>Octopus is a mapping-based variant caller\
    \ that implements several\ncalling models within a unified haplotype-aware framework.\
    \ Octopus takes\ninspiration from particle filtering by constructing a tree of\
    \ haplotypes\nand dynamically pruning and extending the tree based on haplotype\n\
    posterior probabilities in a sequential manner. This allows octopus to\nimplicitly\
    \ consider all possible haplotypes at a given loci in\nreasonable time.</p>\n\
    <p>Octopus Version: 0.7.4</p>\n<p>[<a href=\"https://github.com/luntergroup/octopus\"\
    >https://github.com/luntergroup/octopus</a>]</p>\n<p>Singularity container based\
    \ on the recipe: Singularity.template.def</p>\n<p>Package installation using Miniconda3\
    \ V4.7.12</p>\n<p>Image singularity (V&gt;=3.3) is automatically test and built\
    \ and pushed\non the registry using\n<a href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\
    >test.yml</a>\n&amp;\n<a href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\
    >builder.yml</a></p>\n<p><strong>build</strong>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity build octopus.sif Singularity\nsingularity run octopus.sif\n\
    singularity <span class=\"pl-c1\">exec</span> octopus.sif octopus -h</pre></div>\n\
    <p><strong>pull</strong>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif</pre></div>\n\
    <p><strong>snakemake</strong>:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre>    <span class=\"pl-s1\">singularity</span>: \n        <span class=\"pl-s\"\
    >\"https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif\"\
    </span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623243296.0
sysmso/singularity-multinest:
  data_format: 2
  description: A container for PyMultinest
  filenames:
  - Singularity
  full_name: sysmso/singularity-multinest
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-multinest" class="anchor" href="#singularity-multinest"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-multinest</h1>

    <p>A container for PyMultinest</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602594100.0
tanhnhn/singularityhub-sregistry:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: tanhnhn/singularityhub-sregistry
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-registry" class="anchor" href="#singularity-registry"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Registry</h1>

    <p><a href="http://joss.theoj.org/papers/050362b7e7691d2a5d0ebed8251bc01e" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4cb65855144c475cbe5584c579404a17e3e6984f958da24427dbe46b6202eb3c/687474703a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f30353033363262376537363931643261356430656265643832353162633031652f7374617475732e737667"
    alt="status" data-canonical-src="http://joss.theoj.org/papers/050362b7e7691d2a5d0ebed8251bc01e/status.svg"
    style="max-width:100%;"></a>

    <a href="https://doi.org/10.5281/zenodo.1012531" rel="nofollow"><img src="https://camo.githubusercontent.com/411f713db9ba01edfcb60386aaa1dff3e4ed4464707b95d889900a88d8f54936/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313031323533312e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.1012531.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li><a href="https://singularityhub.github.io/sregistry" rel="nofollow">Documentation</a></li>

    </ul>

    <h2>

    <a id="user-content-what-is-singularity-registry" class="anchor" href="#what-is-singularity-registry"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What
    is Singularity Registry</h2>

    <p>Singularity Registry is a management and storage of Singularity images for
    an institution or user to deploy locally. It does not manage building, but serves
    endpoints to obtain and save containers. The Registry is expected to be available
    for use in the Fall.</p>

    <h2>

    <a id="user-content-images-included" class="anchor" href="#images-included" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Images Included</h2>

    <p>Singularity Registry consists of several Docker images, and they are integrated
    to work together using <a href="docker-compose.yml">docker-compose.yml</a>. The
    images are the following:</p>

    <ul>

    <li>

    <strong>vanessa/sregistry</strong>: is the main uwsgi application, which serves
    a Django (python-based) application.</li>

    <li>

    <strong>nginx</strong>: pronounced (engine-X) is the webserver. The starter application
    is configured for http, however you should follow the instructions to set up https
    properly.</li>

    <li>

    <strong>worker</strong>: is the same uwsgi image, but with a running command that
    is specialized to perform tasks. The tasks are run via <a href="http://www.celeryproject.org/"
    rel="nofollow">celery</a>, a distributed job queue that fits nicely into Django.
    The celery worker uses a</li>

    <li>

    <strong>redis</strong>: database to organize the jobs themselves.</li>

    </ul>

    <p>For more information about Singularity Registry, please reference the <a href="https://singularityhub.github.io/sregistry"
    rel="nofollow">docs</a>. If you have any issues, please <a href="https://github.com/singularityhub/sregistry/issues">let
    me know</a>!</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>This code is licensed under the Affero GPL, version 3.0 or later <a href="LICENSE">LICENSE</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1513562903.0
ternaustralia/coesra-singularity-canopy:
  data_format: 2
  description: null
  filenames:
  - Singularity.canopy
  full_name: ternaustralia/coesra-singularity-canopy
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-canopy" class="anchor" href="#coesra-singularity-canopy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-canopy</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610425023.0
ternaustralia/coesra-singularity-dropbox:
  data_format: 2
  description: null
  filenames:
  - Singularity.dropbox
  full_name: ternaustralia/coesra-singularity-dropbox
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-dropbox" class="anchor" href="#coesra-singularity-dropbox"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-dropbox</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1610425054.0
ternaustralia/coesra-singularity-jupyter:
  data_format: 2
  description: null
  filenames:
  - Singularity.jupyter
  full_name: ternaustralia/coesra-singularity-jupyter
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-jupyter" class="anchor" href="#coesra-singularity-jupyter"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-jupyter</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1610425229.0
ternaustralia/coesra-singularity-kepler:
  data_format: 2
  description: null
  filenames:
  - Singularity.kepler
  full_name: ternaustralia/coesra-singularity-kepler
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-kepler" class="anchor" href="#coesra-singularity-kepler"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-kepler</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610425796.0
ternaustralia/coesra-singularity-knime:
  data_format: 2
  description: Knime
  filenames:
  - Singularity.knime
  full_name: ternaustralia/coesra-singularity-knime
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-knime" class="anchor" href="#coesra-singularity-knime"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-knime</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610426074.0
ternaustralia/coesra-singularity-macroecodesktop:
  data_format: 2
  description: null
  filenames:
  - Singularity.macroecodesktop
  full_name: ternaustralia/coesra-singularity-macroecodesktop
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-openrefine" class="anchor" href="#coesra-singularity-openrefine"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-openrefine</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610426323.0
ternaustralia/coesra-singularity-openrefine:
  data_format: 2
  description: null
  filenames:
  - Singularity.openrefine
  full_name: ternaustralia/coesra-singularity-openrefine
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-openrefine" class="anchor" href="#coesra-singularity-openrefine"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-openrefine</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610426463.0
ternaustralia/coesra-singularity-owncloud:
  data_format: 2
  description: Owncloud
  filenames:
  - Singularity.owncloud
  full_name: ternaustralia/coesra-singularity-owncloud
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-owncloud" class="anchor" href="#coesra-singularity-owncloud"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-owncloud</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610426521.0
ternaustralia/coesra-singularity-panoply:
  data_format: 2
  description: null
  filenames:
  - Singularity.panoply
  full_name: ternaustralia/coesra-singularity-panoply
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-panoply" class="anchor" href="#coesra-singularity-panoply"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-panoply</h1>

    <p>Hoang Nguyen

    25 July 2019</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610426866.0
ternaustralia/coesra-singularity-qgis:
  data_format: 2
  description: null
  filenames:
  - Singularity.qgis
  full_name: ternaustralia/coesra-singularity-qgis
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-qgis" class="anchor" href="#coesra-singularity-qgis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-qgis</h1>

    <p>Hoang Nguyen 24 July 2019</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1610427940.0
ternaustralia/coesra-singularity-rstudio:
  data_format: 2
  description: null
  filenames:
  - Singularity.rstudio
  full_name: ternaustralia/coesra-singularity-rstudio
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-rstudio" class="anchor" href="#coesra-singularity-rstudio"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-rstudio</h1>

    <p>Hoang Nguyen

    25 July 2019</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - coesra
  updated_at: 1610424737.0
tgac-vumc/BLADE:
  data_format: 2
  description: 'BLADE: Bayesian Log-normAl DEconvolution for enhanced in silico microdissection
    of bulk gene expression data'
  filenames:
  - Singularity
  full_name: tgac-vumc/BLADE
  latest_release: null
  readme: "<p align=\"center\">\n  <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/logo_final_small.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"254\" height=\"281\"\
    \ src=\"https://github.com/tgac-vumc/BLADE/raw/master/logo_final_small.png\" style=\"\
    max-width:100%;\"></a>\n</p>\n<h1>\n<a id=\"user-content-blade-bayesian-log-normal-deconvolution\"\
    \ class=\"anchor\" href=\"#blade-bayesian-log-normal-deconvolution\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>BLADE:\
    \ Bayesian Log-normAl DEconvolution</h1>\n<p><a href=\"https://www.python.org/downloads/release/python-360/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8e26ba5220a7019a30342315ff5cc4989f91e698fdfe73a41476dd57524385d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e737667\"\
    \ alt=\"Python 3.6\" data-canonical-src=\"https://img.shields.io/badge/python-3.6-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://badge.fury.io/py/BLADE-Deconvolution\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/09a5fb429236fe0ed8858f2cd7a0d424aa6a58af4fa0bdfda86aa97e8409c118/68747470733a2f2f62616467652e667572792e696f2f70792f424c4144452d4465636f6e766f6c7574696f6e2e737667\"\
    \ alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/BLADE-Deconvolution.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/4861\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://mybinder.org/v2/gh/tgac-vumc/BLADE/master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a></p>\n<p>BLADE (Bayesian Log-normAl DEconvolution) was designed\
    \ to jointly estimate cell type composition and gene expression profiles per cell\
    \ type in a single-step while accounting for the observed gene expression variability\
    \ in single-cell RNA-seq data.</p>\n<p align=\"center\">\n  <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/framework.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"100%\" height=\"\
    100%\" src=\"https://github.com/tgac-vumc/BLADE/raw/master/framework.png\" style=\"\
    max-width:100%;\"></a>\n</p>\n<p>BLADE framework. To construct a prior knowledge\
    \ of BLADE, we used single-cell sequencing data. Cell are subject to phenotyping,\
    \ clustering and differential gene expression analysis. Then, for each cell type,\
    \ we retrieve average expression profiles (red cross and top heatmap), and standard\
    \ deviation per gene (blue circle and bottom heatmap). This prior knowledge is\
    \ then used in the hierarchical Bayesian model (bottom right) to deconvolute bulk\
    \ gene expression data.</p>\n<h4>\n<a id=\"user-content-demo-notebook-is-available-here-you-can-also-run-the-demo-using-binder\"\
    \ class=\"anchor\" href=\"#demo-notebook-is-available-here-you-can-also-run-the-demo-using-binder\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Demo notebook is available <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/jupyter/BLADE%20-%20Demo%20script.ipynb\"\
    >here</a>. You can also run the demo using <a href=\"https://mybinder.org/v2/gh/tgac-vumc/BLADE/master\"\
    \ rel=\"nofollow\">Binder</a>.</h4>\n<p>Note that for the testing on Binder, parallel\
    \ processing has to be disabled by setting <code>Njob</code> to 1. BLADE significantly\
    \ performs better with high number of cores, epecially when <code>Nsample</code>,\
    \ <code>Ngene</code> and <code>Ncell</code> is high. In case of Binder, we recommend\
    \ the following setting:</p>\n<ul>\n<li><code>Ncell=3</code></li>\n<li><code>Ngene=50</code></li>\n\
    <li><code>Nsample=10</code></li>\n</ul>\n<p>It takes about 30 minutes to complete\
    \ the demo execution on Binder.</p>\n<h2>\n<a id=\"user-content-system-requirements\"\
    \ class=\"anchor\" href=\"#system-requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>System Requirements</h2>\n<h3>\n\
    <a id=\"user-content-hardware-requirements\" class=\"anchor\" href=\"#hardware-requirements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Hardware Requirements</h3>\n<p>BLADE can run on the minimal computer\
    \ spec, such as Binder (1 CPU, 2GB RAM on Google Cloud), when data size is small.\
    \ However, BLADE can significantly benefit from the larger amount of CPUs and\
    \ RAM. Empirical Bayes procedure of BLADE runs independent optimization procedure\
    \ that can be parallelized. In our evaluation, we used a computing node with the\
    \ following spec:</p>\n<ul>\n<li>40 threads (Xeon 2.60GHz)</li>\n<li>128 GB RAM</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-os-requirements\" class=\"anchor\" href=\"#os-requirements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>OS Requirements</h3>\n<p>The package development version is tested\
    \ on Linux operating systems. (CentOS 7 and Ubuntu 16.04).</p>\n<h2>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <h3>\n<a id=\"user-content-using-pip\" class=\"anchor\" href=\"#using-pip\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using\
    \ pip</h3>\n<p>The python package of BLADE is available on pip.\nYou can simply\
    \ (takes only &lt;1min):</p>\n<pre><code>pip install BLADE_Deconvolution\n</code></pre>\n\
    <p>We tested BLADE with <code>python =&gt; 3.6</code>.</p>\n<h3>\n<a id=\"user-content-using-conda\"\
    \ class=\"anchor\" href=\"#using-conda\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Conda</h3>\n<p>One can\
    \ create a conda environment contains BLADE and also other dependencies to run\
    \ <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/jupyter/BLADE%20-%20Demo%20script.ipynb\"\
    >Demo</a>.\nThe environment definition is in <a href=\"https://github.com/tgac-vumc/BLADE/environment.yml\"\
    >environment.yml</a>.</p>\n<h3>\n<a id=\"user-content-step-1-installing-miniconda-3\"\
    \ class=\"anchor\" href=\"#step-1-installing-miniconda-3\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1:\
    \ Installing Miniconda 3</h3>\n<p>First, please open a terminal or make sure you\
    \ are logged into your Linux VM. Assuming that you have a 64-bit system, on Linux,\
    \ download and install Miniconda 3 with:</p>\n<pre><code>wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
    bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>\n<p>On MacOS X, download\
    \ and install with:</p>\n<pre><code>curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\
    \ -o Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\
    </code></pre>\n<h3>\n<a id=\"user-content-step-2-create-a-conda-environment\"\
    \ class=\"anchor\" href=\"#step-2-create-a-conda-environment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 2: Create a conda environment</h3>\n<p>You can install all the necessary dependency\
    \ using the following command (may takes few minutes).</p>\n<pre><code>conda env\
    \ create --file environment.yml\n</code></pre>\n<p>Then, the <code>BLADE</code>\
    \ environment can be activate by:</p>\n<pre><code>conda activate BLADE\n</code></pre>\n\
    <h3>\n<a id=\"user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Singularity</h3>\n<p>If you have Singularity, you can simply\
    \ pull the singularity container with all dependency resolved (in few minutes,\
    \ depends on the network speed).</p>\n<pre><code>singularity pull shub://tgac-vumc/BLADE\n\
    </code></pre>\n<h2>\n<a id=\"user-content-overview-of-blade\" class=\"anchor\"\
    \ href=\"#overview-of-blade\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Overview of BLADE</h2>\n<p>In the\
    \ BLADE package, you can load the following functions and modules.</p>\n<ul>\n\
    <li>\n<p><code>BLADE</code>: A class object contains core algorithms of <code>BLADE</code>.\
    \ Users can reach internal variables (<code>Nu</code>, <code>Omega</code>, and\
    \ <code>Beta</code>) and functions for calculating objective functions (ELBO function)\
    \ and gradients with respect to the variational parameters. There also is an optimization\
    \ function (<code>BLADE.Optimize()</code>) for performing L-BFGS optimization.\
    \ Though this is the core, we also provide a more accessible function (<code>BLADE_framework</code>)\
    \ that performs deconvolution. See below to obtain the current estimate of cellualr\
    \ fractions, gene expression profiles per cell type and per sample:</p>\n<ul>\n\
    <li>\n<code>ExpF(self.Beta)</code> : returns a <code>Nsample</code> by <code>Ngene</code>\
    \ matrix contains estimated fraction of each cell type in each sample.</li>\n\
    <li>\n<code>self.Nu</code>: a <code>Nsample</code> by <code>Ngene</code> by <code>Ncell</code>\
    \ multidimensional array contains estimated gene expression levels of each gene\
    \ in each cell type for each sample.</li>\n<li>\n<code>numpy.mean(self.Nu,0)</code>:\
    \ To obtain a estimated gene expression profile per cell type, we can simply take\
    \ an average across the samples.</li>\n</ul>\n</li>\n<li>\n<p><code>Framework</code>:\
    \ A framework based on the <code>BLADE</code> class module above. Users need to\
    \ provide the following input/output arguments.</p>\n<ul>\n<li>Input arguments\n\
    <ul>\n<li>\n<code>X</code>: a <code>Ngene</code> by <code>Ncell</code> matrix\
    \ contains average gene expression profiles per cell type (a signature matrix)\
    \ in log-scale.</li>\n<li>\n<code>stdX</code>: a <code>Ngene</code> by <code>Ncell</code>\
    \ matrix contains standard deviation per gene per cell type (a signature matrix\
    \ of gene expression variability).</li>\n<li>\n<code>Y</code>: a <code>Ngene</code>\
    \ by <code>Nsample</code> matrix contains bulk gene expression data. This should\
    \ be in linear-scale data without log-transformation.</li>\n<li>\n<code>Ind_Marker</code>:\
    \ Index for marker genes. By default, <code>[True]*Ngene</code> (all genes used\
    \ without filtering). For the genes with <code>False</code> they are excluded\
    \ in the first phase (Empirical Bayes) for finidng the best hyperparameters.</li>\n\
    <li>\n<code>Ind_sample</code>: Index for the samples used in the first phase (Empirical\
    \ Bayes). By default, <code>[True]*Nsample</code> (all samples used).</li>\n<li>\n\
    <code>Alphas</code>, <code>Alpha0s</code>, <code>Kappa0s</code> and <code>SYs</code>:\
    \ all possible hyperparameters considered in the phase of Empirical Bayes. A default\
    \ parameters are offered as described in the manuscript (to appear): <code>Alphas=[1,10]</code>,\
    \ <code>Alpha0s=[0.1, 1, 5]</code>, <code>Kappa0s=[1,0.5,0.1]</code> and <code>SYs=[1,0.3,0.5]</code>.</li>\n\
    <li>\n<code>Nrep</code>: Number of repeat for evaluating each parameter configuration\
    \ in Empirical Bayes phase. By default, <code>Nrep=3</code>.</li>\n<li>\n<code>Nrepfinal</code>:\
    \ Number of repeated optimizations for the final parameter set. By default, <code>Nrepfinal=10</code>.</li>\n\
    <li>\n<code>Njob</code>: Number of jobs executed in parallel. By default, <code>Njob=10</code>.</li>\n\
    </ul>\n</li>\n<li>Output values\n<ul>\n<li>\n<code>final_obj</code>: A final <code>BLADE</code>\
    \ object with optimized variational parameters and hyperparameters.</li>\n<li>\n\
    <code>best_obj</code>: The best object form Empirical Bayes step. If no genes\
    \ and samples are filtered, <code>best_obj</code> is the same as <code>final_obj</code>.</li>\n\
    <li>\n<code>best_set</code>: A list contains the hyperparameters selected in the\
    \ Empirical Bayes step.</li>\n<li>\n<code>All_out</code>: A list of <code>BLADE</code>\
    \ objects from the Empirical Bayes step.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n\
    <p><code>BLADE_job</code>/<code>Optimize</code>: Internal functions used by <code>Framework</code>.</p>\n\
    </li>\n</ul>\n"
  stargazers_count: 5
  subscribers_count: 7
  topics: []
  updated_at: 1623031718.0
tgac-vumc/QDNAseq.snakemake:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: tgac-vumc/QDNAseq.snakemake
  latest_release: null
  readme: "<p align=\"center\">\n  <a href=\"https://github.com/tgac-vumc/QDNAseq.snakemake/blob/master/DAG_all.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"100%\" height=\"\
    100%\" src=\"https://github.com/tgac-vumc/QDNAseq.snakemake/raw/master/DAG_all.svg\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>For the\
    \ installation of this pipeline any Python install compatable Conda is required.</p>\n\
    <p>The pipeline itself will run on Python 3.8.5 and R 3.6.3. For exact dependencies\
    \ view <code>environment.yaml</code> and <code>r-dependencies.R</code>.</p>\n\
    <h3>\n<a id=\"user-content-using-condamamba\" class=\"anchor\" href=\"#using-condamamba\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Conda/Mamba</h3>\n<p>for easy installation you need (Mini)Conda.</p>\n\
    <p>Miniconda installation from folder where you want to install Miniconda:</p>\n\
    <pre><code>cd &lt;/path/to/files/dir/&gt;\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
    bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>\n<p>follow the instructions\
    \ of the installation process, give the location where you want Miniconda to be\
    \ installed and answer YES to add Miniconda to your path.</p>\n<p>go to the directory\
    \ where the analysis need to be performed</p>\n<pre><code>cd &lt;/path/to/analysis/dir&gt;\n\
    git clone https://github.com/tgac-vumc/QDNAseq.snakemake/\ncd QDNAseq.snakemake\n\
    </code></pre>\n<p>install Mamba as drop-in replacement for Conda with Mamba's\
    \ improved installation-performance:</p>\n<pre><code>conda install -c conda-forge\
    \ mamba\n</code></pre>\n<p>create  the environment using Mamba:</p>\n<pre><code>mamba\
    \ env create --name QDNAseq-snakemake --file environment.yaml\n</code></pre>\n\
    <p>activate the environment by:</p>\n<pre><code>conda activate QDNAseq-snakemake\n\
    </code></pre>\n\n<p>Then run the R-script r-dependencies.R in the terminal to\
    \ install the non-conda R dependencies in the environment:</p>\n<pre><code>Rscript\
    \ r-dependencies.R\n</code></pre>\n<h3>\n<a id=\"user-content-using-singularity\"\
    \ class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Singularity</h3>\n<p>Under\
    \ development</p>\n\n<h2>\n<a id=\"user-content-preparing-analysis\" class=\"\
    anchor\" href=\"#preparing-analysis\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Preparing analysis</h2>\n<h3>\n\
    <a id=\"user-content-prepare-the-data\" class=\"anchor\" href=\"#prepare-the-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prepare the data</h3>\n<p>go to analysis dir and prepare analysis\
    \ by copy or create links to fastq.gz files:</p>\n<pre><code>cd &lt;/path/to/analysis/dir&gt;\n\
    \nmkdir fastq\ncd fastq\n</code></pre>\n<p>to link a single file:</p>\n<pre><code>ln\
    \ -s &lt;path/to/file&gt;\n</code></pre>\n<p>to link all files from a folder:</p>\n\
    <pre><code>for file in &lt;path/to/fastq/files&gt;/*.fastq.gz\ndo ln -s $file\n\
    done\n</code></pre>\n<h3>\n<a id=\"user-content-prepare-the-snakemake-settings\"\
    \ class=\"anchor\" href=\"#prepare-the-snakemake-settings\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Prepare\
    \ the snakemake settings</h3>\n<p>Open the configuration file <code>config.yaml</code>\
    \ to check the settings that snakemake will use and change according to your needs.\n\
    For providing service-analysis, set <code>setting</code> to <code>'service'</code>.\
    \ For research purposes, set <code>setting</code> to <code>'research'</code>.\
    \ For all settings set <code>setting</code> to <code>'all'</code>.</p>\n<p>One\
    \ of the options in the configfile is <code>dewaving</code>, if set to <code>'true'</code>\
    \ QNDAseq objects will be dewaved before segmentation.</p>\n<p>These options change\
    \ the rules performed in the pipeline, see the rule-graph in the next section.</p>\n\
    <h2>\n<a id=\"user-content-running-analysis\" class=\"anchor\" href=\"#running-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running analysis</h2>\n<p>Make sure that snakemake is able to find\
    \ the excecutive file Snakefile by performing a dry-run:</p>\n<pre><code>cd ../QDNAseq.snakemake\n\
    snakemake -n\n</code></pre>\n<p>Check the rules that are planned to be performed,\
    \ conform the rule-graph.</p>\n<p>An visualization of the order of rules to be\
    \ performed can be viewed by running the following command and opening the DAG-file</p>\n\
    <pre><code>snakemake --forceall --rulegraph | dot -Tsvg &gt; DAG.svg\n</code></pre>\n\
    <p>Rulegraphs for the intial settings <code>'service'</code>, <code>'research'</code>\
    \ and <code>'all'</code> are commited to this repro in the files <code>DAG_&lt;setting&gt;.svg</code>.</p>\n\
    <p>When ready, run the analysis</p>\n<pre><code>snakemake\n</code></pre>\n<p>Useful\
    \ snakemake options</p>\n<p><code>-j , --cores, --jobs</code> : Use at most N\
    \ cores in parallel (default: 1). If N is omitted, the limit is set to the number\
    \ of available cores.</p>\n<p><code>-n , --dryrun</code> : Do not execute anything.\
    \ but show rules which are planned to be performed.</p>\n<p><code>-k , --keep-going</code>\
    \ : Go on with independent jobs if a job fails.</p>\n<p><code>-f , --force</code>\
    \ : Force the execution of the selected target or the first rule regardless of\
    \ already created output.</p>\n<p><code>-R , --forcerun</code> : Force the re-execution\
    \ or creation of the given rules or files. Use this option if you changed a rule\
    \ and want to have all its output in your workflow updated.</p>\n<p><code>-U ,\
    \ --until</code> : Runs the pipeline until it reaches the specified rules or files.\
    \ Only runs jobs that are dependencies of the specified rule or files, does not\
    \ run sibling DAGs.</p>\n<p>for all options go to <a href=\"https://snakemake.readthedocs.io/en/v5.31.1/executing/cli.html#all-options\"\
    \ rel=\"nofollow\">https://snakemake.readthedocs.io/en/v5.31.1/executing/cli.html#all-options</a></p>\n"
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1622800091.0
tgac-vumc/RNA-seq:
  data_format: 2
  description: RNA-seq analysis pipeline based on Snakemake
  filenames:
  - Singularity
  full_name: tgac-vumc/RNA-seq
  latest_release: v1.0.0
  readme: "<h1>\n<a id=\"user-content-rna-seq-analysis-pipeline\" class=\"anchor\"\
    \ href=\"#rna-seq-analysis-pipeline\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>RNA-seq analysis pipeline</h1>\n\
    <p><a href=\"https://snakemake.bitbucket.io\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9e0a726dc69516d51067fd9fc2074a9f2dc9d44eb069ae05434a36f580af32f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b653d3d352e32352e302d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265\"\
    \ alt=\"Snakemake\" data-canonical-src=\"https://img.shields.io/badge/snakemake==5.25.0-brightgreen.svg?style=flat-square\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://singularity-hub.org/collections/3066\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c9d2afb620129b7ba0f4d918b77bfdb2b91c595cd6c6d013e950ee6e3c2bbc55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d73696e67756c61726974792d2d6875622d7265642e737667\"\
    \ alt=\"singularity-hub\" data-canonical-src=\"https://img.shields.io/badge/install%20with-singularity--hub-red.svg\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://docs.conda.io/en/latest/miniconda.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e225eb3891735f81d51e8e6aa377429328cfd43656973ff807bffe9234bc28c7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d636f6e64612d677265656e2e737667\"\
    \ alt=\"miniconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-conda-green.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>This is a <a href=\"https://snakemake.readthedocs.io/en/stable/\"\
    \ rel=\"nofollow\">Snakemake</a> based pipeline for RNA-seq used in the <a href=\"\
    http://www.tgac.nl/\" rel=\"nofollow\">Tumor Genome Core Analysis</a> housed in\
    \ the <a href=\"https://www.vumc.com/departments/cancer-center-amsterdam.htm\"\
    \ rel=\"nofollow\">Cancer Center Amsterdam</a>, at <a href=\"https://www.vumc.nl/\"\
    \ rel=\"nofollow\">Amsterdam UMC location VUmc</a> and part of the Department\
    \ of Pathology.</p>\n<p>The pipeline processes raw data from FastQ inputs (<a\
    \ href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"nofollow\"\
    >FastQC</a>, <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\" rel=\"\
    nofollow\">Trimmomatic</a>), aligns the reads (<a href=\"https://github.com/alexdobin/STAR\"\
    >STAR</a>), generates gene counts (<a href=\"http://bioinf.wehi.edu.au/featureCounts/\"\
    \ rel=\"nofollow\">featureCounts</a>) and performs quality-control on the results\
    \ (<a href=\"https://multiqc.info/\" rel=\"nofollow\">MultiQC</a>). Paired-end\
    \ (PE) and single read (SR) are supported.</p>\n<p align=\"center\">\n  <a href=\"\
    https://github.com/tgac-vumc/RNA-seq/blob/master/DAG_RNAseq.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img width=\"850\" height=\"483\" src=\"https://github.com/tgac-vumc/RNA-seq/raw/master/DAG_RNAseq.png\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>The pipeline\
    \ is preliminary used in linux environment with conda/singularity available.</p>\n\
    <h3>\n<a id=\"user-content-using-conda\" class=\"anchor\" href=\"#using-conda\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Conda</h3>\n<h3>\n<a id=\"user-content-step-1-installing-miniconda-3\"\
    \ class=\"anchor\" href=\"#step-1-installing-miniconda-3\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1:\
    \ Installing Miniconda 3</h3>\n<p>First, please open a terminal or make sure you\
    \ are logged into your Linux VM. Assuming that you have a 64-bit system, on Linux,\
    \ download and install Miniconda 3 with:</p>\n<pre><code>wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
    bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>\n<p>On MacOS X, download\
    \ and install with:</p>\n<pre><code>curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\
    \ -o Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\
    </code></pre>\n<h3>\n<a id=\"user-content-step-2-downloading-repository--creating-environment\"\
    \ class=\"anchor\" href=\"#step-2-downloading-repository--creating-environment\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2: Downloading repository &amp; creating environment</h3>\n<pre><code>mkdir\
    \ snakemake_RNAseq\ncd snakemake_RNAseq\ngit clone https://github.com/tgac-vumc/RNA-seq\n\
    conda env create --name RNAseq --file env.yaml\n</code></pre>\n<h3>\n<a id=\"\
    user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Singularity</h3>\n<p>The singularity container holds a virtual\
    \ environment of CentOS 7 and it's available with:</p>\n<pre><code>singularity\
    \ pull shub://tgac-vumc/RNA-seq\n</code></pre>\n<h2>\n<a id=\"user-content-path-configuration--running-the-pipeline\"\
    \ class=\"anchor\" href=\"#path-configuration--running-the-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Path\
    \ Configuration &amp; Running the pipeline</h2>\n<p>Before attempting to run the\
    \ pipeline, please open <em>config.yaml</em>. Inside, you will encounter <strong>Path\
    \ Configuration</strong> and <strong>Software Options</strong>.</p>\n<ol>\n<li>On\
    \ <strong>Path configuration</strong>, first, you have to choose whether your\
    \ data is PE or SR and after change the fastq path to the path where your fastq\
    \ files are actually stored.</li>\n<li>On <strong>Software Options</strong>, you\
    \ will find several options that can be modified by the user. Please, have a look\
    \ at it before running the pipeline.</li>\n</ol>\n<p>All the software used in\
    \ the pipeline is installed by conda or executed in a wrapper. We recommend to\
    \ run the pipeline from a different location than the pipeline path, like the\
    \ example below:</p>\n<pre><code>snakemake -s PATH_TO_PIPELINE/Snakefile --use-conda\
    \ --cores=24\n</code></pre>\n<p>With --use-conda option, the pipeline will create\
    \ environments to run rules based on <em>env.yaml</em>.\n<strong>Note</strong>\
    \ the pipeline assumes that <em>config.yaml</em> is available at the location\
    \ where the pipeline is executed.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1604920129.0
thehyve/singularity-jupyter:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: thehyve/singularity-jupyter
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-jupyter" class="anchor" href="#singularity-jupyter"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-jupyter</h1>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1611165393.0
thomas-robinson/fms_containers:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity.FMS-gcc10-openmpi-netcdf4.6.3-ubuntu
  - Singularity.FMS-gcc10-openmpi-netcdf4.6.3-ubuntu-compile
  full_name: thomas-robinson/fms_containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-fms_containers" class="anchor" href="#fms_containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>fms_containers</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604411747.0
tikk3r/lofar-grid-hpccloud:
  data_format: 2
  description: null
  filenames:
  - Singularity.lofar_sksp_base_cuda
  - Singularity.lofar_sksp_base
  - Singularity.lofar_sksp
  - Singularity.lofar_sksp_base_mkl_cuda
  full_name: tikk3r/lofar-grid-hpccloud
  latest_release: v3.1
  readme: '<p><a href="https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572"
    data-canonical-src="https://img.shields.io/github/v/release/tikk3r/lofar-grid-hpccloud?sort=semver"
    style="max-width:100%;"></a></p>

    <p><a href="https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562"
    data-canonical-src="https://img.shields.io/github/license/tikk3r/lofar-grid-hpccloud.svg?logo=github"
    style="max-width:100%;"></a></p>

    <a href="https://zenodo.org/badge/latestdoi/136925861" rel="nofollow"><img src="https://camo.githubusercontent.com/ad3ce50d6d0bdd702c67f43f248e79b036a12ebf23efdccde0d13eb15d31bf9e/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3133363932353836312e737667"
    data-canonical-src="https://zenodo.org/badge/136925861.svg" style="max-width:100%;"></a>

    <h1>

    <a id="user-content-lofar-grid-hpccloud" class="anchor" href="#lofar-grid-hpccloud"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>lofar-grid-hpccloud</h1>

    <p>This repository hold resources for deploying the LOFAR software (genericpipeline)
    and related tools through Singularity containers. These containers are general,
    but at the same time somewhat tailored for SKSP use.</p>

    <p>The <code>master</code> branch is empty. Currently the images are based on
    the Fedora 27 Linux distribution, which is available from <a href="https://hub.docker.com/_/fedora"
    rel="nofollow">DockerHub</a>. Recipes to build this container can be found on
    the <code>fedora</code> branch.</p>

    <p>To build a full LOFAR Singularity image, do the following:</p>

    <ol>

    <li>

    <p>Build Singularity.lofarbase</p>

    <p>sudo singularity build lofar_sksp_base.sif Singularity.lofar_sksp_base</p>

    </li>

    <li>

    <p>Build Singularity.lofar (use the <code>From: localimage</code> part instead
    of the Singularity Hub part)</p>

    <p>sudo singularity build lofar_sksp.sif Singularity.lofar_sksp</p>

    </li>

    </ol>

    <p>Pre-built containers are public hosted at <a href="https://lofar-webdav.grid.sara.nl/software/shub_mirror/tikk3r/lofar-grid-hpccloud/"
    rel="nofollow">SURFSara</a>. Sort by date to find the latest container there.</p>

    <p>Visit the  <a href="https://github.com/tikk3r/lofar-grid-hpccloud/wiki">wiki</a>
    for more detailed information and build instructions.</p>

    '
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1623918113.0
timo-singularity/rivet:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: timo-singularity/rivet
  latest_release: null
  readme: '<h1>

    <a id="user-content-recipes" class="anchor" href="#recipes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>recipes</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1622810530.0
tjhendrickson/BIDS_scripts:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: tjhendrickson/BIDS_scripts
  latest_release: v1.0
  readme: "<h2>\n<a id=\"user-content-bids-conversion-scripts\" class=\"anchor\" href=\"\
    #bids-conversion-scripts\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>BIDS Conversion Scripts</h2>\n<h3>\n<a id=\"\
    user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Description</h3>\n\
    <p>This repository contains scripts that will assist in the conversion of raw\
    \ DICOM data sets to <a href=\"http://bids.neuroimaging.io/format\" rel=\"nofollow\"\
    >BIDS</a>. The \"heuristics\" folder contains example scripts that have been used\
    \ to convert data from DICOM to BIDS. <strong>This folder may be helpful to build\
    \ your own heuristics script.</strong> For additional information on how to create\
    \ a heuristic script see the <a href=\"https://github.com/nipy/heudiconv\">heudiconv</a>\
    \ github page.</p>\n<h3>\n<a id=\"user-content-container-hosting\" class=\"anchor\"\
    \ href=\"#container-hosting\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Container Hosting</h3>\n<p>Pull the\
    \ most recent container below, (<strong>NOTE: you only have to do this once!</strong>):</p>\n\
    <pre><code>singularity pull shub://tjhendrickson/BIDS_scripts:latest\n</code></pre>\n\
    <h3>\n<a id=\"user-content-singularity-usage\" class=\"anchor\" href=\"#singularity-usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity Usage</h3>\n<pre><code>singularity run /path/to/singularity/images/directory/imagename.img\
    \ --help\nusage: [-h] [--output_dir OUTPUT_DIR] [--dicom_dir DICOM_DIR]\n    \
    \   [--study_name STUDY_NAME] [--ses_id SES_ID] [--subj_id SUBJ_ID]\n       [--heuristic\
    \ HEURISTIC] [--dry_run]\n\nScript that controls BIDS conversion for individual\
    \ studies\n\noptional arguments:\n  -h, --help            show this help message\
    \ and exit\n  --output_dir OUTPUT_DIR\n                        The directory that\
    \ the BIDS data will be outputted to\n  --dicom_dir DICOM_DIR\n              \
    \          The directory that houses unzipped dicom\n                        directories/files.\n\
    \  --study_name STUDY_NAME\n                        What is the shorthand name\
    \ for this study?\n  --ses_id SES_ID       scanning session id\n  --subj_id SUBJ_ID\
    \     subject id\n  --heuristic HEURISTIC\n                        Path to heuristic\
    \ file, if the file is already within\n                        the container (i.e.\
    \ within heuristics folder) you do\n                        not have to specify\
    \ a path.\n  --dry_run             Dry run. A dicominfo_*.tsv file will generate\
    \ within\n                        .heudiconv/'subj_id'/info directory which can\
    \ be used\n                        to create heuristic script\n\n\n</code></pre>\n\
    <p><strong>This application must be run with either the \"--heuristic\" or \"\
    --dry_run\" argument, it will fail otherwise.</strong></p>\n<p>Use the \"--dry_run\"\
    \ argument to take a closer look at the acquistion parameters for a scanning session.</p>\n\
    <p>To run a single participant with dry_run argument:</p>\n<pre><code>singularity\
    \ run -B /home/timothy/sandbox_DO_NOT_DELETE/BIDS/142_CIFASD_4:/output_dir \\\n\
    -B /path/to/dicom/data/dir:/dicom_dir /path/to/singularity/images/directory/imagename.img\
    \ \\\n--output_dir /output_dir --dicom_dir /dicom_dir --ses_id 10000 --subj_id\
    \ 1000 --dry_run\n</code></pre>\n<p>This will output a hidden folder (named .heudiconv)\
    \ along with sub-folders based on arguments provided to \"--subj_id\" and \"--ses_id\"\
    \ respectively.\nWithin the sub-folders will be a tsv file that begins with \"\
    dicominfo\". <strong>Based on the example above the path to the file will be \"\
    .heudiconv/1000/ses-10000/info/dicominfo_ses-10000.tsv\"</strong></p>\n<p>Use\
    \ this tsv file to design a heuristics script to organize your eventual nifti\
    \ data. <strong>See this tutorial for a how to on heuristic script creation (<a\
    \ href=\"http://reproducibility.stanford.edu/bids-tutorial-series-part-2a/#heuman3\"\
    \ rel=\"nofollow\">heuristics tutorial</a>)</strong></p>\n<p>To run a single participant\
    \ with heuristic argument:</p>\n<pre><code>singularity run -B /home/timothy/sandbox_DO_NOT_DELETE/BIDS/142_CIFASD_4:/output_dir\
    \ \\\n-B /path/to/dicom/data/dir:/dicom_dir -B /path/to/heuristics/script:/heuristic.py\
    \ \\\n /path/to/singularity/images/directory/imagename.img \\\n--output_dir /output_dir\
    \ --dicom_dir /dicom_dir --ses_id 10000 --subj_id 1000 --heuristic /heuristic.py\n\
    \n</code></pre>\n<h2>\n<a id=\"user-content-important-notes\" class=\"anchor\"\
    \ href=\"#important-notes\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Important Notes</h2>\n<h3>\n<a id=\"user-content-gotchas\"\
    \ class=\"anchor\" href=\"#gotchas\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Gotchas</h3>\n<p><strong>1) Bind\
    \ Mounting</strong></p>\n<p>In order to run this container you will have to use\
    \ \"bind mounting\", meaning you will have to link local folders/files to existing\
    \ folders/files within the container with the -B flag. In the example above the\
    \ local folder \"/home/timothy/sandbos_DO_NOT_DELETE/BIDS/142_CIFASD_4\" becomes\
    \ \"/output_dir\" within the container as they are separated by a colon (:). <strong>Notice\
    \ that in both cases above the output and dicom folder and heuristic file are\
    \ bound to /output_dir, /dicom_dir and /heuristic.py respectively, this is very\
    \ important.</strong></p>\n<p><strong>2) DICOM Data Formatting</strong></p>\n\
    <p>Due to the restrictive nature of BIDS the dicom data must be in a particular\
    \ format in order for the conversion to work properly. This application will copy\
    \ dicom data directories by searching for either the --subj_id or --ses_id argument\
    \ present within a dicom directory name, place them in a separate directory, and\
    \ rearrange them. So for example if the dicom directory is named \"XYXY4776XYXY\"\
    \ --subj_id 4776 the application will find the \"4776\" pattern.</p>\n<p><strong>3)\
    \ Subject ID and Session ID names</strong></p>\n<p>You must use alphanumerics\
    \ (i.e. letters or numbers) only (<strong>no special characters</strong>) with\
    \ your subject IDs (subj_id) and session IDs (ses_id). <strong>Note the \"--ses_id\"\
    \ argument is optional</strong>!</p>\n<h3>\n<a id=\"user-content-best-practices\"\
    \ class=\"anchor\" href=\"#best-practices\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Best Practices</h3>\n<p><strong>1)\
    \ Initial Conversion</strong></p>\n<p>While testing the initial BIDS conversion\
    \ it is best to start with one or two datasets and specify the '--dry_run' argument\
    \ (see above for an example of usage).\nThis will create a dicom_info tsv file\
    \ which can be used for heuristic script creation.\nSee Step 3 of 'Run HeuDiConv\
    \ on ses-001 scans to get the dicominfo file' within <a href=\"http://reproducibility.stanford.edu/bids-tutorial-series-part-2a/#heuman2\"\
    \ rel=\"nofollow\">Stanford BIDS Tutorial</a>.</p>\n<p><strong>2) BIDS Validator</strong></p>\n\
    <p>Once satisfied with an initial BIDS conversion, prior to running the conversion\
    \ on an entire study first ensure that the BIDS converted dataset meets the BIDS\
    \ specification by using the <a href=\"http://incf.github.io/bids-validator/\"\
    \ rel=\"nofollow\">BIDS validator web version</a></p>\n<p><strong>3) Longitudinal\
    \ Format</strong></p>\n<p>When converting data to BIDS you can certainly have\
    \ a cross sectonal directory format such as below:\nBIDS_output\nsub-01\nsub-02\n\
    sub-03\nsub-04\netc\nHowever, I suggest placing data within a longitudinal directory\
    \ format even if you have cross-sectional data:\nBIDS_output\nsub-01\nses-01\n\
    ses-02\nsub-02\nses-01\netc</p>\n<p>You can control the BIDS directory format\
    \ by providing both the arguments --subj_id --ses_id for a conversion, if you\
    \ only specify one of the two arguments the data will be outputted in a cross-sectional\
    \ format.</p>\n"
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1624658162.0
tpall/geo-htseq-paper:
  data_format: 2
  description: Code used to generate summaries, models and figures for article "A
    field-wide assessment of differential high throughput sequencing reveals widespread
    bias".
  filenames:
  - Singularity
  full_name: tpall/geo-htseq-paper
  latest_release: null
  readme: '<p><a href="https://github.com/tpall/geo-htseq-paper/workflows/CI/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/tpall/geo-htseq-paper/workflows/CI/badge.svg"
    alt="CI" style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-geo-htseq-paper" class="anchor" href="#geo-htseq-paper" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Geo-htseq-paper</h1>

    <p>We analyzed the field of expression profiling by high throughput sequencing,
    or RNA-seq, in terms of replicability and reproducibility, using data from the
    GEO (Gene Expression Omnibus) repository. Our work puts an upper bound of 56%
    to field-wide reproducibility, based on the types of files submitted to GEO.</p>

    <h2>

    <a id="user-content-getting-data" class="anchor" href="#getting-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting data</h2>

    <p>Got to <a href="https://zenodo.org/record/3754095" rel="nofollow">https://zenodo.org/record/3754095</a>
    and download data archive, let''s say, to your Downloads folder.</p>

    <p>Then create new folder, e.g. "geo-htseq" and enter this folder</p>

    <div class="highlight highlight-source-shell"><pre>mkdir geo-htseq

    <span class="pl-c1">cd</span> geo-htseq</pre></div>

    <p>Copy downloaded dataset to your working directory and uncompress:</p>

    <div class="highlight highlight-source-shell"><pre>cp <span class="pl-k">~</span>/Downloads/geo-htseq-until-2019-12-31.tar.gz
    <span class="pl-c1">.</span>

    tar -xzvf geo-htseq-until-2019-12-31.tar.gz</pre></div>

    <p>Remove tar.gz archive from working directory:</p>

    <div class="highlight highlight-source-shell"><pre>rm geo-htseq-until-2019-12-31.tar.gz</pre></div>

    <p>Now you should have dataset in "output" subdirectory ready for analysis.</p>

    <h2>

    <a id="user-content-workflow-graph" class="anchor" href="#workflow-graph" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Workflow graph</h2>

    <p><a href="images/rulegraph.svg" target="_blank" rel="noopener noreferrer"><img
    src="images/rulegraph.svg" alt="rulegraph" style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619076381.0
tpall/htseq-paper-singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: tpall/htseq-paper-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-htseq-paper-singularity" class="anchor" href="#htseq-paper-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>htseq-paper-singularity</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604657436.0
tpall/singularity-stan:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity.3.6.3
  full_name: tpall/singularity-stan
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7_aci" class="anchor" href="#centos7_aci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos7_aci</h1>

    <p>Centos 7 base image for ACI Singualarity recipe<br>

    This recipe may include unnecessary packages for certain software installation.<br>

    Size of CPU-only container: ~1 GB<br>

    Size of GPU container: ~2.6 GB</p>

    <p>More packages will be added in the future</p>

    <p>2019/2/17

    <strong>Centos 7</strong> with <strong>GCC 8</strong><br>

    To enable GCC 8,</p>

    <pre><code>&gt; source /opt/rh/devtoolset-8/enable

    </code></pre>

    <p>2019/3/1<br>

    OpenMPI is added to <code>$PATH</code></p>

    <p>2019/3/11<br>

    OpenMPI is updated to version 2.1.6</p>

    <p>2019/4/12<br>

    Boost 1.70.0 in added</p>

    <p>2019/7/19<br>

    <del>Python 2 and 3 are updated to version 2.7.16 and version 3.7.4</del><br>

    OpenMPI is updated to version 4.0.1</p>

    <p>2019/7/21<br>

    <del>Few Python packages are added</del></p>

    <p>2019/7/22<br>

    <del>Few corrections are made including Python</del></p>

    <p>2019/7/23<br>

    Pythons are replaced with packages<br>

    To enable Python 2.7.16,</p>

    <pre><code>&gt; source /opt/rh/python27/enable

    </code></pre>

    <p>System version of python is 3.6.8</p>

    <p>2019/7/30<br>

    devtoolset-7 GCC is added (some software can''t be built with GCC 8)</p>

    <p>2019/11/9<br>

    CMake 3.15.5 is added</p>

    <p>2019/11/22<br>

    OpenMPI is downgraded to 1.10.1 to match version on ACI</p>

    <p>2020/2/12<br>

    Boost is upgraded to 1.72.0 and CMake is upgraded to 3.16.4</p>

    <p>2020/3/2<br>

    GPU version is added</p>

    <p>2020/9/21<br>

    Minor updates are made (regarding libxkb)</p>

    <p>2020/9/28<br>

    Recipe for CUDA 9.1 is added (for FSL with CUDA)</p>

    <p>2020/10/11<br>

    Boost is upgraded to 1.74.0 and CMake is upgraded to 3.18.4<br>

    R 4.0.3 is added (Curl 7.72.0 and XZ 5.2.5 are added for R)<br>

    VirtualGL is downgraded to 2.5.2 to match system version</p>

    <p>2020/10/18<br>

    UDUNITS 2.2.26 is added</p>

    <p>2020/10/20<br>

    Tix-devel, Tx-devel, TkInter-devel, LAPACK-devel, and BLAS-devel are added</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1603529584.0
tpall/singularity-tidyverse:
  data_format: 2
  description: Singularity image running R tidyverse + some other libraries
  filenames:
  - Singularity
  - Singularity.3.6.3
  full_name: tpall/singularity-tidyverse
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2366" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-singularity-tidyverse" class="anchor" href="#singularity-tidyverse"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    tidyverse</h2>

    <p>This will run R tidyverse + some other packages, like <em>here</em>, <em>readxl</em>,
    <em>lubridate</em>, <em>bookdown</em>, etc.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1608284812.0
uf-icbr-bioinformatics/biocontainers:
  data_format: 2
  description: null
  filenames:
  - Singularity_fastqc
  - Singularity_trimmomatic
  - Singularity_multiqc
  full_name: uf-icbr-bioinformatics/biocontainers
  latest_release: null
  readme: '<h1>

    <a id="user-content-biocontainers" class="anchor" href="#biocontainers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>biocontainers</h1>

    <p>This repository contains recipes for containers used to perform QC, summary
    statistics, and pre-processing on NGS datasets.</p>

    <p>In the future, we may provide the containers themselves. Stay tuned. Work in
    progress.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1623019187.0
urbanopt/JModelica_simulation:
  data_format: 2
  description: GitHub repo for storing scripts related to simulation using JModelica.
    The initial focus is on simulation in HPC environments.
  filenames:
  - Singularity_Recipes/Singularity_Recipe_Py3_Simulation
  - Singularity_Recipes/Singularity_Recipe_Py2_Compilation_Simulation
  full_name: urbanopt/JModelica_simulation
  latest_release: null
  readme: '<h1>

    <a id="user-content-jmodelica-simulation" class="anchor" href="#jmodelica-simulation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>JModelica
    Simulation</h1>

    <p>GitHub repo for storing scripts related to simulation of Modelica models using
    JModelica. The initial focus is on simulation in HPC environments.</p>

    <h1>

    <a id="user-content-singularity-recipes" class="anchor" href="#singularity-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipes</h1>

    <p>Recipes for building Singularity containers for compilation and simulation
    of Modelica models using PyModelica and PyFMI. Note that the recipe that would
    support compilation and simulation is for use with Python2 only, while a separate
    recipe supports simulation in Python3.</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1608681086.0
uri-ai-lab/singularity-images:
  data_format: 2
  description: Setups for various images used on the dgx.
  filenames:
  - Singularity-PyTorch
  full_name: uri-ai-lab/singularity-images
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-images" class="anchor" href="#singularity-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-images</h1>

    <p>Setups for various images used on the dgx.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1573772605.0
vardigroup/DPMC:
  data_format: 2
  description: An exact weighted model-counting framework based on algebraic decision
    diagrams and tensors.
  filenames:
  - lg/Singularity
  - dmc/Singularity
  full_name: vardigroup/DPMC
  latest_release: mc-2021
  readme: '<h1>

    <a id="user-content-dpmc-dynamic-programming-for-model-counting" class="anchor"
    href="#dpmc-dynamic-programming-for-model-counting" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>DPMC (Dynamic Programming for Model Counting)</h1>

    <p>DPMC computes weighted model counts of formulas in conjunctive normal form
    (CNF)</p>

    <ul>

    <li>The DPMC framework runs in two phases:

    <ul>

    <li>Planning phase: <a href="./lg">LG</a> or <a href="./htb">HTB</a> constructs
    a join tree of a CNF formula</li>

    <li>Execution phase: <a href="./dmc">DMC</a> computes the model count of the formula
    using the join tree</li>

    </ul>

    </li>

    <li>Developers:

    <ul>

    <li>Jeffrey Dudek</li>

    <li>Vu Phan</li>

    </ul>

    </li>

    </ul>


    <h2>

    <a id="user-content-releases" class="anchor" href="#releases" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/vardigroup/DPMC/releases">Releases</a>

    </h2>

    <ul>

    <li>2021/05/25: <a href="https://github.com/vardigroup/DPMC/releases/tag/mc-2021">mc-2021</a>
    <a href="https://zenodo.org/badge/latestdoi/280443175" rel="nofollow"><img src="https://camo.githubusercontent.com/a66989e99eb192ab9857e39b3f1e218d0f4b7bcd8b478436fdace72cf61b408c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3238303434333137352e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/280443175.svg" style="max-width:100%;"></a>

    <ul>

    <li><a href="./mcc">Model Counting Competition MC-2021</a></li>

    </ul>

    </li>

    <li>2021/05/23: <a href="https://github.com/vardigroup/DPMC/releases/tag/v2.0.0">v2.0.0</a>

    <ul>

    <li>SAT-2021 paper: <strong>ProCount: Weighted Projected Model Counting with Graded
    Project-Join Trees</strong>

    <ul>

    <li>Authors: Jeffrey M. Dudek, Vu H. N. Phan, Moshe Y. Vardi</li>

    </ul>

    </li>

    </ul>

    </li>

    <li>2020/07/20: <a href="https://github.com/vardigroup/DPMC/releases/tag/v1.0.0">v1.0.0</a>

    <ul>

    <li>CP-2020 paper: <strong><a href="https://arxiv.org/abs/2008.08748" rel="nofollow">DPMC:
    Weighted Model Counting by Dynamic Programming on Project-Join Trees</a></strong>

    <ul>

    <li>Authors: Jeffrey M. Dudek, Vu H. N. Phan, Moshe Y. Vardi</li>

    </ul>

    </li>

    </ul>

    </li>

    </ul>


    <h2>

    <a id="user-content-example-files" class="anchor" href="#example-files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="./examples">Example
    files</a>

    </h2>


    <h2>

    <a id="user-content-acknowledgment" class="anchor" href="#acknowledgment" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="./ACKNOWLEDGMENT.md">Acknowledgment</a>

    </h2>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1623698161.0
vibaotram/singularity-container:
  data_format: 2
  description: null
  filenames:
  - Singularity.guppy3.6.0cpu-conda-api
  - Singularity.myR_4-0-2_rstudio_1.3
  - Singularity.guppy4.0.14gpu-conda-api
  - Singularity.deepbinner-api
  - Singularity.guppy3.6.0gpu-conda-api
  - Singularity.myR_3-6-3
  - Singularity.guppy-cpu-conda
  - Singularity.guppy4.5.4gpu-conda-api
  - Singularity.guppy3.4gpu-conda-api
  - Singularity.cpu-guppy3.4-conda-api
  - Singularity.guppy4.2.2gpu-conda-api
  full_name: vibaotram/singularity-container
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4054" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-singularity-container" class="anchor" href="#singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-container</h1>

    <h2>

    <a id="user-content-singularity-images-supporting-basedmux-workflow" class="anchor"
    href="#singularity-images-supporting-basedmux-workflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity images
    supporting <a href="https://github.com/vibaotram/baseDmux.git">baseDmux workflow</a>

    </h2>

    <p><strong>Singularity.guppy-cpu-conda</strong></p>

    <p>containing GUPPY version 3.4 CPU, Miniconda3</p>

    <pre><code>shub://vibaotram/singularity-container:guppy-cpu-conda

    </code></pre>

    <p><strong>Singularity.cpu-guppy3.4-conda-api</strong></p>

    <p>containing GUPPY version 3.4 CPU, Miniconda3, ONT_FAST5_API</p>

    <pre><code>shub://vibaotram/singularity-container:cpu-guppy3.4-conda-api

    </code></pre>

    <p><strong>Singularity.guppy3.4gpu-conda-api</strong></p>

    <p>containing GUPPY version 3.4 GPU, Miniconda3, ONT_FAST5_API</p>

    <pre><code>shub://vibaotram/singularity-container:guppy3.4gpu-conda-api

    </code></pre>

    <p><strong>Singularity.deepbinner-api</strong></p>

    <p>containing deepbinner 2.0.0, ONT_FAST5_API, python3</p>

    <pre><code>shub://vibaotram/singularity-container:deepbinner-api

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics:
  - singularity
  updated_at: 1621378901.0
vincent-noel/pc4ecm:
  data_format: 2
  description: PhysiCell Invasion Model
  filenames:
  - src/addons/PhysiBoSSa/MaBoSS-env-2.0/containers/singularity/Singularity
  full_name: vincent-noel/pc4ecm
  latest_release: null
  readme: '<h1>

    <a id="user-content-folding-at-home-gpu" class="anchor" href="#folding-at-home-gpu"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>folding-at-home-gpu</h1>

    <p>gpu image for folding at home</p>

    <p>simple merge of nvidia cl image with folding at home v7.5.1 to enable gpu processing.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1588946949.0
vivekkatial/qaoa-three-sat:
  data_format: 2
  description: An implementation for solving 3SAT (Exact Cover) using the Quantum
    Approximate Optimization Algorithm
  filenames:
  - SingularityFile.def
  full_name: vivekkatial/qaoa-three-sat
  latest_release: null
  readme: '<h1>

    <a id="user-content-qaoa-3sat--" class="anchor" href="#qaoa-3sat--" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>QAOA 3SAT <a href="https://camo.githubusercontent.com/4beb7225857c50a9391b71fbe998bc23c33b4d87ee15e3da9b7c1b7dfdc67a11/68747470733a2f2f7472617669732d63692e636f6d2f766976656b6b617469616c2f71616f612d74687265652d7361742e7376673f6272616e63683d6d6173746572"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/4beb7225857c50a9391b71fbe998bc23c33b4d87ee15e3da9b7c1b7dfdc67a11/68747470733a2f2f7472617669732d63692e636f6d2f766976656b6b617469616c2f71616f612d74687265652d7361742e7376673f6272616e63683d6d6173746572"
    alt="" data-canonical-src="https://travis-ci.com/vivekkatial/qaoa-three-sat.svg?branch=master"
    style="max-width:100%;"></a> <a href="https://qaoa-three-sat.readthedocs.io/en/latest/?badge=latest"
    rel="nofollow"><img src="https://camo.githubusercontent.com/253d508d956ec9315fd5509c8d9cb82640904ab96c15672f2c65c9ec5c2de390/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f71616f612d74687265652d7361742f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/qaoa-three-sat/badge/?version=latest"
    style="max-width:100%;"></a>

    </h1>

    <p>An implementation for solving 3SAT (Exact Cover) using the Quantum Approximate
    Optimization Algorithm</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607596883.0
weatherlab/metview:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: weatherlab/metview
  latest_release: null
  readme: '<h1>

    <a id="user-content-metview" class="anchor" href="#metview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>metview</h1>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1523286570.0
willgpaik/centos7_aci:
  data_format: 2
  description: Centos 7 base image for ACI
  filenames:
  - Singularity.gpu
  - Singularity.test
  - Singularity
  - Singularity.cuda9.1
  full_name: willgpaik/centos7_aci
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7_aci" class="anchor" href="#centos7_aci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos7_aci</h1>

    <p>Centos 7 base image for ACI Singualarity recipe<br>

    This recipe may include unnecessary packages for certain software installation.<br>

    Size of CPU-only container: ~1 GB<br>

    Size of GPU container: ~2.6 GB</p>

    <p>More packages will be added in the future</p>

    <p>2019/2/17

    <strong>Centos 7</strong> with <strong>GCC 8</strong><br>

    To enable GCC 8,</p>

    <pre><code>&gt; source /opt/rh/devtoolset-8/enable

    </code></pre>

    <p>2019/3/1<br>

    OpenMPI is added to <code>$PATH</code></p>

    <p>2019/3/11<br>

    OpenMPI is updated to version 2.1.6</p>

    <p>2019/4/12<br>

    Boost 1.70.0 in added</p>

    <p>2019/7/19<br>

    <del>Python 2 and 3 are updated to version 2.7.16 and version 3.7.4</del><br>

    OpenMPI is updated to version 4.0.1</p>

    <p>2019/7/21<br>

    <del>Few Python packages are added</del></p>

    <p>2019/7/22<br>

    <del>Few corrections are made including Python</del></p>

    <p>2019/7/23<br>

    Pythons are replaced with packages<br>

    To enable Python 2.7.16,</p>

    <pre><code>&gt; source /opt/rh/python27/enable

    </code></pre>

    <p>System version of python is 3.6.8</p>

    <p>2019/7/30<br>

    devtoolset-7 GCC is added (some software can''t be built with GCC 8)</p>

    <p>2019/11/9<br>

    CMake 3.15.5 is added</p>

    <p>2019/11/22<br>

    OpenMPI is downgraded to 1.10.1 to match version on ACI</p>

    <p>2020/2/12<br>

    Boost is upgraded to 1.72.0 and CMake is upgraded to 3.16.4</p>

    <p>2020/3/2<br>

    GPU version is added</p>

    <p>2020/9/21<br>

    Minor updates are made (regarding libxkb)</p>

    <p>2020/9/28<br>

    Recipe for CUDA 9.1 is added (for FSL with CUDA)</p>

    <p>2020/10/11<br>

    Boost is upgraded to 1.74.0 and CMake is upgraded to 3.18.4<br>

    R 4.0.3 is added (Curl 7.72.0 and XZ 5.2.5 are added for R)<br>

    VirtualGL is downgraded to 2.5.2 to match system version</p>

    <p>2020/10/18<br>

    UDUNITS 2.2.26 is added</p>

    <p>2020/10/20<br>

    Tix-devel, Tx-devel, TkInter-devel, LAPACK-devel, and BLAS-devel are added</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1603227322.0
willgpaik/centos8_roar:
  data_format: 2
  description: Centos 8 base image for Roar
  filenames:
  - Singularity.gpu
  - Singularity
  full_name: willgpaik/centos8_roar
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos8_roar" class="anchor" href="#centos8_roar" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos8_roar</h1>

    <p>Centos 8 base image for Roar</p>

    <h3>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTE</h3>

    <ul>

    <li>This recipe may include unnecessary packages for certain software installation</li>

    <li>More packages will be added in the future</li>

    </ul>

    <h2>

    <a id="user-content-updates" class="anchor" href="#updates" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Updates</h2>

    <ul>

    <li>

    <p>2020/11/13</p>

    <ul>

    <li>Initial recipe added</li>

    </ul>

    </li>

    <li>

    <p>2021/03/22</p>

    <ul>

    <li>Default Python3 is updated to Python 3.8</li>

    <li>Lapack, BLAS, OpenBLAS, ATLAS, and NetCDF are added</li>

    <li>CMake 3.19.7, Boost 1.75.0, and R 4.0.4 are added</li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616617880.0
willgpaik/deformetrica_aci:
  data_format: 2
  description: Singularity recipe for Deformetrica on Centos 7
  filenames:
  - Singularity
  full_name: willgpaik/deformetrica_aci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-deformetrica_aci\" class=\"anchor\" href=\"\
    #deformetrica_aci\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>deformetrica_aci</h1>\n<p>Singularity recipe for Deformetrica\
    \ on Centos 7 for ACI-ICS clusters</p>\n<p>2019/2/14<br>\nAnaconda3 ver. 2018.12<br>\n\
    Deformetrica 4.1<br>\nGUI can be used through EoD</p>\n<p>Commands:</p>\n<pre><code>&gt;\
    \ source activate deformetrica  \n&gt; deformetrica  \nOr,  \n&gt; deformetrica\
    \ gui\n</code></pre>\n<p>2020/9/21<br>\nGPU support is added<br>\nAnaconda, Python,\
    \ and Deformetrica are updated</p>\n<p>2020/10/9<br>\nPyTorch and PyKeOps are\
    \ added</p>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1602342657.0
willgpaik/qt5_aci:
  data_format: 2
  description: Singularity recipe for Qt5 on Centos 7 and Ubuntu 16.04
  filenames:
  - Singularity.ubuntu
  - Singularity
  - Singularity.qt5
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_ants
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_fsl
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_centos8
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_ants_fsl_fmriprep
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3
  full_name: willgpaik/qt5_aci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-qt5_aci\" class=\"anchor\" href=\"#qt5_aci\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>qt5_aci</h1>\n<p>Singularity recipe for Qt5 on Centos 7 and Ubuntu\
    \ 16.04 For ICS</p>\n<p><strong>NOTE: DO NOT rebuild \"Singularity.dsi_mrtrix3\"\
    \ image.</strong><br>\n(Last successful build was Mar 12 2019)</p>\n<p>Singularity\
    \ recipe for DSI Studio and MRtrix3 is updated on <strong>dsistudio_mrtrix3</strong>\
    \ folder</p>\n<p>If you want to install DSI Studio and MRtrix3 on Basic Qt5 Container,<br>\n\
    downlaod \"dsistudio_mrtrix3_install.sh\" to preferred location\nand follow commands\
    \ inside Singularity environment:</p>\n<pre><code>&gt; chmod +x dsistudio_mrtrix3_install.sh\
    \  \n&gt; ./dsistudio_mrtrix3_install.sh\n</code></pre>\n<p>2019/2/21<br>\nUnable\
    \ to use <strong>GCC 8.2.1</strong> due to build failure =&gt; Going back to <strong>GCC\
    \ 7.3.1</strong><br>\n(Failed to resolve the issue at this moment)</p>\n<p><del>2019/5/13<br>\n\
    Updated dsistudio_mrtrix3_install.sh due to Qt version issue<br>\n(Requires Qt\
    \ 5.12.2 or above: <a href=\"https://github.com/frankyeh/DSI-Studio/issues/34\"\
    >https://github.com/frankyeh/DSI-Studio/issues/34</a>)</del></p>\n<p>2019/5/24<br>\n\
    Reverted changes made on 2019/5/13</p>\n<p>2019/6/24<br>\n<del>Newer version qt5\
    \ installation recipe added (in progress)</del></p>\n<p>2019/7/22<br>\nQt is updated\
    \ to 5.12 with Qt Charts (for DSI Studio)</p>\n<p>2019/7/24<br>\nQt SVG is added\
    \ (for MRtrix 3)<br>\n32-bit EoD graphics libraries are disable (to aviod warnings)</p>\n\
    <p>2019/7/29<br>\nNVIDIA driver is added to DSI Studio MRtrix3 container</p>\n\
    <p>2019/11/10<br>\nQt version 5.12.5 is used</p>\n<p>2020/4/24<br>\nUbuntu 16.04\
    \ version added with Qt 5.14.2</p>\n<p>2020/6/20<br>\nQt5 container is updated\
    \ to have nvidia driver</p>\n<p>2020/7/27<br>\nUbuntu container is updated to\
    \ have NVIDIA driver (Ubuntu 16.04 based)</p>\n<p>2020/9/28<br>\nQt5 container\
    \ is updated to use CUDA 9.1 version (for FSL with CUDA)<br>\n(Reference: <a href=\"\
    https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GPU\" rel=\"nofollow\">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GPU</a>)</p>\n\
    <p>2020/10/20<br>\nQt5X11Extras is added to the Qt5 recipe<br>\n(Ubuntu container\
    \ will not be updated unless necessary)</p>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1618004326.0
yee379/uresnet-tomo-seg:
  data_format: 2
  description: uresnet based deep neutral network for the segmentation of high resolution
    cryo-EM tomographs
  filenames:
  - Singularity
  full_name: yee379/uresnet-tomo-seg
  latest_release: null
  readme: '<h1>

    <a id="user-content-uresnet-tomo-seg" class="anchor" href="#uresnet-tomo-seg"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>uresnet-tomo-seg</h1>

    <p>uresnet based deep neutral network for the segmentation of high resolution
    cryo-EM tomographs</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1577150888.0
yh549848/singularity-rsem:
  data_format: 2
  description: null
  filenames:
  - 1.3.1/Singularity
  - 1.3.3/Singularity
  full_name: yh549848/singularity-rsem
  latest_release: null
  readme: '<h1>

    <a id="user-content-micall" class="anchor" href="#micall" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MiCall</h1>

    <h2>

    <a id="user-content-processing-fastq-data-from-an-illumina-miseq" class="anchor"
    href="#processing-fastq-data-from-an-illumina-miseq" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Processing FASTQ data
    from an Illumina MiSeq</h2>

    <p><a href="https://travis-ci.com/cfe-lab/MiCall" rel="nofollow"><img src="https://camo.githubusercontent.com/8cfcb6fc58992b1a5293f99aab18dc7fc4c352993a0aaeee39aca2186b9e02b4/68747470733a2f2f7472617669732d63692e636f6d2f6366652d6c61622f4d6943616c6c2e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.com/cfe-lab/MiCall.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://codecov.io/github/cfe-lab/MiCall?branch=master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/01a5da8acb0a78aabfb093a63c28db15b9df951d3e52aaa03d54180dae171b07/68747470733a2f2f636f6465636f762e696f2f6769746875622f6366652d6c61622f4d6943616c6c2f636f7665726167652e7376673f6272616e63683d6d6173746572"
    alt="Code Coverage" data-canonical-src="https://codecov.io/github/cfe-lab/MiCall/coverage.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://doi.org/10.5281/zenodo.1289989" rel="nofollow"><img src="https://camo.githubusercontent.com/91063d68c07e035327f80f12cf9c389ffffd45952c4db811e6bf23fc2973b714/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313238393938392e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.1289989.svg"
    style="max-width:100%;"></a></p>

    <p>Maps all the reads from a sample against a set of reference sequences, then

    stitches all the reads into consensus sequences and coverage maps.</p>

    <p>A monitoring system regularly checks the file system for unprocessed runs,

    transfers FASTQ.gz files to the cluster and executes the pipeline.</p>

    <p>See the <a href="https://cfe-lab.github.io/MiCall/steps" rel="nofollow">list
    of steps and files</a> for details of what the pipeline does.

    The <a href="https://cfe-lab.github.io/MiCall/admin" rel="nofollow">admin</a>
    page describes how to look after the pipeline in Kive, and the

    <a href="https://cfe-lab.github.io/MiCall/getting_started" rel="nofollow">getting
    started</a> page describes how to get the docker version set up and run it

    on your own data.</p>

    <h2>

    <a id="user-content-dual-licensing" class="anchor" href="#dual-licensing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dual Licensing</h2>

    <p>Copyright (C) 2016, University of British Columbia</p>

    <p>This program is free software: you can redistribute it and/or modify

    it under the terms of the GNU Affero General Public License as published

    by the Free Software Foundation, either version 3 of the License, or

    (at your option) any later version.</p>

    <p>This program is distributed in the hope that it will be useful,

    but WITHOUT ANY WARRANTY; without even the implied warranty of

    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

    GNU Affero General Public License for more details.</p>

    <p>You should have received a copy of the GNU Affero General Public License

    along with this program.  If not, visit <a href="https://www.gnu.org/licenses/"
    rel="nofollow">gnu.org</a>. The source code for

    this program is available from <a href="https://github.com/cfe-lab/MiCall">github.com</a>.</p>

    <p>The program is also available for a fee under a more permissive license. For

    example, if you want to run a changed version of the program on a network server

    without publishing the changed source code, <a href="mailto:micalldev@cfenet.ubc.ca">contact
    us</a> about

    purchasing a license.</p>

    <h2>

    <a id="user-content-third-party-components" class="anchor" href="#third-party-components"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Third
    Party Components</h2>

    <p>MiCall makes use of several open-source tools. Here is a list of tools with

    their licenses.</p>

    <p>Requests is distributed under the Apache 2.0 license.</p>

    <p>Python 3 is distributed under the <a href="https://docs.python.org/3/license.html"
    rel="nofollow">Python 3 license</a>.</p>

    <p>Bowtie2, IVA, and Python-Levenshtein are distributed under the GNU General

    Public License (GPL).</p>

    <p>Matplotlib is distributed under the <a href="https://matplotlib.org/users/license.html"
    rel="nofollow">Matplotlib license</a>.</p>

    <p>Reportlab is distributed under the BSD license.</p>

    <p>Pyyaml and Cutadapt are distributed under the MIT license.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623772549.0
yh549848/singularity-rstudio-methylseq:
  data_format: 2
  description: null
  filenames:
  - latest--rstudio1.2.5042r362/Singularity
  full_name: yh549848/singularity-rstudio-methylseq
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-running-rstudio-server-in-a-conda-environment\"\
    \ class=\"anchor\" href=\"#running-rstudio-server-in-a-conda-environment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Rstudio Server in a Conda Environment</h1>\n<p>I usually rely on the <a href=\"\
    https://docs.conda.io/en/latest/\" rel=\"nofollow\">conda package manager</a>\
    \ to manage my environments during development. Thanks to <a href=\"https://conda-forge.org/\"\
    \ rel=\"nofollow\">conda-forge</a> and <a href=\"https://bioconda.github.io/\"\
    \ rel=\"nofollow\">bioconda</a> most R packages are now also available through\
    \ conda. For production,\nI <a href=\"https://github.com/grst/containerize-conda\"\
    >convert them to containers</a> as these are easier to share.</p>\n<p>Unfortunately,\
    \ there seems to be <a href=\"https://community.rstudio.com/t/start-rstudio-server-session-in-conda-environment/12516/15\"\
    \ rel=\"nofollow\">no straightforward way</a> to use conda envs in Rstudio server.\
    \ This repository provides three approaches to make rstudio server work with conda\
    \ envs.</p>\n<ul>\n<li><a href=\"#running-rstudio-server-with-singularity\">Running\
    \ Rstudio Server in a Singularity Container</a></li>\n<li><a href=\"#running-rstudio-server-with-podmandocker\"\
    >Running Rstudio Server in a Docker/Podman Container</a></li>\n<li><a href=\"\
    #running-locally\">Running Rstudio Server locally</a></li>\n</ul>\n<h2>\n<a id=\"\
    user-content-running-rstudio-server-with-singularity\" class=\"anchor\" href=\"\
    #running-rstudio-server-with-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Rstudio Server with Singularity</h2>\n\
    <p>With this approach Rstudio Server runs in a Singularity container (based on\
    \ <a href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\">rocker/rstudio</a>).<br>\n\
    The conda environment gets mounted into the container - like that there's no need\
    \ to rebuild the container to add a package and\n<code>install.packages</code>\
    \ can be used without issues. The container-based approach has the following benefits:</p>\n\
    <ul>\n<li>Authentication works (<a href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\
    >#3</a>)</li>\n<li>Several separate instances of Rstudio server can run in parallel,\
    \ even without the <em>Pro</em> version.</li>\n</ul>\n<h3>\n<a id=\"user-content-prerequisites\"\
    \ class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h3>\n<ul>\n<li><a\
    \ href=\"https://sylabs.io/guides/3.0/user-guide/quick_start.html\" rel=\"nofollow\"\
    >Singularity</a></li>\n<li>\n<a href=\"https://docs.conda.io/en/latest/miniconda.html\"\
    \ rel=\"nofollow\">conda</a> or <a href=\"https://github.com/conda-forge/miniforge#mambaforge\"\
    >mamba</a>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repository</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:grst/rstudio-server-conda.git\n\
    <span class=\"pl-c1\">cd</span> rstudio-server-conda/singularity</pre></div>\n\
    </li>\n<li>\n<p>Activate the target conda env or set the environment variable\
    \ <code>CONDA_PREFIX</code>\nto point to the location of the conda env.</p>\n\
    </li>\n<li>\n<p>Check the <code>run_singularity.sh</code> script. In particular,\
    \ you may need to add additional bind mounts\n(e.g. a global data directory).</p>\n\
    </li>\n<li>\n<p>Execute the <code>run_singularity.sh</code> script. It will automatically\
    \ build the container if it is not available.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>PORT=8787 PASSWORD=notsafe ./run_singularity.sh</pre></div>\n</li>\n<li>\n\
    <p>Log into Rstudio</p>\n<ul>\n<li>open rstudio server at <code>http://localhost:8787</code>\
    \ (or whatever port you specified)</li>\n<li>login with your default username\
    \ and the password you specified via the <code>PASSWORD</code> environment variable.</li>\n\
    </ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-running-rstudio-server-with-podmandocker\"\
    \ class=\"anchor\" href=\"#running-rstudio-server-with-podmandocker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Rstudio Server with Podman/Docker</h2>\n<p>This approach is similar to <a href=\"\
    #running-rstudio-server-with-singularity\">Singularity</a>, but uses\nDocker or\
    \ Podman and a <code>docker-compose.yml</code> file instead.</p>\n<h3>\n<a id=\"\
    user-content-known-limitations\" class=\"anchor\" href=\"#known-limitations\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Known limitations</h3>\n<ul>\n<li>No access to shared group directories\
    \ (<a href=\"https://github.com/grst/rstudio-server-conda/issues/14\">#14</a>)</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h3>\n<ul>\n<li>\n<a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> or <a href=\"https://podman.io/\" rel=\"nofollow\"\
    >Podman</a>\n</li>\n<li>\n<a href=\"https://github.com/docker/compose\">docker-compose</a>\
    \ or <a href=\"https://github.com/containers/podman-compose\">podman-compose</a>\n\
    </li>\n<li>\n<a href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"\
    nofollow\">conda</a> or <a href=\"https://github.com/conda-forge/miniforge#mambaforge\"\
    >mamba</a>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-usage-1\" class=\"anchor\"\
    \ href=\"#usage-1\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repository</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:grst/rstudio-server-conda.git</pre></div>\n\
    </li>\n<li>\n<p>Build the rstudio container (fetches the latest version of <a\
    \ href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\">rocker/rstudio</a>\
    \ and adds some custom scripts)</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> rstudio-server-conda/docker\ndocker-compose\
    \ build     <span class=\"pl-c\"><span class=\"pl-c\">#</span> or podman-compose</span></pre></div>\n\
    </li>\n<li>\n<p>Copy the docker-compose.yml file into your project directory and\
    \ adjust the paths.</p>\n<p>You may want to add additional volumes with your data.</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-s\">[...]</span>\n\
    \   <span class=\"pl-ent\">ports</span>:\n      <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> port on the host : port in the container (the latter is always\
    \ 8787)</span>\n      - <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>8889:8787<span\
    \ class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-ent\">volumes</span>:\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> mount conda env into\
    \ exactely the same path as on the host system - some paths are hardcoded in the\
    \ env.</span>\n      - <span class=\"pl-s\">/home/sturm/anaconda3/envs/R400:/home/sturm/anaconda3/envs/R400</span>\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> Share settings between\
    \ rstudio instances</span>\n      - <span class=\"pl-s\">/home/sturm/.local/share/rstudio/monitored/user-settings:/root/.local/share/rstudio/monitored/user-settings</span>\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> mount the working directory\
    \ containing your R project.</span>\n      - <span class=\"pl-s\">/home/sturm/projects:/projects</span>\n\
    \    <span class=\"pl-ent\">environment</span>:\n      <span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> password used for authentication</span>\n      - <span\
    \ class=\"pl-s\">PASSWORD=notsafe</span>\n      <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> repeat the path of the conda environment (must be identical to\
    \ the path in \"volumes\")</span>\n      - <span class=\"pl-s\">CONDAENV=/home/sturm/anaconda3/envs/R400</span></pre></div>\n\
    </li>\n<li>\n<p>Run your project-specific instance of Rstudio-server</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>docker-compose up </pre></div>\n\
    </li>\n<li>\n<p>Log into Rstudio</p>\n</li>\n</ol>\n<ul>\n<li>Open your server\
    \ at <code>http://localhost:8889</code> (or whatever port you specified)</li>\n\
    <li>Login with the user <code>rstudio</code> (when using Docker) or <code>root</code>\
    \ (when using Podman) and the password you specified\nin the <code>docker-compose.yml</code>.\
    \ If you are using Podman and login with <code>rstudio</code> you won't have permissions\
    \ to\naccess the mounted volumes.</li>\n</ul>\n<h2>\n<a id=\"user-content-running-locally\"\
    \ class=\"anchor\" href=\"#running-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Locally</h2>\n<p>With\
    \ this approach a locally installed Rstudio server is ran such that it uses the\
    \ conda env.</p>\n<h3>\n<a id=\"user-content-known-limitations-1\" class=\"anchor\"\
    \ href=\"#known-limitations-1\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Known limitations</h3>\n<ul>\n<li>no\
    \ authentication (<a href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\
    >#3</a>). Use this approach only in a secure network!</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-prerequisites-2\" class=\"anchor\" href=\"#prerequisites-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h3>\n\
    <ul>\n<li>\n<a href=\"https://www.rstudio.com/products/rstudio/download-server/\"\
    \ rel=\"nofollow\">rstudio server</a> installed locally</li>\n<li>\n<a href=\"\
    https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\">conda</a> or\
    \ <a href=\"https://github.com/conda-forge/miniforge#mambaforge\">mamba</a>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-usage-2\" class=\"anchor\" href=\"#usage-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repo</p>\n<pre><code>git clone\
    \ https://github.com/grst/rstudio-server-conda.git\n</code></pre>\n</li>\n<li>\n\
    <p>Run rstudio server in the conda env</p>\n<pre><code>cd rstudio-server-conda/local\n\
    conda activate my_project\n./start_rstudio_server.sh 8787  # use any free port\
    \ number here. \n</code></pre>\n</li>\n<li>\n<p>Connect to Rstudio</p>\n<p>You\
    \ should now be able to connect to rstudio server on the port you specify.\n<strong>If\
    \ an R Session has previously been running, you'll need to rstart the Rsession\
    \ now</strong>.</p>\n<p>Obviously, if your env does not have a version of <code>R</code>\
    \ installed, this will either not\nwork at all, or fall back to the system-wide\
    \ R installation.</p>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-how-it-works\"\
    \ class=\"anchor\" href=\"#how-it-works\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How it works</h3>\n<ul>\n<li>\n\
    <p>Rstudio server, can be started in non-daemonized mode by each user individually\
    \ on a custom port (similar to a jupyter notebook). This instance can then run\
    \ in a conda environment:</p>\n<pre><code>&gt; conda activate my_project\n&gt;\
    \ /usr/lib/rstudio-server/bin/rserver \\\n   --server-daemonize=0 \\\n   --www-port\
    \ 8787 \\\n   --rsession-which-r=$(which R) \\\n   --rsession-ld-library-path=$CONDA_PREFIX/lib\n\
    </code></pre>\n</li>\n<li>\n<p>To avoid additional problems with library paths,\
    \ also <code>rsession</code> needs to run within the conda environment. This is\
    \ achieved by wrapping <code>rsession</code> into the <a href=\"https://github.com/grst/rstudio-server-conda/blob/master/local/rsession.sh\"\
    >rsession.sh</a> script. The path to the wrapped <code>rsession</code> executable\
    \ can be passed to <code>rserver</code> as command line argument.</p>\n<pre><code>rserver\
    \ # ...\n    --rsession-path=rsession.sh\n</code></pre>\n</li>\n<li>\n<p>When\
    \ using multiple users a unique <code>secret-cookie-key</code> has to be generated\
    \ for each user. The path to the secret cookie key can be passed to <code>rserver</code>\
    \ as a command line parameter.</p>\n<pre><code>uuid &gt; /tmp/rstudio-server/${USER}_secure-cookie-key\n\
    rserver # ...\n  --secure-cookie-key-file /tmp/rstudio-server/${USER}_secure-cookie-key\n\
    </code></pre>\n</li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623386962.0
yh549848/singularity-rstudio-rnaseqde:
  data_format: 2
  description: null
  filenames:
  - bc3.12--rstudio125042r405/Singularity
  - bc3.10--rstudio125042r362/Singularity
  full_name: yh549848/singularity-rstudio-rnaseqde
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-running-rstudio-server-in-a-conda-environment\"\
    \ class=\"anchor\" href=\"#running-rstudio-server-in-a-conda-environment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Rstudio Server in a Conda Environment</h1>\n<p>I usually rely on the <a href=\"\
    https://docs.conda.io/en/latest/\" rel=\"nofollow\">conda package manager</a>\
    \ to manage my environments during development. Thanks to <a href=\"https://conda-forge.org/\"\
    \ rel=\"nofollow\">conda-forge</a> and <a href=\"https://bioconda.github.io/\"\
    \ rel=\"nofollow\">bioconda</a> most R packages are now also available through\
    \ conda. For production,\nI <a href=\"https://github.com/grst/containerize-conda\"\
    >convert them to containers</a> as these are easier to share.</p>\n<p>Unfortunately,\
    \ there seems to be <a href=\"https://community.rstudio.com/t/start-rstudio-server-session-in-conda-environment/12516/15\"\
    \ rel=\"nofollow\">no straightforward way</a> to use conda envs in Rstudio server.\
    \ This repository provides three approaches to make rstudio server work with conda\
    \ envs.</p>\n<ul>\n<li><a href=\"#running-rstudio-server-with-singularity\">Running\
    \ Rstudio Server in a Singularity Container</a></li>\n<li><a href=\"#running-rstudio-server-with-podmandocker\"\
    >Running Rstudio Server in a Docker/Podman Container</a></li>\n<li><a href=\"\
    #running-locally\">Running Rstudio Server locally</a></li>\n</ul>\n<h2>\n<a id=\"\
    user-content-running-rstudio-server-with-singularity\" class=\"anchor\" href=\"\
    #running-rstudio-server-with-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Rstudio Server with Singularity</h2>\n\
    <p>With this approach Rstudio Server runs in a Singularity container (based on\
    \ <a href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\">rocker/rstudio</a>).<br>\n\
    The conda environment gets mounted into the container - like that there's no need\
    \ to rebuild the container to add a package and\n<code>install.packages</code>\
    \ can be used without issues. The container-based approach has the following benefits:</p>\n\
    <ul>\n<li>Authentication works (<a href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\
    >#3</a>)</li>\n<li>Several separate instances of Rstudio server can run in parallel,\
    \ even without the <em>Pro</em> version.</li>\n</ul>\n<h3>\n<a id=\"user-content-prerequisites\"\
    \ class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h3>\n<ul>\n<li><a\
    \ href=\"https://sylabs.io/guides/3.0/user-guide/quick_start.html\" rel=\"nofollow\"\
    >Singularity</a></li>\n<li>\n<a href=\"https://docs.conda.io/en/latest/miniconda.html\"\
    \ rel=\"nofollow\">conda</a> or <a href=\"https://github.com/conda-forge/miniforge#mambaforge\"\
    >mamba</a>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repository</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:grst/rstudio-server-conda.git\n\
    <span class=\"pl-c1\">cd</span> rstudio-server-conda/singularity</pre></div>\n\
    </li>\n<li>\n<p>Activate the target conda env or set the environment variable\
    \ <code>CONDA_PREFIX</code>\nto point to the location of the conda env.</p>\n\
    </li>\n<li>\n<p>Check the <code>run_singularity.sh</code> script. In particular,\
    \ you may need to add additional bind mounts\n(e.g. a global data directory).</p>\n\
    </li>\n<li>\n<p>Execute the <code>run_singularity.sh</code> script. It will automatically\
    \ build the container if it is not available.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>PORT=8787 PASSWORD=notsafe ./run_singularity.sh</pre></div>\n</li>\n<li>\n\
    <p>Log into Rstudio</p>\n<ul>\n<li>open rstudio server at <code>http://localhost:8787</code>\
    \ (or whatever port you specified)</li>\n<li>login with your default username\
    \ and the password you specified via the <code>PASSWORD</code> environment variable.</li>\n\
    </ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-running-rstudio-server-with-podmandocker\"\
    \ class=\"anchor\" href=\"#running-rstudio-server-with-podmandocker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Rstudio Server with Podman/Docker</h2>\n<p>This approach is similar to <a href=\"\
    #running-rstudio-server-with-singularity\">Singularity</a>, but uses\nDocker or\
    \ Podman and a <code>docker-compose.yml</code> file instead.</p>\n<h3>\n<a id=\"\
    user-content-known-limitations\" class=\"anchor\" href=\"#known-limitations\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Known limitations</h3>\n<ul>\n<li>No access to shared group directories\
    \ (<a href=\"https://github.com/grst/rstudio-server-conda/issues/14\">#14</a>)</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h3>\n<ul>\n<li>\n<a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> or <a href=\"https://podman.io/\" rel=\"nofollow\"\
    >Podman</a>\n</li>\n<li>\n<a href=\"https://github.com/docker/compose\">docker-compose</a>\
    \ or <a href=\"https://github.com/containers/podman-compose\">podman-compose</a>\n\
    </li>\n<li>\n<a href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"\
    nofollow\">conda</a> or <a href=\"https://github.com/conda-forge/miniforge#mambaforge\"\
    >mamba</a>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-usage-1\" class=\"anchor\"\
    \ href=\"#usage-1\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repository</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:grst/rstudio-server-conda.git</pre></div>\n\
    </li>\n<li>\n<p>Build the rstudio container (fetches the latest version of <a\
    \ href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\">rocker/rstudio</a>\
    \ and adds some custom scripts)</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> rstudio-server-conda/docker\ndocker-compose\
    \ build     <span class=\"pl-c\"><span class=\"pl-c\">#</span> or podman-compose</span></pre></div>\n\
    </li>\n<li>\n<p>Copy the docker-compose.yml file into your project directory and\
    \ adjust the paths.</p>\n<p>You may want to add additional volumes with your data.</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-s\">[...]</span>\n\
    \   <span class=\"pl-ent\">ports</span>:\n      <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> port on the host : port in the container (the latter is always\
    \ 8787)</span>\n      - <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>8889:8787<span\
    \ class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-ent\">volumes</span>:\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> mount conda env into\
    \ exactely the same path as on the host system - some paths are hardcoded in the\
    \ env.</span>\n      - <span class=\"pl-s\">/home/sturm/anaconda3/envs/R400:/home/sturm/anaconda3/envs/R400</span>\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> Share settings between\
    \ rstudio instances</span>\n      - <span class=\"pl-s\">/home/sturm/.local/share/rstudio/monitored/user-settings:/root/.local/share/rstudio/monitored/user-settings</span>\n\
    \      <span class=\"pl-c\"><span class=\"pl-c\">#</span> mount the working directory\
    \ containing your R project.</span>\n      - <span class=\"pl-s\">/home/sturm/projects:/projects</span>\n\
    \    <span class=\"pl-ent\">environment</span>:\n      <span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> password used for authentication</span>\n      - <span\
    \ class=\"pl-s\">PASSWORD=notsafe</span>\n      <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> repeat the path of the conda environment (must be identical to\
    \ the path in \"volumes\")</span>\n      - <span class=\"pl-s\">CONDAENV=/home/sturm/anaconda3/envs/R400</span></pre></div>\n\
    </li>\n<li>\n<p>Run your project-specific instance of Rstudio-server</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>docker-compose up </pre></div>\n\
    </li>\n<li>\n<p>Log into Rstudio</p>\n</li>\n</ol>\n<ul>\n<li>Open your server\
    \ at <code>http://localhost:8889</code> (or whatever port you specified)</li>\n\
    <li>Login with the user <code>rstudio</code> (when using Docker) or <code>root</code>\
    \ (when using Podman) and the password you specified\nin the <code>docker-compose.yml</code>.\
    \ If you are using Podman and login with <code>rstudio</code> you won't have permissions\
    \ to\naccess the mounted volumes.</li>\n</ul>\n<h2>\n<a id=\"user-content-running-locally\"\
    \ class=\"anchor\" href=\"#running-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Locally</h2>\n<p>With\
    \ this approach a locally installed Rstudio server is ran such that it uses the\
    \ conda env.</p>\n<h3>\n<a id=\"user-content-known-limitations-1\" class=\"anchor\"\
    \ href=\"#known-limitations-1\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Known limitations</h3>\n<ul>\n<li>no\
    \ authentication (<a href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\
    >#3</a>). Use this approach only in a secure network!</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-prerequisites-2\" class=\"anchor\" href=\"#prerequisites-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h3>\n\
    <ul>\n<li>\n<a href=\"https://www.rstudio.com/products/rstudio/download-server/\"\
    \ rel=\"nofollow\">rstudio server</a> installed locally</li>\n<li>\n<a href=\"\
    https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\">conda</a> or\
    \ <a href=\"https://github.com/conda-forge/miniforge#mambaforge\">mamba</a>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-usage-2\" class=\"anchor\" href=\"#usage-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h3>\n<ol>\n<li>\n<p>Clone this repo</p>\n<pre><code>git clone\
    \ https://github.com/grst/rstudio-server-conda.git\n</code></pre>\n</li>\n<li>\n\
    <p>Run rstudio server in the conda env</p>\n<pre><code>cd rstudio-server-conda/local\n\
    conda activate my_project\n./start_rstudio_server.sh 8787  # use any free port\
    \ number here. \n</code></pre>\n</li>\n<li>\n<p>Connect to Rstudio</p>\n<p>You\
    \ should now be able to connect to rstudio server on the port you specify.\n<strong>If\
    \ an R Session has previously been running, you'll need to rstart the Rsession\
    \ now</strong>.</p>\n<p>Obviously, if your env does not have a version of <code>R</code>\
    \ installed, this will either not\nwork at all, or fall back to the system-wide\
    \ R installation.</p>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-how-it-works\"\
    \ class=\"anchor\" href=\"#how-it-works\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How it works</h3>\n<ul>\n<li>\n\
    <p>Rstudio server, can be started in non-daemonized mode by each user individually\
    \ on a custom port (similar to a jupyter notebook). This instance can then run\
    \ in a conda environment:</p>\n<pre><code>&gt; conda activate my_project\n&gt;\
    \ /usr/lib/rstudio-server/bin/rserver \\\n   --server-daemonize=0 \\\n   --www-port\
    \ 8787 \\\n   --rsession-which-r=$(which R) \\\n   --rsession-ld-library-path=$CONDA_PREFIX/lib\n\
    </code></pre>\n</li>\n<li>\n<p>To avoid additional problems with library paths,\
    \ also <code>rsession</code> needs to run within the conda environment. This is\
    \ achieved by wrapping <code>rsession</code> into the <a href=\"https://github.com/grst/rstudio-server-conda/blob/master/local/rsession.sh\"\
    >rsession.sh</a> script. The path to the wrapped <code>rsession</code> executable\
    \ can be passed to <code>rserver</code> as command line argument.</p>\n<pre><code>rserver\
    \ # ...\n    --rsession-path=rsession.sh\n</code></pre>\n</li>\n<li>\n<p>When\
    \ using multiple users a unique <code>secret-cookie-key</code> has to be generated\
    \ for each user. The path to the secret cookie key can be passed to <code>rserver</code>\
    \ as a command line parameter.</p>\n<pre><code>uuid &gt; /tmp/rstudio-server/${USER}_secure-cookie-key\n\
    rserver # ...\n  --secure-cookie-key-file /tmp/rstudio-server/${USER}_secure-cookie-key\n\
    </code></pre>\n</li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1623388496.0
zenotech/paraview-superbuild:
  data_format: 2
  description: https://gitlab.kitware.com/paraview/paraview-superbuild.git
  filenames:
  - Scripts/singularity/Singularity.egl
  - Scripts/singularity/Singularity.osmesa
  full_name: zenotech/paraview-superbuild
  latest_release: null
  readme: "<p><a href=\"Documentation/img/paraview100.png\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img src=\"Documentation/img/paraview100.png\" alt=\"ParaView-Superbuild\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h1>\n<p>ParaView-Superbuild,\
    \ henceforth referred to as \"superbuild\", is a project to\nbuild ParaView and\
    \ its dependencies. ParaView itself can be easily built using\nCMake as long as\
    \ the required external dependencies are available on the build\nmachine. However,\
    \ ParaView's several external dependencies, e.g. Qt, CGNS,\nFFMPEG, etc. can be\
    \ very tedious to build. Also, if you want to generate\nredistributable binaries,\
    \ you need to take extra care when building and\npackaging these dependencies.\
    \ To make our lives easier in supporting both these\nuse-cases, the superbuild\
    \ project was born.</p>\n<p>Although primarily designed to build the official\
    \ ParaView binaries, the\nsuperbuild has since been regularly used to build and\
    \ install ParaView\non various supercomputing systems.</p>\n<h1>\n<a id=\"user-content-obtaining-the-source\"\
    \ class=\"anchor\" href=\"#obtaining-the-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Obtaining the source</h1>\n<p>To\
    \ obtain the superbuild source locally, clone this repository using\n<a href=\"\
    https://git-scm.org\" rel=\"nofollow\">Git</a>.</p>\n<pre><code>$ git clone --recursive\
    \ https://gitlab.kitware.com/paraview/paraview-superbuild.git\n</code></pre>\n\
    <h1>\n<a id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building</h1>\n\
    <p>The superbuild can be built with a Makefiles or Ninja CMake generator. The\
    \ IDE\ngenerators (Xcode and Visual Studio) are not supported.</p>\n<h2>\n<a id=\"\
    user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\
    <p>The superbuild tries to provide all of its own dependencies, but some tooling\n\
    is assumed to be available on the host machine.</p>\n<ul>\n<li>Compiler toolchain\n\
    <ul>\n<li>GCC 4.9 or newer</li>\n<li>Xcode 10 or newer (older is probably supported,\
    \ but untested)</li>\n<li>MSVC 2017 or newer</li>\n<li>ICC (minimum version unknown)</li>\n\
    </ul>\n</li>\n<li>Tools\n<ul>\n<li>\n<code>pkg-config</code> is used on non-Windows\
    \ platforms to find dependencies in\nsome projects</li>\n<li>\n<code>ninja</code>\
    \ (or <code>make</code>) for building</li>\n<li>Python (if not built by the superbuild)\
    \ for building packages</li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-building-a-specific-version\"\
    \ class=\"anchor\" href=\"#building-a-specific-version\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ a specific version</h2>\n<p>The superbuild project uses the same versioning\
    \ scheme as ParaView,\nand gets tagged for every release of ParaView.  For example,\
    \ to build\nParaView version 5.7.1, checkout the <code>v5.7.0</code> tag of ParaView\
    \ and\nsuperbuild.</p>\n<p>Currently available tags are shown\n<a href=\"https://gitlab.kitware.com/paraview/paraview-superbuild/-/tags\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>To checkout a specific tag from the superbuild\
    \ git repository:</p>\n<pre><code>$ cd paraview-superbuild\n$ git fetch origin\
    \ # ensure you have the latest state from the main repo\n$ git checkout v5.7.0\
    \ # replace `v5.7.0` with tag name of your choice\n$ git submodule update\n</code></pre>\n\
    <p>At this point, your superbuild has all of the <em>rules</em> that were used\n\
    when building the selected version of ParaView. Also, note that it's\npossible\
    \ to build a version of ParaView using a different superbuild\nversion.  For example,\
    \ you could use superbuild <code>v5.7.0</code>, to build the\nlatest master (i.e.,\
    \ development) version of ParaView, or a custom\nbranch.  This is done by first\
    \ checking out the superbuild for the\nappropriate version and then setting the\
    \ CMake variables that affect\nwhich ParaView source is to be used.  There are\
    \ several ways to\ncontrol how superbuild finds its source packages:</p>\n<ol>\n\
    <li>If you want to use git to checkout ParaView source (default), then set\n<code>paraview_SOURCE_SELECTION</code>\
    \ to <code>git</code>, ensure <code>paraview_GIT_REPOSITORY</code> is\npointing\
    \ to the ParaView git repository you want to clone (by default it is\nset to the\
    \ offical ParaView repository) and then set the <code>paraview_GIT_TAG</code>\n\
    to be a specific tagname or branch available for the selected git\nrepository.\
    \ Use <code>master</code> for latest development code, <code>v5.7.0</code> for\
    \ the\n5.7.0 release, <code>release</code> for latest stable release, or a specific\
    \ ParaView\ncommit SHA. In this setup, when building the superbuild, it will clone\
    \ and\ncheckout the appropriate revision from the ParaView git repository automatically.</li>\n\
    <li>Instead of letting superbuild do the cloning and updating of the ParaView\n\
    source, you can also manually check it out and keep it updated as needed.\nTo\
    \ use this configuration, set <code>paraview_SOURCE_SELECTION</code> to <code>source</code>,\
    \ and\nset <code>paraview_SOURCE_DIR</code> to point to a custom ParaView source\
    \ tree. See 'offline\nbuilds' below for instructions to download needed dependency\
    \ packages.</li>\n<li>Another option is to use a source tarball of a ParaView\
    \ release. For that,\nset <code>paraview_SOURCE_SELECTION</code> to the version\
    \ to build such as <code>5.7.0</code>.\nThe superbuild offers the lastest stable\
    \ release as well as release\ncandidate in preparation for the release. This is\
    \ the best way to build a\nreleased version of ParaView.</li>\n</ol>\n<p><strong>NOTE:</strong>\
    \ If you switch to a superbuild version older than 5.2, the instructions\ndescribed\
    \ on this page are not relevant since the superbuild was refactored and\nchanged\
    \ considerably for 5.2. For older versions, refer to instructions on the\n<a href=\"\
    http://www.paraview.org/Wiki/index.php?title=ParaView/Superbuild&amp;oldid=59804\"\
    \ rel=\"nofollow\">Wiki</a>.</p>\n<p><strong>ALSO NOTE:</strong> Since this README\
    \ is expected to be updated for each version,\nonce you checkout a specfic version,\
    \ you may want to refer to the README for\nthat specific version.</p>\n<h2>\n\
    <a id=\"user-content-incremental-builds\" class=\"anchor\" href=\"#incremental-builds\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Incremental builds</h2>\n<p>The superbuild is kind of na\xEFve for\
    \ changes to project sources within the\nsuperbuild. This is due to the superbuild\
    \ not tracking all source files for\neach project and instead only \"stamp files\"\
    \ to indicate the steps performed.</p>\n<p>When changing the source of a subproject,\
    \ the best solution is to delete the\n\"stamp file\" for the build step of that\
    \ project:</p>\n<pre><code>$ rm superbuild/$project/stamp/$project-build\n</code></pre>\n\
    <p>and to rerun the superbuild's build step.</p>\n<h2>\n<a id=\"user-content-projects-and-features\"\
    \ class=\"anchor\" href=\"#projects-and-features\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Projects and\
    \ Features</h2>\n<p>The superbuild contains multiple projects which may be used\
    \ to enable\ndifferent features within the resulting ParaView build. Most projects\
    \ involve\ndownloading and adding the feature to the resulting package, but there\
    \ are a\nfew which are used just to enable features within ParaView itself.</p>\n\
    <p>The <code>paraview</code> project must be enabled to build ParaView.</p>\n\
    <p>The <code>paraviewsdk</code> project enables the building of a package which\
    \ includes\nheaders and libraries suitable for developing against ParaView. It\
    \ is only available\non Linux (at the moment).</p>\n<p>The <code>paraviewweb</code>\
    \ project adds web services into the resulting package.</p>\n<p>The <code>paraviewgettingstartedguide</code>,\
    \ and <code>paraviewtutorialdata</code> packages add\nstartup documentation and\
    \ example data to the package.</p>\n<p>ParaView supports multiple rendering engines\
    \ including <code>egl</code>, <code>mesa</code>,\n<code>osmesa</code>, and <code>qt5</code>.\
    \ All of these are incompatible with each other. If none of\nthese are chosen,\
    \ a UI-less ParaView will be built (basically just\n<code>pvpython</code>). On\
    \ Windows and macOS, only the <code>qt5</code> rendering engine is\navailable.</p>\n\
    <p>The <code>python</code> package is available to enable Python support in the\
    \ package. In\naddition, the <code>matplotlib</code> and <code>numpy</code> packages\
    \ are available.</p>\n<p>The following packages enable other features within ParaView:</p>\n\
    <ul>\n<li>\n<code>adios</code>: Enable readers and writers for visualization data\
    \ in the ADIOS\nfile format.</li>\n<li>\n<code>las</code>: Enable reading the\
    \ LAS file format</li>\n<li>\n<code>cosmotools</code>: Enables Cosmo file format\
    \ readers and related filters and\nalgorithms.</li>\n<li>\n<code>ffmpeg</code>:\
    \ Video encoding library for macOS and Linux.</li>\n<li>\n<code>ospray</code>:\
    \ A ray tracing rendering backend from Intel.</li>\n<li>\n<code>silo</code>: Support\
    \ reading the silo file format.</li>\n<li>\n<code>tbb</code>: Improved parallel\
    \ processing support within various VTK and\nParaView filters and algorithms.</li>\n\
    <li>\n<code>visitbridge</code>: Enables readers for file formats provided from\
    \ the VisIt\nproject.</li>\n<li>\n<code>vortexfinder2</code>: A collection of\
    \ tools to visualize and analyze vortices.</li>\n<li>\n<code>vrpn</code>: Virtual\
    \ reality support through the VRPN interface.</li>\n<li>\n<code>vtkm</code>: VTK-m\
    \ Accelerator Filters</li>\n<li>\n<code>xdmf3</code>: A meta file format built\
    \ on top of HDF5.</li>\n</ul>\n<h2>\n<a id=\"user-content-offline-builds\" class=\"\
    anchor\" href=\"#offline-builds\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Offline builds</h2>\n<p>The superbuild\
    \ has a <code>download-all</code> target that will download all of\nthe files\
    \ from the network that are necessary for the currently\nconfigured build. By\
    \ default, they are placed into the <code>downloads</code>\ndirectory of the build\
    \ tree.  This superbuild-plus-downloads tree may\nthen be copied to a non-networked\
    \ machine and pointed at using the\n<code>superbuild_download_location</code>\
    \ variable (or placed in the default\nlocation).</p>\n<p>Note that the <code>nvidiaoptix</code>\
    \ and <code>nvidiamdl</code> project sources are not available\nat their URLs\
    \ in the superbuild outside of Kitware due to their sources being\nbehind click-wrapping.\
    \ They may be manually downloaded from these web pages:</p>\n<ul>\n<li>\n<code>nvidiaoptix</code>:\
    \ <a href=\"https://developer.nvidia.com/designworks/optix/download\" rel=\"nofollow\"\
    >https://developer.nvidia.com/designworks/optix/download</a>\nThough older versions\
    \ are available here:\n<a href=\"https://developer.nvidia.com/designworks/optix/downloads/legacy\"\
    \ rel=\"nofollow\">https://developer.nvidia.com/designworks/optix/downloads/legacy</a>\n\
    </li>\n<li>\n<code>nvidiamdl</code>: <a href=\"https://developer.nvidia.com/mdl-sdk\"\
    \ rel=\"nofollow\">https://developer.nvidia.com/mdl-sdk</a>\n</li>\n</ul>\n<h3>\n\
    <a id=\"user-content-overriding-downloaded-archives\" class=\"anchor\" href=\"\
    #overriding-downloaded-archives\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Overriding downloaded archives</h3>\n\
    <p>On rare occasions, you may want to replace a downloaded archive with a different\n\
    version. You may replace the archive with a newer version preserving its\nname,\
    \ however, on doing so, the hash verification will most likely fail during\nthe\
    \ build step. To skip the hash verification for archives that have been\nmanually\
    \ changed, set the <code>xxx_SKIP_VERIFICATION</code> option, where <code>xxx</code>\n\
    is the name of the project. <code>xxx_SKIP_VERIFICATION</code> must be passed\
    \ on command line\nwhen invoking CMake using <code>-Dxxx_SKIP_VERIFICATION:BOOL=TRUE</code>.</p>\n\
    <p>Alternatively, you can edit the <code>versions.cmake</code> files in the source\
    \ repository\nand modify the <code>URL_MDF5</code> or <code>URL_HASH</code> values\
    \ for the specific project with\nupdated hashes.</p>\n<h2>\n<a id=\"user-content-installing\"\
    \ class=\"anchor\" href=\"#installing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing</h2>\n<p>The superbuild\
    \ supports the <code>install</code> target by selecting a template package\nusing\
    \ the <code>SUPERBUILD_DEFAULT_INSTALL</code> variable. The default and availability\n\
    depends on the platform and selected projects, but valid values for this\ninclude:</p>\n\
    <ul>\n<li><code>paraview/ZIP</code></li>\n<li><code>paraview/DragNDrop</code></li>\n\
    <li><code>paraview/TGZ</code></li>\n<li><code>paraview/TXZ</code></li>\n<li><code>paraviewsdk/TGZ</code></li>\n\
    <li><code>paraviewsdk/TXZ</code></li>\n</ul>\n<p>The CMake cache editors (<code>ccmake</code>\
    \ and <code>cmake-gui</code>) have dropdown options for\nthe supported options.</p>\n\
    <p>The selected package logic will be used to install ParaView and its\ndependencies\
    \ into <code>CMAKE_INSTALL_PREFIX</code> rather than being placed into a\npackage.\
    \ For example, the <code>DragNDrop</code> generator creates <code>.app</code>\
    \ bundles which\nwill be created whereas the <code>TGZ</code>, <code>TXZ</code>,\
    \ and <code>ZIP</code> generators use the standard\n<code>bin/</code>, <code>lib/</code>,\
    \ etc. directories.</p>\n<h3>\n<a id=\"user-content-caveats\" class=\"anchor\"\
    \ href=\"#caveats\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Caveats</h3>\n<p>If using the <code>git</code> source\
    \ selection for ParaView, the build will rerun when\nusing the <code>install</code>\
    \ target due to limitations in the external project\nmechanisms and the way CPack\
    \ works. There are two ways to avoid this:</p>\n<ul>\n<li>the <code>SUPERBUILD_OFFLINE_BUILD</code>\
    \ option may be set to <code>ON</code> to unlink the git\nupdate step from the\
    \ configure/build steps; or</li>\n<li>the initial build can just be run using\
    \ the <code>install</code> target instead of\nthe usual <code>make &amp;&amp;\
    \ make install</code> pattern.</li>\n</ul>\n<h2>\n<a id=\"user-content-external-plugins\"\
    \ class=\"anchor\" href=\"#external-plugins\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>External plugins</h2>\n<p>The\
    \ superbuild supports building more plugins into ParaView using the\n<code>paraviewexternalplugins</code>\
    \ project. As an example, to build two external\nplugins <code>a</code> and <code>b</code>,\
    \ the following settings should be used:</p>\n<ul>\n<li>\n<code>ENABLE_paraviewexternalplugins:BOOL=ON</code>:\
    \ Enables building using external\nplugins.</li>\n<li>\n<code>paraview_PLUGINS_EXTERNAL:STRING=a;b</code>:\
    \ The list of plugins to build.</li>\n<li>\n<code>paraview_PLUGIN_a_PATH:PATH=/path/to/plugin/a</code>:\
    \ The path to plugin <code>a</code>'s\nsource directory. It must contain a <code>plugins.cmake</code>\
    \ to be picked up by\nParaView.</li>\n<li>\n<code>paraview_PLUGIN_b_PATH:PATH=/path/to/plugin/b</code>:\
    \ Same as above, but for\nplugin <code>b</code>.</li>\n</ul>\n<h2>\n<a id=\"user-content-cmake-variables\"\
    \ class=\"anchor\" href=\"#cmake-variables\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CMake Variables</h2>\n<h3>\n\
    <a id=\"user-content-style-guide\" class=\"anchor\" href=\"#style-guide\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Style\
    \ Guide</h3>\n<p>Note that currently not all project and configuration variables\
    \ follow this\nstyle guide but any new projects should use this convention while\
    \ any\nexisting projects and configuration variables will transition to this over\n\
    time.</p>\n<ul>\n<li>All references to a given project name will be lowercase.</li>\n\
    <li>Underscores will be used as word seperators in variable names.</li>\n<li>All\
    \ project specific configuration variables will be lower-case project\nname followed\
    \ by upper-case setting name.\nExamples include:\n<ul>\n<li>\n<code>mesa_USE_SWR</code>\
    \ : Enable the OpenSWR driver for (OS)Mesa.</li>\n<li>\n<code>ospray_BUILD_ISA</code>\
    \ : Select the SIMD architecture used to build OSPray.</li>\n</ul>\n</li>\n<li>Internal\
    \ variables used within a given project's projectname.cmake file\nwill be all\
    \ lower-case.</li>\n<li>Multiple versions:\n<ul>\n<li>Use the <code>superbuild_set_selectable_source</code>\
    \ macro to allow multiple\nversions of a given project.</li>\n<li>Specify source\
    \ selection versions as numeric, i.e. without any \"v\" or\n\"V\" prefix.</li>\n\
    <li>If the project is going through a release candidate cycle, add the\navailable\
    \ RCs as additional sources as they become availabe.  Once\na final release is\
    \ made, replace all the RCs with the updated release.</li>\n</ul>\n</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-build-variables\" class=\"anchor\" href=\"#build-variables\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Build Variables</h3>\n<ul>\n<li>\n<code>superbuild_download_location</code>\
    \ (default <code>${CMAKE_BINARY_DIR}/downloads</code>):\nThe location to store\
    \ downloaded source artifacts. Usually, it is changed\nso that it is preserved\
    \ across a wipe of the build directory.</li>\n<li>\n<code>SUPERBUILD_PROJECT_PARALLELISM</code>\
    \ (default based on the number of available\nprocessors): When using a Makefiles\
    \ generator, subproject builds use <code>-j</code>\nexplicitly with this number.</li>\n\
    <li>\n<code>ENABLE_xxx</code> (generally, default <code>OFF</code>): If selected,\
    \ the <code>xxx</code> project\nwill be built within the superbuild. See above\
    \ for descriptions of the\nvarious projects. <code>ENABLE_</code> flags are not\
    \ shown for projects which must be\nenabled due to a project depending on it (e.g.,\
    \ <code>visitbridge</code> requires\n<code>boost</code>, so enabling <code>visitbridge</code>\
    \ will hide the <code>ENABLE_boost</code> option).</li>\n<li>\n<code>USE_SYSTEM_xxx</code>\
    \ (default <code>OFF</code>): If selected, the <code>xxx</code> project from the\n\
    build environment is used instead of building it within the superbuild.\nNot all\
    \ projects support system copies (the flag is not available if so).</li>\n<li>\n\
    <code>SUPERBUILD_DEBUG_CONFIGURE_STEPS</code> (default <code>OFF</code>): If set,\
    \ the superbuild\nwill log configure steps for each <code>xxx</code> project into\n\
    <code>superbuild/xxx/stamp/xxx-configure-*.log</code> files.</li>\n<li>\n<code>CMAKE_BUILD_TYPE</code>\
    \ (default <code>Release</code>): The build type to use for the\nbuild. Can be\
    \ <code>Release</code>, <code>RelWithDebInfo</code>, or (on not-Windows) <code>Debug</code>.</li>\n\
    </ul>\n<p>The following flags affect ParaView directly:</p>\n<ul>\n<li>\n<p><code>paraview_SOURCE_SELECTION</code>\
    \ (default <code>5.9.0</code>): The source to use for\nParaView itself. The version\
    \ numbers use the source tarballs from the\nwebsite for the release. The <code>source</code>\
    \ selection uses the\n<code>paraview_SOURCE_DIR</code> variable to look at a checked\
    \ out ParaView source\ndirectory. The <code>git</code> selection has the superbuild\
    \ clone and builds a\ncheckout of ParaView from git repository controlled by the\n\
    <code>paraview_GIT_REPOSITORY</code> and <code>paraview_GIT_TAG</code> variables.\
    \ By default, the\n<code>master</code> branch of the main repository is used.</p>\n\
    <p><strong>Note</strong>: When using the <code>source</code> selection, incremental\
    \ builds to the\nsuperbuild may not rebuild ParaView even if the source tree has\
    \ changed.\nThis is because the superbuild is \"blind\" to the source tree other\
    \ than\nits existence.</p>\n</li>\n<li>\n<p><code>CMAKE_BUILD_TYPE_paraview</code>\
    \ (default is the same as the superbuild):\nParaView may be built with a different\
    \ build type (e.g., <code>Release</code> vs.\n<code>RelWithDebInfo</code>) as\
    \ the rest of the superbuild using this variable. In\naddition to <code>&lt;SAME&gt;</code>\
    \ which uses <code>CMAKE_BUILD_TYPE</code>, any valid value for\n<code>CMAKE_BUILD_TYPE</code>\
    \ is also valid.</p>\n</li>\n<li>\n<p><code>BUILD_SHARED_LIBS_paraview</code>\
    \ (default is the same as the superbuild):\nParaView may be built with a different\
    \ selection for BUILD_SHARED_LIBS flag\nthan the rest of the superbuild using\
    \ this variable. For example,\nto build ParaView static while building other projects\
    \ in the superbuild\n(e.g. MPI, Python, etc.) as shared, set <code>BUILD_SHARED_LIBS</code>\
    \ to <code>ON</code>\nand <code>BUILD_SHARED_LIBS_paraview</code> to <code>OFF</code>.</p>\n\
    </li>\n<li>\n<p><code>PARAVIEW_BUILD_WEB_DOCUMENTATION</code> (default <code>OFF</code>):\
    \ Have ParaView build\nits HTML documentation.</p>\n</li>\n<li>\n<p><code>mesa_USE_SWR</code>\
    \ (default <code>ON</code>): If <code>mesa</code> is enabled, this enables\nIntel's\
    \ software rasterization backend (x86 only).</p>\n</li>\n<li>\n<p><code>PARAVIEW_INITIALIZE_MPI_ON_CLIENT</code>\
    \ (default <code>ON</code>): If <code>mpi</code> is enabled, this\nenables MPI\
    \ to be initialized automatically when running the GUI or pvpython.\nSome readers\
    \ use MPI IO and thus must have MPI initialized in order to be\nused so this is\
    \ the default for general ease of use. For some MPI implementations,\na code that\
    \ initializes MPI must be run with the appropriate mpi launcher\n(e.g. mpirun)\
    \ which in this case it may be desirable to disable this option.\nNote that the\
    \ <code>--mpi</code> or <code>--no-mpi</code> command line options to paraview\
    \ and\npvpython can be used to override this option.</p>\n</li>\n<li>\n<p><code>PARAVIEW_EXTRA_CMAKE_ARGUMENTS</code>\
    \ (default <code>\"\"</code>: Extra CMake arguments to\npass to ParaView's configure\
    \ step. This can be used to set CMake variables\nfor the build that are otherwise\
    \ not exposed in the superbuild itself.</p>\n</li>\n<li>\n<p><code>PARAVIEW_ENABLE_VRPLUGIN</code>\
    \ (default <code>ON</code>): Enables the VRPlugin. If\n<code>vrpn</code> is enabled,\
    \ the VRPlugin will support input devices through a VRPN\nconnection. VRUI support\
    \ is enabled unconditionally on Linux.</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-paraview-editions\"\
    \ class=\"anchor\" href=\"#paraview-editions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>ParaView editions</h4>\n<p>A\
    \ typical ParaView build includes several modules and dependencies. While these\n\
    are necessary for a fully functional application, there are cases (e.g. in situ\n\
    use-cases) where a build with limited set of features is adequate. ParaView build\
    \ supports\nthis using the <code>PARAVIEW_BUILD_EDITION</code> setting. Supported\
    \ values for this setting are:</p>\n<ul>\n<li>\n<code>CORE</code>: Build modules\
    \ necessary for core ParaView functionality.\nThis does not include rendering.</li>\n\
    <li>\n<code>RENDERING</code>: Build modules necessary for supporting rendering\
    \ including views\nand representations. This includes everything in <code>CORE</code>.</li>\n\
    <li>\n<code>CATALYST</code>: Build all modules necessary for in situ use cases\
    \ without\nrendering and optional components like NetCDF- and HDF5-based readers\
    \ and\nwriters.</li>\n<li>\n<code>CATALYST_RENDERING</code>: Same as <code>CATALYST</code>\
    \ but with rendering supported added.</li>\n<li>\n<code>CANONICAL</code> (default):\
    \ Build modules necessary for standard ParaView build.</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-packaging-variables\" class=\"anchor\" href=\"#packaging-variables\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Packaging Variables</h3>\n<ul>\n<li>\n<code>PARAVIEW_PACKAGE_SUFFIX</code>\
    \ (default based on selected options): The suffix\nfor the name generated by the\
    \ package.</li>\n<li>\n<code>paraview_PLUGINS_AUTOLOAD</code>: List of plugins\
    \ to autoload in the packaged\nParaView.</li>\n</ul>\n<h1>\n<a id=\"user-content-packaging\"\
    \ class=\"anchor\" href=\"#packaging\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Packaging</h1>\n<p>The packages\
    \ may be built using the <code>cpack-paraview</code> tests via <code>ctest</code>.\
    \ The\neasiest way to build all available packages is to run <code>ctest -R cpack</code>.</p>\n\
    <h1>\n<a id=\"user-content-learning-resources\" class=\"anchor\" href=\"#learning-resources\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Learning Resources</h1>\n<ul>\n<li>\n<p>General information is available\
    \ at the <a href=\"http://www.paraview.org\" rel=\"nofollow\">ParaView Homepage</a>.</p>\n\
    </li>\n<li>\n<p>Community discussion takes place on the <a href=\"http://www.paraview.org/mailing-lists/\"\
    \ rel=\"nofollow\">ParaView Mailing Lists</a>.</p>\n</li>\n<li>\n<p>Commercial\
    \ <a href=\"http://www.kitware.com/products/support.html\" rel=\"nofollow\">support</a>\
    \ and <a href=\"http://www.kitware.com/products/protraining.php\" rel=\"nofollow\"\
    >training</a>\nare available from <a href=\"http://www.kitware.com/\" rel=\"nofollow\"\
    >Kitware</a>.</p>\n</li>\n</ul>\n<h1>\n<a id=\"user-content-reporting-bugs\" class=\"\
    anchor\" href=\"#reporting-bugs\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Reporting Bugs</h1>\n<p>If you have\
    \ found a bug:</p>\n<ol>\n<li>\n<p>If you have a patch, please read the <a href=\"\
    CONTRIBUTING.md\">CONTRIBUTING.md</a> document.</p>\n</li>\n<li>\n<p>Otherwise,\
    \ please join one of the <a href=\"http://www.paraview.org/mailing-lists/\" rel=\"\
    nofollow\">ParaView Mailing Lists</a> and ask\nabout the expected and observed\
    \ behaviors to determine if it is\nreally a bug.</p>\n</li>\n<li>\n<p>Finally,\
    \ if the issue is not resolved by the above steps, open\nan entry in the <a href=\"\
    http://www.paraview.org/Bug\" rel=\"nofollow\">ParaView Issue Tracker</a>.</p>\n\
    </li>\n</ol>\n<h1>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h1>\n<p>Like ParaView, ParaView-Superbuild is distributed\
    \ under the OSI-approved BSD\n3-clause License. See <a href=\"Copyright.txt\"\
    >Copyright.txt</a> for details. For additional licenses,\nrefer to <a href=\"\
    http://www.paraview.org/paraview-license/\" rel=\"nofollow\">ParaView Licenses</a>.</p>\n\
    <h1>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contributing</h1>\n<p>See <a href=\"CONTRIBUTING.md\">CONTRIBUTING.md</a>\
    \ for instructions to contribute.</p>\n"
  stargazers_count: 1
  subscribers_count: 5
  topics:
  - paraview-superbuild
  - cmake
  - superbuild
  updated_at: 1612294938.0
