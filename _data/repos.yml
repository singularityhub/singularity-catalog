54r4/sara-server-vre:
  data_format: 2
  description: Virtual Research Environment for Sara Server - container build scripts
  filenames:
  - Singularity
  full_name: 54r4/sara-server-vre
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-sara-server-vre\" class=\"anchor\" href=\"#sara-server-vre\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>sara-server-vre</h1>\n<p>Virtual Research Environment for Sara Server\
    \ - container build scripts</p>\n<p>This is the VRE main spec containing a Java\
    \ Runtime Environment plus Eclipse\nused for the development of the SARA service.\n\
    A local postgres database is integrated, too. The source is a docker repo\nwhich\
    \ is being pulled on build time and used to locally run a postgresql\nserver using\
    \ udocker.\nThis VRE has no external requirements whatsoever once the image has\
    \ been built.</p>\n<h2>\n<a id=\"user-content-use-prebuild-image\" class=\"anchor\"\
    \ href=\"#use-prebuild-image\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Use prebuild image</h2>\n<pre><code>\
    \   cd /tmp\n   singularity pull --name \"sara-server-vre.img\" shub://c1t4r/sara-server-vre\n\
    \   ./sara-server-vre.img\n</code></pre>\n<h2>\n<a id=\"user-content-build-local-image-singularity-23\"\
    \ class=\"anchor\" href=\"#build-local-image-singularity-23\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Build local\
    \ image (Singularity 2.3)</h2>\n<pre><code>cd /tmp\nsingularity create -s 2048\
    \ sara-server-vre.img\nsingularity bootstrap sara-server-vre.img ./Singularity\n\
    ./sara-server-vre.img\n</code></pre>\n<h2>\n<a id=\"user-content-build-local-image-singularity-24\"\
    \ class=\"anchor\" href=\"#build-local-image-singularity-24\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Build local\
    \ image (Singularity 2.4)</h2>\n<pre><code>sudo singularity build sara-server-vre.simg\
    \ ./Singularity\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1546985098.0
APSIMInitiative/APSIMClassic:
  data_format: 2
  description: APSIM
  filenames:
  - Release/Containers/Singularity/Singularity.ubuntu
  full_name: APSIMInitiative/APSIMClassic
  latest_release: null
  readme: '<h1>

    <a id="user-content-apsim" class="anchor" href="#apsim" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>APSIM</h1>

    <p>The Agricultural Production Systems sIMulator (APSIM) is internationally recognised
    as a highly advanced simulator of agricultural systems. It contains a suite of
    modules which enable the simulation of systems that cover a range of plant, animal,
    soil, climate and management interactions. APSIM is undergoing continual development,
    with new capability added to regular releases of official versions. Its development
    and maintenance is underpinned by rigorous science and software engineering standards.
    The APSIM Initiative has been established to promote the development and use of
    the science modules and infrastructure software of APSIM.</p>

    <p>CI builds of this repository can be found <a href="https://apsimdev.apsim.info/APSIM.Builds.Portal/Bob.aspx"
    rel="nofollow">Here</a>.</p>

    '
  stargazers_count: 17
  subscribers_count: 17
  topics: []
  updated_at: 1621547465.0
Amjadhpc/PHEnix:
  data_format: 2
  description: This is singularity 2.6.0 image for PHEnix -1.4a
  filenames:
  - Singularity-2.6.0
  full_name: Amjadhpc/PHEnix
  latest_release: null
  readme: '<h1>

    <a id="user-content-game-container" class="anchor" href="#game-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>game-container</h1>

    <p>Containers for game AI</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1539686949.0
BennerLab/pipelines:
  data_format: 2
  description: Lab pipelines using Snakemake + Singularity + SCIF
  filenames:
  - chip-seq.scif/Singularity
  - rna-seq-multisamples/Singularity
  full_name: BennerLab/pipelines
  latest_release: null
  readme: '<p><a href="https://sci-f.github.io" rel="nofollow"><img src="https://camo.githubusercontent.com/a7912c9863e897576e5d434d91e359d254976266bee2f9b1405197941f940bdf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66696c6573797374656d2d736369656e74696669632d626c75652e737667"
    alt="scif" data-canonical-src="https://img.shields.io/badge/filesystem-scientific-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://snakemake.readthedocs.io/en/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/93b6e9faa8e75932017af0ff2ca7db9493cc08e51c462e71143809db606cb04d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d253345253344253230342e362e302d626c75652e737667"
    alt="snakemake" data-canonical-src="https://img.shields.io/badge/snakemake-%3E%3D%204.6.0-blue.svg"
    style="max-width:100%;"></a>

    <a href="http://singularity.lbl.gov/" rel="nofollow"><img src="https://camo.githubusercontent.com/8ed7d71f6eadf7149f22c334c2b29e0479f493cfaced652052e122f59b5920be/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d253345253344253230322e342e322d626c75652e737667"
    alt="singularity" data-canonical-src="https://img.shields.io/badge/singularity-%3E%3D%202.4.2-blue.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-svenner-lab-documentation" class="anchor" href="#svenner-lab-documentation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Svenner
    Lab Documentation</h1>

    <p>This repository contains the Svenner lab pipelines for various types of sequencing
    data. All pipelines are implemented in Snakemake and use the Singularity + Scientific
    Filesystem to create reproducible research environments.</p>

    <h2>

    <a id="user-content-pipelines" class="anchor" href="#pipelines" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pipelines</h2>

    <ul>

    <li><a href="https://github.com/BennerLab/pipelines/tree/master/chip-seq.scif">ChIP-seq
    Pipeline</a></li>

    <li><a href="https://github.com/BennerLab/pipelines/tree/master/rna-seq-multisamples">RNA-seq
    Multisample Pipeline</a></li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1527699137.0
Betterton-Lab/C-GLASS:
  data_format: 2
  description: Open source simulation engine for coarse-grained Brownian dynamics
  filenames:
  - Singularity
  full_name: Betterton-Lab/C-GLASS
  latest_release: v0.2.3
  readme: "<h1>\n<a id=\"user-content-c-glass\" class=\"anchor\" href=\"#c-glass\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>C-GLASS</h1>\n<p>A <strong>C</strong>oarse-<strong>G</strong>rained\
    \ <strong>L</strong>iving <strong>A</strong>ctive <strong>S</strong>ystem <strong>S</strong>imulator</p>\n\
    <p><a href=\"https://travis-ci.com/Betterton-Lab/C-GLASS\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/061b295758d2c64d80b7d3e97ceebc53e21cc632602b7b32c77f046c105d84ac/68747470733a2f2f7472617669732d63692e636f6d2f426574746572746f6e2d4c61622f432d474c4153532e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/Betterton-Lab/C-GLASS.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://doi.org/10.5281/zenodo.3841613\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/96212f14675a05209e745328444de906fc58f72d1794af88b9db02b4fe6f24e5/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333834313631332e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3841613.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"figs/cglass_snapshot.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"figs/cglass_snapshot.png\"\
    \ alt=\"A simulation using C-GLASS\" title=\"A simulation using C-GLASS\" style=\"\
    max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-installation\" class=\"\
    anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>First clone\
    \ the repo, including submodule dependencies.</p>\n<pre><code>git clone --recursive\
    \ https://github.com/Betterton-Lab/C-GLASS\ncd C-GLASS\n</code></pre>\n<p>C-GLASS\
    \ can either be run in a container using Docker or Singularity, or be built from\
    \ source using CMake.</p>\n<h3>\n<a id=\"user-content-running-with-docker\" class=\"\
    anchor\" href=\"#running-with-docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running with Docker</h3>\n<p>A\
    \ pre-built image of C-GLASS is available as a <a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> image. To download the image, run</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>docker pull jeffmm/cglass</pre></div>\n\
    <p>To use the image, run the provided script to launch a Docker container named\
    \ <code>cglass_latest</code> in the background</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>./launch_docker.sh</pre></div>\n<p>You may also build the Docker image yourself\
    \ by providing the launch script with the <code>-b</code> flag.</p>\n<p>To launch\
    \ C-GLASS, run</p>\n<div class=\"highlight highlight-source-shell\"><pre>docker\
    \ <span class=\"pl-c1\">exec</span> cglass_latest cglass.exe [optional-flags]\
    \ [parameter-file]</pre></div>\n<h3>\n<a id=\"user-content-running-with-singularity\"\
    \ class=\"anchor\" href=\"#running-with-singularity\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running with\
    \ Singularity</h3>\n<p>If you are using Singularity, C-GLASS is also available\
    \ as a Singularity image. The command</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull docker://jeffmm/cglass</pre></div>\n<p>will create a local\
    \ file named <code>cglass_latest.sif</code>. You may then run</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ cglass_latest.sif cglass.exe [optional-flags] [parameter-file]</pre></div>\n\
    <p>The Singularity image may also be built locally using the provided recipe in\
    \ the file <code>Singularity</code></p>\n<h3>\n<a id=\"user-content-building-from-source\"\
    \ class=\"anchor\" href=\"#building-from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building from source</h3>\n<p>C-GLASS\
    \ is ready to be built from source using CMake, provided several dependencies\
    \ are installed:</p>\n<ul>\n<li>CMake (version 3.13+)</li>\n<li><a href=\"https://github.com/jbeder/yaml-cpp\"\
    >libyaml-cpp</a></li>\n<li>libgsl-dev</li>\n<li>libopenmpi-dev</li>\n<li>libfftw3-dev</li>\n\
    <li>libboost-math1.67-dev</li>\n</ul>\n<p>Included is a script for building C-GLASS\
    \ with CMake. To build C-GLASS (without graphics or parallelization) run</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./install.sh</pre></div>\n\
    <p>There are additional flags for building with OpenMP, building with graphics,\
    \ installing C-GLASS in <code>/usr/local</code>, etc. To see a menu of options,\
    \ run</p>\n<div class=\"highlight highlight-source-shell\"><pre>./install.sh -h</pre></div>\n\
    <h3>\n<a id=\"user-content-building-with-graphics\" class=\"anchor\" href=\"#building-with-graphics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building with graphics</h3>\n<p>C-GLASS is available with graphics\
    \ for Mac OSX. To install on Mac OSX, you will need the glew and glfw3 libraries,\
    \ both of which can be installed using <a href=\"https://brew.sh/\" rel=\"nofollow\"\
    >Homebrew</a>.</p>\n<div class=\"highlight highlight-source-shell\"><pre>brew\
    \ install glew\nbrew install glfw</pre></div>\n<p>You may also need to help CMake\
    \ find your OpenGL Framework libraries.</p>\n<p>Several other libraries are required\
    \ for running C-GLASS with graphics on Linux or in WSL. See the <code>src/CMakeLists.txt</code>\
    \ file for a comprehensive list of libraries passed to the compiler when building\
    \ C-GLASS with graphics on WSL.</p>\n<h2>\n<a id=\"user-content-running-c-glass\"\
    \ class=\"anchor\" href=\"#running-c-glass\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running C-GLASS</h2>\n<p>The\
    \ C-GLASS executable is run as</p>\n<pre><code>cglass.exe [optional-flags] [parameter-file]\
    \ \n</code></pre>\n<p>The following flags are available:</p>\n<pre><code>--help,\
    \ -h\n    Show the help menu which gives short descriptions about each of the\
    \ flags\n    as well as binary usage\n \n --run-name rname, -r rname \n    Overwrites\
    \ the parameter \"run_name\" with rname which serves as a prefix for\n    all\
    \ outputs \n\n--n-runs num, -n num\n    Overwrites the parameter \"n_runs\" with\
    \ num, which tells the simulation how\n    many times to run the given parameter\
    \ set with different random number\n    generator seeds.\n\n--movie, -m\n    Uses\
    \ the parameters file params_file to load any output files that were\n    generated\
    \ from previous runs of the simulation to replay the graphics and\n    record\
    \ the frames as bitmaps into the directory specified with the\n    \"movie_directory\"\
    \ parameter\n\n--analysis, -a\n    Loads posit/spec files into the simulation\
    \ for analysis in the same manner\n    as the movie flag\n\n-reduce reduce_factor,\
    \ -R reduce_factor\n    Reads in output files and writes new output files that\
    \ are smaller by a\n    factor of reduce_factor, effectively reducing time resolution\
    \ of output\n    data.\n\n--load, -l\n    Specifies to load any checkpoint files\
    \ corresponding to the given parameter\n    file, which can be used to continue\
    \ a simulation that ended prematurely.\n    New simulation will be given the name\
    \ old_simulation_name_reload00n where n\n    is the number of reloads performed\
    \ on that simulation.\n\n--with-reloads, -w\n    If running analyses or making\
    \ movies, C-GLASS will look for parameter files\n    that have the same run name\
    \ but with the reload00n addendum and attempt to\n    open the corresponding output\
    \ files whenever it reached EOF while reading\n    an output file.\n\n--blank,\
    \ -b\n    Generates all relevant parameter files using the SimulationManager without\n\
    \    running the simulations. Useful for generating many parameter files from\n\
    \    parameter sets (discussed below) and deploying simulations on different\n\
    \    processors and/or machines.\n\n--auto-graph, -G\n    By default, C-GLASS\
    \ will wait for the user to press the ESC key in the\n    OpenGL graphics window\
    \ before starting to run the simulation. Providing\n    this flag will cause the\
    \ simulation to begin immediately without user\n    input. Goes great with the\
    \ -m flag for creating multiple movies without\n    input from the user.\n</code></pre>\n\
    <h2>\n<a id=\"user-content-parameter-files\" class=\"anchor\" href=\"#parameter-files\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Parameter files</h2>\n<p>All parameters used in the simulation, along\
    \ with their default values and data types, are specified in the <code>default_config.yaml</code>\
    \ file in the <code>config</code> folder.</p>\n<p>The parameter file is a YAML\
    \ file and looks like:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span\
    \ class=\"pl-ent\">global_param_1</span>: <span class=\"pl-s\">gp1_value</span>\n\
    <span class=\"pl-ent\">global_param_2</span>: <span class=\"pl-s\">gp2_value</span>\n\
    <span class=\"pl-ent\">species</span>:\n    <span class=\"pl-ent\">global_species_param_1</span>:\
    \ <span class=\"pl-s\">gsp1_value</span>\n    <span class=\"pl-ent\">global_species_param_2</span>:\
    \ <span class=\"pl-s\">gsp2_value</span>\n<span class=\"pl-ent\">specific_species_name</span>:\n\
    \    <span class=\"pl-ent\">species_param_1</span>: <span class=\"pl-s\">sp1_value</span>\n\
    \    <span class=\"pl-ent\">species_param_2</span>: <span class=\"pl-s\">sp2_value</span></pre></div>\n\
    <p>See the <code>examples</code> folder for examples of parameter files.</p>\n\
    <p>Notice that there are three parameter types: global parameters, global species\
    \ parameters, and species parameters. Global parameters are parameters that are\
    \ common to the entire system, such system size, integration time step, etc. Species\
    \ parameters are unique to the specified species, such as <code>filament</code>.\
    \ There is also an optional global species parameter type that affects every species,\
    \ such as the frequency to write to output files.</p>\n<p>What do I mean by species?\
    \ C-GLASS assumes that any given simulation will likely have many copies of one\
    \ kind of thing, which I call a species, perhaps interacting with other species\
    \ of other kinds. In a system of interacting spheres, the species is 'sphere.'\
    \ In a system of interacting semiflexible filaments, the species is 'filament.'\
    \ Simulations can have many types of species all interacting with each other with\
    \ different species-species interaction potentials.</p>\n<p>If any parameter is\
    \ not specified in the parameter file, any instance of that parameter in the simulation\
    \ will assume its default value specified in the <code>config/default_config.yaml</code>\
    \ file.</p>\n<p>Some important global parameters are:</p>\n<pre><code>seed\n \
    \   simulation seed to use with random number generator \nrun_name\n    prefix\
    \ for all output files\nn_runs\n    number of individual runs of each parameter\
    \ type\nn_random\n    number of samples from a random parameter space (see more\
    \ below)\nn_dim\n    number of dimensions of simulation\nn_periodic\n    number\
    \ of periodic dimensions of simulation\ndelta   \n    simulation time step\nn_steps\n\
    \    total number of steps in each simulation\nsystem_radius\n    \"box radius\"\
    \ of system\ngraph_flag\n    run with graphics enabled\nn_graph\n    how many\
    \ simulation steps to take between updating graphics\nmovie_flag\n    whether\
    \ to record the graphics frames into bitmaps\nmovie_directory\n    local directory\
    \ used to save the recorded bitmaps\nthermo_flag\n    whether to output thermodynamics\
    \ outputs (stress tensors, etc)\nn_thermo\n    how often to output the thermodynamics\
    \ outputs\npotential_type\n    can be 'wca' or 'soft' for now\n</code></pre>\n\
    <p>Some important global species parameters are:</p>\n<pre><code>num\n    how\
    \ many to insert into system\ninsertion_type\n    how to insert object into system\
    \ (e.g. random)\noverlap\n    whether species can overlap at initiation\ndraw_type\n\
    \    (orientation, fixed, or bw) how to color the object\ncolor\n    a double\
    \ that specifies the RGB value of the object\nposit_flag\n    whether to output\
    \ position files\nn_posit\n    how often to output position files\nspec_flag\n\
    \    whether to output species files\nn_spec\n    how often to output species\
    \ files\ncheckpoint_flag\n    whether to output checkpoint files\nn_checkpoint\n\
    \    how often to output checkpoint files\n</code></pre>\n<h2>\n<a id=\"user-content-advanced-usage\"\
    \ class=\"anchor\" href=\"#advanced-usage\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Advanced usage</h2>\n<h3>\n<a\
    \ id=\"user-content-running-unit-tests\" class=\"anchor\" href=\"#running-unit-tests\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running unit tests</h3>\n<p>One may run C-GLASS's unit tests by passing\
    \ <code>-DTESTS=TRUE</code> to CMake</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>mkdir build\n<span class=\"pl-c1\">cd</span> build\ncmake -DTESTS=TRUE ..\n\
    make\nmake <span class=\"pl-c1\">test</span></pre></div>\n<h3>\n<a id=\"user-content-adding-new-parameters\"\
    \ class=\"anchor\" href=\"#adding-new-parameters\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Adding new parameters</h3>\n\
    <p>C-GLASS comes with it's own parameter initialization tool, <code>configure_C-GLASS.exe</code>,\
    \ which is installed automatically along with the C-GLASS binary using CMake.\
    \ The configurator makes it easy to add new parameters to the simulation without\
    \ mucking around in the source code. Just add your new parameter to <code>config/default_config.yaml</code>\
    \ file using the following format:</p>\n<pre><code>new_parameter_name: [default_parameter_value,\
    \ parameter_type] \n</code></pre>\n<p>Then run the configurator using</p>\n<pre><code>./configure_cglass.exe\
    \ config/default_config.yaml\n</code></pre>\n<p>Running configure_cglass.exe will\
    \ look at all the parameters in the default config file and add them seamlessly\
    \ to the proper C-GLASS headers, and you can begin using them after recompiling\
    \ C-GLASS using CMake.</p>\n<h3>\n<a id=\"user-content-parameter-sets\" class=\"\
    anchor\" href=\"#parameter-sets\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Parameter sets</h3>\n<p>Using parameter\
    \ sets, it becomes easier to run many simulations over a given parameter space.\
    \ There are two types of parameter sets possible with C-GLASS: defined and random.\
    \ Each parameter set type works the same with both global parameters and species\
    \ parameters.</p>\n<h4>\n<a id=\"user-content-defined-parameter-sets\" class=\"\
    anchor\" href=\"#defined-parameter-sets\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Defined parameter sets</h4>\n\
    <p>Defined parameter sets are specified by the <code>V</code> prefix in the parameter\
    \ file:</p>\n<pre><code>seed: 4916819461895\nrun_name: defined_set\nn_runs: N\n\
    parameter_name1: param_value1\nparameter_name2: [V, param_value2, param_value3]\n\
    parameter_name3: [V, param_value4, param_value5]\n</code></pre>\n<p>Parameters\
    \ specified in this way (as lists of parameters) will be iterated over until every\
    \ possible combination of parameters has been run. In this example, C-GLASS will\
    \ run N simulations each of the following 4 parameter sets:</p>\n<pre><code>seed:\
    \ random_seed_1\nrun_name: defined_set_v000\nn_runs: N\nparameter_name1: param_value1\n\
    parameter_name2: param_value2\nparameter_name3: param_value4\n\nseed: random_seed_2\n\
    run_name: defined_set_v001\nn_runs: N\nparameter_name1: param_value1\nparameter_name2:\
    \ param_value2\nparameter_name3: param_value5\n\nseed: random_seed_3\nrun_name:\
    \ defined_set_v002\nn_runs: N\nparameter_name1: param_value1\nparameter_name2:\
    \ param_value3\nparameter_name3: param_value4\n\nseed: random_seed_4\nrun_name:\
    \ defined_set_v003\nn_runs: N\nparameter_name1: param_value1\nparameter_name2:\
    \ param_value3\nparameter_name3: param_value5\n</code></pre>\n<h4>\n<a id=\"user-content-random-parameter-sets\"\
    \ class=\"anchor\" href=\"#random-parameter-sets\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Random parameter\
    \ sets</h4>\n<p>Random parameter sets are designed specifically to be used with\
    \ polynomial-chaos theory for n-dimensional parameter spaces for large n. Random\
    \ sets are used in the following way:</p>\n<pre><code>seed: 2546954828254\nn_runs:\
    \ N\nn_random: M\nparameter_name1: param_value1\nparameter_name2: [R, A, B] #\
    \ sets to random real in range (A,B)\nparameter_name3: [RINT, C, D] # sets to\
    \ random int in range [C,D]\nparameter_name4: [RLOG, F, G] # sets to 10^K for\
    \ rand real K in range (F,G)\n</code></pre>\n<p>Given this parameter file, C-GLASS\
    \ will run N simulations each of M random parameter sets. The random parameter\
    \ sets are generated in ranges specified in the lists that are prefixed by the\
    \ R, RINT, RLOG options.</p>\n<p>In this example, the sampled parameter space\
    \ has dimensionality of n=3, since there are only three parameters we are sampling\
    \ over. Each parameter set will have a random real number for parameter_name2\
    \ in the the range (A,B), a random integer in the range [C,D] for parameter_name3,\
    \ and will set parameter_name4 to 10^K for random real number K in the range (F,G).\
    \  C-GLASS will then run each parameter set N times each with a unique seed, and\
    \ repeat this random process M times. It will therefore take N samples of M random\
    \ points in the n-dimensional parameter space.</p>\n<h3>\n<a id=\"user-content-interactions\"\
    \ class=\"anchor\" href=\"#interactions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Interactions</h3>\n<p>The Interaction\
    \ Manager in C-GLASS was written with short-range interactions in mind. For this\
    \ reason, interactions are treated by considering pair-wise interactions between\
    \ neighboring interactor-elements that make up a composite object (e.g. small,\
    \ rigid segments that compose a flexible filament). For this reason, interactions\
    \ use cell lists to improve performance. Furthermore, simulating large objects\
    \ in C-GLASS requires representing the object as a composite of smaller, simple\
    \ objects. An example of how a large object should be decomposed into simple objects\
    \ is done in the Filament class.</p>\n<h3>\n<a id=\"user-content-potentials\"\
    \ class=\"anchor\" href=\"#potentials\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Potentials</h3>\n<p>C-GLASS is\
    \ designed to be able to use interchangable potentials for various objects. However,\
    \ potentials need to be added manually as a subclass of PotentialBase, included\
    \ in PotentialManager, and a corresponding potential_type added to definitions.h\
    \ for lookup purposes (see the InitPotentials method in PotentialManager.h for\
    \ examples).</p>\n<h3>\n<a id=\"user-content-outputs\" class=\"anchor\" href=\"\
    #outputs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Outputs</h3>\n<p>C-GLASS has four output types. Three are species\
    \ specific (posit, spec, checkpoint), and the fourth is the statistical information\
    \ file (thermo). All files are written in binary.</p>\n<p>The posit file has the\
    \ following header format:</p>\n<pre><code>int n_steps, int n_posit, double delta\
    \ \n</code></pre>\n<p>Followed by n_steps/n_posit lines of data with the format:</p>\n\
    <pre><code>double position[3]\ndouble scaled_position[3]\ndouble orientation[3]\n\
    double diameter\ndouble length\n</code></pre>\n<p>Where the scaled position is\
    \ position mapped into the periodic coordinate space. The position itself gives\
    \ the particle trajectory over time independent of periodicity.</p>\n<p>The spec\
    \ file is a custom output file for each species, and can have the same information\
    \ as the posit file or additional information if needed.</p>\n<p>The checkpoint\
    \ file is almost a copy of the spec file, except it also contains the random number\
    \ generator information and is overwritten every n_checkpoint steps in the simulation.\
    \ It can therefore be used to resume a simulation that ended prematurely.</p>\n\
    <p>The thermo file contains the following header information:</p>\n<pre><code>int\
    \ n_steps, int n_thermo, double delta, int n_dim\n</code></pre>\n<p>followed by\
    \ n_steps/n_thermo lines of data in the following format:</p>\n<pre><code>double\
    \ unit_cell[9]\ndouble pressure_tensor[9]\ndouble pressure\ndouble volume\n</code></pre>\n\
    <p>Where the pressure is the isometric pressure, and the pressure tensor is calculated\
    \ from the time-averaged stress tensor.</p>\n<h3>\n<a id=\"user-content-data-analysis\"\
    \ class=\"anchor\" href=\"#data-analysis\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Data analysis</h3>\n<p>If analysis\
    \ operations of output files are already defined for your species, as is the case\
    \ for the Filament species, analyzing outputs is a simple matter. First, make\
    \ sure the desired analysis flag is set in the species parameters for that species.</p>\n\
    <p>For example, in the Filament species there is a persistence length analysis\
    \ that produces .mse2e files that tracks the mean-square end-to-end distance of\
    \ semiflexible filaments. This is triggered by a parameter lp_analysis=1, which\
    \ can be set in the parameter file.</p>\n<p>Anaylses are run by running C-GLASS\
    \ in the following way:</p>\n<pre><code>cglass.exe -a parameter_file.yaml.\n</code></pre>\n\
    <p>NOTE: It is important to keep in mind that the parameter_file should be identical\
    \ to the parameter file used to generate the outputs. There are a few exceptions\
    \ that only affect post-processing, such as analysis flags, but this is true in\
    \ general.</p>\n<p>The way inputs and outputs are meant to work in C-GLASS is\
    \ such that during a simulation, output data are generated in the posit, spec,\
    \ and checkpoint formats, and during analysis, the same output data are read back\
    \ into the data structures in C-GLASS for processing. The .posit files just contain\
    \ bare-bones information that allow many types of simple analyses, but .spec files\
    \ should in general contain all the necessary information to recreate the trajectory\
    \ for a member of a species.</p>\n<p>For a new species analysis method, the analysis\
    \ routines should be defined in the species container class, rather than the species\
    \ member class, and called by the inherited RunAnalysis method of the SpeciesBase\
    \ class (and likewise for analysis initialization and finalization, see examples\
    \ for details).</p>\n<p>For example, the RunSpiralAnalysis routine is called by\
    \ the RunAnalysis method in FilamentSpecies, which uses the Filament .spec file\
    \ as an input to do the necessary analysis, whose results are placed into a new\
    \ file ending in filament.spiral. See Filament and FilamentSpecies for examples\
    \ of how analyses can be initialized, processed, etc.</p>\n<h2>\n<a id=\"user-content-directory-structure\"\
    \ class=\"anchor\" href=\"#directory-structure\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Directory structure</h2>\n<p>The\
    \ directory structure is as follows:</p>\n<pre><code>C-GLASS\n\u251C\u2500\u2500\
    \ include\n\u2502   \u2514\u2500\u2500 cglass\n\u2502       \u2514\u2500\u2500\
    \ (header files)\n\u251C\u2500\u2500 src\n\u2502   \u251C\u2500\u2500 CMakeLists.txt\n\
    \u2502   \u251C\u2500\u2500 executable\n\u2502   \u2502   \u251C\u2500\u2500 CMakeLists.txt\n\
    \u2502   \u2502   \u2514\u2500\u2500 cglass_main.cpp\n\u2502   \u251C\u2500\u2500\
    \ configurator\n\u2502   \u2502   \u251C\u2500\u2500 CMakeLists.txt\n\u2502  \
    \ \u2502   \u2514\u2500\u2500 configurator.cpp\n\u2502   \u2514\u2500\u2500 (source\
    \ files)\n\u251C\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 default_config.yaml\n\
    \u251C\u2500\u2500 analysis\n\u2502   \u2514\u2500\u2500 (Python analysis files)\n\
    \u251C\u2500\u2500 scripts\n\u2502   \u2514\u2500\u2500 (utility files)\n\u251C\
    \u2500\u2500 examples\n\u2502   \u2514\u2500\u2500 (parameter file examples)\n\
    \u251C\u2500\u2500 docker\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251C\u2500\
    \u2500 extern\n\u2502   \u2514\u2500\u2500 KMC\n\u251C\u2500\u2500 tests\n\u2502\
    \   \u251C\u2500\u2500 CMakeLists.txt\n\u2502   \u251C\u2500\u2500 catch2\n\u2502\
    \   \u2502   \u2514\u2500\u2500 catch.hpp\n\u2502   \u2514\u2500\u2500 (C-GLASS\
    \ unit tests)\n\u251C\u2500\u2500 docs\n\u2502   \u251C\u2500\u2500 CMakeLists.txt\n\
    \u2502   \u2514\u2500\u2500 main.md\n\u251C\u2500\u2500 figs\n\u2502   \u2514\u2500\
    \u2500 (example simulation figures)\n\u251C\u2500\u2500 README.md\n\u251C\u2500\
    \u2500 LICENSE\n\u251C\u2500\u2500 CMakeLists.txt\n\u251C\u2500\u2500 install.sh\n\
    \u251C\u2500\u2500 launch_docker.sh\n\u251C\u2500\u2500 .travis.yml\n\u2514\u2500\
    \u2500 .gitignore\n</code></pre>\n<h2>\n<a id=\"user-content-about-c-glass\" class=\"\
    anchor\" href=\"#about-c-glass\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>About C-GLASS</h2>\n<p>C-GLASS is\
    \ written in C++ and designed for general coarse-grained physics simulations of\
    \ active living matter, produced with modularity and scalability in mind. All\
    \ objects in the simulation are representable as a composite of what I call \"\
    simple\" objects (points, spheres, rigid cylinders, and 2d polygon surfaces would\
    \ all qualify). For short-range interactions, C-GLASS uses cell and neighbor lists\
    \ for improved performance and OpenMP for parallelization.</p>\n<h2>\n<a id=\"\
    user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>This software is licensed under the terms of the BSD-3 Clause license. See\
    \ the <code>LICENSE</code> for more details.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1613508797.0
BiomedicalMachineLearning/HEMnet:
  data_format: 2
  description: 'A neural network software for using Molecular labelling to improve
    pathological annotation of H and E tissues '
  filenames:
  - Environments/Singularity.cpu
  full_name: BiomedicalMachineLearning/HEMnet
  latest_release: v1.0.0
  readme: "<p><a href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\"\
    \ alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HEMnet - Haematoxylin &amp; Eosin and Molecular neural network</h1>\n\
    <h2>\n<a id=\"user-content-description\" class=\"anchor\" href=\"#description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Description</h2>\n<p>A deep learning automated cancer diagnosis software\
    \ using molecular labelling to improve pathological annotation of\nHaematoxylin\
    \ and Eosin (H&amp;E) stained tissue.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<ol>\n<li>\n\
    <p>Docker</p>\n<p>You can download and run the docker image using the following\
    \ commands:</p>\n<pre><code>docker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\
    </code></pre>\n</li>\n<li>\n<p>Conda</p>\n<p>Install Openslide (this is necessary\
    \ to open whole slide images) - download it <a href=\"https://openslide.org/download/\"\
    \ rel=\"nofollow\">here</a></p>\n<p>Create a conda environment from the <code>environment.yml</code>\
    \ file</p>\n<pre><code>conda env create -f environment.yml\nconda activate HEMnet\n\
    </code></pre>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-slide-preparation\"\
    \ class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Slide Preparation</h3>\n<p>Name\
    \ slides in the format: <code>slide_id_TP53</code> for TP53 slides and <code>slide_id_HandE</code>\
    \ for H&amp;E slides\nThe <code>TP53</code> and <code>HandE</code> suffix is used\
    \ by HEMnet to identify the stain used.</p>\n<h3>\n<a id=\"user-content-1-generate-training-and-testing-datasets\"\
    \ class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.\
    \ Generate training and testing datasets</h3>\n<p>a. Generate train dataset</p>\n\
    <p><code>python HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides\
    \ -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs\
    \ -v</code></p>\n<p>b. Generate test dataset</p>\n<p><code>python HEMnet_test_dataset.py\
    \ -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory\
    \ -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh\
    \ -n non_cancer_thresh</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is the relative path to the template slide from which all other slides will\
    \ be normalised against. The template\nslide should be the same for each step.</li>\n\
    <li>\n<code>-m</code> is the tile magnification. e.g. if  the input is <code>10</code>\
    \ then the tiles will be output at 10x</li>\n<li>\n<code>-a</code> is the align\
    \ magnification. Paired TP53 and H&amp;E slides will be registered at this magnification.\n\
    To reduce computation time we recommend this be less than the tile magnification\
    \ - a five times downscale generally works well.</li>\n<li>\n<code>-c</code> cancer\
    \ threshold to apply to the DAB channel. DAB intensities less than this threshold\
    \ indicate cancer.</li>\n<li>\n<code>-n</code> non-cancer threshold to apply to\
    \ the DAB channel. DAB intensities greater than this threshold indicate no cancer.</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\"\
    \ href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2. Train and evaluate model</h3>\n\
    <p>a. Training model</p>\n<p><code>python train.py -b /path/to/base/directory\
    \ -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory\
    \ -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size\
    \ -s -w -f -v</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-m</code>\
    \ is CNN base model. eg. <code>resnet50</code>, <code>vgg16</code>, <code>vgg19</code>,\
    \ <code>inception_v3</code> and <code>xception</code>.</li>\n<li>\n<code>-g</code>\
    \ is number of GPUs for training.</li>\n<li>\n<code>-e</code> is training epochs.\
    \ Default is <code>100</code> epochs.</li>\n<li>\n<code>-a</code> is batch size.\
    \ Default is <code>32</code>\n</li>\n<li>\n<code>-s</code> is option to save the\
    \ trained model weights.</li>\n<li>\n<code>-w</code> is option to used transfer\
    \ learning. Model will used pre-trained weights from ImageNet at the initial stage.</li>\n\
    <li>\n<code>-f</code> is fine-tuning option. Model will re-train CNN base.</li>\n\
    </ul>\n<p>b. Test model prediction</p>\n<p><code>python test.py  -b /path/to/base/directory\
    \ -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory\
    \ -w model_weights -m cnn_base -g num_gpus -v</code></p>\n<p>Other parameters:</p>\n\
    <ul>\n<li>\n<code>-w</code> is path to trained model. eg. <code>trained_model.h5</code>.</li>\n\
    <li>\n<code>-m</code> is CNN base model (same to training step).</li>\n<li>\n\
    <code>-g</code> is number of GPUs for prediction.</li>\n</ul>\n<p>c. Evaluate\
    \ model performance and visualise model prediction</p>\n<p><code>python visualisation.py\
    \ -b /path/to/base/directory -t /relative/path/to/training_output_directory -p\
    \ /relative/path/to/test_output_directory  -o /relative/path/to/output/directory\
    \ -i sample</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is path to training outputs.</li>\n<li>\n<code>-p</code> is path to test outputs.</li>\n\
    <li>\n<code>-i</code> is name of Whole Slide Image for visualisation.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\"\
    \ href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3. Apply model to diagnose new\
    \ images</h3>\n<p><code>python HEMnet_inference.py -s '/path/to/new/HE/Slides/'\
    \ -o '/path/to/output/directory/' -t '/path/to/template/slide/' -nn '/path/to/trained/model/'\
    \ -v</code></p>\n<p>Predict on TCGA images with our pretrained model for colorectal\
    \ cancer using <a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\">google colab</a></p>\n<h2>\n<a id=\"user-content-results\"\
    \ class=\"anchor\" href=\"#results\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Results</h2>\n<h2>\n<a id=\"\
    user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citing\
    \ HEMnet</h2>\n<h2>\n<a id=\"user-content-the-team\" class=\"anchor\" href=\"\
    #the-team\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>The Team</h2>\n<p>Please contact Dr Quan Nguyen (<a href=\"mailto:quan.nguyen@uq.edu.au\"\
    >quan.nguyen@uq.edu.au</a>), Andrew Su (<a href=\"mailto:a.su@uqconnect.edu.au\"\
    >a.su@uqconnect.edu.au</a>),\nand Xiao Tan (<a href=\"mailto:xiao.tan@uqconnect.edu.au\"\
    >xiao.tan@uqconnect.edu.au</a>) for issues, suggestions,\nand we are very welcome\
    \ to collaboration opportunities.</p>\n"
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1619292330.0
CAIsr/qsm:
  data_format: 2
  description: This docker and singularity image bundles the tgv-qsm algorithm with
    bet2, dcm2niix and provides a complete QSM processing pipeline.
  filenames:
  - Singularity.tgvqsm
  - Singularity.tgvqsm_amd
  full_name: CAIsr/qsm
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-qsm-pipeline\" class=\"anchor\" href=\"#qsm-pipeline\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>QSM Pipeline</h1>\n<p>This docker and singularity image provides the\
    \ tgv-qsm algorithm (<a href=\"http://www.neuroimaging.at/pages/qsm.php\" rel=\"\
    nofollow\">http://www.neuroimaging.at/pages/qsm.php</a>).</p>\n<p>If you use this\
    \ image, this is the reference to cite describing the QSM algorithm:\nLangkammer,\
    \ C; Bredies, K; Poser, BA; Barth, M; Reishofer, G; Fan, AP; Bilgic, B; Fazekas,\
    \ F; Mainero; C; Ropele, S\nFast Quantitative Susceptibility Mapping using 3D\
    \ EPI and Total Generalized Variation.\nNeuroimage. 2015 May 1;111:622-30. doi:\
    \ 10.1016/j.neuroimage.2015.02.041. PubMed</p>\n<h1>\n<a id=\"user-content-if-you-are-looking-for-a-full-qsm-pipeline-including-dicom-conversion-qsm-solution-image-segmentation-atlas-building\"\
    \ class=\"anchor\" href=\"#if-you-are-looking-for-a-full-qsm-pipeline-including-dicom-conversion-qsm-solution-image-segmentation-atlas-building\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>If you are looking for a full QSM pipeline including dicom conversion,\
    \ QSM solution, image segmentation, atlas building</h1>\n<p><a href=\"https://github.com/QSMxT/QSMxT\"\
    >https://github.com/QSMxT/QSMxT</a></p>\n<h1>\n<a id=\"user-content-using-the-image-in-singularity\"\
    \ class=\"anchor\" href=\"#using-the-image-in-singularity\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using the\
    \ image in singularity</h1>\n<p>installing singularity will depend on your operating\
    \ system, here an exampe for a debian based system</p>\n<pre><code>sudo apt-get\
    \ update &amp;&amp; sudo apt-get install -y \\\n    build-essential \\\n    uuid-dev\
    \ \\\n    libgpgme-dev \\\n    squashfs-tools \\\n    libseccomp-dev \\\n    wget\
    \ \\\n    pkg-config \\\n    git \\\n    cryptsetup-bin\n\nwget https://golang.org/dl/go1.15.2.linux-amd64.tar.gz\n\
    \ntar -C /usr/local -xzf go1.15.2.linux-amd64.tar.gz\n\nexport PATH=$PATH:/usr/local/go/bin\n\
    \nexport VERSION=3.6.3 &amp;&amp; # adjust this as necessary \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz\
    \ &amp;&amp; \\\n    tar -xzf singularity-${VERSION}.tar.gz &amp;&amp; \\\n  \
    \  cd singularity\n\n\n./mconfig &amp;&amp; \\\n    make -C ./builddir &amp;&amp;\
    \ \\\n    sudo make -C ./builddir install\n\n</code></pre>\n<p>then you can download\
    \ and run the container:</p>\n<pre><code>git clone https://github.com/NeuroDesk/transparent-singularity\
    \ tgvqsm_1.0.0_20210317\ncd tgvqsm_1.0.0_20210317\n./run_transparent_singularity.sh\
    \ tgvqsm_1.0.0_20210317\n</code></pre>\n<p>this will download the image, unpack\
    \ it and provide a wrapper script for starting tgv_qsm:</p>\n<p>The wrapper script\
    \ can be started using</p>\n<pre><code>./tgv_qsm\n\n</code></pre>\n<p>Or you can\
    \ open a shell into the container:</p>\n<pre><code> singularity shell tgvqsm_1.0.0_20210317.*\n\
    </code></pre>\n<p>you can also bind a different directory to your image (e.g.\
    \ bind /data from your host to /data in your singularity image)</p>\n<pre><code>singularity\
    \ shell --bind /data:/data/ tgvqsm_1.0.0_20210317.*\n</code></pre>\n<p>Here is\
    \ an example for a single echo QSM processing:</p>\n<pre><code>dcm2niix -o ./\
    \ -f magnitude GR_M_5_QSM_p2_1mmIso_TE20/\ndcm2niix -o ./ -f phase GR_P_6_QSM_p2_1mmIso_TE20/\n\
    \nbet2 magnitude.nii magnitude_bet2\n\ntgv_qsm \\\n  -p phase.nii \\\n  -m magnitude_bet2_mask.nii.gz\
    \ \\\n  -f 2.89 \\\n  -t 0.02 \\\n  -s \\\n  -o qsm\n</code></pre>\n<p>The -s\
    \ option will scale the phase correctly if the phase dicom values are between\
    \ -2048 and 2048 (should be default on Siemens VD and VE platforms). On the VB\
    \ platform the phase is between 0 and 4096, so omit the -s option and scale the\
    \ phase between -pi and pi:</p>\n<h1>\n<a id=\"user-content-using-the-image-in-docker\"\
    \ class=\"anchor\" href=\"#using-the-image-in-docker\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using the image\
    \ in docker</h1>\n<pre><code>docker pull vnmd/tgvqsm_1.0.0:20210317\nsudo docker\
    \ run -it -v $PWD:/data vnmd/tgvqsm_1.0.0:20210317\n\ncd /data\ndcm2niix -o ./\
    \ -f magnitude GR_M_5_QSM_p2_1mmIso_TE20/\ndcm2niix -o ./ -f phase GR_P_6_QSM_p2_1mmIso_TE20/\n\
    \nbet2 magnitude.nii magnitude_bet2\n\ntgv_qsm -p phase.nii -m magnitude_bet2_mask.nii.gz\
    \ -f 2.89 -t 0.02 -s -o qsm\n</code></pre>\n<h1>\n<a id=\"user-content-optimizing-for-your-cpu\"\
    \ class=\"anchor\" href=\"#optimizing-for-your-cpu\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Optimizing for\
    \ your CPU</h1>\n<p>By default, QSM is compiled with the <code>-O3 -march=x86-64</code>\
    \ which should provide a good balance between speed and portability. If you know\
    \ what CPU you're going to be using you can compile with that instruction set\
    \ to improve performance (e.g. <code>-march=ivybridge</code> for Intel Ivy Bridge\
    \ CPUs, <code>-march=native</code> for whatever CPU you're currently on). If you\
    \ would like maximum portability, you can recompile omitting the <code>-march</code>\
    \ flag altogether.</p>\n<h1>\n<a id=\"user-content-using-tgv_qsm-in-windows-subsystem-for-linux-example-debian-based-system\"\
    \ class=\"anchor\" href=\"#using-tgv_qsm-in-windows-subsystem-for-linux-example-debian-based-system\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using tgv_qsm in Windows Subsystem for Linux (example: Debian based\
    \ system)</h1>\n<p>WSL 1.0 doesn't support singularity or docker containers (but\
    \ WSL 2.0 will). But it is possible to directly install TGV QSM in a miniconda\
    \ environment:</p>\n<pre><code>sudo apt install wget unzip gcc\nwget https://repo.anaconda.com/miniconda/Miniconda2-4.6.14-Linux-x86_64.sh\n\
    bash Miniconda2-4.6.14-Linux-x86_64.sh\n(install, accept agreement with yes, after\
    \ install source bash again:)\nbash\nconda install -c anaconda cython==0.25.2\n\
    conda install numpy\nconda install pyparsing\n(make sure pip is not your system\
    \ pip, but the one in miniconda: which pip)\npip install scipy==0.17.1 nibabel==2.1.0\n\
    wget http://www.neuroimaging.at/media/qsm/TGVQSM-plus.zip\nunzip TGVQSM-plus.zip\n\
    cd TGVQSM-master-011045626121baa8bfdd6633929974c732ae35e3\npython setup.py install\n\
    cd test_data\ntgv_qsm  -p epi3d_test_phase.nii.gz -m epi3d_test_mask.nii.gz -f\
    \ 2.89 -t 0.027 -o epi3d_test_QSM\n</code></pre>\n<h1>\n<a id=\"user-content-adding-fsl-to-wsl-ubuntu-1804\"\
    \ class=\"anchor\" href=\"#adding-fsl-to-wsl-ubuntu-1804\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Adding fsl\
    \ to WSL Ubuntu 18.04</h1>\n<pre><code>wget -O- http://neuro.debian.net/lists/bionic.us-ca.full\
    \ | sudo tee /etc/apt/sources.list.d/neurodebian.sources.list\nsudo apt-key adv\
    \ --recv-keys --keyserver hkp://pool.sks-keyservers.net:80 0xA5D32F012649A5A9\n\
    sudo apt-get update\nsudo apt-get install fsl-5.0-core\n</code></pre>\n<p>add\
    \ \". /etc/fsl/5.0/fsl.sh\" to the end of your .profile file</p>\n"
  stargazers_count: 6
  subscribers_count: 4
  topics: []
  updated_at: 1615972979.0
CN-Healthborn/el7tf1.12gpu:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: CN-Healthborn/el7tf1.12gpu
  latest_release: null
  readme: '<h1>

    <a id="user-content-nova-el7-tensorflow-gpu" class="anchor" href="#nova-el7-tensorflow-gpu"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nova-el7-tensorflow-gpu</h1>

    <p>Configurations for docker and singularity for making OSG-compatible CENTOS7
    container with GPU-accelerated tensorflow and keras installed.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1603475388.0
CWRU-MSL/GammaDoublePrime:
  data_format: 2
  description: Python repository of code for preprocessing and extracting metrics
    of the volume fraction and size of the gamma double prime and gamma prime in a
    nickel-based superalloy microstructure
  filenames:
  - Singularity
  full_name: CWRU-MSL/GammaDoublePrime
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterization-of-nanoscale-precipitates-in-superalloy-718-using-high-resolution-sem-imaging\"\
    \ class=\"anchor\" href=\"#characterization-of-nanoscale-precipitates-in-superalloy-718-using-high-resolution-sem-imaging\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Characterization of nanoscale precipitates in superalloy 718 using\
    \ high resolution SEM imaging</h1>\n<h2>\n<a id=\"user-content-tm-smith-a-nm-senanayake-b-ck-sudbrack-c-p-bonacuse-a-rb-rogers-a-p-chao-d-j-carter-b\"\
    \ class=\"anchor\" href=\"#tm-smith-a-nm-senanayake-b-ck-sudbrack-c-p-bonacuse-a-rb-rogers-a-p-chao-d-j-carter-b\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>T.M. Smith a*, N.M. Senanayake b, C.K. Sudbrack c, P. Bonacuse a,\
    \ R.B. Rogers a, P. Chao d, J. Carter b</h2>\n<p>a NASA Glenn Research Center,\
    \ Materials and Structures Division, Cleveland, OH 44135, United States of America\n\
    b Case Western Reserve University, Department of Materials Science and Engineering,\
    \ Cleveland, OH 44106, United States of America\nc QuesTek Innovations LLC, Evanston,\
    \ IL 60201, United States of America\nd Carnegie Mellon University, Department\
    \ of Materials Science and Engineering, Pittsburgh, PA 15213, United States of\
    \ America</p>\n<h2>\n<a id=\"user-content-materials-characterization-212019-v148-p-178-197\"\
    \ class=\"anchor\" href=\"#materials-characterization-212019-v148-p-178-197\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Materials Characterization, (2/1/2019) V148, p 178-197</h2>\n<h2>\n\
    <a id=\"user-content-httpwwwsciencedirectcomsciencearticlepiis1044580318328444\"\
    \ class=\"anchor\" href=\"#httpwwwsciencedirectcomsciencearticlepiis1044580318328444\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"http://www.sciencedirect.com/science/article/pii/S1044580318328444\"\
    \ rel=\"nofollow\">http://www.sciencedirect.com/science/article/pii/S1044580318328444</a>\n\
    </h2>\n<h2>\n<a id=\"user-content-doi-101016jmatchar201812018\" class=\"anchor\"\
    \ href=\"#doi-101016jmatchar201812018\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>DOI: 10.1016/j.matchar.2018.12.018</h2>\n\
    <h3>\n<a id=\"user-content-repo-information\" class=\"anchor\" href=\"#repo-information\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Repo Information</h3>\n<p>This repository contains the codes necessary\
    \ to utilize the algorthims presented in the paper below. When implimented, the\
    \ can be used to obtain accurate volume fraction and size measurements of gamma\
    \ double prime, and gamma prime, precipitates in Superalloy</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-clone-a-repository\" class=\"anchor\" href=\"#clone-a-repository\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Clone a repository</h2>\n<p>Use these steps to clone from SourceTree,\
    \ our client for using the repository command-line free. Cloning allows you to\
    \ work on your files locally. If you don't yet have SourceTree, <a href=\"https://www.sourcetreeapp.com/\"\
    \ rel=\"nofollow\">download and install first</a>. If you prefer to clone from\
    \ the command line, see <a href=\"https://confluence.atlassian.com/x/4whODQ\"\
    \ rel=\"nofollow\">Clone a repository</a>.</p>\n<ol>\n<li>You\u2019ll see the\
    \ clone button under the <strong>Source</strong> heading. Click that button.</li>\n\
    <li>Now click <strong>Check out in SourceTree</strong>. You may need to create\
    \ a SourceTree account or log in.</li>\n<li>When you see the <strong>Clone New</strong>\
    \ dialog in SourceTree, update the destination path and name if you\u2019d like\
    \ to and then click <strong>Clone</strong>.</li>\n<li>Open the directory you just\
    \ created to see your repository\u2019s files.</li>\n</ol>\n<p>Now that you're\
    \ more familiar with your Bitbucket repository, go ahead and add a new file locally.\
    \ You can <a href=\"https://confluence.atlassian.com/x/iqyBMg\" rel=\"nofollow\"\
    >push your change back to Bitbucket with SourceTree</a>, or you can <a href=\"\
    https://confluence.atlassian.com/x/8QhODQ\" rel=\"nofollow\">add, commit,</a>\
    \ and <a href=\"https://confluence.atlassian.com/x/NQ0zDQ\" rel=\"nofollow\">push\
    \ from the command line</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1613426965.0
Characterisation-Virtual-Laboratory/CharacterisationVL-Software:
  data_format: 2
  description: null
  filenames:
  - colmap/Singularity.colmap_3.6-dev.3
  - colmap/Singularity.colmap_3.5
  - cryolo/Singularity.cryolo_v1_0_4
  - cryolo/Singularity.cryolo_v1_0_0
  - cryolo/Singularity.cryolo_v1_6_1
  - cryolo/Singularity.cryolo_v1_5_6
  - mydata-python/Singularity.mydata-python_20200603
  - jupyter-ml/Singularity.jupyter-ml_20201120
  - jupyter-ml/Singularity.jupyter-ml_20210415
  - ants/Singularity.ants_2.3.4
  - ants/Singularity.ants_2.3.1
  - apex/Singularity.apex_master
  - pyprismatic/Singularity.pyprismatic_1_2_1-cuda-11.0
  - openrefine/Singularity.openrefine-3.1
  - ubuntu-base-image/Singularity.2004
  - ubuntu-base-image/Singularity.1804
  - ubuntu-base-image/Singularity.1804-cuda10.1
  - ubuntu-base-image/Singularity.2004-cuda11.0
  - ubuntu-base-image/Singularity.1804-cuda9
  - darknet/Singularity.darknet_yolo_v3-cuda-9.0
  - mrtrix/Singularity.mrtrix_3_beta
  - bidscoin/Singularity.bidscoin_3
  - mantid/Singularity.mantid_v_3_13_0
  - R/Singularity.R_4.0.5
  - dragondisk/Singularity.dragondisk_v1_0_5
  - omero-insight/Singularity.1804
  - dristhi/Singularity.dristhi_2.6.4
  - fiji/Singularity.fiji
  - paraview/Singularity.paraview_5.6.0-cuda-9.0
  - imagemagick/Singularity.imagemagick-7.0.8-68
  - anaconda3/Singularity.anaconda3_5.3.0-cuda-11.0.3
  - anaconda3/Singularity.anaconda3_5.3.0
  - cvmfs-client/Singularity.cvmfs-client
  - cloudstor/Singularity.cloudstor-2.4.1
  - imblproc/Singularity.imblproc
  - quit/Singularity.quit_2.0.2
  - gimp/Singularity.gimp_2.8.22
  - gimp/Singularity.gimp_2.8
  - libertem/Singularity.libertem-v0.5.1
  - libertem/Singularity.libertem-v0.2.2
  - libertem/Singularity.libertem-v0.6.0
  - libertem/Singularity.libertem-21-May-2019
  - libertem/Singularity.libertem-v0.4.1
  - libertem/Singularity.libertem-v0.4.0
  - libertem/Singularity.libertem-v0.5.0
  - globus-cli/Singularity.globus-cli-v2.0.0
  - meshlab/Singularity.meshlab-2019.03-cuda-9.0
  - amide/Singularity.amide-1.0.5
  - cellprofiler/Singularity.cellprofiler_3.1.9
  - cellprofiler/Singularity.cellprofiler_3.1.5
  - mydata/Singularity.mydata_0.9.2-1
  - datalad/Singularity.datalad_0.13.3
  - openmodelica/Singularity.openmodelica_1.14.2-cuda-10.1
  - bids-validator/Singularity.bids-validator-1.2.2
  - bids-validator/Singularity.bids-validator-1.3.1
  - caffe-unet/Singularity.caffe-unet_1.0
  - imagej/Singularity.imagej_1.50e
  - fsl/Singularity.fsl
  - globus-connect-personal/Singularity.globus-connect-personal_latest
  - ariba/Singularity.ariba_2.12.1
  - ariba/Singularity.ariba_2.14.4
  - atom/Singularity.atom_1.45.0
  - atom/Singularity.atom_1.39.1
  - ashs/Singularity.ashs_2.0.0
  - mango/Singularity.mango_4.0.1
  - eman/Singularity.eman_2.3.1
  - eman/Singularity.eman_2.91
  - eman/Singularity.eman_2.3
  - eman/Singularity.eman_2.9
  - eman/Singularity.eman_2.22
  - 3dslicer/Singularity.3dslicer_4.8.1
  - 3dslicer/Singularity.3dslicer_4.10.2
  - haystack/Singularity.haystack_bio_v0_5_0
  - cistem/Singularity.cisTEM-1.0.0-beta
  - deeplabcut/Singularity.latest
  - git-annex/Singularity.git-annex.6.20180227
  - mrtrix3tissue/Singularity.mrtrix3tissue-5.2.8
  - volview/Singularity.VolView_3.4-cuda-9.0
  - imod/Singularity.imod_v4_9_9
  - octopus/Singularity.octopus_8.4
  - octopus/Singularity.octopus_8.4_parallel
  - octave/Singularity.octave-4.2.2
  - caffe/Singularity.caffe_1.0
  - omero.insight/Singularity.omero_5.5.10
  - connectome-workbench/Singularity.connectome-workbench_1.4.2
  - ilastik/Singularity.ilastik_1.3.3post3
  - graphviz/Singularity.graphviz-2.40.1
  - crisprcas/Singularity.crisprcas
  - matlab/Singularity.MATLAB_SAMPLE
  - argos/Singularity.argos_3.0.0-beta53
  - argos/Singularity.argos_3.0.0-beta52
  - cytoscape/Singularity.cytoscape_3.8.0
  full_name: Characterisation-Virtual-Laboratory/CharacterisationVL-Software
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterisationvl-software\" class=\"anchor\"\
    \ href=\"#characterisationvl-software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CharacterisationVL-Software</h1>\n\
    <p>The purpose of this repository is for storing definition files to submit to\
    \ <a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity Hub.</a></p>\n\
    <p>If you are new to Singularity containers, please refer to <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.5/user-guide/</a> or a newer version\
    \ of this documentation.</p>\n<p>Each software package is located in its own folder.\
    \ The files are tagged with the software name and version number or date of build.\
    \ Please read below for the naming convention.</p>\n<p>To add software to the\
    \ repository you will need to create a new branch. The new branch is the name\
    \ of the software product. By convention, the new branch will be checked and merged\
    \ into the master branch and then deleted.</p>\n<h2>\n<a id=\"user-content-steps-to-add-a-software-package\"\
    \ class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Steps to\
    \ add a software package</h2>\n<ol>\n<li>Clone this repository</li>\n<li>Create\
    \ a branch</li>\n</ol>\n<pre><code>$ git branch &lt;software name&gt;\n</code></pre>\n\
    <ol start=\"3\">\n<li>Make a subdirectory for the software product.</li>\n</ol>\n\
    <pre><code>$ mkdir &lt;software name&gt;\n</code></pre>\n<ol start=\"4\">\n<li>Add\
    \ all the necessary files.</li>\n</ol>\n<ul>\n<li>Singularity definition file\
    \ or installation script</li>\n<li>Readme file including install and testing notes</li>\n\
    <li>Desktop files for adding to menus with necessary tags</li>\n<li>For full details,\
    \ <a href=\"template/README.md\">please refer to the 'template' folder in this\
    \ repository.</a>\n</li>\n</ul>\n<ol start=\"4\">\n<li>Commit all changes, including\
    \ a helpful message</li>\n</ol>\n<pre><code>$ git commit -m \"&lt;software name&gt;\
    \ added as requested in support ticket\"\n</code></pre>\n<ol start=\"6\">\n<li>Push\
    \ to the remote repository. i.e. this one.</li>\n<li>Submit merge request</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Naming your Singularity definition file, Singularity Hub and Licensing</h2>\n\
    <p>For all Singularity recipes where the software licensing permits redistribution,\
    \ please use this naming convention:</p>\n<pre><code>   Singularity.applicationName_version\n\
    \   Singularity.applicationName_version-cuda-cudaVersion\n\n</code></pre>\n<p>This\
    \ is where Singularity Hub fits into the equation. There is a webhook between\
    \ this repository and <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >Singularity Hub</a>. When a commit is merged into the master branch, Singularity\
    \ Hub will build the container.</p>\n<p>If successfully built, the path to the\
    \ container on Singularity Hub is:</p>\n<pre><code>  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n\
    \  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\
    \n</code></pre>\n<p>For software where licensing does not support redistribution,\
    \ the container recipe can still be defined, but the container should not be built\
    \ on Singularity Hub.</p>\n<p>An example on how to handle this situation is the\
    \ recipe for CCP-EM.\nThe <a href=\"ccp-em/README.md\">README.md</a> contains\
    \ a section on Prerequisites. This section lists the required files to build the\
    \ container. The license must be accepted by the end user to obtain them.</p>\n\
    <p>Prerequisite files should not be committed to this repository.</p>\n<p>To prevent\
    \ Singularity Hub from attempting to build the container, we simply use a different\
    \ recipe naming convention as follows:</p>\n<pre><code>   applicationName_version.def\n\
    \   applicationName_version-cuda-cudaVersion.def\n\n</code></pre>\n<h2>\n<a id=\"\
    user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Ubuntu Base Images</h2>\n<p>The folder 'ubuntu-base-image' contains\
    \ recipes for pre built base containers. These can be used as a starting point\
    \ to aid/speed up the development of your container recipe.</p>\n<p>The current\
    \ versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.</p>\n\
    <p>These are available on Singularity Hub.</p>\n<p>For example: from the Graphviz\
    \ Singularity.graphviz-2.40.1 recipe</p>\n<pre><code>Bootstrap: shub\nFrom:  \
    \    Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n</code></pre>\n\
    <p>These two lines, will tell Singularity to use the 'shub' bootstrap to obtain\
    \ the '1804' ubuntu-base-image container from Singularity Hub.</p>\n<p>From here\
    \ you just need to add the requirements to build a container for your required\
    \ piece of software. Please see <a href=\"graphviz/Singularity.graphviz-2.40.1\"\
    >Singularity.graphviz-2.40.1</a>\nfor the full recipe.</p>\n<p>The current ubuntu-base-images\
    \ include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.</p>\n\
    <h2>\n<a id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"\
    anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ GUI applications on a non-GPU node</h2>\n<p>The applications in the Singularity\
    \ container should run without the need for a dedicated GPU.</p>\n<p>However,\
    \ an X server needs to be running for this to work. On nodes with GPU, X Server\
    \ is started with NVIDIA driver, and on non-GPU nodes, the X Server is started\
    \ with MESA library.</p>\n<p>X Server can be started during boot (for example,\
    \ using <code>systemctl set-default graphical.target</code>).</p>\n<p>Make sure\
    \ that VirtualGL package is installed in the container. The code below will download\
    \ and install VirtualGL.</p>\n<pre><code>wget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\
    \ndpkg -i virtualgl_2.6.2_amd64.deb\n</code></pre>\n<p>The application startup\
    \ script doesn't need to be modified, however, if the application needs to be\
    \ manually started, then <code>vglrun</code> needs to be appended before running\
    \ the application. For example: <code>singularity exec --nv -B /projects:/projects\
    \ -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX</code></p>\n\
    <p><a href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 6
  subscribers_count: 4
  topics: []
  updated_at: 1622677826.0
ChunCun/container:
  data_format: 2
  description: null
  filenames:
  - Singularity.torch_mmf
  - Singularity.torch
  full_name: ChunCun/container
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos8_roar" class="anchor" href="#centos8_roar" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos8_roar</h1>

    <p>Centos 8 base image for Roar</p>

    <h3>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTE</h3>

    <ul>

    <li>This recipe may include unnecessary packages for certain software installation</li>

    <li>More packages will be added in the future</li>

    </ul>

    <h2>

    <a id="user-content-updates" class="anchor" href="#updates" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Updates</h2>

    <ul>

    <li>

    <p>2020/11/13</p>

    <ul>

    <li>Initial recipe added</li>

    </ul>

    </li>

    <li>

    <p>2021/03/22</p>

    <ul>

    <li>Default Python3 is updated to Python 3.8</li>

    <li>Lapack, BLAS, OpenBLAS, ATLAS, and NetCDF are added</li>

    <li>CMake 3.19.7, Boost 1.75.0, and R 4.0.4 are added</li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605677713.0
CompBio-TDU-Japan/containers:
  data_format: 2
  description: recipes of Singularity
  filenames:
  - Singularity.gatk
  - Singularity.blast-legacy
  - Singularity.rooting_nj
  - Singularity.blast-latest
  - Singularity.vcftools
  - Singularity.snpeff
  - Singularity.cufflinks
  - Singularity.trimmomatic
  - Singularity.samtools
  - Singularity.bwa
  full_name: CompBio-TDU-Japan/containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>containers</h1>

    <p>recipes of Singularity</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1548055469.0
ComputationalRadiationPhysics/picongpu:
  data_format: 2
  description: 'Particle-in-Cell Simulations for the Exascale Era :sparkles:'
  filenames:
  - share/picongpu/dockerfiles/ubuntu-1604/Singularity
  full_name: ComputationalRadiationPhysics/picongpu
  latest_release: 0.5.0
  readme: '<h1>

    <a id="user-content-picongpu---particle-in-cell-simulations-for-the-exascale-era"
    class="anchor" href="#picongpu---particle-in-cell-simulations-for-the-exascale-era"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PIConGPU
    - Particle-in-Cell Simulations for the Exascale Era</h1>

    <p><a href="https://travis-ci.org/ComputationalRadiationPhysics/picongpu/branches"
    rel="nofollow"><img src="https://camo.githubusercontent.com/8001013459189259d918b4f4e2993dbc331e8a13903199c8cc3a1cf561247c40/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6d61737465722e7376673f6c6162656c3d6d6173746572"
    alt="Code Status master" data-canonical-src="https://img.shields.io/travis/ComputationalRadiationPhysics/picongpu/master.svg?label=master"
    style="max-width:100%;"></a>

    <a href="https://travis-ci.org/ComputationalRadiationPhysics/picongpu/branches"
    rel="nofollow"><img src="https://camo.githubusercontent.com/372e2d54a4f7af960e27fc9a084fc93f5cf2b9accfe4e983febe644963c03f1a/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6465762e7376673f6c6162656c3d646576"
    alt="Code Status dev" data-canonical-src="https://img.shields.io/travis/ComputationalRadiationPhysics/picongpu/dev.svg?label=dev"
    style="max-width:100%;"></a>

    <a href="http://picongpu.readthedocs.io" rel="nofollow"><img src="https://camo.githubusercontent.com/dd84c49cf1a8e134ca1a5a87517257a4317eecc4a80ff90195b2d2eebeeb7ced/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7069636f6e6770752f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/picongpu/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="http://computationalradiationphysics.github.io/picongpu" rel="nofollow"><img
    src="https://camo.githubusercontent.com/bea8b749e6bc63f677e6ccfc18ae8a6a4a4e39d55e3aac6c872acc8d1ecdc22b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4150492d446f787967656e2d626c75652e737667"
    alt="Doxygen" data-canonical-src="https://img.shields.io/badge/API-Doxygen-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/compare/master...dev"><img
    src="https://camo.githubusercontent.com/b36d60b89d6eb05b7977d0522b2c74389a9182e2a00879fa4a09361bfd3a4912/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6c61746573742f6465762e737667"
    alt="GitHub commits since last release" data-canonical-src="https://img.shields.io/github/commits-since/ComputationalRadiationPhysics/picongpu/latest/dev.svg"
    style="max-width:100%;"></a>

    <a href="https://isocpp.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/6b5671d1cd5cb649d92fdacc5e3ddc75dc6f6ea22e81efce926664d09ecf04ff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231312d6f72616e67652e737667"
    alt="Language" data-canonical-src="https://img.shields.io/badge/language-C%2B%2B11-orange.svg"
    style="max-width:100%;"></a>

    <a href="https://www.gnu.org/licenses/gpl-3.0.html" rel="nofollow"><img src="https://camo.githubusercontent.com/d9724a7d6c0aab25ad100b9c974369e2ea7585e6d0d0b87b66aa5bd34f6c2abe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c76332d626c75652e7376673f6c6162656c3d5049436f6e475055"
    alt="License PIConGPU" data-canonical-src="https://img.shields.io/badge/license-GPLv3-blue.svg?label=PIConGPU"
    style="max-width:100%;"></a>

    <a href="https://www.gnu.org/licenses/lgpl-3.0.html" rel="nofollow"><img src="https://camo.githubusercontent.com/d8685de1336e9f80f82625f4271fbd11d00beaf91013768daef9ac1a62e6fe2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4c47504c76332d626c75652e7376673f6c6162656c3d504d616363"
    alt="License PMacc" data-canonical-src="https://img.shields.io/badge/license-LGPLv3-blue.svg?label=PMacc"
    style="max-width:100%;"></a></p>

    <p><a href="http://www.youtube.com/watch?v=nwZuG-XtUDE" rel="nofollow"><img src="https://camo.githubusercontent.com/be7eb258510f59135b17c487a2bd50e76f186e1c71e56ab87f5cd1afe4df35d3/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6e775a75472d58745544452f302e6a7067"
    alt="PIConGPU Presentation Video" data-canonical-src="http://img.youtube.com/vi/nwZuG-XtUDE/0.jpg"
    style="max-width:100%;"></a>

    <a href="http://www.youtube.com/watch?v=nwZuG-XtUDE" rel="nofollow"><img src="docs/logo/pic_logo_vert_158x360.png"
    alt="PIConGPU Release" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

    <p>PIConGPU is a fully relativistic,

    <a href="https://en.wikipedia.org/wiki/Manycore_processor" rel="nofollow">manycore</a>,

    3D3V particle-in-cell (<a href="http://en.wikipedia.org/wiki/Particle-in-cell"
    rel="nofollow">PIC</a>)

    code. The Particle-in-Cell algorithm is a central tool in plasma physics.

    It describes the dynamics of a plasma by computing the motion of

    electrons and ions in the plasma based on

    <a href="http://en.wikipedia.org/wiki/Maxwell%27s_equations" rel="nofollow">Maxwell''s
    equations</a>.</p>

    <p>PIConGPU implements various numerical schemes to solve the PIC cycle.

    Its features for the electro-magnetic PIC algorithm include:</p>

    <ul>

    <li>a central or Yee-lattice for fields</li>

    <li>particle pushers that solve the equation of motion for charged and neutral

    particles, e.g., the <em>Boris-</em> and the

    <a href="http://dx.doi.org/10.1063/1.2837054" rel="nofollow"><em>Vay-Pusher</em></a>

    </li>

    <li>Maxwell field solvers, e.g.

    <a href="http://dx.doi.org/10.1109/TAP.1966.1138693" rel="nofollow"><em>Yee''s</em></a>
    and

    <a href="http://dx.doi.org/10.1103/PhysRevSTAB.16.021301" rel="nofollow"><em>Lehe''s</em></a>
    scheme</li>

    <li>rigorously charge conserving current deposition schemes, such as

    <a href="http://dx.doi.org/10.1016/0010-4655%2892%2990169-Y" rel="nofollow"><em>Villasenor-Buneman</em></a>,

    <a href="http://dx.doi.org/10.1016/S0010-4655%2800%2900228-9" rel="nofollow"><em>Esirkepov</em></a>

    and <em>ZigZag</em>

    </li>

    <li>macro-particle form factors ranging from NGP (0th order), CIC (1st),

    TSC (2nd), PSQ (3rd) to P4S (4th)</li>

    </ul>

    <p>and the electro-magnetic PIC algorithm is further self-consistently coupled
    to:</p>

    <ul>

    <li>classical radiation reaction

    (<a href="http://dx.doi.org/10.1016/j.cpc.2016.04.002" rel="nofollow">DOI:10.1016/j.cpc.2016.04.002</a>)</li>

    <li>QED synchrotron radiation (photon emission)

    (<a href="http://dx.doi.org/10.1103/PhysRevE.92.023305" rel="nofollow">DOI:10.1103/PhysRevE.92.023305</a>)</li>

    <li>advanced field ionization methods

    (<a href="http://dx.doi.org/10.1103/PhysRevA.59.569" rel="nofollow">DOI:10.1103/PhysRevA.59.569</a>,

    <a href="http://www.jetp.ac.ru/cgi-bin/dn/e_020_05_1307.pdf" rel="nofollow">LV
    Keldysh</a>, BSI)</li>

    </ul>

    <p>Besides the electro-magnetic PIC algorithm and extensions to it, we developed

    a wide range of tools and diagnostics, e.g.:</p>

    <ul>

    <li>online, far-field radiation diagnostics for coherent and incoherent radiation

    emitted by charged particles</li>

    <li>full restart and output capabilities via <a href="http://openPMD.org" rel="nofollow">openPMD</a>,

    including <a href="http://hdfgroup.org/" rel="nofollow">parallel HDF5</a> (via

    <a href="https://github.com/ComputationalRadiationPhysics/libSplash">libSplash</a>)
    and

    <a href="https://csmd.ornl.gov/adios/" rel="nofollow">ADIOS</a>, allowing for

    extreme I/O scalability and massively parallel online-analysis</li>

    <li>2D and 3D live view and diagnostics tools</li>

    <li>a large selection of extensible

    <a href="http://picongpu.readthedocs.io/en/latest/usage/plugins.html" rel="nofollow">online-plugins</a>

    </li>

    </ul>

    <p>As one of our supported compute platforms, GPUs provide a computational

    performance of several

    <a href="http://en.wikipedia.org/wiki/FLOPS" rel="nofollow">TFLOP/s</a> at considerable
    lower invest and

    maintenance costs compared to multi CPU-based compute architectures of similar

    performance. The latest high-performance systems

    (<a href="http://www.top500.org/" rel="nofollow">TOP500</a>) are enhanced by accelerator
    hardware that

    boost their peak performance up to the multi-PFLOP/s level. With its

    outstanding performance and scalability to more than 18''000 GPUs,

    PIConGPU was one of the <strong>finalists</strong> of the 2013

    <a href="http://sc13.supercomputing.org/content/acm-gordon-bell-prize" rel="nofollow">Gordon
    Bell Prize</a>.</p>

    <p>PIConGPU is developed and maintained by the

    <a href="https://www.hzdr.de/db/Cms?pNid=2097" rel="nofollow">Computational Radiation
    Physics Group</a>

    at the <a href="http://www.hzdr.de/db/Cms?pNid=132" rel="nofollow">Institute for
    Radiation Physics</a>

    at <a href="http://www.hzdr.de/" rel="nofollow">HZDR</a> in close collaboration
    with the Center

    for Information Services and High Performance Computing

    (<a href="http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih" rel="nofollow">ZIH</a>)
    of the

    Technical University Dresden (<a href="http://www.tu-dresden.de" rel="nofollow">TUD</a>).
    We are a

    member of the <a href="http://ccoe-dresden.de/" rel="nofollow">Dresden GPU Center
    of Excellence</a> that

    cooperates on a broad range of scientific GPU and manycore applications,

    workshops and teaching efforts.</p>

    <h2>

    <a id="user-content-attribution" class="anchor" href="#attribution" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Attribution</h2>

    <p>PIConGPU is a <em>scientific project</em>. If you <strong>present and/or publish</strong>
    scientific

    results that used PIConGPU, you should set a <strong>reference</strong> to show
    your support.</p>

    <p>Our according <strong>up-to-date publication</strong> at <strong>the time of
    your publication</strong>

    should be inquired from:</p>

    <ul>

    <li><a href="https://raw.githubusercontent.com/ComputationalRadiationPhysics/picongpu/master/REFERENCE.md"
    rel="nofollow">REFERENCE.md</a></li>

    </ul>

    <p>Please also consider adding yourself to our <a href="https://github.com/ComputationalRadiationPhysics/picongpu-communitymap">community
    map</a>.

    We would love to hear from you!</p>

    <h2>

    <a id="user-content-oral-presentations" class="anchor" href="#oral-presentations"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Oral
    Presentations</h2>

    <p>The following slide should be part of <strong>oral presentations</strong>.
    It is intended to

    acknowledge the team maintaining PIConGPU and to support our community:</p>

    <p>(<em>coming soon</em>) presentation_picongpu.pdf

    (svg version, key note version, png version: 1920x1080 and 1024x768)</p>

    <h2>

    <a id="user-content-software-license" class="anchor" href="#software-license"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    License</h2>

    <p><em>PIConGPU</em> is licensed under the <strong>GPLv3+</strong>. Furthermore,
    you can develop your

    own particle-mesh algorithms based on our general library <em>PMacc</em> that
    is

    shipped alongside PIConGPU. <em>PMacc</em> is <em>dual licensed</em> under both
    the

    <strong>GPLv3+ and LGPLv3+</strong>.

    For a detailed description, please refer to <a href="LICENSE.md">LICENSE.md</a></p>

    <hr>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>See our notes in <a href="INSTALL.rst">INSTALL.rst</a>.</p>

    <h2>

    <a id="user-content-users" class="anchor" href="#users" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Users</h2>

    <p>Dear User, please be aware that this is an <strong>open beta release</strong>!

    We hereby emphasize that we are still actively developing PIConGPU at great

    speed and do, from time to time, break backwards compatibility.</p>

    <p>When using this software, please stick to the <code>master</code> branch containing
    the

    latest <em>stable</em> release. It also contains a file <code>CHANGELOG.md</code>
    with the

    latest changes (and how to update your simulations). Read it first before

    updating between two versions! Also, we add a git <code>tag</code> according to
    a version

    number for each release in <code>master</code>.</p>

    <p>For any questions regarding the usage of PIConGPU please <strong>do not</strong>
    contact the

    developers and maintainers directly.</p>

    <p>Instead, please sign up to our <strong>PIConGPU-Users</strong> mailing list
    so we can

    distribute and archive user questions:

    <a href="https://cg.hzdr.de/Lists/picongpu-users/List.html" rel="nofollow">Subscribe
    (select "Feed" on bottom left)</a>.</p>

    <p>Before you post a question, browse the PIConGPU

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/search?l=markdown">documentation</a>,

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/wiki">wiki</a>,

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/issues">issue
    tracker</a> and the

    <a href="https://cg.hzdr.de/Lists/picongpu-users/List.html" rel="nofollow">mailing
    list history</a>

    to see if your question has been answered, already.</p>

    <p>PIConGPU is a collaborative project.

    We thus encourage users to engage in answering questions of other users and post
    solutions to problems to the list.

    A problem you have encountered might be the future problem of another user.</p>

    <p>In addition, please consider using the collaborative features of GitHub if
    you have questions or comments on code or documentation.

    This will allow other users to see the piece of code or documentation you are
    referring to.</p>

    <p>Main ressources are in our <a href="https://picongpu.readthedocs.io" rel="nofollow">online
    manual</a>, the <a href="https://github.com/ComputationalRadiationPhysics/picongpu/wiki">user
    section</a> of our wiki, documentation files in <a href="http://commonmark.org/help/"
    rel="nofollow"><code>.md</code> (Markdown)</a> and <a href="http://www.sphinx-doc.org/en/stable/rest.html"
    rel="nofollow"><code>.rst</code> (reStructuredText)</a> format in this repository
    and a <a href="http://www.youtube.com/watch?v=7ybsD8G4Rsk" rel="nofollow">getting
    started video</a>.

    Feel free to visit <a href="http://picongpu.hzdr.de" rel="nofollow">picongpu.hzdr.de</a>
    to learn more about the PIC algorithm.</p>

    <h2>

    <a id="user-content-software-upgrades" class="anchor" href="#software-upgrades"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    Upgrades</h2>

    <p>PIConGPU follows a

    <a href="http://nvie.com/posts/a-successful-git-branching-model/" rel="nofollow">master
    - dev</a>

    development model. That means our latest stable release is shipped in a branch

    called <code>master</code> while new and frequent changes to the code are incooporated

    in the development branch <code>dev</code>.</p>

    <p>Every time we update the <em>master</em> branch, we publish a new release

    of PIConGPU. Before you pull the changes in, please read our

    <a href="CHANGELOG.md">ChangeLog</a>!

    You may have to update some of your simulation <code>.param</code> and <code>.cfg</code>
    files by

    hand since PIConGPU is an active project and new features often require changes

    in input files. Additionally, a full description of new features and fixed bugs

    in comparison to the previous release is provided in that file.</p>

    <p>In case you decide to use <em>new, potentially buggy and experimental</em>
    features

    from our <code>dev</code> branch, be aware that support is very limited and you
    must

    participate or at least follow the development yourself. Syntax changes

    and in-development bugs will <em>not</em> be announced outside of their according
    pull

    requests and issues.</p>

    <p>Before drafting a new release, we open a new <code>release-*</code> branch
    from <code>dev</code> with

    the <code>*</code> being the version number of the upcoming release. This branch
    only

    receives bug fixes (feature freeze) and users are welcome to try it out

    (however, the change log and a detailed announcement might still be missing in

    it).</p>

    <h2>

    <a id="user-content-developers" class="anchor" href="#developers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Developers</h2>

    <h3>

    <a id="user-content-how-to-participate" class="anchor" href="#how-to-participate"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How
    to participate</h3>

    <p>See <a href="CONTRIBUTING.md">CONTRIBUTING.md</a></p>

    <p>If you like to jump in right away, see<br>

    <a href="https://github.com/ComputationalRadiationPhysics/picongpu/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"><img
    src="https://camo.githubusercontent.com/32b730b309ff90c1713e8ae39a73ae2145c4cceb0b7e72bacef523db9fc85d62/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f676f6f64253230666972737425323069737375652e7376673f636f6c6f723d353663626566"
    alt=''open "good first issue" issues'' data-canonical-src="https://img.shields.io/github/issues-raw/ComputationalRadiationPhysics/picongpu/good%20first%20issue.svg?color=56cbef"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-active-team" class="anchor" href="#active-team" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Active Team</h2>

    <h3>

    <a id="user-content-scientific-supervision" class="anchor" href="#scientific-supervision"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scientific
    Supervision</h3>

    <ul>

    <li>Dr. Michael Bussmann</li>

    <li>Dr. Axel Huebl</li>

    </ul>

    <h3>

    <a id="user-content-maintainers-and-core-developers" class="anchor" href="#maintainers-and-core-developers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Maintainers*
    and core developers</h3>

    <ul>

    <li>Dr. Sergei Bastrakov*</li>

    <li>Dr. Alexander Debus</li>

    <li>Marco Garten*</li>

    <li>Dr. Axel Huebl*</li>

    <li>Alexander Matthes</li>

    <li>Dr. Richard Pausch*</li>

    <li>Sophie Rudat</li>

    <li>Sebastian Starke</li>

    <li>Dr. Klaus Steiniger</li>

    <li>Rene Widera*</li>

    </ul>

    <h3>

    <a id="user-content-former-members-contributions-and-thanks" class="anchor" href="#former-members-contributions-and-thanks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Former
    Members, Contributions and Thanks</h3>

    <p>The PIConGPU Team expresses its gratitude to:</p>

    <p>Florian Berninger, Heiko Burau, Robert Dietrich, Carlchristian Eckert,

    Wen Fu, Ph.D., Alexander Grund, Sebastian Hahn, Anton Helm, Wolfgang Hoehnig,

    Dr.-Ing. Guido Juckeland, Jeffrey Kelling, Maximilian Knespel, Dr. Remi Lehe,

    Felix Schmitt, Benjamin Schneider, Joseph Schuchart, Conrad Schumann,

    Stefan Tietze, Marija Vranic, Ph.D., Benjamin Worpitz, and Erik Zenker.</p>

    <p>Kudos to everyone, mentioned or unmentioned, who contributed further in any

    way!</p>

    <hr>

    <p><a href="docs/images/lwfa_iso.png" target="_blank" rel="noopener noreferrer"><img
    src="docs/images/lwfa_iso.png" alt="image of an lwfa" title="LWFA" style="max-width:100%;"></a>

    <a href="docs/images/StrongScalingPIConGPU_log.png" target="_blank" rel="noopener
    noreferrer"><img src="docs/images/StrongScalingPIConGPU_log.png" alt="image of
    our strong scaling" title="Strong Scaling" style="max-width:100%;"></a></p>

    '
  stargazers_count: 463
  subscribers_count: 45
  topics:
  - laser
  - plasma
  - physics
  - gpu
  - physics-simulation
  - gpu-computing
  - particle-accelerator
  - particle-in-cell
  - pic
  - research
  updated_at: 1622377043.0
D-Lo/bambi:
  data_format: 2
  description: R wrapper for bamdb
  filenames:
  - src/bamdb/Singularity.bamdb
  full_name: D-Lo/bambi
  latest_release: null
  readme: "<p><a href=\"https://travis-ci.org/mskilab/bambi\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/47c82ab2d405aa684f3a5004ed8fc79887c025105127effda9ce1d35b5568974/68747470733a2f2f7472617669732d63692e6f72672f6d736b696c61622f62616d62692e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/mskilab/bambi.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/github/mskilab/bambi?branch=master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ccb3814df2f3f1c65e518dd49a10732518ba754f251e50546a0d42ec9fd9cdab/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6d736b696c61622f62616d62692e737667\"\
    \ alt=\"codecov.io\" data-canonical-src=\"https://img.shields.io/codecov/c/github/mskilab/bambi.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-bambi\" class=\"\
    anchor\" href=\"#bambi\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>bambi</h1>\n<p>R package for querying 10x WGS\
    \ and single-cell BAMs</p>\n<h2>\n<a id=\"user-content-dependencies\" class=\"\
    anchor\" href=\"#dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Dependencies</h2>\n<pre lang=\"{r}\"\
    ><code>devtools::install_github('mskilab/gUtils')\n</code></pre>\n<pre lang=\"\
    {r}\"><code>devtools::install_github('mskilab/bamUtils')\n</code></pre>\n<h2>\n\
    <a id=\"user-content-bambi-commands\" class=\"anchor\" href=\"#bambi-commands\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>bambi commands</h2>\n<p>Instantiate a bambi object:</p>\n<p>Methods:</p>\n\
    <ul>\n<li>\n<p>grab_bx()</p>\n<ul>\n<li><code>grab_bx(barcodes, query=NULL, data.table\
    \ = FALSE, verbose = FALSE, mc.cores = 1)</code></li>\n</ul>\n</li>\n<li>\n<p>grab_cb()</p>\n\
    <ul>\n<li><code>grab_cb(barcodes, query=NULL, data.table = FALSE, verbose = FALSE,\
    \ mc.cores = 1)</code></li>\n</ul>\n</li>\n<li>\n<p>grab_ub()</p>\n<ul>\n<li><code>grab_ub(barcodes,\
    \ query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)</code></li>\n\
    </ul>\n</li>\n<li>\n<p>fetch_by_tag()</p>\n<ul>\n<li><code>fetch_by_tag(tag, tag_queries,\
    \ query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)</code></li>\n\
    </ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-demo\" class=\"anchor\" href=\"\
    #demo\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Demo</h2>\n<ul>\n<li>Instantiate a <code>bambi</code> object</li>\n\
    </ul>\n<pre lang=\"{r}\"><code>library(bambi)\n\n&gt; hcc1143_subset = bambi$new(bam_file\
    \ = \"subsetHCC1143_phased_possorted0001.bam\", bamdb_path=\"subsetHCC1143_phased_possorted0001_lmdb\"\
    )\n</code></pre>\n<ul>\n<li>Call methods</li>\n</ul>\n<pre lang=\"{r}\"><code>&gt;\
    \ hcc1143_subset$grab_bx('CGACGTGTCCTCTAGC-1')\nGRanges object with 2 ranges and\
    \ 11 metadata columns:\n      seqnames                 ranges strand |\n     \
    \    &lt;Rle&gt;              &lt;IRanges&gt;  &lt;Rle&gt; |\n  [1]     chr1 [147975454,\
    \ 147975580]      + |\n  [2]     chr1 [147975675, 147975824]      - |\n      \
    \                                   qname      flag      mapq       cigar\n  \
    \                                 &lt;character&gt; &lt;numeric&gt; &lt;numeric&gt;\
    \ &lt;character&gt;\n  [1] ST-K00126:3:H5TL3BBXX:2:2109:25926:37800        99\
    \        16        127M\n  [2] ST-K00126:3:H5TL3BBXX:2:2109:25926:37800      \
    \ 147        16        150M\n            rnext     pnext      tlen\n      &lt;character&gt;\
    \ &lt;numeric&gt; &lt;numeric&gt;\n  [1]           = 147975676       371\n  [2]\
    \           = 147975455      -371\n                                          \
    \                                                                            \
    \                                   seq\n                                    \
    \                                                                            \
    \                                 &lt;character&gt;\n  [1]                   \
    \     ATGTCTTCTTCCTCATTATCTGGCACTGGTTAGGAAGCACTCATCTCCATGAAGTCATCTTTTGTTAATTCCTCTGGTGTGGTGTGTATTAGCTCTTAAATTCCTCCAAGATCCATATCTTGCAACC\n\
    \  [2] ATCTGGACACAAATTGTACTTTTGTCCAGCACGAATTTATTGTTTTGAGTTTCATGGTTTTCTATATCAACTGATGACATCTTGAAAGGTGTAAGCCTTCCAGACTTCCATGATGTTCTCTCTATTGGGTTTCTCTTTTGCAATGTTGAC\n\
    \                                                                            \
    \                                                                            qual\n\
    \                                                                            \
    \                                                                     &lt;character&gt;\n\
    \  [1]                        JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJFJJJJJJJJJJJAJFJJJJJJJJJFJJJJJJJJJJFJJJJFFFJJJFJJJJJJAAJFJJJFAFAFFFJAA&lt;7F&lt;\n\
    \  [2] A&lt;7FFFJFFFAJJAAAJJF&lt;F&lt;7A-&lt;AA-&lt;&lt;&lt;AFFJJJJJJJJFFJAFFAAFJFJJJAFFJJJJJJJJJJFJFAJJJJJJFJJJJJJ&lt;FFJJJFJJJFJJJJJJJJJJJJJFJJJJFFJ7JJJJF&lt;JJJJJJJJJJJJJJJJJJJFFAA&lt;\n\
    \                      BX    qwidth\n             &lt;character&gt; &lt;integer&gt;\n\
    \  [1] CGACGTGTCCTCTAGC-1       127\n  [2] CGACGTGTCCTCTAGC-1       150\n  -------\n\
    \  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1531085438.0
DiamondLightSource/Opt-ID:
  data_format: 2
  description: Code for the Optimisation of ID's using Python and Opt-AI
  filenames:
  - Singularity.env-v2
  - Singularity
  full_name: DiamondLightSource/Opt-ID
  latest_release: v2.0
  readme: '<p><a href="https://travis-ci.org/DiamondLightSource/Opt-ID" rel="nofollow"><img
    src="https://camo.githubusercontent.com/54cbf520664efa3f8fc3298323da50593160dd744d2ec6bd5de8ed8dd3593e0d/68747470733a2f2f7472617669732d63692e6f72672f4469616d6f6e644c69676874536f757263652f4f70742d49442e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/DiamondLightSource/Opt-ID.svg?branch=master"
    style="max-width:100%;"></a>  <a href="https://coveralls.io/github/DiamondLightSource/Opt-ID?branch=master&amp;service=github"
    rel="nofollow"><img src="https://camo.githubusercontent.com/dc50340a825cc5da0454649fde18840b6c0ec2d3b4dd91c5e8319d2319850548/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4469616d6f6e644c69676874536f757263652f4f70742d49442f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562"
    alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/DiamondLightSource/Opt-ID/badge.svg?branch=master&amp;service=github"
    style="max-width:100%;"></a>  <a href="https://scrutinizer-ci.com/g/DiamondLightSource/Opt-ID/?branch=master"
    rel="nofollow"><img src="https://camo.githubusercontent.com/c0b7776aa669724907a66bc7d335a00b5606e9f3dc41409d189185f47b791cbb/68747470733a2f2f7363727574696e697a65722d63692e636f6d2f672f4469616d6f6e644c69676874536f757263652f4f70742d49442f6261646765732f7175616c6974792d73636f72652e706e673f623d6d6173746572"
    alt="Scrutinizer Code Quality" data-canonical-src="https://scrutinizer-ci.com/g/DiamondLightSource/Opt-ID/badges/quality-score.png?b=master"
    style="max-width:100%;"></a> <a href="https://doi.org/10.5281/zenodo.3968577"
    rel="nofollow"><img src="https://camo.githubusercontent.com/5bb0569d502774e80da711f971a82ba8beb8a3decdc75b595131dd5a79f03bf9/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333936383537372e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.3968577.svg"
    style="max-width:100%;"></a> <a href="https://singularity-hub.org/collections/4728"
    rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-opt-id" class="anchor" href="#opt-id" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Opt-ID</h1>

    <p>Code for the Optimisation of ID''s using Python and Opt-AI</p>

    <h2>

    <a id="user-content-overview-of-how-to-use-opt-id" class="anchor" href="#overview-of-how-to-use-opt-id"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview
    of how to use Opt-ID</h2>

    <p>Opt-ID is run is by providing:</p>

    <ul>

    <li>a main configuration file in YAML format which contains all the various

    parameters for the sort/shim job</li>

    <li>an existing directory in which output data will be written to</li>

    </ul>

    <p>There are two main flags, <code>--sort</code> and <code>--shim</code>, to run
    sort and shim jobs. The

    idea is that using either of these flags in conjunction with the YAML config

    file will go through and run all the scripts that are used to produce

    intermediate files and pass them around appropriately, so then there''s only one

    command needed to be executed to run a sort or shim job, and the YAML config

    file is the single source of all the parameter information used for that

    particular job.</p>

    <p>There are several other processes that Opt-ID provides that are desired to
    be

    done after a sort/shim but don''t require the sequence of scripts that a

    sort/shim job does (for example, the use of <code>compare.py</code> to compare
    a shimmed

    genome to the original genome), so the <code>--sort</code> and <code>--shim</code>
    flags aren''t able

    to provide these sorts of processes. To do so, there are several shell scripts

    that are autogenerated when a sort or shim job is run that can be executed.

    These scripts run Opt-ID in the particular way that is needed to perform the

    process, without the user needing to worry about extra configuration on top of

    the YAML file.</p>

    <p>Taking the <code>compare.py</code> example previously mentioned, a script would
    be

    autogenerated after a shim job called <code>compare_shim.sh</code> that can be
    passed any

    shimmed genome file in the data directory, and it will take care of calling

    Opt-ID in the particular way it needs to in order to run the <code>compare.py</code>
    script

    with the appropriate parameters. More details on how to use these autogenerated

    shell scripts are below in the "Using the autogenerated shell scripts" section.</p>

    <h2>

    <a id="user-content-data-directory" class="anchor" href="#data-directory" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Data directory</h2>

    <p>The data outputted by Opt-ID is split roughly into two categories:</p>

    <ul>

    <li>large files such as <code>.h5</code> files</li>

    <li>smaller files such as <code>.json</code>, <code>.mag</code>, <code>.sh</code>
    files</li>

    </ul>

    <p>The smaller files get written to the directory passed as the second parameter
    to

    Opt-ID, so if OptID was passed <code>/home/FedID/my_dir</code> then the smaller
    files would

    get written to <code>/home/FedID/my_dir</code>.</p>

    <p>The larger files get written to a directory within <code>/dls/tmp/FedID</code>
    whose path

    is based on the user''s FedID and also the name of the data directory passed to

    Opt-ID. The name of the directory created in <code>/dls/tmp/FedID</code> will
    be the name

    of the very last directory in the path passed to Opt-ID. For example, if the

    path <code>/home/FedID/my_dir</code> is passed to Opt-ID, then the directory

    <code>/dls/tmp/FedID/my_dir</code> will be created. Symlinks are then created
    in

    <code>/home/FedID/my_dir</code> to point to the larger files inside

    <code>/dls/tmp/FedID/my_dir</code>.</p>

    <p>One reason behind having two separate directories containing different data

    files is due to the large size of the <code>.h5</code> files produced by Opt-ID
    and not

    having the space to put them just anywhere in the filesystem (<code>/dls/tmp</code>
    has

    much more available space than, for example, the home directory associated to
    a

    FedID). Another reason is that the automatic deletion of files in <code>/dls/tmp</code>
    can

    be used to do some automatic periodic cleanup of old, large files.</p>

    <h3>

    <a id="user-content-intended-usage" class="anchor" href="#intended-usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Intended usage</h3>

    <p>The intended usage of this dual-directory structure is that the smaller files

    are written to somewhere away from <code>/dls/tmp</code> so then they''re not
    deleted

    periodically and can be referred to later if needed, whilst the larger files are

    written to the user''s directory in <code>/dls/tmp</code> so then they <em>are</em>
    deleted

    periodically. Therefore, it is advised that the directory provided to Opt-ID is

    not a directory in <code>/dls/tmp/FedID</code>; this is not only because of potential

    deletion of the smaller files, but also because passing a directory in

    <code>/dls/tmp/FedID</code> can cause some confusion regarding the directory that
    is

    subsequently created by Opt-ID in <code>/dls/tmp/FedID</code>.</p>

    <p><strong>In particular, it is advised that the directory passed to Opt-ID is
    one within

    <code>/dls/technical/id</code></strong>, as this is where output data from other
    Opt-ID jobs has

    typically been placed.</p>

    <h3>

    <a id="user-content-example-directory-structures" class="anchor" href="#example-directory-structures"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example
    directory structures</h3>

    <p>For example, if the directory <code>/dls/technical/id/test/</code> is passed
    to Opt-ID, the

    expected directory structures right after having run a sort job on a cluster is

    given below:</p>

    <p><code>/dls/technical/id/test/</code>:</p>

    <ul>

    <li>

    <code>test_sort.json</code> (file)</li>

    <li>

    <code>test_sort.mag</code> (file)</li>

    <li>

    <code>test_sort.h5 -&gt; /dls/tmp/FedID/test/test_sort.h5</code> (symlink to a
    file)</li>

    <li>

    <code>generate_report.sh</code> (file)</li>

    <li>

    <code>restart_sort.sh</code> (file)</li>

    <li>

    <code>logfiles/</code> (directory)</li>

    <li>

    <code>genomes -&gt; /dls/tmp/FedID/test/genomes/</code> (symlink to a directory)</li>

    <li>

    <code>process_genome_output -&gt; /dls/tmp/FedID/test/process_genome_output/</code>

    (symlink to a directory)</li>

    </ul>

    <p><code>/dls/tmp/FedID/test/</code>:</p>

    <ul>

    <li>

    <code>test_sort.h5</code> (file, the symlink <code>/dls/technical/id/test/test_sort.h5</code>
    points

    to this file)</li>

    <li>

    <code>genomes/</code> (directory, the symlink <code>/dls/technical/id/test/genomes</code>
    points to

    this directory)</li>

    <li>

    <code>process_genome_output/</code> (directory, the symlink

    <code>/dls/technical/id/test/process_genome_output</code> points to this directory)</li>

    </ul>

    <p>As another example, for the same directory being passed but instead a shim
    job

    being run on a cluster, the expected directory structures right after the job

    are:</p>

    <p><code>/dls/technical/id/test/</code>:</p>

    <ul>

    <li>

    <code>test_shim.json</code> (file)</li>

    <li>

    <code>test_shim.mag</code> (file)</li>

    <li>

    <code>test_shim.h5 -&gt; /dls/tmp/FedID/test/test_shim.h5</code> (symlink to a
    file)</li>

    <li>

    <code>generate_report.sh</code> (file)</li>

    <li>

    <code>compare_shim.sh</code> (file)</li>

    <li>

    <code>logfiles/</code> (directory)</li>

    <li>

    <code>shimmed_genomes -&gt; /dls/tmp/FedID/test/shimmed_genomes/</code> (symlink
    to a

    directory)</li>

    <li>

    <code>process_genome_output -&gt; /dls/tmp/FedID/test/process_genome_output/</code>

    (symlink to a directory)</li>

    </ul>

    <p><code>/dls/tmp/FedID/test/</code>:</p>

    <ul>

    <li>

    <code>test_shim.h5</code> (file, the symlink <code>/dls/technical/id/test/test_shim.h5</code>
    points

    to this file)</li>

    <li>

    <code>shimmed_genomes/</code> (directory, the symlink

    <code>/dls/technical/id/test/shimmed_genomes</code> points to this directory)</li>

    <li>

    <code>process_genome_output/</code> (directory, the symlink

    <code>/dls/technical/id/test/process_genome_output</code> points to this directory)</li>

    </ul>

    <p>Note that the filenames <code>test_sort.*</code> and <code>test_shim.*</code>
    are just placeholders

    and have been chosen only for illustrative purposes, these files can be named
    as

    desired in the YAML config file.</p>

    <h2>

    <a id="user-content-preliminary-steps-to-be-able-to-run-opt-id" class="anchor"
    href="#preliminary-steps-to-be-able-to-run-opt-id" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Preliminary steps to be able to run Opt-ID</h2>

    <p>A process that is not done by Opt-ID is the transfer of magnet information
    in

    the Excel files provided by the supplier to <code>.sim</code> files. To do so,
    from the

    Excel files supplied by the supplier, create tab delimited <code>.sim</code> files
    of

    magnetisation. This is a manual procedure done only on Windows. Note that,

    currently, Opt-ID requires the magnet names in the <code>.sim</code> files to
    have leading

    zeros that pad out the name to 3 digits. For example, instead of ''1'' it should

    be ''001''.</p>

    <p>To get the code, clone the Opt-ID repo to the desired place in the filesystem.

    To set up the environment for running Opt-ID on a Linux machine, in a terminal

    run the following commands:</p>

    <pre><code>module load python/3

    module load global/cluster

    export PYTHONPATH=$PYTHONPATH:/path/to/Opt-ID

    </code></pre>

    <p>where <code>/path/to/Opt-ID</code> is the path to the root directory of the
    cloned repo.

    (There is a change to how <code>python</code> is used to run the code which is
    detailed in

    the next section, and so the third command is to enable <code>python</code> to
    find the

    code in the repo).</p>

    <h2>

    <a id="user-content-running-opt-id-with-the-python-command" class="anchor" href="#running-opt-id-with-the-python-command"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    Opt-ID with the <code>python</code> command</h2>

    <p>The main script that is used for running Opt-ID is <code>IDSort/src/optid.py</code>.
    It

    should be run using the syntax <code>python -m IDSort.src.optid</code> as opposed
    to

    <code>python /path/to/Opt-ID/IDSort/src/optid.py</code>.</p>

    <h2>

    <a id="user-content-different-options-that-opt-id-can-be-run-with" class="anchor"
    href="#different-options-that-opt-id-can-be-run-with" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Different options that
    Opt-ID can be run with</h2>

    <p>There are two sets of flags from which one flag from each set is mandatory
    to be

    passed to Opt-ID, and the rest are optional and have sensible default values if

    they are not provided.</p>

    <p>The mandatory sets of flags are</p>

    <ul>

    <li>

    <code>--sort</code> vs <code>--shim</code>

    </li>

    <li>

    <code>--cluster-on</code> vs <code>--cluster-off</code>

    </li>

    </ul>

    <p>where only one flag from each bullet point should be provided.</p>

    <p>Examples of running Opt-ID with the bare mininum flags and parameters it needs

    are:</p>

    <pre><code>python -m IDSort.src.optid --sort --cluster-on /path/to/yaml /path/to/data/dir

    python -m IDSort.src.optid --shim --cluster-off /path/to/yaml /path/to/data/dir

    </code></pre>

    <h3>

    <a id="user-content---sort-and---shim" class="anchor" href="#--sort-and---shim"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--sort</code>
    and <code>--shim</code>

    </h3>

    <p>These are used for specifying what type of job is desired.</p>

    <h3>

    <a id="user-content---cluster-on-and---cluster-off" class="anchor" href="#--cluster-on-and---cluster-off"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--cluster-on</code>
    and <code>--cluster-off</code>

    </h3>

    <p>These are used for specifying whether the job is run on the local machine or

    submitted to run on a cluster.</p>

    <h4>

    <a id="user-content---num-threads---queue-and---node-os" class="anchor" href="#--num-threads---queue-and---node-os"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--num-threads</code>,
    <code>--queue</code>, and <code>--node-os</code>

    </h4>

    <p>These are used in conjunction with <code>--cluster-on</code>. Some examples
    of using these

    flags would be</p>

    <pre><code>python -m IDSort.src.optid --sort --cluster-on --node-os rhel7 /path/to/yaml
    /path/to/data/dir

    python -m IDSort.src.optid --shim --cluster-on --queue low.q /path/to/yaml /path/to/data/dir

    </code></pre>

    <h4>

    <a id="user-content---seed-and---seed-value" class="anchor" href="#--seed-and---seed-value"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--seed</code>
    and <code>--seed-value</code>

    </h4>

    <p>These are used in conjunction with <code>--cluster-off</code>. <code>--seed</code>
    is used to specify

    that the random number generator (RNG) should be seeded and thus produce the

    same output across multiple runs with the same parameters. <code>--seed-value</code>
    is

    specified if a particular value to seed the RNG is desired (by default its value

    is 1). Some examples of using these flags would be</p>

    <pre><code>python -m IDSort.src.optid --sort --cluster-off --seed /path/to/yaml
    /path/to/data/dir

    python -m IDSort.src.optid --shim --cluster-off --seed --seed-value 30 /path/to/yaml
    /path/to/data/dir

    </code></pre>

    <h2>

    <a id="user-content-yaml-config-files" class="anchor" href="#yaml-config-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>YAML
    config files</h2>

    <p>The YAML config files contain the parameters used by the various scripts that

    Opt-ID runs. The top-level sections of the YAML config files are the script

    names minus the <code>.py</code> and the subsections are the different parameters
    passed to

    that particular script. For the most part, the subsection names are exactly the

    same as the script parameters they''re associated to, for example, the

    <code>id_setup.py</code> script has a <code>--periods</code> flag, and the YAML
    subsection

    corresponding to that parameter is <code>id_setup.periods</code>.</p>

    <p>A few exceptions exist to try and be more descriptive with what the parameter

    is, for example, <code>process_genome.py</code> refers to the files it''s given
    as elements

    of the <code>args</code> list, but in the YAML the corresponding subsection for
    a shim job

    is <code>process_genome.readable_genome_file</code> which is hopefully a more
    useful

    description.</p>

    <p>Examples of YAML config files can be found in the <code>IDSort/example_configs</code>

    directory. There are some placeholder values in these config files that aren''t

    valid values for their associated section in the YAML, and the following

    sections detail the changes that need to be made to the example config files to

    get them in a state ready to run a job.</p>

    <h3>

    <a id="user-content-sort-config-example" class="anchor" href="#sort-config-example"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sort
    config example</h3>

    <p>There are three values that need to be changed:</p>

    <ul>

    <li><code>magnets.hmags</code></li>

    <li><code>magnets.hemags</code></li>

    <li><code>magnets.htmags</code></li>

    </ul>

    <p>Their values should be absolute paths to any <code>.sim</code> files of the
    relevant type.</p>

    <h3>

    <a id="user-content-shim-config-example" class="anchor" href="#shim-config-example"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Shim
    config example</h3>

    <p>There are five values that need to be changed:</p>

    <ul>

    <li><code>magnets.hmags</code></li>

    <li><code>magnets.hemags</code></li>

    <li><code>magnets.htmags</code></li>

    <li><code>process_genome.readable_genome_file</code></li>

    <li><code>mpi_runner_for_shim_opt.bfield_filename</code></li>

    </ul>

    <p>The first three are the same as in the sort config example. The value of

    <code>process_genome.readable_genome_file</code> should be an absolute path to
    the <code>.inp</code>

    file that is used to start the shim job from. The value of

    <code>mpi_runner_for_shim_opt.bfield_filename</code> should be an absolute path
    to the

    <code>.h5</code> file that is converted from <code>.bfield</code> files that are
    produced by igor.</p>

    <p>Note that, currently, the use of the <code>igor2h5.py</code> script hasn''t
    yet been

    integrated into the YAML configuration file for Opt-ID, so the process of

    converting <code>.bfield</code> data into <code>.h5</code> data is one that needs
    to be done by

    manually executing the <code>igor2h5.py</code> script (or by any other means)
    prior to

    running a shim job with Opt-ID.</p>

    <h2>

    <a id="user-content-using-the-autogenerated-shell-scripts" class="anchor" href="#using-the-autogenerated-shell-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    the autogenerated shell scripts</h2>

    <p>All the autogenerated scripts can be executed from anywhere in the filesystem,

    it''s not necessary for the current working directory to be the same directory

    that the script is in.</p>

    <p>Due to the facts that</p>

    <ul>

    <li>these scripts are generated on a job-by-job basis and are only meant to be
    run

    for the particular data within the directory the scripts are in</li>

    <li>the structure of the data directories are fixed and known in advance</li>

    </ul>

    <p>when it comes to passing parameters to these scripts they are aware of the

    specific directories that the files they''re expecting should be in, so only

    filenames need to be given to them and not absolute or even relative filepaths.

    Concrete examples are given below in the <code>generate_report.sh</code> and

    <code>compare_shim.sh</code> sections that hopefully explain in more detail how
    to pass

    parameters to these scripts.</p>

    <h3>

    <a id="user-content-generate_reportsh" class="anchor" href="#generate_reportsh"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>generate_report.sh</code>

    </h3>

    <p>This script is used to create a report with some useful data visualisation
    in a

    PDF file. For a sort job it can be passed multiple <code>.genome</code> and <code>.inp</code>
    files,

    and for a shim job it can be passed multiple <code>.h5</code> files that are associated
    to

    the "full genomes" (as opposed to the smaller-sized "compare genomes") in the

    shim output.</p>

    <p>For a sort job, Opt-ID will look in both the <code>genomes/</code> and

    <code>process_genome_output/</code> directories for the given <code>.genome</code>
    and <code>.inp</code> files,

    and for a shim job Opt-ID will look in the <code>shimmed_genomes/</code> directory
    for the

    given <code>.h5</code> files. Therefore, the parameters passed to <code>generate_report.sh</code>

    should only be the filenames and not filepaths.</p>

    <p>For example, for a sort job, the correct way to pass a genome and a <code>.inp</code>
    file

    to the script would be</p>

    <pre><code>/path/to/generate_report.sh foo.genome bar.inp

    </code></pre>

    <p>as opposed to</p>

    <pre><code>/path/to/generate_report.sh genomes/foo.genome process_genome_output/bar.inp

    </code></pre>

    <p>Another example: for a shim job, the correct way to pass <code>.h5</code> files
    to the

    script would be</p>

    <pre><code>/path/to/generate_report.sh foo.h5 bar.h5

    </code></pre>

    <p>as opposed to</p>

    <pre><code>/path/to/generate_report.sh shimmed_genomes/foo.h5 shimmed_genomes/bar.h5

    </code></pre>

    <p>An optional <code>--report-filename</code> flag can be passed before the files
    to specify

    the name of the PDF file, and genome reports are stored in the <code>genome_reports/</code>

    directory within the directory passed to Opt-ID. Report filenames should have
    a

    <code>.pdf</code> extension to enable a simple check between the report filename
    parameter

    and <code>.genome</code>/<code>.inp</code> file parameters that follow it. The
    <code>--report-filename</code>

    option can be omitted and in that case the report filename will be a

    concatenation of all the filenames passed with an underscore character "_" as

    the separator between the filenames.</p>

    <p>An example of using the <code>--report-filename</code> flag is</p>

    <pre><code>/path/to/generate_report --report-filename report.pdf foo.genome bar.inp

    </code></pre>

    <h3>

    <a id="user-content-restart_sortsh" class="anchor" href="#restart_sortsh" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>restart_sort.sh</code>

    </h3>

    <p>This script requires no parameters and can be run simply as

    <code>/path/to/restart_sort.sh</code>, Opt-ID will take care of loading the YAML
    config of

    the previous sort job and will use all the same flags and paramters as the

    original sort job. One example is that if the original sort job was run on a

    cluster, so will the restart-sort job, and another example is that the same

    <code>.json</code>, <code>.mag</code> and <code>.h5</code> (lookup table) files
    from the original sort job will

    be reused in the restart-sort job instead of being regenerated.</p>

    <h3>

    <a id="user-content-compare_shimsh" class="anchor" href="#compare_shimsh" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>compare_shim.sh</code>

    </h3>

    <p>This can be passed a single <code>.genome</code> file that is in the <code>shimmed_genomes/</code>

    directory and it will generate a human readable diff between the original and

    shimmed genomes that will be written to the <code>shim_diffs/</code> directory.
    It''s not

    necessary to pass the original genome to this script, Opt-ID will take care of

    finding it so only the shimmed genome needs to be given as a parameter.</p>

    <p>Similarly to what <code>generate_report.sh</code> does, <code>compare_shim.sh</code>
    will look in the

    <code>shimmed_genomes/</code> directory so only filenames should be passed to
    it and not

    filepaths. An example of using this script would be:</p>

    <pre><code>/path/to/compare_shim.sh foo.genome

    </code></pre>

    <p>An optional <code>--diff-filename</code> flag can be passed before the shimmed
    genome file

    to specify the filename of the human readable diff. Currently Opt-ID appends a

    <code>.txt</code> extension to the filename so it''s not necessary to put that
    in the

    parameter. Again, similarly to what <code>generate_report.sh</code> does, if this
    flag is

    omitted then the diff filename is a concatenation of the original genome and

    shimmed genome filenames with an underscore character as the separator, and then

    also prepended with <code>shim_</code>. For example, if the original genome is
    <code>foo.genome</code>

    and the shimmed genome is <code>bar.genome</code>, then if the <code>--diff-filename</code>
    flag is

    omitted then the diff filename would be <code>shim_foo.genome_bar.genome.txt</code>.
    An

    example of using the <code>--diff-filename</code> flag is</p>

    <pre><code>/path/to/compare_shim.sh --diff-filename my_shim foo.genome

    </code></pre>

    <h2>

    <a id="user-content-hidden-options-of-opt-id" class="anchor" href="#hidden-options-of-opt-id"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"Hidden"
    options of Opt-ID</h2>

    <p>There are several options that Opt-ID has but are only meant to be used by
    the

    autogenerated shell scripts and not intended to be invoked directly by a user;

    therefore, these options aren''t of much interest to users and only of potential

    interest to developers. The following are just some useful notes to any

    developers viewing this document:</p>

    <ul>

    <li>these options are related to those kinds of processes that a user would want

    to do that aren''t full sort/shim jobs that were referred to in the "Overview

    of how to use Opt-ID" section of this document</li>

    <li>these options are all used by the autogenerated shell scripts that were also

    referred to in the "Overview of how to use Opt-ID" section, hence why the

    users need not directly use them, the autogenerated scripts should take care

    of using these "hidden options" where necessary</li>

    <li>these are also processes that are done after a sort/shim, so they assume the

    existence of a YAML config that has already been used for the sort/shim job,

    as well as any output data from a sort/shim job</li>

    </ul>

    <h3>

    <a id="user-content---generate-report" class="anchor" href="#--generate-report"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--generate-report</code>

    </h3>

    <p>This option starts off the process of using the

    <code>IDSort/src/genome_report_template.ipynb</code> file to generate a Jupyter
    notebook

    file, and then running it to produce a PDF report.</p>

    <h3>

    <a id="user-content---restart-sort" class="anchor" href="#--restart-sort" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--restart-sort</code>

    </h3>

    <p>This option starts off the process of reusing the same YAML config file that
    was

    used for the sort job to get all the parameters used for the original sort job,

    and then running Opt-ID to generate genomes from an initial population as

    opposed to generating genomes from scratch.</p>

    <h3>

    <a id="user-content---compare-shim" class="anchor" href="#--compare-shim" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--compare-shim</code>

    </h3>

    <p>This option starts off the process of comparing the given shimmed genome to
    the

    original genome that was used to start the shim job.</p>

    <h2>

    <a id="user-content-running-the-tests" class="anchor" href="#running-the-tests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    the tests</h2>

    <p>Navigate to the root directory of the Opt-ID repo:</p>

    <pre><code>cd /path/to/Opt-ID

    </code></pre>

    <p>To run all the tests:</p>

    <pre><code>python -m pytest IDSort/test/

    </code></pre>

    <p>To run a particular test in the <code>test/</code> directory, it can be specified
    in the

    path in the above command. For example, to run <code>IDSort/test/magnets_test.py</code>:</p>

    <pre><code>python -m pytest IDSort/test/magnets_test.py

    </code></pre>

    <p><a href="https://codescene.io/projects/6289/jobs/latest-successful/results"
    rel="nofollow"><img src="https://camo.githubusercontent.com/f3827d47125aaed62ec3276ebe498b2f14e96da020a3a3c25000597585019c5a/68747470733a2f2f636f64657363656e652e696f2f70726f6a656374732f363238392f7374617475732e737667"
    alt="" data-canonical-src="https://codescene.io/projects/6289/status.svg" style="max-width:100%;">
    Get more details at <strong>codescene.io</strong>.</a></p>

    '
  stargazers_count: 6
  subscribers_count: 8
  topics: []
  updated_at: 1609847329.0
DiamondLightSource/Savu:
  data_format: 2
  description: Tomography Reconstructon Pipeline
  filenames:
  - install/savu_singularity/conda-recipes/Singularity
  - install/savu_singularity/singularity-recipes/Singularity.SavuDeps
  - install/savu_singularity/singularity-recipes/Singularity.SavuAstra
  - install/savu_singularity/singularity-recipes/Singularity.SavuCore
  full_name: DiamondLightSource/Savu
  latest_release: v3.0
  readme: "<p><a href=\"doc/source/images/portcullis_logo.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"doc/source/images/portcullis_logo.png\"\
    \ alt=\"alt text\" title=\"Portcullis\" style=\"max-width:100%;\"></a></p>\n<h1>\n\
    <a id=\"user-content-portcullis\" class=\"anchor\" href=\"#portcullis\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Portcullis</h1>\n\
    <p><a href=\"https://github.com/maplesond/portcullis/releases\"><img src=\"https://camo.githubusercontent.com/b9c31b04d2671e6317cdfd9e4fdf893512936091302d1b1b56c99cb89ab43df7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7461672f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Version\" data-canonical-src=\"https://img.shields.io/github/tag/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://jenkins.sdlmapleson.net/job/portcullis/job/develop/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f696e3e0136cfb90f0e05f4f4e0a257ece7cd1e52ff19a0c8963b32df756d3a7/68747470733a2f2f6a656e6b696e732e73646c6d61706c65736f6e2e6e65742f6275696c645374617475732f69636f6e3f6a6f623d706f727463756c6c6973253246646576656c6f70\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://jenkins.sdlmapleson.net/buildStatus/icon?job=portcullis%2Fdevelop\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/ad4d6f3e16da4f0dddcd142fa3b6088042b13242787f5ad939d2db28282d3eb5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d627269676874677265656e2e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPL%20v3-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/issues\"\
    ><img src=\"https://camo.githubusercontent.com/d3bedf8e24750956939d66108f9ba197e72b83d1de8fc7305708ab2d67c20c17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues-raw/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Portcullis stands for PORTable CULLing\
    \ of Invalid Splice junctions from pre-aligned RNA-seq data.  It is known that\
    \ RNAseq mapping tools generate many invalid junction predictions, particularly\
    \ in deep datasets with high coverage over splice sites.  In order to address\
    \ this, instead for creating a new RNAseq mapper, with a focus on SJ accuracy\
    \ we created a tool that takes in a BAM file generated by an RNAseq mapper of\
    \ the user's own choice (e.g. Tophat2, Gsnap, STAR2 or HISAT2) as input (i.e.\
    \ it's portable).  It then, analyses and quantifies all splice junctions in the\
    \ file before, filtering (culling) those which are unlikely to be genuine.  Portcullis\
    \ output's junctions in a variety of formats making it suitable for downstream\
    \ analysis (such as differential splicing analysis and gene modelling) without\
    \ additional work.  Portcullis can also filter the original BAM file removing\
    \ alignments associated with <em>bad</em> junctions.  Both the filtered junctions\
    \ and BAM files are cleaner and more usable resources which can more effectively\
    \ be used to assist in downstream analyses such as gene prediction and genome\
    \ annotation.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h2>\n<p>We support multiple methods\
    \ for installing and running portcullis.  Hopefully your favourite container or\
    \ package manager is supported below.  If not let us know and we'll try to work\
    \ to get it integrated there.</p>\n<p><strong>Docker</strong></p>\n<p><a href=\"\
    https://hub.docker.com/r/maplesond/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/de966674ebe7a3dec2fed423683dd2c64e3630527fab6a691add53421292e384/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code># Keep in mind you need to mount\
    \ in any working directories to the container with the `-v` option.\n# Ideally,\
    \ mount these into the /data directory which is the container's working directory.\n\
    docker run --it --rm -v /abspath/to/data/on/host:/data maplesond/portcullis:stable\
    \ portcullis --help\n</code></pre>\n<p><strong>Singularity</strong></p>\n<pre><code>#\
    \ First download the container:\nsingularity pull --name portcullis.img shub://maplesond/portcullis:master\n\
    \n# Then to execute commands in the container:\nsingularity exec portcullis.img\
    \ portcullis --help\n</code></pre>\n<p><strong>Conda</strong></p>\n<p><a href=\"\
    https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/381a7739b713a2bae02343a6ac934de39148a7866dbf4e52b597391b2a07fd4b/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f6c61746573745f72656c656173655f646174652e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/latest_release_date.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/77a7c650d2675de3588df907d8e8aec11957abc95bcfd87d3b1b07f78a2bc4ec/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f706c6174666f726d732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/platforms.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/83781f462972e76ba4f2d046533fd48deb7cb72a0512481ff304f79c51bc01e3/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f646f776e6c6f6164732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/downloads.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code>conda install portcullis --channel=bioconda\n\
    </code></pre>\n<p><strong>Brew</strong></p>\n<pre><code>brew install brewsci/bio/portcullis\n\
    </code></pre>\n<p><strong>From source</strong></p>\n<p><a href=\"https://github.com/maplesond/portcullis/releases\"\
    ><img src=\"https://camo.githubusercontent.com/3885a69f4777ec0c98cf3d0bee17eb7ca3d3eb69bbf850df2f36895b80168ade/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d61706c65736f6e642f706f727463756c6c69732f746f74616c2e737667\"\
    \ alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/github/downloads/maplesond/portcullis/total.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>If you wish to install from source please\
    \ first confirm that first you have these dependencies are installed and configured:</p>\n\
    <ul>\n<li>\n<strong>GCC</strong> V4.8+</li>\n<li>\n<strong>autoconf</strong> V2.53+</li>\n\
    <li>\n<strong>automake</strong> V1.11+</li>\n<li><strong>make</strong></li>\n\
    <li>\n<strong>libtool</strong> V2.4.2+</li>\n<li><strong>zlib-dev</strong></li>\n\
    <li><strong>pthreads</strong></li>\n<li>\n<strong>boost-dev</strong> V1.52+</li>\n\
    <li>\n<strong>samtools</strong> V1.2+</li>\n<li>\n<strong>Python3-dev</strong>\
    \ V3.5+ (Make sure the following packages are installed: <em>pandas</em>, <em>matplotlib</em>,\
    \ <em>setuptools</em>, <em>sphinx</em>, <em>tabulate</em>)</li>\n</ul>\n<p>Then\
    \ proceed with the following steps:</p>\n<pre><code># Clone the repo\ngit clone\
    \ git@github.com:maplesond/portcullis.git\n\n# Move into repo directory\ncd portcullis\n\
    \n# Generate configure script\n./autogen.sh\n\n# Confirm dependencies and generate\
    \ makefiles\n# Adding --prefix &lt;dir&gt; will tell make install to put everything\
    \ in a \n# particular directory.  Default is /usr/local.\n./configure\n\n# Compile\
    \ (increasing -j will make it go faster!\nmake -j 2\n\n# Run some unit tests (you\
    \ can increase -j here too)\nmake -j 2 check\n\n# Install to prefix dir\nmake\
    \ install\n</code></pre>\n<p><strong>Common problems</strong></p>\n<ul>\n<li>\n\
    <p>Many system python installations do not come with the C API immediately available,\
    \ which prevents Portcullis from embedding python code.  We typically would recommend\
    \ installing anaconda3 as this would include the latest version of python, all\
    \ required python packages as well as the C API.  If you are running a debian\
    \ system and the C libraries are not available by default and you wish to use\
    \ the system python installation the you can install them using: <code>sudo apt-get\
    \ install python-dev</code>.  Also, if you have installed python to a custom location\
    \ please verify that the <em>bin</em> directors on the <em>PATH</em> environment\
    \ variable, and the lib (or lib64) directory is on the <em>LD_LIBRARY_PATH</em>\
    \ or <em>LD_RUN_PATH</em> as appropriate.</p>\n</li>\n<li>\n<p>If Portcullis is\
    \ failing at the <code>./autogen.sh</code> step you will likely need to install\
    \ autotools.  The following command should do this on MacOS: <code>brew install\
    \ autoconf automake libtool</code>.  On a debian system this can be done with:\
    \ <code>sudo apt-get install autoconf automake libtool</code>.</p>\n</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quickstart</h2>\n<p>After portcullis has been installed, the <code>portcullis</code>\
    \ executable should be available.  Typing <code>portcullis</code> or <code>portcullis\
    \ --help</code> at the command line will present you with the portcullis help\
    \ message.</p>\n<p>These modes are available:</p>\n<ul>\n<li>\n<strong>prep</strong>\
    \    - Prepares input data so that it is suitable for junction analysis</li>\n\
    <li>\n<strong>junc</strong>    - Calculates junction metrics for the prepared\
    \ data</li>\n<li>\n<strong>filter</strong>  - Separates alignments based on whether\
    \ they are likely to represent genuine splice junctions or not</li>\n<li>\n<strong>bamfilt</strong>\
    \ - Filters a BAM to remove any reads associated with invalid junctions</li>\n\
    <li>\n<strong>full</strong>    - Runs prep, junc, filter and optionally bamfilt\
    \ as a complete pipeline</li>\n</ul>\n<p>Typing <code>portcullis &lt;mode&gt;\
    \ --help</code> will bring up help and usage information specific to that mode.</p>\n\
    <p>In addition to portcullis, we provide a tool-suite for manipulating junction\
    \ files called junctools.  Typing <code>junctools --help</code> will provide you\
    \ with the program options.</p>\n<p>For much more information about portcullis'\
    \ capabilities and how to configure and run it, an online version of the manual\
    \ can be found here: <a href=\"https://portcullis.readthedocs.org/en/latest/\"\
    \ rel=\"nofollow\">https://portcullis.readthedocs.org/en/latest/</a>.</p>\n<h2>\n\
    <a id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Licensing</h2>\n\
    <p>GNU GPL V3.  See COPYING file for more details.</p>\n<h2>\n<a id=\"user-content-authors\"\
    \ class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Authors</h2>\n<ul>\n<li>Daniel\
    \ Mapleson</li>\n<li>Luca Venturini</li>\n<li>David Swarbreck</li>\n</ul>\n<p>See\
    \ AUTHORS file for more details.</p>\n<h2>\n<a id=\"user-content-acknowledgements\"\
    \ class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgements</h2>\n<p>Affiliation:\
    \ The Earlham Institute (EI)\nFunding: The Biotechnology and Biological Sciences\
    \ Research Council (BBSRC)</p>\n"
  stargazers_count: 28
  subscribers_count: 14
  topics: []
  updated_at: 1622038219.0
DoaneAS/atacflow:
  data_format: 2
  description: Analysis pipeline for ATACseq data using Nextflow
  filenames:
  - Singularity
  full_name: DoaneAS/atacflow
  latest_release: null
  readme: '

    <h1>

    <a id="user-content-atacflow" class="anchor" href="#atacflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>AtacFlow</h1>

    <h2>

    <a id="user-content-analysis-pipeline-for-atac-seq-data-using-nextflow" class="anchor"
    href="#analysis-pipeline-for-atac-seq-data-using-nextflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Analysis pipeline for
    ATAC-seq data using Nextflow</h2>

    <p>This pipeline inspired by and based on the <a href="https://www.encodeproject.org/atac-seq/"
    rel="nofollow">ENCODE ATAC-seq processubg pipeline</a> and

    the <em>prototype</em> ATAC-seq pipeline

    developed by <a href="https://github.com/kundajelab/atac_dnase_pipelines">Anshul
    Kundaje''s lab</a> at Stanford University</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <ul>

    <li>Install <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>

    </li>

    <li>Clone repository

    <ul>

    <li>using nextflow: <code>nextflow clone DoaneAS/atacflow ./</code>

    </li>

    <li>or using git: <code>git clone https://github.com/DoaneAS/atacflow.git</code>

    </li>

    </ul>

    </li>

    <li>Install conda dependencies:

    <pre><code>conda update conda

    conda env create --file requirements.atacFlow.yml

    conda env create --file deep.yml

    </code></pre>

    </li>

    </ul>

    <h2>

    <a id="user-content-setup-data" class="anchor" href="#setup-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup data</h2>

    <ul>

    <li>ATAC-seq reads go in <code>data/&lt;Sample&gt;/*_001.fastq.gz</code>

    <ul>

    <li>Concatenate read pairs per sample <code>parallel -j8 ''./bin/catlanes.sh {}''
    ::: data/Sample*</code>

    </li>

    </ul>

    </li>

    <li>Create sample index: <code>python bin/makeIndex.py</code>

    </li>

    </ul>

    <h2>

    <a id="user-content-execution" class="anchor" href="#execution" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Execution</h2>

    <pre><code>nextflow run -with-trace -with-dag flow.html main.nf --index sampleIndex.csv
    --genome hg38

    </code></pre>

    <ul>

    <li>supported genomes on panda WCM cluster:  hg38, mm10</li>

    </ul>

    '
  stargazers_count: 5
  subscribers_count: 1
  topics: []
  updated_at: 1620275923.0
EnriqueDoster/bioinformatic-nextflow-pipelines:
  data_format: 2
  description: Collection of bioinformatic pipelines written in nextflow
  filenames:
  - containers/Singularity
  - containers/Singularity.qiime2
  - containers/Singularity.RGI
  - containers/Singularity.cfsansnp
  full_name: EnriqueDoster/bioinformatic-nextflow-pipelines
  latest_release: null
  readme: '<h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667"
    alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/6b7af09ab5d3e54feb3acda4c7b70aef9718f2928a49a50c92ea6ce95e96b2f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e657874666c6f772d254532253839254135302e32352e312d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/Nextflow-%E2%89%A50.25.1-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p>The goal of many metagenomics studies is to characterize the content and relative
    abundance of sequences of interest from the DNA of a given sample or set of samples.
    You may want to know what is contained within your sample or how abundant a given
    sequence is relative to another.</p>

    <p>Often, metagenomics is performed when the answer to these questions must be
    obtained for a large number of targets where techniques like multiplex PCR and
    other targeted methods would be too cumbersome to perform. AmrPlusPlus can process
    the raw data from the sequencer, identify the fragments of DNA, and count them.
    It also provides a count of the polymorphisms that occur in each DNA fragment
    with respect to the reference database.</p>

    <p>Additionally, you may want to know if the depth of your sequencing (how many
    reads you obtain that are on target) is high enough to identify rare organisms
    (organisms with low abundance relative to others) in your population. This is
    referred to as rarefaction and is calculated by randomly subsampling your sequence
    data at intervals between 0% and 100% in order to determine how many targets are
    found at each depth. AmrPlusPlus can perform this analysis as well.</p>

    <p>With AmrPlusPlus, you will obtain count files for each sample that can be combined
    into a count matrix and analyzed using any statistical and mathematical techniques
    that can operate on a matrix of observations.</p>

    <h2>

    <a id="user-content-more-information" class="anchor" href="#more-information"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More
    Information</h2>

    <ul>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/requirements.md">Software
    Requirements</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/installation.md">Installation</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/usage.md">Usage</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/configuration.md">Configuration</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/output.md">Output</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/dependencies.md">Dependencies</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/contact.md">Contact</a></li>

    </ul>

    <h2>

    <a id="user-content-description-of-scripts" class="anchor" href="#description-of-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Description
    of scripts</h2>

    <p>main_qiime2.nf</p>

    <pre><code>nextflow run main_qiime2.nf --reads "/s/angus/index/projs/mega_tylan/concat_16S_LN/raw_data/*_{1,2}.fq"
    --output XIT_LN_qiime2 -profile local --metadata /media/AngusWorkspace/run_Jake/LN_metadata.tsv
    --classifier /media/AngusWorkspace/run_Jake/bioinformatic-nextflow-pipelines/gg-13-8-99-515-806-nb-classifier.qza
    -resume --threads 25

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619390181.0
EnriqueDoster/sing_biotools:
  data_format: 2
  description: Bioinformatic tools in a singularity container
  filenames:
  - containers/Singularity.etoki
  - containers/Singularity.lyveset
  - containers/Singularity
  full_name: EnriqueDoster/sing_biotools
  latest_release: null
  readme: '<h1>

    <a id="user-content-sing_biotools" class="anchor" href="#sing_biotools" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>sing_biotools</h1>

    <p>Bioinformatic tools in a singularity container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606287922.0
HanLabUNLV/hanlab_singularity_defs:
  data_format: 2
  description: definition files for containers used in Hanlab
  filenames:
  - singularity.SAD/SAD.def
  - singularity.R.3.6.3.phylo/R.3.6.3.phylo.def
  - singularity.Rconda/R.3.6.3.def
  - singularity.R.4.0.2.Bioc/R.4.0.2.Bioc.def
  - singularity.mkl/mkl.def
  - singularity.mkl/mkl.ubuntu.def
  - singularity.py37.ml.openblas/py37.ml.openblas.def
  - singularity.phylo/phylo.def
  - singularity.py37.ml.mkl/py37.ml.mkl.def
  - singularity.R.3.6.3.Bioc/R.3.6.3.Bioc.def
  - singularity.rnaseq/rnaseq.def
  full_name: HanLabUNLV/hanlab_singularity_defs
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc_mpi_cuda_singu_def_file" class="anchor" href="#hpc_mpi_cuda_singu_def_file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>hpc_mpi_cuda_singu_def_file</h1>

    <p>A collect of definition files to build images for singularity containers, which
    includes hpc benchmarks and mpis with cuda support.</p>

    <p><a href="https://singularity-hub.org/collections/4181" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1617130184.0
JCSDA-internal/containers:
  data_format: 2
  description: Contains tools for building and distributing the JEDI/JCSDA Containers
  filenames:
  - Singularity.clang-mpich-dev
  - Singularity.gnu-openmpi-dev
  - Singularity.tutorial
  full_name: JCSDA-internal/containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Containers</h1>

    <p>This repository contains tools for building Singularity and Charliecloud containers.</p>

    <p>Furthermore, since Intel Docker containers cannot be distributed through Docker
    Hub, they are also handled here.</p>

    <p>The instructions below are intended for the JEDI core team, who are responsible
    for maintaining JEDI containers and distributing them publicly or privately.</p>

    <p>However, since the JEDI core team cannot legally distribute intel containers
    for licensing reasons, JEDI users and developers are encouraged to build their
    own intel development container.</p>

    <p><a href="myIntel/Intel.md">See here for instructions on how to build your own
    JEDI Intel development container: Docker, Singularity, or Charliecloud</a></p>

    <h2>

    <a id="user-content-organization-of-repository" class="anchor" href="#organization-of-repository"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Organization
    of Repository</h2>

    <ul>

    <li>top-level directory: tools for building Singularity, Docker, and Charliecloud
    containers</li>

    <li>

    <code>vagrant</code>: tools for building Vagrant virtual machines that are provisioned
    to run JEDI containers</li>

    <li>

    <code>modulefiles</code>, <code>runscripts</code>: These directories contain sample
    modulefiles and batch scripts for running JEDI "Supercontainers" across nodes
    on HPC systems</li>

    <li>

    <code>myIntel</code> is intended to help users from the general JEDI community
    build their own JEDI intel development containers.</li>

    <li>

    <code>intel19</code> contains deprecated build tools for intel Parallel Studio.</li>

    <li>

    <code>examples</code> is a sandbox, containing instructive examples of how to
    implement features that may not be used now but might be used in the future.  An
    example is how to build writable singularity containers.   These scripts are not
    maintained; there is no guarantee that they will run as is.</li>

    </ul>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>In order to build Docker, Singularity, or Charliecloud containers, you will
    of course need to have the appropriate software installed, namely <code>docker</code>,
    <code>singularity</code>, or <code>charliecloud</code>.  Members of the JEDI core
    team can launch an AWS node with all of these pre-installed.  Or, you can install
    them yourself as described in the JEDI documentation.</p>

    <p>The scripts in this directory also assume that you have root privileges.</p>

    <p>Also, core developers often find it necessary to access feature or bugfix branches
    of the jedi stack for testing purposes.  So, the <code>build_container.sh</code>
    script uses the JCSDA-internal (private) jedi-stack repo.  For this reason, you
    need to provide an ssh key for access.  This script uses a generic academy ssh
    key to ensure that it has read-only access to selected JCSDA repositories.  If
    you do not have access to this key, you can replace it with another by changing
    the <code>KEY</code> variable in <code>build_containers.sh</code>.  But it is
    recommended to retain the read-only access.  You can build the <code>myIntel</code>
    container without an ssh key.</p>

    <p>Note: to build the tutorial container you have to copy the ssh key into the
    directory <code>ssh-key</code> and modify the singularity recipe file accordingly
    if it has a different name.</p>

    <h2>

    <a id="user-content-build-a-dev-container" class="anchor" href="#build-a-dev-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    a dev container</h2>

    <p>To build a Singularity, Charliecloud, and/or a Docker container, enter this
    and respond to the prompts to build the containers of your choice.</p>

    <div class="highlight highlight-source-shell"><pre>./build_containers.sh <span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span></pre></div>

    <p>where <code>&lt;name&gt;</code> matches one of the available Dockerfile extensions,
    e.g. <code>gnu-openmpi-dev</code>.  It also accepts an optional second argument
    to specify a tag.  The default tag is <code>beta</code>.</p>

    <p>For the the gnu and clang containers, the Singularity containers are built
    directly from the images on Docker Hub.  A Docker container will only be created
    if you choose to build a Charliecloud container, which is then built from the
    Docker container.</p>

    <p>For the <code>intel-impi-dev</code> container, a Docker file is always created
    and then the Singularity and Charliecloud containers are created from that.</p>

    <p>The intel Docker container is the one used for CI so it is kept relatively
    compact.  If you wish to add additional components such as Vtune, it is recommended
    you use the companion scripts in the <code>myIntel</code> directory.  These scripts
    are simplified in the root directory but they are intended for use by the general
    JEDI user and developer community.  The main simplification is that there is no
    need to supply an ssh key because those scripts only access the public jedi-stack
    repo.</p>

    <p>To build the tutorial container, just specify <code>tutorial</code> for the
    <code>&lt;name&gt;</code>.  The tutorial container is exclusively a Singularity
    container and uses GNU-OpenMPI: There are no clang or intel options and there
    are no Docker or Charliecloud containers created.</p>

    <p>The Singularity and Charliecloud container files will be placed in a subdirectory
    called <code>containers</code>.</p>

    <p>Note: building the Mellanox-enabled HPC container isn''t yet automated.  For
    this or other non-standard cases, you can edit the Dockerfiles, Singularity files,
    and scripts manually as needed.</p>

    <h2>

    <a id="user-content-test-the-container" class="anchor" href="#test-the-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Test
    the container</h2>

    <p>Before distributing a container, it''s always important to test it.  A good
    test is usually to enter the container and then build and test fv3-bundle.</p>

    <p>To enter the Singularity container, enter:</p>

    <div class="highlight highlight-source-shell"><pre>singularity shell -e <span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span>.sif</pre></div>

    <p>And, for CharlieCloud, you can do this:</p>

    <div class="highlight highlight-source-shell"><pre>mkdir -p <span class="pl-k">~</span>/ch-jedi

    <span class="pl-c1">cd</span> <span class="pl-k">~</span>/ch-jedi

    ch-tar2dir <span class="pl-k">&lt;</span>path-to-tarfile<span class="pl-k">&gt;</span>/ch-jedi-gnu-openmpi-dev.tar.gz
    <span class="pl-c1">.</span>

    ch-run ch-jedi-gnu-openmpi-dev -- bash</pre></div>

    <h2>

    <a id="user-content-distribute-the-latest-container" class="anchor" href="#distribute-the-latest-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Distribute
    the latest container</h2>

    <p>The latest Singularity containers are made available on Sylabs Cloud, the latest
    Charliecloud containers are made available on a public AWS S3 bucket, and the
    latest intel containers are made available on a private AWS S3 bucket.  The purpose
    of the <code>push_containers.sh</code> script is to push the new container to
    these distribution sites.</p>

    <p>The <code>beta</code> tag is a special case.  If the tag is <code>beta</code>,
    it is assumed that, after it passes tests, this container is ready to be deployed
    as <code>latest</code>.  In this case, a copy of the current <code>latest</code>
    container is saved with the tag <code>revert</code>.</p>

    <p>So, the typical workflow would be to enter</p>

    <div class="highlight highlight-source-shell"><pre>./push_containers.sh <span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span></pre></div>

    <p>As with <code>build_containers.sh</code>, <code>push_containers.sh</code> accepts
    an optional second argument which is a tag.  This is sometimes useful for experimental
    cases but is not part of the normal workflow.</p>

    <p>For instructions on how to download these containers, see <a href="https://jointcenterforsatellitedataassimilation-jedi-docs.readthedocs-hosted.com/en/latest/using/jedi_environment/containers.html#available-containers"
    rel="nofollow">the JEDI Documentation</a>.</p>

    <h2>

    <a id="user-content-tagged-releases" class="anchor" href="#tagged-releases" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tagged Releases</h2>

    <p>Most developers use the latest development containers but it''s also useful
    to have tagged containers that accompany JEDI releases.  This is particularly
    relevant for scientific users (as opposed to developers) who may wish to use tagged
    releases and containers for reproducibility in research.  Tagged containers can
    also be used to provide stability for operational or Near-Real-Time (NRT) workflows.</p>

    <p>Sylabs cloud has a storage quota (currently 11 GB) that would be quickly overwhelmed
    if we were to store many release containers there.  So, this is reserved for "latest"
    and "revert".</p>

    <p>Tagged singularity containers are distributed on the <a href="http://data.jcsda.org/pages/containers.html"
    rel="nofollow">JCSDA Public Container Repository</a> along with the latest and
    tagged Charliecloud containers.  Tagged docker release containers can be obtained
    from Docker Hub.  For example:</p>

    <div class="highlight highlight-source-shell"><pre>docker pull jcsda/docker-gnu-openmpi-dev:v1.0.0</pre></div>

    '
  stargazers_count: 2
  subscribers_count: 5
  topics: []
  updated_at: 1622674490.0
JoshLoecker/MAPT:
  data_format: 2
  description: The purpose of this project is to map Oxford Nanopore Sequencing data
    down to the species level
  filenames:
  - setup/Singularity
  full_name: JoshLoecker/MAPT
  latest_release: null
  readme: '<h1>

    <a id="user-content-mapt" class="anchor" href="#mapt" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MAPT</h1>

    <p><a href="https://github.com/JoshLoecker/MAPT/wiki">Please view the Wiki</a>
    for more information.</p>

    <h3>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h3>

    <p>If you need help, have questions, or have feature ideas please <a href="https://github.com/JoshLoecker/MAPT/issues">open
    a new issue</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620064890.0
Lizhen0909/LSHVec:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity
  full_name: Lizhen0909/LSHVec
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-lshvec-a-vector-representation-of-dna-sequences-using-locality-sensitive-hashing\"\
    \ class=\"anchor\" href=\"#lshvec-a-vector-representation-of-dna-sequences-using-locality-sensitive-hashing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>LSHVec: A Vector Representation of DNA Sequences Using Locality Sensitive\
    \ Hashing</h1>\n<h2>\n<a id=\"user-content-summary\" class=\"anchor\" href=\"\
    #summary\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Summary</h2>\n<p>LSHVec is a k-mer/sequence embedding/classfication\
    \ software which extends <a href=\"https://fasttext.cc/\" rel=\"nofollow\">FastText</a>\
    \ . It applies LSH (Locality Sensitive Hashing) to reduce the size of k-mer vocabulary\
    \ and improve the performance of embedding.</p>\n<p>Besides building from source\
    \ code, LSHVec can run using docker or singularity.</p>\n<p>Please refer to <a\
    \ href=\"https://www.biorxiv.org/content/10.1101/726729v1\" rel=\"nofollow\">A\
    \ Vector Representation of DNA Sequences Using Locality Sensitive Hashing</a>\
    \ for the idea and experiments.</p>\n<p>There are also some pretained models that\
    \ can be used, please see <a href=\"https://github.com/Lizhen0909/PyLSHvec/blob/master/README.md\"\
    >PyLSHvec</a> for details.</p>\n<h2>\n<a id=\"user-content-requirements\" class=\"\
    anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<p>Here is the\
    \ environment I worked on.  Other versions may also work. Python 3 should work,\
    \ but I don't use it a lot.</p>\n<ol>\n<li>Linux, gcc with C++11</li>\n<li>Python\
    \ 2.7 or Python 3.6 or 3.7\n<ul>\n<li>joblib 0.12.4</li>\n<li>tqdm 4.28.1</li>\n\
    <li>numpy 1.15.0</li>\n<li>pandas 0.23.4</li>\n<li>sklearn 0.19.1 (only for evaluation)</li>\n\
    <li>MulticoreTSNE (only for visualization)</li>\n<li>cython 0.28.5</li>\n<li>csparc\
    \ (included)</li>\n</ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-build-from-source\"\
    \ class=\"anchor\" href=\"#build-from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build from Source</h2>\n<ul>\n\
    <li>\n<p>clone from git</p>\n<p><code>git clone https://LizhenShi@bitbucket.org/LizhenShi/lshvec.git</code></p>\n\
    <p><code>cd lshvec</code></p>\n</li>\n<li>\n<p>install csparc which wraps a c\
    \ version of k-mer generator I used in another project</p>\n<p>for python 2.7</p>\n\
    <p><code>pip install pysparc-0.1-cp27-cp27mu-linux_x86_64.whl</code></p>\n<p>or\
    \ for python 3.6</p>\n<p><code>pip install pysparc-0.1-cp36-cp36m-linux_x86_64.whl</code></p>\n\
    <p>or for python 3.7</p>\n<p><code>pip install pysparc-0.1-cp37-cp37m-linux_x86_64.whl</code></p>\n\
    </li>\n<li>\n<p>make</p>\n<p><code>make</code></p>\n</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-jupyter-notebook-examples\" class=\"anchor\" href=\"#jupyter-notebook-examples\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Jupyter Notebook Examples</h2>\n<p>A toy example, which is laptop\
    \ friendly and should finish in 10 minutes,  can be found in <a href=\"notebook/Tutorial_Toy_Example.ipynb\"\
    >Tutorial_Toy_Example.ipynb</a>. Because of randomness the result may be different.</p>\n\
    <p><a href=\"notebook/Tutorial_Toy_Example.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"notebook/Tutorial_Toy_Example.png\" alt=\"Tutorial_Toy_Example\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>A practical example which uses ActinoMock\
    \ Nanopore data can be found at <a href=\"notebook/Tutorial_ActinoMock_Nanopore.ipynb\"\
    >Tutorial_ActinoMock_Nanopore.ipynb</a>. The notebook ran on a 16-core 64G-mem\
    \ node and took a few hours (I think 32G mem should work too).</p>\n<p>\u200B\t\
    \t\t\t\t\t <a href=\"notebook/Tutorial_ActinoMock_Nanopore.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"notebook/Tutorial_ActinoMock_Nanopore.png\"\
    \ alt=\"Tutorial_ActinoMock_Nanopore\" style=\"max-width:100%;\"></a></p>\n<h2>\n\
    <a id=\"user-content-command-line-options\" class=\"anchor\" href=\"#command-line-options\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Command line options</h2>\n<h3>\n<a id=\"user-content-fastqtoseqpy\"\
    \ class=\"anchor\" href=\"#fastqtoseqpy\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>fastqToSeq.py</h3>\n<p>convert\
    \ a fastq file to a seq file</p>\n<pre><code>python fastqToSeq.py -i &lt;fastq_file&gt;\
    \ -o &lt;out seq file&gt; -s &lt;1 to shuffle, 0 otherwise&gt;\n</code></pre>\n\
    <h3>\n<a id=\"user-content-hashseqpy\" class=\"anchor\" href=\"#hashseqpy\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>hashSeq.py</h3>\n\
    <p>Encode reads in a seq file use an encoding method.</p>\n<pre><code>python hashSeq.py\
    \ -i &lt;seq_file&gt; --hash &lt;fnv or lsh&gt; -o &lt;outfile&gt; [-k &lt;kmer_size&gt;]\
    \ [--n_thread &lt;n&gt;] [--hash_size &lt;m&gt;] [--batch_size &lt;n&gt;] [--bucket\
    \ &lt;n&gt;] [--lsh_file &lt;file&gt;] [--create_lsh_only]\n\n  --hash_size &lt;m&gt;:\
    \        only used by lsh which defines 2^m bucket.\n  --bucket &lt;n&gt;:   \
    \        number of bucket for hash trick, useless for onehot.\n   \t\t\t\t   \
    \       For fnv and lsh it limits the max number of words.\n   \t\t\t\t      \
    \    For lsh the max number of words is min(2^m, n).\n  --batch_size &lt;b&gt;:\
    \       how many reads are processed at a time. A small value uses less memory.\n\
    </code></pre>\n<h3>\n<a id=\"user-content-lshvec\" class=\"anchor\" href=\"#lshvec\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>lshvec</h3>\n<p>Please refer to <a href=\"https://fasttext.cc/docs/en/options.html\"\
    \ rel=\"nofollow\">fasttext options</a>.  However note that options of <code>wordNgrams</code>,\
    \ <code>minn</code>,<code>maxn</code> does not work with lshvec.</p>\n<h2>\n<a\
    \ id=\"user-content-example-of-docker-run\" class=\"anchor\" href=\"#example-of-docker-run\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Example of Docker Run</h2>\n<p>Pull from docker hub:</p>\n<pre><code>docker\
    \ pull lizhen0909/lshvec:latest\n</code></pre>\n<p>Assume <code>data.fastq</code>\
    \ file is in folder <code>/path/in/host</code>.</p>\n<p>convert fastq to a seq\
    \ file:</p>\n<pre><code>docker run -v /path/in/host:/host lshvec:latest bash -c\
    \ \"cd /host &amp;&amp; fastqToSeq.py  -i data.fastq -o data.seq\"\n</code></pre>\n\
    <p>create LSH:</p>\n<pre><code>docker run -v /path/in/host:/host lshvec:latest\
    \ bash -c \"cd /host &amp;&amp; hashSeq.py -i data.seq --hash lsh -o data.hash\
    \ -k 15\"\n</code></pre>\n<p>run lshvec:</p>\n<pre><code>docker run -v /path/in/host:/host\
    \ lshvec:latest bash -c \"cd /host &amp;&amp; lshvec skipgram -input data.hash\
    \ -output model\"\n</code></pre>\n<h2>\n<a id=\"user-content-example-of-singularity-run\"\
    \ class=\"anchor\" href=\"#example-of-singularity-run\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Example of Singularity\
    \ Run</h2>\n<p>When running using Singularity, it is probably in an HPC environment.\
    \ The running is similar to docker. However depending on the version of singularity,\
    \ commands and paths might be different, especially from 2.x to 3.x. Here is an\
    \ example for version 2.5.0.</p>\n<p>Also it is better to specify number of threads,\
    \ otherwise max number of cores will be used which is not desired in HPC environment.</p>\n\
    <p>Pull from docker hub:</p>\n<pre><code>singularity pull --name lshvec.sif shub://Lizhen0909/LSHVec\n\
    </code></pre>\n<p>Put <code>data.fastq</code> file is in host <code>/tmp</code>,\
    \  since Singularity automatically mount <code>/tmp</code> folder.</p>\n<p>convert\
    \ fastq to a seq file:</p>\n<pre><code>singularity run /path/to/lshvec.sif bash\
    \ -c \"cd /tmp &amp;&amp; fastqToSeq.py  -i data.fastq -o data.seq\"\n</code></pre>\n\
    <p>create LSH:</p>\n<pre><code>singularity run /path/to/lshvec.sif bash -c \"\
    cd /tmp &amp;&amp; hashSeq.py -i data.seq --hash lsh -o data.hash -k 15 --n_thread\
    \ 12\"\n</code></pre>\n<p>run lshvec:</p>\n<pre><code>singularity run /path/to/lshvec.sif\
    \ bash -c \"cd /tmp &amp;&amp; lshvec skipgram -input data.hash -output model\
    \ -thread 12\"\n</code></pre>\n<h2>\n<a id=\"user-content-questions\" class=\"\
    anchor\" href=\"#questions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Questions</h2>\n<ul>\n<li>\n<code>lshvec</code>\
    \ gets stuck at <code>Read xxxM words</code>\n</li>\n</ul>\n<p>Search <code>MAX_VOCAB_SIZE</code>\
    \ in the source code and change it to a bigger one.  When a word's index is bigger\
    \ than that number, a loop is carried to query it, which is costly. The number\
    \ is 30M in FastText which is good for languages. But it is too small for k-mers.\
    \ The number has been already increased to 300M in FastSeq. But for large and/or\
    \ high-error-rate data, it may be still not enough.</p>\n<ul>\n<li>\n<p>I have\
    \ big data</p>\n<p>hashSeq reads all data into memory to sample k-mers for hyperplanes.\
    \ If data is too big it may not fit into memory. One can</p>\n<ol>\n<li>Try sampling.\
    \ DNA reads generally have high coverage. Such high coverage may not be necessary.</li>\n\
    <li>Or use <code>create_hash_only</code> to create lsh on a small (sampled) data;\
    \ then split your data into multiple files and run hashSeq with <code>lsh_file</code>\
    \ option on many nodes.</li>\n</ol>\n</li>\n<li>\n<p>core dumped when hashing</p>\n\
    <p>Error like</p>\n<pre><code>terminate called after throwing an instance of 'std::out_of_range'\n\
    what(): map::at\nAborted (core dumped)\n</code></pre>\n<p>mostly because a sequence\
    \ contains characters other than ACGTN. So please convert non-ACGT characters\
    \ to N's.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\"\
    \ href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>License</h2>\n<p>Inherit license from FastText which\
    \ is BSD License</p>\n"
  stargazers_count: 3
  subscribers_count: 1
  topics:
  - locality-sensitive-hashing
  - sequence-vector
  - classfication
  updated_at: 1610550474.0
Lizhen0909/PyLSHvec:
  data_format: 2
  description: LSHVec pre-trained models and its Python bindings
  filenames:
  - singularity/Singularity
  full_name: Lizhen0909/PyLSHvec
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-lshvec-pre-trained-models-and-its-python-bindings\"\
    \ class=\"anchor\" href=\"#lshvec-pre-trained-models-and-its-python-bindings\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>LSHVec pre-trained models and its Python bindings</h1>\n<h2>\n<a id=\"\
    user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Summary</h2>\n\
    <p>This repository presents a few of pre-tained models with JLSHVec (which is\
    \ a rewritten java version of  <a href=\"https://github.com/Lizhen0909/LSHVec\"\
    >LSHVec</a>).  See <a href=\"#remark\">Remark</a> for technical details.</p>\n\
    <p>Python codes and examples to uses these models are also provided.</p>\n<h2>\n\
    <a id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\
    <ol>\n<li>Python 3.6</li>\n<li>cython&gt;=0.28.5</li>\n<li>Jnius &gt;=1.1.0</li>\n\
    <li>java &gt;=1.8</li>\n</ol>\n<h2>\n<a id=\"user-content-install\" class=\"anchor\"\
    \ href=\"#install\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Install</h2>\n<h3>\n<a id=\"user-content-build-from-source\"\
    \ class=\"anchor\" href=\"#build-from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>build from source</h3>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/Lizhen0909/PyLSHvec.git\
    \ <span class=\"pl-k\">&amp;&amp;</span> <span class=\"pl-c1\">cd</span> PyLSHvec\
    \ <span class=\"pl-k\">&amp;&amp;</span> python setup.py install</pre></div>\n\
    <h3>\n<a id=\"user-content-or-use-pip\" class=\"anchor\" href=\"#or-use-pip\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>or use pip</h3>\n<div class=\"highlight highlight-source-shell\"><pre>pip\
    \ install pylshvec</pre></div>\n<h3>\n<a id=\"user-content-or-use-docker\" class=\"\
    anchor\" href=\"#or-use-docker\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>or use docker</h3>\n<div class=\"\
    highlight highlight-source-shell\"><pre>docker pull lizhen0909/pylshvec</pre></div>\n\
    <h3>\n<a id=\"user-content-or-use-singularity-3\" class=\"anchor\" href=\"#or-use-singularity-3\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>or use singularity 3</h3>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull --name pylshvec.sif shub://Lizhen0909/PyLSHvec</pre></div>\n\
    <h2>\n<a id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to use</h2>\n<p>Put things simply, just</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">pylshvec</span> <span class=\"pl-k\">import</span> <span class=\"pl-c1\"\
    >*</span>\n\n<span class=\"pl-c\">#here needs jlshvec jar file, download it first</span>\n\
    <span class=\"pl-en\">set_lshvec_jar_path</span>(<span class=\"pl-s\">\"/mnt/jlshvec-assembly-0.1.jar\"\
    </span>)\n\n<span class=\"pl-c\">#since vector model is usually large, set a big\
    \ java memory limit is preferred. </span>\n<span class=\"pl-en\">add_java_options</span>(<span\
    \ class=\"pl-s\">\"-Xmx32G\"</span>)\n\n<span class=\"pl-c\">#here need model\
    \ file and lsh function file, download them first</span>\n<span class=\"pl-c\"\
    >#use help(model) to see all the methods and constructor options </span>\n<span\
    \ class=\"pl-s1\">model</span><span class=\"pl-c1\">=</span> <span class=\"pl-v\"\
    >LSHVec</span>(<span class=\"pl-s1\">model_file</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s\">\"/mnt/refdb_viruses_model_gs_k23_l3000_rand_model_299\"</span>,\
    \ \n              <span class=\"pl-s1\">hash_file</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s\">\"/mnt/lsh_nt_NonEukaryota_k23_h25.crp\"</span>)\n\
    \n<span class=\"pl-s1\">reads</span> <span class=\"pl-c1\">=</span> [<span class=\"\
    pl-s\">'ACGTACGT.....'</span>, <span class=\"pl-s\">'ACGTACGT.....'</span>, <span\
    \ class=\"pl-s\">'ACGTACGT.....'</span>, <span class=\"pl-s\">'ACGTACGT.....'</span>,\
    \ ....]\n\n<span class=\"pl-s1\">predicts</span> <span class=\"pl-c1\">=</span>\
    \ <span class=\"pl-s1\">model</span>.<span class=\"pl-en\">predict</span>(<span\
    \ class=\"pl-s1\">reads</span>)</pre></div>\n<p>For more complete examples please\
    \ see the notebooks (see <a href=\"#download\">Download</a> for minimum memory\
    \ requirement):</p>\n<p><a href=\"notebook/example_use_virus_classfication_model.ipynb\"\
    >example_use_virus_classfication_model.ipynb</a></p>\n<p><a href=\"notebook/example_use_bacteria_classfication_model.ipynb\"\
    >example_use_bacteria_classfication_model.ipynb</a></p>\n<p><a href=\"notebook/example_use_vectors_in_bacteria_classfication_model.ipynb\"\
    >example_use_vectors_in_bacteria_classfication_model.ipynb</a></p>\n<p><a href=\"\
    notebook/example_use_Illumina_bacteria_classfication_model.ipynb\">example_use_Illumina_bacteria_classfication_model.ipynb</a></p>\n\
    <p><a href=\"notebook/example_use_Pacbio_bacteria_classfication_model.ipynb\"\
    >example_use_Pacbio_bacteria_classfication_model.ipynb</a></p>\n<h3>\n<a id=\"\
    user-content-use-docker\" class=\"anchor\" href=\"#use-docker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use\
    \ Docker</h3>\n<p>Assume you put your data in /mnt/data and your notebook in /mnt/notebook.</p>\n\
    <ul>\n<li>run python or ipython</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker run -v /mnt/data:/data -it lizhen0909/pylshvec python <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span>or ipython</span></pre></div>\n<ul>\n<li>run\
    \ Jupyter notebook</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker run -v /mnt/data:/data -v /mnt/notebook:/notebook -p 8888:8888  -it\
    \ lizhen0909/pylshvec jupyter_notebook</pre></div>\n<p>Find connection url in\
    \ the console output.</p>\n<h3>\n<a id=\"user-content-use-singularity\" class=\"\
    anchor\" href=\"#use-singularity\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Use Singularity</h3>\n<p>Since singularity\
    \ maps the $HOME directory, here just assumes data/model are going to locate in\
    \ $HOME. Otherwise, you need map the directories like docker.</p>\n<ul>\n<li>run\
    \ python or ipython</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity run pylshvec.sif python <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span>the nrun any pylshvec code </span></pre></div>\n<ul>\n<li>run Jupyter\
    \ notebook</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span>It should work, however singularity\
    \ maps too many things that host settings may affect the notebook</span>\nsingularity\
    \ run  --bind <span class=\"pl-smi\">$HOME</span>/notebook:/notebook pylshvec.sif\
    \ jupyter_notebook </pre></div>\n<h2>\n<a id=\"user-content-download\" class=\"\
    anchor\" href=\"#download\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Download</h2>\n<h3>\n<a id=\"user-content-jlshvec-jar-file\"\
    \ class=\"anchor\" href=\"#jlshvec-jar-file\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>JLSHVec jar file</h3>\n<p>The\
    \ pre-trained models were trained with a rewritten  <a href=\"https://github.com/Lizhen0909/LSHVec\"\
    >LSHVec</a> in java.\nThe assembly jar file is needed to load the models.</p>\n\
    <p><a href=\"https://www.amazon.com/clouddrive/share/4NiogpuW1lzBMyGmMlkrDbjhSMYpQgWjW5GUcKFR7Q6\"\
    \ rel=\"nofollow\">Download jlshvec-assembly-0.1.jar</a></p>\n<p><strong>md5sum</strong>:\
    \ aeb207b983b3adc27e14fd9c431e2130</p>\n<h3>\n<a id=\"user-content-pre-trained-models\"\
    \ class=\"anchor\" href=\"#pre-trained-models\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pre-trained models</h3>\n<p><strong>Be\
    \ Warned</strong> that like all the machine learning models, the model cannot\
    \ preform better beyond the data. If your data is significant other than the pre-trained\
    \ model data, training your own model is preferred.</p>\n<p>Here are issues I\
    \ can think of:</p>\n<ul>\n<li>Some NCBI taxonomy id may never be predicted since\
    \ not all ids have train data.</li>\n<li>Data is not balanced. Some ids (e.g.\
    \ a specified species) have much more data than others, which makes prediction\
    \ may prefer to the rich-data ids.</li>\n<li>Strain (even some species) prediction\
    \ is terrible. Don't expect it.</li>\n</ul>\n<h4>\n<a id=\"user-content-refdb-viruses-classfication-model\"\
    \ class=\"anchor\" href=\"#refdb-viruses-classfication-model\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RefDB\
    \ viruses classfication model</h4>\n<p>Trainned with 9.3k viruses assemblies of\
    \ RefDB. Minimum Java memory: 16G.</p>\n<ul>\n<li>\n<p>model file: <a href=\"\
    https://www.amazon.com/clouddrive/share/RmoJ1lduzlqstAJFnKg0aAlx82AyCjnzKncfGjQIQMg\"\
    \ rel=\"nofollow\">refdb_viruses_model_gs_k23_l3000_rand_model_299</a> [size:\
    \ 5.3G]</p>\n<p><strong>md5sum</strong> 2502b284b336734300c2297d23d1d349</p>\n\
    </li>\n<li>\n<p>hash function file: <a href=\"https://www.amazon.com/clouddrive/share/6ZNvMXMy30b4vc0RYNVG1lbf1ih8WgpoQ9w4lX91IXy\"\
    \ rel=\"nofollow\">lsh_nt_NonEukaryota_k23_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ 5eea8a98d224b7ff505091bd483ca75c</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-refdb-bacteria-classfication-model\"\
    \ class=\"anchor\" href=\"#refdb-bacteria-classfication-model\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RefDB\
    \ bacteria classfication model</h4>\n<p>Trainned with 42k bacteria assemblies\
    \ of RefDB. Minimum Java memory: 32G.</p>\n<ul>\n<li>\n<p>model file: <a href=\"\
    https://www.amazon.com/clouddrive/share/LoXz6k229SwYuElPTHvu0SSJOq56nJenvBbOTGVeb9a\"\
    \ rel=\"nofollow\">refdb_bacteria_model_gs_k23_l3000_rand_model_214</a> [size:\
    \ 11G]</p>\n<p><strong>md5sum</strong> 402e9a286b71068999caa9766b2dbf8c</p>\n\
    </li>\n<li>\n<p>hash function file: <a href=\"https://www.amazon.com/clouddrive/share/6ZNvMXMy30b4vc0RYNVG1lbf1ih8WgpoQ9w4lX91IXy\"\
    \ rel=\"nofollow\">lsh_nt_NonEukaryota_k23_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ 5eea8a98d224b7ff505091bd483ca75c</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-genbank-bacteria-and-viruses-classfication-model-illumina-simulation\"\
    \ class=\"anchor\" href=\"#genbank-bacteria-and-viruses-classfication-model-illumina-simulation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>GenBank bacteria and viruses classfication model (Illumina Simulation)</h4>\n\
    <p>Trainned with 54k assemblies from GenBank. <strong>Only one assembly was sampled\
    \ for each species.</strong> Because viruses data is too samll compared to bateria,\
    \ it rarely predicts any viruses. Just take it as a bateria model.</p>\n<p><a\
    \ href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3278762/\" rel=\"nofollow\"\
    >art_illumina</a> was used to simulate the paired-end reads with length of 150,\
    \ mean size of 270 and stddev of 27.</p>\n<p>Minimum Java memory: 48G.</p>\n<ul>\n\
    <li>\n<p>model file: <a href=\"https://www.amazon.com/clouddrive/share/zQnu2ti1vfBMGcXrRqsohgfzuaYzZs4HGESP58vobRn\"\
    \ rel=\"nofollow\">genbank_model_ill_k23_model_299</a> [size: 12G]</p>\n<p><strong>md5sum</strong>\
    \ d6b117a4c7ffe4f25e6c532a88bb3a47</p>\n</li>\n<li>\n<p>hash function file: <a\
    \ href=\"https://www.amazon.com/clouddrive/share/efWceiTHId4EVhY1DEppmW6amyBQoEt3iIU6oW5FbcX\"\
    \ rel=\"nofollow\">lsh_CAMI2_illumina_k23_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ 706633919e347f920ce6ab3277091efb</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-genbank-bacteria-and-viruses-classfication-model-pacbio-simulation\"\
    \ class=\"anchor\" href=\"#genbank-bacteria-and-viruses-classfication-model-pacbio-simulation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>GenBank bacteria and viruses classfication model (Pacbio Simulation)</h4>\n\
    <p>Trainned with 54k assemblies from GenBank. <strong>Only one assembly was sampled\
    \ for each species.</strong> Because viruses data is too samll compared to bateria,\
    \ it rarely predicts any viruses. Just take it as a bateria model.</p>\n<p><a\
    \ href=\"https://github.com/pfaucon/PBSIM-PacBio-Simulator\">pbsim</a> was used\
    \ to simulate the pacbio reads with Continuous Long Read (CLR) profile, mean size\
    \ of 3000 and stddev of 1000.</p>\n<p>Minimum Java memory: 16G.</p>\n<ul>\n<li>\n\
    <p>model file: <a href=\"https://www.amazon.com/clouddrive/share/OmU9cmVKknacpt0W9HpI6QY2jXC17dQpWaaERpLhOGl\"\
    \ rel=\"nofollow\">genbank_model_pb_k9_model_299</a> [size: 121M]</p>\n<p><strong>md5sum</strong>\
    \ 351275531493a4866be4afcd9df3932c</p>\n</li>\n<li>\n<p>hash function file: <a\
    \ href=\"https://www.amazon.com/clouddrive/share/zw4JwJCE4Lst5I4q36ijwrhc3db9rHYsCuyQ4KkihVC\"\
    \ rel=\"nofollow\">lsh_CAMI2_pacbio_k9_h25.crp</a></p>\n<p><strong>md5sum</strong>\
    \ df7ee38cf8b58d5f8034bb9b266e3334</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-sample-data\"\
    \ class=\"anchor\" href=\"#sample-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sample data</h3>\n<ul>\n<li>\n\
    <p>ActinoMock Nanopore Sample [size: 500M].</p>\n<p>The data is used in example\
    \ notebook <a href=\"notebook/example_use_vectors_in_bacteria_classfication_model.ipynb\"\
    >example_use_vectors_in_bacteria_classfication_model.ipynb</a>.</p>\n<p><a href=\"\
    http://ww2.cs.fsu.edu/~lshi/ActinoMock_Nanopore.seq.gz\" rel=\"nofollow\">Download\
    \ from FSU</a>\n\u2003\u2003\n<a href=\"https://www.amazon.com/clouddrive/share/eTIKYVLckXUCMnMQSpO8TCqZOwekmBrx23ZhMa3XO8d\"\
    \ rel=\"nofollow\">Download from Amazon Drive</a></p>\n<p><strong>md5sum</strong>:\
    \ b7f3e55438fdc05920aee693a98ded2e</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-remark\"\
    \ class=\"anchor\" href=\"#remark\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Remark</h2>\n<h3>\n<a id=\"user-content-what-is-jlshvec--why-jlshvec-instead-of-lshvec\"\
    \ class=\"anchor\" href=\"#what-is-jlshvec--why-jlshvec-instead-of-lshvec\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>What\
    \ is JLSHVec ? Why JLSHVec instead of LSHVec?</h3>\n<p>JLSHVec is a rewritten\
    \ version of <a href=\"https://github.com/Lizhen0909/LSHVec\">LSHVec</a> in Java\
    \ language.</p>\n<p>When we use LSHVec with big dataset (e.g. <a href=\"https://www.ncbi.nlm.nih.gov/genbank/\"\
    \ rel=\"nofollow\">GenBank</a>, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/12652131\"\
    \ rel=\"nofollow\">RefDB</a>), we found that LSHVec is hard to process such a\
    \ big data size.</p>\n<p>The reason is that LSHVec which inherits from <a href=\"\
    https://fasttext.cc/\" rel=\"nofollow\">FastText</a> requires the input is text\
    \ format separated by white space and then loads all the text in memory. This\
    \ is acceptable for natural languages since the data size is at most tens GBs.</p>\n\
    <p>However in LSHVec k-mers are used instead of words. Suppose we want to train\
    \ a k-mer embedding of simulated Illumina reads with RefDB bacteria assemblies\
    \ (about 500G genetic bits). The number of kmers is about D*n, where D is the\
    \ assembly data size and n is coverage. In our case, assuming n=10 and k=23, the\
    \ number of kmers is 5T and requires a disk space of 125TB, of which the data\
    \ preparation and loading process will take forever.</p>\n<h3>\n<a id=\"user-content-how-were-jlshvec-pre-trained-models-trained-\"\
    \ class=\"anchor\" href=\"#how-were-jlshvec-pre-trained-models-trained-\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How\
    \ were JLSHVec pre-trained models trained ?</h3>\n<p>First we prepared a <a href=\"\
    https://rocksdb.org/\" rel=\"nofollow\">RockDB</a> for the reference sequences\
    \ (e.g. all bacteria assemblies in RefDB).</p>\n<p>Then we have several nodes\
    \ to train the model: one node (train node) trains the model and others (hash\
    \ nodes) generate and hash kmers. The nodes communicates by passing <a href=\"\
    https://developers.google.com/protocol-buffers\" rel=\"nofollow\">protocol-buf</a>\
    \ message with a <a href=\"https://redis.io/\" rel=\"nofollow\">Redis</a> server.</p>\n\
    <p>A hash node randomly reads reference sequences from the RockDB, simulates (e.g.\
    \ simulations Illumina, Pacbio, Gold Standard) reads, generates kmers and hashes\
    \ them, then feeds the hashed-kmer-sequences to a Redis queue.</p>\n<p>Train node\
    \ reads from the Redis queue and does jobs of embedding or classification training.\
    \  Our training code supports hierarchical softmax using NCBI taxonomy tree, which\
    \ is essential for multi-label(an instance can have a label for each rank) and\
    \ multi-class(an instance can only have one label for a rank)  mixture classification\
    \ model.</p>\n<h2>\n<a id=\"user-content-citation\" class=\"anchor\" href=\"#citation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Citation</h2>\n<p>Please cite:</p>\n<p><a href=\"https://www.biorxiv.org/content/biorxiv/early/2019/08/06/726729.full.pdf\"\
    \ rel=\"nofollow\">A Vector Representation of DNA Sequences Using Locality Sensitive\
    \ Hashing</a></p>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"\
    #license\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h2>\n<p><a href=\"https://www.gnu.org/licenses/gpl-3.0\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/400c4e52df43f6a0ab8a89b74b1a78d1a64da56a7848b9110c9d2991bb7c3105/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1572883846.0
Lizhen0909/graph_clustering_toolkit:
  data_format: 2
  description: graph clustering toolkit
  filenames:
  - singularity/Singularity
  full_name: Lizhen0909/graph_clustering_toolkit
  latest_release: null
  readme: "<h2>\n<a id=\"user-content-graph-clustering-toolkit\" class=\"anchor\"\
    \ href=\"#graph-clustering-toolkit\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Graph Clustering Toolkit</h2>\n\
    <h3>\n<a id=\"user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Summary:</h3>\n\
    <p>The toolkit collects many academic graph clustering programs and make them\
    \ avaliable as package. Docker image is provided for easy access.</p>\n<h3>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation:</h3>\n\
    <p>Use docker is convenient as</p>\n<pre><code>docker pull lizhen0909/graph_clustering_toolkit\n\
    </code></pre>\n<p>For more information, please refer to <a href=\"https://lizhen0909.github.io/graph_clustering_toolkit/\"\
    \ rel=\"nofollow\">online document</a> for a full description</p>\n<h3>\n<a id=\"\
    user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage:</h3>\n\
    <p>Start python from docker:</p>\n<pre><code>docker run -it --rm lizhen0909/graph_clustering_toolkit\
    \ python\n</code></pre>\n<p>Run the script from the command line:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> <span\
    \ class=\"pl-s1\">gct</span>\n<span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">gct</span>.<span class=\"pl-s1\">dataset</span> <span class=\"pl-k\">import</span>\
    \ <span class=\"pl-s1\">random_dataset</span>\n<span class=\"pl-c\">#create a\
    \ random graph use LFR generator</span>\n<span class=\"pl-s1\">ds</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s1\">random_dataset</span>.<span class=\"\
    pl-en\">generate_undirected_unweighted_random_graph_LFR</span>(<span class=\"\
    pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"random_graph\"\
    </span>, \\\n                                       <span class=\"pl-v\">N</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-s1\"\
    >k</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">16</span>, <span\
    \ class=\"pl-s1\">maxk</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >32</span>, <span class=\"pl-s1\">mu</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">0.2</span>, <span class=\"pl-s1\">minc</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">32</span>)\n<span class=\"pl-c\"># run pScan\
    \ graph algorithm</span>\n<span class=\"pl-s1\">pscan_clustering</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">gct</span>.<span class=\"pl-en\">scan_pScan</span>(<span\
    \ class=\"pl-s\">\"get_start_pscan\"</span>, <span class=\"pl-s1\">ds</span>)</pre></div>\n\
    <p>See more to visit <a href=\"https://lizhen0909.github.io/graph_clustering_toolkit/usage/usage.html\"\
    \ rel=\"nofollow\">online usage</a>.</p>\n<h3>\n<a id=\"user-content-citation\"\
    \ class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Citation:</h3>\n<p>Please cite\
    \ <a href=\"https://arxiv.org/abs/2005.04806\" rel=\"nofollow\">Comparison and\
    \ Benchmark of Graph Clustering Algorithms</a> for this work.</p>\n<p>For individual\
    \ algorithms, see <a href=\"https://lizhen0909.github.io/graph_clustering_toolkit/usage/pydoc_alg.html\"\
    \ rel=\"nofollow\">Algorithms</a> for their publications.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1589386210.0
LuisBonillaR/singularity:
  data_format: 2
  description: containers
  filenames:
  - Singularity.pyhon3
  - Singularity.py3_tfstable
  full_name: LuisBonillaR/singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3399" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PAML tool for phylogenetic analyses of DNA
    or protein sequences using maximum likelihood.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610738330.0
MI-911/cold-start-framework:
  data_format: 2
  description: Data generation, model training and evaluation pipelines for the cold-start
    setting.
  filenames:
  - Singularity.def
  full_name: MI-911/cold-start-framework
  latest_release: null
  readme: '<h1>

    <a id="user-content-cold-start-framework" class="anchor" href="#cold-start-framework"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cold-start
    Framework</h1>

    <p>Data partitioning, model training and evaluation pipelines for the cold-start
    setting.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>We have fully dockerized an evaluation pipeline, from downloading the most
    recent dataset to conducting interviews.

    The pipeline was developed using Docker version 19.03.5-ce.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick start</h2>

    <p>From a clean slate, run the pipeline by running the script <code>scripts/run_pipeline.sh</code>.
    The pipeline will:</p>

    <ul>

    <li>Download the latest stable MindReader version and the related entities.</li>

    <li>Partition the downloaded dataset into training (warm-start) and testing (cold-start).</li>

    <li>Run all models on the partitioned dataset.</li>

    </ul>

    <p>We recommend running the entire pipeline initially.

    Following this, one can run the experiments alone by running <code>scripts/run_interview.sh</code>.

    Note that if changes are made to the code, the base image should be rebuilt by
    running <code>scripts/build_base.sh</code>.</p>

    '
  stargazers_count: 4
  subscribers_count: 2
  topics:
  - pipeline
  - recommender-system
  - cold-start
  - evaluation-pipelines
  - dataset
  - interview
  - hacktoberfest
  updated_at: 1607268817.0
MPIB/singularity-fsl:
  data_format: 2
  description: various singularity recipes for FSL
  filenames:
  - Singularity.5.0.9
  - Singularity.6.0.2-Cuda8
  - Singularity.6.0.3
  - Singularity.5-Cuda8
  - Singularity.6.0.0
  - Singularity.5.0.10
  - Singularity.6.0.4
  - Singularity.5.0.11
  - Singularity.6.0.2-Cuda8-xtract_viewer
  - Singularity.6.0.4-Cuda8
  - Singularity.6.0.1
  - Singularity.6.0.2
  full_name: MPIB/singularity-fsl
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/702" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick start</h2>

    <pre><code># Download a (versioned) container

    singularity pull shub://MPIB/singularity-fsl:6.0.4


    # Run it

    singularity exec singularity-fsl_6.0.4.sif fslmaths

    singularity exec --nv singularity-fsl_6.0.4.sif eddy_cuda9.1

    </code></pre>

    <h2>

    <a id="user-content-fsl" class="anchor" href="#fsl" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>FSL</h2>

    <p>Project Home: <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/" rel="nofollow">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/</a></p>

    <p>These are containers primarily used at the MPI for Human Development.</p>

    <h2>

    <a id="user-content-cuda" class="anchor" href="#cuda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cuda</h2>

    <p>Starting with Singularity 6.0.2 we include Nvidia CUDA through Debian backports
    repositories.

    Make sure your Nvidia driver on the host <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility"
    rel="nofollow">supports it</a> and add the <code>--nv</code> flag with singularity.</p>

    <h2>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Note</h2>

    <p>Please be aware of FSL''s strict license regarding non-commercial use.</p>

    '
  stargazers_count: 4
  subscribers_count: 3
  topics:
  - containers
  - science
  updated_at: 1616068515.0
MRtrix3/containers:
  data_format: 2
  description: Hosts DockerFiles to build MRtrix3 containers
  filenames:
  - Singularity
  full_name: MRtrix3/containers
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-containers-for-mrtrix3\" class=\"anchor\" href=\"\
    #containers-for-mrtrix3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Containers for <em>MRtrix3</em>\n</h1>\n<p>Hosts\
    \ recipe files to build <em>MRtrix3</em> containers</p>\n<h2>\n<a id=\"user-content-using-docker\"\
    \ class=\"anchor\" href=\"#using-docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Docker</h2>\n<h4>\n<a id=\"\
    user-content-run-terminal-command\" class=\"anchor\" href=\"#run-terminal-command\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run terminal command</h4>\n<pre><code>docker run --rm -it mrtrix3\
    \ &lt;command&gt;\n</code></pre>\n<p>If not built locally, <code>docker</code>\
    \ will download the latest image from DockerHub.</p>\n<h4>\n<a id=\"user-content-run-gui\"\
    \ class=\"anchor\" href=\"#run-gui\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run GUI</h4>\n<p>These instructions\
    \ are for Linux.</p>\n<pre><code>xhost +local:root\ndocker run --rm -it -v /tmp/.X11-unix:/tmp/.X11-unix\
    \ -e DISPLAY=$DISPLAY mrtrix3 mrview\nxhost -local:root  # Run this when finished.\n\
    </code></pre>\n<h4>\n<a id=\"user-content-locally-build-docker-image\" class=\"\
    anchor\" href=\"#locally-build-docker-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Locally build Docker image</h4>\n\
    <pre><code>docker build --tag mrtrix3 .\n</code></pre>\n<p>Set <code>DOCKER_BUILDKIT=1</code>\
    \ to build parts of the Docker image in parallel, which can speed up build time.\n\
    Use <code>--build-arg MAKE_JOBS=4</code> to build <em>MRtrix3</em> with 4 processors\
    \ (can substitute this with any number of processors &gt; 0); if omitted, <em>MRtrix3</em>\
    \ will be built using a single thread only.</p>\n<h2>\n<a id=\"user-content-using-singularity\"\
    \ class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Singularity</h2>\n<h4>\n\
    <a id=\"user-content-build-container-natively\" class=\"anchor\" href=\"#build-container-natively\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Build container natively</h4>\n<pre><code>singularity build MRtrix3_&lt;version&gt;.sif\
    \ Singularity\n</code></pre>\n<h4>\n<a id=\"user-content-convert-from-docker-container\"\
    \ class=\"anchor\" href=\"#convert-from-docker-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Convert\
    \ from Docker container</h4>\n<pre><code>singularity build MRtrix3_&lt;version&gt;.sif\
    \ docker://mrtrix/mrtrix3:&lt;version&gt;\n</code></pre>\n<h4>\n<a id=\"user-content-run-terminal-command-1\"\
    \ class=\"anchor\" href=\"#run-terminal-command-1\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run terminal\
    \ command</h4>\n<pre><code>MRtrix3_&lt;version&gt;.sif &lt;command&gt;\n</code></pre>\n\
    <h4>\n<a id=\"user-content-run-gui-1\" class=\"anchor\" href=\"#run-gui-1\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run\
    \ GUI</h4>\n<pre><code>singularity exec -B /run MRtrix3_&lt;version&gt;.sif mrview\n\
    </code></pre>\n<hr>\n<h2>\n<a id=\"user-content-developers-update-minified-external-dependencies\"\
    \ class=\"anchor\" href=\"#developers-update-minified-external-dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Developers: Update minified external dependencies</h2>\n<p>This process\
    \ can only be completed by those with write access to the <a href=\"https://osf.io/5rwp3/\"\
    \ rel=\"nofollow\">\"<em>MRtrix3</em> container dependencies\" OSF project</a>.\n\
    These files contain \"minified\" versions of external neuroimaging software package\
    \ dependencies, containing only those components that are utilised by <em>MRtrix3</em>\
    \ scripts.\nThese files should only need to be updated if:</p>\n<ul>\n<li>An <em>MRtrix3</em>\
    \ update introduces a new feature that invokes some new external software tool\
    \ not previously utilised;</li>\n<li>A requisite update occurs in one of these\
    \ external softwares.</li>\n</ul>\n<ol>\n<li>\n<p>Install the <code>docker</code>\
    \ and <code>neurodocker</code> Python packages.</p>\n<pre><code>pip install docker\
    \ neurodocker\n</code></pre>\n</li>\n<li>\n<p>Download the ART ACPCdetect tool\
    \ from NITRC into the working directory.</p>\n<p>This cannot be downloaded directly\
    \ via e.g. <code>wget</code>, as it requires logging in to NITRC; instead, visit\
    \ the following link with a web browser:\n<a href=\"https://www.nitrc.org/frs/download.php/10595/acpcdetect_v2.0_LinuxCentOS6.7.tar.gz\"\
    \ rel=\"nofollow\"><code>https://www.nitrc.org/frs/download.php/10595/acpcdetect_v2.0_LinuxCentOS6.7.tar.gz</code></a></p>\n\
    </li>\n<li>\n<p>Download test data necessary for minification process.</p>\n<pre><code>curl\
    \ -fL -# https://github.com/MRtrix3/script_test_data/archive/master.tar.gz | tar\
    \ xz\n</code></pre>\n</li>\n<li>\n<p>Update file <code>minify.Dockerfile</code>\
    \ to install the desired versions of external software packages.</p>\n</li>\n\
    <li>\n<p>Build Docker image for <code>neurodocker-minify</code>, with complete\
    \ installations of external packages.</p>\n<pre><code>DOCKER_BUILDKIT=1 docker\
    \ build --tag mrtrix3:minify --file minify.Dockerfile --build-arg MAKE_JOBS=4\
    \ .\n</code></pre>\n<p><code>DOCKER_BUILDKIT=1</code> enables BuildKit, which\
    \ builds separate build stages in parallel.\nThis can speed up Docker build times\
    \ in some circumstances.\nIn this case, ANTs and <em>MRtrix3</em> will be compiled\
    \ in parallel, and other downloads will be performed at the same time as well.</p>\n\
    <p>The <code>MAKE_JOBS</code> argument controls how many cores are used for compilation\
    \ of ANTs and <em>MRtrix3</em>.\nIf BuildKit is utilised, do not specify all of\
    \ the available threads; specify half or fewer, so that threads are not unnecessarily\
    \ split across jobs and RAM usage is not excessive.</p>\n</li>\n<li>\n<p>Create\
    \ a minified version of the Docker image.</p>\n<pre><code>docker run --rm -itd\
    \ --name mrtrix3 --security-opt=seccomp:unconfined --volume $(pwd)/script_test_data-master:/mnt\
    \ mrtrix3:minify\nneurodocker-minify --dirs-to-prune /opt --container mrtrix3\
    \ --commands \"bash cmds-to-minify.sh\"\ndocker export mrtrix3 | docker import\
    \ - mrtrix3:minified\ndocker stop mrtrix3\n</code></pre>\n</li>\n<li>\n<p>Generate\
    \ tarballs for each of the utilised dependencies.</p>\n<pre><code>mkdir -p tarballs\n\
    docker run --rm -itd --workdir /opt --name mrtrix3 \\\n    --volume $(pwd)/tarballs:/output\
    \ mrtrix3:minified bash\ndocker exec mrtrix3 bash -c \"tar c art | pigz -9 &gt;\
    \ /output/acpcdetect_&lt;version&gt;.tar.gz\"\ndocker exec mrtrix3 bash -c \"\
    tar c ants | pigz -9 &gt; /output/ants_&lt;version&gt;.tar.gz\"\ndocker exec mrtrix3\
    \ bash -c \"tar c fsl | pigz -9 &gt; /output/fsl_&lt;version&gt;.tar.gz\"\ndocker\
    \ stop mrtrix3\n</code></pre>\n<p>For each tarball, manually replace text \"<code>&lt;version&gt;</code>\"\
    \ with the version number of that particular software that was installed in the\
    \ container.</p>\n</li>\n<li>\n<p>Upload these files to <a href=\"https://osf.io/nfx85/\"\
    \ rel=\"nofollow\">OSF</a>.</p>\n</li>\n</ol>\n<p>File <code>Dockerfile</code>\
    \ can then be modified to download the desired versions of external software packages.\n\
    As OSF file download links do not contain file names, which would otherwise indicate\
    \ the version of each software to be downloaded, please ensure that comments within\
    \ that file are updated to indicate the version of that software within the tarball.</p>\n"
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1612696118.0
Malindrie/scShapes:
  data_format: 2
  description: A Statistical Framework for Modeling and Identifying Differential Distributions
    in Single-cell RNA-sequencing Data
  filenames:
  - inst/Singularity
  full_name: Malindrie/scShapes
  latest_release: null
  readme: "\n<h1>\n<a id=\"user-content-scshapes\" class=\"anchor\" href=\"#scshapes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>scShapes</h1>\n\n<p><a href=\"https://travis-ci.com/Malindrie/scShapes\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f46be69a2babf0d92348a4d2abbc1834186998fa8fb4d023c1e85bc5f9405e8a/68747470733a2f2f7472617669732d63692e636f6d2f4d616c696e647269652f73635368617065732e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Travis build status\" data-canonical-src=\"https://travis-ci.com/Malindrie/scShapes.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://ci.appveyor.com/project/Malindrie/scShapes\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9b16a84bf2877273e52d87887ce418eb0fff59cd1066cca77a8700f6bae73d6b/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f4d616c696e647269652f73635368617065733f6272616e63683d6d6173746572267376673d74727565\"\
    \ alt=\"AppVeyor build status\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/github/Malindrie/scShapes?branch=master&amp;svg=true\"\
    \ style=\"max-width:100%;\"></a></p>\n\n<p>We present a novel statistical framework\
    \ for identifying differential\ndistributions in single-cell RNA-sequencing (scRNA-seq)\
    \ data between\ntreatment conditions by modeling gene expression read counts using\n\
    generalized linear models (GLMs). We model each gene independently under\neach\
    \ treatment condition using the error distributions Poisson (P),\nNegative Binomial\
    \ (NB), Zero-inflated Poisson (ZIP) and Zero-inflated\nNegative Binomial (ZINB)\
    \ with log link function and model based\nnormalization for differences in sequencing\
    \ depth. Since all four\ndistributions considered in our framework belong to the\
    \ same family of\ndistributions, we first perform a Kolmogorov-Smirnov (KS) test\
    \ to select\ngenes belonging to the family of ZINB distributions. Genes passing\
    \ the\nKS test will be then modeled using GLMs. Model selection is done by\ncalculating\
    \ the Bayesian Information Criterion and likelihood ratio test\nstatistic.</p>\n\
    <p>While most methods for differential gene expression analysis aim to\ndetect\
    \ a shift in the mean of expressed values, single cell data are\ndriven by over-dispersion\
    \ and dropouts requiring statistical\ndistributions that can handle the excess\
    \ zeros. By modeling gene\nexpression distributions, our framework can identify\
    \ subtle variations\nthat do not involve the change in mean. It also has the flexibility\
    \ to\nadjust for covariates and perform multiple comparisons while explicitly\n\
    modeling the variability between samples.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>You can\
    \ install the released version of scShapes with:</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-e\">devtools</span><span class=\"pl-k\">::</span>install_github(<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span>Malindrie/scShapes<span class=\"\
    pl-pds\">'</span></span>)</pre></div>\n<div class=\"highlight highlight-source-r\"\
    ><pre>library(<span class=\"pl-smi\">scShapes</span>)</pre></div>\n<h2>\n<a id=\"\
    user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Example</h2>\n\
    <p>This is a basic example which shows how you can use scShapes for\nidentifying\
    \ differential distributions in single-cell RNA-seq data. For\nthis example data\
    \ we use the human immune cells (PBMC) dataset\ndistributed through the\n<a href=\"\
    https://github.com/satijalab/seurat-data\">SeuratData</a> package.</p>\n<div class=\"\
    highlight highlight-source-r\"><pre>library(<span class=\"pl-smi\">scShapes</span>)\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Loading and preparing data\
    \ for input </span>\nlibrary(<span class=\"pl-smi\">Seurat</span>)\nlibrary(<span\
    \ class=\"pl-smi\">SeuratData</span>)\nlibrary(<span class=\"pl-smi\">dplyr</span>)\n\
    library(<span class=\"pl-smi\">BiocParallel</span>)\nset.seed(<span class=\"pl-c1\"\
    >0xBEEF</span>)\n\nInstallData(<span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>ifnb<span class=\"pl-pds\">\"</span></span>)\nLoadData(<span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>ifnb<span class=\"pl-pds\">\"</span></span>)\n\
    <span class=\"pl-smi\">ifnb.list</span> <span class=\"pl-k\">&lt;-</span> SplitObject(<span\
    \ class=\"pl-smi\">ifnb</span>, <span class=\"pl-v\">split.by</span> <span class=\"\
    pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>stim<span\
    \ class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>We first filter the genes\
    \ to keep only genes expressed in at least 10%\nof cells:</p>\n<div class=\"highlight\
    \ highlight-source-r\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>First\
    \ extract the RNA-seq counts from the 'RNA' assay of the seurat object</span>\n\
    <span class=\"pl-smi\">ifnb.obj</span> <span class=\"pl-k\">&lt;-</span> lapply(<span\
    \ class=\"pl-smi\">ifnb.list</span>, <span class=\"pl-k\">function</span> (<span\
    \ class=\"pl-smi\">x</span>) as.matrix(<span class=\"pl-smi\">x</span><span class=\"\
    pl-k\">@</span><span class=\"pl-smi\">assays</span><span class=\"pl-k\">$</span><span\
    \ class=\"pl-smi\">RNA</span><span class=\"pl-k\">@</span><span class=\"pl-smi\"\
    >counts</span>))\n<span class=\"pl-smi\">ifnb.filtered</span> <span class=\"pl-k\"\
    >&lt;-</span> lapply(<span class=\"pl-smi\">ifnb.obj</span>, <span class=\"pl-k\"\
    >function</span> (<span class=\"pl-smi\">x</span>) filter_counts(<span class=\"\
    pl-smi\">x</span>, <span class=\"pl-v\">perc.zero</span> <span class=\"pl-k\"\
    >=</span> <span class=\"pl-c1\">0.1</span>))\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span>&gt; Removing 527 rows of genes with all zero counts</span>\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span>&gt; Removing 778 rows of genes\
    \ with all zero counts</span></pre></div>\n<p>In order to normalize for differences\
    \ in sequencing depth, the log of\nthe total UMI counts assigned per cell will\
    \ be used as an offset in the\nGLM. This function is inbuilt in the algorithm;\
    \ however the user is\nrequired to input the library sizes. We can calculate the\
    \ library sizes\nfor the two treatment conditions as;</p>\n<div class=\"highlight\
    \ highlight-source-r\"><pre><span class=\"pl-smi\">ifnb.lib.size</span> <span\
    \ class=\"pl-k\">&lt;-</span> lapply(<span class=\"pl-smi\">ifnb.filtered</span>,\
    \ <span class=\"pl-k\">function</span> (<span class=\"pl-smi\">x</span>) apply(<span\
    \ class=\"pl-smi\">x</span>,<span class=\"pl-c1\">2</span>, <span class=\"pl-k\"\
    >function</span>(<span class=\"pl-smi\">y</span>) sum(<span class=\"pl-smi\">y</span>)))</pre></div>\n\
    <p>The \u2018meta.data\u2019 slot of the Seurat object also contains information\
    \ on\nthe cell-types, which will be used as a covariate in the GLM model to\n\
    account for known biological variation in the data.</p>\n<div class=\"highlight\
    \ highlight-source-r\"><pre><span class=\"pl-smi\">ifnb.variables</span> <span\
    \ class=\"pl-k\">&lt;-</span> lapply(<span class=\"pl-smi\">ifnb.list</span>,\
    \ <span class=\"pl-k\">function</span> (<span class=\"pl-smi\">x</span>) <span\
    \ class=\"pl-k\">data.frame</span>(\n                        <span class=\"pl-v\"\
    >cell.type</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">factor</span>(<span\
    \ class=\"pl-smi\">x</span><span class=\"pl-k\">@</span><span class=\"pl-smi\"\
    >meta.data</span><span class=\"pl-k\">$</span><span class=\"pl-smi\">seurat_annotations</span>),\n\
    \                        <span class=\"pl-v\">row.names</span> <span class=\"\
    pl-k\">=</span> colnames(<span class=\"pl-smi\">x</span><span class=\"pl-k\">@</span><span\
    \ class=\"pl-smi\">assays</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >RNA</span>)))</pre></div>\n<p>For the purpose of this example we only run the\
    \ pipeline for randomly\nselected 20 common genes under both treatment conditions\
    \ \u2018CTRL\u2019 and\n\u2018STIM\u2019.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>Randomly select 20 genes\
    \ among common genes between the two treatment conditions</span>\n<span class=\"\
    pl-smi\">comm.genes</span> <span class=\"pl-k\">&lt;-</span> intersect(rownames(<span\
    \ class=\"pl-smi\">ifnb.filtered</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">CTRL</span>), rownames(<span class=\"pl-smi\">ifnb.filtered</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>))\n<span class=\"\
    pl-smi\">comm.20.genes</span> <span class=\"pl-k\">&lt;-</span> sample(<span class=\"\
    pl-smi\">comm.genes</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\"\
    >replace</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FALSE</span>)\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Subset the randomly selected\
    \ 20 genes</span>\n<span class=\"pl-smi\">ifnb.ctrl</span> <span class=\"pl-k\"\
    >&lt;-</span> <span class=\"pl-smi\">ifnb.filtered</span><span class=\"pl-k\"\
    >$</span><span class=\"pl-smi\">CTRL</span>[rownames(<span class=\"pl-smi\">ifnb.filtered</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">CTRL</span>) <span class=\"pl-k\"\
    >%in%</span> <span class=\"pl-smi\">comm.20.genes</span>,]\n<span class=\"pl-smi\"\
    >ifnb.stim</span> <span class=\"pl-k\">&lt;-</span> <span class=\"pl-smi\">ifnb.filtered</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>[rownames(<span class=\"\
    pl-smi\">ifnb.filtered</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >STIM</span>) <span class=\"pl-k\">%in%</span> <span class=\"pl-smi\">comm.20.genes</span>,]\n\
    <span class=\"pl-smi\">ifnb.subset</span> <span class=\"pl-k\">&lt;-</span> <span\
    \ class=\"pl-k\">list</span>(<span class=\"pl-v\">CTRL</span> <span class=\"pl-k\"\
    >=</span> <span class=\"pl-smi\">ifnb.ctrl</span>, <span class=\"pl-v\">STIM</span>\
    \ <span class=\"pl-k\">=</span> <span class=\"pl-smi\">ifnb.stim</span>)</pre></div>\n\
    <p>Perform Kolmogorov-Smirnov test to select genes belonging to the family\nof\
    \ ZINB distributions.</p>\n<div class=\"highlight highlight-source-r\"><pre><span\
    \ class=\"pl-smi\">ifnb.ctrl.KS</span> <span class=\"pl-k\">&lt;-</span> ks_test(<span\
    \ class=\"pl-smi\">ifnb.subset</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">CTRL</span>, <span class=\"pl-v\">cexpr</span><span class=\"pl-k\">=</span><span\
    \ class=\"pl-smi\">ifnb.variables</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">CTRL</span>, <span class=\"pl-v\">lib.size</span><span class=\"pl-k\"\
    >=</span><span class=\"pl-smi\">ifnb.lib.size</span><span class=\"pl-k\">$</span><span\
    \ class=\"pl-smi\">CTRL</span>, <span class=\"pl-v\">BPPARAM</span><span class=\"\
    pl-k\">=</span>SnowParam(<span class=\"pl-v\">workers</span><span class=\"pl-k\"\
    >=</span><span class=\"pl-c1\">4</span>,<span class=\"pl-v\">type</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SOCK<span\
    \ class=\"pl-pds\">\"</span></span>))\n<span class=\"pl-smi\">ifnb.stim.KS</span>\
    \ <span class=\"pl-k\">&lt;-</span> ks_test(<span class=\"pl-smi\">ifnb.subset</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>, <span class=\"pl-v\"\
    >cexpr</span><span class=\"pl-k\">=</span><span class=\"pl-smi\">ifnb.variables</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>, <span class=\"pl-v\"\
    >lib.size</span><span class=\"pl-k\">=</span><span class=\"pl-smi\">ifnb.lib.size</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>, <span class=\"pl-v\"\
    >BPPARAM</span><span class=\"pl-k\">=</span>SnowParam(<span class=\"pl-v\">workers</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>,<span class=\"pl-v\">type</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SOCK<span\
    \ class=\"pl-pds\">\"</span></span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span>Select genes significant from the KS test.</span>\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span>By default the 'ks_sig' function performs Benjamini-Hochberg\
    \ correction for multiple hypothese testing</span>\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span>and selects genes significant at p-value of 0.01</span>\n\
    \n<span class=\"pl-smi\">ifnb.ctrl.sig.KS</span> <span class=\"pl-k\">&lt;-</span>\
    \ ks_sig(<span class=\"pl-smi\">ifnb.ctrl.KS</span>)\n<span class=\"pl-smi\">ifnb.stim.sig.KS</span>\
    \ <span class=\"pl-k\">&lt;-</span> ks_sig(<span class=\"pl-smi\">ifnb.stim.KS</span>)\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Subset UMI counts corresponding\
    \ to the genes significant from the KS test</span>\n<span class=\"pl-smi\">ifnb.sig.genes</span>\
    \ <span class=\"pl-k\">&lt;-</span> <span class=\"pl-k\">list</span>(<span class=\"\
    pl-v\">CTRL</span> <span class=\"pl-k\">=</span> as.data.frame(<span class=\"\
    pl-smi\">ifnb.ctrl.sig.KS</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >genes</span>),\n                       <span class=\"pl-v\">STIM</span> <span\
    \ class=\"pl-k\">=</span> as.data.frame(<span class=\"pl-smi\">ifnb.stim.sig.KS</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">genes</span>))\n<span class=\"\
    pl-smi\">ifnb.ctrl.KS</span> <span class=\"pl-k\">&lt;-</span> <span class=\"\
    pl-smi\">ifnb.filtered</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >CTRL</span>[rownames(<span class=\"pl-smi\">ifnb.filtered</span><span class=\"\
    pl-k\">$</span><span class=\"pl-smi\">CTRL</span>) <span class=\"pl-k\">%in%</span>\
    \ rownames(<span class=\"pl-smi\">ifnb.sig.genes</span><span class=\"pl-k\">$</span><span\
    \ class=\"pl-smi\">CTRL</span>),]\n  <span class=\"pl-smi\">ifnb.stim.KS</span>\
    \ <span class=\"pl-k\">&lt;-</span> <span class=\"pl-smi\">ifnb.filtered</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>[rownames(<span class=\"\
    pl-smi\">ifnb.filtered</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >STIM</span>) <span class=\"pl-k\">%in%</span> rownames(<span class=\"pl-smi\"\
    >ifnb.sig.genes</span><span class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>),]</pre></div>\n\
    <p>Fit the 4 distributions P,NB,ZIP,ZINB for genes that belong to the ZINB\nfamily\
    \ of distributions by fitting GLM with log of the library sizes as\nan offset\
    \ and cell types as a covariate in the GLM.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-smi\">ifnb.ctrl.fit</span> <span class=\"pl-k\">&lt;-</span>\
    \ fit_models(<span class=\"pl-v\">counts</span><span class=\"pl-k\">=</span><span\
    \ class=\"pl-smi\">ifnb.ctrl.KS</span>, <span class=\"pl-v\">cexpr</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-smi\">ifnb.variables</span><span class=\"\
    pl-k\">$</span><span class=\"pl-smi\">CTRL</span>, <span class=\"pl-v\">lib.size</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-smi\">ifnb.lib.size</span><span class=\"\
    pl-k\">$</span><span class=\"pl-smi\">CTRL</span>, <span class=\"pl-v\">BPPARAM</span><span\
    \ class=\"pl-k\">=</span>SnowParam(<span class=\"pl-v\">workers</span><span class=\"\
    pl-k\">=</span><span class=\"pl-c1\">4</span>,<span class=\"pl-v\">type</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SOCK<span\
    \ class=\"pl-pds\">\"</span></span>))\n<span class=\"pl-smi\">ifnb.stim.fit</span>\
    \ <span class=\"pl-k\">&lt;-</span> fit_models(<span class=\"pl-v\">counts</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-smi\">ifnb.stim.KS</span>, <span class=\"\
    pl-v\">cexpr</span><span class=\"pl-k\">=</span><span class=\"pl-smi\">ifnb.variables</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>, <span class=\"pl-v\"\
    >lib.size</span><span class=\"pl-k\">=</span><span class=\"pl-smi\">ifnb.lib.size</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>, <span class=\"pl-v\"\
    >BPPARAM</span><span class=\"pl-k\">=</span>SnowParam(<span class=\"pl-v\">workers</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>,<span class=\"pl-v\">type</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SOCK<span\
    \ class=\"pl-pds\">\"</span></span>))</pre></div>\n<p>Once the 4 distributions\
    \ are fitted, we next calculate the BIC value for\neach model and select the model\
    \ with the least BIC value.</p>\n<div class=\"highlight highlight-source-r\"><pre><span\
    \ class=\"pl-smi\">ifnb.ctrl.bic.val</span> <span class=\"pl-k\">&lt;-</span>\
    \ model_bic(<span class=\"pl-smi\">ifnb.ctrl.fit</span>)\n<span class=\"pl-smi\"\
    >ifnb.stim.bic.val</span> <span class=\"pl-k\">&lt;-</span> model_bic(<span class=\"\
    pl-smi\">ifnb.stim.fit</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>select\
    \ model with least bic value</span>\n<span class=\"pl-smi\">ifnb.ctrl.lbic</span>\
    \ <span class=\"pl-k\">&lt;-</span> lbic_model(<span class=\"pl-smi\">ifnb.ctrl.bic.val</span>,\
    \ <span class=\"pl-smi\">ifnb.ctrl.KS</span>)\n<span class=\"pl-smi\">ifnb.stim.lbic</span>\
    \ <span class=\"pl-k\">&lt;-</span> lbic_model(<span class=\"pl-smi\">ifnb.stim.bic.val</span>,\
    \ <span class=\"pl-smi\">ifnb.stim.KS</span>)</pre></div>\n<p>To ensure the fit\
    \ of the models selected based on the least BIC value,\nadditionally we perform\
    \ LRT to test for model adequacy and presence of\nzero-inflation.</p>\n<div class=\"\
    highlight highlight-source-r\"><pre><span class=\"pl-smi\">ifnb.ctrl.gof</span>\
    \ <span class=\"pl-k\">&lt;-</span> gof_model(<span class=\"pl-smi\">ifnb.ctrl.lbic</span>,\
    \ <span class=\"pl-smi\">ifnb.variables</span><span class=\"pl-k\">$</span><span\
    \ class=\"pl-smi\">CTRL</span>, <span class=\"pl-smi\">ifnb.lib.size</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">CTRL</span>, <span class=\"pl-v\"\
    >BPPARAM</span><span class=\"pl-k\">=</span>SerialParam())\n<span class=\"pl-smi\"\
    >ifnb.stim.gof</span> <span class=\"pl-k\">&lt;-</span> gof_model(<span class=\"\
    pl-smi\">ifnb.stim.lbic</span>, <span class=\"pl-smi\">ifnb.variables</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>, <span class=\"pl-smi\"\
    >ifnb.lib.size</span><span class=\"pl-k\">$</span><span class=\"pl-smi\">STIM</span>,\
    \ <span class=\"pl-v\">BPPARAM</span><span class=\"pl-k\">=</span>SerialParam())</pre></div>\n\
    <p>Finally based on the results of the model adequacy tests, we can\nidentify\
    \ the distribution of best fit for each gene.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-smi\">ifnb.ctrl.dist.fit</span> <span class=\"pl-k\">&lt;-</span>\
    \ select_model(<span class=\"pl-smi\">ifnb.ctrl.gof</span>)\n<span class=\"pl-smi\"\
    >ifnb.stim.dist.fit</span> <span class=\"pl-k\">&lt;-</span> select_model(<span\
    \ class=\"pl-smi\">ifnb.stim.gof</span>)</pre></div>\n<p>Once the distribution\
    \ of best fit is identified for genes of interest,\nit is also possible to extract\
    \ parameters of interest for the models.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-smi\">ifnb.ctrl.params</span> <span class=\"pl-k\">&lt;-</span>\
    \ model_param (<span class=\"pl-smi\">ifnb.ctrl.fit</span>, <span class=\"pl-smi\"\
    >ifnb.ctrl.dist.fit</span>, <span class=\"pl-v\">model</span><span class=\"pl-k\"\
    >=</span><span class=\"pl-c1\">NULL</span>)\n<span class=\"pl-smi\">ifnb.stim.params</span>\
    \ <span class=\"pl-k\">&lt;-</span> model_param (<span class=\"pl-smi\">ifnb.stim.fit</span>,\
    \ <span class=\"pl-smi\">ifnb.stim.dist.fit</span>, <span class=\"pl-v\">model</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-c1\">NULL</span>)</pre></div>\n<p>Using\
    \ above results we can now identify the differentially distributed\ngenes between\
    \ \u2018CTRL\u2019 and \u2018STIM\u2019. First we need to subset genes that is\n\
    significant in the KS test in both conditions.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>Subset the common genes\
    \ between the two groups, that pass the GOF test</span>\n<span class=\"pl-smi\"\
    >ifnb.ctrl.fit</span> <span class=\"pl-k\">&lt;-</span> unlist(<span class=\"\
    pl-smi\">ifnb.ctrl.dist.fit</span>)\n<span class=\"pl-smi\">ifnb.stim.fit</span>\
    \ <span class=\"pl-k\">&lt;-</span> unlist(<span class=\"pl-smi\">ifnb.stim.dist.fit</span>)\n\
    <span class=\"pl-smi\">ifnb.gof.sig</span> <span class=\"pl-k\">&lt;-</span> intersect(<span\
    \ class=\"pl-smi\">ifnb.ctrl.fit</span>, <span class=\"pl-smi\">ifnb.stim.fit</span>)\n\
    \n<span class=\"pl-smi\">ifnb.dist.ctrl</span> <span class=\"pl-k\">&lt;-</span>\
    \ <span class=\"pl-k\">data.frame</span>(<span class=\"pl-v\">gene</span> <span\
    \ class=\"pl-k\">=</span> c(<span class=\"pl-smi\">ifnb.ctrl.dist.fit</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">P_genes</span>, <span class=\"\
    pl-smi\">ifnb.ctrl.dist.fit</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">NB_genes</span>, <span class=\"pl-smi\">ifnb.ctrl.dist.fit</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">ZIP_genes</span>, <span class=\"\
    pl-smi\">ifnb.ctrl.dist.fit</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">ZINB_genes</span>))\n<span class=\"pl-smi\">ifnb.dist.ctrl</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">dist</span> <span class=\"pl-k\"\
    >&lt;-</span> c(rep(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Po<span\
    \ class=\"pl-pds\">\"</span></span>, length(<span class=\"pl-smi\">ifnb.ctrl.dist.fit</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">P_genes</span>)), rep(<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>NB<span class=\"pl-pds\">\"</span></span>,\
    \ length(<span class=\"pl-smi\">ifnb.ctrl.dist.fit</span><span class=\"pl-k\"\
    >$</span><span class=\"pl-smi\">NB_genes</span>)), rep(<span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>ZIP<span class=\"pl-pds\">\"</span></span>, length(<span\
    \ class=\"pl-smi\">ifnb.ctrl.dist.fit</span><span class=\"pl-k\">$</span><span\
    \ class=\"pl-smi\">ZIP_genes</span>)), rep(<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>ZINB<span class=\"pl-pds\">\"</span></span>, length(<span class=\"\
    pl-smi\">ifnb.ctrl.dist.fit</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">ZINB_genes</span>)))\n\n<span class=\"pl-smi\">ifnb.dist.stim</span>\
    \ <span class=\"pl-k\">&lt;-</span> <span class=\"pl-k\">data.frame</span>(<span\
    \ class=\"pl-v\">gene</span> <span class=\"pl-k\">=</span> c(<span class=\"pl-smi\"\
    >ifnb.stim.dist.fit</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >P_genes</span>, <span class=\"pl-smi\">ifnb.stim.dist.fit</span><span class=\"\
    pl-k\">$</span><span class=\"pl-smi\">NB_genes</span>, <span class=\"pl-smi\"\
    >ifnb.stim.dist.fit</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >ZIP_genes</span>, <span class=\"pl-smi\">ifnb.stim.dist.fit</span><span class=\"\
    pl-k\">$</span><span class=\"pl-smi\">ZINB_genes</span>))\n<span class=\"pl-smi\"\
    >ifnb.dist.stim</span><span class=\"pl-k\">$</span><span class=\"pl-smi\">dist</span>\
    \ <span class=\"pl-k\">&lt;-</span> c(rep(<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>Po<span class=\"pl-pds\">\"</span></span>, length(<span class=\"\
    pl-smi\">ifnb.stim.dist.fit</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">P_genes</span>)), rep(<span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>NB<span class=\"pl-pds\">\"</span></span>, length(<span class=\"pl-smi\"\
    >ifnb.stim.dist.fit</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >NB_genes</span>)), rep(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ZIP<span\
    \ class=\"pl-pds\">\"</span></span>, length(<span class=\"pl-smi\">ifnb.stim.dist.fit</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">ZIP_genes</span>)), rep(<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>ZINB<span class=\"pl-pds\">\"\
    </span></span>, length(<span class=\"pl-smi\">ifnb.stim.dist.fit</span><span class=\"\
    pl-k\">$</span><span class=\"pl-smi\">ZINB_genes</span>)))\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span>Dataframe consisting of distributions followed by\
    \ each gene passing the KS test</span>\n<span class=\"pl-smi\">ifnb.gof.ctrl</span>\
    \ <span class=\"pl-k\">&lt;-</span> <span class=\"pl-smi\">ifnb.dist.ctrl</span>[<span\
    \ class=\"pl-smi\">ifnb.dist.ctrl</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">gene</span> <span class=\"pl-k\">%in%</span> <span class=\"pl-smi\">ifnb.gof.sig</span>,]\n\
    <span class=\"pl-smi\">ifnb.gof.stim</span> <span class=\"pl-k\">&lt;-</span>\
    \ <span class=\"pl-smi\">ifnb.dist.stim</span>[<span class=\"pl-smi\">ifnb.dist.stim</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">gene</span> <span class=\"pl-k\"\
    >%in%</span> <span class=\"pl-smi\">ifnb.gof.sig</span>,]\n\n<span class=\"pl-smi\"\
    >ifnb.distr</span> <span class=\"pl-k\">&lt;-</span> <span class=\"pl-k\">data.frame</span>(<span\
    \ class=\"pl-v\">ctrl</span> <span class=\"pl-k\">=</span> <span class=\"pl-smi\"\
    >ifnb.gof.ctrl</span><span class=\"pl-k\">$</span><span class=\"pl-smi\">dist</span>,\
    \ <span class=\"pl-v\">row.names</span> <span class=\"pl-k\">=</span> <span class=\"\
    pl-smi\">ifnb.gof.ctrl</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >gene</span>)\n<span class=\"pl-smi\">ifnb.distr</span><span class=\"pl-k\">$</span><span\
    \ class=\"pl-smi\">stim</span> <span class=\"pl-k\">&lt;-</span> <span class=\"\
    pl-smi\">ifnb.gof.stim</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >dist</span>[match(rownames(<span class=\"pl-smi\">ifnb.distr</span>), <span class=\"\
    pl-smi\">ifnb.gof.stim</span><span class=\"pl-k\">$</span><span class=\"pl-smi\"\
    >gene</span>)]</pre></div>\n<p>Using the dataframe of genes and distribution followed\
    \ under each\ncondition now we can identify genes changing distribution between\
    \ \u2018CTRL\u2019\nand \u2018STIM\u2019</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-smi\">ifnb.DD.genes</span> <span class=\"pl-k\">&lt;-</span>\
    \ change_shape(<span class=\"pl-smi\">ifnb.distr</span>)</pre></div>\n<p>This\
    \ will give a list of two lists with genes changing distribution\nbetween condition\
    \ and genes changing distribution from unimodal in one\ncondition to zero-inflated\
    \ in the other condition.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1622699052.0
Martybird/7TBEaST:
  data_format: 2
  description: Adapt the BEaST skull stripping method for 7T MRI as a BIDS app
  filenames:
  - Singularity.v0.0.1a
  full_name: Martybird/7TBEaST
  latest_release: null
  readme: '<h1>

    <a id="user-content-7tbeast" class="anchor" href="#7tbeast" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>7TBEaST</h1>

    <p>Adapt the BEaST skull stripping method for 7T MRI as a BIDS app</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1530840788.0
MiddelkoopT/RC-2019-Spring:
  data_format: 2
  description: Research Computing Spring 2019 (IMSE 8410)
  filenames:
  - examples/containers/Singularity
  full_name: MiddelkoopT/RC-2019-Spring
  latest_release: null
  readme: '<h1>

    <a id="user-content-nipype-tutorial-notebooks" class="anchor" href="#nipype-tutorial-notebooks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nipype
    Tutorial Notebooks</h1>

    <p><a href="https://circleci.com/gh/miykael/nipype_tutorial/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64"
    alt="CircleCi" data-canonical-src="https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/issues/"><img src="https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/pulls/"><img src="https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub pull-requests" data-canonical-src="https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/"><img
    src="https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/commits/master"><img src="https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub Commits" data-canonical-src="https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/archive/master.zip"><img src="https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub size" data-canonical-src="https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/miykael/nipype_tutorial/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000"
    style="max-width:100%;"></a>

    <a href="http://hits.dwyl.io/miykael/nipype_tutorial" rel="nofollow"><img src="https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub HitCount" data-canonical-src="http://hits.dwyl.io/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a></p>

    <p>This is the Nipype Tutorial in Jupyter Notebook format. You can access the
    tutorial in two ways:</p>

    <ol>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/" rel="nofollow">Nipype Tutorial
    Homepage</a>: This website contains a static, read-only version of all the notebooks.</li>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html"
    rel="nofollow">Nipype Tutorial Docker Image</a>: This guide explains how to use
    Docker to run the notebooks interactively on your own computer. The nipype tutorial
    docker image is the best interactive way to learn Nipype.</li>

    </ol>

    <h1>

    <a id="user-content-feedback-help--support" class="anchor" href="#feedback-help--support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feedback,
    Help &amp; Support</h1>

    <p>If you want to help with this tutorial or have any questions, feel free to
    fork the repo of the <a href="https://github.com/miykael/nipype_tutorial">Notebooks</a>
    or interact with other contributors on the slack channel <a href="https://brainhack.slack.com/messages/nipype/"
    rel="nofollow">brainhack.slack.com/messages/nipype/</a>. If you have any questions
    or found a problem, open a new <a href="https://github.com/miykael/nipype_tutorial/issues">issue
    on github</a>.</p>

    <h1>

    <a id="user-content-thanks-and-acknowledgment" class="anchor" href="#thanks-and-acknowledgment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thanks
    and Acknowledgment</h1>

    <p>A huge thanks to <a href="https://github.com/mwaskom">Michael Waskom</a>, <a
    href="https://github.com/oesteban">Oscar Esteban</a>, <a href="https://github.com/chrisfilo">Chris
    Gorgolewski</a> and <a href="https://github.com/satra">Satrajit Ghosh</a> for
    their input to this tutorial! And a huge thanks to <a href="https://github.com/djarecka/">Dorota
    Jarecka</a> who updated this tutorial to Python 3 and is helping me with keeping
    this tutorial updated and running!</p>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1569468582.0
MiguelJulia/GCC2019_GalaxyAnsibleDeplyoment_CVMFS:
  data_format: 2
  description: Example of deployment of a Galaxy Production Instance using CVMFS with
    Ansible
  filenames:
  - Singularity
  full_name: MiguelJulia/GCC2019_GalaxyAnsibleDeplyoment_CVMFS
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-gcc2019_galaxyansibledeplyoment_cvmfs\" class=\"\
    anchor\" href=\"#gcc2019_galaxyansibledeplyoment_cvmfs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>GCC2019_GalaxyAnsibleDeplyoment_CVMFS</h1>\n\
    <p>Example of deployment of a Galaxy Production Instance using CVMFS with Ansible.\n\
    For more info, look into <a href=\"https://galaxyproject.github.io/training-material/topics/admin/\"\
    \ rel=\"nofollow\">galaxy admin training materials</a></p>\n<h4>\n<a id=\"user-content-deploying-a-galaxy-stance\"\
    \ class=\"anchor\" href=\"#deploying-a-galaxy-stance\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Deploying a galaxy\
    \ stance</h4>\n<pre><code>ansible-playbook -i host cvmfs_playbook.yml\n</code></pre>\n\
    <h4>\n<a id=\"user-content-restart-galaxy\" class=\"anchor\" href=\"#restart-galaxy\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Restart galaxy</h4>\n<pre><code>sudo su - galaxy\nsupervisorctl restart\
    \ galaxy\n</code></pre>\n<h4>\n<a id=\"user-content-variables-to-modify-for-quick-deployment\"\
    \ class=\"anchor\" href=\"#variables-to-modify-for-quick-deployment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Variables\
    \ to modify for quick deployment</h4>\n<p>Admin user name. This user is not created,\
    \ still has to be registered the first time and it will automatically get admin\
    \ permissions:</p>\n<pre><code>galaxy_config:\n  galaxy:\n    admin_users: admin@example.com\n\
    </code></pre>\n<p>Brand: Whatever appears on the banner</p>\n<pre><code>galaxy_config:\n\
    \  galaxy:\n    brand: \"Freiburg GCC\"\n</code></pre>\n<h4>\n<a id=\"user-content-welcomehtml\"\
    \ class=\"anchor\" href=\"#welcomehtml\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>welcome.html</h4>\n<p>Frontpage\
    \ is not created by default. You can find the template inside <code>galaxy_root:\
    \ /srv/galaxy</code>, in <code>server/static/welcome.html.sample</code>. Just\
    \ create a <code>welcome.html</code> page from this template in that same location\
    \ and restart galaxy.</p>\n<h4>\n<a id=\"user-content-deploying-your-ansible-managed-galaxy-into-a-container-not-working-yet\"\
    \ class=\"anchor\" href=\"#deploying-your-ansible-managed-galaxy-into-a-container-not-working-yet\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Deploying your ansible-managed galaxy into a container (not working\
    \ yet!)</h4>\n<p>We will use <a href=\"https://github.com/ansible-community/ansible-bender\"\
    >ansible-bender</a> for this task. Your playbook will have to be adapted to this\
    \ plugging standars as described in their documentation, or compare the differences\
    \ between my cvmfs_playbook.yml and ansible-bender-test.yml to have a quick idea\
    \ of how it has to be done.</p>\n<p>Make sure you are running the right version\
    \ of ansible, as ansible-bender only works with python3. Still, playbooks designed\
    \ for python2 can still be used. You will also need to install <a href=\"https://github.com/containers/buildah/blob/master/install.md\"\
    >buildah</a> and <a href=\"https://github.com/containers/libpod/blob/master/install.md\"\
    >podman</a>.</p>\n<p>Finally, you will need to configurate podman repo config\
    \ file <code>/etc/containers/registries.conf</code> to tell it where to look for\
    \ your containers. For example, to search in dokerhub add <code>'docker.io'</code>\
    \ inside</p>\n<pre><code>[registries.search]\nregistries = ['docker.io']\n</code></pre>\n\
    <p>The image is required to have python interpreter build in.</p>\n<h4>\n<a id=\"\
    user-content-building-galaxy-container-with-docker-idea---not-testet-yet\" class=\"\
    anchor\" href=\"#building-galaxy-container-with-docker-idea---not-testet-yet\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building galaxy container with Docker (idea - not testet yet)</h4>\n\
    <p>Use galaxy-container <a href=\"https://github.com/bgruening/docker-galaxy-stable/blob/master/galaxy/Dockerfile\"\
    >Dockerfile</a> as template.</p>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1562583598.0
NIH-HPC/singularity-def-files:
  data_format: 2
  description: definition files and wrapper scripts used by NIH HPC staff to install
    user-facing apps on the Biowulf cluster
  filenames:
  - molecular-modeling-graphics/blender/2.82/blender.def
  - molecular-modeling-graphics/starseqr/0.6.7/starseqr.def
  - molecular-modeling-graphics/chimerax/1.1/chimerax.def
  - molecular-modeling-graphics/chimerax/0.93/chimerax.def
  - systems-biology/cellphonedb/2.1.7/cellphonedb.def
  - systems-biology/cellphonedb/2.1.2/cellphonedb.def
  - sequence-analysis/cicero/1.4.0/cicero.def
  - sequence-analysis/roary/3.13.0/roary.def
  - sequence-analysis/roary/3.12.0/roary.def
  - sequence-analysis/arriba/2.0.0/arriba.def
  - sequence-analysis/arriba/1.2.0/arriba.def
  - sequence-analysis/xhla/2018-04-04/xhla.def
  - sequence-analysis/qtltools/1.3.1/qtltools.def
  - sequence-analysis/glu/1.0b3/glu.def
  - sequence-analysis/annogesic/1.0.2/annogesic.def
  - sequence-analysis/focus/0.6.10/focus.def
  - sequence-analysis/wisexome/20180814/wisexome.def
  - sequence-analysis/anvio/7/anvio.def
  - sequence-analysis/cactus/1.2.3/cactus.def
  - sequence-analysis/bamgineer/2-20200624/bamgineer.def
  - sequence-analysis/svtools/0.5.1/svtools.def
  - sequence-analysis/phaser/1.1.1/phaser.def
  - sequence-analysis/accurity/20210209/accurity.def
  - sequence-analysis/accurity/20180724/accurity.def
  - sequence-analysis/sonicparanoid/1.3.5/sonicparanoid.def
  - sequence-analysis/sonicparanoid/1.3.2/sonicparanoid.def
  - sequence-analysis/orffinder/0.4.3-sing-install/orffinder.def
  - sequence-analysis/vcf-kit/0.1.6/vcf-kit.def
  - sequence-analysis/smoove/0.2.5/smoove.def
  - sequence-analysis/smoove/0.2.1/smoove.def
  - sequence-analysis/m-tools/20210208/m-tools.def
  - sequence-analysis/eukrep/20180308/eukrep.def
  - sequence-analysis/asgal/1.0/asgal.def
  - sequence-analysis/netoglyc/3.1d/netoglyc.def
  - sequence-analysis/saige/0.44.1/saige.def
  - sequence-analysis/intarna/3.2.0/intarna.def
  - sequence-analysis/chipseq_pipeline/1.2.0/chipseq_pipeline.def
  - sequence-analysis/mmarge/1.0/mmarge.def
  - sequence-analysis/acfs/20180316/acfs.def
  - sequence-analysis/ldsc/3d0c4464/ldsc.def
  - sequence-analysis/deepsea/0.94c/deepsea.def
  - sequence-analysis/braker/2/braker.def
  - sequence-analysis/prokka/1.13/prokka.def
  - sequence-analysis/prokka/1.14.6/prokka.def
  - sequence-analysis/htgtsrep/9fe74ff/htgtsrep.def
  - sequence-analysis/glnexus/1.1.11/glnexus.def
  - sequence-analysis/glnexus/1.2.7/glnexus.def
  - sequence-analysis/augustus/3.3.3/augustus.def
  - image-analysis/terastitcher/1.11.10/terastitcher.def
  - image-analysis/terastitcher/1.10.8/terastitcher.def
  - image-analysis/mrtrix/3.0.2-cuda9.1/mrtrix.def
  - image-analysis/mrtrix/3.0.1/mrtrix.def
  - image-analysis/mrtrix/3.0.0/mrtrix.def
  - image-analysis/mriqc/0.16.1/mriqc.def
  - image-analysis/mriqc/0.15.1/mriqc.def
  - image-analysis/mriqc/0.15.2/mriqc.def
  - image-analysis/mriqc/0.15.2-0be03bf/mriqc.def
  - image-analysis/xcpengine/1.2.1/xcpengine.def
  - image-analysis/xcpengine/1.0/xcpengine.def
  - image-analysis/xcpengine/1.2.3/xcpengine.def
  - image-analysis/tesseract/4.1.1/tesseract.def
  - image-analysis/civet/2.1.1/civet.def
  - image-analysis/topaz/0.2.5/topaz.def
  - image-analysis/deepmedic/0.8.0/deepmedic.def
  - image-analysis/deepmedic/0.8.2/deepmedic.def
  - image-analysis/broccoli/1.0.1/broccoli.def
  - image-analysis/qsiprep/0.8.0/qsiprep.def
  - image-analysis/fitlins/0.8.0/fitlins.def
  - image-analysis/fitlins/0.7.0/fitlins.def
  - image-analysis/minc-toolkit/1.9.16/minc-toolkit.def
  - image-analysis/minc-toolkit/1.9.18/minc-toolkit.def
  - image-analysis/resmap/1.95/resmap.def
  - image-analysis/fmriprep/20.2.1/fmriprep.def
  - image-analysis/fmriprep/20.0.5/fmriprep.def
  - image-analysis/fmriprep/20.1.1/fmriprep.def
  - image-analysis/fmriprep/20.1.3/fmriprep.def
  - image-analysis/fmriprep/20.2.0/fmriprep.def
  - image-analysis/baracus/1.1.4/baracus.def
  - computational-chemistry/ampl/f35623d4/ampl.def
  - mass-spectrometry/maxquant/1.6.17.0/maxquant.def
  - mass-spectrometry/maxquant/1.6.3.3/maxquant.def
  - mass-spectrometry/maxquant/1.6.7.0/maxquant.def
  - utilities/sysbench/1.0.11/sysbench.def
  - utilities/sysbench/1.0.20/sysbench.def
  - utilities/uropa/3.5.0/uropa.def
  - utilities/pyega3/3.3.0/pyega3.def
  - utilities/whatshap/0.18/whatshap.def
  - utilities/snp-sites/2.4.1/snp-sites.def
  - utilities/gdc-client/1.5.0/gdc-client.def
  - utilities/datalad/0.13.0rc2/datalad.def
  - utilities/visidata/2.2/visidata.def
  - utilities/ariba/2.14.4/ariba.def
  - utilities/atom/1.13.1/atom.def
  - utilities/xvfb/1.19.6/xvfb.def
  - utilities/longshot/0.3.5/longshot.def
  - utilities/pdf2svg/0.2.3/pdf2svg.def
  - utilities/vcf2db/2020.09.14/vcf2db.def
  - high-throughput-sequencing/pepr/1.1.24/pepr.def
  - high-throughput-sequencing/cicero/0.3.0/cicero.def
  - high-throughput-sequencing/deepsignal/0.1.8/deepsignal.def
  - high-throughput-sequencing/bamutil/1.0.15/bamutil.def
  - high-throughput-sequencing/vagrent/3.3.4/vagrent.def
  - high-throughput-sequencing/metaphlan/3.0/metaphlan.def
  - high-throughput-sequencing/metaphlan/3.0.6/metaphlan.def
  - high-throughput-sequencing/hap.py/0.3.9/hap.py.def
  - high-throughput-sequencing/rilseq/0.75/rilseq.def
  - high-throughput-sequencing/tetoolkit/2.2.1/tetoolkit.def
  - high-throughput-sequencing/tetoolkit/2.1.4/tetoolkit.def
  - high-throughput-sequencing/ascatngs/4.3.4/ascatngs.def
  - high-throughput-sequencing/ascatngs/4.3.3/ascatngs.def
  - high-throughput-sequencing/ascatngs/4.5.0/ascatngs.def
  - high-throughput-sequencing/htseq/0.11.4/htseq.def
  - high-throughput-sequencing/metabat/2.13/metabat.def
  - high-throughput-sequencing/atropos/1.1.18/atropos.def
  - high-throughput-sequencing/epic2/0.0.41/epic2.def
  - high-throughput-sequencing/deeptools/3.4.2/deeptools.def
  - high-throughput-sequencing/deeptools/3.5.0/deeptools.def
  - high-throughput-sequencing/vep/101/vep.def
  - high-throughput-sequencing/vep/103/vep.def
  - high-throughput-sequencing/vep/97/vep.def
  - high-throughput-sequencing/atac_dnase_pipelines/0.3.4-19-gcbd2a00/atac_dnase_pipelines.def
  - high-throughput-sequencing/sve/0.1.0/sve.def
  - high-throughput-sequencing/sicer/2-1.0.2/sicer.def
  - high-throughput-sequencing/salmon/1.4.0/salmon.def
  - high-throughput-sequencing/dropest/0.8.6/dropest.def
  - high-throughput-sequencing/tandemtools/current/tandemtools.def
  - high-throughput-sequencing/multiqc/1.9/multiqc.def
  - high-throughput-sequencing/multiqc/1.10/multiqc.def
  - high-throughput-sequencing/svtk/0.1/svtk.def
  - high-throughput-sequencing/ricopili/2019_Jun_25.001/ricopili.def
  - high-throughput-sequencing/bison/0.4.0/bison.def
  - high-throughput-sequencing/umitools/1.1.1/umitools.def
  - high-throughput-sequencing/eager/1.92/eager.def
  - high-throughput-sequencing/deepvariant/0.10.0/deepvariant.def
  - high-throughput-sequencing/deepvariant/0.9.0/deepvariant.def
  - high-throughput-sequencing/deepvariant/1.1.0/deepvariant.def
  - high-throughput-sequencing/macs/2.2.7.1/macs.def
  - high-throughput-sequencing/medaka/1.0.3/medaka.def
  - high-throughput-sequencing/medaka/0.12.1/medaka.def
  - high-throughput-sequencing/medaka/1.2.0/medaka.def
  - high-throughput-sequencing/svtyper/0.7.1/svtyper.def
  - high-throughput-sequencing/scramble/0.0.20190211.82c78b9/scramble.def
  - high-throughput-sequencing/scramble/1.0.1-32893ef/scramble.def
  - high-throughput-sequencing/parliament/0.1.7/parliament.def
  - high-throughput-sequencing/cutadapt/3.0/cutadapt.def
  - high-throughput-sequencing/cutadapt/2.10/cutadapt.def
  - high-throughput-sequencing/cutadapt/1.18/cutadapt.def
  - high-throughput-sequencing/abruijn/1.0/abruijn.def
  - high-throughput-sequencing/canvas/1.40/canvas.def
  - high-throughput-sequencing/seqlinkage/1.0/seqlinkage.def
  - high-throughput-sequencing/gossamer/ac492a8/gossamer.def
  - high-throughput-sequencing/hicexplorer/3.5.1/hicexplorer.def
  - high-throughput-sequencing/hicpro/2.11.4/hicpro.def
  - high-throughput-sequencing/pvactools/2.0.1/pvactools.def
  - high-throughput-sequencing/pvactools/1.5.5/pvactools.def
  - high-throughput-sequencing/mtoolbox/1.1/mtoolbox.def
  - high-throughput-sequencing/bamsurgeon/1111e5d/bamsurgeon.def
  - high-throughput-sequencing/gridss/2.9.4/gridss.def
  - high-throughput-sequencing/bigscale2/20191119/bigscale2.def
  - high-throughput-sequencing/xengsort/28762aac/xengsort.def
  - high-throughput-sequencing/csvkit/1.0.5/csvkit.def
  - high-throughput-sequencing/cnvkit/0.9.8/cnvkit.def
  - high-throughput-sequencing/cnvkit/0.9.6/cnvkit.def
  - high-throughput-sequencing/fusioninspector/2.5.0/fusioninspector.def
  - high-throughput-sequencing/fusioninspector/2.3.0/fusioninspector.def
  - high-throughput-sequencing/megalodon/2.2.9/megalodon.def
  - high-throughput-sequencing/slamdunk/0.4.3/slamdunk.def
  - high-throughput-sequencing/rsd/1.1.7/rsd.def
  - high-throughput-sequencing/tvc/5.10.1/tvc.def
  - high-throughput-sequencing/maggie/0.3.4/maggie.def
  - high-throughput-sequencing/flye/2.8-1/flye.def
  - high-throughput-sequencing/flye/2.7/flye.def
  - high-throughput-sequencing/tpmcalculator/0.0.4/tpmcalculator.def
  - high-throughput-sequencing/tpmcalculator/0.0.3/tpmcalculator.def
  - high-throughput-sequencing/idep/0.81/idep.def
  - high-throughput-sequencing/transvar/2.5.9/transvar.def
  - high-throughput-sequencing/flappie/1.0.0/flappie.def
  - high-throughput-sequencing/flappie/2.1.3/flappie.def
  - high-throughput-sequencing/cellsnp/0.1.7/cellsnp.def
  - high-throughput-sequencing/cellsnp/0.3.2/cellsnp.def
  - high-throughput-sequencing/crossmap/0.5.2/crossmap.def
  - high-throughput-sequencing/brass/6.3.4/brass.def
  - high-throughput-sequencing/brass/6.1.2/brass.def
  - high-throughput-sequencing/rseqc/4.0.0/rseqc.def
  - high-throughput-sequencing/hail/0.2.61/hail.def
  - high-throughput-sequencing/hail/0.2.56/hail.def
  - high-throughput-sequencing/hail/0.2.3/hail.def
  - high-throughput-sequencing/rmats/4.0.2/rmats.def
  - high-throughput-sequencing/mitosuite/1.0.9b/mitosuite.def
  - high-throughput-sequencing/taiji/1.1.0/taiji.def
  - high-throughput-sequencing/taiji/1.2.0/taiji.def
  - high-throughput-sequencing/humann2/2.8.1/humann2.def
  - high-throughput-sequencing/vireosnp/0.5.1/vireosnp.def
  - high-throughput-sequencing/vireosnp/0.3.2/vireosnp.def
  - high-throughput-sequencing/cnvnator/0.4.1/cnvnator.def
  - high-throughput-sequencing/delly/0.8.7/delly.def
  - high-throughput-sequencing/mageck-vispr/0.5.4/mageck-vispr.def
  - high-throughput-sequencing/guppy/4.2.2/guppy.def
  - high-throughput-sequencing/guppy/3.4.5/guppy.def
  - high-throughput-sequencing/guppy/4.0.15/guppy.def
  - high-throughput-sequencing/cancerit-wgs/2.1.0/cancerit-wgs.def
  - high-throughput-sequencing/repeatmodeler/2.0.1/repeatmodeler.def
  - high-throughput-sequencing/surpi/1.0.67/surpi.def
  - high-throughput-sequencing/bamliquidator/1.3.8/bamliquidator.def
  - high-throughput-sequencing/neusomatic/0.2.1/neusomatic.def
  - high-throughput-sequencing/rnapeg/current/rnapeg.def
  - high-throughput-sequencing/biom-format/2.1.10/biom-format.def
  - high-throughput-sequencing/pcap-core/4.3.5/pcap-core.def
  - high-throughput-sequencing/humann/3.0.0-alpha.3/humann.def
  - high-throughput-sequencing/crispresso/2.0.40/crispresso.def
  - high-throughput-sequencing/crispresso/2.0.45/crispresso.def
  - high-throughput-sequencing/busco/5.0.0/busco.def
  - high-throughput-sequencing/busco/4.1.3/busco.def
  - high-throughput-sequencing/bamreadcount/cram-v0.0.1/bamreadcount.def
  - high-throughput-sequencing/lefse/1.0.8/lefse.def
  - high-throughput-sequencing/lefse/1.0.7/lefse.def
  - high-throughput-sequencing/cgpbattenberg/3.5.3/cgpbattenberg.def
  - high-throughput-sequencing/pychopper/2.4.0/pychopper.def
  - high-throughput-sequencing/freebayes/1.3.5/freebayes.def
  - high-throughput-sequencing/raremetal/4.15.1/raremetal.def
  - high-throughput-sequencing/stream/20180816/stream.def
  - linkage-phylogenetics/bali-phy/3.5/bali-phy.def
  - linkage-phylogenetics/gubbins/2.3.4/gubbins.def
  - structural-biology/parsnip/20180507/parsnip.def
  - structural-biology/rdock/2013.1/rdock.def
  - structural-biology/pymol/2.4.0/pymol.def
  - structural-biology/pymol/2.3.0/pymol_2.3.0.def
  - deep-learning/basset/0.1.0/basset.def
  - deep-learning/deeplab/20180816/deeplab.def
  - deep-learning/caffe2/0.8.1/caffe2.def
  - deep-learning/dextr-pytorch/20180710/dextr-pytorch.def
  - deep-learning/tensorrt/18.09/tensorrt.def
  - deep-learning/clairvoyante/1.0/clairvoyante.def
  - deep-learning/polyrnnpp/20180718/polyrnnpp.def
  - deep-learning/few-shot-ssl/20180723/few-shot-ssl.def
  - deep-learning/digits/6.0/digits.def
  - deep-learning/unet/20180704/unet.def
  - mathematical-statistics/omeclust/1.1.4/omeclust.def
  - mathematical-statistics/omeclust/1.1.6/omeclust.def
  - mathematical-statistics/m2clust/1.1.3/m2clust.def
  - mathematical-statistics/m2clust/0.0.7/m2clust.def
  - mathematical-statistics/m2clust/0.0.8/m2clust.def
  full_name: NIH-HPC/singularity-def-files
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-nih-hpc-singularity-definition-files\" class=\"\
    anchor\" href=\"#nih-hpc-singularity-definition-files\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>NIH HPC Singularity\
    \ Definition Files</h1>\n<p>These definition files and wrapper scripts are used\
    \ by the <a href=\"https://hpc.nih.gov/\" rel=\"nofollow\">NIH HPC (Biowulf)</a>\
    \ staff to install containerized applications using <a href=\"https://github.com/sylabs/singularity\"\
    >Singularity</a>. Each app is installed in a self-contained directory and access\
    \ to the app is controlled through a module system (<a href=\"https://github.com/TACC/Lmod\"\
    >Lmod</a>). This strategy allows users to transparently access apps that are installed\
    \ within containers as though they were installed directly on the host system.\
    \ More details can be found <a href=\"https://hpc.nih.gov/apps/singularity.html#bind-stationary\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>Typically, apps are installed under in a\
    \ directory structure like so:</p>\n<pre><code>$ tree appname/ver\nappname/ver\n\
    |-- bin\n|   |-- cmd1 -&gt; ../libexec/wrapper.sh\n|   |-- cmd2 -&gt; ../libexec/wrapper.sh\n\
    |   `-- cmd3 -&gt; ../libexec/wrapper.sh\n`-- libexec\n    |-- app.sif\n    `--\
    \ wrapper.sh\n</code></pre>\n<p>Because <code>wrapper.sh</code> is written to\
    \ be introspective, any command symlinked to it will be carried through and executed\
    \ within the associated container. The wrapper script is also sufficiently generic\
    \ that it can be reused across apps with little or no modification.</p>\n<p>Each\
    \ app has its own <code>README.md</code> that contains:</p>\n<ul>\n<li>a link\
    \ to the NIH HPC app page or developer's documentation</li>\n<li>a list of symlinks\
    \ that should be created to the wrapper script to expose executables within the\
    \ container</li>\n<li>any app specific installation notes</li>\n</ul>\n<p>Finally,\
    \ please note that these definition files <strong>are not guaranteed to reproduce\
    \ the same container, or even to produce any container at all</strong>. The internet,\
    \ upon which these definition files are based, is subject to change without notice.\
    \ These definition files are therefore intended to be treated as (potentially)\
    \ helpful suggestions.</p>\n<h2>\n<a id=\"user-content-computational-chemistry\"\
    \ class=\"anchor\" href=\"#computational-chemistry\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"/computational-chemistry\"\
    >Computational Chemistry</a>\n</h2>\n<ul>\n<li><a href=\"/computational-chemistry/ampl\"\
    >ampl</a></li>\n</ul>\n<h2>\n<a id=\"user-content-deep-learning\" class=\"anchor\"\
    \ href=\"#deep-learning\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"/deep-learning\">Deep Learning</a>\n\
    </h2>\n<ul>\n<li><a href=\"/deep-learning/caffe2\">Caffe2</a></li>\n<li><a href=\"\
    /deep-learning/dextr-pytorch\">DEXTR-PyTorch</a></li>\n<li><a href=\"/deep-learning/polyrnnpp\"\
    >PolyRNNpp</a></li>\n<li><a href=\"/deep-learning/basset\">basset</a></li>\n<li><a\
    \ href=\"/deep-learning/clairvoyante\">clairvoyante</a></li>\n<li><a href=\"/deep-learning/deeplab\"\
    >deeplab</a></li>\n<li><a href=\"/deep-learning/digits\">digits</a></li>\n<li><a\
    \ href=\"/deep-learning/few-shot-ssl\">few-shot-ssl</a></li>\n<li><a href=\"/deep-learning/tensorrt\"\
    >tensorrt</a></li>\n<li><a href=\"/deep-learning/unet\">unet</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-high-throughput-sequencing\" class=\"anchor\" href=\"\
    #high-throughput-sequencing\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><a href=\"/high-throughput-sequencing\"\
    >High Throughput Sequencing</a>\n</h2>\n<ul>\n<li><a href=\"/high-throughput-sequencing/atac_dnase_pipelines\"\
    >ATAC-Seq / DNase-Seq Pipeline</a></li>\n<li><a href=\"/high-throughput-sequencing/ascatngs\"\
    >AscatNGS</a></li>\n<li><a href=\"/high-throughput-sequencing/atropos\">Atropos</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/bamsurgeon\">BAMSurgeon</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/brass\">BRASS</a></li>\n<li><a href=\"/high-throughput-sequencing/canvas\"\
    >Canvas</a></li>\n<li><a href=\"/high-throughput-sequencing/maggie\">MAGGIE</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/pcap-core\">PCAP-core</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/pepr\">PePr</a></li>\n<li><a href=\"/high-throughput-sequencing/rsd\"\
    >RSD</a></li>\n<li><a href=\"/high-throughput-sequencing/surpi\">SURPI</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/tpmcalculator\">TPMCalculator</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/tvc\">TVC</a></li>\n<li><a href=\"/high-throughput-sequencing/vagrent\"\
    >VAGrENT</a></li>\n<li><a href=\"/high-throughput-sequencing/vep\">VEP</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/abruijn\">abruijn</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamliquidator\">bamliquidator</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamreadcount\">bamreadcount</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamutil\">bamutil</a></li>\n<li><a href=\"/high-throughput-sequencing/bigscale2\"\
    >bigscale2</a></li>\n<li><a href=\"/high-throughput-sequencing/biom-format\">biom-format</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/bison\">bison</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/busco\">busco</a></li>\n<li><a href=\"/high-throughput-sequencing/cancerit-wgs\"\
    >cancerit-wgs</a></li>\n<li><a href=\"/high-throughput-sequencing/cellsnp\">cellsnp</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/cgpbattenberg\">cgpBattenberg</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/cicero\">cicero</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/cnvkit\">cnvkit</a></li>\n<li><a href=\"/high-throughput-sequencing/cnvnator\"\
    >cnvnator</a></li>\n<li><a href=\"/high-throughput-sequencing/crispresso\">crispresso</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/crossmap\">crossmap</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/csvkit\">csvkit</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/cutadapt\">cutadapt</a></li>\n<li><a href=\"/high-throughput-sequencing/deepsignal\"\
    >deepsignal</a></li>\n<li><a href=\"/high-throughput-sequencing/deeptools\">deeptools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/deepvariant\">deepvariant</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/delly\">delly</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/dropest\">dropest</a></li>\n<li><a href=\"/high-throughput-sequencing/eager\"\
    >eager</a></li>\n<li><a href=\"/high-throughput-sequencing/epic2\">epic2</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/flappie\">flappie</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/flye\">flye</a></li>\n<li><a href=\"/high-throughput-sequencing/freebayes\"\
    >freebayes</a></li>\n<li><a href=\"/high-throughput-sequencing/fusioninspector\"\
    >fusioninspector</a></li>\n<li><a href=\"/high-throughput-sequencing/gossamer\"\
    >gossamer</a></li>\n<li><a href=\"/high-throughput-sequencing/gridss\">gridss</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/guppy\">guppy</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/hail\">hail</a></li>\n<li><a href=\"/high-throughput-sequencing/hap.py\"\
    >hap.py</a></li>\n<li><a href=\"/high-throughput-sequencing/hicexplorer\">hicexplorer</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/hicpro\">hicpro</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/htseq\">htseq</a></li>\n<li><a href=\"/high-throughput-sequencing/humann2\"\
    >humann2</a></li>\n<li><a href=\"/high-throughput-sequencing/idep\">idep</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/lefse\">lefse</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/macs\">macs</a></li>\n<li><a href=\"/high-throughput-sequencing/mageck-vispr\"\
    >mageck-vispr</a></li>\n<li><a href=\"/high-throughput-sequencing/medaka\">medaka</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/megalodon\">megalodon</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/metabat\">metabat</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/metaphlan\">metaphlan</a></li>\n<li><a href=\"/high-throughput-sequencing/mitosuite\"\
    >mitosuite</a></li>\n<li><a href=\"/high-throughput-sequencing/mtoolbox\">mtoolbox</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/multiqc\">multiqc</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/neusomatic\">neusomatic</a></li>\n<li><a href=\"/high-throughput-sequencing/parliament\"\
    >parliament</a></li>\n<li><a href=\"/high-throughput-sequencing/pvactools\">pvactools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/pychopper\">pychopper</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/raremetal\">raremetal</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/repeatmodeler\">repeatmodeler</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/ricopili\">ricopili</a></li>\n<li><a href=\"/high-throughput-sequencing/rilseq\"\
    >rilseq</a></li>\n<li><a href=\"/high-throughput-sequencing/rmats\">rmats</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/rnapeg\">rnapeg</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/rseqc\">rseqc</a></li>\n<li><a href=\"/high-throughput-sequencing/salmon\"\
    >salmon</a></li>\n<li><a href=\"/high-throughput-sequencing/scramble\">scramble</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/seqlinkage\">seqlinkage</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/sicer\">sicer</a></li>\n<li><a href=\"/high-throughput-sequencing/slamdunk\"\
    >slamdunk</a></li>\n<li><a href=\"/high-throughput-sequencing/stream\">stream</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/sve\">sve</a></li>\n<li><a href=\"/high-throughput-sequencing/svtk\"\
    >svtk</a></li>\n<li><a href=\"/high-throughput-sequencing/svtyper\">svtyper</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/taiji\">taiji</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/tandemtools\">tandemtools</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/tetoolkit\">tetoolkit</a></li>\n<li><a href=\"/high-throughput-sequencing/transvar\"\
    >transvar</a></li>\n<li><a href=\"/high-throughput-sequencing/umitools\">umitools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/vireosnp\">vireosnp</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/xengsort\">xengsort</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-image-analysis\" class=\"anchor\" href=\"#image-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/image-analysis\">Image Analysis</a>\n</h2>\n<ul>\n<li><a\
    \ href=\"/image-analysis/resmap\">ResMap</a></li>\n<li><a href=\"/image-analysis/terastitcher\"\
    >TeraStitcher</a></li>\n<li><a href=\"/image-analysis/baracus\">baracus</a></li>\n\
    <li><a href=\"/image-analysis/broccoli\">broccoli</a></li>\n<li><a href=\"/image-analysis/civet\"\
    >civet</a></li>\n<li><a href=\"/image-analysis/ctf\">ctf</a></li>\n<li><a href=\"\
    /image-analysis/deepmedic\">deepmedic</a></li>\n<li><a href=\"/image-analysis/fitlins\"\
    >fitlins</a></li>\n<li><a href=\"/image-analysis/fmriprep\">fmriprep</a></li>\n\
    <li><a href=\"/image-analysis/minc-toolkit\">minc-toolkit</a></li>\n<li><a href=\"\
    /image-analysis/mriqc\">mriqc</a></li>\n<li><a href=\"/image-analysis/mrtrix\"\
    >mrtrix</a></li>\n<li><a href=\"/image-analysis/qsiprep\">qsiprep</a></li>\n<li><a\
    \ href=\"/image-analysis/tesseract\">tesseract</a></li>\n<li><a href=\"/image-analysis/topaz\"\
    >topaz</a></li>\n<li><a href=\"/image-analysis/xcpengine\">xcpengine</a></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-linkage-phylogenetics\" class=\"anchor\" href=\"\
    #linkage-phylogenetics\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"/linkage-phylogenetics\">Linkage Phylogenetics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/linkage-phylogenetics/bali-phy\">bali-phy</a></li>\n\
    <li><a href=\"/linkage-phylogenetics/gubbins\">gubbins</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-mass-spectrometry\" class=\"anchor\" href=\"#mass-spectrometry\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/mass-spectrometry\">Mass Spectrometry</a>\n</h2>\n<ul>\n\
    <li><a href=\"/mass-spectrometry/maxquant\">maxquant</a></li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-mathematicalstatistics\" class=\"anchor\" href=\"#mathematicalstatistics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/mathematical-statistics\">Mathematical/Statistics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/mathematical-statistics/m2clust\">m2clust</a></li>\n\
    <li><a href=\"/mathematical-statistics/omeclust\">omeClust</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-molecular-modeling-graphics\" class=\"anchor\" href=\"#molecular-modeling-graphics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/molecular-modeling-graphics\">Molecular Modeling Graphics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/molecular-modeling-graphics/chimerax\">ChimeraX</a></li>\n\
    <li><a href=\"/molecular-modeling-graphics/blender\">blender</a></li>\n<li><a\
    \ href=\"/molecular-modeling-graphics/starseqr\">starseqr</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-sequence-analysis\" class=\"anchor\" href=\"#sequence-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/sequence-analysis\">Sequence Analysis</a>\n</h2>\n<ul>\n\
    <li><a href=\"/sequence-analysis/acfs\">ACFS</a></li>\n<li><a href=\"/sequence-analysis/annogesic\"\
    >ANNOgesic</a></li>\n<li><a href=\"/sequence-analysis/asgal\">ASGAL</a></li>\n\
    <li><a href=\"/sequence-analysis/accurity\">Accurity</a></li>\n<li><a href=\"\
    /sequence-analysis/eukrep\">EukRep</a></li>\n<li><a href=\"/sequence-analysis/glu\"\
    >GLU</a></li>\n<li><a href=\"/sequence-analysis/htgtsrep\">HTGTSrep</a></li>\n\
    <li><a href=\"/sequence-analysis/orffinder\">ORFfinder</a></li>\n<li><a href=\"\
    /sequence-analysis/saige\">SAIGE</a></li>\n<li><a href=\"/sequence-analysis/vcf-kit\"\
    >VCF-kit</a></li>\n<li><a href=\"/sequence-analysis/anvio\">anvio</a></li>\n<li><a\
    \ href=\"/sequence-analysis/arriba\">arriba</a></li>\n<li><a href=\"/sequence-analysis/augustus\"\
    >augustus</a></li>\n<li><a href=\"/sequence-analysis/bamgineer\">bamgineer</a></li>\n\
    <li><a href=\"/sequence-analysis/braker\">braker</a></li>\n<li><a href=\"/sequence-analysis/cactus\"\
    >cactus</a></li>\n<li><a href=\"/sequence-analysis/chipseq_pipeline\">chipseq_pipeline</a></li>\n\
    <li><a href=\"/sequence-analysis/cicero\">cicero</a></li>\n<li><a href=\"/sequence-analysis/deepsea\"\
    >deepsea</a></li>\n<li><a href=\"/sequence-analysis/focus\">focus</a></li>\n<li><a\
    \ href=\"/sequence-analysis/glnexus\">glnexus</a></li>\n<li><a href=\"/sequence-analysis/intarna\"\
    >intarna</a></li>\n<li><a href=\"/sequence-analysis/ldsc\">ldsc</a></li>\n<li><a\
    \ href=\"/sequence-analysis/m-tools\">m-tools</a></li>\n<li><a href=\"/sequence-analysis/mmarge\"\
    >mmarge</a></li>\n<li><a href=\"/sequence-analysis/netoglyc\">netOglyc</a></li>\n\
    <li><a href=\"/sequence-analysis/phaser\">phaser</a></li>\n<li><a href=\"/sequence-analysis/prokka\"\
    >prokka</a></li>\n<li><a href=\"/sequence-analysis/qtltools\">qtltools</a></li>\n\
    <li><a href=\"/sequence-analysis/roary\">roary</a></li>\n<li><a href=\"/sequence-analysis/smoove\"\
    >smoove</a></li>\n<li><a href=\"/sequence-analysis/sonicparanoid\">sonicparanoid</a></li>\n\
    <li><a href=\"/sequence-analysis/svtools\">svtools</a></li>\n<li><a href=\"/sequence-analysis/wisexome\"\
    >wisexome</a></li>\n<li><a href=\"/sequence-analysis/xhla\">xHLA</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-structural-biology\" class=\"anchor\" href=\"#structural-biology\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/structural-biology\">Structural Biology</a>\n</h2>\n<ul>\n\
    <li><a href=\"/structural-biology/parsnip\">parsnip</a></li>\n<li><a href=\"/structural-biology/pymol\"\
    >pymol</a></li>\n<li><a href=\"/structural-biology/rdock\">rDock</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-systems-biology\" class=\"anchor\" href=\"#systems-biology\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/systems-biology\">Systems Biology</a>\n</h2>\n<ul>\n<li><a\
    \ href=\"/systems-biology/cellphonedb\">cellphonedb</a></li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-utilities\" class=\"anchor\" href=\"#utilities\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"/utilities\">Utilities</a>\n</h2>\n<ul>\n<li><a href=\"/utilities/xvfb\"\
    >Xvfb</a></li>\n<li><a href=\"/utilities/ariba\">ariba</a></li>\n<li><a href=\"\
    /utilities/atom\">atom</a></li>\n<li><a href=\"/utilities/datalad\">datalad</a></li>\n\
    <li><a href=\"/utilities/gdc-client\">gdc-client</a></li>\n<li><a href=\"/utilities/longshot\"\
    >longshot</a></li>\n<li><a href=\"/utilities/pdf2svg\">pdf2svg</a></li>\n<li><a\
    \ href=\"/utilities/pyega3\">pyega3</a></li>\n<li><a href=\"/utilities/snp-sites\"\
    >snp-sites</a></li>\n<li><a href=\"/utilities/sysbench\">sysbench</a></li>\n<li><a\
    \ href=\"/utilities/uropa\">uropa</a></li>\n<li><a href=\"/utilities/vcf2db\"\
    >vcf2db</a></li>\n<li><a href=\"/utilities/visidata\">visidata</a></li>\n<li><a\
    \ href=\"/utilities/whatshap\">whatshap</a></li>\n</ul>\n"
  stargazers_count: 8
  subscribers_count: 6
  topics: []
  updated_at: 1622586137.0
NYU-Molecular-Pathology/NGS580-nf:
  data_format: 2
  description: Target exome sequencing analysis for NYU NGS580 gene panel
  filenames:
  - containers/sambamba-0.6.8/Singularity.sambamba-0.6.8
  - containers/lofreq-2.1.3/Singularity.lofreq-2.1.3
  - containers/cnv_facets-0.14.0/Singularity.cnv_facets-0.14.0
  - containers/deconstructSigs-1.8.0/Singularity.deconstructSigs-1.8.0
  - containers/annovar-150617/Singularity.annovar-150617
  - containers/variant-calling-0.0.2/Singularity.variant-calling-0.0.2
  - containers/R-3.2.3/Singularity.R-3.2.3
  - containers/bedtools-2.27.1/Singularity.bedtools-2.27.1
  - containers/python-2.7/Singularity.python-2.7
  - containers/samtools-1.7/Singularity.samtools-1.7
  - containers/strelka-2.9.10/Singularity.strelka-2.9.10
  - containers/delly2-0.7.7/Singularity.delly2-0.7.7
  - containers/multiqc-1.4/Singularity.multiqc-1.4
  - containers/varscan-2.4.3/Singularity.varscan-2.4.3
  - containers/manta-1.5.0/Singularity.manta-1.5.0
  - containers/fastqc-0.11.7/Singularity.fastqc-0.11.7
  - containers/reporting-3.4.3/Singularity.reporting-3.4.3
  - containers/cnvkit-0.9.5/Singularity.cnvkit-0.9.5
  - containers/R-3.5.1/Singularity.R-3.5.1
  - containers/msisensor-0.2/Singularity.msisensor-0.2
  - containers/htslib-1.7/Singularity.htslib-1.7
  - containers/cnvkit-0.9.0/Singularity.cnvkit-0.9.0
  - containers/multiqc-1.5/Singularity.multiqc-1.5
  - containers/bwa-0.7.17/Singularity.bwa-0.7.17
  - containers/IGV-2.4.10/Singularity.IGV-2.4.10
  - containers/bedtools-2.26.0/Singularity.bedtools-2.26.0
  - containers/sambamba-0.6.6/Singularity.sambamba-0.6.6.old
  - containers/sambamba-0.6.6/Singularity.sambamba-0.6.6
  - containers/R-3.4.3/Singularity.R-3.4.3
  - containers/python-3.6/Singularity.python-3.6
  - containers/trimmomatic-0.36/Singularity.trimmomatic-0.36
  - containers/variant-calling-0.0.1/Singularity.variant-calling-0.0.1
  - containers/bwa-0.7.17-sambamba-0.6.8/Singularity.bwa-0.7.17-sambamba-0.6.8
  - containers/base/Singularity.base
  - containers/bcftools-1.3/Singularity.bcftools-1.3
  - containers/pindel-0.2.5b9/Singularity.pindel-0.2.5b9
  full_name: NYU-Molecular-Pathology/NGS580-nf
  latest_release: 19.10.1
  readme: '<h1>

    <a id="user-content-ngs580-nf" class="anchor" href="#ngs580-nf" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NGS580-nf</h1>

    <p>Target exome analysis for 580 gene panel (NGS580)</p>

    <p><strong>NOTE:</strong> Details listed here may change during development</p>

    <h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p>This pipeline is designed to run targeted exome analysis on Illumina Next-Gen
    sequencing genomic data, in support of the NGS580 cancer diagnostic panel for
    NYU''s Molecular Pathology Department.</p>

    <p>This pipeline starts from paired-end fastq data (<code>.fastq.gz</code>), and
    is meant to accompany the output from the Illumina demultiplexing pipeline listed
    here: <a href="https://github.com/NYU-Molecular-Pathology/demux-nf">https://github.com/NYU-Molecular-Pathology/demux-nf</a>.</p>

    <p>The NGS580-nf analysis workflow includes read trimming, QC, alignment, variant
    calling, annotation, and reporting, along with many other steps.</p>

    <h2>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h2>

    <p>Some key pipeline components included in this repository:</p>

    <ul>

    <li>

    <p><code>bin</code>: directory of custom scripts used throughout the pipeline</p>

    </li>

    <li>

    <p><code>containers</code>: directory of container recipes (Docker, Singularity)
    for use with the pipeline</p>

    </li>

    <li>

    <p><code>example</code>: directory of example samplesheets, etc., to show the
    format used with this pipeline</p>

    </li>

    <li>

    <p><code>targets</code>: directory of target region .bed files included with the
    pipeline for typical analyses</p>

    </li>

    <li>

    <p><code>Makefile</code>: A Makefile with recipes for configuring, starting, and
    managing the pipeline. This is meant to be the main interface between the end-user
    and the pipeline. The Makefile should be reviewed as-needed to familiarize yourself
    with the methods and configurations that are meant to be used for running and
    managing the pipeline.</p>

    </li>

    <li>

    <p><code>main.nf</code>: the main Nextflow pipeline script</p>

    </li>

    <li>

    <p><code>nextflow.config</code>: configuration file for the main Nextflow pipeline
    script</p>

    </li>

    <li>

    <p><code>.config.json</code>: a template for the required <code>config.json</code>
    file used in the pipeline, shows the default pipeline settings that are meant
    to be easily modified by the end-user and used within the pipeline for data processing.</p>

    </li>

    <li>

    <p><code>annovar_db.nf</code>, <code>cnv-pool.nf</code>, <code>hapmap-pool.nf</code>,
    <code>ref.nf</code>: workflows for generating and downloading extra reference
    files used in the main pipeline.</p>

    </li>

    <li>

    <p><code>ref</code>: default location for the storage of reference files (not
    used on NYU Big Purple HPC)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-pipeline-items" class="anchor" href="#pipeline-items" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline Items</h2>

    <p>Some key components that are created during setup, configuration, and execution
    of the pipeline:</p>

    <ul>

    <li>

    <p><code>samples.analysis.tsv</code>: the main samplesheet definig input items
    for the pipeline (described below)</p>

    </li>

    <li>

    <p><code>config.json</code>: configuration file used for pipeline settings (see
    <code>.config.json</code> template for example)</p>

    </li>

    <li>

    <p><code>output</code>: analysis output files published by the Nextflow pipeline</p>

    </li>

    <li>

    <p><code>work</code>: Nextflow temporary directories for execution of pipeline
    tasks</p>

    </li>

    <li>

    <p><code>trace.txt</code>, <code>nextflow.html</code>, <code>timeline.html</code>,
    <code>.nextflow.log</code>: Nextflow execution logs and reports</p>

    </li>

    <li>

    <p><code>logs</code>: directory for pipeline execution logs</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-setup" class="anchor" href="#setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h1>

    <p>This repository should first be cloned from GitHub:</p>

    <pre><code>git clone --recursive https://github.com/NYU-Molecular-Pathology/NGS580-nf.git

    cd NGS580-nf

    </code></pre>

    <ul>

    <li>Once a copy of the repo is made, it can be used to "deploy" new copies of
    the workflow in a pre-configured state</li>

    </ul>

    <h2>

    <a id="user-content-reference-data" class="anchor" href="#reference-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Reference Data</h2>

    <p>Nextflow pipelines have been included for downloading required reference data,
    including ANNOVAR reference databases. You can run them with the following command:</p>

    <pre><code>make setup

    </code></pre>

    <h3>

    <a id="user-content-hapmap-pool-bam" class="anchor" href="#hapmap-pool-bam" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HapMap Pool .bam</h3>

    <p>A negative control HapMap pool .bam file can be prepared using the following
    command:</p>

    <pre><code>make hapmap-pool

    </code></pre>

    <ul>

    <li>Requires <code>samples.hapmap.tsv</code> file specifying the .bam files to
    be combined (example included at <code>example/samples.hapmap.tsv</code>).</li>

    </ul>

    <p>This file is typically built from multiple HapMap samples previously aligned
    by this pipeline. For demonstration purposes, you can provide any .bam and .bai
    files.</p>

    <p>The HapMap Pool files to be used in the pipeline should be set under the <code>HapMapBam</code>
    and <code>HapMapBai</code> keys of <code>config.json</code>.</p>

    <h3>

    <a id="user-content-cnv-pool" class="anchor" href="#cnv-pool" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CNV Pool</h3>

    <p>A control normal sample .cnn file for CNV calling can be prepared using the
    following command:</p>

    <pre><code>make cnv-pool

    </code></pre>

    <ul>

    <li>Requires <code>samples.cnv.tsv</code> file specifying the .bam files to be
    used (example included at <code>example/samples.cnv.tsv</code>)</li>

    </ul>

    <p>This file is typically built from .bam files of specially chosen normal tissue
    sequencing samples previously aligned by this pipeline. For demonstration purposes,
    you can create the .cnn file from any desired .bam file. Note that the targets
    .bed file used to create the .cnn file must match the targets used in the rest
    of the pipeline.</p>

    <p>The .cnn file to be used in the pipeline should be set under the <code>CNVPool</code>
    key in <code>config.json</code>.</p>

    <h2>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Containers</h2>

    <p>The <code>containers</code> directory contains instructions and recipes for
    building the Docker and Singularity containers used in the pipeline.</p>

    <p>Docker is typically used for local container development, while Singularity
    containers are used on the NYU Big Purple HPC cluster. The current pipeline configuration
    for Big Purple uses <code>.simg</code> files stored in a common location on the
    file system.</p>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <p>The pipeline is designed to start from demultiplexed paired end <code>.fastq.gz</code>
    files, with sample ID, tumor ID, and matched normal ID associations defined for
    each set of R1 and R2 .fastq file using a file <code>samples.analysis.tsv</code>
    (example included at <code>example/samples.analysis.tsv</code>).</p>

    <h2>

    <a id="user-content-deployment" class="anchor" href="#deployment" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deployment</h2>

    <p>The easiset way to use the pipeline is to "deploy" a new instance of it based
    on output from the demultiplexing pipeline <a href="https://github.com/NYU-Molecular-Pathology/demux-nf"><code>demux-nf</code></a>.
    This will automatically propagate configurations and information from the demultiplexing
    output.</p>

    <p>The pipeline can also deploy a new, pre-configured copy of itself using the
    included <code>deploy</code> recipe:</p>

    <pre><code>make deploy PRODDIR=/path/to/NGS580_analyses RUNID=Name_for_analysis
    FASTQDIR=/path/to/fastq_files

    </code></pre>

    <ul>

    <li>An optional argument <code>DEMUX_SAMPLESHEET</code> can be used to provide
    a specially formatted demultiplexing samplesheet to be used for extracting extra
    sample information (example included at <code>example/demux-SampleSheet.csv</code>;
    note the extra columns labeling tumor-normal pair IDs, used later).</li>

    </ul>

    <h2>

    <a id="user-content-create-config" class="anchor" href="#create-config" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Create Config</h2>

    <p>A file <code>config.json</code> is required to hold settings for the pipeline.
    It should be created using the built-in methods:</p>

    <pre><code>make config RUNID=my_run_ID FASTQDIR=/path/to/fastqs

    </code></pre>

    <p>or</p>

    <pre><code>make config RUNID=my_run_ID FASTQDIRS=''/path/to/fastqs1 /path/to/fastqs2''

    </code></pre>

    <p>or</p>

    <pre><code>cp .config.json config.json

    </code></pre>

    <p>and then simply edit the new <code>config.json</code> and update the items
    to match your pipeline settings.</p>

    <p>Once created, the <code>config.json</code> file can be updated manually as
    needed. The template and default values can be viewed in the included <code>.config.json</code>
    file.</p>

    <ul>

    <li>

    <code>config.json</code> should be generated automatically if you used <code>make
    deploy</code>

    </li>

    </ul>

    <h2>

    <a id="user-content-create-samplesheet" class="anchor" href="#create-samplesheet"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    Samplesheet</h2>

    <p>A samplesheet file <code>samples.analysis.tsv</code> is required in order to
    define the input samples and their associated .fastq files (example included at
    <code>example/samples.analysis.tsv</code>). Create a samplesheet, based on the
    config file, using the built-in methods:</p>

    <pre><code>make samplesheet

    </code></pre>

    <ul>

    <li>Note that this uses the values previously saved in <code>config.json</code>
    to create the samplesheet</li>

    </ul>

    <h3>

    <a id="user-content-sample-pairs-optional" class="anchor" href="#sample-pairs-optional"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sample
    Pairs (Optional)</h3>

    <p>The NGS580-nf pipeline has special processing for tumor-normal pairs. These
    pairs should be defined in the <code>samples.analysis.tsv</code> file, by listing
    the matched Normal sample for each applicable sample.</p>

    <p>In order to update <code>samples.analysis.tsv</code> automatically with these
    sample pairs, an extra samplesheet can be provided with the tumor-normal pairs.</p>

    <p>Create a <code>samples.tumor.normal.csv</code> samplesheet (example included
    at <code>example/samples.tumor.normal.csv</code>) with the tumor-normal groupings
    for your samples, and update the original samplesheet with it by running the following
    script:</p>

    <pre><code>python update-samplesheets.py --tumor-normal-sheet samples.tumor.normal.csv

    </code></pre>

    <p>If a demultiplexing samplesheet with extra tumor-normal pairs information was
    supplied (see example: <code>example/demux-SampleSheet.csv</code>), then it can
    be used to update the samplesheet with pairs information with the following recipe:</p>

    <pre><code>make pairs PAIRS_SHEET=demux-SampleSheet.csv PAIRS_MODE=demux

    </code></pre>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <p>The pipeline includes an auto-run functionality that attempts to determine
    the best configuration to use for NYU phoenix and Big Purple HPC clusters:</p>

    <pre><code>make run

    </code></pre>

    <p>This will run the pipeline in the current session.</p>

    <p>In order to run the pipeline in the background as a job on NYU''s Big Purple
    HPC, you should instead use the <code>submit</code> recipe:</p>

    <pre><code>make submit SUBQ=fn_medium

    </code></pre>

    <p>Where <code>SUBQ</code> is the name of the SLURM queue you wish to use.</p>

    <p>Refer to the <code>Makefile</code> for more run options.</p>

    <p>Due to the scale of the pipeline, a "local" run option is not currently configured,
    but can be set up easily based on the details shown in the Makefile and <code>nextflow.config</code>.</p>

    <h3>

    <a id="user-content-extra-parameters" class="anchor" href="#extra-parameters"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Extra
    Parameters</h3>

    <p>You can supply extra parameters for Nextflow by using the <code>EP</code> variable
    included in the Makefile, like this:</p>

    <pre><code>make run EP=''--runID 180320_NB501073_0037_AH55F3BGX5

    </code></pre>

    <h3>

    <a id="user-content-demo" class="anchor" href="#demo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Demo</h3>

    <p>A demo dataset can be loaded using the following command:</p>

    <pre><code>make demo

    </code></pre>

    <p>This will:</p>

    <ul>

    <li>

    <p>checkout a demo dataset</p>

    </li>

    <li>

    <p>create a <code>samples.analysis.tsv</code> samplesheet for the analysis</p>

    </li>

    </ul>

    <p>You can then proceed to run the analysis with the commands described above.</p>

    <h2>

    <a id="user-content-more-functionality" class="anchor" href="#more-functionality"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More
    Functionality</h2>

    <p>Extra functions included in the Makefile for pipeline management include:</p>

    <h3>

    <a id="user-content-make-clean" class="anchor" href="#make-clean" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>make clean</code>

    </h3>

    <p>Removes all Nextflow output except for the most recent run. Use <code>make
    clean-all</code> to remove all pipeline outputs.</p>

    <h3>

    <a id="user-content-make-record-presome_prefix_" class="anchor" href="#make-record-presome_prefix_"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>make
    record PRE=some_prefix_</code>

    </h3>

    <p>"Records" copies of the most recent pipeline run''s output logs, configuration,
    Nextflow reports, etc.. Useful for recording analyses that failed or had errors
    in order to debug. Include the optional argument <code>TASK</code> to specify
    a Nextflow <code>work</code> directory to include in the records (example: <code>make
    record PRE=error_something_broke_ TASK=e9/d9ff34</code>).</p>

    <h3>

    <a id="user-content-make-kill" class="anchor" href="#make-kill" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>make kill</code>

    </h3>

    <p>Attempts to cleanly shut down a pipeline running on a remote host e.g. inside
    a SLURM HPC compute job. Note that you can also use <code>scancel</code> to halt
    the parent Nextflow pipeline job as well.</p>

    <h3>

    <a id="user-content-make-fix-permissions-make-fix-group" class="anchor" href="#make-fix-permissions-make-fix-group"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>make
    fix-permissions</code>, <code>make fix-group</code>

    </h3>

    <p>Attempts to fix usergroup and permissions issues that may arise on shared systems
    with multiple users. Be sure to use the extra argument <code>USERGROUP=somegroup</code>
    to specify the usergroup to update to.</p>

    <h3>

    <a id="user-content-make-finalize-work-rm" class="anchor" href="#make-finalize-work-rm"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>make
    finalize-work-rm</code>

    </h3>

    <p>Examines the <code>trace.txt</code> output from the most recent completed pipeline
    run in order to determine while subdirectories in the Nextflow <code>work</code>
    dir are no longer needed, and then deletes them. Can delete multiple subdirs in
    parallel when run with <code>make finalize-work-rm -j 20</code> e.g. specifying
    to delete 20 at a time, etc.</p>

    <h1>

    <a id="user-content-software" class="anchor" href="#software" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Software</h1>

    <p>Developed under Centos 6, RHEL 7, macOS 10.12</p>

    <ul>

    <li>

    <p>bash</p>

    </li>

    <li>

    <p>GNU <code>make</code>, standard GNU tools</p>

    </li>

    <li>

    <p>Python 2/3</p>

    </li>

    <li>

    <p>Java 8+ for Nextflow</p>

    </li>

    <li>

    <p>Docker/Singularity as needed for containers</p>

    </li>

    </ul>

    '
  stargazers_count: 8
  subscribers_count: 2
  topics:
  - nextflow
  - pipeline
  - docker
  - singularity
  - exome
  - exome-sequencing-analysis
  updated_at: 1599538191.0
NYU-Molecular-Pathology/demux-nf:
  data_format: 2
  description: Nextflow pipeline for Illumina NGS demultiplexing
  filenames:
  - containers/bcl2fastq-2.17.1/Singularity.bcl2fastq-2.17.1
  - containers/report-r-3.4.3/Singularity.report-r-3.4.3
  - containers/python-2.7/Singularity.python-2.7
  - containers/fastqc-0.11.7/Singularity.fastqc-0.11.7
  - containers/multiqc-1.5/Singularity.multiqc-1.5
  - containers/dos2unix-7.4.0/Singularity.dos2unix-7.4.0
  full_name: NYU-Molecular-Pathology/demux-nf
  latest_release: 19.04.0
  readme: "<h1>\n<a id=\"user-content-demux-nf\" class=\"anchor\" href=\"#demux-nf\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>demux-nf</h1>\n<p>Nextflow pipeline for demultiplexing Illumina Next-Gen\
    \ sequencing data.</p>\n<h1>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"\
    #usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h1>\n<p>Clone this repository:</p>\n<pre><code>git clone --recursive\
    \ https://github.com/NYU-Molecular-Pathology/demux-nf.git\n</code></pre>\n<h2>\n\
    <a id=\"user-content-deployment\" class=\"anchor\" href=\"#deployment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Deployment</h2>\n\
    <p>The included <code>deploy</code> recipe should be used to create a new directory\
    \ for demultiplexing based on a currently existing sequencing run directory. Include\
    \ arguments that describe the configuration for your sequencing run.</p>\n<pre><code>cd\
    \ demux-nf\nmake deploy RUNID=170809_NB501073_0019_AH5FFYBGX3 SAMPLESHEET=SampleSheet.csv\
    \ SEQTYPE=Archer\n</code></pre>\n<p>arguments:</p>\n<ul>\n<li>\n<p><code>RUNID</code>:\
    \ the identifier given to the run by the sequencer</p>\n</li>\n<li>\n<p><code>SAMPLESHEET</code>:\
    \ the samplesheet required for demultiplexing with <code>bcl2fastq</code></p>\n\
    </li>\n<li>\n<p><code>SEQTYPE</code>: the type of sequencing; currently only <code>Archer</code>\
    \ or <code>NGS580</code> are used</p>\n</li>\n<li>\n<p><code>SEQDIR</code>: parent\
    \ directory where the sequencer outputs its data (pre-configured for NYU server\
    \ locations)</p>\n</li>\n<li>\n<p><code>PRODDIR</code>: parent directory where\
    \ demultiplexing output should be stored (pre-configured for NYU server locations)</p>\n\
    </li>\n</ul>\n<p>This will first check that the specified run exists on the server\
    \ before cloning into a new directory at the given production output location\
    \ and configuring it for demultiplexing using the subsequent commands described\
    \ here.</p>\n<h2>\n<a id=\"user-content-run-workflow\" class=\"anchor\" href=\"\
    #run-workflow\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Run Workflow</h2>\n<p>Assuming you used <code>make\
    \ deploy</code> or <code>make config</code> to prepare your demultiplexing directory,\
    \ the following command can be used to automatically run the workflow based on\
    \ the pre-defined settings and settings from your current system.</p>\n<pre><code>make\
    \ run\n</code></pre>\n<p>Extra parameters to be passed to Nextflow can be supplied\
    \ with the <code>EP</code> argument:</p>\n<pre><code>make run EP='--samplesheet\
    \ SampleSheet.csv --runDir /path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3'\n\
    </code></pre>\n<p>To submit the parent Nextflow pipeline as a job on the HPC cluster:</p>\n\
    <pre><code>make submit\n\n# with a different submission queue:\nmake submit SUBQ=fn_long\n\
    \n# with a different submission time:\nmake submit SUBQ=cpu_long SUBTIME='--time=6-00:00:00'\n\
    </code></pre>\n<p>For alternative <code>run</code> methods, consult the <code>Makefile</code>.</p>\n\
    <h1>\n<a id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Configuration</h1>\n<p>Demultiplexing metadata for the workflow can\
    \ be provided through several methods, evaluated in the following order:</p>\n\
    <ul>\n<li>parameters can be supplied directly to Nextflow via CLI</li>\n</ul>\n\
    <pre><code>nextflow run main.nf --runID 12345\n</code></pre>\n<ul>\n<li>if the\
    \ file <code>config.json</code> is present, non-<code>null</code> parameters will\
    \ be retrieved</li>\n</ul>\n<pre><code>{\n    \"runDir\": \"/path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3\"\
    ,\n    \"samplesheet\": \"SampleSheet.csv\",\n    \"runID\": \"170809_NB501073_0019_AH5FFYBGX3\"\
    \n}\n</code></pre>\n<ul>\n<li>\n<p>this file is generated automatically during\
    \ the <code>deploy</code> step, using the included <code>config.py</code> script</p>\n\
    </li>\n<li>\n<p>the following items in the current directory will be used if present:</p>\n\
    <ul>\n<li>\n<p><code>SampleSheet.csv</code>: default samplesheet file</p>\n</li>\n\
    <li>\n<p><code>runDir</code> : default sequencing run source directory (can be\
    \ a symlink)</p>\n</li>\n<li>\n<p><code>runID.txt</code>: a text file, the first\
    \ line of which will be used as the run ID</p>\n</li>\n</ul>\n</li>\n</ul>\n<h1>\n\
    <a id=\"user-content-extras\" class=\"anchor\" href=\"#extras\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Extras</h1>\n\
    <ul>\n<li>(re)initialize configurations (overwrites old <code>config.json</code>):</li>\n\
    </ul>\n<pre><code>make config RUNDIR=/path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3\
    \ SAMPLESHEET=SampleSheet.csv RUNID=170809_NB501073_0019_AH5FFYBGX3\n</code></pre>\n\
    <ul>\n<li>update an existing directory to the latest version of this repo:</li>\n\
    </ul>\n<pre><code>make update\n</code></pre>\n<ul>\n<li>clean up workflow intermediary\
    \ files to save space (workflow cannot be resumed after this):</li>\n</ul>\n<pre><code>make\
    \ finalize\n</code></pre>\n<ul>\n<li>clean up output from all old workflows (saves\
    \ current workflow output):</li>\n</ul>\n<pre><code>make clean\n</code></pre>\n\
    <ul>\n<li>delete the output from all workflows:</li>\n</ul>\n<pre><code>make clean-all\n\
    </code></pre>\n<ul>\n<li>mark that the demultiplexing suceeded and the results\
    \ passed QC for downstream analysis:</li>\n</ul>\n<pre><code>make passed\n</code></pre>\n\
    <ul>\n<li>deploy a new NGS580 analysis using the current results:</li>\n</ul>\n\
    <pre><code>make deploy-NGS580\n</code></pre>\n<ul>\n<li>make a 'deliverables'\
    \ directory with just the results for samples for a specific client</li>\n</ul>\n\
    <pre><code>make deliverable CLIENT=somelab SHEET=list_of_clients_samples.txt\n\
    </code></pre>\n<h1>\n<a id=\"user-content-software\" class=\"anchor\" href=\"\
    #software\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Software</h1>\n<p>Required:</p>\n<ul>\n<li>\n<p>Java 8 (Nextflow)</p>\n\
    </li>\n<li>\n<p>Python 2.7+</p>\n</li>\n<li>\n<p>GNU <code>make</code></p>\n</li>\n\
    </ul>\n<p>Optional; must be installed to system or available with Singularity\
    \ containers:</p>\n<ul>\n<li>\n<p><code>bcl2fastq</code> version 2.17.1</p>\n\
    </li>\n<li>\n<p>FastQC version 0.11.7</p>\n</li>\n<li>\n<p>R (3.3.0+, with <code>knitr</code>\
    \ and <code>rmarkdown</code> libraries)</p>\n</li>\n<li>\n<p>Pandoc 1.13.1+</p>\n\
    </li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - nextflow
  - pipeline
  - demultiplexing
  - bcl2fastq
  updated_at: 1599538142.0
Nahuel-Mk2/def-space:
  data_format: 2
  description: Def File of Singularity
  filenames:
  - def/lafin.def
  - def/wav2pix.def
  - def/contextual-attention.def
  - def/singan.def
  - def/stargan.def
  - def/edge-connect.def
  - def/vae-mnist.def
  - def/sc-fegan.def
  full_name: Nahuel-Mk2/def-space
  latest_release: null
  readme: '<h1>

    <a id="user-content-def-space" class="anchor" href="#def-space" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>def-space</h1>

    <p>This repository is def-space for Singularity</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606189900.0
ORNL/faro:
  data_format: 2
  description: 'Face Recognition from Oak Ridge (FaRO) provides a well-defined server-client
    interface to a some of the best open source face recognition projects on the web. '
  filenames:
  - services/rcnn/Singularity
  full_name: ORNL/faro
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-faro-readme\" class=\"anchor\" href=\"#faro-readme\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>FARO: Readme</h1>\n<h2>\n<a id=\"user-content-overview\" class=\"\
    anchor\" href=\"#overview\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Overview</h2>\n<p>Face Recognition from Oak\
    \ Ridge (FaRO) provides a well-defined server-client\ninterface to some of the\
    \ best open source face recognition projects on the\nweb.  The intention is to\
    \ support an open platform for face recognition research\nand to provide a well-defined\
    \ and modern baseline for face recognition accuracy.<br>\nWhile many universities\
    \ and independent developers have released high quality\nface recognition models,\
    \ they often lack many useful features such as\nconfiguration management, easy\
    \ to use interfaces, deployment tools, backend\ndatabases, and analysis tools\
    \ that FaRO provides.</p>\n<p>In our research we have found that there are many\
    \ high quality and open source\nface analysis and recognition algorithms available\
    \ for research; however,\nend-to-end systems that can support larger systems or\
    \ that can be retrained for niche\napplications are lacking. We hope FARO can\
    \ fill some of those needs.</p>\n<p>The primary goals of this project are:</p>\n\
    <ol>\n<li>Create an easy to use foundation that can support complex face recognition\
    \ systems.</li>\n<li>Provide well-defined benchmark algorithms.</li>\n<li>Allow\
    \ for algorithm improvements via open source software and models and to support\
    \ improvements using techniques like transfer learning.</li>\n</ol>\n<p>FaRO is\
    \ designed as a client/server system to accomodate the need for high speed GPU\n\
    hardware to support deep learning face processing.  GRPC calls are used to communicate\n\
    with the server components which allows the clients to be written in many languages\
    \ and\nimplemented on a varity of computationally limited platforms such as cellphones\
    \ or biometric\ncollection devices.</p>\n<h2>\n<a id=\"user-content-publications\"\
    \ class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Publications</h2>\n<p>If you\
    \ use FARO for publications please cite as:</p>\n<pre><code>@misc{bolme2019faro,\n\
    \    title={{FaRO}: {FA}ce {R}ecognition From {O}ak ridge},\n    author={David\
    \ S. Bolme and David C. Cornett III and Nisha Srinivas},\n    year={2019},\n \
    \   howpublished={https://github.com/ORNL/faro}\n}\n</code></pre>\n<h2>\n<a id=\"\
    user-content-system-requirements\" class=\"anchor\" href=\"#system-requirements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>System Requirements:</h2>\n<p>Many FaRO services should run nicely\
    \ on limited hardware resources.  As we\nintegrate more deep learning algorithms,\
    \ those may require GPUs and additional\nhardware.</p>\n<ul>\n<li>Software: python3,\
    \ virtualenv, cmake, wget</li>\n<li>Python Libraries: see requirements.txt</li>\n\
    <li>NVidia GPU with 8GB of Ram - GTX Titan X/1070/1080 or better</li>\n<li>nvidia-docker2\
    \ - supporting Cuda 9.0</li>\n</ul>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick Start</h2>\n<p>This is\
    \ intended to get Dlib algorithm up and running quickly.  This is a good\nplace\
    \ to start and will allow you to test the FaRO interface.  A few\ndependencies\
    \ may be needed on a fresh Ubuntu installation including: cmake,\npython2, and\
    \ python3.  The install scripts will download and install many other\ndependencies\
    \ in the user directory as well as some large machine learning\nmodels.  To get\
    \ some initial dependencies install:</p>\n<pre><code>$ sudo apt install cmake\n\
    $ sudo apt install python2-dev\n$ sudo apt install python3-dev\n$ sudo apt install\
    \ virtualenv\n$ sudo apt install wget\n</code></pre>\n<p>First build the client\
    \ environment and compile the proto interfaces.</p>\n<pre><code>$ ./build-env-universal.sh\n\
    #For Mac users run - $echo \"export PYTHONPATH=`pwd`/src:$PYTHONPATH\" &gt;&gt;\
    \ \"$HOME/.bash_profile\" - after running build-env-universal.sh\nif using virtualenv,\n\
    \    $ source env_faro_server/bin/activate\n\nif using conda,\n    $ source activate\
    \ env_faro_server\n    or\n    $ conda activate env_faro_server\n\n$ ./build-proto.sh\n\
    </code></pre>\n<p>In one terminal run the Dlib service.  When you do this for\
    \ the first time it\nwill create a \"faro-storage\" directory and will download\
    \ and extract the machine\nlearning models.  At the end it will print out messages\
    \ for each started worker:\n\"Worker N Started.\"  By default the service is started\
    \ on port localhost:50030.</p>\n<p>If using virtualenv,</p>\n<pre><code>$ source\
    \ env_faro_server/bin/activate\n$ cd services/dlib\n$ ./run-dlib.sh\n</code></pre>\n\
    <p>If using conda,</p>\n<pre><code>$ source activate env_faro_server or conda\
    \ activate env_faro_server\n$ cd services/dlib\n$ ./run_dlib.sh\n</code></pre>\n\
    <p>The VGG2Resnet model can also be run using similar commands, but only run one\n\
    service at a time unless you carefully configure the ports and check available\n\
    memory, etc.</p>\n<p>If using virtualenv,</p>\n<pre><code>$ source env_faro_server/bin/activate\n\
    $ cd services/vggface2\n$ ./run-vgg2.sh\n</code></pre>\n<p>If using conda,</p>\n\
    <pre><code>$ source activate env_faro_server or conda activate env_faro_server\n\
    $ cd services/vggface2\n$ ./run_vgg2.sh\n</code></pre>\n<p>Similarly, InsightFace\
    \ algorithms can be executed using similar commands.\nFace detection is performed\
    \ using RetinaFace and features are extracted using ArcFace.\nCurrently, InsightFace\
    \ works only with 1 GPU and worker.</p>\n<p>If using virtualenv,</p>\n<pre><code>$\
    \ source env_faro_server/bin/activate \n$ cd services/arcface\n$ ./run_arcface.sh\n\
    </code></pre>\n<p>If using conda,</p>\n<pre><code>$ source activate env_faro_server\
    \ or conda activate env_faro_server    \n$ cd services/arcface\n$ ./run_arcface.sh\n\
    </code></pre>\n<p>In a second terminal run client applications. For this you can\
    \ use either the\n\"env_faro\" or \"env_faro_server\" environments.  Test scripts\
    \ are available in\nthe test directory to test the workings of the different functionalities\
    \ in FaRO.</p>\n<p>To test the scripts,</p>\n<p>If using virtualenv,</p>\n<pre><code>$\
    \ source env_faro/bin/activate\n$ cd tests\n</code></pre>\n<p>If using conda,</p>\n\
    <pre><code>$ source activate env_faro or conda activate env_faro\n$ cd tests\n\
    </code></pre>\n<p>To test the detect functionality on images execute,</p>\n<pre><code>$./test_detect.sh\n\
    </code></pre>\n<p>To test the detect functionality in videos execute,</p>\n<pre><code>$./test_detect_videos.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-install-with-pip\" class=\"anchor\"\
    \ href=\"#install-with-pip\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Install With PIP</h2>\n<p>This is a simple way\
    \ to add FaRO to the environment.  It should install everything needed to run\
    \ client api calls, but it may not provide all the configurations or models needed\
    \ to run services.</p>\n<pre><code>$ pip install git+https://github.com/ORNL/faro.git\n\
    </code></pre>\n<h2>\n<a id=\"user-content-run-a-service-command-line\" class=\"\
    anchor\" href=\"#run-a-service-command-line\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run a Service Command Line</h2>\n\
    <p>Starting python services can be done with a simple command line.  This will\
    \ start the service specifying the port, the number of workers, and the algorithm.</p>\n\
    <pre><code>$ python -m faro.FaceService --port=localhost:50030 --worker-count=2\
    \ --algorithm=dlib\n</code></pre>\n<h2>\n<a id=\"user-content-using-the-client-api\"\
    \ class=\"anchor\" href=\"#using-the-client-api\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using the Client API</h2>\n<p>Examples\
    \ can be found in the Notebooks directory.  The best place to start is the <a\
    \ href=\"https://github.com/ORNL/faro/blob/master/Notebooks/FaRO%20Client%20Usage.ipynb\"\
    >FaRO Client Usage notebook</a>.</p>\n<p>or</p>\n<p>FaRO_Client_Face_Detection_Video_and_Images.ipynb</p>\n\
    <p>The client can access the services using the FaRO command line interface. The\
    \ CLI includes the following functions/commands</p>\n<pre><code>#client environment\
    \ has to be activated\n$ cd bin\n$ ./faro \n\nusage : ./faro &lt;command&gt; --help\n\
    list the commands to be used\nCommands:\n    flist - List the faces in a gallery.\n\
    \    detectExtract - Run face detection and template extraction.\n    glist -\
    \ List the galleries on the service.\n    test - Process a probe and gallery directory\
    \ and produce a distance matrix.\n    extractOnly - Only run face extraction and\
    \ attribute extraction.\n    enroll - Extract faces and enroll faces in a gallery.\n\
    \    search - Search images for faces in a gallery.\n    detect - Only run face\
    \ detection.\n    \n#to run detect command and find its input options execute,\n\
    $./faro detect --help\n\nUsage: ./faro command [OPTIONS] [image] [image_directory]\
    \ [video] [...]\n\nRun detection on a collection of images.\n\nOptions:\n  --version\
    \             show program's version number and exit\n  -h, --help           \
    \ show this help message and exit\n  -v, --verbose         Print out more program\
    \ information.\n  -n MAX_IMAGES, --max-images=MAX_IMAGES\n                   \
    \     Process at N images and then stop.\n  --maximum-size=MAX_SIZE\n        \
    \                If too large, images will be scaled to have this\n          \
    \              maximum size. Default=1920\n\n  Detector Options:\n    Configuration\
    \ for the face detector.\n\n    -d DETECTIONS_CSV, --detections-csv=DETECTIONS_CSV\n\
    \                        Save detection data to the file.\n    -a ATTRIBUTES_CSV,\
    \ --attributes-csv=ATTRIBUTES_CSV\n                        Save attributes data\
    \ to the file.\n    --detect-log=DETECT_LOG\n                        A directory\
    \ for detection images.\n    --face-log=FACE_LOG\n                        A directory\
    \ for faces.\n    -b, --best          Detect the 'best' highest scoring face in\
    \ the image.\n    --detect-thresh=DETECT_THRESH\n                        The threshold\
    \ for a detection.\n    --min-size=MIN_SIZE\n                        Faces with\
    \ a height less that this will be ignored.\n    --attribute-filter=ATTRIBUTE_FILTER\n\
    \                        A comma separated list of filters example: 'Male&gt;0.5'\n\
    \n  Connection Options:\n    Control the connection to the FaRO service.\n\n \
    \   --max-async=MAX_ASYNC\n                        The maximum number of asyncronous\
    \ call to make at a\n                        time. Default=8\n    --max-message-size=MAX_MESSAGE_SIZE\n\
    \                        Maximum GRPC message size. Set to -1 for unlimited.\n\
    \                        Default=67108864\n    -p DETECT_PORT, --port=DETECT_PORT\n\
    \                        The port used for the recognition service.\n    --detect-port=DETECT_PORT\n\
    \                        The port used for the recognition service.\n    --recognition-port=REC_PORT\n\
    \                        The port used for the recognition service.\n\n</code></pre>\n\
    <h2>\n<a id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Getting Help</h2>\n<p>We currently have limited resources to support\
    \ FaRO but will do our best to provide support.  If you encounter\nproblems please\
    \ submit tickets to the issues list so that they can be properly tracked.</p>\n\
    <p><a href=\"https://github.com/ORNL/faro/issues\">https://github.com/ORNL/faro/issues</a></p>\n\
    <p>We would also like to see new features or fixes submitted as pull requests.</p>\n\
    <p><a href=\"https://github.com/ORNL/faro/pulls\">https://github.com/ORNL/faro/pulls</a></p>\n"
  stargazers_count: 3
  subscribers_count: 9
  topics: []
  updated_at: 1621287719.0
OSC/bc_osc_example_shiny:
  data_format: 2
  description: Batch Connect - Example Shiny App that runs on OSC OnDemand
  filenames:
  - ext/Singularity
  full_name: OSC/bc_osc_example_shiny
  latest_release: null
  readme: '<h1>

    <a id="user-content-wip-batch-connect---osc-example-shiny-app" class="anchor"
    href="#wip-batch-connect---osc-example-shiny-app" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>[WIP] Batch Connect - OSC Example Shiny
    App</h1>

    <p><a href="https://camo.githubusercontent.com/a8152a2780451d58acdca1e79b03f771d6e84ae12087e6e7a824b6759b715dc1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f6578616d706c655f7368696e792e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a8152a2780451d58acdca1e79b03f771d6e84ae12087e6e7a824b6759b715dc1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f6578616d706c655f7368696e792e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_example_shiny.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>A Batch Connect app designed for OSC OnDemand that launches a Shiny App within

    an Owens batch job.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://shiny.rstudio.com/" rel="nofollow">Shiny</a> x.y.z+</li>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module purge</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>Use git to clone this app and checkout the desired branch/version you want
    to

    use:</p>

    <div class="highlight highlight-source-shell"><pre>scl <span class="pl-c1">enable</span>
    git19 -- git clone <span class="pl-k">&lt;</span>repo<span class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>You will not need to do anything beyond this as all necessary assets are

    installed. You will also not need to restart this app as it isn''t a Passenger

    app.</p>

    <p>To update the app you would:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git fetch

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_example_shiny/fork">https://github.com/OSC/bc_osc_example_shiny/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1527005209.0
OSC/bc_osc_rstudio_server:
  data_format: 2
  description: Batch Connect - OSC RStudio Server
  filenames:
  - Singularity
  full_name: OSC/bc_osc_rstudio_server
  latest_release: v0.10.0
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Owens batch job.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module restore</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job before

    launching the RStudio Server.</li>

    </ul>

    <p><strong>without Singularity</strong></p>

    <ul>

    <li>

    <a href="https://www.r-project.org/" rel="nofollow">R</a> 3.3.2+ (earlier versions
    are untested but may work for you)</li>

    <li>

    <a href="https://www.rstudio.com/products/rstudio-server/" rel="nofollow">RStudio
    Server</a> 1.0.136+ (earlier versions are untested by may work for you)</li>

    <li>

    <a href="https://proot-me.github.io/" rel="nofollow">PRoot</a> 5.1.0+ (used to
    setup fake bind mount)</li>

    </ul>

    <p><strong>or with Singularity</strong></p>

    <ul>

    <li>

    <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a> 2.4.2+</li>

    <li>A Singularity image similar to <a href="https://www.singularity-hub.org/collections/463"
    rel="nofollow">nickjer/singularity-rstudio</a>

    </li>

    <li>Corresponding module to launch the above Singularity image (see

    <a href="https://github.com/nickjer/singularity-rstudio/blob/master/example_module/">example_module</a>)</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>Use git to clone this app and checkout the desired branch/version you want
    to

    use:</p>

    <div class="highlight highlight-source-shell"><pre>scl <span class="pl-c1">enable</span>
    git19 -- git clone <span class="pl-k">&lt;</span>repo<span class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>You will not need to do anything beyond this as all necessary assets are

    installed. You will also not need to restart this app as it isn''t a Passenger

    app.</p>

    <p>To update the app you would:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git fetch

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_rstudio_server/fork">https://github.com/OSC/bc_osc_rstudio_server/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <ul>

    <li>Documentation, website content, and logo is licensed under

    <a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow">CC-BY-4.0</a>

    </li>

    <li>Code is licensed under MIT (see LICENSE.txt)o</li>

    <li>RStudio, Shiny and the RStudio logo are all registered trademarks of RStudio.</li>

    </ul>

    '
  stargazers_count: 4
  subscribers_count: 11
  topics: []
  updated_at: 1614963192.0
OSC/bc_osc_rstudio_server_pitzer:
  data_format: 2
  description: Batch Connect - OSC RStudio Server - Pitzer
  filenames:
  - Singularity
  full_name: OSC/bc_osc_rstudio_server_pitzer
  latest_release: v0.1.5
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server_pitzer.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Pitzer batch job.</p>

    <h2>

    <a id="user-content-deprecated-application-warning" class="anchor" href="#deprecated-application-warning"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deprecated
    application warning</h2>

    <p>This application no longer works.  It raises an exception when users attempt
    to submit jobs.

    This is because we now have functionality to submit to multiple clusters and

    <a href="https://github.com/OSC/bc_osc_rstudio_server">the generic application</a>
    now submits

    to pitzer rendering this application useless.</p>

    <p>For historic versions, see the last released you can still view

    <a href="https://github.com/OSC/bc_osc_rstudio_server_pitzer/tree/v0.3.0">v0.3.0</a>
    as it was the last

    working version of this application.</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1598640242.0
OSC/bc_osc_rstudio_server_quick:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: OSC/bc_osc_rstudio_server_quick
  latest_release: v0.0.1
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Owens batch job.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module restore</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job before

    launching the RStudio Server.</li>

    </ul>

    <p><strong>without Singularity</strong></p>

    <ul>

    <li>

    <a href="https://www.r-project.org/" rel="nofollow">R</a> 3.3.2+ (earlier versions
    are untested but may work for you)</li>

    <li>

    <a href="https://www.rstudio.com/products/rstudio-server/" rel="nofollow">RStudio
    Server</a> 1.0.136+ (earlier versions are untested by may work for you)</li>

    <li>

    <a href="https://proot-me.github.io/" rel="nofollow">PRoot</a> 5.1.0+ (used to
    setup fake bind mount)</li>

    </ul>

    <p><strong>or with Singularity</strong></p>

    <ul>

    <li>

    <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a> 2.4.2+</li>

    <li>A Singularity image similar to <a href="https://www.singularity-hub.org/collections/463"
    rel="nofollow">nickjer/singularity-rstudio</a>

    </li>

    <li>Corresponding module to launch the above Singularity image (see

    <a href="https://github.com/nickjer/singularity-rstudio/blob/master/example_module/">example_module</a>)</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>Use git to clone this app and checkout the desired branch/version you want
    to

    use:</p>

    <div class="highlight highlight-source-shell"><pre>scl <span class="pl-c1">enable</span>
    git19 -- git clone <span class="pl-k">&lt;</span>repo<span class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>You will not need to do anything beyond this as all necessary assets are

    installed. You will also not need to restart this app as it isn''t a Passenger

    app.</p>

    <p>To update the app you would:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    <span class="pl-k">&lt;</span>dir<span class="pl-k">&gt;</span>

    scl <span class="pl-c1">enable</span> git19 -- git fetch

    scl <span class="pl-c1">enable</span> git19 -- git checkout <span class="pl-k">&lt;</span>tag/branch<span
    class="pl-k">&gt;</span></pre></div>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_rstudio_server/fork">https://github.com/OSC/bc_osc_rstudio_server/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 11
  topics: []
  updated_at: 1570733859.0
OSC/centos7-launcher:
  data_format: 2
  description: A thin Singularity image used as an alternative to Proot to wrap applications
    in an arbitrary file system.
  filenames:
  - Singularity
  full_name: OSC/centos7-launcher
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7-launcher" class="anchor" href="#centos7-launcher"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>centos7-launcher</h1>

    <p>A Singularity image used wrap applications RStudio <code>rserver</code> instances
    in an arbitrary file system for use with <a href="http://openondemand.org/" rel="nofollow">OnDemand</a>.
    Tested as compatible with Singularity 2.x and 3.x.</p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage:</h2>

    <h3>

    <a id="user-content-singularity-2x" class="anchor" href="#singularity-2x" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity 2.x</h3>

    <p>TODO...</p>

    <h3>

    <a id="user-content-singularity-3x" class="anchor" href="#singularity-3x" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity 3.x</h3>

    <p>TODO...</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1550176998.0
OSC/sa_singularity_iqmol:
  data_format: 2
  description: IQmol in a Singularity container
  filenames:
  - Singularity.2.14
  - Singularity.2.13b
  - Singularity.2.11.2
  - Singularity
  full_name: OSC/sa_singularity_iqmol
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-iqmol" class="anchor" href="#singularity-iqmol"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    IQmol</h1>

    <p><a href="https://singularity-hub.org/collections/3599" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="http://iqmol.org/index.html" rel="nofollow">IQmol</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a> or CentOS image <a href="https://hub.docker.com/_/centos"
    rel="nofollow">centos</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>iqmol.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build iqmol.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull iqmol.sif
    shub://OSC/sa_singularity_iqmol</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-start-iqmol" class="anchor" href="#start-iqmol" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start IQmol</h3>

    <p>IQmol is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run iqmol.sif</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./iqmol.sif</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 8
  topics: []
  updated_at: 1599018735.0
OSC/sa_singularity_molgfx:
  data_format: 2
  description: ' Molecular graphics systems in a Singularity container'
  filenames:
  - Singularity.1.0
  - Singularity
  full_name: OSC/sa_singularity_molgfx
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-molgfx" class="anchor" href="#singularity-molgfx"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Molgfx</h1>

    <p><a href="https://singularity-hub.org/collections/4301" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://github.com/OpenChemistry">Open Chemistry</a>,
    Gabedit and Jmol. It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>molgfx.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build molgfx.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull molgfx.sif
    shub://OSC/sa_singularity_molgfx</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-find-versions-of-molecular-graphics-systems" class="anchor"
    href="#find-versions-of-molecular-graphics-systems" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Find versions of molecular graphics systems</h3>

    <div class="highlight highlight-source-shell"><pre>singularity inspect -H molgfx.sif</pre></div>

    <h3>

    <a id="user-content-start-avogadro2" class="anchor" href="#start-avogadro2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start Avogadro2</h3>

    <p>Avogadro2 is started using the default exec command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity <span class="pl-c1">exec</span>
    molgfx.sif avogadro2</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1588619360.0
OSC/sa_singularity_openexr:
  data_format: 2
  description: OpenEXR in a Singularity container
  filenames:
  - Singularity.2.2
  - Singularity
  full_name: OSC/sa_singularity_openexr
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-openexr" class="anchor" href="#singularity-openexr"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    OpenEXR</h1>

    <p><a href="https://singularity-hub.org/collections/3586" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://www.openexr.com/" rel="nofollow">OpenEXR</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a></p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>openexr.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build openexr.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name openexr.sif
    shub://OSC/sa_singularity_openexr</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-render-exr-image" class="anchor" href="#render-exr-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Render
    .EXR image</h3>

    <p>The <code>exrdisplay</code> command is launched using the command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity <span class="pl-c1">exec</span>
    openexr.sif exrdisplay -h</pre></div>

    <p>Example:</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    <span class="pl-c1">exec</span> openexr.sif exrdisplay rendertest_0001.exr</span></pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 8
  topics: []
  updated_at: 1569951214.0
OSC/sa_singularity_qgis:
  data_format: 2
  description: QGIS in a Singularity container
  filenames:
  - Singularity.3.4.12
  - Singularity
  full_name: OSC/sa_singularity_qgis
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-qgis" class="anchor" href="#singularity-qgis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    QGIS</h1>

    <p><a href="https://singularity-hub.org/collections/3587" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://qgis.org/en/site/index.html" rel="nofollow">QGIS</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>. Packages installed: <code>qgis qgis-plugin-grass</code></p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>qgis.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build qgis.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull qgis.sif shub://OSC/sa_singularity_qgis</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-start-qgis" class="anchor" href="#start-qgis" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start QGIS</h3>

    <p>QGIS is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run qgis.sif</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./qgis.sif</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 8
  topics: []
  updated_at: 1569951156.0
OSC/sa_singularity_winehq:
  data_format: 2
  description: WineHQ in a Singularity container
  filenames:
  - Singularity.5.0.0
  - Singularity
  - Singularity.4.0.3
  full_name: OSC/sa_singularity_winehq
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-winehq" class="anchor" href="#singularity-winehq"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    WineHQ</h1>

    <p><a href="https://singularity-hub.org/collections/3891" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://www.winehq.org/" rel="nofollow">WineHQ</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>winehq.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build winehq.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull winehq.sif
    shub://OSC/sa_singularity_winehq</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-run-64-bit-windows-binary" class="anchor" href="#run-64-bit-windows-binary"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    64-bit Windows binary</h3>

    <p>WineHQ is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run winehq.sif
    /path/to/windows_64bit_exe</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./winehq.sif /path/to/windows_64bit_exe</pre></div>

    <h3>

    <a id="user-content-run-32-bit-windows-binary" class="anchor" href="#run-32-bit-windows-binary"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    32-bit Windows binary</h3>

    <div class="highlight highlight-source-shell"><pre>singularity <span class="pl-c1">exec</span>
    winehq.sif wine /path/to/windows_32bit_exe</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 8
  topics: []
  updated_at: 1581361908.0
OSC/sa_singularity_xcrysden:
  data_format: 2
  description: XCrySDen in a Singularity container
  filenames:
  - Singularity.1.6.2
  - Singularity
  full_name: OSC/sa_singularity_xcrysden
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-xcrysden" class="anchor" href="#singularity-xcrysden"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    XCrySDen</h1>

    <p><a href="https://singularity-hub.org/collections/4445" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="http://www.xcrysden.org/Download.html" rel="nofollow">XCrysDen</a>.
    It was built on top of the base Docker image <a href="https://hub.docker.com/_/ubuntu"
    rel="nofollow">ubuntu</a>.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>xcrysden.sif</code> with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build xcrysden.sif
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from <a
    href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull xcrysden.sif
    shub://OSC/sa_singularity_xcrysden</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-start-xcrysden" class="anchor" href="#start-xcrysden" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Start XCrysDen</h3>

    <p>XCrysDen is started using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run xcrysden.sif</pre></div>

    <p>or as a native command</p>

    <div class="highlight highlight-source-shell"><pre>./xcrysden.sif</pre></div>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1592244254.0
OSC/shiny_launcher:
  data_format: 2
  description: null
  filenames:
  - ext/Singularity
  full_name: OSC/shiny_launcher
  latest_release: null
  readme: '<h1>

    <a id="user-content-wip-batch-connect---osc-shiny-app-launcher" class="anchor"
    href="#wip-batch-connect---osc-shiny-app-launcher" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>[WIP] Batch Connect - OSC Shiny App Launcher</h1>

    <p><a href="https://camo.githubusercontent.com/5e71a5a07e8e6e15f922edb76b5fb68e7ee6087e336807c38095861b8c7f5fc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f7368696e795f6c61756e636865722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/5e71a5a07e8e6e15f922edb76b5fb68e7ee6087e336807c38095861b8c7f5fc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f7368696e795f6c61756e636865722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/shiny_launcher.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>A Batch Connect app designed for OSC OnDemand that launches a Shiny App within

    an Owens batch job.</p>

    <p>The Shiny app is included a submodule and each deployment can modified which

    Shiny app to deploy using git config to specify the URL for the submodule. This

    way the launcher code can be reused for multiple apps but the launcher and the

    app itself can be managed separately.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <p>This Batch Connect app requires the following software be installed on the

    <strong>compute nodes</strong> that the batch job is intended to run on (<strong>NOT</strong>
    the

    OnDemand node):</p>

    <ul>

    <li>

    <a href="https://shiny.rstudio.com/" rel="nofollow">Shiny</a> x.y.z+</li>

    <li>

    <a href="https://www.tacc.utexas.edu/research-development/tacc-projects/lmod"
    rel="nofollow">Lmod</a> 6.0.1+ or any other <code>module purge</code> and <code>module
    load &lt;modules&gt;</code> based

    CLI used to load appropriate environments within the batch job</li>

    </ul>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p><strong>TODO</strong></p>

    <p>Again, you do not need to restart the app as it isn''t a Passenger app.</p>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <ol>

    <li>Fork it ( <a href="https://github.com/OSC/bc_osc_example_shiny/fork">https://github.com/OSC/bc_osc_example_shiny/fork</a>
    )</li>

    <li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>

    <li>Commit your changes (<code>git commit -am ''Add some feature''</code>)</li>

    <li>Push to the branch (<code>git push origin my-new-feature</code>)</li>

    <li>Create a new Pull Request</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1569007230.0
PGP-UK/GenomeChronicler:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: PGP-UK/GenomeChronicler
  latest_release: '0.91'
  readme: "<pre><code> #####                                         #####       \
    \                                                     \n#     # ###### #    #\
    \  ####  #    # ######    #     # #    # #####   ####  #    # #  ####  #     \
    \ ###### #####  \n#       #      ##   # #    # ##  ## #         #       #    #\
    \ #    # #    # ##   # # #    # #      #      #    # \n#  #### #####  # #  # #\
    \    # # ## # #####     #       ###### #    # #    # # #  # # #      #      #####\
    \  #    # \n#     # #      #  # # #    # #    # #         #       #    # #####\
    \  #    # #  # # # #      #      #      #####  \n#     # #      #   ## #    #\
    \ #    # #         #     # #    # #   #  #    # #   ## # #    # #      #     \
    \ #   #  \n #####  ###### #    #  ####  #    # ######     #####  #    # #    #\
    \  ####  #    # #  ####  ###### ###### #    #  \n</code></pre>\n<p><a href=\"\
    https://singularity-hub.org/collections/3664\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-welcome\" class=\"\
    anchor\" href=\"#welcome\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Welcome</h1>\n<p>This is the repository for\
    \ Genome Chronicler, the Personal Genome Project United Kingdom (PGP-UK) genomic\
    \ report generation scripts.</p>\n<h1>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h1>\n<p>After\
    \ cloning this repository, run the SetupMeFirst.sh script in your local system\
    \ to retrieve the extra files needed to run the pipeline (around 10GB, so too\
    \ big for git).</p>\n<h1>\n<a id=\"user-content-input-files\" class=\"anchor\"\
    \ href=\"#input-files\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Input files</h1>\n<p>The main script (GenomeChronicler_mainDruid.pl)\
    \ needs a BAM file as input, and optionally can also use a VEP generated summary\
    \ html file, if variants have already been called on the data and summaries are\
    \ to be produced.</p>\n<h1>\n<a id=\"user-content-dependencies\" class=\"anchor\"\
    \ href=\"#dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Dependencies</h1>\n<p>To handle the myriad dependencies\
    \ present in this pipeline, it is avaliable through Singularity Hub as a singularity\
    \ container (see badge at top of the page).</p>\n<h1>\n<a id=\"user-content-easy-start-using-singularity\"\
    \ class=\"anchor\" href=\"#easy-start-using-singularity\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Easy Start\
    \ using Singularity</h1>\n<p>If you don't already have singularity on your system,\
    \ or want to know more about it, head to their userguide at: <a href=\"https://sylabs.io/guides/3.1/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.1/user-guide/</a>\nWhile Singularity\
    \ is not needed to run GenomeChronicler, it does make setup much easier.</p>\n\
    <p>For a manual installation without Singularity, please follow the steps in the\
    \ %post section of the Singularity file in this repository, to install all the\
    \ dependencies.</p>\n<p>Downloading pre-packaged GenomeChronicler from SingularityHub</p>\n\
    <pre><code>singularity pull shub://PGP-UK/GenomeChronicler\n</code></pre>\n<p>Getting\
    \ some test data (NA12878 from ENA, pre-mapped to GRCh38, and the respective reference)</p>\n\
    <pre lang=\"wget\"><code>singularity exec GenomeChronicler_latest.sif wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/CEU/NA12878/alignment/NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram\n\
    \nsingularity exec GenomeChronicler_latest.sif wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa\n\
    \n</code></pre>\n<p>Converting data to BAM format</p>\n<pre><code>singularity\
    \ exec GenomeChronicler_latest.sif samtools view -T GRCh38_full_analysis_set_plus_decoy_hla.fa\
    \ -b -o NA12878wxs.bam NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram\n\
    </code></pre>\n<p>Running GenomeChronicler on the data</p>\n<pre><code>singularity\
    \ run GenomeChronicler_latest.sif --bamFile=NA12878wxs.bam \n</code></pre>\n<h1>\n\
    <a id=\"user-content-command-line-options\" class=\"anchor\" href=\"#command-line-options\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Command Line Options</h1>\n<table>\n<thead>\n<tr>\n<th align=\"center\"\
    >Option</th>\n<th align=\"center\">Requirement</th>\n<th>Description</th>\n</tr>\n\
    </thead>\n<tbody>\n<tr>\n<td align=\"center\">--bamFile</td>\n<td align=\"center\"\
    >REQUIRED</td>\n<td>The path to a BAM file that has been preprocessed through\
    \ markDuplicates and VariantQualityScoreRecalibration. This can be obtained by\
    \ running the first step of the Sarek nextflow pipeline, or through other means\
    \ that do respect the general principles of the GATK Variation Calling Best Practices\
    \ workflow. Note that no variation calling is needed to run GenomeChronicler.</td>\n\
    </tr>\n<tr>\n<td align=\"center\">--vepFile</td>\n<td align=\"center\">OPTIONAL</td>\n\
    <td>For the summary tables to appear in the report, a VEP summary HTML file must\
    \ be provided. This will likely be generated if the data is from whole genome\
    \ sequencing and variants were called (e.g. by running all the germline calling\
    \ steps of the Sarek nextflow pipeline or other GATK Best Practices based workflow).\
    \ If this isn't provided, summary tables and plots will automatically be excluded\
    \ from the final report.</td>\n</tr>\n<tr>\n<td align=\"center\">--resultsDir</td>\n\
    <td align=\"center\">OPTIONAL</td>\n<td>For setting the absolute path of the results\
    \ folder to be produced when running GenomeChronicler.</td>\n</tr>\n<tr>\n<td\
    \ align=\"center\">--customTemplate</td>\n<td align=\"center\">OPTIONAL</td>\n\
    <td>For customising the output report, set this variable to the path of a custom\
    \ LaTeX file to act as a template for the report. The default templates bundled\
    \ with this software can also be found in the project github page.</td>\n</tr>\n\
    <tr>\n<td align=\"center\">--GATKthreads</td>\n<td align=\"center\">OPTIONAL</td>\n\
    <td>Number of threads to use for the GATK genotyping steps of this processing\
    \ pipeline.</td>\n</tr>\n</tbody>\n</table>\n"
  stargazers_count: 5
  subscribers_count: 2
  topics: []
  updated_at: 1617920191.0
PreibischLab/BigStitcher-Singularity:
  data_format: 2
  description: Singularity container description for BigStitcher
  filenames:
  - Singularity-BigStitcher
  full_name: PreibischLab/BigStitcher-Singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-bigstitcher-singularity" class="anchor" href="#bigstitcher-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BigStitcher-Singularity</h1>

    <p>Singularity container description that automatically creates an Uber-JAR of
    the current BigStitcher version (including all dependencies) using local copy
    of the Oracle JDK.</p>

    <p>Can easily be deployed for example on a cluster for parallel resaving.</p>

    '
  stargazers_count: 0
  subscribers_count: 11
  topics: []
  updated_at: 1584624624.0
QTIM-Lab/DeepNeuro:
  data_format: 2
  description: 'A deep learning python package for neuroimaging data. Made by:'
  filenames:
  - deepneuro/pipelines/Skull_Stripping/Singularity.deepneuro_skullstripping
  - deepneuro/pipelines/Segment_GBM/Singularity.deepneuro_segment_gbm
  - deepneuro/pipelines/Segment_Brain_Mets/Singularity.deepneuro_segment_mets
  - deepneuro/pipelines/Ischemic_Stroke/Singularity.deepneuro_segment_ischemic_stroke
  full_name: QTIM-Lab/DeepNeuro
  latest_release: null
  readme: "<p><a href=\"./package_resources/logos/DeepNeuro_alt.PNG?raw=true\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"./package_resources/logos/DeepNeuro_alt.PNG?raw=true\"\
    \ alt=\"Alt text\" title=\"DeepNeuro\" style=\"max-width:100%;\"></a></p>\n<p><a\
    \ href=\"https://travis-ci.org/QTIM-Lab/DeepNeuro\" rel=\"nofollow\"><img src=\"\
    https://camo.githubusercontent.com/645f122a503a7934dcfcfc971aff595f877adc6da0142112c94b4df371fdd88d/68747470733a2f2f7472617669732d63692e6f72672f5154494d2d4c61622f446565704e6575726f2e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/QTIM-Lab/DeepNeuro.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-deepneuro\" class=\"\
    anchor\" href=\"#deepneuro\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>DeepNeuro</h1>\n<p>A deep learning python package\
    \ for neuroimaging data. Focused on validated command-line tools you can use today.\
    \ Created by the Quantitative Tumor Imaging Lab at the Martinos Center (Harvard-MIT\
    \ Program in Health, Sciences, and Technology / Massachusetts General Hospital).</p>\n\
    <h2>\n<a id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Table of Contents</h2>\n<ul>\n<li><a href=\"#about\"><g-emoji class=\"\
    g-emoji\" alias=\"question\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2753.png\"\
    >\u2753</g-emoji> About</a></li>\n<li><a href=\"#installation\"><g-emoji class=\"\
    g-emoji\" alias=\"floppy_disk\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png\"\
    >\U0001F4BE</g-emoji> Installation</a></li>\n<li><a href=\"#tutorials\"><g-emoji\
    \ class=\"g-emoji\" alias=\"mortar_board\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f393.png\"\
    >\U0001F393</g-emoji> Tutorials</a></li>\n<li><a href=\"#modules\"><g-emoji class=\"\
    g-emoji\" alias=\"gift\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f381.png\"\
    >\U0001F381</g-emoji> Modules</a></li>\n<li><a href=\"#contact\"><g-emoji class=\"\
    g-emoji\" alias=\"speech_balloon\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png\"\
    >\U0001F4AC</g-emoji> Contact</a></li>\n<li><a href=\"#citation\"><g-emoji class=\"\
    g-emoji\" alias=\"mega\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4e3.png\"\
    >\U0001F4E3</g-emoji> Citation</a></li>\n<li><a href=\"#acknowledgements\"><g-emoji\
    \ class=\"g-emoji\" alias=\"yellow_heart\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f49b.png\"\
    >\U0001F49B</g-emoji> Acknowledgements</a></li>\n</ul>\n<h2>\n<a id=\"user-content-about\"\
    \ class=\"anchor\" href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>About</h2>\n<p>DeepNeuro is an open-source\
    \ toolset of deep learning applications for neuroimaging. We have several goals\
    \ for this package:</p>\n<ul>\n<li>Provide easy-to-use command line tools for\
    \ neuroimaging using deep learning.</li>\n<li>Create Docker containers for each\
    \ tool and all out-of-package pre-processing steps, so they can each can be run\
    \ without having install prerequisite libraries.</li>\n<li>Provide freely available\
    \ deep learning models trained on a wealth of neuroimaging data.</li>\n<li>Provide\
    \ training scripts and links to publically-available data to replicate the results\
    \ of DeepNeuro's models.</li>\n<li>Provide implementations of popular models for\
    \ medical imaging data, and pre-processed datasets for educational purposes.</li>\n\
    </ul>\n<p>This package is under active development, but we encourage users to\
    \ both try the modules with pre-trained modules highlighted below, and try their\
    \ hand at making their own DeepNeuro modules using the tutorials below.</p>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<ol>\n<li>\n<p>Install Docker from Docker's website\
    \ here: <a href=\"https://www.docker.com/get-started\" rel=\"nofollow\">https://www.docker.com/get-started</a>.\
    \ Follow instructions on that link to get Docker set up properly on your workstation.</p>\n\
    </li>\n<li>\n<p>Install the Docker Engine Utility for NVIDIA GPUs, AKA nvidia-docker.\
    \ You can find installation instructions at their Github page, here: <a href=\"\
    https://github.com/NVIDIA/nvidia-docker\">https://github.com/NVIDIA/nvidia-docker</a></p>\n\
    </li>\n<li>\n<p>Pull the DeepNeuro Docker container from <a href=\"https://hub.docker.com/r/qtimlab/deepneuro_segment_gbm/\"\
    \ rel=\"nofollow\">https://hub.docker.com/r/qtimlab/deepneuro_segment_gbm/</a>.\
    \ Use the command \"docker pull qtimlab/deepneuro\"</p>\n</li>\n<li>\n<p>If you\
    \ want to run DeepNeuro outside of a Docker container, you can install the DeepNeuro\
    \ Python package locally using the pip package manager. On the command line, run\
    \ <code>pip install deepneuro</code></p>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-tutorials\"\
    \ class=\"anchor\" href=\"#tutorials\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Tutorials</h2>\n<p align=\"center\"\
    >\n<a href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Preprocess_and_Augment.ipynb\"\
    \ rel=\"nofollow\">\n<img src=\"./notebooks/resources/train_preprocess_icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Train_Model.ipynb\"\
    \ rel=\"nofollow\">\n<img src=\"./notebooks/resources/train_model_icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Run_Inference.ipynb\"\
    \ rel=\"nofollow\">\n<img src=\"./notebooks/resources/model_inference_icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<h2>\n<a id=\"\
    user-content-modules\" class=\"anchor\" href=\"#modules\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Modules</h2>\n\
    <p align=\"center\">\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Segment_GBM\"\
    >\n<img src=\"./deepneuro/pipelines/Segment_GBM/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Skull_Stripping\"\
    >\n<img src=\"./deepneuro/pipelines/Skull_Stripping/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Segment_Brain_Mets\"\
    >\n<img src=\"./deepneuro/pipelines/Segment_Brain_Mets/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<p align=\"center\"\
    >\n<a href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Ischemic_Stroke\"\
    >\n<img src=\"./deepneuro/pipelines/Ischemic_Stroke/resources/icon.png?raw=true\"\
    \ width=\"684\" alt=\"\" style=\"max-width:100%;\">\n</a>\n</p>\n<h2>\n<a id=\"\
    user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citation</h2>\n\
    <p>If you use DeepNeuro in your published work, please cite:</p>\n<p>Beers, A.,\
    \ Brown, J., Chang, K., Hoebel, K., Patel, J., Ly, K. Ina, Tolaney, S.M., Brastianos,\
    \ P., Rosen, B., Gerstner, E., and Kalpathy-Cramer, J. (2020). <a href=\"https://link.springer.com/article/10.1007/s12021-020-09477-5\"\
    \ rel=\"nofollow\">DeepNeuro: an open-source deep learning toolbox for neuroimaging</a>.\
    \ Neuroinformatics. DOI: 10.1007/s12021-020-09477-5. PMID: 32578020</p>\n<p>If\
    \ you use the MRI skull-stripping or glioblastoma segmentation modules, please\
    \ cite:</p>\n<p>Chang, K., Beers, A.L., Bai, H.X., Brown, J.M., Ly, K.I., Li,\
    \ X., Senders, J.T., Kavouridis, V.K., Boaro, A., Su, C., Bi, W.L., Rapalino,\
    \ O., Liao, W., Shen, Q., Zhou, H., Xiao, B., Wang, Y., Zhang, P.J., Pinho, M.C.,\
    \ Wen, P.Y., Batchelor, T.T., Boxerman, J.L., Arnaout, O., Rosen, B.R., Gerstner,\
    \ E.R., Yang, L., Huang, R.Y., and Kalpathy-Cramer, J., 2019. <a href=\"https://academic.oup.com/neuro-oncology/advance-article/doi/10.1093/neuonc/noz106/5514498?searchresult=1\"\
    \ rel=\"nofollow\">Automatic assessment of glioma burden: A deep learning algorithm\
    \ for fully automated volumetric and bi-dimensional measurement</a>. Neuro-Oncology.\
    \ DOI: 10.1093/neuonc/noz106. PMID: 31190077</p>\n<h2>\n<a id=\"user-content-contact\"\
    \ class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contact</h2>\n<p>DeepNeuro is\
    \ under active development, and you may run into errors or want additional features.\
    \ Send any questions or requests for methods to <a href=\"mailto:qtimlab@gmail.com\"\
    >qtimlab@gmail.com</a>. You can also submit a Github issue if you run into a bug.</p>\n\
    <h2>\n<a id=\"user-content-acknowledgements\" class=\"anchor\" href=\"#acknowledgements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Acknowledgements</h2>\n<p>The Center for Clinical Data Science at\
    \ Massachusetts General Hospital and the Brigham and Woman's Hospital provided\
    \ technical and hardware support for the development of DeepNeuro, including access\
    \ to graphics processing units. The DeepNeuro project is also indebted to the\
    \ following <a href=\"https://github.com/ellisdg/3DUnetCNN\">Github repository</a>\
    \ for the 3D UNet by user ellisdg, which formed the original kernel for much of\
    \ its code in early stages. Long live open source deep learning :)</p>\n<h2>\n\
    <a id=\"user-content-disclaimer\" class=\"anchor\" href=\"#disclaimer\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Disclaimer</h2>\n\
    <p>This software package and the deep learning models within are intended for\
    \ research purposes only and have not yet been validated for clinical use.</p>\n"
  stargazers_count: 104
  subscribers_count: 15
  topics: []
  updated_at: 1622068798.0
RBigData/singularity:
  data_format: 2
  description: Singularity configurations for R and pbdR packages.
  filenames:
  - pbdR/pbdR/mpich/Singularity.1.0-1
  - pbdR/pbdR/openmpi/Singularity.1.0-1
  - pbdR/pbdR-minimal/mpich/Singularity.1.0-1
  - pbdR/pbdR-minimal/openmpi/Singularity.1.0-1
  - R/r/Singularity.3.5.1
  - R/jupyter/Singularity
  - R/rstudio/Singularity.1.1.456
  - R/rstudio-server/Singularity.1.1.456
  - R/r-minimal/Singularity.3.5.1
  full_name: RBigData/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-r-and-pbdr-singularity-recipes" class="anchor" href="#r-and-pbdr-singularity-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>R
    and pbdR Singularity Recipes</h1>

    <p>Singularity recipes for R and pbdR.</p>

    <p>Build requirements:</p>

    <ul>

    <li>

    <a href="https://www.sylabs.io/" rel="nofollow">singularity</a> &gt;= 2.3</li>

    <li>Modify the <code>make -j</code> line of each recipe to your liking.</li>

    </ul>

    '
  stargazers_count: 3
  subscribers_count: 4
  topics: []
  updated_at: 1571942199.0
ResearchIT/MolecularGraphicsToolbox:
  data_format: 2
  description: null
  filenames:
  - Singularity.centos7.tbx-MG
  full_name: ResearchIT/MolecularGraphicsToolbox
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterisationvl-software\" class=\"anchor\"\
    \ href=\"#characterisationvl-software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CharacterisationVL-Software</h1>\n\
    <p>The purpose of this repository is for storing definition files to submit to\
    \ <a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity Hub.</a></p>\n\
    <p>If you are new to Singularity containers, please refer to <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.5/user-guide/</a> or a newer version\
    \ of this documentation.</p>\n<p>Each software package is located in its own folder.\
    \ The files are tagged with the software name and version number or date of build.\
    \ Please read below for the naming convention.</p>\n<p>To add software to the\
    \ repository you will need to create a new branch. The new branch is the name\
    \ of the software product. By convention, the new branch will be checked and merged\
    \ into the master branch and then deleted.</p>\n<h2>\n<a id=\"user-content-steps-to-add-a-software-package\"\
    \ class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Steps to\
    \ add a software package</h2>\n<ol>\n<li>Clone this repository</li>\n<li>Create\
    \ a branch</li>\n</ol>\n<pre><code>$ git branch &lt;software name&gt;\n</code></pre>\n\
    <ol start=\"3\">\n<li>Make a subdirectory for the software product.</li>\n</ol>\n\
    <pre><code>$ mkdir &lt;software name&gt;\n</code></pre>\n<ol start=\"4\">\n<li>Add\
    \ all the necessary files.</li>\n</ol>\n<ul>\n<li>Singularity definition file\
    \ or installation script</li>\n<li>Readme file including install and testing notes</li>\n\
    <li>Desktop files for adding to menus with necessary tags</li>\n<li>For full details,\
    \ <a href=\"template/README.md\">please refer to the 'template' folder in this\
    \ repository.</a>\n</li>\n</ul>\n<ol start=\"4\">\n<li>Commit all changes, including\
    \ a helpful message</li>\n</ol>\n<pre><code>$ git commit -m \"&lt;software name&gt;\
    \ added as requested in support ticket\"\n</code></pre>\n<ol start=\"6\">\n<li>Push\
    \ to the remote repository. i.e. this one.</li>\n<li>Submit merge request</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Naming your Singularity definition file, Singularity Hub and Licensing</h2>\n\
    <p>For all Singularity recipes where the software licensing permits redistribution,\
    \ please use this naming convention:</p>\n<pre><code>   Singularity.applicationName_version\n\
    \   Singularity.applicationName_version-cuda-cudaVersion\n\n</code></pre>\n<p>This\
    \ is where Singularity Hub fits into the equation. There is a webhook between\
    \ this repository and <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >Singularity Hub</a>. When a commit is merged into the master branch, Singularity\
    \ Hub will build the container.</p>\n<p>If successfully built, the path to the\
    \ container on Singularity Hub is:</p>\n<pre><code>  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n\
    \  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\
    \n</code></pre>\n<p>For software where licensing does not support redistribution,\
    \ the container recipe can still be defined, but the container should not be built\
    \ on Singularity Hub.</p>\n<p>An example on how to handle this situation is the\
    \ recipe for CCP-EM.\nThe <a href=\"ccp-em/README.md\">README.md</a> contains\
    \ a section on Prerequisites. This section lists the required files to build the\
    \ container. The license must be accepted by the end user to obtain them.</p>\n\
    <p>Prerequisite files should not be committed to this repository.</p>\n<p>To prevent\
    \ Singularity Hub from attempting to build the container, we simply use a different\
    \ recipe naming convention as follows:</p>\n<pre><code>   applicationName_version.def\n\
    \   applicationName_version-cuda-cudaVersion.def\n\n</code></pre>\n<h2>\n<a id=\"\
    user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Ubuntu Base Images</h2>\n<p>The folder 'ubuntu-base-image' contains\
    \ recipes for pre built base containers. These can be used as a starting point\
    \ to aid/speed up the development of your container recipe.</p>\n<p>The current\
    \ versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.</p>\n\
    <p>These are available on Singularity Hub.</p>\n<p>For example: from the Graphviz\
    \ Singularity.graphviz-2.40.1 recipe</p>\n<pre><code>Bootstrap: shub\nFrom:  \
    \    Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n</code></pre>\n\
    <p>These two lines, will tell Singularity to use the 'shub' bootstrap to obtain\
    \ the '1804' ubuntu-base-image container from Singularity Hub.</p>\n<p>From here\
    \ you just need to add the requirements to build a container for your required\
    \ piece of software. Please see <a href=\"graphviz/Singularity.graphviz-2.40.1\"\
    >Singularity.graphviz-2.40.1</a>\nfor the full recipe.</p>\n<p>The current ubuntu-base-images\
    \ include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.</p>\n\
    <h2>\n<a id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"\
    anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ GUI applications on a non-GPU node</h2>\n<p>The applications in the Singularity\
    \ container should run without the need for a dedicated GPU.</p>\n<p>However,\
    \ an X server needs to be running for this to work. On nodes with GPU, X Server\
    \ is started with NVIDIA driver, and on non-GPU nodes, the X Server is started\
    \ with MESA library.</p>\n<p>X Server can be started during boot (for example,\
    \ using <code>systemctl set-default graphical.target</code>).</p>\n<p>Make sure\
    \ that VirtualGL package is installed in the container. The code below will download\
    \ and install VirtualGL.</p>\n<pre><code>wget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\
    \ndpkg -i virtualgl_2.6.2_amd64.deb\n</code></pre>\n<p>The application startup\
    \ script doesn't need to be modified, however, if the application needs to be\
    \ manually started, then <code>vglrun</code> needs to be appended before running\
    \ the application. For example: <code>singularity exec --nv -B /projects:/projects\
    \ -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX</code></p>\n\
    <p><a href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 2
  subscribers_count: 6
  topics: []
  updated_at: 1582812783.0
ResearchIT/NMRPipe:
  data_format: 2
  description: Singularity recipe for NMRPipe
  filenames:
  - Singularity
  - Singularity.212_64
  full_name: ResearchIT/NMRPipe
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-nmrpipe" class="anchor" href="#singularity-recipe-for-nmrpipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for NMRPipe</h1>

    <p>This repo contains the recipe to run <a href="https://www.ibbr.umd.edu/nmrpipe/"
    rel="nofollow">NMRPipe</a>

    within a <a href="https://singularity.lbl.gov" rel="nofollow">Singularity</a>
    container, which can be built using <a href="https://singularity-hub.org" rel="nofollow">Singularity
    Hub</a></p>

    <p>Versions:</p>

    <ul>

    <li>212_64 - NMRPipe linux212_64 built on centos7.4</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1523030864.0
ResearchIT/Scanfold:
  data_format: 2
  description: Singularity container for Scanfold
  filenames:
  - Singularity
  full_name: ResearchIT/Scanfold
  latest_release: null
  readme: '<h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667"
    alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/6b7af09ab5d3e54feb3acda4c7b70aef9718f2928a49a50c92ea6ce95e96b2f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e657874666c6f772d254532253839254135302e32352e312d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/Nextflow-%E2%89%A50.25.1-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p>The goal of many metagenomics studies is to characterize the content and relative
    abundance of sequences of interest from the DNA of a given sample or set of samples.
    You may want to know what is contained within your sample or how abundant a given
    sequence is relative to another.</p>

    <p>Often, metagenomics is performed when the answer to these questions must be
    obtained for a large number of targets where techniques like multiplex PCR and
    other targeted methods would be too cumbersome to perform. AmrPlusPlus can process
    the raw data from the sequencer, identify the fragments of DNA, and count them.
    It also provides a count of the polymorphisms that occur in each DNA fragment
    with respect to the reference database.</p>

    <p>Additionally, you may want to know if the depth of your sequencing (how many
    reads you obtain that are on target) is high enough to identify rare organisms
    (organisms with low abundance relative to others) in your population. This is
    referred to as rarefaction and is calculated by randomly subsampling your sequence
    data at intervals between 0% and 100% in order to determine how many targets are
    found at each depth. AmrPlusPlus can perform this analysis as well.</p>

    <p>With AmrPlusPlus, you will obtain count files for each sample that can be combined
    into a count matrix and analyzed using any statistical and mathematical techniques
    that can operate on a matrix of observations.</p>

    <h2>

    <a id="user-content-more-information" class="anchor" href="#more-information"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More
    Information</h2>

    <ul>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/requirements.md">Software
    Requirements</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/installation.md">Installation</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/usage.md">Usage</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/configuration.md">Configuration</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/output.md">Output</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/dependencies.md">Dependencies</a></li>

    <li><a href="https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/contact.md">Contact</a></li>

    </ul>

    <h2>

    <a id="user-content-description-of-scripts" class="anchor" href="#description-of-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Description
    of scripts</h2>

    <p>main_qiime2.nf</p>

    <pre><code>nextflow run main_qiime2.nf --reads "/s/angus/index/projs/mega_tylan/concat_16S_LN/raw_data/*_{1,2}.fq"
    --output XIT_LN_qiime2 -profile local --metadata /media/AngusWorkspace/run_Jake/LN_metadata.tsv
    --classifier /media/AngusWorkspace/run_Jake/bioinformatic-nextflow-pipelines/gg-13-8-99-515-806-nb-classifier.qza
    -resume --threads 25

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1570729149.0
ResearchIT/SimNIBS:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ResearchIT/SimNIBS
  latest_release: null
  readme: '<h3>

    <a id="user-content-simnibs-singularity-recipe" class="anchor" href="#simnibs-singularity-recipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SimNIBS
    singularity recipe</h3>

    <p>Before building, place the SimNIBS source tarball in the /tmp directory. (recipe
    version 2.1.1)</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1546981375.0
ResearchIT/nwchem:
  data_format: 2
  description: Singularity Recipe for NWChem
  filenames:
  - Singularity.6.6-openmpi
  full_name: ResearchIT/nwchem
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-nwchem" class="anchor" href="#singularity-recipe-for-nwchem"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for NWChem</h1>

    <p>This repo contains recipes to run <a href="http://www.nwchem-sw.org/index.php/Main_Page"
    rel="nofollow">NWChem</a>

    within a <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a>
    container, which can be built

    using <a href="https://singularity-hub.org/" rel="nofollow">Singularity Hub</a></p>

    <p>Versions:</p>

    <ul>

    <li>6.6 - NWChem with OpenMPI installed via EPEL</li>

    </ul>

    <h2>

    <a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to Use:</h2>

    <p>You need to have openmpi v1 installed on your local machine (via yum or as
    a module).

    Testing was performed with openmpi 1.10.6.</p>

    <p>Run example:</p>

    <p>mpirun -np 2 singularity run shub://ResearchIT/nwchem:6.6-openmpi test.nw</p>

    <h2>

    <a id="user-content-alternative-method" class="anchor" href="#alternative-method"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Alternative
    method:</h2>

    <p>use the provided bash wrapper and module file to use the nwchem singularity
    container like a standard module

    (this assumes you have a singularity/2.4 and openmpi/1 modules)</p>

    <p>e.g.</p>

    <p>module load nwchem/6.6</p>

    <p>mpirun -np 2 nwchem test.nw</p>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics:
  - nwchem
  - singularity
  updated_at: 1551769652.0
ResearchIT/qiime2:
  data_format: 2
  description: Singularity Recipe for QIIME 2
  filenames:
  - Singularity.2019.4
  - Singularity.2017.12
  - Singularity
  - Singularity.2018.6
  - Singularity.2018.2
  - Singularity.2019.10
  full_name: ResearchIT/qiime2
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-qiime2" class="anchor" href="#singularity-recipe-for-qiime2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for QIIME2</h1>

    <p>This repo contains recipe run <a href="https://qiime2.org" rel="nofollow">qiime2</a>
    within a

    <a href="https://singularity.lbl.gov/" rel="nofollow">Singularity</a> container,
    which can be built

    using <a href="https://singularity-hub.org/" rel="nofollow">Singularity Hub</a></p>

    <p>Versions:

    2017.12 - QIIME2-2017.12 installed on CentOS 7</p>

    <h2>

    <a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to Use:</h2>

    <p>singularity run shub://ResearchIT/qiime2 --help</p>

    '
  stargazers_count: 3
  subscribers_count: 6
  topics:
  - qiime
  - singularity
  updated_at: 1576035067.0
ResearchIT/revbayes-singularity:
  data_format: 2
  description: Singularity container for https://github.com/revbayes/revbayes
  filenames:
  - Singularity
  full_name: ResearchIT/revbayes-singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3722" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PLINK association analysis toolset</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1589324901.0
ResearchIT/scanindel:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ResearchIT/scanindel
  latest_release: null
  readme: '<h3>

    <a id="user-content-scanindel-singularity-recipe" class="anchor" href="#scanindel-singularity-recipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ScanIndel
    Singularity recipe</h3>

    <p>ScanIndel is a python program to detect indels (insertions and deletions) from
    NGS data by re-align and de novo assemble soft clipped reads.</p>

    <p>Original repository <a href="https://github.com/cauyrd/ScanIndel">here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1539032220.0
ResearchIT/scipion:
  data_format: 2
  description: Singularity Recipe for scipion
  filenames:
  - Singularity.2.0
  - Singularity.2.0.cuda
  - Singularity.1.1
  - Singularity.1.1.cuda
  full_name: ResearchIT/scipion
  latest_release: null
  readme: '<p><a href="https://zenodo.org/badge/latestdoi/124456755" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4641d504e79e577f2add43b190e60f3910a1688ac8f26f972d799fd6f3f4b213/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3132343435363735352e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/124456755.svg" style="max-width:100%;"></a>
    <a href="https://github.com/ambv/black"><img src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667"
    alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    class="anchor" href="#ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ancient
    hybridization and adaptive introgression of an invadolysin gene in <em>Schistosoma
    haematobium</em>.</h1>

    <p>Roy N. Platt II, Marina McDew-White, Winka Le Clec''h, Frederic D. Chevalier,
    Fiona Allan, Aidan M. Emery, Amadou Garba, Shaali M. Ame, Joanne P. Webster, David
    Rollinson, Bonnie L. Webster, Timothy J. C. Anderson.</p>

    <p>The parasitic blood fluke <em>Schistosoma</em> <em>haematobium</em> causes
    urogenital schistosomiasis in humans and is a major cause of morbidity and mortality
    across sub-Saharan Africa. <em>S</em>. <em>haematobium</em> can hybridize with
    closely-related livestock schistosomes, including <em>S</em>. <em>bovis</em>,
    however the frequency, direction, age and genomic consequences of hybridization
    in nature are unknown. We sequenced 96 <em>S</em>. <em>haematobium</em> exomes
    from Niger and the Zanzibar archipelago. We found evidence of an ancient, adaptive
    introgression event between Nigerien <em>S</em>. <em>haematobium</em> and <em>S</em>.
    <em>bovis</em> occurring 108-613 generations ago. Introgressed S. bovis alleles
    constitute 3.3-8.2% of Nigerien <em>S</em>. <em>haematobium</em> genomes. Some
    <em>S</em>. <em>bovis</em> alleles have reached high frequency and show signatures
    of directional selection; the strongest signal spans a single gene in the invadolysin
    gene family, an M8 metalloprotease associated with parasitic life-history traits.</p>

    <h4>

    <a id="user-content-biorxiv-pre-print" class="anchor" href="#biorxiv-pre-print"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="https://doi.org/10.1101/539353" rel="nofollow">bioRxiv pre-print</a>

    </h4>

    <hr>

    <h3>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTES:</h3>

    <p>All analyses were conducted on a HPCC in a <code>singularity</code> container
    or in a <code>conda</code> managed environment. The singularity recipe and conda
    environmental yaml are in the <code>config</code> dir.</p>

    <p>Raw code is found in the <code>scripts</code> dir</p>

    <p>Data that is not readily available through the SRA is in the <code>data</code>
    dir.  These will be housed in an online repository (ex. Dryad), but provided here
    for documentation purposes.</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics:
  - singularity
  - scipion
  updated_at: 1592515044.0
ResearchIT/singularity-freesurfer:
  data_format: 2
  description: Singularity recipe for freesurfer
  filenames:
  - Singularity
  full_name: ResearchIT/singularity-freesurfer
  latest_release: null
  readme: '<h1>

    <a id="user-content-uresnet-tomo-seg" class="anchor" href="#uresnet-tomo-seg"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>uresnet-tomo-seg</h1>

    <p>uresnet based deep neutral network for the segmentation of high resolution
    cryo-EM tomographs</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics: []
  updated_at: 1603915556.0
ResearchIT/spack-singularity:
  data_format: 2
  description: Singularity containers with software installed via Spack
  filenames:
  - Singularity.trinity
  - Singularity.openmpi
  - Singularity.busco
  - Singularity.spack
  - Singularity.gcc
  full_name: ResearchIT/spack-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7-reprozipfslbuild-centos5" class="anchor" href="#centos7-reprozipfslbuild-centos5"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>centos7-reprozip.fslbuild-centos5</h1>

    <p>PreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)</p>

    '
  stargazers_count: 1
  subscribers_count: 7
  topics:
  - spack
  - openmpi
  - singularity
  updated_at: 1557449681.0
ResearchIT/tofu2:
  data_format: 2
  description: Singularity Recipe for Tofu2
  filenames:
  - Singularity
  - Singularity.v17
  full_name: ResearchIT/tofu2
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-tofu2" class="anchor" href="#singularity-recipe-for-tofu2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipe for Tofu2</h1>

    <p>This repo contains recipes to run <a href="https://github.com/PacificBiosciences/IsoSeq_SA3nUP/wiki/%5BBeta%5D-ToFU2:-running-and-installing-ToFU2#install">Tofu2</a>

    within a <a href="http://singularity.lbl.gov/" rel="nofollow">Singularity</a>
    container, which can be built

    using <a href="https://singularity-hub.org/" rel="nofollow">Singularity Hub</a></p>

    <p>Versions:</p>

    <ul>

    <li>v17 - Tofu2 installed on Ubuntu</li>

    </ul>

    <h2>

    <a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to Use:</h2>

    <p>Run example:</p>

    <p>singularity run shub://ResearchIT/tofu2 run_preCluster.py --cpus=4</p>

    <h2>

    <a id="user-content-alternative-method" class="anchor" href="#alternative-method"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Alternative
    method:</h2>

    <p>use the provided bash wrapper and module file to use the tofu2 singularity
    container like a standard module

    (this assumes you have a singularity/2.4 module)</p>

    <p>e.g.</p>

    <p>module load tofu2/v17

    tofu2 run_preCluster.py --cpus=4</p>

    '
  stargazers_count: 0
  subscribers_count: 6
  topics:
  - tofu
  - pacbio
  - singularity
  updated_at: 1522255502.0
Sarah145/batch_correct:
  data_format: 2
  description: Comparison of batch correction methods for scRNA-seq data - basically
    a clone of BatchBench
  filenames:
  - Singularity
  full_name: Sarah145/batch_correct
  latest_release: null
  readme: '<h2>

    <a id="user-content-batch-correction-pipeline" class="anchor" href="#batch-correction-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    correction pipeline</h2>

    <p>This repository contains scripts to run a Nextflow pipeline to compare different
    batch correction methods for single-cell RNA-seq data. This is mostly just a clone
    of the <a href="https://github.com/cellgeni/batchbench">BatchBench</a> pipeline
    from the CellGen IT team at Sanger but I couldn''t get that to run so made some
    edits and added one or two extra things.</p>

    <p>The input files for this pipeline must be .Rds files of the uncorrected data
    as a SingleCellExperiment object (all batches in one object) with batch labels
    stored in the <code>batch_key</code> (''Batch'' by default) column and cell type
    labels stored in the <code>celltype_key</code> (''cell_type1'' by default) column.</p>

    <p>The pipeline will run 7 different batch correction methods on the data:</p>

    <ul>

    <li>Scanorama</li>

    <li>BBKNN</li>

    <li>Seurat 3</li>

    <li>Combat</li>

    <li>Harmony</li>

    <li>limma</li>

    <li>MNNCorrect</li>

    </ul>

    <p>For each method, 5 different evaluation metrics are returned:</p>

    <ul>

    <li>Batch entropy (from <a href="https://www.biorxiv.org/content/10.1101/2020.05.22.111211v2"
    rel="nofollow">BatchBench</a>) - measure of how well batches are aligned after
    correction - related to the probability that for each cell, its <em>k</em> nearest
    neighbors come from a different batch - value reported is average entropy scaled
    between 0-1 - high batch entropy = well-mixed batches, low batch entropy = poorly-mixed
    batches.</li>

    <li>Cell type entropy (from <a href="https://www.biorxiv.org/content/10.1101/2020.05.22.111211v2"
    rel="nofollow">BatchBench</a>) - same as batch entropy but using cell type labels
    instead - high cell type entropy = mixing of cell types (not good), low cell type
    entropy = cell types are not mixing (good).</li>

    <li>Batch ASW (from <a href="https://github.com/theislab/scib">scIB</a>) - average
    silhouette width of batches - scaled between -1-1 - high batch ASW = dense, well-separated
    batches (bad), low batch ASW = well mixed batches (good).</li>

    <li>Cell type ASW (from <a href="https://github.com/theislab/scib">scIB</a>) -
    same as batch ASW but for cell type labels - high cell type ASW = good, low cell
    type ASW = bad.</li>

    <li>Recovery of marker genes - this idea was taken from the BatchBench paper but
    couldn''t find code for it so wrote my own - not sure if it''s right. For methods
    that correct the expression matrix (Scanorama, Seurat3, Combat, limma, MNNCorrect),
    found marker genes for each cell type (by batch and in the merged dataset), before
    and after batch correction, then compared the list of total marker genes identified
    before batch correction to the list of total marker genes identified after batch
    correction and calculated the Jaccard similarity index of the two lists. High
    Jaccard index = gene expression was not distorted too much by batch correction,
    most markers genes could still be identified (good), low Jaccard index = batch
    correction highly distorted the gene expression values so not as many marker genes
    could be recovered (bad). Jaccard index = 1 - all marker genes recovered, Jaccard
    index = 0 - no marker genes recovered.</li>

    </ul>

    <p>To run pipeline:</p>

    <ul>

    <li>Need to have Nextflow and Singularity installed.</li>

    <li>Clone this repo and <code>cd</code> into it.</li>

    <li>Pull Singularity image - <code>singularity pull shub://Sarah145/batch_correct</code>.</li>

    <li>Edit the <code>nextflow.config</code> script with location of data, batch
    key, cell type key, etc. <em>Note: profile section of the nextflow.config script
    in this repo is configured to run on cluster with slurm.</em>

    </li>

    <li>Edit <code>dataset_list.txt</code> file with name of files - one file on each
    line, no file extension.</li>

    <li>Run pipeline with <code>nextflow run main.nf -profile singularity -with-trace
    trace.txt -with-dag flowchart.png</code>.</li>

    <li>Compile html report of run by running <code>./compile_report.R &lt;sample_name&gt;</code>.</li>

    </ul>

    <p><strong>Overview of pipeline</strong></p>

    <p><a href="https://github.com/Sarah145/batch_correct/blob/master/imgs/flowchart.png?raw=true"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/Sarah145/batch_correct/raw/master/imgs/flowchart.png?raw=true"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - scrna-seq-analysis
  - batch-correction
  updated_at: 1602016843.0
TormodLandet/Ocellaris:
  data_format: 2
  description: This is a github MIRROR of the main ocellaris repo on bitbucket (https://bitbucket.org/ocellarisproject/ocellaris).
    NO pull request or issues should go to this repo, please! This repository is only
    here to support Singularity Hub which lacks bitbucket support. The code in this
    repository may be severely out of date! It is synced with bitbucket manually and
    may be months or years behind!
  filenames:
  - containers/Singularity
  full_name: TormodLandet/Ocellaris
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>News</h1>\n\
    <p>First release of gemBS-rs, a complete rewrite of the gemBS pipeline (apart\
    \ from the mapper) in Rust bringing increased\nstability while maintaining the\
    \ high performance of gemBS: <a href=\"https://github.com/heathsc/gemBS-rs.git\"\
    >https://github.com/heathsc/gemBS-rs.git</a></p>\n<h1>\n<a id=\"user-content-gembs\"\
    \ class=\"anchor\" href=\"#gembs\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>gemBS</h1>\n<p>gemBS is a high performance\
    \ bioinformatic pipeline designed for highthroughput analysis\nof DNA methylation\
    \ data from whole genome bisulfites sequencing data\n(WGBS). It combines GEM3,\
    \ a high performance read aligner and\nbs_call, a high performance variant and\
    \ methyation caller, into a streamlined and efficient pipeline for\nbisulfite\
    \ sueqnce analysis.</p>\n<p>The manuscript describing the pipeline is available\
    \ <a href=\"https://www.biorxiv.org/content/early/2017/10/11/201988\" rel=\"nofollow\"\
    >here</a></p>\n<hr>\n<h2>\n<a id=\"user-content-licensing\" class=\"anchor\" href=\"\
    #licensing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Licensing</h2>\n<p>gemBS is licensed under GPL.</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Download</h2>\n\
    <p>Use <code>git clone --recursive</code> to retrieve the complete source code\
    \ including the code from external projects such as <code>bs_call</code> and <code>gem3-mapper</code>.</p>\n\
    <pre><code>git clone --recursive https://github.com/heathsc/gemBS.git\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<ol>\n<li>Before starting the installation of gemBS,\
    \ you will need to install\nor check the installation of several packages.</li>\n\
    </ol>\n<p>a) gcc with development libraries\nb) python3, pip3, matplotlib, multiprocess\n\
    c) zlib, lzma, openssl, libcurl, libncurses, wget, pigz</p>\n<p>If you are working\
    \ on a clean (fairly recent) Ubuntu installation, you\ncan install everything\
    \ required with the followiwg commands:</p>\n<pre><code>sudo apt-get update\n\
    sudo apt-get install -y python3 build-essential git python3-pip wget pigz\nsudo\
    \ apt-get install -y zlib1g-dev libbz2-dev\nsudo apt-get install -y libncurses5-dev\
    \ liblzma-dev libssl-dev libcurl4-openssl-dev\npip3 install matplotlib multiprocess\n\
    </code></pre>\n<ol start=\"2\">\n<li>\n<p>Download the gemBS distribution if you\
    \ haven't already done so:</p>\n<p><code>git clone --recursive https://github.com/heathsc/gemBS.git</code></p>\n\
    </li>\n<li>\n<p>Use python install command:</p>\n</li>\n</ol>\n<p>To install to\
    \ the standard system location (i.e., so that all users\ncan use gemBS):</p>\n\
    <pre><code>``python3 setup.py install``\n</code></pre>\n<p>To install to the user's\
    \ home directory:</p>\n<pre><code>``python3 setup.py install --user``\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-check-your-installation\" class=\"anchor\" href=\"\
    #check-your-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Check your installation</h2>\n<p>For checking\
    \ your installation follow this\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/example.html\"\
    \ rel=\"nofollow\">worked example</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Documentation\
    \ can be found at\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/index.html\"\
    \ rel=\"nofollow\">gemBS</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-changelog\"\
    \ class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Changelog:</h2>\n<pre><code>3.5.5\
    \ Fix logging bug caused by trimming change in 3.5.3\n3.5.4 Fix bug in the output\
    \ of strand specific cpg txt files (not\n      encode Bed files) where the 'C'\
    \ entry was not being printed\n3.5.3 Allow for read end specific trimming in bs_call\n\
    3.5.3 Enable range checks and asserts in bs_call all target; add bs_call debug\
    \ target\n3.5.2 Correct problems with gcc10.  Move to htslib/samtools/bcftools\
    \ version 1.11\n3.5.1 Check if C compiler requires --std=c99 flag for standards\
    \ conformant behaviour\n3.5.1 Make sure bgzip is copied correctly during installation\n\
    3.5.0 Make bs_call process contig pools from largest to smallest (this change\
    \ alters the sqlite db format so\n      if you have a previously started gemBS\
    \ run you should (a) remove the .gemBS directory, (b) redo the\n      'gemBS prepare'\
    \ step to recreate the db file and (3) run 'gemBS db-sync'. \n3.5.0 Switch bs_call\
    \ and snpxtr to use the new dbSNP index format\n3.5.0 Add ability of dbSNP to\
    \ read the new JSON and VCF  dbSNP format files\n      that are now used for human\
    \ and non-human species respectively\n3.5.0 Add multithreading to dbSNP_idx\n\
    3.5.0 Change format of dbSNP index to allow (a) efficient loading\n      of SNP\
    \ data for individual contigs and (b) parallel index creation \n3.5.0 Rewrite\
    \ mextr and snpxtr as standalone tools rather than\n      bcftools plugins.  Now\
    \ multithreaded and (relatively) memoryefficient\n3.5.0 Replace bedToBigBed and\
    \ wigToBigWig to reduce memory usage\n      and improve speed\n3.4.5 Fix crash\
    \ when using the -k (keep-mismatch) flag, and fix rare hangs at end of processing\n\
    3.4.4 Sort input bcf files to bcftools concat stage to ensure reproducibility.\n\
    3.4.4 Add extra sort keys when generating pools to ensure stability of pool membership\
    \ in the event of multiple contigs\n      having the same size\n3.4.3 Remove calculation\
    \ of the goodness of filter (GOF) as this is expensive, non-standard and unreliable.\
    \  Removing this\n      removes the dependency on GSL.\n3.4.3 Add autodetection\
    \ of output format to bs_call (unless explicitly specified on the command line)\n\
    3.4.2 Add CRAM support (via make_cram option in configuration file)\n3.4.1 Add\
    \ benchmark-mode that does not write date or program version numbers into SAM/BAM\
    \ or VCF/BCF files\n      Switch to samtools, bcftools and htslib v1.10\n3.4.0\
    \ Move to new bs_call version (2.1.0) which is more efficient\n      in memory\
    \ use and can read BAMs and write BCFs natively.\n      The new bs_call requires\
    \ a faidx indexed reference, so gemBS\n      no creates this during indexing.\n\
    3.4.0 Add switches to give more control to threads and memory\n      usage in\
    \ mapping and calling stages\n3.3.3 Remove legacy pathway for config files with\
    \ no header line (fix issue 'error in gemBS index #65)\n3.3.2 Fix error where\
    \ header line for wig files could be omitted\n3.3.2 Fix generation of non_cpg\
    \ files\n3.3.1 Fix Attribute error bug due to not checking if conversion is a\
    \ list\n3.3.0 Make new release for IHEC\n3.3.0 Switch conversion default in IHEC_standard\
    \ configuration to 0.01,0.05 rather than auto, which can give odd results if conversion\
    \ controls not present or not working correctly\n3.3.0 Fix bug where conversion\
    \ parameters could be ignored\n3.2.13 Fix formatting bug in mextr with multiple\
    \ samples (not triggered in normal gemBS use)\n3.2.12 Ensure that conversion statistics\
    \ are correctly calculated for non-stranded or reverse conversion protocols\n\
    3.2.11 Introduce reverse_conversion option for mapping where read 1 is G2A converted\
    \ and read 2 is C2T converted\n3.2.10 Correct regex patch for single end reads\n\
    3.2.9 Update Singularity and Dockerfile recipes to allow kemp utils to be built\
    \ correctly\n3.2.9 Make setup.py and gemBS/commands.py read the version information\
    \ from gemBS/version.py, so ensuring consistency\n3.2.9 Fix bug added in last\
    \ version where options in config file were not being taken into account\n3.2.8\
    \ Fix mis specification errors in long options for mextr. \n3.2.8 Fix bug where\
    \ mextr (methyl extract plugin for bcftools) would crash if cpg output  option\
    \ was not set.\n3.2.7 Apply patches for bugs in handling single end reads (suggested\
    \ by I. Moghul)\n3.2.7 Changed regex for filenames to make it more general (suggested\
    \ by I. Moghul)\n3.2.7 Fixed bug (reported by chhylp123) where zero arguments\
    \ to some options were being ignored\n3.2.6 Cleaned up compilation and cleaning\
    \ of gemBS tools\n3.2.6 Fixed python error if either the over conversion reference\
    \ sequence was not defined\n3.2.6 Added check in bs_call that conversion parameters\
    \ are valid (between 0 and 1)\n3.2.6 Perform more stringent sanity checking on\
    \ conversion vaalues when autocomputed by gemBS\n3.2.6 Use --diasble-lzma configuration\
    \ flag for samtools and bcftools as we don't need it and it removes an unneccesary\
    \ dependency\n3.2.6 Add install options --disable-cuda (on by default) and --enable-cuda\
    \ that affect GEM3 comppilation\n3.2.6 Bug fix with incorrect handling of duplicate\
    \ reads\n3.2.5 Minor bug fix - correct error with non-paired end non-bisulfite\
    \ reads\n3.2.4 Modify the bisulfite processing in gem-mapper to be more efficient\
    \ (in particular for the non-stranded option)\n3.2.4 Modify gemBS to use the new\
    \ conversion options for gem-mapper\n3.2.4 Switch gem-mapper to use option --underconversion-sequence\
    \ and --overconversion-sequence rather than --underconversion_sequence to be consistent\
    \ with other options\n3.2.3 Fixed bug if conversion parameters were not set\n\
    3.2.2 Rework non-stranded mode so that both possible conversions are tried and\
    \ the results merged\n3.2.2 Fix bug where non-stranded flag was not being passed\
    \ to mapper in paired end mode\n3.2.1 Move warning message from bscall from stdout\
    \ to stderr\n3.2.1 Switch Singularity build to use Ubuntu 16.04 rather than 18.04\
    \ to allow the image to work in CentOS 6 (Docker build changed as well to keep\
    \ the two in sync)\n3.2.1 Fix undeclared variable bugs and missing --ignore-deps\
    \ option in merge-bcfs\n3.2.1 Add default for dbSNP_index if dbSNP_files is set\n\
    3.2.1 Add gsl-path install option\n3.2.0 Make new release\n3.1.0 Make installation\
    \ process more modular.  Allow for sub-installs\n3.1.0 Add support for reading\
    \ config from ${index_dir}/gemBS.json if it exists\n3.1.0 Add --reference-bias\
    \ option to mextr and gemBS extract\n3.1.0 Add support for non-bisulfite mapping\
    \ of individual datasets\n3.1.0 Allow white space in variable values\n3.1.0 Allow\
    \ fallback to gzip if pigz not present\n3.1.0 Add --dry-run, --json, --ignore-db\
    \ and --ignore-dep to extract command\n3.1.0 Add --ignore-dep option to call and\
    \ merge-bcfs commands\n3.1.0 Add SNP extraction function to extract command\n\
    3.0 Make v3.0 release\n3.0 Merge with master branch.\n3.0 Bump samtools sort memory\
    \ limit to 2G\n3.0 Add extra_references option for reference generation\n3.0 Allow\
    \ input files to mapping to be shell commands\n3.0 Add links to documentation\n\
    3.0 Upload new yeast example and add documentation\n3.0 Add --dir option to gemBS\n\
    3.0 Add --ignore-db options for --dry-run / --json\n3.0 Add --json output option\
    \ for dry runs\n3.0 Update help text to match new functions\n3.0 Introduce standard\
    \ analysis configurations stored within distribution\n3.0 Switch gem3-mapper distribution\
    \ to gembs branch on official gem3-mapper repo\n3.0 Removal of incomplete files\
    \ and roll back of db in the event of pipeline failure\n3.0 Automatic removal\
    \ of individual BAMs and BCFs after successful merging\n3.0 Prevent pipelines\
    \ hanging in event of failure\n3.0 Generate ENCODE bed and bigbed files\n3.0 Switch\
    \ to python 3\n3.0 Switch to mextr for BCF filtering\n3.0 Include fetch and build\
    \ of samtools / bcftools during build process\n3.0 Add dry-run capability to map\
    \ and call commands\n3.0 Introduce contig pools to automatically group small contigs\n\
    3.0 Automatic generation of contig.size files from index build\n3.0 Allow use\
    \ of in memory sqlite3 db as an option\n3.0 Allow multiple instances of gemBS\
    \ (possible on different hosts) to work \n    simultaneously on the same analysis\n\
    3.0 Reduce and simply commands\n3.0 Add Dockerfile\n3.0 Add multi-threading and\
    \ multi-processing options for most commands\n3.0 Use sqlite3 to track progress\
    \ of analyses, file paths etc.\n3.0 Added more flexible configuration options\
    \ (new csv format + new configuration file)\n3.0 Remove test dataset from distribution\
    \ (distribute from web site)\n2.1.0 Ensure commands run during pipeline come from\
    \ installation\n2.1.0 Added Singularity build recipe\n2.1.0 Add new command gemBS\
    \ direct-mapping\n2.1.0 Fixed Makefile clean in tools\n2.0.2 Fixed bug related\
    \ with the percentage of High Quality Variant in Variants summary report.\n2.0.2\
    \ Check temporary directory existence.\n2.0.2 Fixed QualityNonRefCpg sample name\
    \ in png image.\n2.0.2 Fixed mapper issues related with aligning performace.\n\
    2.0.2 Fixed arguments for Under/Over Conversion sequence name in gem3-mapper\n\
    2.0.1 On bscall repository, fixed argument -k about discarded reads that do not\
    \ form proper pairs.\n2.0 Check tmp folder before starting mapping process.\n\
    2.0 Added Left and Right Trimming optional arguments to gemBS bscall.\n2.0 Added\
    \ GC Coverage correlation value to BS Call Stats Summary.\n2.0 Fixed error when\
    \ reporting complete path to not found bam files.\n2.0 Fixed iteration over sampleBams\
    \ dictionary in MergeAll method.\n2.0 Updated: Avoid redo indexing when merging\
    \ just one file.\n2.0 Changed conversion formula.\n2.0 Added parameter for dbSNP.\n\
    2.0 Added threads to bscall.\n2.0 Removed CpGs reports. Already done from bscall\
    \ report.\n2.0 Fixed bs_call makefile for the gcc to be used.\n2.0 New bscall\
    \ version. Generates JSON report.\n2.0 Removed gemBS options snp-stats,cpg-report,cpg-stats.\n\
    2.0 Added summary report from the bs_call json stats\n2.0 New BSCall Report. From\
    \ bscall son file generates three types of reports:\n    Mapping and Coverage\
    \ Report\n    Bs-Genotypes Calls Report\n    Methylation Statistics report\n1.7\
    \ Added non stranded read conversion parameter\n1.7 Fixed SE crash when estimating\
    \ overlapped bases.\n1.7 Fixed gem-index (gem3) to follow fastq and SAM specifications.\
    \ \n    Modified gem3-mapper repository external module.\n    New external module\
    \ https://github.com/heathsc/gem3-mapper.git\n1.7 Fixed threads parameter to samtools\
    \ merge\n1.7 Fixed threads parameter to gem-mapper\n1.7 Removed Indels Field on\
    \ Variants Report.\n1.7 Added Overlapping Bases at Mapping Report\n1.7 Modified\
    \ Base Counts Overall, removed Base Counts general and Base Counts Overall\n1.7\
    \ New Dinucleotide CpGs Report\n    New table dinucleotide stats\n    New plots\
    \ for Informative Reads and CpGs\n    Methylation levels plots for different types\
    \ of CpGs\n    Summary Table\n1.7 New Readme file to inform about report test\n\
    1.7 New basic statis table for Variants Report\n1.7 Removed parameter -r (reference\
    \ length) parameter for mapping reports command (gemBS bsMap).\n1.6 New CpGs Density\
    \ plot, include box plos, bar plot and fitting curve\n1.6 Change name at CpG report:\n\
    \    \"Heterozygous\" for \"Alternative CX\"\n    \"De Novo CpGs Methylation Status\"\
    \ for \"Non Reference CpGs\"\n    \"CpGs with SNP\" for \"SNPs (CX) at Reference\
    \ CpGs\"\n1.6 CpGs Report Simplified to Q&gt;20\n1.6 BigWig Default parameters\
    \ for filtering CpG per a given quality and a total number of supported informative\
    \ reads   \n1.5 Initial Release  \n</code></pre>\n<hr>\n<h2>\n<a id=\"user-content-developers\"\
    \ class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Developers</h2>\n<p>gemBS:</p>\n\
    <ul>\n<li>Marcos Fernandez-Callejo - <a href=\"mailto:marcos.fernandez@cnag.crg.eu\"\
    >marcos.fernandez@cnag.crg.eu</a>\n</li>\n<li>Simon Heath - <a href=\"mailto:simon.heath@gmail.com\"\
    >simon.heath@gmail.com</a>\n</li>\n</ul>\n<p>gem mapper:</p>\n<ul>\n<li>Santiago\
    \ Marco-Sola - <a href=\"mailto:santiagomsola@gmail.com\">santiagomsola@gmail.com</a>\n\
    </li>\n</ul>\n<p>bisulfite caller and filtering:</p>\n<ul>\n<li>Simon Heath -\
    \ <a href=\"mailto:simon.heath@gmail.com\">simon.heath@gmail.com</a>\n</li>\n\
    </ul>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1553974960.0
UNM-CARC/FEniCS:
  data_format: 2
  description: FEniCS containers for CARC systems
  filenames:
  - Singularity.docker
  - Singularity.ubuntu
  full_name: UNM-CARC/FEniCS
  latest_release: null
  readme: '<h1>

    <a id="user-content-fenics" class="anchor" href="#fenics" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FEniCS</h1>

    <p>This repository contains a FEniCS container for UNM CARC high performance systems</p>

    <ul>

    <li>Singularity.docker - Singularity container built from the standard FEniCS
    docker container</li>

    <li>Singularity.ubuntu - Singularity container built from the FEniCS ubuntu packages</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1511832970.0
UNM-CARC/heudiconv:
  data_format: 2
  description: null
  filenames:
  - Singularity.ubuntu
  full_name: UNM-CARC/heudiconv
  latest_release: null
  readme: '<p>Not much</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1536784012.0
UNM-CARC/singularity-test:
  data_format: 2
  description: Singularity recipes for CARC systems
  filenames:
  - Singularity.centos
  - Singularity.ubuntu-mpich
  - Singularity.ubuntu-ompi
  full_name: UNM-CARC/singularity-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-tests" class="anchor" href="#singularity-tests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Tests</h1>

    <p>This repository contains test singularity recipes for Ubuntu and CentOS repository
    builds for

    HPC systems at the UNM Center for Advanced Research Computing. These recipes are
    generally built

    using Singularity Hub, which links to this repository, and are meant for debugging
    basic

    container setups that are then used to develop other more complex recipes.</p>

    <p>Note that these containers pull the CARC modules //into// the containers when
    they run so that

    code compiled outside the container can run inside the container. That''s rarely
    something you want to

    do, as one of the main point of containers is that they''re stable and reproducible.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1536783389.0
UNR-HPC/singularity-recipes:
  data_format: 2
  description: null
  filenames:
  - QE/Singularity.QuantumESPRESSO-6.3-intel-2018b-unrrc
  full_name: UNR-HPC/singularity-recipes
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipes" class="anchor" href="#singularity-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-recipes</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1544465938.0
UniversalDataTool/universal-data-tool:
  data_format: 2
  description: Collaborate & label any type of data, images, text, or documents, in
    an easy web interface or desktop app.
  filenames:
  - Singularity
  full_name: UniversalDataTool/universal-data-tool
  latest_release: v0.14.26
  readme: "<h1>\n<a id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"\
    #universal-data-tool\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Universal Data Tool</h1>\n<p><a href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ alt=\"Master Branch\" style=\"max-width:100%;\"></a>\n<a href=\"https://badge.fury.io/js/universal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    ><img src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\"\
    \ alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\"\
    \ alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://twitter.com/UniversalDataTl\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\"\
    \ alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Try it out at <a href=\"https://udt.dev\"\
    \ rel=\"nofollow\">udt.dev</a>, <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >download the desktop app</a> or <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">run on-premise</a>.</p>\n<p align=\"center\">\n  <a href=\"\
    https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p align=\"center\">\n  <b>\n  <a href=\"\
    https://docs.universaldatatool.com\" rel=\"nofollow\">Docs</a> \u2022 <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">Website</a> \u2022 <a href=\"\
    https://udt.dev\" rel=\"nofollow\">Playground</a> \u2022 <a href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\"\
    \ rel=\"nofollow\">Library Usage</a> \u2022 <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">On-Premise</a>\n  </b>\n</p>\n<p>The Universal Data Tool is\
    \ a web/desktop app for editing and annotating images, text, audio, documents\
    \ and to view and edit any data defined in the extensible <a href=\"https://github.com/UniversalDataTool/udt-format\"\
    >.udt.json and .udt.csv standard</a>.</p>\n<h2>\n<a id=\"user-content-supported-data\"\
    \ class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Supported Data</h2>\n<p align=\"\
    center\">\n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\"\
    \ rel=\"nofollow\">Image Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\"\
    \ rel=\"nofollow\">Image Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\"\
    \ rel=\"nofollow\">Text Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\"\
    \ rel=\"nofollow\">Named Entity Recognition</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\"\
    \ rel=\"nofollow\">Named Entity Relations / Part of Speech Tagging</a> \u2022\
    \ \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\"\
    \ rel=\"nofollow\">Audio Transcription</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\"\
    \ rel=\"nofollow\">Data Entry</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\"\
    \ rel=\"nofollow\">Video Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\"\
    \ rel=\"nofollow\">Landmark / Pose Annotation</a>\n</p>\n<h2>\n<a id=\"user-content-recent-updates\"\
    \ class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recent Updates</h2>\n<p><a href=\"\
    https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\">Follow\
    \ our development on Youtube!</a></p>\n\n<ul>\n<li><a href=\"https://youtu.be/q20WrCRcG4k\"\
    \ rel=\"nofollow\">Community Update Video 9</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\"\
    \ rel=\"nofollow\">Community Update Video 8</a></li>\n<li>\n<a href=\"https://youtu.be/glPPFgXibdw\"\
    \ rel=\"nofollow\">Community Update Video 7</a> <a href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\"\
    \ rel=\"nofollow\">(blog version)</a>\n\n</li>\n</ul>\n<h2>\n<a id=\"user-content-features\"\
    \ class=\"anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Features</h2>\n<ul>\n<li><strong>Collaborate\
    \ with others in real time, no sign up!</strong></li>\n<li>Usable on <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">web</a> or as <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\
    >Windows,Mac or Linux desktop application</a>\n</li>\n<li>Configure your project\
    \ with an easy-to-use GUI</li>\n<li><a href=\"https://universaldatatool.com/courses\"\
    \ rel=\"nofollow\">Easily create courses to train your labelers</a></li>\n<li>Download/upload\
    \ as easy-to-use CSV (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\
    >sample.udt.csv</a>) or JSON (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\
    >sample.udt.json</a>)</li>\n<li>Support for Images, Videos, PDFs, Text, Audio\
    \ Transcription and many other formats</li>\n<li>Can be <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\
    >easily integrated into a React application</a>\n</li>\n<li>Annotate images or\
    \ videos with classifications, tags, bounding boxes, polygons and points</li>\n\
    <li>Fast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly</li>\n\
    <li>Import data from Google Drive, Youtube, CSV, Clipboard and more</li>\n<li>Annotate\
    \ NLP datasets with Named Entity Recognition (NER), classification and Part of\
    \ Speech (PoS) tagging.</li>\n<li>Easily <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\
    >load into pandas</a> or <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\
    >use with fast.ai</a>\n</li>\n<li>Runs <a href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\"\
    \ rel=\"nofollow\">with docker</a> <code>docker run -p 3000:3000 universaldatatool/universaldatatool</code>\n\
    </li>\n<li>Runs <a href=\"https://singularity-hub.org/collections/4792\" rel=\"\
    nofollow\">with singularity</a> <code>singularity run universaldatatool/universaldatatool</code>\n\
    </li>\n</ul>\n<p align=\"center\"><kbd><a href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<h2>\n<a id=\"user-content-sponsors\"\
    \ class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sponsors</h2>\n<p><a href=\"\
    https://wao.ai\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\"\
    \ alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"></a>\n<a href=\"\
    https://momentum-tech.ca/\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\"\
    \ alt=\"momentum image\" style=\"max-width:100%;\"></a>\n<a href=\"https://www.enabledintelligence.net/\"\
    \ rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\"\
    \ alt=\"enabled intelligence image\" style=\"max-width:100%;\"></a></p>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <h3>\n<a id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Web\
    \ App</h3>\n<p>Just visit <a href=\"https://universaldatatool.com\" rel=\"nofollow\"\
    >universaldatatool.com</a>!</p>\n<p><em>Trying to run the web app locally? Run\
    \ <code>npm install</code> then <code>npm run start</code> after cloning this\
    \ repository to start the web server.</em></p>\n<h3>\n<a id=\"user-content-desktop-application\"\
    \ class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Desktop Application</h3>\n<p>Download\
    \ the latest release from the <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >releases page</a> and run the executable you downloaded.</p>\n<h2>\n<a id=\"\
    user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n\
    <ul>\n<li>(Optional) Say hi in the <a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\">Slack channel</a>!</li>\n<li>Read <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\
    >this guide to get started with development</a>.</li>\n</ul>\n<h2>\n<a id=\"user-content-contributors-\"\
    \ class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors <g-emoji class=\"\
    g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\
    >\u2728</g-emoji>\n</h2>\n<p>Thanks goes to these wonderful people (<a href=\"\
    https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\">emoji key</a>):</p>\n\
    \n\n\n<table>\n  <tr>\n    <td align=\"center\">\n<a href=\"https://twitter.com/seveibar\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Severin Ibarluzea</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://puskuruk.github.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Puskuruk</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/CedricJean\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>CedricJean</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://berupon.hatenablog.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>beru</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Ownmarc\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Marc</b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Wafaa-arbash\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Wafaa-arbash</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/pgrimaud\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Pierre Grimaud</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/sreevardhanreddi\"><img src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>sreevardhanreddi</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/mrdadah\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Mohammed Eldadah</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://x8795278.blogspot.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>x213212</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hysios\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>hysios </b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://congdv.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Cong Dao</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Renato Junior</b></sub></a><br><a\
    \ href=\"#translation-MrJunato\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://gitlab.com/rickstaa\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Rick</b></sub></a><br><a\
    \ href=\"#translation-rickstaa\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/anaplian\"><img src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>anaplian</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.behance.net/MiguelCarvalho13\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Miguel Carvalho</b></sub></a><br><a\
    \ href=\"#translation-miguelcarvalho13\" title=\"Translation\"><g-emoji class=\"\
    g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://kyleo.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Kyle OBrien</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hakkiyagiz\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Hakk\u0131 Ya\u011F\u0131z ERD\u0130\
    N\xC7</b></sub></a><br><a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/jvdavim\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Jo\xE3o Victor Davim</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n</table>\n\n\n\n<p>This project follows the <a\
    \ href=\"https://github.com/all-contributors/all-contributors\">all-contributors</a>\
    \ specification. Contributions of any kind welcome!</p>\n"
  stargazers_count: 1428
  subscribers_count: 31
  topics:
  - computer-vision
  - annotate-images
  - entity-recognition
  - desktop
  - classification
  - dataset
  - annotation-tool
  - deep-learning
  - text-annotation
  - named-entity-recognition
  - text-labeling
  - semantic-segmentation
  - image-segmentation
  - image-labeling-tool
  - machine-learning
  - image-annotation
  - csv
  - labeling
  - labeling-tool
  - hacktoberfest
  updated_at: 1622684072.0
VIB-CBD/scanpy-images:
  data_format: 2
  description: Docker and Singularity images for Scanpy
  filenames:
  - Singularity
  full_name: VIB-CBD/scanpy-images
  latest_release: null
  readme: '<p>Dependency full Scanpy Docker and Scanpy images based on Alpine.</p>

    <p>Includes:</p>

    <ul>

    <li>Loompy</li>

    <li>Louvain</li>

    <li>igraph</li>

    <li>ipython</li>

    <li>Jupyter</li>

    <li>Cython</li>

    <li>MulticoreTSNE</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1556798801.0
VUIIS/FMRIQA_app:
  data_format: 2
  description: ' Build for docker and singularity containers for FMRIQA'
  filenames:
  - Singularity.4.0.0
  - Singularity
  full_name: VUIIS/FMRIQA_app
  latest_release: null
  readme: '<h1>

    <a id="user-content-fmriqa_app" class="anchor" href="#fmriqa_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FMRIQA_app</h1>

    <p>This includes everything required (except for the "spm12r6225_with_vbm8r435_compiled"
    directory and "FMRIQA_v4_0_0" compiled MATLAB executable, which are too large
    to commit) to build a docker and corresponding singularity container for the FMRIQA
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/fmriqa/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://www.singularity-hub.org/collections/920" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/FMRIQA_app.git

    cd FMRIQA_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have "spm12r6225_with_vbm8r435_compiled" directory and "FMRIQA_v4_0_0"
    compiled MATLAB executable.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/fmriqa

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/FMRIQA_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1592512932.0
VUIIS/Multi_Atlas_app:
  data_format: 2
  description: ' Build for docker and singularity containers for Multi Atlas'
  filenames:
  - Singularity.2.1.0
  - Singularity
  full_name: VUIIS/Multi_Atlas_app
  latest_release: null
  readme: '<h1>

    <a id="user-content-multi_atlas_app" class="anchor" href="#multi_atlas_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Multi_Atlas_app</h1>

    <p>This includes everything required (except for the "full-multi-atlas" directory)
    to build a docker and corresponding singularity container for the Multi Atlas
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/multi_atlas/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://singularity-hub.org/collections/734" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Multi_Atlas_app.git

    cd Multi_Atlas_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have full-multi-atlas directory which contains atlases.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/multi_atlas

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Multi_Atlas_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1592512956.0
VUIIS/Temporal_Lobe_app:
  data_format: 2
  description: ' Build for docker and singularity containers for temporal lobe segmentation'
  filenames:
  - Singularity
  - Singularity.3.1.0
  full_name: VUIIS/Temporal_Lobe_app
  latest_release: null
  readme: '<h1>

    <a id="user-content-temporal_lobe_app" class="anchor" href="#temporal_lobe_app"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Temporal_Lobe_app</h1>

    <p>This includes everything required to build a docker and corresponding singularity
    container for the Temporal Lobe pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/temporal_lobe/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://www.singularity-hub.org/collections/828" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Temporal_Lobe_app.git

    cd Temporal_Lobe_app/

    ./build.sh

    </code></pre>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/temporal_lobe

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Temporal_Lobe_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1592512741.0
abs-tudelft/ArrowSAM:
  data_format: 2
  description: Genomics datastructures using Apache Arrow
  filenames:
  - Singularity/Singularity
  full_name: abs-tudelft/ArrowSAM
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-arrowsam\" class=\"anchor\" href=\"#arrowsam\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ArrowSAM</h1>\n<p>ArrowSAM is an in-memory Sequence Alignment/Map\
    \ (SAM) representation which uses <a href=\"https://arrow.apache.org/\" rel=\"\
    nofollow\">Apache Arrow framework</a> (A cross-language development platform for\
    \ in-memory data) and <a href=\"https://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/\"\
    \ rel=\"nofollow\">Plasma (Shared-Memory) Object Store</a> to store and process\
    \ SAM columnar data in-memory.</p>\n<h3>\n<a id=\"user-content-citing-arrowsam\"\
    \ class=\"anchor\" href=\"#citing-arrowsam\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-cite\"\
    ></a>Citing ArrowSAM</h3>\n<p>The following paper describes the ArrowSAM format\
    \ and its usage to speedup genomics pipelines. If you use ArrowSAM in your work,\
    \ please cite the following paper.</p>\n<blockquote>\n<p>Ahmad et al., (2020).\
    \ \"ArrowSAM: In-Memory Genomics Data Processing Using Apache Arrow\",\n<em>ICCAIS</em>.\
    \ <a href=\"https://doi.org/10.1109/ICCAIS48893.2020.9096725\" rel=\"nofollow\"\
    >doi.org/10.1109/ICCAIS48893.2020.9096725</a></p>\n</blockquote>\n<blockquote>\n\
    <p>Ahmad et al., \"Optimizing performance of GATK workflows using Apache Arrow\
    \ In-Memory data framework\",\n<em>BMC Genomics, presented at APBC2020</em>. <a\
    \ href=\"https://doi.org/10.1186/s12864-020-07013-y\" rel=\"nofollow\">https://doi.org/10.1186/s12864-020-07013-y</a></p>\n\
    </blockquote>\n<p>This repo contains following three components:</p>\n<ol>\n<li>\n\
    <p>ArrowSAM (In-memory SAM data representation) integrated <a href=\"https://github.com/tahashmi/bwa\"\
    >BWA-MEM</a>, Picard and GATK tools.<br></p>\n</li>\n<li>\n<p>A Singularity container\
    \ def file (To create an environment to use all Apache Arrow related tools and\
    \ libraries for ArrowSAM).<br></p>\n</li>\n<li>\n<p>Scripts to run different GATK\
    \ best practices recommended workflows (using different in-memory data placement\
    \ techniques like ArrowSAM, ramDisk and pipes for fast processing) to run complete\
    \ DNA analysis pipeline efficiently.<br></p>\n</li>\n</ol>\n<p>Note: ArrowSAM\
    \ and all other workflows are based on single node, multi-core machines.</p>\n\
    <h2>\n<a id=\"user-content-how-to-run\" class=\"anchor\" href=\"#how-to-run\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to run</h2>\n<ol>\n<li>\n<p>Install <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> container</p>\n</li>\n<li>\n<p>Download our\
    \ Singularity <a href=\"https://github.com/abs-tudelft/arrow-gen/tree/master/Singularity\"\
    >script</a> and generate singularity image (this image contains all Arrow related\
    \ packges necessary for building/compiling BWA-MEM, Picard and GATK)</p>\n</li>\n\
    <li>\n<p>Now enter into generated image using command:</p>\n<pre><code> sudo singularity\
    \ shell &lt;image_name&gt;.simg\n</code></pre>\n</li>\n<li>\n<p>Download <a href=\"\
    https://github.com/tahashmi/bwa\">BWA-MEM</a> inside image</p>\n<pre><code> git\
    \ clone https://github.com/tahashmi/bwa.git\n</code></pre>\n</li>\n<li>\n<p>Go\
    \ into bwa dir and compile BWA-MEM:</p>\n<pre><code> cd bwa\n make\n</code></pre>\n\
    </li>\n<li>\n<p>Now you can run BWA-MEM.</p>\n</li>\n</ol>\n"
  stargazers_count: 14
  subscribers_count: 3
  topics: []
  updated_at: 1619243360.0
aces/cbrain-containers-recipes:
  data_format: 2
  description: Recipes for singularity and docker containers used in CBRAIN
  filenames:
  - dcm2nii/Singularity.dcm2nii_v4AUGUST2014
  - ANTs/Singularity.ants_v2.1.0-gGIT-N
  - FreeSurfer/Singularity.FreeSurfer_v5.3
  - QEEG/Singularity.qeeg.v1.0-gGit-S
  - FSL/Singularity.fsl_v6.0.1
  - FSL/Singularity.fsl_v5.0.9
  full_name: aces/cbrain-containers-recipes
  latest_release: null
  readme: '<h1>

    <a id="user-content-cbrain-containers-recipes" class="anchor" href="#cbrain-containers-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cbrain-containers-recipes</h1>

    <p>Recipes for singularity and docker containers used in CBRAIN</p>

    '
  stargazers_count: 1
  subscribers_count: 9
  topics:
  - singularity
  - docker
  - cbrain
  updated_at: 1568207307.0
aertslab/SCope:
  data_format: 2
  description: Fast visualization tool for large-scale and high dimensional single-cell
    data
  filenames:
  - Singularity
  full_name: aertslab/SCope
  latest_release: untagged-64f13ec6d922418df08e
  readme: "<p><a href=\"https://www.codefactor.io/repository/github/aertslab/scope\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2a72055500b7462275ffcdff1c3045b8ec007caf55f0aabab6980bf1816e7e60/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f61657274736c61622f73636f70652f6261646765\"\
    \ alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/aertslab/scope/badge\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-scope-v182-visualization-of-large-scale-and-high-dimensional-single-cell-data\"\
    \ class=\"anchor\" href=\"#scope-v182-visualization-of-large-scale-and-high-dimensional-single-cell-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>SCope v1.8.2: Visualization of large-scale and high dimensional single\
    \ cell data</h1>\n<p><a href=\"images/SCope_Logo.png\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img src=\"images/SCope_Logo.png\" width=\"640\" style=\"\
    max-width:100%;\"></a></p>\n<p>SCope is a fast visualization tool for large-scale\
    \ and high dimensional scRNA-seq datasets.\nCurrently the data format supported\
    \ by SCope is <code>.loom</code>. This file format for very large omics datasets\
    \ is maintained by the Linnarsson Lab through the <code>loompy</code> Python package\
    \ (<a href=\"https://github.com/linnarsson-lab/loompy\">https://github.com/linnarsson-lab/loompy</a>).</p>\n\
    <p>View the <a href=\"CHANGELOG.md\">change log here</a>.</p>\n<h2>\n<a id=\"\
    user-content-demo\" class=\"anchor\" href=\"#demo\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Demo</h2>\n<p>Visit\
    \ <a href=\"https://scope.aertslab.org\" rel=\"nofollow\">https://scope.aertslab.org</a>\
    \ to test out SCope on several published datasets! Personal loom file files can\
    \ be uploaded but will only be kept for 5 days.</p>\n<h2>\n<a id=\"user-content-loom-file-generation\"\
    \ class=\"anchor\" href=\"#loom-file-generation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Loom File Generation</h2>\n<p>Currently\
    \ there are two packages to generate extended loom files compatible with SCope.</p>\n\
    <ul>\n<li>R: <a href=\"https://github.com/aertslab/SCopeLoomR\">SCopeLoomR</a>\
    \ - Dedicated R package</li>\n<li>Python: <a href=\"https://github.com/aertslab/pySCENIC\"\
    >pySCENIC</a> - Single function for generation from SCENIC results</li>\n</ul>\n\
    <p>Eventually the functionality from pySCENIC will be expanded and put in its\
    \ own python package.</p>\n<h2>\n<a id=\"user-content-run-scope\" class=\"anchor\"\
    \ href=\"#run-scope\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Run SCope</h2>\n<h3>\n<a id=\"user-content-standalone-app\"\
    \ class=\"anchor\" href=\"#standalone-app\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Standalone App</h3>\n<p>Standalone\
    \ apps for <strong>macOS</strong> and <strong>Linux</strong> can be downloaded\
    \ from <a href=\"https://github.com/aertslab/SCope/releases\">the releases page.</a>.</p>\n\
    <p><g-emoji class=\"g-emoji\" alias=\"exclamation\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2757.png\"\
    >\u2757</g-emoji> SCope standalone app requires Node.js (&gt; v9). To install\
    \ it, go to <a href=\"https://nodejs.org/en/download/\" rel=\"nofollow\">https://nodejs.org/en/download/</a>.</p>\n\
    <p>A <strong>Windows</strong> app is under development, but currently has no ETA.</p>\n\
    <h3>\n<a id=\"user-content-command-line\" class=\"anchor\" href=\"#command-line\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Command Line</h3>\n<p>You will need access to at least Python 3.7\
    \ do run this.</p>\n<ol>\n<li>Clone the GitHub repository and install,</li>\n\
    </ol>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Define where you want to clone the SCope repository.</span>\n\
    LOCAL_SCOPE_REPO=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${HOME}</span>/repos/SCope<span class=\"pl-pds\">\"</span></span>\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Clone SCope git repository.</span>\n\
    git clone https://github.com/aertslab/SCope <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Go to your local cloned SCope repository.</span>\n<span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >${LOCAL_SCOPE_REPO}</span><span class=\"pl-pds\">\"</span></span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Install SCope.</span>\nnpm install</pre></div>\n\
    <ol start=\"2\">\n<li>Run,</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Go to your local cloned\
    \ SCope repository.</span>\n<span class=\"pl-c1\">cd</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span\
    \ class=\"pl-pds\">\"</span></span>\nSCOPE_CONFIG=config.json npm run scope</pre></div>\n\
    <h2>\n<a id=\"user-content-deploy-a-cloud-based-instance\" class=\"anchor\" href=\"\
    #deploy-a-cloud-based-instance\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Deploy a Cloud-based Instance</h2>\n\
    <h3>\n<a id=\"user-content-amazon-web-services\" class=\"anchor\" href=\"#amazon-web-services\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Amazon Web Services</h3>\n<h4>\n<a id=\"user-content-public-ami\"\
    \ class=\"anchor\" href=\"#public-ami\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Public AMI</h4>\n<p>No ETA.</p>\n\
    <h4>\n<a id=\"user-content-source\" class=\"anchor\" href=\"#source\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Source</h4>\n\
    <p>To create a SCope AWS instance from scratch please read the tutorial <a href=\"\
    https://github.com/aertslab/SCope/tree/master/tutorials/aws-deployment-source\"\
    >aws-deployment-source</a>.</p>\n<h2>\n<a id=\"user-content-features\" class=\"\
    anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Features</h2>\n<h3>\n<a id=\"user-content-enabling-orcid-functionality\"\
    \ class=\"anchor\" href=\"#enabling-orcid-functionality\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Enabling\
    \ ORCID Functionality</h3>\n<p>To enable colaborative annotations and login via\
    \ ORCID ID, API credentials (<code>orcidAPIClientID</code>, <code>orcidAPIClientSecret</code>\
    \ and <code>orcidAPIRedirectURI</code>) must be added to the config file provided.\n\
    These can be generated at the <a href=\"https://orcid.org/developer-tools\" rel=\"\
    nofollow\">orcid developer tools page</a>.</p>\n<p>The <code>dataHashSecret</code>\
    \ entry in the config file should be filled in with a randomly generated string\
    \ for example from the python <a href=\"https://docs.python.org/3/library/secrets.html\"\
    \ rel=\"nofollow\">secrets package</a>.\nThis string will be used to salt all\
    \ annotation data, allowing validation of data generated on the instance of SCope.\
    \ Any changes in this string will invalidate all pre-existing annotations.</p>\n\
    <h2>\n<a id=\"user-content-development\" class=\"anchor\" href=\"#development\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Development</h2>\n<ol>\n<li>Clone the GitHub repository and install,</li>\n\
    </ol>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Define where you want to clone the SCope repository.</span>\n\
    LOCAL_SCOPE_REPO=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${HOME}</span>/repos/SCope<span class=\"pl-pds\">\"</span></span>\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Clone SCope git repository.</span>\n\
    git clone https://github.com/aertslab/SCope <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Go to your local cloned SCope repository.</span>\n<span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >${LOCAL_SCOPE_REPO}</span><span class=\"pl-pds\">\"</span></span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Install SCope.</span>\nnpm install</pre></div>\n\
    <ol start=\"2\">\n<li>Run,</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Go to your local cloned\
    \ SCope repository.</span>\n<span class=\"pl-c1\">cd</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${LOCAL_SCOPE_REPO}</span><span\
    \ class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Start SCope Server (terminal 1).</span>\n<span class=\"pl-c1\">cd</span>\
    \ opt\npoetry run hypercorn main:scope_api --reload\n\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> Start SCope Client (terminal 2).</span>\n<span class=\"\
    pl-c1\">cd</span> ..\nnpm run dev</pre></div>\n<h3>\n<a id=\"user-content-configuration-file-configjson\"\
    \ class=\"anchor\" href=\"#configuration-file-configjson\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Configuration\
    \ file (<code>config.json</code>)</h3>\n<p>Keys:</p>\n<ul>\n<li>\n<code>data</code>:\
    \ This is a directory containing data files (e.g. the <code>motd.txt</code> message\
    \ of the day).\nCan be an absolute path or a relative path from where you start\
    \ SCope. By default it is\n<code>./data/</code>.</li>\n</ul>\n<h3>\n<a id=\"user-content-deploying-scope-with-docker\"\
    \ class=\"anchor\" href=\"#deploying-scope-with-docker\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Deploying\
    \ SCope with Docker</h3>\n<p><code>docker-compose.yml</code> is configured to\
    \ spin up 2 containers: One to run the SCope backend and another to run an Apache\n\
    reverse proxy server.</p>\n<p>The SCope application will be available on port\
    \ <code>80</code> by default. You can specify a port by using env variable: <code>SCOPE_PORT</code>\n\
    before running the docker-compose command. Apache will proxy requests through\
    \ to the appropriate port inside the container.</p>\n<p>The <code>docker-compose.yml</code>\
    \ will serve the assets from inside the scope container, and the <code>docker-compose.host.yml</code>\
    \ will serve them from the host.\nThis supports as many use cases as possible,\
    \ because you can either build the assets on the host yourself using whatever\
    \ configuration you need,\nor serve them from the container if your environment\
    \ doesn't allow for that (e.g. you don't have npm installed on the host).</p>\n\
    <p>Before running the compose build, you can specify a SCOPE_PORT with: <code>docker-compose\
    \ build --build-arg SCOPE_PORT=8080</code></p>\n<p>The scope webpack assets will\
    \ have to be built with the config: <code>\"reverseProxyOn\": true</code>.\nYou\
    \ can use environment variable: <code>SCOPE_CONFIG=path to your config</code>\
    \ to specify a config file instead of changing the main one.</p>\n<p>You can configure\
    \ where the dockerised SCope data directories should be located\non the host machine\
    \ by using the env var <code>SCOPE_DATA_DIR</code> before launching the docker-compose.\n\
    The default location is <code>./scope_data</code> which will be created if you\
    \ do not specify one.</p>\n<p><strong>Note</strong>: in this config, you do not\
    \ need to specify the port in <code>publicHostAddress</code>. The env var <code>SCOPE_PORT</code>\
    \ gets appended for you.</p>\n<p>If deploying the container on a specific port\
    \ with another external apache reverse-proxy server,\nyou may have to add a config\
    \ to the external apache site config to allow http and websocket reverse-proxying.\n\
    Here is an example:</p>\n<pre><code>    ProxyPass / http://0.0.0.0:8080/\n   \
    \ RewriteEngine on\n    RewriteCond %{HTTP:Upgrade} websocket [NC]\n    RewriteCond\
    \ %{HTTP:Connection} upgrade [NC]\n    RewriteRule ^/?(.*) \"ws://0.0.0.0:8080/$1\"\
    \ [P,L]\n</code></pre>\n<h5>\n<a id=\"user-content-example-serve-from-container\"\
    \ class=\"anchor\" href=\"#example-serve-from-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Example\
    \ serve from container</h5>\n<ol>\n<li>Copy <code>config.json</code> to a new\
    \ file and modify with <code>\"reverseProxyOn\": true,</code> and <code>publicHostAddress</code>\
    \ set to your domain</li>\n<li><code>docker-compose build --build-arg SCOPE_PORT=8080</code></li>\n\
    <li><code>SCOPE_DATA_DIR=$HOME/scope_data SCOPE_PORT=8080 docker-compose up -d</code></li>\n\
    </ol>\n<h5>\n<a id=\"user-content-or-serve-from-host\" class=\"anchor\" href=\"\
    #or-serve-from-host\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>OR Serve from host</h5>\n<ol>\n<li><code>npm\
    \ run build</code></li>\n<li><code>SCOPE_DATA_DIR=$HOME/scope_data SCOPE_PORT=8080\
    \ docker-compose -f docker-compose.host.yml up -d</code></li>\n</ol>\n<p>You should\
    \ be able to visit <code>http://localhost:8080</code> and see the app!</p>\n"
  stargazers_count: 49
  subscribers_count: 8
  topics:
  - single-cell
  - large-scale-data-visualization
  - gene-expression
  - gene-regulatory-network
  - aws
  - cloud
  - loom
  - reactjs
  - grpc
  updated_at: 1621327198.0
aertslab/pySCENIC:
  data_format: 2
  description: pySCENIC is a lightning-fast python implementation of the SCENIC pipeline
    (Single-Cell rEgulatory Network Inference and Clustering) which enables biologists
    to infer transcription factors, gene regulatory networks and cell types from single-cell
    RNA-seq data.
  filenames:
  - Singularity.0.9.18
  full_name: aertslab/pySCENIC
  latest_release: 0.11.2
  readme: '<p><a href="https://singularity-hub.org/collections/702" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick start</h2>

    <pre><code># Download a (versioned) container

    singularity pull shub://MPIB/singularity-fsl:6.0.4


    # Run it

    singularity exec singularity-fsl_6.0.4.sif fslmaths

    singularity exec --nv singularity-fsl_6.0.4.sif eddy_cuda9.1

    </code></pre>

    <h2>

    <a id="user-content-fsl" class="anchor" href="#fsl" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>FSL</h2>

    <p>Project Home: <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/" rel="nofollow">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/</a></p>

    <p>These are containers primarily used at the MPI for Human Development.</p>

    <h2>

    <a id="user-content-cuda" class="anchor" href="#cuda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cuda</h2>

    <p>Starting with Singularity 6.0.2 we include Nvidia CUDA through Debian backports
    repositories.

    Make sure your Nvidia driver on the host <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility"
    rel="nofollow">supports it</a> and add the <code>--nv</code> flag with singularity.</p>

    <h2>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Note</h2>

    <p>Please be aware of FSL''s strict license regarding non-commercial use.</p>

    '
  stargazers_count: 151
  subscribers_count: 13
  topics:
  - single-cell
  - transcriptomics
  - gene-regulatory-network
  - transcription-factors
  updated_at: 1621994634.0
agaveplatform/SC17-container-tutorial:
  data_format: 2
  description: 'SC17 tutorial - "HPC via HTTP: Portable, Scalable Computing using
    App Containers and the Agave API"'
  filenames:
  - content/images/funwave-tvd/Singularity
  full_name: agaveplatform/SC17-container-tutorial
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hpc-via-http-portable-scalable-computing-using-app-containers-and-the-agave-api\"\
    \ class=\"anchor\" href=\"#hpc-via-http-portable-scalable-computing-using-app-containers-and-the-agave-api\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HPC via HTTP: Portable, Scalable Computing using App Containers and\
    \ the Agave API</h1>\n<p>Supercomputing matters. So does user experience. Standing\
    \ between the mainstream adoption of supercomputing and a new generation of users\
    \ is the reality that the entry cost to using these systems, both in terms of\
    \ dollars and in time spent learning the technology, has not significantly changed\
    \ in the last 20 years. The rise of cloud computing only complicates the learning\
    \ curve further. Over the last 6 years, the authors have been addressing this\
    \ gap through the development of a Science-as-a-Service platform enabling users\
    \ to go from their desktop, to their local data center, to the cloud, and back\
    \ without sacrificing their existing tool chain or user experience.</p>\n<p>In\
    \ this tutorial, we combine best practices and lessons learned while on-boarding\
    \ the last 70k new users to TACC\u2019s data center through the Agave Platform.\
    \ Participants will walk through the process of scaling their application from\
    \ a local environment to the Jetstream academic cloud and to a high performance\
    \ computing system at the Texas Advanced Computing Center. They will learn to\
    \ use multiple container technologies to harmonize app execution between cloud\
    \ and HPC resources, and they will learn to use modern APIs to orchestrate job\
    \ execution, capture provenance information, and foster collaboration.</p>\n<h1>\n\
    <a id=\"user-content-preview\" class=\"anchor\" href=\"#preview\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Preview</h1>\n\
    <p><a href=\"http://www.youtube.com/watch?v=hVnIrjn_aBI\" title=\"HPC via HTTP:\
    \ Portable, Scalable Computing using App Containers and the Agave API\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/c65f2550bc90ecabb4429c52335a19f58b324a579f4cb9a3f92dfcab4bc7391f/687474703a2f2f696d672e796f75747562652e636f6d2f76692f68566e49726a6e5f6142492f6d617872657364656661756c742e6a7067\"\
    \ alt=\"Intro Video\" data-canonical-src=\"http://img.youtube.com/vi/hVnIrjn_aBI/maxresdefault.jpg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-schedule\" class=\"\
    anchor\" href=\"#schedule\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Schedule</h1>\n<table>\n  <tr>\n    <th>Time</th>\n\
    \    <th>Presenterr</th>\n    <th>Topic</th>\n  </tr>\n  <tr>\n    <td>08:30 -\
    \ 08:45</td>\n    <td>John, Steve</td>\n    <td>[Introductions](01%20Introduction.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>08:45 - 09:05</td>\n    <td>Rion</td>\n    <td>[Agave\
    \ Overview](02%20Agave%20Overview.pdf)</td>\n  </tr>\n  <tr>\n    <td>09:05 -\
    \ 09:15</td>\n    <td>Kathy</td>\n    <td>[Jupyter, Sanbox, and Logging In](03%20Jupyter%2C%20Sandboxes%2C%20and%20Logging%20In.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>09:15 - 09:30</td>\n    <td>Steve</td>\n    <td>[Code,\
    \ Build, and Test](04%20Code%20Build%20and%20Test.ipynb)</td>\n  </tr>\n  <tr>\n\
    \    <td>09:30 - 10:00</td>\n    <td>Rion, John</td>\n    <td>[Hands on with Agave](05%20Hands%20on%20with%20Agave.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>10:00 - 10:30</td>\n    <td>--</td>\n    <td>Break</td>\n\
    \  </tr>\n  <tr>\n    <td>10:30 - 11:00</td>\n    <td>Steve,John</td>\n    <td>[Docker\
    \ and Singularity](06%20Docker%20and%Singularity.ipynb)</td>\n  </tr>\n  <tr>\n\
    \    <td>11:00 - 11:15</td>\n    <td>Rion</td>\n    <td>[Automation an Benchmarking](07%20Automation%20and%20Benchmarking.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>11:15 - 11:45</td>\n    <td>Kathy, Rion</td>\n    <td>[Packaging,\
    \ publishing, and Portability](08%20Packaging%20publishing%20and%20Portability.ipynb)</td>\n\
    \  </tr>\n  <tr>\n    <td>11:45 - 12:00</td>\n    <td>Steve, John</td>\n    <td>[Future\
    \ Directions and Homework)[09%20Future%20Directions%20and%20Homework.ipynb]</td>\n\
    \  </tr>\n</table>\n<h1>\n<a id=\"user-content-table-of-contents\" class=\"anchor\"\
    \ href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Table of Contents</h1>\n<ul>\n<li>01:\
    \ <a href=\"01-Requirements-and-Preparation.md\">Requirements and Preparation</a>\n\
    </li>\n<li>02: <a href=\"02-Installation-and-Infrastructure.md\">Installation\
    \ and Infrastructure</a>\n</li>\n<li>03: <a href=\"03-Auth-Notebooks-and-Web-Console.md\"\
    >Auth, Notebooks, and the Web Interface</a>\n</li>\n<li>04: <a href=\"04-SciOps-and-Sample-Application.md\"\
    >SciOps and our Sample Application</a>\n</li>\n<li>05: <a href=\"05-Code-Build-and-Run-Locally.md\"\
    >Code, Build, and Run Locally</a>\n</li>\n<li>06: <a href=\"06-Containerize-Existing-Applications.md\"\
    >Containerize Existing Applications</a>\n</li>\n<li>07: <a href=\"07-Automation-Registries-and-App-Catalogues\"\
    >Automation, Registries, and App Catalogues</a>\n</li>\n</ul>\n<ul>\n<li>Agave</li>\n\
    <li>CI/CD</li>\n<li>Image publishing</li>\n</ul>\n<ul>\n<li>08: <a href=\"\">Scaling\
    \ and Portability</a>\n</li>\n</ul>\n<ul>\n<li>Image caching</li>\n<li>Runtime\
    \ environments</li>\n<li>Data scheduling</li>\n<li>Reproducibility anti-patterns</li>\n\
    </ul>\n<ul>\n<li>09: <a href=\"\">Viewing simulation results, sharing, provenance</a>\n\
    </li>\n<li>10: <a href=\"\">Packaging and Publishing Experiments</a>\n</li>\n\
    <li>11: <a href=\"\">Benchmarking and Performance Considerations</a>\n</li>\n\
    <li>12: <a href=\"\">Functions, Microcodes, and Exascale</a>\n</li>\n<li>13: <a\
    \ href=\"\">Homework an Further Reading</a>\n</li>\n<li>90: <a href=\"90-Appendix-A.md\"\
    >Appendix A</a>\n</li>\n<li>99: <a href=\"99-References.md\">References</a>\n\
    </li>\n</ul>\n"
  stargazers_count: 3
  subscribers_count: 6
  topics: []
  updated_at: 1529379287.0
agladstein/SimPrily_update:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: agladstein/SimPrily_update
  latest_release: null
  readme: '<h1>

    <a id="user-content-simprily_update" class="anchor" href="#simprily_update" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SimPrily_update</h1>

    <p>Created by Ariella Gladstein, based on <a href="https://agladstein.github.io/SimPrily/index.html"
    rel="nofollow">SimPrily</a>.</p>

    <h2>

    <a id="user-content-about" class="anchor" href="#about" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>About</h2>

    <p>SimPrily runs genome simulations with user defined parameters or parameters
    randomly generated by priors and computes genomic statistics on the simulation
    output.</p>

    <ol>

    <li>Run genome simulation with model defined by prior distributions of parameters
    and demographic model structure.</li>

    <li>Take into account SNP array ascertainment bias by creating pseudo array based
    on priors of number of samples of discovery populations and allele frequency cut-off.</li>

    <li>Calculate genomic summary statistics on simulated genomes and pseudo arrays.</li>

    </ol>

    <p>This is ideal for use with Approximate Bayesian Computation on whole genome
    or SNP array data.</p>

    <p>Uses c++ programs macs and GERMLINE. For more information on these programs,
    see:<br>

    <a href="https://github.com/gchen98/macs">https://github.com/gchen98/macs</a><br>

    <a href="https://github.com/sgusev/GERMLINE">https://github.com/sgusev/GERMLINE</a></p>

    <h2>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h2>

    <p>cd to the directory you want to work in,</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/agladstein/SimPrily.git</pre></div>

    <h4>

    <a id="user-content-environment-set-up" class="anchor" href="#environment-set-up"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment
    Set up</h4>

    <p>If using Vagrant (this is recommended if running on non-Linux OS):</p>

    <p>Start Vagrant, ssh into Vagrant, cd to SimPrily directory:</p>

    <div class="highlight highlight-source-shell"><pre>vagrant up

    vagrant ssh

    <span class="pl-c1">cd</span> /vagrant</pre></div>

    <p>Install the virtual environment and install the requirements.</p>

    <div class="highlight highlight-source-shell"><pre>./setup/setup_env_vbox_2.7.sh</pre></div>

    <p>If not using Vagrant, just install the virtual environment and install the
    requirements:</p>

    <div class="highlight highlight-source-shell"><pre>./setup/setup_env_2.7.sh</pre></div>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>e.g. One Test simulation:</p>

    <pre><code>python simprily.py -p examples/eg1/param_file_eg1_asc.txt -m examples/eg1/model_file_eg1_asc.csv
    -g genetic_map_b37/genetic_map_GRCh37_chr1.txt.macshs -a array_template/ill_650_test.bed
    -i 1 -o output_dir -v

    </code></pre>

    <p>For quick help:</p>

    <pre><code>python simprily.py --help

    </code></pre>

    <h4>

    <a id="user-content-input" class="anchor" href="#input" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Input</h4>

    <p><code>simprily.py</code> takes 4 required arguments and 2 optional arguments,
    and help, verbose, and profile options.</p>

    <p>Run as</p>

    <pre><code>python simprily.py [-h] -p PARAM -m MODEL -i ID -o OUT [-g MAP] [-a
    ARRAY] [-v] [--profile]

    </code></pre>

    <h5>

    <a id="user-content-required" class="anchor" href="#required" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Required</h5>

    <p><code>-p PARAM</code> or <code>--param PARAM</code> = The location of the parameter
    file<br>

    <code>-m MODEL</code> or <code>--model MODEL</code> = The location of the model
    file<br>

    <code>-i ID</code> or <code>--id ID</code> = The unique identifier of the job<br>

    <code>-o OUT</code> or <code>--out OUT</code> = The location of the output directory</p>

    <h5>

    <a id="user-content-optional" class="anchor" href="#optional" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Optional</h5>

    <p><code>-h</code> or <code>--help</code> = shows a help message and exists<br>

    <code>-v</code> = increase output verbosity. This includes 3 levels, <code>-v</code>,
    <code>-vv</code>, and <code>-vvv</code><br>

    <code>--profile</code> = Print a log file containing the time in seconds and memory
    use in Mb for main functions<br>

    <code>-g MAP</code> or <code>--map MAP</code> = The location of the genetic map
    file<br>

    <code>-a ARRAY</code> or <code>--array ARRAY</code> = The location of the array
    template file, in bed form</p>

    <h4>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h4>

    <p>Three subdirectories are created in the directory specified in the <code>output_dir</code>
    argument.</p>

    <pre><code>output_dir/results

    output_dir/sim_data

    output_dir/germline_out

    </code></pre>

    <h5>

    <a id="user-content-intermediate-files" class="anchor" href="#intermediate-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Intermediate
    files</h5>

    <p>Intermediate files go to <code>output_dir/sim_data</code> and <code>output_dir/germline_out</code>.<br>

    <code>output_dir/sim_data</code> contains PLINK formated .ped and .map files created
    from the pseudo array, which are necessary to run GERMLINE.<br>

    <code>output_dir/germline_out</code> contains the GERMLINE .match output and .log.
    The .match contains all of the identified IBD segments.<br>

    These files are NOT automatically removed in python script, but are unnecessary
    once the job is complete.</p>

    <h5>

    <a id="user-content-results-files" class="anchor" href="#results-files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Results files</h5>

    <p>Output files go to <code>output_dir/results</code>.<br>

    <code>output_dir/results</code> contains the parameter values used in the simulation
    and the summary statistics calculated from the simulation.<br>

    The first line is a header with the parameter names and summary statistics names.

    The second line is the parameter values and summary statistics values.</p>

    <hr>

    <h2>

    <a id="user-content-abc_update_wfpy" class="anchor" href="#abc_update_wfpy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ABC_update_wf.py</h2>

    <p>This script creates all the necessary files for running ABC on simulations,
    and runs ABC.</p>

    <ol>

    <li>Combines the simulated results into one file in <code>obs{}/chr{}/ABC/results_combined.txt</code>
    (unless the file already exists).</li>

    <li>For chr1 randomly picks one of the simulations to use as observed data,

    and for all other chromosomes uses the parameter values of the observed data from
    chr1 to simulate observed data,

    and create file in <code>obs{}/chr{}/ABC/results_observed.txt</code>.</li>

    <li>Run R to get PLS components.</li>

    <li>Use ABCtoolbox to transform summary stats to PLS components for simulated
    and observed data.</li>

    <li>Use ABCtoolbox to get posteriors of parameters.</li>

    <li>Create parameter file with posterior file.</li>

    </ol>

    <h3>

    <a id="user-content-usage-1" class="anchor" href="#usage-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h3>

    <div class="highlight highlight-source-shell"><pre>ABC_update_wf.py path_sim param_file_name
    chrom obs</pre></div>

    <p>where,</p>

    <ul>

    <li>

    <code>path_sim</code> is the path to simulation output (before <code>obs{}</code>)</li>

    <li>

    <code>param_file_name</code> is the parameter file used to perform the simulations</li>

    <li>

    <code>chrom</code> is the chromosome number</li>

    <li>

    <code>obs</code> is the iteration with observed data.</li>

    </ul>

    <hr>

    <h2>

    <a id="user-content-hpc-workflow" class="anchor" href="#hpc-workflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPC Workflow</h2>

    <p>For chromosome 1 use <code>checkque.sh</code> to submit jobs to Ocelote.</p>

    <p>Arguments are <code>goal_number</code>, <code>max_que</code>, <code>chr</code></p>

    <div class="highlight highlight-source-shell"><pre>/home/u15/agladstein/SimPrily_update/update_test/checkque.sh
    10000 500 1</pre></div>

    <p>Then, run ABC with <code>ABC_update_wf.py</code> with the appropriate chromosome:</p>

    <div class="highlight highlight-source-shell"><pre>rsync -za SimPrily_update/
    /xdisk/agladstein/SimPrily_update

    <span class="pl-c1">cd</span> /xdisk/agladstein/SimPrily_update

    qsub update_test/PBS/run_ABC_chr1.pbs</pre></div>

    <p>Then, run the simulations with the appropriate chromosome:</p>

    <div class="highlight highlight-source-shell"><pre>rsync -za SimPrily_update/
    /xdisk/agladstein/SimPrily_update

    <span class="pl-c1">cd</span> /xdisk/agladstein/SimPrily_update

    qsub update_test/PBS/run_sims_update_chr2.pbs</pre></div>

    <hr>

    <h2>

    <a id="user-content-known-issues" class="anchor" href="#known-issues" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Known Issues</h2>

    <ul>

    <li>If exponential growth is large, macs simulation will not finish. (This is
    a macs bug).</li>

    <li>If the same id is used with the same output dir as a previous run, the .map
    file will be appended to.</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1615073384.0
alexlancaster/pypop:
  data_format: 2
  description: Python for Population Genomics
  filenames:
  - Singularity
  full_name: alexlancaster/pypop
  latest_release: null
  readme: "<h2>\n<a id=\"user-content-python-for-population-genomics-pypop\" class=\"\
    anchor\" href=\"#python-for-population-genomics-pypop\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Python for Population\
    \ Genomics (PyPop)</h2>\n<p>PyPop is a framework for processing genotype and allele\
    \ data and running population genetic analyses.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<h3>\n<a id=\"\
    user-content-1-install-os-specific-development-environment\" class=\"anchor\"\
    \ href=\"#1-install-os-specific-development-environment\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1. Install\
    \ OS-specific development environment</h3>\n<h4>\n<a id=\"user-content-macos-x\"\
    \ class=\"anchor\" href=\"#macos-x\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>MacOS X</h4>\n<ol>\n<li>install\
    \ developer command-line tools: <a href=\"https://developer.apple.com/downloads/\"\
    \ rel=\"nofollow\">https://developer.apple.com/downloads/</a>  (includes <code>git</code>,\
    \ <code>gcc</code>)</li>\n<li>Visit <a href=\"http://macports.org\" rel=\"nofollow\"\
    >http://macports.org</a> and follow the instructions there to install the latest\
    \ version of MacPorts for your version of MacOS X.</li>\n<li>Set environment variables\
    \ to use macports version of Python and other packages, packages add the following\
    \ to <code>~/.bash_profile</code>\n</li>\n</ol>\n<pre><code>export PATH=/opt/local/bin:$PATH\n\
    export LIBRARY_PATH=/opt/local/lib/:$LIBRARY_PATH\nexport CPATH=/opt/local/include:$CPATH\n\
    </code></pre>\n<ol start=\"4\">\n<li>Rerun your bash shell login in order to make\
    \ these new exports active in your environment.  At the command line type:</li>\n\
    </ol>\n<pre><code>exec bash -login\n</code></pre>\n<h3>\n<a id=\"user-content-2-clone-the-repository\"\
    \ class=\"anchor\" href=\"#2-clone-the-repository\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Clone the\
    \ repository:</h3>\n<pre><code>git clone https://github.com/alexlancaster/pypop.git\n\
    </code></pre>\n<h3>\n<a id=\"user-content-3-install-dependencies\" class=\"anchor\"\
    \ href=\"#3-install-dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>3. Install dependencies</h3>\n<h4>\n\
    <a id=\"user-content-macos\" class=\"anchor\" href=\"#macos\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>MacOS:</h4>\n\
    <p>Install the MacPorts packages</p>\n<pre><code>  sudo port install swig-python\
    \ gsl py27-numpy py-libxml2 py27-libxslt py-setuptools py27-pip\n</code></pre>\n\
    <p>Set MacPorts to use the just-installed 2.7 MacPorts version of Python and pip:</p>\n\
    <pre><code>  sudo port select --set python python27\n  sudo port select --set\
    \ pip pip27\n</code></pre>\n<p>Check that the MacPorts version of Python is active\
    \ by typing: <code>which python</code>, if it is working correctly you should\
    \ see the following:</p>\n<pre><code>/opt/local/bin/python\n</code></pre>\n<h4>\n\
    <a id=\"user-content-linux-fedoracentosrhel\" class=\"anchor\" href=\"#linux-fedoracentosrhel\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Linux (Fedora/Centos/RHEL):</h4>\n<p>Need at least Fedora 25 for the\
    \ appropriate dependencies:</p>\n<pre><code>  sudo dnf install swig gsl-devel\
    \ python2-numpy python-libxml2 libxslt-python python-setuptools python-pip\n</code></pre>\n\
    <p>See <a href=\"DEV_NOTES.md\">DEV_NOTES.md</a> for instructions on containerizing\
    \ the install on a Centos/RHEL release.</p>\n<h4>\n<a id=\"user-content-linux-ubuntu\"\
    \ class=\"anchor\" href=\"#linux-ubuntu\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linux (Ubuntu)</h4>\n<p>Install\
    \ the following packages</p>\n<pre><code>  sudo apt install git libgsl-dev python-numpy\
    \ python-libxml2 python-libxslt1 python-setuptools python-pip\n</code></pre>\n\
    <p>The <code>swig</code> package in recent Ubuntu releases has bugs, you will\
    \ need to compile the most recent from source, see also <a href=\"DEV_NOTES.md\"\
    >DEV_NOTES.md</a> for details.</p>\n<h3>\n<a id=\"user-content-4-build\" class=\"\
    anchor\" href=\"#4-build\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>4. Build</h3>\n<pre><code>./setup.py build\n\
    </code></pre>\n<h2>\n<a id=\"user-content-examples\" class=\"anchor\" href=\"\
    #examples\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Examples</h2>\n<p>These are examples of how to use PyPop. Specify\
    \ the <code>--help</code> option to see an\nexplanation of the options available.</p>\n\
    <h3>\n<a id=\"user-content-run-a-minimal-dataset\" class=\"anchor\" href=\"#run-a-minimal-dataset\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run a minimal dataset:</h3>\n<pre><code>./bin/pypop.py -c  tests/data/minimal.ini\
    \ tests/data/USAFEL-UchiTelle-small.pop\n</code></pre>\n<p>This will generate\
    \ the following two files, an XML output file and a plain text version:</p>\n\
    <pre><code>USAFEL-UchiTelle-small-out.xml\nUSAFEL-UchiTelle-small-out.txt\n</code></pre>\n\
    <h2>\n<a id=\"user-content-running-test-suite\" class=\"anchor\" href=\"#running-test-suite\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running test suite</h2>\n<pre><code>  ./setup.py test\n</code></pre>\n\
    <p>If you run into errors, file a bug (as per Support, below), include the output\
    \ of <code>py.test</code> run in verbose mode and capturing the output</p>\n<pre><code>\
    \  py.test -s -v\n</code></pre>\n<p>(See DEV_NOTES.md for more details on installing\
    \ or running <code>py.test</code> outside the context of setuptools.)</p>\n<h2>\n\
    <a id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Support</h2>\n\
    <p>Please submit bug reports and feature requests</p>\n<pre><code>https://github.com/alexlancaster/pypop/issues\n\
    </code></pre>\n<h3>\n<a id=\"user-content-bug-reporting\" class=\"anchor\" href=\"\
    #bug-reporting\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Bug reporting</h3>\n<p>When reporting bugs, especially\
    \ during installation, please run the following and include the output:</p>\n\
    <pre><code>echo $CPATH\necho $LIBRARY_PATH\necho $PATH\nwhich python\n</code></pre>\n\
    <p>If you are running on MacOS please also run and include the output of:</p>\n\
    <pre><code>port installed\n</code></pre>\n<h2>\n<a id=\"user-content-development\"\
    \ class=\"anchor\" href=\"#development\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development</h2>\n<p>The code\
    \ for PyPop is at</p>\n<pre><code>https://github.com/alexlancaster/pypop\n</code></pre>\n\
    <h2>\n<a id=\"user-content-copyright-and-license\" class=\"anchor\" href=\"#copyright-and-license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Copyright and License</h2>\n<p>PyPop is Copyright (C) 2003-2015. The\
    \ Regents of the University of California (Regents)</p>\n<p>PyPop is distributed\
    \ under the terms of GPLv2</p>\n"
  stargazers_count: 11
  subscribers_count: 10
  topics:
  - population-genomics
  - evolutionary-biology
  - bioinformatics
  - open-source
  - free-software
  updated_at: 1615988636.0
andquintero/singularity_builds:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: andquintero/singularity_builds
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_builds" class="anchor" href="#singularity_builds"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_builds</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1554218133.0
andyrevell/docker_GitHub:
  data_format: 2
  description: null
  filenames:
  - imaging/nipy/Singularity
  full_name: andyrevell/docker_GitHub
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CNTdocker</h1>\n<h2>\n<a id=\"user-content-about\" class=\"anchor\"\
    \ href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>About</h2>\n<p>Dockerfiles to create Docker images\
    \ used by the CNT at the university of Pennsylvania</p>\n<h2>\n<a id=\"user-content-directory-contents-explanation\"\
    \ class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directory\
    \ contents explanation</h2>\n<h3>\n<a id=\"user-content-eeg\" class=\"anchor\"\
    \ href=\"#eeg\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>EEG</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common EEG analysis tools. Usually python 3</p>\n<p><strong>echobase</strong>:\
    \ Dockerfiles used to create images that can calculate functional connectivity\
    \ of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase\
    \ code is from <a href=\"https://github.com/andyrevell/paper001\">https://github.com/andyrevell/paper001</a></p>\n\
    <pre><code>Ubuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\n\
    scipy 1.4.1\n</code></pre>\n<h3>\n<a id=\"user-content-imaging\" class=\"anchor\"\
    \ href=\"#imaging\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>Imaging</strong>:</h3>\n<p>Dockerfiles used\
    \ to create images with common MRI analysis tools.</p>\n<pre><code>  Ubuntu 18.04\n\
    \  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n\
    \  FSL 6.0.1\n</code></pre>\n<h3>\n<a id=\"user-content-ml\" class=\"anchor\"\
    \ href=\"#ml\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>ml</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common machine learning tools.</p>\n<p><strong>wavenet</strong>:\
    \ Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet\
    \ paper\n<a href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\"\
    \ rel=\"nofollow\">Wavenet blog</a>\n<a href=\"https://arxiv.org/pdf/1609.03499.pdf\"\
    \ rel=\"nofollow\">Wavenet paper</a></p>\n<pre><code>  Ubuntu 18.04\n  tensorflow\
    \ 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n</code></pre>\n<p><strong>Tensorflow_2.1</strong>:\
    \ Dockerfile to create compatible dependencies to with tensorflow 2.1</p>\n<pre><code>\
    \  Ubuntu 18.04\n  tensorflow 2.1\n</code></pre>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1600370006.0
angelettilab/scMouseBcellFlu:
  data_format: 2
  description: null
  filenames:
  - envs/sauron/Singularity_remote.def
  full_name: angelettilab/scMouseBcellFlu
  latest_release: null
  readme: '<h1>

    <a id="user-content-scmousebcellflu" class="anchor" href="#scmousebcellflu" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>scMouseBcellFlu</h1>

    <p>This is the repository associated with the publication <em>Single cell BCR
    and RNA analysis after respiratory virus infection reveals spatiotemporal dynamics
    of antigen specific B cell response</em>.</p>

    <p>This repository contains the code and supporting files necessary to reproduce
    the analyses reported in the publication. In essence, the anlaysis workflow herein
    was done using the <a href="https://github.com/NBISweden/sauron">Sauron</a> analysis
    philosophy.</p>

    <h2>

    <a id="user-content-single-cell-rna-seq-analysis" class="anchor" href="#single-cell-rna-seq-analysis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Single-cell
    RNA-seq analysis</h2>

    <p>To re-run the analysis of the scRNA-Seq data, the necessary workflow and scripts
    can be found in the <a href="scripts/scRNAseq_pipeline"><code>scripts/scRNAseq_pipeline/</code></a>
    subdirectory.</p>

    <h2>

    <a id="user-content-bcr-seq-analysis" class="anchor" href="#bcr-seq-analysis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BCR
    seq analysis</h2>

    <p>Scripts necessary to reproduce the analysis of the B-cell receptor sequencing
    data can be found in the <a href="scripts/VDJ_analysis"><code>scripts/VDJ_analysis/</code></a>
    subdirectory.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1619609010.0
arcsUVA/R:
  data_format: 2
  description: R containers
  filenames:
  - Singularity.3.6.0
  full_name: arcsUVA/R
  latest_release: null
  readme: '<h1>

    <a id="user-content-r" class="anchor" href="#r" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>R</h1>

    <p>R containers</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1573410996.0
arcsUVA/caffe2:
  data_format: 2
  description: null
  filenames:
  - Singularity.0.8.0
  full_name: arcsUVA/caffe2
  latest_release: null
  readme: '<h1>

    <a id="user-content-caffe2" class="anchor" href="#caffe2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>caffe2</h1>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1550983020.0
arcsUVA/cellprofiler:
  data_format: 2
  description: singularity scripts for cellprofiler
  filenames:
  - Singularity.3.1.8
  - Singularity.3.0.0
  - Singularity.2.2.0
  full_name: arcsUVA/cellprofiler
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2270" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Stacks software pipeline for building loci
    from short-read sequences</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1556734065.0
arcsUVA/cryoCARE:
  data_format: 2
  description: null
  filenames:
  - Singularity.0.1.0
  full_name: arcsUVA/cryoCARE
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-images" class="anchor" href="#singularity-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-images</h1>

    <p>Setups for various images used on the dgx.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1574198978.0
arcsUVA/omero-client:
  data_format: 2
  description: Omero client Singularity recipes.
  filenames:
  - Singularity.5.4.10
  - Singularity.5.4.0
  full_name: arcsUVA/omero-client
  latest_release: null
  readme: '<h1>

    <a id="user-content-omero-client" class="anchor" href="#omero-client" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>omero-client</h1>

    <p><a href="https://singularity-hub.org/collections/2227" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a><br>

    Omero client Singularity recipes</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1557760203.0
arcsUVA/patric:
  data_format: 2
  description: null
  filenames:
  - Singularity.1.026
  full_name: arcsUVA/patric
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CNTdocker</h1>\n<h2>\n<a id=\"user-content-about\" class=\"anchor\"\
    \ href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>About</h2>\n<p>Dockerfiles to create Docker images\
    \ used by the CNT at the university of Pennsylvania</p>\n<h2>\n<a id=\"user-content-directory-contents-explanation\"\
    \ class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directory\
    \ contents explanation</h2>\n<h3>\n<a id=\"user-content-eeg\" class=\"anchor\"\
    \ href=\"#eeg\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>EEG</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common EEG analysis tools. Usually python 3</p>\n<p><strong>echobase</strong>:\
    \ Dockerfiles used to create images that can calculate functional connectivity\
    \ of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase\
    \ code is from <a href=\"https://github.com/andyrevell/paper001\">https://github.com/andyrevell/paper001</a></p>\n\
    <pre><code>Ubuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\n\
    scipy 1.4.1\n</code></pre>\n<h3>\n<a id=\"user-content-imaging\" class=\"anchor\"\
    \ href=\"#imaging\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>Imaging</strong>:</h3>\n<p>Dockerfiles used\
    \ to create images with common MRI analysis tools.</p>\n<pre><code>  Ubuntu 18.04\n\
    \  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n\
    \  FSL 6.0.1\n</code></pre>\n<h3>\n<a id=\"user-content-ml\" class=\"anchor\"\
    \ href=\"#ml\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>ml</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common machine learning tools.</p>\n<p><strong>wavenet</strong>:\
    \ Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet\
    \ paper\n<a href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\"\
    \ rel=\"nofollow\">Wavenet blog</a>\n<a href=\"https://arxiv.org/pdf/1609.03499.pdf\"\
    \ rel=\"nofollow\">Wavenet paper</a></p>\n<pre><code>  Ubuntu 18.04\n  tensorflow\
    \ 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n</code></pre>\n<p><strong>Tensorflow_2.1</strong>:\
    \ Dockerfile to create compatible dependencies to with tensorflow 2.1</p>\n<pre><code>\
    \  Ubuntu 18.04\n  tensorflow 2.1\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1570548606.0
arcsUVA/pytorch:
  data_format: 2
  description: null
  filenames:
  - Singularity.1.0.0-py36
  - Singularity.1.3.1-py36
  full_name: arcsUVA/pytorch
  latest_release: null
  readme: '<h1>

    <a id="user-content-pytorch" class="anchor" href="#pytorch" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pytorch</h1>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1573410610.0
arcsUVA/supernova:
  data_format: 2
  description: Singularity container script for 10x Genomics SuperNova software
  filenames:
  - Singularity.2.0.0
  full_name: arcsUVA/supernova
  latest_release: null
  readme: '<h1>

    <a id="user-content-supernova" class="anchor" href="#supernova" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>supernova</h1>

    <p>Singularity container script for 10x Genomics SuperNova software</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1551891095.0
arcsUVA/tensorflow:
  data_format: 2
  description: TensorFlow Singularity recipes.
  filenames:
  - Singularity.1.14.0-py36
  - Singularity.1.13.1-py36
  - Singularity.1.13.0-py36
  - Singularity.1.6.0-py36
  - Singularity.1.12.0-py36
  - Singularity.1.6.0-py27
  - Singularity.1.12.0-py27
  full_name: arcsUVA/tensorflow
  latest_release: null
  readme: '<h1>

    <a id="user-content-tensorflow" class="anchor" href="#tensorflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>tensorflow</h1>

    <p><a href="https://singularity-hub.org/collections/2235" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a><br>

    TensorFlow Singularity recipes.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1567631554.0
arcsUVA/theano:
  data_format: 2
  description: Theano Singularity container scripts
  filenames:
  - Singularity.1.0.4-py36
  full_name: arcsUVA/theano
  latest_release: null
  readme: '<h1>

    <a id="user-content-theano" class="anchor" href="#theano" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>theano</h1>

    <p>Theano Singularity container scripts</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1554499739.0
arezaii/pf_singularity_demo:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: arezaii/pf_singularity_demo
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-parflow-singularity-container-demonstration\"\
    \ class=\"anchor\" href=\"#parflow-singularity-container-demonstration\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>ParFlow\
    \ Singularity Container Demonstration</h1>\n<p>The Singularity container is built\
    \ with ParFlow installed as a SCIF-app, providing access to both sequential and\
    \ parallel\nbuilds of ParFlow. See additional information about <a href=\"https://sylabs.io/guides/3.3/user-guide/definition_files.html?highlight=apps#apps\"\
    \ rel=\"nofollow\">Apps in Singularity</a></p>\n<h2>\n<a id=\"user-content-prerequisites\"\
    \ class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Prerequisites</h2>\n<ul>\n<li>Host\
    \ OS must have Singularity installed (See <a href=\"https://sylabs.io/guides/3.3/user-guide/installation.html\"\
    \ rel=\"nofollow\">Installing Singularity</a>)</li>\n</ul>\n<h2>\n<a id=\"user-content-linux-hosts\"\
    \ class=\"anchor\" href=\"#linux-hosts\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linux Hosts</h2>\n<p>Verify Singularity\
    \ is installed with the command:</p>\n<pre><code>singularity --version\n</code></pre>\n\
    <p>Then, see the Quickstart directions below</p>\n<h2>\n<a id=\"user-content-windowsmac-hosts\"\
    \ class=\"anchor\" href=\"#windowsmac-hosts\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Windows/Mac Hosts</h2>\n<p>Follow\
    \ the instructions to <a href=\"https://sylabs.io/guides/3.3/user-guide/installation.html#install-on-windows-or-mac\"\
    \ rel=\"nofollow\">install Singularity</a></p>\n<p>Make sure you are ssh'd into\
    \ the Vagrant box before beginning the Quickstart steps below</p>\n<pre><code>vagrant\
    \ ssh\nvagrant@vagrant:~$ singularity --version\n</code></pre>\n<h2>\n<a id=\"\
    user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Quickstart</h2>\n\
    <p>Steps:</p>\n<ol>\n<li>Clone this repository</li>\n</ol>\n<pre><code>git clone\
    \ https://github.com/arezaii/pf_singularity_demo\n</code></pre>\n<ol start=\"\
    2\">\n<li>cd to the repository directory</li>\n</ol>\n<pre><code>cd pf_singularity_demo\n\
    </code></pre>\n<ol start=\"3\">\n<li>run the shell script to execute tests for\
    \ Little Washita domain on 1 processor, for 1 timestep</li>\n</ol>\n<pre><code>./run_test.sh\
    \ LW 1 1 1 1\n</code></pre>\n<h2>\n<a id=\"user-content-running-performance-test-cases\"\
    \ class=\"anchor\" href=\"#running-performance-test-cases\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ Performance Test Cases</h2>\n<p>The shell script run_test.sh facilitates running\
    \ tests on different domains.</p>\n<p>Usage:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ ./run_test.sh <span class=\"pl-k\">&lt;</span>domain<span class=\"pl-k\"\
    >&gt;</span> <span class=\"pl-k\">&lt;</span>P<span class=\"pl-k\">&gt;</span>\
    \ <span class=\"pl-k\">&lt;</span>Q<span class=\"pl-k\">&gt;</span> <span class=\"\
    pl-k\">&lt;</span>R<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>TimeSteps<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<p>where</p>\n<ul>\n<li>domain is a\
    \ test domain defined below</li>\n<li>P, Q, R are integers defining processor\
    \ topology in X, Y, Z directions</li>\n<li>Timesteps is number of timesteps to\
    \ execute</li>\n</ul>\n<h2>\n<a id=\"user-content-test-domains\" class=\"anchor\"\
    \ href=\"#test-domains\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Test Domains</h2>\n<p>There are several test\
    \ domains for performance analysis contained in the perf_tests folder.</p>\n<ul>\n\
    <li>LW - Little Washita</li>\n<li>clayl - ClayL</li>\n<li>conus_ru - CONUS Clip\
    \ - Run off</li>\n<li>conus_tfg - CONUS Clip - Terrain Following Grid</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-little-washita\" class=\"anchor\" href=\"#little-washita\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Little Washita</h3>\n<p>Natural model of the Little Washita watershed\
    \ in Oklahoma.</p>\n<p><em><strong>Domain Details</strong></em></p>\n<ul>\n<li>Number\
    \ of Cells: 84,050, 41x41x50 (X,Y,Z)</li>\n<li>Horizontal Resolution: 1km</li>\n\
    <li>Vertical Resolution: 2m</li>\n</ul>\n<p><em><strong>Technical Details</strong></em></p>\n\
    <ul>\n<li>CLM enabled with NLDAS Forcings</li>\n<li>Timestep: 1hr</li>\n<li>Suburface:\
    \ Heterogeneous</li>\n<li>Initial Condition: Pressure file from spin-up</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-clayl\" class=\"anchor\" href=\"#clayl\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>ClayL</h3>\n\
    <p>Synthetic model with completely flat surface and many thin, vertical layers</p>\n\
    <p><em><strong>Domain Details</strong></em></p>\n<ul>\n<li>Number of Cells: 2.4M\
    \ for 1 core. Scales with processor count, 100Px100Qx240 (X,Y,Z)</li>\n<li>Horizontal\
    \ Resolution: 1m</li>\n<li>Vertical Resolution: 0.025m</li>\n</ul>\n<p><em><strong>Technical\
    \ Details</strong></em></p>\n<ul>\n<li>No CLM, constant simulated rain on top\
    \ surface @ .0008 mm/hr</li>\n<li>Timestep 1hr</li>\n<li>Subsurface: Homogeneous</li>\n\
    <li>Initial Condition: Dry</li>\n</ul>\n<h3>\n<a id=\"user-content-conus-run-off\"\
    \ class=\"anchor\" href=\"#conus-run-off\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CONUS Run-off</h3>\n<p>Natural\
    \ topography with an impervious surface (parking lot simulation)</p>\n<p><em><strong>Domain\
    \ Details</strong></em></p>\n<ul>\n<li>Number of Cells: 1,562,500 1250x1250x1\
    \ (X,Y,Z)</li>\n<li>Horizontal Resolution: 1km</li>\n<li>Vertical Resolution:\
    \ 0.10m</li>\n</ul>\n<p><em><strong>Technical Details</strong></em></p>\n<ul>\n\
    <li>No CLM, period of 1 hour simulated rain on top surface @ .005 mm/hr, then\
    \ recession for 1000 hours</li>\n<li>Timestep: 6 minutes</li>\n<li>Subsurface:\
    \ Homogeneous</li>\n<li>Initial Condition: Dry</li>\n</ul>\n<h3>\n<a id=\"user-content-conus-terrain-following-grid\"\
    \ class=\"anchor\" href=\"#conus-terrain-following-grid\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CONUS Terrain\
    \ Following Grid</h3>\n<p>Natural topography with the terrain following grid (TFG)\
    \ feature enabled</p>\n<p><em><strong>Domain Details</strong></em></p>\n<ul>\n\
    <li>Number of Cells: 1,125,000 750x750x2 (X,Y,Z)</li>\n<li>Horizontal Resolution:\
    \ 1km</li>\n<li>Vertical Resolution: toplayer=1m, bottomlayer=100m</li>\n</ul>\n\
    <p><em><strong>Technical Details</strong></em></p>\n<ul>\n<li>No CLM, seepage\
    \ face boundary condition type on top layer, @ 0.00001</li>\n<li>Timestep: 100000</li>\n\
    <li>Subsurface: Homogeneous</li>\n<li>Initial Condition: Water Table at 45m above\
    \ lower boundary</li>\n</ul>\n<h2>\n<a id=\"user-content-about-apps\" class=\"\
    anchor\" href=\"#about-apps\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>About Apps</h2>\n<p>The demo container\
    \ has two apps installed:</p>\n<ul>\n<li>par = distributed build of ParFlow, -DPARFLOW_AMPS_SEQUENTIAL_IO=False</li>\n\
    <li>seq = sequential build of ParFlow, -DPARFLOW_AMPS_SEQUENTIAL_IO=True</li>\n\
    </ul>\n<p>to run:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$\
    \ singularity run --app <span class=\"pl-k\">&lt;</span>app_name<span class=\"\
    pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>/path/to/singularity_container.sif<span\
    \ class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>.tcl input file<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<p>See additional information about\
    \ <a href=\"https://sylabs.io/guides/3.3/user-guide/definition_files.html?highlight=apps#apps\"\
    \ rel=\"nofollow\">Apps in Singularity</a></p>\n<h2>\n<a id=\"user-content-to-build-container\"\
    \ class=\"anchor\" href=\"#to-build-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To Build Container</h2>\n<p>The\
    \ quickest way to build is to use a remote build service such as <a href=\"https://cloud.sylabs.io/builder\"\
    \ rel=\"nofollow\">cloud.sylabs.io</a>\nIf a user has root access, they can build\
    \ from the definition file, conventionally named Singularity.</p>\n<p>General\
    \ build command is of the form:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ sudo singularity build <span class=\"pl-k\">&lt;</span>destination/path/to/singularity_container.sif<span\
    \ class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>Singularity definition\
    \ file<span class=\"pl-k\">&gt;</span></pre></div>\n<p>as a specific example:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ sudo singularity build\
    \ <span class=\"pl-k\">~</span>/pf_singularity_demo.sif Singularity</pre></div>\n\
    <h2>\n<a id=\"user-content-to-use-parflow-in-container\" class=\"anchor\" href=\"\
    #to-use-parflow-in-container\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>To Use ParFlow in Container</h2>\n\
    <p>Example of running the LW test case in <code>parflow/test/washita/tcl_scripts</code>\
    \ directory</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ singularity\
    \ run --app par <span class=\"pl-k\">~</span>/pf_singularity_demo.sif LW_Test.tcl</pre></div>\n\
    <h2>\n<a id=\"user-content-pull-from-sylabs-cloud\" class=\"anchor\" href=\"#pull-from-sylabs-cloud\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pull from Sylabs Cloud</h2>\n<p>To pull the pre-built image from Sylabs\
    \ Cloud:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ singularity\
    \ pull [destination image name] library://arezaii/default/parflow_demo:master</pre></div>\n\
    <h2>\n<a id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Testing</h2>\n\
    <p>Because singularity containers are write protected and ParFlow tests write\
    \ to disk, you must expand the image to a writable sandbox.\nThis requires super\
    \ user access, similar to building a container from the definition file.</p>\n\
    <h3>\n<a id=\"user-content-make-container-writable\" class=\"anchor\" href=\"\
    #make-container-writable\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Make Container Writable</h3>\n<p>First, create\
    \ a writable sandbox from the immutable container using Singularity's build command:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>sudo singularity build --sandbox\
    \ <span class=\"pl-k\">&lt;</span>directory_to_create_for_sandbox/<span class=\"\
    pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>singularity_container<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<p>as an example, if you had pulled\
    \ the parflow_ompi image from shub:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity build --sandbox parflow_demo_master_sandbox/ parflow_demo_master.sif</pre></div>\n\
    <p>There will now be a new directory parflow_demo_master_sandbox/ that is the\
    \ root of the container.\nEditing any of the folder contents will require super\
    \ user permissions.</p>\n<p>You can enter a console into the container now by\
    \ using the Singularity shell command:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity shell --writable <span class=\"pl-k\">&lt;</span>directory_to_create_for_sandbox/<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<h3>\n<a id=\"user-content-run-tests\"\
    \ class=\"anchor\" href=\"#run-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run Tests</h3>\n<p>After making\
    \ the container writable and accessing it through a shell, both documented above,\
    \ running the ParFlow\ntests can be done by changing directories and exporting\
    \ the PARFLOW_DIR environment variable for either distributed\nor sequential builds\
    \ of ParFlow.</p>\n<p>Take note of the ParFlow build and install directories in\
    \ the container:</p>\n<p><strong>Sequential Build</strong></p>\n<ul>\n<li>build\
    \ directory: /home/parflow/build_seq</li>\n<li>install directory: /home/parflow/pfdir_seq</li>\n\
    </ul>\n<p><strong>Distributed Build</strong></p>\n<ul>\n<li>build directory: /home/parflow/build_par</li>\n\
    <li>install directory: /home/parflow/pfdir_par</li>\n</ul>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-k\">&gt;</span> <span class=\"\
    pl-c1\">cd</span> /home/parflow/<span class=\"pl-k\">&lt;</span>build_dir<span\
    \ class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;</span> <span class=\"\
    pl-k\">export</span> PARFLOW_DIR=/home/parflow/<span class=\"pl-k\">&lt;</span>install_dir<span\
    \ class=\"pl-k\">&gt;</span> \n<span class=\"pl-k\">&gt;</span> make <span class=\"\
    pl-c1\">test</span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583512107.0
arzwa/wgd:
  data_format: 2
  description: Python package and CLI for whole-genome duplication related analyses
  filenames:
  - Singularity
  full_name: arzwa/wgd
  latest_release: v1.1.1
  readme: "<p><a href=\"http://wgd.readthedocs.io/en/latest/?badge=latest\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/482fadaf050db308e22a1c37fb38a8d39be0f5a0d3b645ed48b114c86e55497e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7767642f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/wgd/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/2097\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9b0b8670bab3cab652cf5c31fdae614cf89b2ceb2e013cd2d7dd570e9f8530f2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d73696e67756c61726974792d2d6875622d626c75652e737667\"\
    \ alt=\"Hosted\" data-canonical-src=\"https://img.shields.io/badge/hosted-singularity--hub-blue.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Copyright (C) 2018 Arthur Zwaenepoel</p>\n\
    <p>VIB/UGent center for plant systems biology -\nBioinformatics &amp; evolutionary\
    \ genomics group <a href=\"https://www.vandepeerlab.org/\" rel=\"nofollow\">https://www.vandepeerlab.org/</a></p>\n\
    <h1>\n<a id=\"user-content-wgd---simple-command-line-tools-for-the-analysis-of-ancient-whole-genome-duplications\"\
    \ class=\"anchor\" href=\"#wgd---simple-command-line-tools-for-the-analysis-of-ancient-whole-genome-duplications\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>wgd - simple command line tools for the analysis of ancient whole-genome\
    \ duplications</h1>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h2>\n<p>Python package and command\
    \ line interface (CLI) for the analysis of\nwhole-genome duplications (WGDs).\
    \ Tested with Python3 on Linux. If you don't have\npython or pip installed a simple\
    \ <code>sudo apt-get install python3-pip</code> should do.</p>\n<p>To install,\
    \ simply run</p>\n<pre><code>git clone https://github.com/arzwa/wgd.git\ncd wgd\n\
    pip install --user .\n</code></pre>\n<p>Note that depending on your python installation\
    \ and whether you're in a\nvirtualenv, <code>pip</code> may default either to\
    \ <code>pip2</code> or <code>pip3</code>. If the\nabove installation step fails,\
    \ please try to use <code>pip3</code> instead of\n<code>pip</code>.</p>\n<p>For\
    \ the command line interface, upon installation run</p>\n<pre><code>$ wgd\n</code></pre>\n\
    <p>to get a list of the available commands. To get usage instructions for\na command\
    \ (e.g. <code>ksd</code>) run</p>\n<pre><code>$ wgd ksd --help\n</code></pre>\n\
    <p>For <strong>external software</strong> requirements: please consult the relevant\
    \ section\nin the <a href=\"https://wgd.readthedocs.io/en/latest/index.html#external-software\"\
    \ rel=\"nofollow\">docs</a></p>\n<p><strong>Note:</strong> if you encounter issues,\
    \ do verify you have the latest\n<a href=\"http://abacus.gene.ucl.ac.uk/software/#phylogenetic-analysis-by-maximum-likelihood-paml\"\
    \ rel=\"nofollow\">PAML</a> version.\nTo install the latest version, you best\
    \ not rely on <code>apt-get</code> or any other\npackage manager but install from\
    \ source. Something like this should work\n(from within the directory where you\
    \ want to install paml)</p>\n<pre><code>wget http://abacus.gene.ucl.ac.uk/software/paml4.9j.tgz\n\
    tar -xzf paml4.9j.tgz\npushd paml4.9j/src &amp;&amp; make -f Makefile &amp;&amp;\
    \ popd \nexport PATH=$PATH:$PWD/paml4.9j/src/\n</code></pre>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick start</h2>\n<p>The main\
    \ aim of <code>wgd</code> is computing whole-paranome and one-vs.-one ortholog\
    \ Ks\ndistributions. For a whole-paranome distribution of a CDS sequence fasta\
    \ file,\nthe minimal commands are:</p>\n<pre><code>$ wgd dmd ath.cds.fasta\n$\
    \ wgd ksd wgd_dmd/ath.cds.fasta.mcl ath.cds.fasta\n</code></pre>\n<p>For one-vs.one\
    \ orthologs the minimal commands are</p>\n<pre><code>$ wgd dmd ath.cds.fasta vvi.cds.fasta\n\
    $ wgd ksd wgd_dmd/ath1000.fasta_vvi1000.fasta.rbh ath.cds.fasta vvi.cds.fasta\n\
    </code></pre>\n<p>For more information and these methods and other tools implemented\
    \ in <code>wgd</code>,\nplease consult the <a href=\"https://wgd.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">docs</a>.</p>\n<h2>\n<a id=\"user-content-singularity-container\"\
    \ class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Singularity container</h2>\n\
    <p><strong>Note</strong> this hasn't been updated in a while, it may or may not\
    \ work.</p>\n<p>A singularity container is available for <code>wgd</code>, allowing\
    \ all to use\nall tools in <code>wgd</code> except <code>wgd syn</code>, without\
    \ having to install all\nrequired software on your system. To install Singularity\
    \ follow\nthe instructions <a href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\
    >here</a></p>\n<p>If you have singulaity installed (and you're in the virtual\
    \ machine when\nrunning on Windows or Mac), you can run the following to get the\
    \ container</p>\n<pre><code>singularity pull --name wgd.simg shub://arzwa/wgd\n\
    </code></pre>\n<p>Then you can use <code>wgd</code> as follows</p>\n<pre><code>singularity\
    \ exec wgd.simg wgd &lt;command&gt;\n</code></pre>\n<h2>\n<a id=\"user-content-notes\"\
    \ class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Notes</h2>\n<p><strong>Bug tracking:</strong>\
    \ If the program crashes, exits unexpectedly or some\nunexpected results are obtained,\
    \ please run it again with the\n<code>--verbosity debug</code> flag <em>before</em>\
    \ the subcommand of interest (<em>e.g.</em>\n<code>wgd --verbosity debug ksd gf.mcl\
    \ cds.fasta</code>). If the anomaly persists,\nplease open an issue on this GitHub\
    \ site.</p>\n<p><strong>Note on input data:</strong> while the input data is rather\
    \ straightforward\n(a CDS fasta file will do for most analyses) it may be of interest\
    \ that\nthe wgd suite was extensively tested with data from the PLAZA platform,\n\
    so for examples of the right input data formats (in particular CDS fasta\nfiles\
    \ for sequence data and GFF files for structural annotation), please\nhave a look\
    \ <a href=\"https://bioinformatics.psb.ugent.be/plaza/versions/plaza_v4_dicots/download/\"\
    \ rel=\"nofollow\">there</a>.\nIt is generally advised not to include pipe characters\
    \ (<code>|</code>) in your gene\nIDs, since these can have special meanings in\
    \ certain parts of <code>wgd</code>.</p>\n<p><strong>Note on virtualenv:</strong>\
    \ you can install wgd in a <em>virtual environment</em>\n(using <a href=\"https://virtualenv.pypa.io/en/stable/\"\
    \ rel=\"nofollow\"><code>virtualenv</code></a>). If you\nwould however encounter\
    \ problems with running the executable directly\n(e.g. <code>wgd --help</code>\
    \ doesn't work) you can circumvent this by directly\ncalling the CLI, using <code>python3\
    \ ./wgd_cli.py --help</code> (assuming you are\ncurrently in the directory where\
    \ you cloned wgd).</p>\n<h2>\n<a id=\"user-content-citation\" class=\"anchor\"\
    \ href=\"#citation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Citation</h2>\n<p>Please cite us at <a href=\"\
    https://doi.org/10.1093/bioinformatics/bty915\" rel=\"nofollow\">https://doi.org/10.1093/bioinformatics/bty915</a></p>\n\
    <pre><code>Zwaenepoel, A., and Van de Peer, Y. wgd - simple command line tools\
    \ for the analysis of ancient whole genome duplications. Bioinformatics., bty915,\
    \ https://doi.org/10.1093/bioinformatics/bty915\n</code></pre>\n<p>For citation\
    \ of the tools used in wgd, please consult the documentation at\n<a href=\"https://wgd.readthedocs.io/en/latest/index.html#citation\"\
    \ rel=\"nofollow\">https://wgd.readthedocs.io/en/latest/index.html#citation</a>.</p>\n"
  stargazers_count: 56
  subscribers_count: 4
  topics:
  - wgd
  - duplication
  - polyploidy
  - bioinformatics
  - genomics
  - evolution
  updated_at: 1622595408.0
asafpr/singularity:
  data_format: 2
  description: Singularity description files
  filenames:
  - fusorsv/Singularity
  - mousegwas/Singularity
  full_name: asafpr/singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616613441.0
baxpr/cersuit:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.1.0
  full_name: baxpr/cersuit
  latest_release: v2.1.0
  readme: "<h1>\n<a id=\"user-content-cersuit\" class=\"anchor\" href=\"#cersuit\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>cersuit</h1>\n<p>Cerebellar segmentation with the <a href=\"http://diedrichsenlab.org/imaging/suit.htm\"\
    \ rel=\"nofollow\">SUIT atlas and toolbox</a>. In the container, the pipeline\
    \ is installed in the <code>/opt/cersuit</code> directory. Matlab code is in the\
    \ <code>src</code> directory, and the entrypoint is <code>src/cersuit.m</code>.\
    \ Compiled Matlab code for use in the singularity container without a Matlab license\
    \ is in <code>bin</code>.</p>\n<p>See the <code>external</code> directory for\
    \ links, references, and license information for the underlying SPM12 and SUIT\
    \ Matlab software. <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki\" rel=\"nofollow\"\
    >FSL version 6.0.2</a> is also used for image file manipulation and creating the\
    \ QA PDF.</p>\n<p>The container has a full installation of both SPM12 (compiled)\
    \ and FSL.</p>\n<h2>\n<a id=\"user-content-references-for-suit\" class=\"anchor\"\
    \ href=\"#references-for-suit\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>References for SUIT</h2>\n<ul>\n<li>\n\
    <p><a href=\"https://doi.org/10.1016/j.neuroimage.2006.05.056\" rel=\"nofollow\"\
    >Diedrichsen, J. (2006). A spatially unbiased atlas template of the human cerebellum.\
    \ Neuroimage, 33, 1, p. 127-138.</a></p>\n</li>\n<li>\n<p><a href=\"https://doi.org/10.1016/j.neuroimage.2009.01.045\"\
    \ rel=\"nofollow\">Diedrichsen, J., Balsters, J. H., Flavell, J., Cussans, E.,\
    \ &amp; Ramnani, N. (2009). A probabilistic atlas of the human cerebellum. Neuroimage\
    \ 46(1):39-46.</a></p>\n</li>\n<li>\n<p><a href=\"https://doi.org/10.1016/j.neuroimage.2010.10.035\"\
    \ rel=\"nofollow\">Diedrichsen, J., Maderwald, S., Kuper, M., Thurling, M., Rabe,\
    \ K., Gizewski, E. R., et al. (2011). Imaging the deep cerebellar nuclei: A probabilistic\
    \ atlas and normalization procedure. Neuroimage 54(3):1786-94</a></p>\n</li>\n\
    <li>\n<p><a href=\"https://doi.org/10.1371/journal.pone.0133402\" rel=\"nofollow\"\
    >Diedrichsen, J. &amp; Zotow, E. (2015). Surface-based display of volume-averaged\
    \ cerebellar data. PLoS One, 7, e0133402.</a></p>\n</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Pipeline</h2>\n\
    <ul>\n<li>\n<p>Adjustment of the source T1 file to axial data ordering using fslreorient2std,\
    \ to meet a requirement of the SUIT toolbox.</p>\n</li>\n<li>\n<p>Translation-only\
    \ alignment of the supplied gray matter image to SPM12's gray matter probabilistic\
    \ atlas (TPM.nii). This is accomplished by aligning the centers of mass. Rotations\
    \ are not estimated, to avoid an issue with SUIT's bounding box computation. The\
    \ supplied gray matter image must be in register with the supplied T1. The estimated\
    \ registration is saved to file and also applied to the T1.</p>\n</li>\n<li>\n\
    <p>SUIT estimation of the affine transformation and warp of the cerebellar area\
    \ of the T1 to the SUIT atlas.</p>\n</li>\n<li>\n<p>Resampling of the T1 and related\
    \ images to the SUIT atlas space. Gray matter and white matter images are resampled\
    \ both with and without modulation by the Jacobian.</p>\n</li>\n<li>\n<p>Resampling\
    \ of the SUIT-supplied atlases to the original T1 native space.</p>\n</li>\n<li>\n\
    <p>Computation of regional volumes for the Lobules_SUIT atlas in the native T1\
    \ space.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-usage-of-the-singularity-container\"\
    \ class=\"anchor\" href=\"#usage-of-the-singularity-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage\
    \ of the singularity container</h2>\n<p>See <code>singularity_examples.sh</code>\
    \ for examples of using the container for SUIT warp estimation, and transformation\
    \ from native to SUIT space and back using an existing estimated warp. The transformations\
    \ can also be done directly from matlab with the <code>transform_???.m</code>\
    \ functions in <code>src</code>).</p>\n<h2>\n<a id=\"user-content-parameters-and-inputs\"\
    \ class=\"anchor\" href=\"#parameters-and-inputs\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parameters and\
    \ inputs</h2>\n<pre><code>&lt;temporary-home-dir&gt;      Matlab will use this\
    \ for temp files\n&lt;tmp-dir&gt;                 Other location for temp files\
    \          \n&lt;input-dir&gt;               Directory containing the input T1\
    \ image file\n&lt;output-dir&gt;              Outputs will be stored here\n&lt;t1-niigz-filename&gt;\
    \       Filename of the input T1 - expecting &lt;something&gt;.nii.gz\n&lt;mask-threshold&gt;\
    \          SPM mask threshold for separating brain from background\n&lt;project-name&gt;\
    \            Project/subject/session/scan names from XNAT, if XNAT is\n&lt;subject-name&gt;\
    \               used. These are only used to decorate the PDF report.\n&lt;session-name&gt;\
    \    \n&lt;scan-name&gt;\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<p>PDF report for\
    \ quality assurance</p>\n<pre><code>PDF               cersuit.pdf\n</code></pre>\n\
    <p>Transformation from native to atlas space. Apply in this order</p>\n<pre><code>RIGID\
    \             coreg_t1_to_mni.mat\nAFFINE            Affine_c_t1_seg1.mat\nFLOWFIELD\
    \         u_a_c_t1_seg1.nii.gz\n</code></pre>\n<p>Cropped T1 in both spaces</p>\n\
    <pre><code>T1_CROP_NATIVE    c_t1.nii.gz\nT1_CROP_SUIT      wc_t1.nii.gz\n</code></pre>\n\
    <p>Cerebellum mask, segmented gray matter and white matter volume fraction images\
    \ in native and atlas space</p>\n<pre><code>MASK_NATIVE       c_t1_pcereb.nii.gz\n\
    GRAY_NATIVE       c_t1_seg1.nii.gz\nWHITE_NATIVE      c_t1_seg2.nii.gz\nMASK_SUIT\
    \         wc_t1_pcereb.nii.gz\nGRAY_SUIT         wc_t1_seg1.nii.gz\nWHITE_SUIT\
    \        wc_t1_seg2.nii.gz\n</code></pre>\n<p>Jacobian-modulated gray and white\
    \ matter images in atlas space</p>\n<pre><code>GRAYMOD_SUIT      wdc_t1_seg1.nii.gz\n\
    WHITEMOD_SUIT     wdc_t1_seg2.nii.gz\n</code></pre>\n<p>Segmented regions in native\
    \ and atlas space, with lookup table</p>\n<pre><code>ATLASES_NATIVE    SUIT-supplied\
    \ atlases resampled to original T1 space\nATLASES_SUIT      The SUIT-supplied\
    \ atlases themselves\n</code></pre>\n<p>Volumetry of segmented regions, computed\
    \ from native space images. The \"Total\" is the volume of the atlas region after\
    \ transformation to native space. The \"Gray\" is the sum of voxel gray matter\
    \ fraction within the atlas region, in native space; similar for \"White\".</p>\n\
    <pre><code>NATIVE_VOLS       iw_Lobules-SUIT_u_a_c_t1_seg1-volumes.csv\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1601771070.0
baxpr/connprep:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.2.0
  full_name: baxpr/connprep
  latest_release: v2.2.0
  readme: "<h1>\n<a id=\"user-content-connprep\" class=\"anchor\" href=\"#connprep\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>connprep</h1>\n<p>Produce preprocessed fMRI images ready for connectivity\
    \ analysis.</p>\n<h2>\n<a id=\"user-content-pipeline\" class=\"anchor\" href=\"\
    #pipeline\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pipeline</h2>\n<ol>\n<li>Drop initial or final volumes as specified.\
    \ Default: Analyze all volumes.</li>\n<li>Get the TR (volume acquisition time)\
    \ from pixdim[4] field of the Nifti header.</li>\n<li>Slice timing correction.\
    \ Default: none.</li>\n<li>Head motion realignment (SPM12 two-stage) and production\
    \ of mean fMRI.</li>\n<li>Rigid body coregistration of mean fMRI to T1 structural.</li>\n\
    <li>Compute volume quality metrics FD, DVARS.</li>\n<li>Reslice realigned fMRI\
    \ to native space, and also warp to MNI space using CAT12 transform.</li>\n<li>Remove\
    \ confounds from the native and MNI space fMRIs by simultaneous regression. Defaults:\n\
    <ul>\n<li>0.01 - 0.10 Hz bandpass filter</li>\n<li>6 estimated motion parameters\
    \ and their first differences</li>\n<li>6 principal components from the white\
    \ matter + CSF compartment</li>\n</ul>\n</li>\n<li>Repeat the confound removal,\
    \ additionally removing the mean signal of the gray matter compartment.</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Inputs</h2>\n\
    <pre><code>num_initial_vols_to_drop      0       Number of initial volumes to\
    \ drop\nnum_vols_to_analyze           all     Total number of volumes to analyze\n\
    bandpasslo_hz                 0.01    Low edge of bandpass filter in Hz\nbandpasshi_hz\
    \                 0.10    High edge of bandpass filter\nmot_PCs              \
    \         6       Number of PCs of motion params to remove\nmotderiv_PCs     \
    \             6       Same for motion derivatives\nwmcsf_PCs                 \
    \    6       Same for white matter/CSF compartment\nslorder                  \
    \     none    Slice timing correction, SPM12 nomenclature \nfmri_niigz       \
    \                     fMRI images, 4D Nifti\nmt1_niigz                       \
    \      T1 structural\ndeffwd_niigz                          Forward deformation\
    \ of T1 to MNI\ngray_niigz                            Gray matter volume fraction\n\
    white_niigz                           White matter volume fraction\ncsf_niigz\
    \                             CSF volume fraction\nproject                   \
    \            XNAT project label\nsubject                               XNAT subject\
    \ label\nsession                               XNAT session label\nscan      \
    \                            XNAT scan label\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<pre><code>connprep.pdf\
    \                               Processing report\nrp_adfmri.txt             \
    \                 Realignment parameters\nFD.txt                             \
    \        Framewise displacement\nDVARS.txt                                  Framewise\
    \ noise\nfiltered_keepgm_noscrub_nadfmri.nii.gz     Filtered data, native space,\
    \ gray matter signal retained\nfiltered_keepgm_noscrub_wadfmri.nii.gz     Filtered\
    \ data, MNI space, gray matter signal retained\nfiltered_removegm_noscrub_nadfmri.nii.gz\
    \   Filtered data, native space, gray matter signal removed\nfiltered_removegm_noscrub_wadfmri.nii.gz\
    \   Filtered data, MNI space, gray matter signal removed\nmeanadfmri.nii.gz  \
    \                        Mean fMRI, native space\nwmeanadfmri.nii.gz         \
    \                Mean fMRI, MNI space\nstats_keepgm_noscrub.txt              \
    \     Processing info when gray matter signal retained\nstats_removegm_noscrub.txt\
    \                 Processing info when gray matter signal removed\ngm_mask.nii.gz\
    \                             Native space gray matter mask\nwmcsf_mask.nii.gz\
    \                          Native space white matter/CSF mask\nconfounds_keepgm_noscrub.txt\
    \               Confounds matrix when gray matter signal retained\nconfounds_removegm_noscrub.txt\
    \             Confounds matrix  when gray matter signal removed\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1595372367.0
baxpr/fmri_conncalc:
  data_format: 2
  description: null
  filenames:
  - Singularity.v1.0.0
  full_name: baxpr/fmri_conncalc
  latest_release: v1.0.0-rc0
  readme: "<h1>\n<a id=\"user-content-fmri_conncalc\" class=\"anchor\" href=\"#fmri_conncalc\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>fmri_conncalc</h1>\n<p>Preprocessing and functional connectivity computation\
    \ for fMRI</p>\n<h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"\
    #quickstart\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Quickstart</h2>\n<p>Here is an example for the \"\
    jsins\" version of the processor, as described in\n<a href=\"conncalc_jsins_v1.0.0.yaml\"\
    >conncalc_jsins_v1.0.0.yaml</a>.</p>\n<pre><code>singularity\n  run\n  --bind\
    \ &lt;INDIR&gt;:/INPUTS\n  --bind &lt;OUTDIR&gt;:/OUTPUTS\n  baxpr-fmri_conncalc-master-v1.0.0.simg\n\
    \  magick_path /usr/bin\n  param_file params_JSins.csv\n  wroi_file rois_JSins.nii.gz\n\
    \  roi_file ''\n  roiinfo_file rois_JSins.csv\n  coregmat_file /INPUTS/coreg_mat.txt\
    \ \\\n  deffwd_file /INPUTS/y_deffwd.nii.gz \\\n  ct1_file /INPUTS/ct1.nii.gz\
    \ \\\n  wgm_file /INPUTS/wgm.nii.gz \\\n  wcseg_file /INPUTS/wcseg.nii.gz \\\n\
    \  func_file /INPUTS/fmri.nii.gz \\\n  project PROJECT_LABEL \\\n  subject SUBJECT_LABEL\
    \ \\\n  session SESSION_LABEL \\\n  scan SCAN_LABEL \\\n  out_dir /OUTPUTS\n</code></pre>\n\
    <p>The inputs <code>coregmat_file</code>, <code>deffwd_file</code>, <code>ct1_file</code>,\
    \ <code>wgm_file</code>, <code>wcseg_file</code> would typically be obtained from\
    \ the outputs of the <code>MAGM_Coreg_Normalize_v2</code> spider.</p>\n<p>The\
    \ outputs are:</p>\n<pre><code>fmri_conncalc.pdf    Report\nparams.csv       \
    \    Parameters used in the analysis\nFD.txt               Framewise displacement\
    \ time series\nDVARS.txt            Framewise variance time series\nbadvols.txt\
    \          Scrubbed volumes indicator time series\nrp_adfunc.txt        Realignment\
    \ (motion) values\nwmeanadfunc.nii.gz   Mean functional image in standard space\n\
    wadfunc.nii.gz       Slice time corrected and realigned functional images in standard\
    \ space\nrroi_labels.nii.gz   Region of interest label image\nroi_snr.nii.gz \
    \      ROI SNR image\nroi_info.csv         ROI info\nroi_labels.csv       ROI\
    \ names (if available)\n\nSeries of results repeated for each of the four processing\
    \ streams\n(keep or remove mean gray matter; scrub or no scrub):\n\n  confounds_removegm_noscrub.txt\
    \               Confound (filter) matrix\n  connectivity_matrix_R_removegm_noscrub.csv\
    \   Connectivity matrix\n  filtered_removegm_noscrub.nii.gz             Filtered\
    \ functional images\n  roi_timeseries_removegm_noscrub.csv          Filtered ROI\
    \ time series\n  stats_removegm_noscrub.txt                   Various statistics\n\
    \  Zmap_removegm_noscrub.nii.gz                 Unsmoothed ROI connectivity maps\n\
    \  sZmap_removegm_noscrub.nii.gz                Smoothed ROI connectivity maps\n\
    </code></pre>\n<h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"\
    #dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Dependencies</h2>\n<p>The built singularity container\
    \ <code>baxpr-fmri_conncalc-master-v1.0.0.simg</code> (URL is shub://baxpr/fmri_conncalc:v1.0.0)\
    \ is stand-alone with no external dependencies. The compiled matlab <a href=\"\
    bin/run_fmri_conncalc.sh\">run_fmri_conncalc.sh</a> requires only the appropriate\
    \ MATLAB Runtime to execute. To build these there are two stages:</p>\n<ol>\n\
    <li>\n<p>Compile the MATLAB code into a stand-alone executable, using <a href=\"\
    compile_matlab.sh\">compile_matlab.sh</a>. This requires a full MATLAB installation\
    \ (R2017a, v92) and SPM12 (<a href=\"https://www.fil.ion.ucl.ac.uk/spm/\" rel=\"\
    nofollow\">https://www.fil.ion.ucl.ac.uk/spm/</a>).</p>\n</li>\n<li>\n<p>Build\
    \ the singularity container. In addition to a few specific OS packages, this requires\
    \ the MATLAB Compiled Runtime. All are specified to be downloaded during the build\
    \ in the singularity recipe <a href=\"Singularity.v1.0.0\">Singularity.v1.0.0</a>.\
    \ The container help text gives build instructions. Alternatively the built container\
    \ can be obtained from singularity-hub:\n<code>singularity pull shub://baxpr/fmri_conncalc:v1.0.0</code></p>\n\
    </li>\n</ol>\n<h2>\n<a id=\"user-content-peculiarities-of-specific-pipelines\"\
    \ class=\"anchor\" href=\"#peculiarities-of-specific-pipelines\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Peculiarities\
    \ of specific pipelines</h2>\n<p>Some critical analysis parameters are specified\
    \ in the <code>param_file</code>, e.g. <code>params_JSins.csv</code>. This is\
    \ a reference to a file that's in the built container, but these can also be viewed\
    \ in the code repository e.g. <a href=\"src/params/params_JSins.csv\">src/params/params_JSins.csv</a>.\
    \ The parameters get as detailed as the repetition time of the fMRI scans. If\
    \ the needed parameter file is not in the container already:</p>\n<ul>\n<li>Add\
    \ the new parameter file in <code>src/params</code>\n</li>\n<li>Update the matlab\
    \ compilation code to include it with <code>-a</code>\n</li>\n<li>Recompile the\
    \ matlab</li>\n<li>Commit to github. Note that the compiled matlab executable\
    \ is stored using LFS</li>\n<li>Rebuild the container (increment the patch number,\
    \ e.g. 1.0.0 to 1.0.1)</li>\n<li>Create an updated YAML file appropriate for the\
    \ parameter set</li>\n</ul>\n<h3>\n<a id=\"user-content-jsins-version\" class=\"\
    anchor\" href=\"#jsins-version\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>jsins version</h3>\n<p><a href=\"\
    conncalc_jsins_v1.0.0.yaml\">conncalc_jsins_v1.0.0.yaml</a></p>\n<p>Standard space\
    \ regions of interest are used, <a href=\"src/params/JS_insula/rois_JSins.nii.gz\"\
    >rois_JSins.nii.gz</a>, identical for every subject.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R). A connectivity map is computed\
    \ for each ROI (Fisher Z transform applied to Pearson bivariate correlation).\
    \ Spatial smoothing is applied to the connectivity maps only.</p>\n<p>Parameter\
    \ settings in <a href=\"src/params/params_JSins.csv\">params_JSins.csv</a>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>Use all\
    \ fMRI volumes (none dropped)</li>\n<li>No slice timing correction</li>\n<li>6mm\
    \ FWHM Gaussian spatial smoothing applied to connectivity maps</li>\n<li>Filter\
    \ settings (confound regressor matrix):\n<ul>\n<li>0.01 Hz - 0.10 Hz bandpass\
    \ filter (Fourier basis)</li>\n<li>6 motion parameters (translation and rotation)</li>\n\
    <li>6 first differences of motion parameters</li>\n<li>First 6 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n<li>Connectivity maps\
    \ are saved for each ROI.</li>\n</ul>\n<h3>\n<a id=\"user-content-szhab-version\"\
    \ class=\"anchor\" href=\"#szhab-version\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>szhab version</h3>\n<p>No YAML\
    \ available yet.</p>\n<p>Subject-specific regions of interest are used, as described\
    \ in the native space ROI image supplied as input. This image must be in the same\
    \ space as the subject's native space structural.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R of filtered time series). Spatial\
    \ smoothing is not used.</p>\n<p>Parameter settings in <code>params_SZhab.csv</code>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>5 initial\
    \ volumes are dropped, and the following 60 volumes are used for the analysis</li>\n\
    <li>No slice timing correction</li>\n<li>Filter settings (confound regressor matrix):\n\
    <ul>\n<li>0.01 Hz - 0.15 Hz bandpass filter (Fourier basis)</li>\n<li>6 motion\
    \ parameters (translation and rotation)</li>\n<li>First 3 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-general-pipeline\" class=\"anchor\" href=\"#general-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>General\
    \ pipeline</h2>\n<p>Other than the above, processing proceeds as follows.</p>\n\
    <ol>\n<li>\n<p>Drop functional volumes as specified.</p>\n</li>\n<li>\n<p>Perform\
    \ slice timing correction as specified. (SPM12 slice timing correction)</p>\n\
    </li>\n<li>\n<p>Perform motion realignment: two-stage alignment to mean image.\
    \ (SPM12 realignment)</p>\n</li>\n<li>\n<p>Coregister the mean functional image\
    \ to the T1 weighted structural using a rigid body transform. The structural is\
    \ first skull-stripped by zeroing all voxels that were not labeled by the multiatlas\
    \ segmentation. The transformation is then applied to all functional volumes.\
    \ (SPM12 coregistration)</p>\n</li>\n<li>\n<p>Quality parameters are computed:\
    \ framewise displacement FD and framewise signal variance DVARS. Volumes exceeding\
    \ scrubbing criteria are marked (\"badvols\").</p>\n</li>\n<li>\n<p>The functional\
    \ and structural images are warped to standard space using the supplied nonlinear\
    \ transform (forward deformation image). (SPM12 deformation tools)</p>\n</li>\n\
    <li>\n<p>The supplied standard space ROI image file is resampled to match the\
    \ standard space fMRI geometry. (SPM12 reslice)</p>\n</li>\n<li>\n<p>Connectivity\
    \ computation. All filtering is done in a single step: a design matrix of confounds\
    \ is created (see lists above), it is fit to each voxel time series, and the residuals\
    \ are extracted. Then bivariate Pearson correlation is computed between ROI residual\
    \ time series to produce the connectivity matrix. Fisher transformed correlation\
    \ between ROIs/voxel residual time series is used to produce connectivity maps\
    \ if that option is selected.</p>\n</li>\n</ol>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1543615331.0
baxpr/fmri_modularity:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.0.0
  full_name: baxpr/fmri_modularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-fmri_conncalc\" class=\"anchor\" href=\"#fmri_conncalc\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>fmri_conncalc</h1>\n<p>Preprocessing and functional connectivity computation\
    \ for fMRI</p>\n<h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"\
    #quickstart\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Quickstart</h2>\n<p>Here is an example for the \"\
    jsins\" version of the processor, as described in\n<a href=\"conncalc_jsins_v1.0.0.yaml\"\
    >conncalc_jsins_v1.0.0.yaml</a>.</p>\n<pre><code>singularity\n  run\n  --bind\
    \ &lt;INDIR&gt;:/INPUTS\n  --bind &lt;OUTDIR&gt;:/OUTPUTS\n  baxpr-fmri_conncalc-master-v1.0.0.simg\n\
    \  magick_path /usr/bin\n  param_file params_JSins.csv\n  wroi_file rois_JSins.nii.gz\n\
    \  roi_file ''\n  roiinfo_file rois_JSins.csv\n  coregmat_file /INPUTS/coreg_mat.txt\
    \ \\\n  deffwd_file /INPUTS/y_deffwd.nii.gz \\\n  ct1_file /INPUTS/ct1.nii.gz\
    \ \\\n  wgm_file /INPUTS/wgm.nii.gz \\\n  wcseg_file /INPUTS/wcseg.nii.gz \\\n\
    \  func_file /INPUTS/fmri.nii.gz \\\n  project PROJECT_LABEL \\\n  subject SUBJECT_LABEL\
    \ \\\n  session SESSION_LABEL \\\n  scan SCAN_LABEL \\\n  out_dir /OUTPUTS\n</code></pre>\n\
    <p>The inputs <code>coregmat_file</code>, <code>deffwd_file</code>, <code>ct1_file</code>,\
    \ <code>wgm_file</code>, <code>wcseg_file</code> would typically be obtained from\
    \ the outputs of the <code>MAGM_Coreg_Normalize_v2</code> spider.</p>\n<p>The\
    \ outputs are:</p>\n<pre><code>fmri_conncalc.pdf    Report\nparams.csv       \
    \    Parameters used in the analysis\nFD.txt               Framewise displacement\
    \ time series\nDVARS.txt            Framewise variance time series\nbadvols.txt\
    \          Scrubbed volumes indicator time series\nrp_adfunc.txt        Realignment\
    \ (motion) values\nwmeanadfunc.nii.gz   Mean functional image in standard space\n\
    wadfunc.nii.gz       Slice time corrected and realigned functional images in standard\
    \ space\nrroi_labels.nii.gz   Region of interest label image\nroi_snr.nii.gz \
    \      ROI SNR image\nroi_info.csv         ROI info\nroi_labels.csv       ROI\
    \ names (if available)\n\nSeries of results repeated for each of the four processing\
    \ streams\n(keep or remove mean gray matter; scrub or no scrub):\n\n  confounds_removegm_noscrub.txt\
    \               Confound (filter) matrix\n  connectivity_matrix_R_removegm_noscrub.csv\
    \   Connectivity matrix\n  filtered_removegm_noscrub.nii.gz             Filtered\
    \ functional images\n  roi_timeseries_removegm_noscrub.csv          Filtered ROI\
    \ time series\n  stats_removegm_noscrub.txt                   Various statistics\n\
    \  Zmap_removegm_noscrub.nii.gz                 Unsmoothed ROI connectivity maps\n\
    \  sZmap_removegm_noscrub.nii.gz                Smoothed ROI connectivity maps\n\
    </code></pre>\n<h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"\
    #dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Dependencies</h2>\n<p>The built singularity container\
    \ <code>baxpr-fmri_conncalc-master-v1.0.0.simg</code> (URL is shub://baxpr/fmri_conncalc:v1.0.0)\
    \ is stand-alone with no external dependencies. The compiled matlab <a href=\"\
    bin/run_fmri_conncalc.sh\">run_fmri_conncalc.sh</a> requires only the appropriate\
    \ MATLAB Runtime to execute. To build these there are two stages:</p>\n<ol>\n\
    <li>\n<p>Compile the MATLAB code into a stand-alone executable, using <a href=\"\
    compile_matlab.sh\">compile_matlab.sh</a>. This requires a full MATLAB installation\
    \ (R2017a, v92) and SPM12 (<a href=\"https://www.fil.ion.ucl.ac.uk/spm/\" rel=\"\
    nofollow\">https://www.fil.ion.ucl.ac.uk/spm/</a>).</p>\n</li>\n<li>\n<p>Build\
    \ the singularity container. In addition to a few specific OS packages, this requires\
    \ the MATLAB Compiled Runtime. All are specified to be downloaded during the build\
    \ in the singularity recipe <a href=\"Singularity.v1.0.0\">Singularity.v1.0.0</a>.\
    \ The container help text gives build instructions. Alternatively the built container\
    \ can be obtained from singularity-hub:\n<code>singularity pull shub://baxpr/fmri_conncalc:v1.0.0</code></p>\n\
    </li>\n</ol>\n<h2>\n<a id=\"user-content-peculiarities-of-specific-pipelines\"\
    \ class=\"anchor\" href=\"#peculiarities-of-specific-pipelines\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Peculiarities\
    \ of specific pipelines</h2>\n<p>Some critical analysis parameters are specified\
    \ in the <code>param_file</code>, e.g. <code>params_JSins.csv</code>. This is\
    \ a reference to a file that's in the built container, but these can also be viewed\
    \ in the code repository e.g. <a href=\"src/params/params_JSins.csv\">src/params/params_JSins.csv</a>.\
    \ The parameters get as detailed as the repetition time of the fMRI scans. If\
    \ the needed parameter file is not in the container already:</p>\n<ul>\n<li>Add\
    \ the new parameter file in <code>src/params</code>\n</li>\n<li>Update the matlab\
    \ compilation code to include it with <code>-a</code>\n</li>\n<li>Recompile the\
    \ matlab</li>\n<li>Commit to github. Note that the compiled matlab executable\
    \ is stored using LFS</li>\n<li>Rebuild the container (increment the patch number,\
    \ e.g. 1.0.0 to 1.0.1)</li>\n<li>Create an updated YAML file appropriate for the\
    \ parameter set</li>\n</ul>\n<h3>\n<a id=\"user-content-jsins-version\" class=\"\
    anchor\" href=\"#jsins-version\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>jsins version</h3>\n<p><a href=\"\
    conncalc_jsins_v1.0.0.yaml\">conncalc_jsins_v1.0.0.yaml</a></p>\n<p>Standard space\
    \ regions of interest are used, <a href=\"src/params/JS_insula/rois_JSins.nii.gz\"\
    >rois_JSins.nii.gz</a>, identical for every subject.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R). A connectivity map is computed\
    \ for each ROI (Fisher Z transform applied to Pearson bivariate correlation).\
    \ Spatial smoothing is applied to the connectivity maps only.</p>\n<p>Parameter\
    \ settings in <a href=\"src/params/params_JSins.csv\">params_JSins.csv</a>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>Use all\
    \ fMRI volumes (none dropped)</li>\n<li>No slice timing correction</li>\n<li>6mm\
    \ FWHM Gaussian spatial smoothing applied to connectivity maps</li>\n<li>Filter\
    \ settings (confound regressor matrix):\n<ul>\n<li>0.01 Hz - 0.10 Hz bandpass\
    \ filter (Fourier basis)</li>\n<li>6 motion parameters (translation and rotation)</li>\n\
    <li>6 first differences of motion parameters</li>\n<li>First 6 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n<li>Connectivity maps\
    \ are saved for each ROI.</li>\n</ul>\n<h3>\n<a id=\"user-content-szhab-version\"\
    \ class=\"anchor\" href=\"#szhab-version\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>szhab version</h3>\n<p>No YAML\
    \ available yet.</p>\n<p>Subject-specific regions of interest are used, as described\
    \ in the native space ROI image supplied as input. This image must be in the same\
    \ space as the subject's native space structural.</p>\n<p>Connectivity matrix\
    \ is computed (Pearson bivariate correlation R of filtered time series). Spatial\
    \ smoothing is not used.</p>\n<p>Parameter settings in <code>params_SZhab.csv</code>:</p>\n\
    <ul>\n<li>FMRI repetition time (TR) is assumed to be 2.000 sec</li>\n<li>5 initial\
    \ volumes are dropped, and the following 60 volumes are used for the analysis</li>\n\
    <li>No slice timing correction</li>\n<li>Filter settings (confound regressor matrix):\n\
    <ul>\n<li>0.01 Hz - 0.15 Hz bandpass filter (Fourier basis)</li>\n<li>6 motion\
    \ parameters (translation and rotation)</li>\n<li>First 3 principal components\
    \ of voxel time series from the eroded white matter/CSF compartment</li>\n</ul>\n\
    </li>\n<li>For scrubbed results, volumes before and after an excursion of FD &gt;\
    \ 0.5 are removed. DVARS is not used for scrubbing.</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-general-pipeline\" class=\"anchor\" href=\"#general-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>General\
    \ pipeline</h2>\n<p>Other than the above, processing proceeds as follows.</p>\n\
    <ol>\n<li>\n<p>Drop functional volumes as specified.</p>\n</li>\n<li>\n<p>Perform\
    \ slice timing correction as specified. (SPM12 slice timing correction)</p>\n\
    </li>\n<li>\n<p>Perform motion realignment: two-stage alignment to mean image.\
    \ (SPM12 realignment)</p>\n</li>\n<li>\n<p>Coregister the mean functional image\
    \ to the T1 weighted structural using a rigid body transform. The structural is\
    \ first skull-stripped by zeroing all voxels that were not labeled by the multiatlas\
    \ segmentation. The transformation is then applied to all functional volumes.\
    \ (SPM12 coregistration)</p>\n</li>\n<li>\n<p>Quality parameters are computed:\
    \ framewise displacement FD and framewise signal variance DVARS. Volumes exceeding\
    \ scrubbing criteria are marked (\"badvols\").</p>\n</li>\n<li>\n<p>The functional\
    \ and structural images are warped to standard space using the supplied nonlinear\
    \ transform (forward deformation image). (SPM12 deformation tools)</p>\n</li>\n\
    <li>\n<p>The supplied standard space ROI image file is resampled to match the\
    \ standard space fMRI geometry. (SPM12 reslice)</p>\n</li>\n<li>\n<p>Connectivity\
    \ computation. All filtering is done in a single step: a design matrix of confounds\
    \ is created (see lists above), it is fit to each voxel time series, and the residuals\
    \ are extracted. Then bivariate Pearson correlation is computed between ROI residual\
    \ time series to produce the connectivity matrix. Fisher transformed correlation\
    \ between ROIs/voxel residual time series is used to produce connectivity maps\
    \ if that option is selected.</p>\n</li>\n</ol>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550158474.0
baxpr/fmriqa:
  data_format: 2
  description: null
  filenames:
  - Singularity.v4.2.0
  full_name: baxpr/fmriqa
  latest_release: null
  readme: '<h1>

    <a id="user-content-functional-mri-qa-pipeline" class="anchor" href="#functional-mri-qa-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Functional
    MRI QA pipeline</h1>

    <h2>

    <a id="user-content-building" class="anchor" href="#building" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h2>

    <ol>

    <li>Test the matlab code before compiling: <code>src/testmatlab.m</code>

    </li>

    <li>Compile: <code>compile_matlab.sh</code>

    </li>

    <li>Test the compiled runtime: <code>bin/test_compiled_matlab.sh</code>

    </li>

    <li>Build the Singularity container: <code>Singularity.v4.2.0</code>, <a href="https://www.singularity-hub.org/collections/2945"
    rel="nofollow">https://www.singularity-hub.org/collections/2945</a>

    </li>

    </ol>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ol>

    <li>See <code>test_sing_container.sh</code>.</li>

    </ol>

    <h2>

    <a id="user-content-inputs" class="anchor" href="#inputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Inputs</h2>

    <p>The inputs must all be provided, in the correct order. Paths are with respect
    to the container root.</p>

    <ol>

    <li>Name of the output directory</li>

    <li>Filename of the T1 structural image (.nii.gz)</li>

    <li>Filename of the segmented T1 image (.nii.gz), typically the SEG output of
    a MultiAtlas or SLANT pipeline</li>

    <li>Filename of the 4D fMRI (.nii.gz)</li>

    <li>XNAT project label</li>

    <li>XNAT subject label</li>

    <li>XNAT session label</li>

    <li>XNAT scan label (of the fMRI)</li>

    </ol>

    <h2>

    <a id="user-content-processing" class="anchor" href="#processing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Processing</h2>

    <ol>

    <li>Motion realignment and creation of mean fMRI</li>

    <li>Coregister T1 to mean fMRI</li>

    <li>Compute SNR and quality metrics</li>

    <li>Carpet plots, graphical report</li>

    </ol>

    <h2>

    <a id="user-content-outputs" class="anchor" href="#outputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Outputs</h2>

    <pre><code>fmriqa.pdf                               PDF report

    rp_fmri.txt                              Realignment parameters (SPM12 style)

    fmriqa_stats.csv                         Summary stats

    fmriqa_stats_wide.csv                    Summary stats in wide format (XNAT/REDCap
    compatible)

    FD.txt                                   Framewise displacement time series

    DVARS.txt                                DVARS time series

    global.txt                               Global mean time series

    meanfmri.nii.gz                          Mean fMRI image after realignment

    median_voxel_displacement_mm.txt         Framewise displacement, median over voxels

    temporal_snr.nii.gz                      Temporal signal-to-noise ratio image

    voxel_displacement_mm_95prctile.nii.gz   Framewise displacement image (95th percentile
    over time)

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1558037991.0
baxpr/makerois-PMAT:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: baxpr/makerois-PMAT
  latest_release: v1.0.13
  readme: "<h1>\n<a id=\"user-content-create-study-specific-roi-image-in-mni-space\"\
    \ class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Create\
    \ study-specific ROI image in MNI space</h1>\n<p>PMAT resting state connectivity\
    \ study.</p>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Inputs:</h2>\n<p>All should be matched to the same T1 image.</p>\n\
    <ul>\n<li>T1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)</li>\n\
    <li>Deformation from T1 subject space to atlas space (typically DEF_FWD resource\
    \ of cat12 assessor)</li>\n<li>SUBJECT directory of Freesurfer output (typically\
    \ SUBJECT resource of freesurfer_dev assessor)</li>\n<li>Temporal lobe segmentation\
    \ (typically SEG resource of Temporal_Lobe assessor)</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n\
    <pre><code>rois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv\
    \        Region labels and volumes\nmakerois-PMAT.pdf           Visual report\
    \ of final ROI image\n</code></pre>\n<h2>\n<a id=\"user-content-regions-of-interest\"\
    \ class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Regions of interest</h2>\n<h3>\n\
    <a id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spheres (atlas space)</h3>\n<p>Source: <em>Libby LA, Ekstrom AD, Ragland\
    \ JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal\
    \ cortices within human hippocampal subregions revealed by high-resolution functional\
    \ imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012</em></p>\n\
    <p>Method: <em>Schr\xF6der TN, Haak K V., Jimenez NIZ, et al. Functional topography\
    \ of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"\
    #entorhinal-cortex-atlas-space\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Entorhinal cortex (atlas space)</h3>\n\
    <p>Anterior lateral and posterior medial sections. Source and method: <em>Schr\xF6\
    der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal\
    \ cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\"\
    \ href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Temporal lobe (Subject space,\
    \ warped)</h3>\n<p>Head for anterior hippocampus; body and tail combined for posterior\
    \ hippocampus. Method: <em>Plassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale\
    \ Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE\
    \ Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017\
    \ Feb 24. PMID: 28781411; PMCID: PMC5544133.</em></p>\n<h3>\n<a id=\"user-content-parahippocampal-perirhinal-subject-space-warped\"\
    \ class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parahippocampal,\
    \ perirhinal (Subject space, warped)</h3>\n<p>Generated by Freesurfer 6. Parahippocampal\
    \ (1016,2016) and perirhinal (surface patch resampled to volume, overlap with\
    \ parahippocampus was assigned to perirhinal). Method: <em>Bruce Fischl, Andre\
    \ van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H.\
    \ Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne\
    \ Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating\
    \ the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.</em></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607988459.0
baxpr/mp2rage:
  data_format: 2
  description: null
  filenames:
  - Singularity.v2.1.0
  full_name: baxpr/mp2rage
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-mp2rage\" class=\"anchor\" href=\"#mp2rage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>mp2rage</h1>\n<p>Reconstructs a T1-weighted image from images at multiple\
    \ inversion times following Marques et al. 2009. The robust adjustment (beta factor)\
    \ of O'Brien 2014 is also implemented.</p>\n<p><strong>Marques JP, Kober T, Krueger\
    \ G, van der Zwaag W, Van de Moortele PF, Gruetter  R. MP2RAGE, a self bias-field\
    \ corrected sequence for improved segmentation and T1-mapping at high field. Neuroimage.\
    \ 2010 Jan 15;49(2):1271-81. doi:10.1016/j.neuroimage.2009.10.002. PMID: 19819338.</strong></p>\n\
    <p><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/19819338\" rel=\"nofollow\">https://www.ncbi.nlm.nih.gov/pubmed/19819338</a></p>\n\
    <p>The large spatial inhomogeneity in transmit B(1) field (B(1)(+)) observable\
    \ in human MR images at high static magnetic fields (B(0)) severely impairs image\
    \ quality. To overcome this effect in brain T(1)-weighted images, the MPRAGE sequence\
    \ was modified to generate two different images at different inversion times,\
    \ MP2RAGE. By combining the two images in a novel fashion, it was possible to\
    \ create T(1)-weighted images where the result image was free of proton density\
    \ contrast, T(2) contrast, reception bias field, and, to first order, transmit\
    \ field inhomogeneity. MP2RAGE sequence parameters were optimized using Bloch\
    \ equations to maximize contrast-to-noise ratio per unit of time between brain\
    \ tissues and minimize the effect of B(1)(+) variations through space. Images\
    \ of high anatomical quality and excellent brain tissue differentiation suitable\
    \ for applications such as segmentation and voxel-based morphometry were obtained\
    \ at 3 and 7 T. From such T(1)-weighted images, acquired within 12 min, high-resolution\
    \ 3D T(1) maps were routinely calculated at 7 T with sub-millimeter voxel resolution\
    \ (0.65-0.85 mm isotropic). T(1) maps were validated in phantom experiments. In\
    \ humans, the T(1) values obtained at 7 T were 1.15+/-0.06 s for white matter\
    \ (WM) and 1.92+/-0.16 s for grey matter (GM), in good agreement with literature\
    \ values obtained at lower spatial resolution. At 3 T, where whole-brain acquisitions\
    \ with 1 mm isotropic voxels were acquired in 8 min, the T(1) values obtained\
    \ (0.81+/-0.03 s for WM and 1.35+/-0.05 for GM) were once again found to be in\
    \ very good agreement with values in the literature.</p>\n<p><strong>O'Brien KR,\
    \ Kober T, Hagmann P, et al. Robust T1-weighted Structural Brain Imaging and Morphometry\
    \ at 7T Using MP2RAGE PLoS One. 2014;9(6):e99676. Published 2014 Jun 16. doi:10.1371/journal.pone.0099676</strong></p>\n\
    <p><a href=\"https://pubmed.ncbi.nlm.nih.gov/24932514/\" rel=\"nofollow\">https://pubmed.ncbi.nlm.nih.gov/24932514/</a></p>\n\
    <p>Purpose: To suppress the noise, by sacrificing some of the signal homogeneity\
    \ for numerical stability, in uniform T1 weighted (T1w) images obtained with the\
    \ magnetization prepared 2 rapid gradient echoes sequence (MP2RAGE) and to compare\
    \ the clinical utility of these robust T1w images against the uniform T1w images.</p>\n\
    <p>Materials and methods: 8 healthy subjects (29.0 \xB1 4.1 years; 6 Male), who\
    \ provided written consent, underwent two scan sessions within a 24 hour period\
    \ on a 7T head-only scanner. The uniform and robust T1w image volumes were calculated\
    \ inline on the scanner. Two experienced radiologists qualitatively rated the\
    \ images for: general image quality; 7T specific artefacts; and, local structure\
    \ definition. Voxel-based and volume-based morphometry packages were used to compare\
    \ the segmentation quality between the uniform and robust images. Statistical\
    \ differences were evaluated by using a positive sided Wilcoxon rank test.</p>\n\
    <p>Results: The robust image suppresses background noise inside and outside the\
    \ skull. The inhomogeneity introduced was ranked as mild. The robust image was\
    \ significantly ranked higher than the uniform image for both observers (observer\
    \ 1/2, p-value = 0.0006/0.0004). In particular, an improved delineation of the\
    \ pituitary gland, cerebellar lobes was observed in the robust versus uniform\
    \ T1w image. The reproducibility of the segmentation results between repeat scans\
    \ improved (p-value = 0.0004) from an average volumetric difference across structures\
    \ of \u2248 6.6% to \u2248 2.4% for the uniform image and robust T1w image respectively.</p>\n\
    <p>Conclusions: The robust T1w image enables MP2RAGE to produce, clinically familiar\
    \ T1w images, in addition to T1 maps, which can be readily used in uniform morphometry\
    \ packages.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1595274818.0
baxpr/segwarp:
  data_format: 2
  description: null
  filenames:
  - Singularity.v1.0.0
  full_name: baxpr/segwarp
  latest_release: null
  readme: '<p>Warp SEG output of a multi-atlas assessor to MNI space using the supplied
    SPM warp field.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605062943.0
bbodi/rustarok:
  data_format: 2
  description: Multiplayer, fast-paced Moba style game
  filenames:
  - docker/Singularity.wine
  - docker/Singularity
  full_name: bbodi/rustarok
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-table-of-contents\" class=\"anchor\" href=\"\
    #table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Table of contents</h1>\n<ul>\n<li><a href=\"\
    #building\">Building</a></li>\n<li><a href=\"#running-on-windows\">Running on\
    \ Windows</a></li>\n<li><a href=\"#running-with-docker\">Running with Docker</a></li>\n\
    <li><a href=\"#how-to-play\">How to play</a></li>\n<li><a href=\"#design-decisions\"\
    >Design decisions</a></li>\n<li><a href=\"#blog\">Blog</a></li>\n<li><a href=\"\
    #current-status-and-gallery\">Current Status and Gallery</a></li>\n<li>\n<a href=\"\
    #background-story\">Background story</a>\n<ul>\n<li><a href=\"#about-the-used-game-assets\"\
    >About the used game assets</a></li>\n</ul>\n</li>\n<li><a href=\"#thanks\">Thanks</a></li>\n\
    </ul>\n<h1>\n<a id=\"user-content-rustarok\" class=\"anchor\" href=\"#rustarok\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Rustarok</h1>\n<p>A project whose primary goals are to have fun developing\
    \ it and experiment with interesting technical problems from the world of game\
    \ development.</p>\n<p>It is intended to be a multiplayer, fast-paced Moba style\
    \ game. Check <a href=\"#background-story\">Background story</a> for details.</p>\n\
    <h2>\n<a id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building</h2>\n\
    <ul>\n<li><code>git clone https://github.com/bbodi/rustarok.git</code></li>\n\
    <li><code>cargo build</code></li>\n</ul>\n<h2>\n<a id=\"user-content-running-on-windows\"\
    \ class=\"anchor\" href=\"#running-on-windows\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running on Windows</h2>\n<p>You\
    \ will need Ragnarok Online asset files to run the game.</p>\n<ul>\n<li>\n<p>Download\
    \ a Ragnarok Online client from <a href=\"https://talonro.com/\" rel=\"nofollow\"\
    >some</a> <a href=\"http://playdreamerro.com/\" rel=\"nofollow\">popular</a> <a\
    \ href=\"https://topg.org/ragnarok-private-servers/\" rel=\"nofollow\">private\
    \ server</a></p>\n</li>\n<li>\n<p>Install it</p>\n</li>\n<li>\n<p>Check the installation\
    \ directory for any <code>*.grf</code> files, and put their paths into <code>config.toml</code>,\
    \ e.g.:</p>\n<pre><code>grf_paths = [\n  \"d:\\\\Games\\\\TalonRO\\\\rdata.grf\"\
    ,\n  \"d:\\\\Games\\\\TalonRO\\\\sdata.grf\",\n  \"d:\\\\Games\\\\TalonRO\\\\\
    tdata.grf\"\n]\n</code></pre>\n</li>\n<li>\n<p>Run <code>cargo run</code> from\
    \ rustarok directory.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-running-with-docker\"\
    \ class=\"anchor\" href=\"#running-with-docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running with Docker</h2>\n<p>See\
    \ the README.md in the <a href=\"docker\">docker</a> folder for complete instructions.</p>\n\
    <h2>\n<a id=\"user-content-how-to-play\" class=\"anchor\" href=\"#how-to-play\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to play</h2>\n<ul>\n<li>Move your character with the right mouse\
    \ button</li>\n<li>Cast skills with Q (fire wall), W (lightning), E (heal), R\
    \ (huge boom) keys</li>\n<li>Spawn entities with the \"Players\" and \"Monsters\"\
    \ sliders in the window</li>\n<li>Move the camera with the cursor keys</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-design-decisions\" class=\"anchor\" href=\"\
    #design-decisions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Design decisions</h2>\n<ul>\n<li><a href=\"https://github.com/bbodi/rustarok/issues/1\"\
    >Statuses</a></li>\n<li><a href=\"https://github.com/bbodi/rustarok/issues/4\"\
    >Rendering system</a></li>\n</ul>\n<h2>\n<a id=\"user-content-blog\" class=\"\
    anchor\" href=\"#blog\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Blog</h2>\n<ul>\n<li><a href=\"https://github.com/bbodi/rustarok/issues/6\"\
    >2019W30</a></li>\n</ul>\n<h2>\n<a id=\"user-content-current-status-and-gallery\"\
    \ class=\"anchor\" href=\"#current-status-and-gallery\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Current Status\
    \ and Gallery</h2>\n<p>Currently the project is in a very early stage. Nothing\
    \ is set in stone yet, I mostly prototyping ideas and techniques.</p>\n<p>List\
    \ of developed features:</p>\n<ul>\n<li>[x] Asset file loading (grf, gnd, rsm,\
    \ rsw, spr, act, str)</li>\n<li>[x] Rendering\n<ul>\n<li>[x] Map (ground, static\
    \ models, lighting)</li>\n</ul>\n</li>\n<li>[x] Sprites for UI\n<ul>\n<li>[x]\
    \ Sprites in 3D world (animated sprites and effects as well)\n<ul>\n<li>[x] Different\
    \ actions (idle, sit, walk, attack, cast etc)</li>\n<li>[x] Directions</li>\n\
    </ul>\n</li>\n</ul>\n</li>\n<li>[x] Amount of damages, heals, etc\n<ul>\n<li>[x]\
    \ Health and Mana bar above the characters</li>\n</ul>\n</li>\n<li>[x] Input handling,\
    \ Control\n<ul>\n<li>[x] Moving around with your character</li>\n<li>[x] Assigning\
    \ skills to Q, W, E, R, etc keys</li>\n<li>[x] Continuous movement towards the\
    \ mouse if RMB is down</li>\n</ul>\n</li>\n<li>[x] Skills\n<ul>\n<li>[x] Skill\
    \ target area/entity selection mode</li>\n<li>[x] Skill casting</li>\n<li>[x]\
    \ Skill manifestations (the manifested outcome of using a skill, e.g. a fire wall\
    \ in the 3D world which can't be walk through and it damages contacting entities)</li>\n\
    <li>[x] Quick cast settings (on, on-release, off)</li>\n</ul>\n</li>\n<li>[x]\
    \ Battle\n<ul>\n<li>[x] Attacking an enemy</li>\n<li>[x] Attack speed</li>\n<li>[x]\
    \ Health, dying</li>\n</ul>\n</li>\n<li>[x] Collision\n<ul>\n<li>[x] Static objects</li>\n\
    <li>[x] Characters, a.k.a <a href=\"https://www.youtube.com/watch?v=nk2O6YsCWwI\"\
    \ rel=\"nofollow\">body block</a>\n</li>\n</ul>\n</li>\n</ul>\n<p>\n<a href=\"\
    readme_assets/body_blocking.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img width=\"300\" src=\"readme_assets/body_blocking.gif\" title=\"Body blocking\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/normal_aspd.gif\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/normal_aspd.gif\"\
    \ title=\"Normal attack\" style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/quick_aspd.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/quick_aspd.gif\"\
    \ title=\"Quick attack\" style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/heal.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/heal.gif\"\
    \ title=\"Heal\" style=\"max-width:100%;\"></a>\n<a href=\"readme_assets/aoe.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"300\" src=\"readme_assets/aoe.gif\"\
    \ title=\"AoE skill\" style=\"max-width:100%;\"></a>\n</p>\n<h2>\n<a id=\"user-content-background-story\"\
    \ class=\"anchor\" href=\"#background-story\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Background story</h2>\n<p>I play\
    \ computer games rarely, but when I do, I play 1 or 2 sessions of Heroes of The\
    \ Storm match.</p>\n<p>But still, whenever I play, I am constantly thinking about\
    \ how I would implement some mechanics of the game.</p>\n<p>So finally I reached\
    \ the point where fantasizing is not enough anymore, and wanted to actually try\
    \ myself in this area as well.</p>\n<p>Don't be surprised if the game is heavily\
    \ inspired by HoTS, most probably the playable character styles and skills will\
    \ be based on my favourite characters from it, or the ones whose skill mechanics\
    \ I find challenging or interesting.</p>\n<h3>\n<a id=\"user-content-about-the-used-game-assets\"\
    \ class=\"anchor\" href=\"#about-the-used-game-assets\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>About the used\
    \ game assets</h3>\n<p>The visuals of Rustarok might be familiar to you. It is\
    \ because the game uses assets from an existing game, an older popular Korean\
    \ MMORPG, Ragnarok Online. The reasons I used them:</p>\n<ul>\n<li>\n<p>I am not\
    \ a graphic designer, I don't have the skills nor the temptation to create the\
    \ visuals of a game myself.</p>\n</li>\n<li>\n<p>Again, my primary goal is to\
    \ experiment, learn and have fun while <strong>developing</strong> something challenging.</p>\n\
    </li>\n<li>\n<p>I am in love with the unique 2D/3D graphic style of the game.</p>\n\
    </li>\n<li>\n<p>Ragnarok Online game asset file structures are known and there\
    \ are example implementations for processing them.</p>\n</li>\n<li>\n<p>Ragnarok\
    \ Online had a huge impact on me when I was younger. I played a lot with it, this\
    \ might have been the only game I was obsessed with.</p>\n<p>Thanks to it, I know\
    \ all the skills, sprites, maps, models etc, which is useful when I try to come\
    \ up with visualities of some new skill.</p>\n</li>\n<li>\n<p>The server code\
    \ of Ragnarok Online has been exposed for a very long time. That was the first\
    \ professional C source code I studied, hacked and even fixed when I was around\
    \ 14-15, so it had a huge impact on me as a software developer.</p>\n</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Thanks</h2>\n\
    <ul>\n<li>\n<a href=\"https://github.com/vthibault/roBrowser/\">roBrowser</a>:\
    \ Its source code was useful for decoding the game asset files</li>\n</ul>\n"
  stargazers_count: 224
  subscribers_count: 13
  topics:
  - rust
  - opengl
  - ragnarok
  - moba
  - game
  - multiplayer
  - 2d
  - 3d
  updated_at: 1620891423.0
bobeobibo/phigaro:
  data_format: 2
  description: Phigaro is a scalable command-line tool for predicting phages and prophages
  filenames:
  - Singularity
  full_name: bobeobibo/phigaro
  latest_release: v2.2.6
  readme: "<h1>\n<a id=\"user-content-phigaro-v230\" class=\"anchor\" href=\"#phigaro-v230\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Phigaro v2.3.0</h1>\n<p><a href=\"https://badge.fury.io/py/phigaro\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b9bcb6f0446a21e8f918a9a8253f32a15df7cc3df72ced3bcd42d9f23bbc993b/68747470733a2f2f62616467652e667572792e696f2f70792f7068696761726f2e737667\"\
    \ alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/phigaro.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/bfa41b5d9d74183c62a1e89d4718527f319c054e8090c55ce7837c88f19e5350/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f7068696761726f2f6261646765732f696e7374616c6c65722f636f6e64612e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/bfa41b5d9d74183c62a1e89d4718527f319c054e8090c55ce7837c88f19e5350/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f7068696761726f2f6261646765732f696e7374616c6c65722f636f6e64612e737667\"\
    \ alt=\"Conda installation\" data-canonical-src=\"https://anaconda.org/bioconda/phigaro/badges/installer/conda.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/bobeobibo/phigaro/workflows/Phigaro%20Tests/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/bobeobibo/phigaro/workflows/Phigaro%20Tests/badge.svg\"\
    \ alt=\"Actions Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/5fab2edf3816ef9fb3ebcaf6e613fa7b40ff7652ec69e5f6e7f695aa24bf5ce6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5fab2edf3816ef9fb3ebcaf6e613fa7b40ff7652ec69e5f6e7f695aa24bf5ce6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\"\
    \ alt=\"License: MIT\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/psf/black\"><img\
    \ src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"\
    \ alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Phigaro is a standalone command-line\
    \ application that is able to detect prophage regions taking raw genome and metagenome\
    \ assemblies as an input. It also produces dynamic annotated \u201Cprophage genome\
    \ maps\u201D and marks possible transposon insertion spots inside prophages. It\
    \ is applicable for mining prophage regions from large metagenomic datasets.</p>\n\
    <h2>\n<a id=\"user-content-updates-tracker\" class=\"anchor\" href=\"#updates-tracker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Updates tracker</h2>\n<p>You can find the information about updates\
    \ and releases by <a href=\"https://github.com/bobeobibo/phigaro/blob/master/version_tracker.md\"\
    >link.</a></p>\n<h2>\n<a id=\"user-content-documentation-installation--usage\"\
    \ class=\"anchor\" href=\"#documentation-installation--usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Documentation:\
    \ Installation &amp; Usage</h2>\n<p>Please, follow <a href=\"https://phigaro.readthedocs.io/\"\
    \ rel=\"nofollow\">the documentation link</a> to find installation and usage information.</p>\n\
    <h2>\n<a id=\"user-content-methods-overview\" class=\"anchor\" href=\"#methods-overview\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Methods overview</h2>\n<p>Open-reading frames (i.e. proteins) are\
    \ predicted from the input FASTA file using Prodigal. Phage genes are annotated\
    \ with prokaryotic viral orthologous groups (pVOGs) profile Hidden Markov Models\
    \ (HMMs), which can be downloaded stand-alone from <a href=\"http://dmk-brain.ecn.uiowa.edu/pVOGs/\"\
    \ rel=\"nofollow\">http://dmk-brain.ecn.uiowa.edu/pVOGs/</a>. Each contig is represented\
    \ as a sequence of phage and non-phage genes. A smoothing window algorithm (a\
    \ triangular window function) determines regions with a high density of phage\
    \ genes and therefore the prophage regions and boundaries, considering the pVOG\
    \ annotations and the GC content.</p>\n<h2>\n<a id=\"user-content-known-issues\"\
    \ class=\"anchor\" href=\"#known-issues\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Known issues</h2>\n<p>Phigaro\
    \ is tested on Linux systems. For MacOS, you may need to add the following softlink\
    \ <code>ln -s /usr/libexec/locate.updatedb /usr/local/bin/updated</code> and run\
    \ <code>brew install wget</code>. If you encounter any issues while running Phigaro\
    \ on test data, please report them to us at <a href=\"mailto:estarikova@rcpcm.org\"\
    >estarikova@rcpcm.org</a>.</p>\n<h2>\n<a id=\"user-content-publication\" class=\"\
    anchor\" href=\"#publication\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Publication</h2>\n<p>Elizaveta V.\
    \ Starikova, Polina O. Tikhonova, Nikita A. Prianichnikov, Chris M. Rands, Evgeny\
    \ M. Zdobnov, Vadim M. Govorun <br>Phigaro: high throughput prophage sequence\
    \ annotation</p>\n<ul>\n<li>Bioinformatics, 2020; doi: <a href=\"https://doi.org/10.1093/bioinformatics/btaa250\"\
    \ rel=\"nofollow\">https://doi.org/10.1093/bioinformatics/btaa250</a>\n</li>\n\
    <li>bioRxiv, 2019; doi: <a href=\"https://doi.org/10.1101/598243\" rel=\"nofollow\"\
    >https://doi.org/10.1101/598243</a>\n</li>\n</ul>\n<p>(c) E.Starikova, P. Tikhonova,\
    \ N.Pryanichnikov, 2019</p>\n"
  stargazers_count: 25
  subscribers_count: 3
  topics:
  - bioinformatics
  - bioinformatics-tool
  - bioinformatics-pipeline
  - bioinformatics-analysis
  - biological-data-analysis
  - phage
  - phages
  - prophage
  - genomics
  - genomic-data-analysis
  - genomic-regions
  updated_at: 1622668825.0
buregab/openspiel_singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - other_images/Singularity.custom_openspiel
  full_name: buregab/openspiel_singularity
  latest_release: null
  readme: '<p>For building openspiel singularity containers.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604380357.0
chenhongluo/horovord:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: chenhongluo/horovord
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ class=\"anchor\" href=\"#singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity container for molecular electrostatic calculations using\
    \ PDB2PQR/APBS and Brownian dynamics with BrownDye.</h1>\n<p>This singularity\
    \ image contains a complete software environment for running <a href=\"http://browndye.ucsd.edu/\"\
    \ rel=\"nofollow\">BrownDye (version 1 and 2)</a> simulations. It also includes\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">PDB2PQR</a> and\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">APBS</a>.</p>\n\
    <p>Please <a href=\"http://eepurl.com/by4eQr\" rel=\"nofollow\">register</a> your\
    \ use of APBS and PDB2PQR.</p>\n<p>The image has been verified to work on XSEDE\
    \ <a href=\"https://portal.xsede.org/sdsc-comet\" rel=\"nofollow\">comet</a> and\
    \ <a href=\"https://www.sdsc.edu/support/user_guides/tscc-quick-start.html\" rel=\"\
    nofollow\">TSCC</a> shared cluster at SDSC. It will automatically bind <code>/cvmfs</code>\
    \ <code>/oasis</code> <code>/projects</code> <code>/scratch</code> directories,\
    \ if available on the host.</p>\n<h2>\n<a id=\"user-content-using-the-container\"\
    \ class=\"anchor\" href=\"#using-the-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using the container</h2>\n<p>Pull\
    \ the singularity image:</p>\n<pre><code>singularity pull shub://nbcrrolls/electrostatics-singularity\n\
    </code></pre>\n<p>Start bash shell in the container:</p>\n<pre><code>singularity\
    \ shell nbcrrolls-electrostatics-singularity-master-latest.simg\n</code></pre>\n\
    <p>Now the container is running and we can start a BrownDye2 job (using the Thrombin\
    \ example):</p>\n<pre><code>module load browndye2\ncp -ai $BD2_PATH/examples/thrombin\
    \ .\ncd thrombin\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n_trajectories&gt; 10000\
    \ /&lt;n_trajectories&gt; 1000 /' t_m_simulation.xml.bak\nmake all # takes about\
    \ min to run\nmodule unload browndye2\n</code></pre>\n<p>And if you want to use\
    \ BrownDye version 1:</p>\n<pre><code>module load browndye1\ncp -ai $BD1_PATH/thrombin-example\
    \ .\ncd thrombin-example\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n-trajectories&gt;\
    \ 10000 /&lt;n-trajectories&gt; 1000 /' input.xml.bak # limit the number of calculated\
    \ trajectories\nmake all\nbd_top input.xml\nnam_simulation t-m-simulation.xml\
    \ # this takes about 3 min to run\ncat results.xml\nmodule unload browndye1\n\
    </code></pre>\n<p>After we are finished we can quit the container:</p>\n<pre><code>exit\n\
    </code></pre>\n<p>You can also access individual applications from the electrostatics\
    \ container.</p>\n<p>To list available applications:</p>\n<pre><code>$ singularity\
    \ apps nbcrrolls-electrostatics-singularity-master-latest.simg \napbs\npdb2pqr\n\
    nam_simulation\nwe_simulation\n</code></pre>\n<p>To run, for example, apbs calculation:</p>\n\
    <pre><code>singularity exec nbcrrolls-electrostatics-singularity-master-latest.simg\
    \ apbs input.in\n</code></pre>\n<p>or</p>\n<pre><code>singularity run --app apbs\
    \ nbcrrolls-electrostatics-singularity-master-latest.simg input.in\n</code></pre>\n\
    <p>This Singularity image is hosted on Singularity Hub: <a href=\"https://singularity-hub.org/collections/2497\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h6>\n<a id=\"user-content-this-project-is-supported-by-nbcr\"\
    \ class=\"anchor\" href=\"#this-project-is-supported-by-nbcr\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>This\
    \ project is supported by <a href=\"http://nbcr.ucsd.edu\" rel=\"nofollow\">NBCR</a>.</h6>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1553181936.0
ckrilow/dev-ckrilow:
  data_format: 2
  description: null
  filenames:
  - pipelines/0025-qc_cluster/env/Singularity.sc_qc_cluster
  - pipelines/0037-cell_cell_interaction/env/Singularity.cell_cell_interaction
  - pipelines/0015-preprocessing/env/Singularity.preprocessing
  full_name: ckrilow/dev-ckrilow
  latest_release: null
  readme: '<h1>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h1>

    <p>This README is pulled from a default template for workflows.</p>

    <h1>

    <a id="user-content-workflow-template-setup" class="anchor" href="#workflow-template-setup"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Workflow
    template setup</h1>

    <h2>

    <a id="user-content-lib" class="anchor" href="#lib" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>lib</h2>

    <ul>

    <li>The <code>lib</code> directory contains general libraries that may be referenced
    by multiple workflows, for instance cromwell configs and python configs. Currently
    nothing in this directory is used.</li>

    </ul>

    <h2>

    <a id="user-content-pipelines" class="anchor" href="#pipelines" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pipelines</h2>

    <ul>

    <li>

    <p>Each pipeline is a full analysis. Think of it like the heading of a methods
    section in a paper. For instance if this were genetic summary statistics workflow,
    a pipeline might be "fine-mapping" that does both conditional and credible set
    analysis. Another pipeline may be "colocalization".</p>

    </li>

    <li>

    <p>Pipelines may have numbers prior to their name (e.g., <code>example_pipeline_1</code>
    to <code>0025-example_pipeline_1</code>). These numbers do not mean anything,
    but merely used to keep pipelines in their general order of execution. These are
    optional.</p>

    </li>

    <li>

    <p>A pipeline consists of :</p>

    </li>

    </ul>

    <ol>

    <li>A workflow.</li>

    <li>A <code>scripts</code> directory with <em>all</em> scripts referenced by that
    workflow (unless a general lib script is called). Scripts may have numbers prior
    to their name. These numbers do not mean anything, but merely used to keep scripts
    in their general order of execution. These are optional.</li>

    <li>A <code>docs</code> directory that contains a documentation of the default
    parameters written in a style that is publishable as methods in a paper (including
    citations). Within the <code>docs</code> directory there may be a <code>reference</code>
    with any additional reference materials.</li>

    <li>An <code>example_runtime_setup</code> directory contains files that give an
    example of actual config files and any other files used to run the pipeline.</li>

    </ol>

    <h2>

    <a id="user-content-studies" class="anchor" href="#studies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>studies</h2>

    <ul>

    <li>A studies directory should either exist within the workflow repo or be a separate
    repo that has the same name as the workflow repo, but with <code>studies</code>
    appended to it (e.g. <code>template-workflow</code> becomes <code>template-workflow-studies</code>).</li>

    <li>If there is a standard set of plots that will always look the same way, a
    pipeline should generate such plots. Otherwise, all code to analyze the results
    of a pipeline run should be in the <code>studies</code> directory. For instance
    if this were genetic summary statistics workflow, <code>studies</code> may contain
    a <code>t2d</code> directory and a <code>weight</code> directory.</li>

    <li>Within a study is either an Jupyter notebook (either python or R kernel) or
    an R markdown file. Nearly all plots / analysis of the results of running the
    various pipelines should be done in the notebook / markdown file.</li>

    <li>A study may also contain a scripts directory with scripts to aggregate data
    for a one off analysis (if the analysis is going to be repeated, consider making
    a new pipeline or adding it to an existing pipeline) or for special plots that
    cannot be done in the notebook / markdown file.</li>

    </ul>

    <h1>

    <a id="user-content-new-workflow-reminders" class="anchor" href="#new-workflow-reminders"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>New
    workflow reminders</h1>

    <ul>

    <li>[ ] Documentation</li>

    <li>[ ] Environment version control</li>

    <li>[ ] Pipeline version control</li>

    <li>[ ] Git branches</li>

    <li>[ ] Code review</li>

    </ul>

    <h1>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h1>

    <p>Be sure to document your code!</p>

    <h1>

    <a id="user-content-environment-version-control" class="anchor" href="#environment-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment
    version control</h1>

    <p>Analysis environment is controlled using conda. Each pipeline should have an
    <code>environment.yml</code> file with all of the packages used. If a required
    package or library is missing from conda (and therefore not in the <code>environment.yml</code>),
    it should be noted in the <code>README.md</code> of the pipeline.</p>

    <div class="highlight highlight-source-shell"><pre>conda env <span class="pl-k">export</span>
    --no-builds <span class="pl-k">|</span> grep -v prefix <span class="pl-k">|</span>
    grep -v name <span class="pl-k">&gt;</span> environment.yml</pre></div>

    <h1>

    <a id="user-content-pipeline-version-control" class="anchor" href="#pipeline-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    version control</h1>

    <p>Each pipeline within this workflow uses <a href="https://pypi.org/project/bumpversion"
    rel="nofollow">bumpversion</a> for automatic <a href="https://semver.org" rel="nofollow">semantic
    versioning</a>.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    bump the appropriate increment</span>

    bumpversion patch --verbose --dry-run

    bumpversion minor --verbose --dry-run

    bumpversion major --verbose --dry-run


    <span class="pl-c"><span class="pl-c">#</span> commit with tags</span>

    git push --tags</pre></div>

    <h1>

    <a id="user-content-github-forks" class="anchor" href="#github-forks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GitHub forks</h1>

    <p>Forking the repository allows developers to work independently while retaining
    well-maintained code on the master fork. For instructions on how to fork, follow
    the <a href="https://help.github.com/en/articles/fork-a-repo">Fork a repo</a>
    instructions.</p>

    <p>After forking the repo, clone the repo to your local desktop:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    to use SSH</span>

    git clone git@github.com:<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git


    <span class="pl-c"><span class="pl-c">#</span> to use Https</span>

    git clone https://github.com/<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git</pre></div>

    <p>This creates a replica of the remote repository on your local desktop. <em>Note</em>:
    When you create your local repository, it will also make a local clone of the
    remote repository (typically as <code>origin</code>). So, your local master branch
    would simply be <code>master</code>. But, your remote master branch will be <code>origin/master</code>.
    You can also add multiple remote repositories. For instance, let us say our main
    repository is under the remote repository <code>my_repo</code>. We will want to
    add it as a remote repository, so we can fetch the most up-to-date code. You could
    add it by:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Add the my_repo remote repo to your local desktop -- this will allow you to pull
    and push to branches on the my_repo repository</span>

    git remote add my_repo git@github.com:my_repo/template-workflow.git</pre></div>

    <h1>

    <a id="user-content-git-branches" class="anchor" href="#git-branches" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Git branches</h1>

    <p>Branching is how git actually tracks code development. For more information,
    see the <a href="https://www.atlassian.com/git/tutorials/using-branches" rel="nofollow">Git
    Branch Tutorial</a> on Atlassian. If you want to add a new feature, pipeline,
    or fix a bug, a common work flow would look like this:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Update your local copy of the master branch to make sure you are getting the most
    up-to-date code</span>

    git pull


    <span class="pl-c"><span class="pl-c">#</span> Create the branch on your local
    machine and switch in this branch </span>

    git checkout -b [name_of_your_new_branch]


    <span class="pl-c"><span class="pl-c">#</span> Push the branch on github</span>

    git push origin [name_of_your_new_branch]</pre></div>

    <p>As you develop, you want to commit your work to your branch, so you don''t
    lose it all if something happens!</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Confirm we''re on the right branch</span>

    git branch -a


    <span class="pl-c"><span class="pl-c">#</span> Add all your work to be tracked
    (Note: there are many ways to add specific files, etc. See https://git-scm.com/docs/git-add
    for more information). The following command adds everything in your currently
    directory.</span>

    git add <span class="pl-c1">.</span>


    <span class="pl-c"><span class="pl-c">#</span> Commit your work to the branch
    with a message describing what''s in the commit</span>

    git commit -m <span class="pl-s"><span class="pl-pds">"</span>Created the scATAC-seq
    pipeline!<span class="pl-pds">"</span></span>


    <span class="pl-c"><span class="pl-c">#</span> You can add a -u parameter to set-upstream
    for a push</span>

    <span class="pl-c"><span class="pl-c">#</span> Alternatively, git will also automatically
    query you when you do your first push.</span>

    <span class="pl-c"><span class="pl-c">#</span> You can also set this manually
    by adding a new remote for your branch:</span>

    <span class="pl-c"><span class="pl-c">#</span>git remote add [name_of_your_remote]
    [name_of_your_new_branch]</span>


    <span class="pl-c"><span class="pl-c">#</span> Here is another push where we specify
    HEAD</span>

    <span class="pl-c"><span class="pl-c">#</span>git push origin HEAD # HEAD pushes
    everything up to the most recent commit</span></pre></div>

    <h1>

    <a id="user-content-code-review" class="anchor" href="#code-review" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Code review</h1>

    <p>Create a <a href="https://help.github.com/en/articles/creating-a-pull-request">GitHub
    Pull Request</a>. A PR allows other developers a chance to go through and comment
    on lines of code they believe can be improved. In addition, it will tell you if
    the code you are trying to merge into the <code>my_repo</code> branch actually
    conflicts with code that already exists in the branch, so you don''t overwrite
    someone else''s work.</p>

    <p>Once another developer approves the PR, you have the go-ahead to merge your
    code! Congrats, you finished your feature!</p>

    <p><em>Note</em>: There are some cases where you may just want to push directly
    to the my_repo fork, thereby avoiding code reviews. For instance, if you''re working
    on a one-off project that you want people to be able to see, but no one else is
    necessarily working on, you can always push directly to the branches on my_repo
    fork. Or, you could also still go through the steps of a PR, but simply merge
    your own code without CR.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1596817893.0
clemsonciti/singularity-images:
  data_format: 2
  description: Scripts for building Singularity images
  filenames:
  - tensorflow/ubuntu.def
  - mxnet/ubuntu.def
  - caffe2/ubuntu.def
  - dl/ubuntu.def
  - circuitscape/ubuntu.def
  - caffe/ubuntu.def
  full_name: clemsonciti/singularity-images
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-image-scripts" class="anchor" href="#singularity-image-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    image scripts</h1>

    <p>Scripts to generate singularity images

    for running different software on Palmetto cluster.</p>

    '
  stargazers_count: 10
  subscribers_count: 5
  topics: []
  updated_at: 1597386388.0
cmaumet/nipype_tutorial:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: cmaumet/nipype_tutorial
  latest_release: null
  readme: '<h1>

    <a id="user-content-nipype-tutorial-notebooks" class="anchor" href="#nipype-tutorial-notebooks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nipype
    Tutorial Notebooks</h1>

    <p><a href="https://circleci.com/gh/miykael/nipype_tutorial/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64"
    alt="CircleCi" data-canonical-src="https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/issues/"><img src="https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/pulls/"><img src="https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub pull-requests" data-canonical-src="https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/"><img
    src="https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/commits/master"><img src="https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub Commits" data-canonical-src="https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/archive/master.zip"><img src="https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub size" data-canonical-src="https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/miykael/nipype_tutorial/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000"
    style="max-width:100%;"></a>

    <a href="http://hits.dwyl.io/miykael/nipype_tutorial" rel="nofollow"><img src="https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub HitCount" data-canonical-src="http://hits.dwyl.io/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a></p>

    <p>This is the Nipype Tutorial in Jupyter Notebook format. You can access the
    tutorial in two ways:</p>

    <ol>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/" rel="nofollow">Nipype Tutorial
    Homepage</a>: This website contains a static, read-only version of all the notebooks.</li>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html"
    rel="nofollow">Nipype Tutorial Docker Image</a>: This guide explains how to use
    Docker to run the notebooks interactively on your own computer. The nipype tutorial
    docker image is the best interactive way to learn Nipype.</li>

    </ol>

    <h1>

    <a id="user-content-feedback-help--support" class="anchor" href="#feedback-help--support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feedback,
    Help &amp; Support</h1>

    <p>If you want to help with this tutorial or have any questions, feel free to
    fork the repo of the <a href="https://github.com/miykael/nipype_tutorial">Notebooks</a>
    or interact with other contributors on the slack channel <a href="https://brainhack.slack.com/messages/nipype/"
    rel="nofollow">brainhack.slack.com/messages/nipype/</a>. If you have any questions
    or found a problem, open a new <a href="https://github.com/miykael/nipype_tutorial/issues">issue
    on github</a>.</p>

    <h1>

    <a id="user-content-thanks-and-acknowledgment" class="anchor" href="#thanks-and-acknowledgment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thanks
    and Acknowledgment</h1>

    <p>A huge thanks to <a href="https://github.com/mwaskom">Michael Waskom</a>, <a
    href="https://github.com/oesteban">Oscar Esteban</a>, <a href="https://github.com/chrisfilo">Chris
    Gorgolewski</a> and <a href="https://github.com/satra">Satrajit Ghosh</a> for
    their input to this tutorial! And a huge thanks to <a href="https://github.com/djarecka/">Dorota
    Jarecka</a> who updated this tutorial to Python 3 and is helping me with keeping
    this tutorial updated and running!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1538064645.0
cokelaer/damona:
  data_format: 2
  description: 'singularity environment manager for NGS pipelines '
  filenames:
  - damona/recipes/phantompeakqualtools/Singularity.phantompeakqualtools_1.2.2
  - damona/recipes/bcl2fastq/Singularity.bcl2fastq_2.20.0
  - damona/recipes/rnaseqc/Singularity.rnaseqc_2.35.0
  - damona/recipes/gffread/Singularity.gffread_0.12.1
  - damona/recipes/fastqc/Singularity.fastqc_0.11.9
  - damona/recipes/fastqc/Singularity.fastqc_0.11.8
  - damona/recipes/damona/Singularity.damona_0.3.0
  - damona/recipes/damona/Singularity.damona_0.4.2
  - damona/recipes/R/Singularity.R_3.6.3
  - damona/recipes/R/Singularity.R_4.0.2
  - damona/recipes/canu/Singularity.canu_1.6.0
  - damona/recipes/canu/Singularity.canu_1.8.0
  - damona/recipes/salmon/Singularity.salmon_1.3.0
  - damona/recipes/art/Singularity.art_3.11.14
  - damona/recipes/fitter/Singularity.fitter_1.3.0
  - damona/recipes/rnadiff/Singularity.rnadiff_1.7.1
  - damona/recipes/ucsc/Singularity.ucsc_0.1.0
  - damona/recipes/kraken/Singularity.kraken_2.0.9
  - damona/recipes/kraken/Singularity.kraken_1.1
  - damona/recipes/rtools/Singularity.Rtools_1.0.0
  - damona/recipes/rtools/Singularity.Rtools_1.1.0
  - damona/recipes/minimap2/Singularity.minimap2_2.17.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.11.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.10.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.9.0
  - damona/recipes/sequana_perl_tools/Singularity.sequana_perl_tools_0.1.0
  - damona/recipes/conda/Singularity.conda_4.7.12
  - damona/recipes/conda/Singularity.conda_4.9.2
  - damona/recipes/trf/Singularity.trf_4.10.0
  - damona/recipes/trf/Singularity.trf_4.09
  - damona/recipes/prokka/Singularity.prokka_1.14.5
  - damona/recipes/falco/Singularity.falco_0.2.1
  - damona/recipes/graphviz/Singularity.graphviz_2.43.0
  full_name: cokelaer/damona
  latest_release: v0.4.3
  readme: '<h1>

    <a id="user-content-hpc-singularity" class="anchor" href="#hpc-singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>hpc-singularity</h1>

    <p>Singularity for HPC</p>

    <p>Make sure the sigularity is built on <a href="https://singularity-hub.org"
    rel="nofollow">https://singularity-hub.org</a></p>

    <p>if ready use:</p>

    <p><code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers3.0.2-imagecrawl</code></p>

    <p>Transformer 2.11.0:

    <code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers2.11.0-imagecrawl</code></p>

    <p>Make sure the imagecrawl is updated (latest commit)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1620052198.0
cokelaer/graphviz4all:
  data_format: 2
  description: dot and other graphviz executable in a simple singularity container
  filenames:
  - singularity/Singularity.v1
  full_name: cokelaer/graphviz4all
  latest_release: null
  readme: '<h1>

    <a id="user-content-graphviz4all" class="anchor" href="#graphviz4all" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>graphviz4all</h1>

    <p><strong>DEPRECATED, Aug 2020</strong>: This is now part of <a href="https://damona.readthedocs.io"
    rel="nofollow">https://damona.readthedocs.io</a> project.</p>

    <pre><code>damona install graphviz

    </code></pre>

    <p>A container with graphviz (<a href="http://www.graphviz.org/" rel="nofollow">http://www.graphviz.org/</a>)
    executables (dot, circo, etc).</p>

    <p>This is for Singularity 2.4 at least and is available on singularity-hub</p>

    <pre><code>singularity pull --name graphviz.img shub://cokelaer/graphviz4all:v1

    </code></pre>

    <p>Conversion of the dot file into SVG conterpart:</p>

    <pre><code>./graphviz.img dot -Tsvg test.dot -o test.svg

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - dot
  - circo
  - graphviz
  - singularity
  updated_at: 1597173467.0
cokelaer/pacbio4all:
  data_format: 2
  description: pacbio tools
  filenames:
  - singularity/Singularity.v2
  - singularity/Singularity.v3
  - singularity/Singularity.v1
  full_name: cokelaer/pacbio4all
  latest_release: null
  readme: '<h1>

    <a id="user-content-pacbio4all" class="anchor" href="#pacbio4all" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pacbio4all</h1>

    <p>A container with some of the pacbio tools. This is for Singularity 2.4 at least
    !</p>

    <p>::</p>

    <pre><code>singularity pull --name pacbio.img shub://cokelaer/pacbio4all:v2

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1508516491.0
cschu/dada2_container:
  data_format: 2
  description: null
  filenames:
  - Singularity.latest
  full_name: cschu/dada2_container
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-create-study-specific-roi-image-in-mni-space\"\
    \ class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Create\
    \ study-specific ROI image in MNI space</h1>\n<p>PMAT resting state connectivity\
    \ study.</p>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Inputs:</h2>\n<p>All should be matched to the same T1 image.</p>\n\
    <ul>\n<li>T1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)</li>\n\
    <li>Deformation from T1 subject space to atlas space (typically DEF_FWD resource\
    \ of cat12 assessor)</li>\n<li>SUBJECT directory of Freesurfer output (typically\
    \ SUBJECT resource of freesurfer_dev assessor)</li>\n<li>Temporal lobe segmentation\
    \ (typically SEG resource of Temporal_Lobe assessor)</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n\
    <pre><code>rois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv\
    \        Region labels and volumes\nmakerois-PMAT.pdf           Visual report\
    \ of final ROI image\n</code></pre>\n<h2>\n<a id=\"user-content-regions-of-interest\"\
    \ class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Regions of interest</h2>\n<h3>\n\
    <a id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spheres (atlas space)</h3>\n<p>Source: <em>Libby LA, Ekstrom AD, Ragland\
    \ JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal\
    \ cortices within human hippocampal subregions revealed by high-resolution functional\
    \ imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012</em></p>\n\
    <p>Method: <em>Schr\xF6der TN, Haak K V., Jimenez NIZ, et al. Functional topography\
    \ of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"\
    #entorhinal-cortex-atlas-space\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Entorhinal cortex (atlas space)</h3>\n\
    <p>Anterior lateral and posterior medial sections. Source and method: <em>Schr\xF6\
    der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal\
    \ cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\"\
    \ href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Temporal lobe (Subject space,\
    \ warped)</h3>\n<p>Head for anterior hippocampus; body and tail combined for posterior\
    \ hippocampus. Method: <em>Plassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale\
    \ Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE\
    \ Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017\
    \ Feb 24. PMID: 28781411; PMCID: PMC5544133.</em></p>\n<h3>\n<a id=\"user-content-parahippocampal-perirhinal-subject-space-warped\"\
    \ class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parahippocampal,\
    \ perirhinal (Subject space, warped)</h3>\n<p>Generated by Freesurfer 6. Parahippocampal\
    \ (1016,2016) and perirhinal (surface patch resampled to volume, overlap with\
    \ parahippocampus was assigned to perirhinal). Method: <em>Bruce Fischl, Andre\
    \ van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H.\
    \ Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne\
    \ Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating\
    \ the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.</em></p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1618484418.0
csiro-crop-informatics/nextflow-embl-abr-webinar:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: csiro-crop-informatics/nextflow-embl-abr-webinar
  latest_release: v1.2
  readme: "<p>This repository contains information for the EMBL-ABR webinar on \"\
    Nextflow: Scalable, Sharable and Reproducible Computational Workflows across Clouds\
    \ and Clusters\" presented by Rad Suchecki on 14th March 2019.</p>\n<h1>\n<a id=\"\
    user-content-webinar-details\" class=\"anchor\" href=\"#webinar-details\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Webinar\
    \ details</h1>\n<p><strong>Abstract:</strong>\nLarge analysis workflows are fragile\
    \ ecosystems of software tools, scripts and dependencies. This complexity commonly\
    \ makes these workflows not only irreproducible but sometimes even not re-runnable\
    \ outside their original development environment. Nextflow is a reactive workflow\
    \ framework and a domain specific programming language which follows the dataflow\
    \ paradigm and offers an alternative, and arguably superior, approach to developing,\
    \ executing and sharing pipelines.</p>\n<p>In this webinar we will follow the\
    \ steps required for developing sharable, version controlled, container-backed\
    \ workflows, which can be seamlessly executed across different environments from\
    \ a laptop to cluster to cloud. We will do this by leveraging Nextflow\u2019s\
    \ integration with code and container image hosting services such as GitHub and\
    \ Docker Hub, and out of the box support for various HPC cluster schedulers and\
    \ the Amazon AWS cloud.</p>\n<p><strong>Date/time:</strong> Thursday 14 March\
    \ 2019 13:00-14:00 AEDT /12:00-13:00 AEST</p>\n<p><strong>Presenter:</strong>\
    \ <a href=\"https://orcid.org/0000-0003-4992-9497\" rel=\"nofollow\">Rad Suchecki</a>,\
    \ CSIRO Crop Bioinformatics and Data Science</p>\n<p><a href=\"https://twitter.com/bioinforad\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/059d6c1e6596889bce7982a4745bea213207aae7fa1cd8a3053ed1e6b3f5190f/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f62696f696e666f7261642e7376673f7374796c653d736f6369616c\"\
    \ alt=\"Twitter Follow\" data-canonical-src=\"https://img.shields.io/twitter/follow/bioinforad.svg?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>Registration:</strong> <del><a\
    \ href=\"https://attendee.gotowebinar.com/register/8408436403729692931\" rel=\"\
    nofollow\">https://attendee.gotowebinar.com/register/8408436403729692931</a></del></p>\n\
    <p><strong>Video link:</strong> <a href=\"https://www.youtube.com/channel/UC5WlFNBSfmt3e8Js8o2fFqQ\"\
    \ rel=\"nofollow\">EMBL-ABR YouTube Channel</a></p>\n<p><a href=\"http://www.youtube.com/watch?v=lqm-VV5dOgk\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/72039949253c34d78d283c828b267bdbec41479ad5b39928f41704a9182e4f5c/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6c716d2d565635644f676b2f687164656661756c742e6a7067\"\
    \ alt=\"Nextflow Webinar Video\" data-canonical-src=\"http://img.youtube.com/vi/lqm-VV5dOgk/hqdefault.jpg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>Slides</strong></p>\n<p><a href=\"\
    https://csiro-crop-informatics.github.io/nextflow-embl-abr-webinar/nextflow-embl-abr.html\"\
    \ rel=\"nofollow\">https://csiro-crop-informatics.github.io/nextflow-embl-abr-webinar/nextflow-embl-abr.html</a></p>\n\
    <h1>\n<a id=\"user-content-data-for-the-webinar\" class=\"anchor\" href=\"#data-for-the-webinar\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Data for the Webinar</h1>\n<p>For the purpose of demonstrating a Nextflow\
    \ workflow in reasonable time, we will use the dataset used in <a href=\"https://github.com/UofABioinformaticsHub/2019_EMBL-ABR_Snakemake_webinar#data-for-the-webinar\"\
    >this Snakemake webinar</a>.</p>\n<h1>\n<a id=\"user-content-tutorial\" class=\"\
    anchor\" href=\"#tutorial\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Tutorial</h1>\n<p><a href=\"nextflow-tutorial.md\"\
    >nextflow-tutorial.md</a></p>\n"
  stargazers_count: 5
  subscribers_count: 3
  topics: []
  updated_at: 1568158524.0
ctpelok77/fd-red-black-postipc2018:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ctpelok77/fd-red-black-postipc2018
  latest_release: null
  readme: '<p>Fast Downward is a domain-independent planning system.</p>

    <p>For documentation and contact information see <a href="http://www.fast-downward.org/"
    rel="nofollow">http://www.fast-downward.org/</a>.</p>

    <p>The following directories are not part of Fast Downward as covered by this

    license:</p>

    <ul>

    <li>./src/search/ext</li>

    </ul>

    <p>For the rest, the following license applies:</p>

    <pre><code>Fast Downward is free software: you can redistribute it and/or modify
    it under

    the terms of the GNU General Public License as published by the Free Software

    Foundation, either version 3 of the License, or (at your option) any later

    version.


    Fast Downward is distributed in the hope that it will be useful, but WITHOUT ANY

    WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
    A

    PARTICULAR PURPOSE. See the GNU General Public License for more details.


    You should have received a copy of the GNU General Public License along with

    this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.

    </code></pre>

    '
  stargazers_count: 3
  subscribers_count: 1
  topics: []
  updated_at: 1615360825.0
ctpelok77/kstar:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ctpelok77/kstar
  latest_release: null
  readme: '<h2>

    <a id="user-content-welcome-to-the-page-of-k-planner----a-state-of-the-art-top-k-planner-integrating-the-k-algorithm-into-fast-downward"
    class="anchor" href="#welcome-to-the-page-of-k-planner----a-state-of-the-art-top-k-planner-integrating-the-k-algorithm-into-fast-downward"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome
    to the page of K* planner -- a state of the art Top-k planner integrating the
    K* algorithm into Fast Downward.</h2>

    <h3>

    <a id="user-content-building" class="anchor" href="#building" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Building</h3>

    <pre><code>## Suggested build for 64bit


    ./build.py release64

    </code></pre>

    <h3>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h3>

    <pre><code># ./fast-downward.py --build release64 &lt;domain_file&gt; &lt;problem_file&gt;
    --search "kstar(heuristic,k=&lt;number-of-plans&gt;)"


    ./fast-downward.py --build release64 examples/gripper/domain.pddl examples/gripper/prob01.pddl
    --search "kstar(blind(),k=100)"


    </code></pre>

    <ul>

    <li>

    <em>heurisitic</em>:  any heuristic provided by Fast Downward<br>

    (<a href="http://www.fast-downward.org/Doc/Heuristic" rel="nofollow">http://www.fast-downward.org/Doc/Heuristic</a>).<br>

    <strong>Disclaimer</strong>: Optimality of K* is only guaranteed with an admissible
    and consistent heuristic.</li>

    </ul>

    <h3>

    <a id="user-content-citation" class="anchor" href="#citation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Citation</h3>

    <p>Michael Katz, Shirin Sohrabi, Octavian Udrea and Dominik Winterer<br>

    <strong>A Novel Iterative Approach to Top-k Planning</strong> <a href="https://www.aaai.org/ocs/index.php/ICAPS/ICAPS18/paper/download/17749/16971"
    rel="nofollow">[pdf]</a> <a href="/top_k.bib">[bib]</a><br>

    <em>In ICAPS 2018</em></p>

    <h3>

    <a id="user-content-contact" class="anchor" href="#contact" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h3>

    <p>For questions and comments please get in touch with Michael Katz (<a href="mailto:michael.katz1@ibm.com">michael.katz1@ibm.com</a>).</p>

    '
  stargazers_count: 5
  subscribers_count: 1
  topics: []
  updated_at: 1617563735.0
czbiohub/demux:
  data_format: 2
  description: Demultiplex sequencing experiments with Nextflow
  filenames:
  - Singularity
  full_name: czbiohub/demux
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-coredemux" class="anchor" href="#nf-coredemux" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/demux</h1>

    <p><strong>Demultiplex sequencing experiments</strong></p>

    <p><a href="https://travis-ci.org/nf-core/demux" rel="nofollow"><img src="https://camo.githubusercontent.com/21a4d7a0262a3b096d2521e08bc4e18bf7340ea1b97ea36ca247a7d06ccd04b1/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f64656d75782e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/demux.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/demux" rel="nofollow"><img src="https://camo.githubusercontent.com/8b87efd0f3651289c43d7f37a2a1092964f35f8501ccbcbe7ef15bfc1b38ae67/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f64656d75782e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/demux.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/demux pipeline comes with documentation about the pipeline, found
    in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1562208393.0
czbiohub/nf-core-crisprvar:
  data_format: 2
  description: Run CRISPResso on genome editing experiments
  filenames:
  - Singularity
  full_name: czbiohub/nf-core-crisprvar
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corecrisprvar" class="anchor" href="#nf-corecrisprvar"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/crisprvar</h1>

    <p><strong>Run CRISPResso on genome editing experiments</strong></p>

    <p><a href="https://travis-ci.org/nf-core/crisprvar" rel="nofollow"><img src="https://camo.githubusercontent.com/8bfb8f49f69e465c023fb291cefc2ccbea97c9a7cc04377260fa6052d9370b28/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f6372697370727661722e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/crisprvar.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/crisprvar" rel="nofollow"><img src="https://camo.githubusercontent.com/5e5f9f1b9479b9d19c758902e0a06e81ab060fa6a9207a5e935aee26edc728ac/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f6372697370727661722e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/crisprvar.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/crisprvar pipeline comes with documentation about the pipeline,
    found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1579658690.0
czbiohub/nf-core-test:
  data_format: 2
  description: test of nf-core create
  filenames:
  - Singularity
  full_name: czbiohub/nf-core-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-coretest" class="anchor" href="#nf-coretest" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/test</h1>

    <p><strong>test of nf-core</strong></p>

    <p><a href="https://travis-ci.org/nf-core/test" rel="nofollow"><img src="https://camo.githubusercontent.com/5656ec3ca80ae8775904761dfc7b47e3357d325de15a8d013edd4a0093630611/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f746573742e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/test.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/test" rel="nofollow"><img src="https://camo.githubusercontent.com/8a74c7ad053a343b2d1b30e0ef0f86afe191999cfc823635773862aefd840fd2/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f746573742e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/test.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/test pipeline comes with documentation about the pipeline, found
    in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1554245021.0
czbiohub/nf-large-assembly:
  data_format: 2
  description: Nextflow workflow for assembling large, diploid, eukaryotic genomes
    (2 gigabases haploid size or bigger)
  filenames:
  - Singularity
  full_name: czbiohub/nf-large-assembly
  latest_release: null
  readme: '<h1>

    <a id="user-content-czbiohubnf-large-assembly" class="anchor" href="#czbiohubnf-large-assembly"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>czbiohub/nf-large-assembly</h1>

    <p><strong>Assemble large diploid eukaryotic genomes (2 gigabases haploid size
    or bigger)</strong></p>

    <p><a href="https://travis-ci.org/czbiohub/nf-large-assembly" rel="nofollow"><img
    src="https://camo.githubusercontent.com/8d428dc306e8c519b4952b8239ab3eace188860f1c5dfabe1a4059c42c067a1e/68747470733a2f2f7472617669732d63692e6f72672f637a62696f6875622f6e662d6c617267652d617373656d626c792e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/czbiohub/nf-large-assembly.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/nf-large-assembly" rel="nofollow"><img
    src="https://camo.githubusercontent.com/767f13dee3d8a1039b493b285b876f4ef216154825cb6401031b09e8d959b916/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f6e662d6c617267652d617373656d626c792e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/nf-large-assembly.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The czbiohub/nf-large-assembly pipeline comes with documentation about the
    pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1556036860.0
czbiohub/splicemotifs:
  data_format: 2
  description: Nextflow workflow for finding conserved motifs intersecting with splice
    junctions
  filenames:
  - Singularity
  full_name: czbiohub/splicemotifs
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corebedtools-intersect" class="anchor" href="#nf-corebedtools-intersect"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/bedtools-intersect</h1>

    <p><strong>Intersect lots of bed files with lots of other bed files</strong></p>

    <p><a href="https://travis-ci.org/nf-core/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/811368779316af4f70b4dd35fc2c24cebcc4dc194cd63234e130384ec38ac89f/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f626564746f6f6c732d696e746572736563742e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/bedtools-intersect.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ca7e06b0d2929a9cba14da1892e90c6d4673a695806cb07ea82e89a1cbecef92/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f626564746f6f6c732d696e746572736563742e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/bedtools-intersect.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/bedtools-intersect pipeline comes with documentation about the
    pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1564673719.0
d-w-moore/anjuta_via_singularity:
  data_format: 2
  description: launch the C++ IDE Anjuta from a Singularity container
  filenames:
  - Singularity
  full_name: d-w-moore/anjuta_via_singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-anjuta-ide-via-singularity" class="anchor" href="#anjuta-ide-via-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Anjuta
    IDE via Singularity</h1>

    <p>The container includes libraries for building and debugging  C++

    programs with GCC 9, with C++17 support and Boost libraries. C/Xlib

    applications are also supported.</p>

    <p>To build the container under Singularity ~2.5.1 :</p>

    <ul>

    <li>get <a href="http://sylabs.io" rel="nofollow">Singularity</a> . If you''re
    on Ubuntu/Debian,

    the <a href="https://neuro.debian.net" rel="nofollow">NeuroDebian</a> repo can
    offer the

    most up-to-date Singularity packages</li>

    <li>in a local copy of this repo, use the build command:</li>

    </ul>

    <pre><code>$ sudo singularity build anjuta.simg Singularity

    </code></pre>

    <ul>

    <li>The IDE can be lauched by running anjuta.simg as an executable</li>

    </ul>

    <pre><code>$ ./anjuta.simg

    </code></pre>

    <p>or via the singularity application</p>

    <pre><code>$ singularity run anjuta.simg

    </code></pre>

    <p>To alter an existing image:</p>

    <pre><code>$ sudo singularity build --sandbox anjuta anjuta.simg

    $ sudo singularity shell --writable anjuta

    Singularity&gt; apt update; apt install {custom-packages...}

    Singularity&gt; exit

    $ sudo singularity build anjuta_updated.simg anjuta

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1600000384.0
d-w-moore/new_d2c:
  data_format: 2
  description: null
  filenames:
  - store_pw/Singularity.pw_embed
  - store_pw/Singularity.base-4.2.5
  - store_pw/Singularity.python-4.2.5
  - store_pw/Singularity.pw_encrypt
  - store_pw/Singularity.4.2.5
  - docs/Singularity.3_0.debian9
  - os_recipes/Singularity.centos7
  - os_recipes/Singularity.deboot.ubuntu
  - os_recipes/Singularity.centos6
  - os_recipes/Singularity.archive.debian
  - os_recipes/Singularity.SuSE
  - os_recipes/Singularity.base-4.2.5
  - os_recipes/Singularity.usmirror.debian
  - os_recipes/Singularity.4.2.5
  full_name: d-w-moore/new_d2c
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-installing-and-running-slurm-on-ubuntu-16-or-18\"\
    \ class=\"anchor\" href=\"#installing-and-running-slurm-on-ubuntu-16-or-18\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ and Running SLURM on ubuntu 16 or 18</h1>\n<h2>\n<a id=\"user-content-install-slurm\"\
    \ class=\"anchor\" href=\"#install-slurm\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install SLURM</h2>\n<pre><code>sudo\
    \ apt install slurm-wlm\ngit clone http://github.com/d-w-moore/new_d2c\ncd new_d2c\n\
    perl process_slurm_template.pl  | sudo dd of=/etc/slurm-llnl/slurm.conf\nsudo\
    \ systemctl restart slurmctld slurmd\nsudo systemctl enable  slurmctld slurmd\n\
    </code></pre>\n<p>to test:</p>\n<ul>\n<li>sudo apt install bc</li>\n<li>locate\
    \ command file slurm_install_test.sh containing:</li>\n</ul>\n<pre><code>  #!/bin/bash\n\
    \  bc -l &lt;&lt;&lt;\"scale=4000;a(1)*4\"\n</code></pre>\n<ul>\n<li>run the above\
    \ mentioned test script using : <code>sbatch &lt;script&gt;</code>\n</li>\n<li>type:\
    \ <code>squeue</code> and note the job present (most likely running)</li>\n<li>when\
    \ it disappears from queue (<code>watch -n1 squeue</code>), look for <code>slurm-&lt;JOBNUM&gt;.out</code>\n\
    containing the job's output</li>\n</ul>\n<h2>\n<a id=\"user-content-datacompute-automated-setup---install-irods-hook-scripts-for-slurm-prolog--epilog\"\
    \ class=\"anchor\" href=\"#datacompute-automated-setup---install-irods-hook-scripts-for-slurm-prolog--epilog\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Data/Compute automated setup - install iRODS hook scripts for slurm\
    \ prolog / epilog</h2>\n<p>The following command will setup prolog and epilog\
    \ scripts to be run (pre- and post-,\nrespectively) for each job executed by SLURM:</p>\n\
    <pre><code>sudo ./slurm_hook_setup.sh\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1561308424.0
d-w-moore/singularity-icommands-4.2.1:
  data_format: 2
  description: for singularity biuld
  filenames:
  - Singularity
  full_name: d-w-moore/singularity-icommands-4.2.1
  latest_release: null
  readme: '<h2>

    <a id="user-content-news" class="anchor" href="#news" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>News</h2>

    <h3>

    <a id="user-content-2019-12-20" class="anchor" href="#2019-12-20" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><em><strong>2019-12-20</strong></em>:</h3>

    <ul>

    <li>cisTopic v3: Faster topic modelling based on WarpLDA (see vignettes for details).</li>

    <li>The function runModels() is deprecated. Use runCGSModels() for modelling based
    on Collapsed Gibbs Sampling (equivalent to runModels()), or runWarpLDAModels()
    (for modelling based on WarpLDA).</li>

    <li>Version 2 objects (with or without models) can be used and analyzed with version
    3.</li>

    </ul>

    <h1>

    <a id="user-content-cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    class="anchor" href="#cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cisTopic:
    Probabilistic modelling of cis-regulatory topics from single cell epigenomics
    data</h1>

    <p>cisTopic is an R-package to simultaneously identify cell states and cis-regulatory
    topics from single cell epigenomics data.</p>

    <h2>

    <a id="user-content-dependencies-for-r--35" class="anchor" href="#dependencies-for-r--35"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies
    (for R &lt; 3.5)</h2>

    <p>The following packages have to be installed manually before installing cisTopic:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/RcisTarget<span
    class="pl-pds">"</span></span>)

    <span class="pl-e">devtools</span><span class="pl-k">::</span>install_github(<span
    class="pl-s"><span class="pl-pds">"</span>aertslab/AUCell<span class="pl-pds">"</span></span>)</pre></div>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>For installing and loading cisTopic, run:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/cisTopic<span
    class="pl-pds">"</span></span>)

    library(<span class="pl-smi">cisTopic</span>)</pre></div>

    <h2>

    <a id="user-content-databases" class="anchor" href="#databases" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Databases</h2>

    <p>RcisTarget feather databases are available at <a href="https://resources.aertslab.org/cistarget/"
    rel="nofollow">https://resources.aertslab.org/cistarget/</a>.</p>

    <h2>

    <a id="user-content-tutorials" class="anchor" href="#tutorials" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tutorials</h2>

    <h3>

    <a id="user-content-version-2" class="anchor" href="#version-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 2</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/Runningwithmm10andhg38.html"
    rel="nofollow">Running GREAT and motif enrichment with the mm10 and hg38 genome
    assemblies</a>.</li>

    </ul>

    <h3>

    <a id="user-content-version-3" class="anchor" href="#version-3" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 3</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1527027070.0
d-w-moore/singularity-python-irodsclient:
  data_format: 2
  description: null
  filenames:
  - Singularity.prc-0_8_0
  full_name: d-w-moore/singularity-python-irodsclient
  latest_release: null
  readme: '<h2>

    <a id="user-content-news" class="anchor" href="#news" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>News</h2>

    <h3>

    <a id="user-content-2019-12-20" class="anchor" href="#2019-12-20" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><em><strong>2019-12-20</strong></em>:</h3>

    <ul>

    <li>cisTopic v3: Faster topic modelling based on WarpLDA (see vignettes for details).</li>

    <li>The function runModels() is deprecated. Use runCGSModels() for modelling based
    on Collapsed Gibbs Sampling (equivalent to runModels()), or runWarpLDAModels()
    (for modelling based on WarpLDA).</li>

    <li>Version 2 objects (with or without models) can be used and analyzed with version
    3.</li>

    </ul>

    <h1>

    <a id="user-content-cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    class="anchor" href="#cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cisTopic:
    Probabilistic modelling of cis-regulatory topics from single cell epigenomics
    data</h1>

    <p>cisTopic is an R-package to simultaneously identify cell states and cis-regulatory
    topics from single cell epigenomics data.</p>

    <h2>

    <a id="user-content-dependencies-for-r--35" class="anchor" href="#dependencies-for-r--35"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies
    (for R &lt; 3.5)</h2>

    <p>The following packages have to be installed manually before installing cisTopic:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/RcisTarget<span
    class="pl-pds">"</span></span>)

    <span class="pl-e">devtools</span><span class="pl-k">::</span>install_github(<span
    class="pl-s"><span class="pl-pds">"</span>aertslab/AUCell<span class="pl-pds">"</span></span>)</pre></div>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>For installing and loading cisTopic, run:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">devtools</span><span
    class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>aertslab/cisTopic<span
    class="pl-pds">"</span></span>)

    library(<span class="pl-smi">cisTopic</span>)</pre></div>

    <h2>

    <a id="user-content-databases" class="anchor" href="#databases" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Databases</h2>

    <p>RcisTarget feather databases are available at <a href="https://resources.aertslab.org/cistarget/"
    rel="nofollow">https://resources.aertslab.org/cistarget/</a>.</p>

    <h2>

    <a id="user-content-tutorials" class="anchor" href="#tutorials" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tutorials</h2>

    <h3>

    <a id="user-content-version-2" class="anchor" href="#version-2" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 2</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/Runningwithmm10andhg38.html"
    rel="nofollow">Running GREAT and motif enrichment with the mm10 and hg38 genome
    assemblies</a>.</li>

    </ul>

    <h3>

    <a id="user-content-version-3" class="anchor" href="#version-3" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version 3</h3>

    <ul>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_CompleteAnalysis.html"
    rel="nofollow">Basic tutorial on simulated single cell epigenomes from melanoma
    cell lines</a>. Data available <a href="https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing"
    rel="nofollow">here</a>.</li>

    <li>

    <a href="http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_10X_workflow.html"
    rel="nofollow">10X tutorial on the 5k PBMC data set from 10X</a>. Data available
    <a href="https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing"
    rel="nofollow">here</a>.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1530133683.0
d-w-moore/zipit:
  data_format: 2
  description: parallel gzipper in pure python
  filenames:
  - Singularity.alpine
  full_name: d-w-moore/zipit
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-zipit\" class=\"anchor\" href=\"#zipit\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>zipit</h1>\n\
    <p>This repo contains two scripts useful for gzipping and checking large files\n\
    as quickly as possible leveraging the parallelism of your machine.</p>\n<p>They\
    \ require only that python be installed, and they depend only on modules\nincluded\
    \ in the Python Standard Library -- particularly, of course, gzip.</p>\n<h2>\n\
    <a id=\"user-content-zipitpy\" class=\"anchor\" href=\"#zipitpy\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>zipit.py</h2>\n\
    <p>Example uses:</p>\n<pre><code> $ ./zipit.py -v large.tar    # =&gt; Creates\
    \ large.tar.gz at default level of parallelism.\n                            \
    \  #    (-v verbosely informs of the piece-wise gzip tasks)\n\n $ ./zipit.py -qm\
    \ large.tar   # =&gt; creates large.tar.gz using all available CPU's\n\n $ some_command\
    \ | ./zipit.py - &gt; out.gz   # =&gt; gzips from the stdin stream, onto stdout\n\
    \n $ docker export cimg | ./zipit.py \\      # =&gt; export and compress the filesystem\
    \ of\n      -d cimg.dig - &gt;cimg.tgz             #    a docker container\n</code></pre>\n\
    <h2>\n<a id=\"user-content-testzippy\" class=\"anchor\" href=\"#testzippy\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>testzip.py</h2>\n\
    <p>Example use (for context, see the final <code>zipit.py</code> example above):</p>\n\
    <pre><code> $ ./testzip.py cimg.tgz cimg.dig      # =&gt; tests the gzipped file's\
    \ integrity using a digest file\n                                       #    (returns\
    \ 0 if the integrity is good)\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1602285708.0
davecwright3/bart-singularity:
  data_format: 2
  description: Bayesian Atmospheric Radiative Transfer (BART) packaged in a Singularity
    container https://github.com/davecwright3/bart-singularity
  filenames:
  - Singularity
  full_name: davecwright3/bart-singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4946" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-bart-singularity-guide" class="anchor" href="#bart-singularity-guide"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BART
    Singularity Guide</h1>

    <p>The Singularity image has BART installed at <code>/bart_dir</code>. The <code>$topdir</code>
    environment variable is set to this directory inside the image. This means that
    the instructions for the demo listed here <a href="https://github.com/exosports/BART/tree/master/examples/demo">https://github.com/exosports/BART/tree/master/examples/demo</a>
    still work, but we need to mount a directory for outputs into the container for
    two reasons:</p>

    <ol>

    <li>The demo expects your output directory to be parallel to the BART directory</li>

    <li>The container file system is read-only (this is only a problem because of
    (1); being read-only is actually preferred because it helps ensure reproducible
    results)</li>

    </ol>

    <ul>

    <li>If the output directory wasn''t required to be parallel to BART, you could
    run the container anywhere in <code>$HOME</code> because Singularity mounts <code>$HOME</code>
    of the current user into the container by default</li>

    </ul>

    <p>The image has a directory parallel to BART that is meant for output at <code>/bart_dir/run</code>.
    Make a directory on your host system where you want to store results. For the
    sake of this guide, let''s say it''s under your current directory at <code>demo/run</code>
    and you have pulled the singularity image</p>

    <pre><code>singularity pull --name bart.sif shub://davecwright3/bart-singularity

    </code></pre>

    <p>to your current directory as well. Then start a shell in the singularity container
    with the bind mount specified</p>

    <pre><code>singularity shell -B demo/run:/bart_dir/run bart.sif

    </code></pre>

    <p>The BART conda environment will be automatically activated. Now just <code>cd
    $topdir/run</code> and follow the instructions here <a href="https://github.com/exosports/BART/tree/master/examples/demo">https://github.com/exosports/BART/tree/master/examples/demo</a>
    if you would like to do a demo run. You can <code>exit</code> the container whenever
    you are done, and your results will remain in your <code>demo/run</code> directory.</p>

    <h1>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h1>

    <p>Bayesian Atmospheric Radiative Transfer (BART), a code to infer

    properties of planetary atmospheres based on observed spectroscopic

    information.</p>

    <p>This project was completed with the support of the NASA Planetary

    Atmospheres Program, grant NNX12AI69G, held by Principal Investigator

    Joseph Harrington. Principal developers included graduate students

    Patricio E. Cubillos and Jasmina Blecic, programmer Madison Stemm, and

    undergraduates M. Oliver Bowman and Andrew S. D. Foster.  The included

    ''transit'' radiative transfer code is based on an earlier program of

    the same name written by Patricio Rojo (Univ. de Chile, Santiago) when

    he was a graduate student at Cornell University under Joseph

    Harrington.  Statistical advice came from Thomas J. Loredo and Nate

    B. Lust.</p>

    <p>Copyright (C) 2015-2016 University of Central Florida.

    All rights reserved.</p>

    <p>This is a test version only, and may not be redistributed to any third

    party.  Please refer such requests to us.  This program is distributed

    in the hope that it will be useful, but WITHOUT ANY WARRANTY; without

    even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR

    PURPOSE.</p>

    <p>Our intent is to release this software under an open-source,

    reproducible-research license, once the code is mature and the first

    research paper describing the code has been accepted for publication

    in a peer-reviewed journal.  We are committed to development in the

    open, and have posted this code on github.com so that others can test

    it and give us feedback.  However, until its first publication and

    first stable release, we do not permit others to redistribute the code

    in either original or modified form, nor to publish work based in

    whole or in part on the output of this code.  By downloading, running,

    or modifying this code, you agree to these conditions.  We do

    encourage sharing any modifications with us and discussing them

    openly.</p>

    <p>We welcome your feedback, but do not guarantee support.  Please send

    feedback or inquiries to:

    Patricio Cubillos <a href="mailto:patricio.cubillos@oeaw.ac.at">patricio.cubillos@oeaw.ac.at</a>

    Jasmina Blecic <a href="mailto:jasmina@physics.ucf.edu">jasmina@physics.ucf.edu</a>

    Joseph Harrington <a href="mailto:jh@physics.ucf.edu">jh@physics.ucf.edu</a></p>

    <p>or alternatively,

    Joseph Harrington, Patricio Cubillos, and Jasmina Blecic

    UCF PSB 441

    4111 Libra Drive

    Orlando, FL 32816-2385

    USA</p>

    <p>Thank you for testing BART!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604965509.0
daviesdrew/variantcalling:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity_1.0.0
  full_name: daviesdrew/variantcalling
  latest_release: null
  readme: '<h1>

    <a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon
    octicon-link"></span></a><a href="docs/images/nf-core-illuminavariantcalling_logo.png"
    target="_blank" rel="noopener noreferrer"><img src="docs/images/nf-core-illuminavariantcalling_logo.png"
    alt="nf-core/illuminavariantcalling" style="max-width:100%;"></a>

    </h1>

    <p><strong>Illumina paired end reads variant calling pipeline</strong>.</p>

    <p><a href="https://github.com/nf-core/illuminavariantcalling/actions"><img src="https://github.com/nf-core/illuminavariantcalling/workflows/nf-core%20CI/badge.svg"
    alt="GitHub Actions CI Status" style="max-width:100%;"></a>

    <a href="https://github.com/nf-core/illuminavariantcalling/actions"><img src="https://github.com/nf-core/illuminavariantcalling/workflows/nf-core%20linting/badge.svg"
    alt="GitHub Actions Linting Status" style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/illuminavariantcalling" rel="nofollow"><img
    src="https://camo.githubusercontent.com/609e7a6579baf2276f34ef713d9cc0b55f7fd62e2c5c7618d40423779d41fd44/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f696c6c756d696e6176617269616e7463616c6c696e672e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/illuminavariantcalling.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker containers making installation trivial and
    results highly reproducible.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>i. Install <a href="https://nf-co.re/usage/installation" rel="nofollow"><code>nextflow</code></a></p>

    <p>ii. Install either <a href="https://docs.docker.com/engine/installation/" rel="nofollow"><code>Docker</code></a>
    or <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow"><code>Singularity</code></a>
    for full pipeline reproducibility (please only use <a href="https://conda.io/miniconda.html"
    rel="nofollow"><code>Conda</code></a> as a last resort; see <a href="https://nf-co.re/usage/configuration#basic-configuration-profiles"
    rel="nofollow">docs</a>)</p>

    <p>iii. Download the pipeline and test it on a minimal dataset with a single command</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run nf-core/illuminavariantcalling
    -profile test,<span class="pl-k">&lt;</span>docker/singularity/conda/institute<span
    class="pl-k">&gt;</span></pre></div>

    <blockquote>

    <p>Please check <a href="https://github.com/nf-core/configs#documentation">nf-core/configs</a>
    to see if a custom config file to run nf-core pipelines already exists for your
    Institute. If so, you can simply use <code>-profile &lt;institute&gt;</code> in
    your command. This will enable either <code>docker</code> or <code>singularity</code>
    and set the appropriate execution settings for your local compute environment.</p>

    </blockquote>

    <p>iv. Start running your own analysis!</p>


    <div class="highlight highlight-source-shell"><pre>nextflow run nf-core/illuminavariantcalling
    -profile <span class="pl-k">&lt;</span>docker/singularity/conda/institute<span
    class="pl-k">&gt;</span> --reads <span class="pl-s"><span class="pl-pds">''</span>*_R{1,2}.fastq.gz<span
    class="pl-pds">''</span></span> --genome GRCh37</pre></div>

    <p>See <a href="docs/usage.md">usage docs</a> for all of the available options
    when running the pipeline.</p>

    <h2>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>The nf-core/illuminavariantcalling pipeline comes with documentation about
    the pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="https://nf-co.re/usage/installation" rel="nofollow">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="https://nf-co.re/usage/local_installation" rel="nofollow">Local installation</a></li>

    <li><a href="https://nf-co.re/usage/adding_own_config" rel="nofollow">Adding your
    own system config</a></li>

    <li><a href="https://nf-co.re/usage/reference_genomes" rel="nofollow">Reference
    genomes</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="https://nf-co.re/usage/troubleshooting" rel="nofollow">Troubleshooting</a></li>

    </ol>


    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>nf-core/illuminavariantcalling was originally written by Drew Davies.</p>

    <h2>

    <a id="user-content-contributions-and-support" class="anchor" href="#contributions-and-support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributions
    and Support</h2>

    <p>If you would like to contribute to this pipeline, please see the <a href=".github/CONTRIBUTING.md">contributing
    guidelines</a>.</p>

    <p>For further information or help, don''t hesitate to get in touch on <a href="https://nfcore.slack.com/channels/illuminavariantcalling"
    rel="nofollow">Slack</a> (you can join with <a href="https://nf-co.re/join/slack"
    rel="nofollow">this invite</a>).</p>

    <h2>

    <a id="user-content-citation" class="anchor" href="#citation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Citation</h2>



    <p>You can cite the <code>nf-core</code> publication as follows:</p>

    <blockquote>

    <p><strong>The nf-core framework for community-curated bioinformatics pipelines.</strong></p>

    <p>Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg,
    Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p>

    <p><em>Nat Biotechnol.</em> 2020 Feb 13. doi: <a href="https://dx.doi.org/10.1038/s41587-020-0439-x"
    rel="nofollow">10.1038/s41587-020-0439-x</a>.<br>

    ReadCube: <a href="https://rdcu.be/b1GjZ" rel="nofollow">Full Access Link</a></p>

    </blockquote>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1593036214.0
dfornika/nf-core-cpo:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: dfornika/nf-core-cpo
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corecpo" class="anchor" href="#nf-corecpo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/cpo</h1>

    <p><strong>Genomic Analysis of Carbapenem Resistant Organisms</strong></p>

    <p><a href="https://travis-ci.org/nf-core/cpo" rel="nofollow"><img src="https://camo.githubusercontent.com/6b4a4d26450e93f9c13ce85f059bb61ebe27051414d40e4f4ba81966ca0029a4/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f63706f2e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/cpo.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/cpo" rel="nofollow"><img src="https://camo.githubusercontent.com/4bc4e99ea4ca2a2f9b15fda9e4d3855153c0fd74431b920ed885080d46e0cc73/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f63706f2e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/cpo.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/cpo pipeline comes with documentation about the pipeline, found
    in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>


    <h3>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>

    <p>nf-core/cpo was originally written by Dan Fornika.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1544054866.0
dgruber/wfl:
  data_format: 2
  description: ' A Simple Way of Creating Job Workflows in Go running in Processes,
    Containers, Tasks, Pods, or Jobs '
  filenames:
  - examples/singularity/SingularityRecipe
  full_name: dgruber/wfl
  latest_release: v1.2.6
  readme: "<h1>\n<a id=\"user-content-wfl---a-simple-and-pluggable-workflow-language-for-go\"\
    \ class=\"anchor\" href=\"#wfl---a-simple-and-pluggable-workflow-language-for-go\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>wfl - A Simple and Pluggable Workflow Language for Go</h1>\n<p><em>Don't\
    \ mix wfl with <a href=\"https://en.wikipedia.org/wiki/Work_Flow_Language\" rel=\"\
    nofollow\">WFL</a>.</em></p>\n<p><a href=\"https://circleci.com/gh/dgruber/wfl/tree/master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2e68f5d3e6715dff94d43886169409bb991226c5562ede8aa6996cb6cd43277d/68747470733a2f2f636972636c6563692e636f6d2f67682f646772756265722f77666c2f747265652f6d61737465722e7376673f7374796c653d737667\"\
    \ alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/dgruber/wfl/tree/master.svg?style=svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/gh/dgruber/wfl\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c0ba5989abe58a671d275f14915277b3c1ab0782fd00555895dfc667863f9c93/68747470733a2f2f636f6465636f762e696f2f67682f646772756265722f77666c2f6272616e63682f6d61737465722f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/dgruber/wfl/branch/master/graph/badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<blockquote>\n<p><em>Update</em>: In order\
    \ to reflect the underlying drmaa2os changes which separates\ndifferent backends\
    \ more clearly some context creation functions are moved\nto pkg/context. That\
    \ avoids having to deal with dependencies from bigger libraries\nlike Kubernetes\
    \ or Docker when not using them.</p>\n</blockquote>\n<p>Creating process, container,\
    \ pod, task, or job workflows based on raw interfaces of\noperating systems, Docker,\
    \ Singularity, Kubernetes, Cloud Foundry, and HPC job schedulers can be\na tedios.\
    \ Lots of repeating code is required. All workload management systems have a\n\
    different API.</p>\n<p><em>wfl</em> abstracts away from the underlying details\
    \ of the processes, containers, and\nworkload management systems. <em>wfl</em>\
    \ provides a simple, unified interface which allows\nto quickly define and execute\
    \ a job workflow and change between different execution\nbackends without changing\
    \ the workflow itself.</p>\n<p><em>wfl</em> does not come with many features but\
    \ is simple to use and enough to define and\nrun jobs and job workflows with inter-job\
    \ dependencies.</p>\n<p>In its simplest form a process can be started and waited\
    \ for:</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span class=\"\
    pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()).<span class=\"pl-en\"\
    >Run</span>(<span class=\"pl-s\">\"convert\"</span>, <span class=\"pl-s\">\"image.jpg\"\
    </span>, <span class=\"pl-s\">\"image.png\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n\
    <p>If the output of the command needs to be displayed on the terminal you can\
    \ set the out path in the\ndefault <em>JobTemplate</em> (see below) configuration:</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>\t<span class=\"pl-s1\">template</span>\
    \ <span class=\"pl-c1\">:=</span> drmaa2interface.<span class=\"pl-smi\">JobTemplate</span>{\n\
    \t\t<span class=\"pl-c1\">ErrorPath</span>:  <span class=\"pl-s\">\"/dev/stderr\"\
    </span>,\n\t\t<span class=\"pl-c1\">OutputPath</span>: <span class=\"pl-s\">\"\
    /dev/stdout\"</span>,\n\t}\n\t<span class=\"pl-s1\">flow</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContextByCfg</span>(wfl.<span\
    \ class=\"pl-smi\">ProcessConfig</span>{\n\t\t<span class=\"pl-c1\">DefaultTemplate</span>:\
    \ <span class=\"pl-s1\">template</span>,\n\t}))\n\t<span class=\"pl-s1\">flow</span>.<span\
    \ class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"\
    pl-s\">\"hello\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n<p>Running\
    \ a job as a Docker container requires a different context (and the image\nalready\
    \ pulled before).</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-k\">import</span> (\n\t<span class=\"pl-s\">\"github.com/dgruber/drmaa2interface\"\
    </span>\n\t<span class=\"pl-s\">\"github.com/dgruber/wfl\"</span>\n\t<span class=\"\
    pl-s\">\"github.com/dgruber/wfl/pkg/context/docker\"</span>\n    )\n    \n   \
    \ <span class=\"pl-s1\">ctx</span> <span class=\"pl-c1\">:=</span> <span class=\"\
    pl-s1\">docker</span>.<span class=\"pl-en\">NewDockerContextByCfg</span>(docker.<span\
    \ class=\"pl-smi\">Config</span>{<span class=\"pl-c1\">DefaultDockerImage</span>:\
    \ <span class=\"pl-s\">\"golang:latest\"</span>})\n    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>).<span class=\"\
    pl-en\">Run</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\"\
    >\"60\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n<p>Starting a\
    \ Docker container without a <em>run command</em> which exposes ports requires\
    \ more\nconfiguration which can be provided by using a <em>JobTemplate</em> together\
    \ with the <em>RunT()</em>\nmethod.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">jt</span> <span class=\"pl-c1\">:=</span> drmaa2interface.<span\
    \ class=\"pl-smi\">JobTemplate</span>{\n        <span class=\"pl-c1\">JobCategory</span>:\
    \ <span class=\"pl-s\">\"swaggerapi/swagger-editor\"</span>,\n    }\n    <span\
    \ class=\"pl-s1\">jt</span>.<span class=\"pl-c1\">ExtensionList</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-k\">map</span>[<span class=\"pl-smi\">string</span>]<span\
    \ class=\"pl-smi\">string</span>{<span class=\"pl-s\">\"exposedPorts\"</span>:\
    \ <span class=\"pl-s\">\"80:8080/tcp\"</span>}\n    \n    <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">docker</span>.<span\
    \ class=\"pl-en\">NewDockerContext</span>())).<span class=\"pl-en\">RunT</span>(<span\
    \ class=\"pl-s1\">jt</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n\
    <p>Starting a Kubernetes batch job and waiting for its end is not much different.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">kubernetes</span>.<span\
    \ class=\"pl-en\">NewKubernetesContext</span>()).<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"60\"</span>).<span class=\"\
    pl-en\">Wait</span>()</pre></div>\n<p><em>wfl</em> also supports submitting jobs\
    \ into HPC schedulers like SLURM, Grid Engine and so on.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">libdrmaa</span>.<span class=\"\
    pl-en\">NewLibDRMAAContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"60\"</span>).<span class=\"pl-en\"\
    >Wait</span>()</pre></div>\n<p><em>wfl</em> aims to work for any kind of workload.\
    \ It works on a Mac and Raspberry Pi the same way\nas on a high-performance compute\
    \ cluster. Things missing: On small scale you probably miss data\nmanagement -\
    \ moving results from one job to another. That's deliberately not implemented.\n\
    On large scale you are missing checkpoint and restart functionality or HA of the\
    \ workflow\nprocess itself.</p>\n<p><em>wfl</em> works with simple primitives:\
    \ <em>context</em>, <em>workflow</em>, <em>job</em>, and <em>jobtemplate</em></p>\n\
    <p>Experimental: Jobs can also be processed in <a href=\"https://github.com/dgruber/wfl/blob/master/examples/stream/stream.go\"\
    >job control streams</a>.</p>\n<p>First support for logging is also available.\
    \ Log levels can be controlled by environment variables\n(<em>export WFL_LOGLEVEL=DEBUG</em>\
    \ or <em>INFO</em>/<em>WARNING</em>/<em>ERROR</em>/<em>NONE</em>). Applications\
    \ can use the same\nlogging facility by getting the logger from the workflow (<em>workflow.Logger()</em>)\
    \ or registering\nyour own logger in a workflow <em>(workflow.SetLogger(Logger\
    \ interface)</em>). Default is set to ERROR.</p>\n<h3>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h3>\n<p>Dependencies\
    \ of <em>wfl</em> (like drmaa2) are vendored in. The only external package required\
    \ to be installed\nmanually is the <em>drmaa2interface</em>.</p>\n<div class=\"\
    highlight highlight-source-go\"><pre>    <span class=\"pl-k\">go</span> <span\
    \ class=\"pl-s1\">get</span> <span class=\"pl-s1\">github</span>.<span class=\"\
    pl-c1\">com</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">dgruber</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">drmaa2interface</span></pre></div>\n\
    <h2>\n<a id=\"user-content-context\" class=\"anchor\" href=\"#context\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Context</h2>\n\
    <p>A context defines the execution backend for the workflow. Contexts can be easily\
    \ created\nwith the <em>New</em> functions which are defined in the <em>context.go</em>\
    \ file or in the separate\npackages found in <em>pkg/context</em>.</p>\n<p>For\
    \ creating a context which executes the jobs of a workflow in operating system\
    \ processses use:</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()</pre></div>\n\
    <p>If the workflow needs to be executed in containers the <em>DockerContext</em>\
    \ can be used:</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-s1\">docker</span>.<span class=\"pl-en\">NewDockerContext</span>()</pre></div>\n\
    <p>If the Docker context needs to be configured with a default Docker image\n\
    (when Run() is used or RunT() without a configured <em>JobCategory</em> (which\
    \ <em>is</em> the Docker image))\nthen the <em>ContextByCfg()</em> can be called.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">docker</span>.<span\
    \ class=\"pl-en\">NewDockerContextByCfg</span>(docker.<span class=\"pl-smi\">Config</span>{<span\
    \ class=\"pl-c1\">DefaultDockerImage</span>: <span class=\"pl-s\">\"golang:latest\"\
    </span>})</pre></div>\n<p>When you want to run the workflow as Cloud Foundry tasks\
    \ the <em>CloudFoundryContext</em> can be used:</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">cloudfoundry</span>.<span class=\"pl-en\">NewCloudFoundryContext</span>()</pre></div>\n\
    <p>Without a config it uses following environment variables to access the Cloud\
    \ Foundry cloud controller API:</p>\n<ul>\n<li>CF_API (like <a href=\"https://api.run.pivotal.io\"\
    \ rel=\"nofollow\">https://api.run.pivotal.io</a>)</li>\n<li>CF_USER</li>\n<li>CF_PASSWORD</li>\n\
    </ul>\n<p>For submitting Kubernetes batch jobs a Kubernetes context exists.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>   <span class=\"pl-s1\">ctx</span>\
    \ <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">kubernetes</span>.<span\
    \ class=\"pl-en\">NewKubernetesContext</span>()</pre></div>\n<p>Note that each\
    \ job requires a container image specified which can be done by using\nthe JobTemplate's\
    \ JobCategory. When the same container image is used within the whole\njob workflow\
    \ it makes sense to use the Kubernetes config.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>   <span class=\"pl-s1\">ctx</span> <span class=\"pl-c1\">:=</span> <span\
    \ class=\"pl-s1\">kubernetes</span>.<span class=\"pl-en\">NewKubernetesContextByCfg</span>(kubernetes.<span\
    \ class=\"pl-smi\">Config</span>{<span class=\"pl-c1\">DefaultImage</span>: <span\
    \ class=\"pl-s\">\"busybox:latest\"</span>})</pre></div>\n<p><a href=\"https://en.wikipedia.org/wiki/Singularity_(software)\"\
    \ rel=\"nofollow\">Singularity</a> containers can be executed\nwithin the Singularity\
    \ context. When setting the <em>DefaultImage</em> (like in the Kubernetes Context)\n\
    then then <em>Run()</em> methods can be used otherwise the Container image must\
    \ be specified in the\nJobTemplate's <em>JobCategory</em> field separately for\
    \ each job. The <em>DefaultImage</em>\ncan always be overridden by the <em>JobCategory</em>.\
    \ Note that each task / job\nexecutes a separate Singularity container process.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>   <span class=\"pl-s1\">ctx</span>\
    \ <span class=\"pl-c1\">:=</span> wfl.<span class=\"pl-smi\">NewSingularityContextByCfg</span>(wfl.<span\
    \ class=\"pl-smi\">SingularityConfig</span>{<span class=\"pl-c1\">DefaultImage</span>:\
    \ <span class=\"pl-s\">\"\"</span>}))</pre></div>\n<p>For working with HPC schedulers\
    \ the libdrmaa context can be used. This context requires\n<em>libdrmaa.so</em>\
    \ available in the library path at runtime. Grid Engine ships <em>libdrmaa.so</em>\n\
    but the <em>LD_LIBRARY_PATH</em> needs to be typically set. For SLURM <em>libdrmaa.so</em>\
    \ often needs\nto be <a href=\"https://github.com/natefoo/slurm-drmaa\">build</a>.</p>\n\
    <p>Since C go is used under the hood (drmaa2os which uses go drmaa) some compiler\
    \ flags needs\nto be set during build time. Those flags depend on the workload\
    \ manager used. Best check\nout the go drmaa project for finding the right flags.</p>\n\
    <p>For building SLURM requires:</p>\n<pre><code>export CGO_LDFLAGS=\"-L$SLURM_DRMAA_ROOT/lib\"\
    \nexport CGO_CFLAGS=\"-DSLURM -I$SLURM_DRMAA_ROOT/include\"\n</code></pre>\n<p>If\
    \ all set a libdrmaa context can be created by importing:</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>   <span class=\"pl-s1\">ctx</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">libdrmaa</span>.<span class=\"pl-en\"\
    >NewLibDRMAAContext</span>()</pre></div>\n<p>The JobCategory is whatever the workloadmanager\
    \ associates with it. Typically it is a\nset of submission parameters. A basic\
    \ example is <a href=\"https://github.com/dgruber/wfl/blob/master/examples/libdrmaa/libdrmaa.go\"\
    >here</a>.</p>\n<h2>\n<a id=\"user-content-workflow\" class=\"anchor\" href=\"\
    #workflow\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Workflow</h2>\n<p>A workflow encapsulates a set of jobs using the\
    \ same backend (context). Depending on the execution\nbackend it can be seen as\
    \ a namespace.</p>\n<p>It can be created by using:</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">wf</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)</pre></div>\n<p>Errors during creation can be catched\
    \ with</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span class=\"\
    pl-s1\">wf</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>).<span class=\"\
    pl-en\">OnError</span>(<span class=\"pl-k\">func</span>(<span class=\"pl-s1\"\
    >e</span> <span class=\"pl-smi\">error</span>) {<span class=\"pl-en\">panic</span>(<span\
    \ class=\"pl-s1\">e</span>)})</pre></div>\n<p>or with</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-k\">if</span> <span class=\"\
    pl-s1\">wf</span>.<span class=\"pl-en\">HasError</span>() {\n        <span class=\"\
    pl-en\">panic</span>(<span class=\"pl-s1\">wf</span>.<span class=\"pl-en\">Error</span>())\n\
    \    }</pre></div>\n<h2>\n<a id=\"user-content-job\" class=\"anchor\" href=\"\
    #job\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Job</h2>\n<p>Jobs are the main objects in <em>wfl</em>. A job defines\
    \ helper methods. Many of them return the job object itself to allow chaining\
    \ calls in an easy way. A job can also be seen as a container and control unit\
    \ for tasks. Tasks are often mapped to jobs of the underlying\nworkload manager\
    \ (like in Kubernetes, HPC schedulers etc.).</p>\n<p>In some systems it is required\
    \ to delete job related resources after the job is finished\nand no more information\
    \ needs to be queried about its execution. This functionality is\nimplemented\
    \ in the DRMAA2 Reap() method which can be executed by ReapAll() for each\ntask\
    \ in the job object. Afterwards the job object should not be used anymore as some\n\
    information might not be available anymore.</p>\n<p>Methods can be classified\
    \ in blocking, non-blocking, job template based, function based, and error handlers.</p>\n\
    <h3>\n<a id=\"user-content-job-submission\" class=\"anchor\" href=\"#job-submission\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Job Submission</h3>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n\
    <th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td>Run()</td>\n<td>Starts a process, container, or submits a task and comes\
    \ back immediately</td>\n<td>no</td>\n<td></td>\n</tr>\n<tr>\n<td>RunT()</td>\n\
    <td>Like above but with a JobTemplate as parameter</td>\n<td>no</td>\n<td></td>\n\
    </tr>\n<tr>\n<td>RunArray()</td>\n<td>Submits a bulk job which runs many iterations\
    \ of the same command</td>\n<td>no</td>\n<td></td>\n</tr>\n<tr>\n<td>Resubmit()</td>\n\
    <td>Submits a job <em>n</em>-times (Run().Run().Run()...)</td>\n<td>no</td>\n\
    <td></td>\n</tr>\n<tr>\n<td>RunEvery()</td>\n<td>Submits a task every d <em>time.Duration</em>\n\
    </td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>RunEveryT()</td>\n<td>Like <em>RunEvery()</em>\
    \ but with JobTemplate as param</td>\n<td>yes</td>\n<td></td>\n</tr>\n</tbody>\n\
    </table>\n<h3>\n<a id=\"user-content-job-control\" class=\"anchor\" href=\"#job-control\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Job Control</h3>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n\
    <th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td>Suspend()</td>\n<td>Stops a task from execution (e.g. sending SIGTSTP\
    \ to the process group)...</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Resume()</td>\n\
    <td>Continues a task (e.g. sending SIGCONT)...</td>\n<td></td>\n<td></td>\n</tr>\n\
    <tr>\n<td>Kill()</td>\n<td>Stops process (SIGKILL), container, task, job immediately.</td>\n\
    <td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h3>\n<a id=\"user-content-function-execution\"\
    \ class=\"anchor\" href=\"#function-execution\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Function Execution</h3>\n<table>\n\
    <thead>\n<tr>\n<th>Function Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>Do()</td>\n<td>Executes a Go function</td>\n\
    <td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Then()</td>\n<td>Waits for end of process\
    \ and executes a Go function</td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>OnSuccess()</td>\n\
    <td>Executes a function if the task run successfully (exit code 0)</td>\n<td>yes</td>\n\
    <td></td>\n</tr>\n<tr>\n<td>OnFailure()</td>\n<td>Executes a function if the task\
    \ failed (exit code != 0)</td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>OnError()</td>\n\
    <td>Executes a function if the task could not be created</td>\n<td>yes</td>\n\
    <td></td>\n</tr>\n</tbody>\n</table>\n<h3>\n<a id=\"user-content-blocker\" class=\"\
    anchor\" href=\"#blocker\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Blocker</h3>\n<table>\n<thead>\n<tr>\n<th>Function\
    \ Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td>After()</td>\n<td>Blocks a specific amount of time and continues</td>\n\
    <td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Wait()</td>\n<td>Waits until the task\
    \ submitted latest finished</td>\n<td>yes</td>\n<td></td>\n</tr>\n<tr>\n<td>Synchronize()</td>\n\
    <td>Waits until all submitted tasks finished</td>\n<td>yes</td>\n<td></td>\n</tr>\n\
    </tbody>\n</table>\n<h3>\n<a id=\"user-content-job-flow-control\" class=\"anchor\"\
    \ href=\"#job-flow-control\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Job Flow Control</h3>\n<table>\n<thead>\n<tr>\n\
    <th>Function Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n<th>Examples</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>ThenRun()</td>\n<td>Wait() (last task finished)\
    \ followed by an async Run()</td>\n<td>partially</td>\n<td></td>\n</tr>\n<tr>\n\
    <td>ThenRunT()</td>\n<td>ThenRun() with template</td>\n<td>partially</td>\n<td></td>\n\
    </tr>\n<tr>\n<td>OnSuccessRun()</td>\n<td>Wait() if Success() then Run()</td>\n\
    <td>partially</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>OnSuccessRunT()</td>\n<td>OnSuccessRun()\
    \ but with template as param</td>\n<td>partially</td>\n<td></td>\n</tr>\n<tr>\n\
    <td>OnFailureRun()</td>\n<td>Wait() if Failed() then Run()</td>\n<td>partially</td>\n\
    <td>\_</td>\n</tr>\n<tr>\n<td>OnFailureRunT()</td>\n<td>OnFailureRun() but with\
    \ template as param</td>\n<td>partially</td>\n<td></td>\n</tr>\n<tr>\n<td>Retry()</td>\n\
    <td>wait() + !success() + resubmit() + wait() + !success()</td>\n<td>yes</td>\n\
    <td>\_</td>\n</tr>\n<tr>\n<td>AnyFailed()</td>\n<td>Cchecks if one of the tasks\
    \ in the job failed</td>\n<td>yes</td>\n<td>\_</td>\n</tr>\n</tbody>\n</table>\n\
    <h3>\n<a id=\"user-content-job-status-and-general-checks\" class=\"anchor\" href=\"\
    #job-status-and-general-checks\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Job Status and General Checks</h3>\n\
    <table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Purpose</th>\n<th>Blocking</th>\n\
    <th>Examples</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JobID()</td>\n<td>Returns\
    \ the ID of the submitted job</td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>JobInfo()</td>\n\
    <td>Returns the DRMAA2 JobInfo of the job</td>\n<td>no</td>\n<td>\_</td>\n</tr>\n\
    <tr>\n<td>Template()</td>\n<td></td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n\
    <td>State()</td>\n<td></td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>LastError()</td>\n\
    <td></td>\n<td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>Failed()</td>\n<td></td>\n\
    <td>no</td>\n<td>\_</td>\n</tr>\n<tr>\n<td>Success()</td>\n<td></td>\n<td>no</td>\n\
    <td>\_</td>\n</tr>\n<tr>\n<td>ExitStatus()</td>\n<td></td>\n<td>no</td>\n<td>\_\
    </td>\n</tr>\n<tr>\n<td>ReapAll()</td>\n<td>Cleans up all job related resources\
    \ from the workload manager. Do not</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n\
    <td>use the job object afterwards. Calls DRMAA2 Reap() on all tasks.</td>\n<td>no</td>\n\
    <td>\_</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-jobtemplate\"\
    \ class=\"anchor\" href=\"#jobtemplate\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>JobTemplate</h2>\n<p>JobTemplates\
    \ are specifying the details about a job. In the simplest case the job is specified\
    \ by the application name and its arguments like it is typically done in the OS\
    \ shell. In that case the <em>Run()</em> methods (<em>ThenRun()</em>, <em>OnSuccessRun()</em>,\
    \ <em>OnFailureRun()</em>) can be used. Job template based methods (like <em>RunT()</em>)\
    \ can be completely avoided by providing a\ndefault template when creating the\
    \ context (<em>...ByConfig()</em>). Then each <em>Run()</em> inherits the settings\
    \ (like <em>JobCategory</em> for the container image name and <em>OutputPath</em>\
    \ for redirecting output to <em>stdout</em>). If more details for specifying the\
    \ jobs are required the <em>RunT()</em> methods needs to be used.\nI'm using currently\
    \ the <a href=\"https://github.com/dgruber/drmaa2interface/blob/master/jobtemplate.go\"\
    >DRMAA2 Go JobTemplate</a>. In most cases only <em>RemoteCommand</em>, <em>Args</em>,\
    \ <em>WorkingDirectory</em>, <em>JobCategory</em>, <em>JobEnvironment</em>,  <em>StageInFiles</em>\
    \ are evaluated. Functionality and semantic is up to the underlying <a href=\"\
    https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker\">drmaa2os job\
    \ tracker</a>.</p>\n<ul>\n<li><a href=\"https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker/simpletracker\"\
    >For the process mapping see here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/libdrmaa\"\
    >For the mapping to a drmaa1 implementation (libdrmaa.so) for SLURM, Grid Engine,\
    \ PBS, ...</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker/dockertracker\"\
    >For the Docker mapping here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/cftracker\"\
    >For the Cloud Foundry Task mapping here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/kubernetestracker\"\
    >For the Kubernetes batch job mapping here</a></li>\n<li><a href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/singularity\"\
    >Singularity support</a></li>\n</ul>\n<p>The <a href=\"https://github.com/dgruber/wfl/blob/master/template.go\"\
    ><em>Template</em></a> object provides helper functions for job templates and\
    \ required as generators of job <a href=\"https://github.com/dgruber/wfl/blob/master/examples/stream/stream.go\"\
    >streams</a>. For an example see <a href=\"https://github.com/dgruber/wfl/tree/master/examples/template/template.go\"\
    >here</a>.</p>\n<h1>\n<a id=\"user-content-examples\" class=\"anchor\" href=\"\
    #examples\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Examples</h1>\n<p>For examples please have a look into the examples\
    \ directory. <a href=\"https://github.com/dgruber/wfl/tree/master/examples/template/template.go\"\
    >template</a> is a canonical example of a pre-processing job, followed by parallel\
    \ execution, followed by a post-processing job.</p>\n<p><a href=\"https://github.com/dgruber/wfl/blob/master/test/test.go\"\
    >test</a> is an use case for testing. It compiles\nall examples with the local\
    \ go compiler and then within a Docker container using the <em>golang:latest</em>\
    \ image\nand reports errors.</p>\n<p><a href=\"https://github.com/dgruber/wfl/blob/master/examples/cloudfoundry/cloudfoundry.go\"\
    >cloudfoundry</a> demonstrates how a Cloud Foundry taks can be created.</p>\n\
    <p><a href=\"https://github.com/dgruber/wfl/blob/master/examples/singularity/singularity.go\"\
    >Singularity containers</a> can also be created which is helpful when managing\
    \ a simple Singularity <em>wfl</em> container workflow within a single HPC job\
    \ either to fully exploit all resources and reduce the amount of HPC jobs.</p>\n\
    <h2>\n<a id=\"user-content-creating-a-workflow-which-is-executed-as-os-processes\"\
    \ class=\"anchor\" href=\"#creating-a-workflow-which-is-executed-as-os-processes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Creating a Workflow which is Executed as OS Processes</h2>\n<p>The\
    \ allocated context defines which workload management system / job execution backend\
    \ is used.</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span class=\"\
    pl-s1\">ctx</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewProcessContext</span>()</pre></div>\n<p>Different contexts\
    \ can be used within a single program. That way multi-clustering potentially\n\
    over different cloud solutions is supported.</p>\n<p>Using a context a workflow\
    \ can be established.</p>\n<div class=\"highlight highlight-source-go\"><pre>\
    \    <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>())</pre></div>\n\
    <p>Handling an error during workflow generation can be done by specifying a function\
    \ which\nis only called in the case of an error.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()).<span\
    \ class=\"pl-en\">OnError</span>(<span class=\"pl-k\">func</span>(<span class=\"\
    pl-s1\">e</span> <span class=\"pl-smi\">error</span>) {\n\t\t<span class=\"pl-en\"\
    >panic</span>(<span class=\"pl-s1\">e</span>)\n\t})</pre></div>\n<p>The workflow\
    \ is used in order to instantiate the first job using the <em>Run()</em> method.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewProcessContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"123\"</span>)</pre></div>\n<p>But\
    \ you can also create an initial job like that:</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">job</span> <span class=\"pl-c1\">:=</span> <span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewJob</span>(<span class=\"\
    pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()))</pre></div>\n<p>For\
    \ more detailed settings (like resource limits) the DRMAA2 job template can be\
    \ used as parameter for <em>RunT()</em>.</p>\n<p>Jobs allow the execution of workload\
    \ as well as expressing dependencies.</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewProcessContext</span>()).<span\
    \ class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"\
    pl-s\">\"2\"</span>).<span class=\"pl-en\">ThenRun</span>(<span class=\"pl-s\"\
    >\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).<span class=\"pl-en\">Wait</span>()</pre></div>\n\
    <p>The line above executes two OS processes sequentially and waits until the last\
    \ job in chain is finished.</p>\n<p>In the following example the two sleep processes\
    \ are executed in parallel. <em>Wait()</em> only waitf for the sleep 1 job. Hence\
    \ sleep 2 still runs after the wait call comes back.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewProcessContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"pl-s\"\
    >\"sleep\"</span>, <span class=\"pl-s\">\"2\"</span>).<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).<span class=\"\
    pl-en\">Wait</span>()</pre></div>\n<p>Running two jobs in parallel and waiting\
    \ until all jobs finished can be done <em>Synchronize()</em>.</p>\n<div class=\"\
    highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewProcessContext</span>()).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"2\"</span>).<span class=\"pl-en\"\
    >Run</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"\
    </span>).<span class=\"pl-en\">Synchronize</span>()</pre></div>\n<p>Jobs can also\
    \ be suspended (stopped) and resumed (continued) - if supported by the execution\
    \ backend (like OS, Docker).</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">wf</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).<span class=\"\
    pl-en\">After</span>(<span class=\"pl-s1\">time</span>.<span class=\"pl-c1\">Millisecond</span>\
    \ <span class=\"pl-c1\">*</span> <span class=\"pl-c1\">100</span>).<span class=\"\
    pl-en\">Suspend</span>().<span class=\"pl-en\">After</span>(<span class=\"pl-s1\"\
    >time</span>.<span class=\"pl-c1\">Millisecond</span> <span class=\"pl-c1\">*</span>\
    \ <span class=\"pl-c1\">100</span>).<span class=\"pl-en\">Resume</span>().<span\
    \ class=\"pl-en\">Wait</span>()</pre></div>\n<p>The exit status is available as\
    \ well. <em>ExitStatus()</em> blocks until the previously submitted job is finished.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>).<span class=\"\
    pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\"\
    >\"hello\"</span>).<span class=\"pl-en\">ExitStatus</span>()</pre></div>\n<p>In\
    \ order to run jobs depending on the exit status the <em>OnFailure</em> and <em>OnSuccess</em>\
    \ methods can be used:</p>\n<div class=\"highlight highlight-source-go\"><pre>\
    \    <span class=\"pl-s1\">wf</span>.<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"false\"</span>).<span class=\"pl-en\">OnFailureRun</span>(<span class=\"\
    pl-s\">\"true\"</span>).<span class=\"pl-en\">OnSuccessRun</span>(<span class=\"\
    pl-s\">\"false\"</span>)</pre></div>\n<p>For executing a function on a submission\
    \ error <em>OnError()</em> can be used.</p>\n<p>More methods can be found in the\
    \ sources.</p>\n<h2>\n<a id=\"user-content-basic-workflow-patterns\" class=\"\
    anchor\" href=\"#basic-workflow-patterns\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Basic Workflow Patterns</h2>\n\
    <h3>\n<a id=\"user-content-sequence\" class=\"anchor\" href=\"#sequence\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Sequence</h3>\n\
    <p>The successor task runs after the completion of the pre-decessor task.</p>\n\
    <div class=\"highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">flow</span>\
    \ <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)\n    <span class=\"\
    pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"\
    </span>, <span class=\"pl-s\">\"first task\"</span>).<span class=\"pl-en\">ThenRun</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"second task\"</span>)\n\
    \    <span class=\"pl-c1\">...</span></pre></div>\n<p>or</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">flow</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)\n    <span class=\"pl-s1\">job</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>)\n\
    \    <span class=\"pl-s1\">job</span>.<span class=\"pl-en\">Wait</span>()\n  \
    \  <span class=\"pl-s1\">job</span>.<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"second task\"</span>)\n    <span\
    \ class=\"pl-c1\">...</span></pre></div>\n<h3>\n<a id=\"user-content-parallel-split\"\
    \ class=\"anchor\" href=\"#parallel-split\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Parallel Split</h3>\n<p>After\
    \ completion of a task run multiple branches of tasks.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>\n    <span class=\"pl-s1\">flow</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)\n    <span class=\"pl-s1\">flow</span>.<span class=\"\
    pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\"\
    >\"first task\"</span>).<span class=\"pl-en\">Wait</span>()\n\n    <span class=\"\
    pl-s1\">notifier</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewNotifier</span>()\n\n    <span class=\"pl-k\"\
    >go</span> <span class=\"pl-k\">func</span>() {\n        <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)).\n   \
    \         <span class=\"pl-en\">TagWith</span>(<span class=\"pl-s\">\"BranchA\"\
    </span>).\n            <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"\
    sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).\n            <span class=\"\
    pl-en\">ThenRun</span>(<span class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\"\
    >\"3\"</span>).\n            <span class=\"pl-en\">Synchronize</span>().\n   \
    \         <span class=\"pl-en\">Notify</span>(<span class=\"pl-s1\">notifier</span>)\n\
    \    }\n\n    <span class=\"pl-k\">go</span> <span class=\"pl-k\">func</span>()\
    \ {\n        <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewJob</span>(<span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"\
    pl-s1\">ctx</span>)).\n            <span class=\"pl-en\">TagWith</span>(<span\
    \ class=\"pl-s\">\"BranchB\"</span>).\n            <span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).\n     \
    \       <span class=\"pl-en\">ThenRun</span>(<span class=\"pl-s\">\"sleep\"</span>,\
    \ <span class=\"pl-s\">\"3\"</span>).\n            <span class=\"pl-en\">Synchronize</span>().\n\
    \            <span class=\"pl-en\">Notify</span>(<span class=\"pl-s1\">notifier</span>)\n\
    \    }\n\n    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \n    <span class=\"pl-c1\">...</span></pre></div>\n<h3>\n<a id=\"user-content-synchronization-of-tasks\"\
    \ class=\"anchor\" href=\"#synchronization-of-tasks\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Synchronization\
    \ of Tasks</h3>\n<p>Wait until all tasks of a job which are running in parallel\
    \ are finished.</p>\n<div class=\"highlight highlight-source-go\"><pre>    <span\
    \ class=\"pl-s1\">flow</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\"\
    >wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)\n\
    \    <span class=\"pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>).\n\
    \        <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>,\
    \ <span class=\"pl-s\">\"second task\"</span>).\n        <span class=\"pl-en\"\
    >Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"third\
    \ task\"</span>).\n        <span class=\"pl-en\">Synchronize</span>()</pre></div>\n\
    <h3>\n<a id=\"user-content-synchronization-of-branches\" class=\"anchor\" href=\"\
    #synchronization-of-branches\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Synchronization of Branches</h3>\n\
    <p>Wait until all branches of a workflow are finished.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>\n    <span class=\"pl-s1\">notifier</span> <span\
    \ class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewNotifier</span>()\n\n    <span class=\"pl-k\">go</span> <span class=\"pl-k\"\
    >func</span>() {\n        <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>)).\n            <span class=\"pl-en\">TagWith</span>(<span\
    \ class=\"pl-s\">\"BranchA\"</span>).\n            <span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"sleep\"</span>, <span class=\"pl-s\">\"1\"</span>).\n     \
    \       <span class=\"pl-en\">Wait</span>().\n\t\t\t<span class=\"pl-en\">Notify</span>(<span\
    \ class=\"pl-s1\">notifier</span>)\n    }\n\n    <span class=\"pl-k\">go</span>\
    \ <span class=\"pl-k\">func</span>() {\n        <span class=\"pl-s1\">wfl</span>.<span\
    \ class=\"pl-en\">NewJob</span>(<span class=\"pl-s1\">wfl</span>.<span class=\"\
    pl-en\">NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)).\n            <span\
    \ class=\"pl-en\">TagWith</span>(<span class=\"pl-s\">\"BranchB\"</span>).\n \
    \           <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"sleep\"</span>,\
    \ <span class=\"pl-s\">\"1\"</span>).\n            <span class=\"pl-en\">Wait</span>().\n\
    \t\t\t<span class=\"pl-en\">Notify</span>(<span class=\"pl-s1\">notifier</span>)\n\
    \    }\n\n    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \    <span class=\"pl-s1\">notifier</span>.<span class=\"pl-en\">ReceiveJob</span>()\n\
    \n    <span class=\"pl-c1\">...</span></pre></div>\n<h3>\n<a id=\"user-content-exclusive-choice\"\
    \ class=\"anchor\" href=\"#exclusive-choice\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Exclusive Choice</h3>\n<div class=\"\
    highlight highlight-source-go\"><pre>    <span class=\"pl-s1\">flow</span> <span\
    \ class=\"pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\"\
    >NewWorkflow</span>(<span class=\"pl-s1\">ctx</span>)\n    <span class=\"pl-s1\"\
    >job</span> <span class=\"pl-c1\">:=</span> <span class=\"pl-s1\">flow</span>.<span\
    \ class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>, <span class=\"\
    pl-s\">\"first task\"</span>)\n    <span class=\"pl-s1\">job</span>.<span class=\"\
    pl-en\">Wait</span>()\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-s1\"\
    >job</span>.<span class=\"pl-en\">Success</span>() {\n        <span class=\"pl-c\"\
    >// do something</span>\n    } <span class=\"pl-k\">else</span> {\n        <span\
    \ class=\"pl-c\">// do something else</span>\n    }\n    <span class=\"pl-c1\"\
    >...</span></pre></div>\n<h3>\n<a id=\"user-content-fork-pattern\" class=\"anchor\"\
    \ href=\"#fork-pattern\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Fork Pattern</h3>\n<p>When a task is finished\
    \ <em>n</em> tasks needs to be started in parallel.</p>\n<div class=\"highlight\
    \ highlight-source-go\"><pre>    <span class=\"pl-s1\">job</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span\
    \ class=\"pl-s1\">ctx</span>).<span class=\"pl-en\">Run</span>(<span class=\"\
    pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>).\n       \
    \ <span class=\"pl-en\">ThenRun</span>(<span class=\"pl-s\">\"echo\"</span>, <span\
    \ class=\"pl-s\">\"parallel task 1\"</span>).\n        <span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"parallel task 2\"</span>).\n\
    \        <span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>,\
    \ <span class=\"pl-s\">\"parallel task 3\"</span>)\n    <span class=\"pl-c1\"\
    >...</span></pre></div>\n<p>or</p>\n<div class=\"highlight highlight-source-go\"\
    ><pre>    <span class=\"pl-s1\">flow</span> <span class=\"pl-c1\">:=</span> <span\
    \ class=\"pl-s1\">wfl</span>.<span class=\"pl-en\">NewWorkflow</span>(<span class=\"\
    pl-s1\">ctx</span>)\n    \n    <span class=\"pl-s1\">job</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-s1\">flow</span>.<span class=\"pl-en\">Run</span>(<span\
    \ class=\"pl-s\">\"echo\"</span>, <span class=\"pl-s\">\"first task\"</span>)\n\
    \    <span class=\"pl-s1\">job</span>.<span class=\"pl-en\">Wait</span>()\n  \
    \  <span class=\"pl-k\">for</span> <span class=\"pl-s1\">i</span> <span class=\"\
    pl-c1\">:=</span> <span class=\"pl-c1\">1</span>; <span class=\"pl-s1\">i</span>\
    \ <span class=\"pl-c1\">&lt;=</span> <span class=\"pl-c1\">3</span>; <span class=\"\
    pl-s1\">i</span><span class=\"pl-c1\">++</span> {\n        <span class=\"pl-s1\"\
    >job</span>.<span class=\"pl-en\">Run</span>(<span class=\"pl-s\">\"echo\"</span>,\
    \ <span class=\"pl-s1\">fmt</span>.<span class=\"pl-en\">Sprintf</span>(<span\
    \ class=\"pl-s\">\"parallel task %d\"</span>, <span class=\"pl-s1\">i</span>))\n\
    \    }\n    <span class=\"pl-c1\">...</span></pre></div>\n<p>For missing functionality\
    \ or bugs please open an issue on github. Contributions welcome!</p>\n"
  stargazers_count: 33
  subscribers_count: 4
  topics:
  - docker
  - processes
  - cloud-foundry
  - k8s
  - workflow
  - hpc
  - macos
  - linux
  - high-throughput
  - singularity
  updated_at: 1622034364.0
djarecka/tmp_nipype_tut:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: djarecka/tmp_nipype_tut
  latest_release: null
  readme: '<h1>

    <a id="user-content-nipype-tutorial-notebooks" class="anchor" href="#nipype-tutorial-notebooks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nipype
    Tutorial Notebooks</h1>

    <p><a href="https://circleci.com/gh/miykael/nipype_tutorial/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64"
    alt="CircleCi" data-canonical-src="https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/issues/"><img src="https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/pulls/"><img src="https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub pull-requests" data-canonical-src="https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/"><img
    src="https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/commits/master"><img src="https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub Commits" data-canonical-src="https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/miykael/nipype_tutorial/archive/master.zip"><img src="https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub size" data-canonical-src="https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/miykael/nipype_tutorial/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000"
    style="max-width:100%;"></a>

    <a href="http://hits.dwyl.io/miykael/nipype_tutorial" rel="nofollow"><img src="https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667"
    alt="GitHub HitCount" data-canonical-src="http://hits.dwyl.io/miykael/nipype_tutorial.svg"
    style="max-width:100%;"></a></p>

    <p>This is the Nipype Tutorial in Jupyter Notebook format. You can access the
    tutorial in two ways:</p>

    <ol>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/" rel="nofollow">Nipype Tutorial
    Homepage</a>: This website contains a static, read-only version of all the notebooks.</li>

    <li>

    <a href="https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html"
    rel="nofollow">Nipype Tutorial Docker Image</a>: This guide explains how to use
    Docker to run the notebooks interactively on your own computer. The nipype tutorial
    docker image is the best interactive way to learn Nipype.</li>

    </ol>

    <h1>

    <a id="user-content-feedback-help--support" class="anchor" href="#feedback-help--support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feedback,
    Help &amp; Support</h1>

    <p>If you want to help with this tutorial or have any questions, feel free to
    fork the repo of the <a href="https://github.com/miykael/nipype_tutorial">Notebooks</a>
    or interact with other contributors on the slack channel <a href="https://brainhack.slack.com/messages/nipype/"
    rel="nofollow">brainhack.slack.com/messages/nipype/</a>. If you have any questions
    or found a problem, open a new <a href="https://github.com/miykael/nipype_tutorial/issues">issue
    on github</a>.</p>

    <h1>

    <a id="user-content-thanks-and-acknowledgment" class="anchor" href="#thanks-and-acknowledgment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Thanks
    and Acknowledgment</h1>

    <p>A huge thanks to <a href="https://github.com/mwaskom">Michael Waskom</a>, <a
    href="https://github.com/oesteban">Oscar Esteban</a>, <a href="https://github.com/chrisfilo">Chris
    Gorgolewski</a> and <a href="https://github.com/satra">Satrajit Ghosh</a> for
    their input to this tutorial! And a huge thanks to <a href="https://github.com/djarecka/">Dorota
    Jarecka</a> who updated this tutorial to Python 3 and is helping me with keeping
    this tutorial updated and running!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1547566090.0
dshyshlov/funannotate_singularity:
  data_format: 2
  description: Recipe for funannotate pipeline Singularity recipy for UA HPC
  filenames:
  - Singularity
  full_name: dshyshlov/funannotate_singularity
  latest_release: null
  readme: '<p>Singularity recipe files for SEX-DETector, a tool for the statistical
    inferrence of sex-linked genes from RNA / DNA reads from a cross (parents and
    set of childrens)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602202847.0
duke-chsi-informatics/singularity-rnaseq:
  data_format: 2
  description: Singularity Image for RNA-Seq analysis
  filenames:
  - Singularity.test_jupyter
  - Singularity
  full_name: duke-chsi-informatics/singularity-rnaseq
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"\
    #singularity-rnaseq\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-rnaseq</h1>\n<h2>\n<a id=\"user-content-running-jupyter\"\
    \ class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Jupyter</h2>\n<p>Run\
    \ this to start Jupyter:</p>\n<pre><code>singularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\
    </code></pre>\n<p>Then follow the instructions that Jupyter printed to the terminal\
    \ when you started it up to access Jupyter in your web browser</p>\n<h3>\n<a id=\"\
    user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Accessing Jupyter on a remote server</h3>\n<p>If you are running the\
    \ container on a remote server, you will need to set up port forwarding with ssh\
    \ to be able to access Jupyter.  Run this command to forward the default Jupyter\
    \ port (8888)</p>\n<pre><code>ssh -L 8888:localhost:8888 bug\n</code></pre>\n\
    <blockquote>\n<p>Note if the default Jupyter port is not available, Jupyter will\
    \ choose a different port.  In this case you will need to substitute the port\
    \ that Jupyter outputs for 8888 in the ssh port forwarding command above.</p>\n\
    </blockquote>\n<h2>\n<a id=\"user-content-running-on-a-slurm-cluster\" class=\"\
    anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running on a SLURM Cluster</h2>\n\
    <p>You can use this image interactively on a SLURM-managed cluster by running\
    \ launching RStudio or Jupyter. The following instructions work on the Duke Compute\
    \ Cluster (DCC).  Doing this on other cluster will require some modification and\
    \ may not work, depending on how the cluster is configured.</p>\n<h3>\n<a id=\"\
    user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RStudio</h3>\n\
    <ol>\n<li>ssh to DCC login node: <code>ssh NETID@dcc-login-01.rc.duke.edu</code>\n\
    </li>\n<li>run tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n\
    <li>Run this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty\
    \ bash -i</code>\n</li>\n<li>Run <code>hostname -A</code> on compute node and\
    \ record results</li>\n<li>Run on the following on a compute node and note the\
    \ port, username, and password that the command prints:</li>\n</ol>\n<pre><code>mkdir\
    \ -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\n\
    singularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind\
    \ /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\
    </code></pre>\n<ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the port returned\
    \ but the \"singularity run\" commmand</li>\n<li>Where COMPUTE_HOSTNAME is the\
    \ hostname returned by running \"hostname -A\" on the compute node</li>\n<li>Where\
    \ NETID is your NetID</li>\n</ul>\n</li>\n<li>Go to \"localhost:PORT\" in a webrowser\
    \ and enter the username and password printed by the \"singularity run\" commmand</li>\n\
    <li>Have fun!!</li>\n<li>At the end of an analysis you will probably want to copy\
    \ results to your directory in <code>/work</code> or <code>/hpc/group</code>\n\
    </li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Jupyter</h3>\n<ol>\n<li>ssh to dcc-login-01.rc.duke.edu</li>\n<li>run\
    \ tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n<li>Run\
    \ this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty bash -i</code>\n\
    </li>\n<li>Run on compute node:</li>\n</ol>\n<pre><code>mkdir -p /scratch/josh/rnaseq_demo/rawdata\
    \ /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\
    \n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace\
    \ \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n</code></pre>\n\
    <ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the number after\
    \ <code>http://127.0.0.1:</code> in the URL given by Jupyter (defaults to 8888,\
    \ but Jupyter will use a different one if the default is in use, or if a different\
    \ port is supplied as an argument using <code>--port</code> when running the singularity\
    \ container</li>\n<li>Where COMPUTE_HOSTNAME is the hostname returned by running\
    \ \"hostname -A\" on the compute node</li>\n<li>Where NETID is your NetID</li>\n\
    </ul>\n</li>\n<li>Copy the URL supplied by jupyter that starts <code>http://127.0.0.1:</code>\
    \ and paste it in a webbrowser</li>\n<li>Have fun!!</li>\n<li>At the end of an\
    \ analysis you will probably want to copy results to your directory in <code>/work</code>\
    \ or <code>/hpc/group</code>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter-on-gpu-node\"\
    \ class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Jupyter on GPU node</h3>\n<p>Same\
    \ as above, but the srun command should use the <code>chsi-gpu</code> partition\
    \ and request a gpu, but less CPUs and Memory:</p>\n<p><code>srun -A chsi -p chsi-gpu\
    \ --gres=gpu:1 --mem=15866 -c 2 --pty bash -i</code></p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1619726636.0
edoapra/nwchem-singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - nwchem-dev.ompi40x.skylake/Singularity
  - nwchem-701.mpich321.ivybridge/Singularity
  - nwchem-701.ompi313.ivybridge/Singularity
  - nwchem-701.ifort/Singularity
  - nwchem-dev.ompi40x.ifort.skylake/Singularity
  - nwchem-702.ompi313.ivybridge/Singularity
  full_name: edoapra/nwchem-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-nwchem-singularity" class="anchor" href="#nwchem-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>NWChem
    singularity</h1>

    <p>Singularity recipes for NWChem</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1613067966.0
ejolly/IntroToSingularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ejolly/IntroToSingularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-getting-setup-with-singularity\" class=\"anchor\"\
    \ href=\"#getting-setup-with-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Setup with Singularity</h1>\n\
    <p><em>This is a guide to getting started with <a href=\"http://singularity.lbl.gov/\"\
    \ rel=\"nofollow\">Singularity containers</a> in conjunction with Dartmouth College's\
    \ <a href=\"http://techdoc.dartmouth.edu/discovery/\" rel=\"nofollow\">Discovery\
    \ HPC</a>.<br>\nQuestions can be addressed to <a href=\"mailto:eshin.jolly.gr@dartmouth.edu\"\
    >eshin.jolly.gr@dartmouth.edu</a> or <a href=\"mailto:mvdoc.gr@dartmouth.edu\"\
    >mvdoc.gr@dartmouth.edu</a>.<br>\nWe're not experts but we're happy to try to\
    \ help!</em></p>\n<h4>\n<a id=\"user-content-i-pre-requisites-osx-only\" class=\"\
    anchor\" href=\"#i-pre-requisites-osx-only\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"#prereqs\">I. Pre-requisites\
    \ (OSX only)</a>\n</h4>\n<h4>\n<a id=\"user-content-ii-creating-a-singularity-container\"\
    \ class=\"anchor\" href=\"#ii-creating-a-singularity-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"#creation\">II. Creating a Singularity container</a>\n</h4>\n<h4>\n<a\
    \ id=\"user-content-iii-basic-container-usage\" class=\"anchor\" href=\"#iii-basic-container-usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"#basicusage\">III. Basic container usage</a>\n</h4>\n<h4>\n\
    <a id=\"user-content-iv-using-a-container-on-discovery\" class=\"anchor\" href=\"\
    #iv-using-a-container-on-discovery\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"#discovery\">IV. Using\
    \ a container on Discovery</a>\n</h4>\n<h4>\n<a id=\"user-content-v-updating-an-existing-container\"\
    \ class=\"anchor\" href=\"#v-updating-an-existing-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"\
    #updating\">V. Updating an existing container</a>\n</h4>\n<h4>\n<a id=\"user-content-vi-sharing-containers\"\
    \ class=\"anchor\" href=\"#vi-sharing-containers\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"#sharing\"\
    >VI. Sharing containers</a>\n</h4>\n<h4>\n<a id=\"user-content-vii-extra-resources\"\
    \ class=\"anchor\" href=\"#vii-extra-resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"#resources\">VII. Extra\
    \ resources</a>\n</h4>\n<h2>\n<a id=\"user-content--pre-requisites-osx-only\"\
    \ class=\"anchor\" href=\"#-pre-requisites-osx-only\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-prereqs\"\
    ></a> Pre-requisites (OSX only!)</h2>\n<p><em>Because singularity runs primarily\
    \ on linux, we need to create a virtual linux environment on OSX in order to build/manipulate\
    \ singularity containers. Follow this step first if you're using OSX.</em></p>\n\
    <h4>\n<a id=\"user-content-install-homebrew-package-manager\" class=\"anchor\"\
    \ href=\"#install-homebrew-package-manager\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install Homebrew package manager</h4>\n\
    <p>Homebrew is a package manager for OSX similar to apt-get or yum on linux. It\
    \ allows you to download and install different software (e.g. wget, or curl) and\
    \ allows you to build your own packages. Just copy and run the command below:</p>\n\
    <pre><code>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\
    \n</code></pre>\n<h4>\n<a id=\"user-content-use-homebrew-to-install-vagrant\"\
    \ class=\"anchor\" href=\"#use-homebrew-to-install-vagrant\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use Homebrew\
    \ to install Vagrant</h4>\n<p>Vagrant is a virtual development environment that\
    \ can be used to create virtual-machines (kind of similar to Virtualbox, but much\
    \ more powerful). It can be used to install and run another operating system on\
    \ your computer that's completely independent from your host OS. First we're going\
    \ to install vagrant via Homebrew.</p>\n<pre><code>brew cask install Virtualbox\n\
    brew cask install vagrant\nbrew cask install vagrant-manager\n</code></pre>\n\
    <h4>\n<a id=\"user-content-use-vagrant-to-create-a-virtual-machine\" class=\"\
    anchor\" href=\"#use-vagrant-to-create-a-virtual-machine\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use Vagrant\
    \ to create a virtual machine</h4>\n<p>Now that we have vagrant installed, we\
    \ can use it to make a brand new linux- based virtual machine, <strong>within</strong>\
    \ which singularity will be installed. It's from inside this vm that we're going\
    \ to do all future singularity container creation, modification etc.</p>\n<p>First\
    \ let's create a folder that our virtual machine will live in.</p>\n<pre><code>mkdir\
    \ singularity-vm\ncd singularity-vm\n</code></pre>\n<p>Now lets download a <em>vagrantfile</em>\
    \ for a prebuilt Ubuntu system that already has singularity installed.</p>\n<pre><code>vagrant\
    \ init singularityware/singularity-2.4\n</code></pre>\n<p>Finally we can start\
    \ up virtual machine and move into it.</p>\n<pre><code>#If this is the first time\
    \ you're building the vm the vagrant up command might take a minute or so to complete\n\
    vagrant up\nvagrant ssh\n</code></pre>\n<p>Whenver you're done using a vagrant\
    \ vm just use <code>ctrl+c</code> to exit the machine and type <code>vagrant halt</code>\
    \ to shut it down.</p>\n<h2>\n<a id=\"user-content-creating-a-singularity-container\"\
    \ class=\"anchor\" href=\"#creating-a-singularity-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"\
    user-content-creation\"></a>Creating a Singularity container</h2>\n<p>Let's begin\
    \ by creating a new folder within our vm for our brand new container (this isn't\
    \ strictly necessary but nice to keep different containers organized):</p>\n<pre><code>mkdir\
    \ miniconda\ncd miniconda\n</code></pre>\n<p>The first thing we need to do in\
    \ order to create a singularity container is make a singularity <em>definition</em>\
    \ file. This is just an instruction set that singularity will use to create a\
    \ container. Think of this definition file as a recipe, and the container as the\
    \ final product. Within this recipe, specify everything you need to in order create\
    \ your custom analysis environment. Sharing this definition file with others will\
    \ enable them to identically reproduce the steps it took to create your container.</p>\n\
    <p>To get you started here's an example definition file that we're going to use\
    \ for this demo. This is a simple neurodebian flavored container with miniconda\
    \ installed along with numpy and scipy.<br>\nLet's save this to a file called\
    \ <code>miniconda.def</code></p>\n<pre><code># Singularity definition example\
    \ with miniconda\n# Matteo Visconti di Oleggio Castello; Eshin Jolly\n# mvdoc.gr@dartmouth.edu;\
    \ eshin.jolly.gr@dartmouth.edu\n# May 2017\n\nbootstrap: docker\nfrom: neurodebian:jessie\n\
    \n# this command assumes at least singularity 2.3\n%environment\n    PATH=\"/usr/local/anaconda/bin:$PATH\"\
    \n%post\n    # install debian packages\n    apt-get update\n    apt-get install\
    \ -y eatmydata\n    eatmydata apt-get install -y wget bzip2 \\\n      ca-certificates\
    \ libglib2.0-0 libxext6 libsm6 libxrender1 \\\n      git git-annex-standalone\n\
    \    apt-get clean\n\n    # install anaconda\n    if [ ! -d /usr/local/anaconda\
    \ ]; then\n         wget https://repo.continuum.io/miniconda/Miniconda2-4.3.14-Linux-x86_64.sh\
    \ \\\n            -O ~/anaconda.sh &amp;&amp; \\\n         bash ~/anaconda.sh\
    \ -b -p /usr/local/anaconda &amp;&amp; \\\n         rm ~/anaconda.sh\n    fi\n\
    \    # set anaconda path\n    export PATH=\"/usr/local/anaconda/bin:$PATH\"\n\n\
    \    # install the bare minimum\n    conda install\\\n      numpy scipy\n    conda\
    \ clean --tarballs\n\n    # make /data and /scripts so we can mount it to access\
    \ external resources\n    if [ ! -d /data ]; then mkdir /data; fi\n    if [ !\
    \ -d /scripts ]; then mkdir /scripts; fi\n\n%runscript\n    echo \"Now inside\
    \ Singularity container woah...\"\n    exec /bin/bash\n\n</code></pre>\n<p>Now\
    \ lets use our vagrant vm and create a blank singularity image allocating 4gb\
    \ of disk space within our container. You may need to adjust this depending on\
    \ how much software you plan to install. By default the vagrant vm will share\
    \ <code>/vagrant</code> with your host OS so lets perform our operation in there\
    \ within the container folder we created earlier.</p>\n<pre><code>vagrant up\n\
    vagrant ssh\ncd /vagrant/miniconda\n# Now let's build it!\nsudo singularity build\
    \ miniconda.img miniconda.def\n</code></pre>\n<h2>\n<a id=\"user-content-basic-container-usage\"\
    \ class=\"anchor\" href=\"#basic-container-usage\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-basicusage\"\
    ></a>Basic container usage</h2>\n<p>If all went well we should be able to issue\
    \ a python command to the python version installed <em>within</em> our container\
    \ like so:</p>\n<pre><code>singularity exec miniconda.img python -c 'print \"\
    Hello from Singularity!\"'\n</code></pre>\n<p>We can also open up our container\
    \ and work <em>inside</em> it interactively:</p>\n<pre><code>singularity run miniconda.img\n\
    conda list\n</code></pre>\n<p>Press <code>ctrl+d</code> to exit the container.</p>\n\
    <p>Most commonly you'll use one of three commands with a container:<br>\n<code>singularity\
    \ exec</code> to run a specific command/file/script using the container<br>\n\
    <code>singularity run</code> to move into a container and use it interactively;\
    \ what   gets run by this command is dictated by your singularity <em>definition</em>\
    \ file<br>\n<code>singularity shell</code> similar to above, but specifically\
    \ open up a shell within the container</p>\n<p>A few other useful flags include:\n\
    <code>-B</code> mount an external folder to the container<br>\n<code>-c</code>\
    \ don't automatically map /home and /tmp to shared folders with the host OS</p>\n\
    <h2>\n<a id=\"user-content-using-a-container-on-discovery\" class=\"anchor\" href=\"\
    #using-a-container-on-discovery\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><a name=\"user-content-discovery\"\
    ></a>Using a container on Discovery</h2>\n<p>In order to use a container on Discovery\
    \ you have to first upload the generated .img file to your home directory. Since\
    \ containers can be rather large lets compress this and then uncompress on Discovery\
    \ (starting with Singularity &gt;=2.3.0 this functionality works through <code>import</code>\
    \ and <code>export</code> commands)</p>\n<pre><code>tar -cvzf miniconda.tar.gz\
    \ miniconda.img\nscp miniconda.tar.gz ejolly@discovery.dartmouth.edu:~\nssh ejolly@discovery.dartmouth.edu\n\
    tar -xvzf miniconda.tar.gz\n</code></pre>\n<p>Now you can utilize the container\
    \ by loading the singularity module and utilizing any of the singularity commands\
    \ above. There is <strong>one catch</strong> however: by default singularity will\
    \ try to melt together any environment variables defined in your account on discovery\
    \ with environment variables defined within the container. The rationale behind\
    \ this is that singularity offers the ability to <em>seamlessly</em> blend a custom\
    \ environment (i.e. your container built with all your goodies) and the functionality\
    \ of your HPC (i.e. all the goodies that already exist on Discovery). However,\
    \ often times you want to turn this functionality off and only use environment\
    \ variables within your container to avoid conflicts (i.e. completely ignore environment\
    \ variables set on Discovery). Here's how we do that:</p>\n<pre><code>module load\
    \ singularity\nsingularity run -e miniconda.img\n</code></pre>\n<p>To make our\
    \ lives easier we can create a simple bash script that executes a command in our\
    \ container making sure to call it with all the extra flags we want (e.g. mounting\
    \ some folders, ignoring environment variables). I personally like to create two\
    \ scripts one for interactively working with a container and one for using it\
    \ to execute commands for example with job submission. Here are some examples,\
    \ you'll need to adapt them to mount the directories you want:<br>\nLet's save\
    \ the following code into a bash file called: exec_miniconda</p>\n<pre><code>#!/bin/bash\n\
    singularity -e  exec \\\n    -B /idata/lchang/Projects:/data \\\n    -B /ihome/ejolly/scripts/:/scripts\
    \ \\\n    miniconda.img \"$@\"\n</code></pre>\n<p>Let's save the following code\
    \ into a bash file called: interact_miniconda</p>\n<pre><code>#!/bin/bash\nsingularity\
    \ -e run \\\n\t-c \\\n\t-B /idata/lchang/Projects/Pinel:/data \\\n\t-B ~/scripts:/scripts\
    \ \\\n\tminiconda.img\n</code></pre>\n<p>Now we issue a command to our container\
    \ (e.g. when submitting a job) like this:</p>\n<pre><code>./exec_miniconda python\
    \ -c 'print \"Hello World!\"'\n</code></pre>\n<p>We can also use our container\
    \ interactively with. Here let's actually serve a jupyter notebook server from\
    \ the cluster and interact with it using our local web browser. To do so we need\
    \ to reconnect to Discovery with port-forwarding.  The demo container here isn't\
    \ built with a jupyter notebook so this won't work, but we you can use the same\
    \ command when building your own container</p>\n<pre><code># You should really\
    \ connect to something other than the head node here!\nssh ejolly@discovery.dartmouth.edu\
    \ -N -f -L localhost:3129:localhost:9999\n\n./exec_miniconda jupyter notebook\
    \ --no-browser --port=9999\n# On local machine navigate to localhost:3129 in a\
    \ web browser\n</code></pre>\n<h2>\n<a id=\"user-content-updating-an-existing-container\"\
    \ class=\"anchor\" href=\"#updating-an-existing-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a name=\"\
    user-content-updating\"></a>Updating an existing container</h2>\n<p>The preferred\
    \ way to update a container is to modify the definition file and rebuild the image\
    \ using the steps above. This ensures that any container image is always a product\
    \ of its definition file and is therefore easy to reproduce.</p>\n<p>However,\
    \ singularity makes it easy to make changes to an existing container as well using\
    \ the <code>--writable</code> flag with the <code>exec</code>, <code>run</code>,\
    \ or <code>shell</code> commands, e.g.</p>\n<pre><code>singularity exec --writable\
    \ miniconda.img apt-get install curl\n</code></pre>\n<p>You can also increase\
    \ the size of an existing container with the <code>expand</code> command, e.g.</p>\n\
    <pre><code>#Expand a container by 2gb\nsingularity expand --size 2048 miniconda.img\n\
    </code></pre>\n<h2>\n<a id=\"user-content-sharing-containers\" class=\"anchor\"\
    \ href=\"#sharing-containers\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><a name=\"user-content-sharing\"></a>Sharing\
    \ containers</h2>\n<p>One of the nice things about using singularity (and containers\
    \ in general) is that you can share your analysis environment with others. These\
    \ are served on <a href=\"https://singularity-hub.org\" rel=\"nofollow\">Singularity\
    \ hub</a>. Many prebuilt containers already exist that you easily download and\
    \ use.</p>\n<p>Let's say we want to use this <a href=\"https://singularity-hub.org/containers/105/\"\
    \ rel=\"nofollow\">container</a> prebuilt with tensor flow for GPUs. This is as\
    \ simple as:</p>\n<pre><code>singularity pull shub://researchapps/tensorflow:gpu\n\
    </code></pre>\n<p>Then you can setup run and execute scripts like above to use\
    \ it on Discovery.</p>\n<p>You can also easily share you custom container on Singularity\
    \ hub by committing your singularity definition file to github and flipping the\
    \ switch for that repository on singularity hub.</p>\n<h2>\n<a id=\"user-content-extra-resources\"\
    \ class=\"anchor\" href=\"#extra-resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a name=\"user-content-resources\"\
    ></a>Extra resources</h2>\n<p>Much of this tutorial is borrowed/integrated from\
    \ several helpful resources:</p>\n<h4>\n<a id=\"user-content-quick-guides\" class=\"\
    anchor\" href=\"#quick-guides\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Quick guides</h4>\n<p><a href=\"http://mvdoc.me/2017/using-singularity-to-make-analyses-reproducible.html\"\
    \ rel=\"nofollow\">Matteo Visconti's blogpost on getting started with singularity</a><br>\n\
    <a href=\"http://jinhyuncheong.com/jekyll/update/2016/07/24/How-to-use-the-Discovery-cluster.html\"\
    \ rel=\"nofollow\">Jin Cheong's quick guide to using the discovery cluster</a></p>\n\
    <h4>\n<a id=\"user-content-more-comprehensive-guides\" class=\"anchor\" href=\"\
    #more-comprehensive-guides\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>More comprehensive guides</h4>\n<p><a href=\"\
    http://singularity.lbl.gov/quickstart\" rel=\"nofollow\">Singularity Documentation</a><br>\n\
    <a href=\"http://techdoc.dartmouth.edu/discovery/\" rel=\"nofollow\">Discovery\
    \ Documentation</a></p>\n<h4>\n<a id=\"user-content-sharing-is-caring\" class=\"\
    anchor\" href=\"#sharing-is-caring\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sharing is caring</h4>\n<p><a\
    \ href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity hub</a><br>\n\
    <a href=\"https://hub.docker.com/\" rel=\"nofollow\">Docker hub</a></p>\n"
  stargazers_count: 12
  subscribers_count: 2
  topics: []
  updated_at: 1601041353.0
evanfloden/dpa-analysis:
  data_format: 2
  description: Nextflow Pipeline for the analysis of Double Progressive Alignment
    (DPA)
  filenames:
  - singularity/Singularity
  - singularity/.ipynb_checkpoints/Singularity-checkpoint
  full_name: evanfloden/dpa-analysis
  latest_release: v0.2.6
  readme: '<h1>

    <a id="user-content-fast-and-accurate-large-multiple-sequence-alignments-using-root-to-leave-regressive-computation"
    class="anchor" href="#fast-and-accurate-large-multiple-sequence-alignments-using-root-to-leave-regressive-computation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Fast
    and accurate large multiple sequence alignments using root-to-leave regressive
    computation</h1>

    <p>This repository contains data, documentation, analysis and Nextflow workflow
    for the manuscript "Fast and accurate large multiple sequence alignments using
    root-to-leave regressive computation".</p>

    <h4>

    <a id="user-content-for-details-on-how-to-use-the-regressive-multiple-sequence-alignment-method-see-the-t-coffee-documentation"
    class="anchor" href="#for-details-on-how-to-use-the-regressive-multiple-sequence-alignment-method-see-the-t-coffee-documentation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>For
    details on how to use the Regressive Multiple Sequence Alignment method, see the
    <a href="https://tcoffee.readthedocs.io/en/latest/tcoffee_quickstart_regressive.html"
    rel="nofollow">T-Coffee documentation</a>.</h4>

    <h3>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>

    <p>This workflow was written by Evan Floden (<a href="https://github.com/evanfloden">evanfloden</a>)
    and

    Edgar(<a href="https://github.com/edgano">edgano</a>) at the <a href="http://www.crg.eu"
    rel="nofollow">Center for Genomic Regulation (CRG)</a>.</p>

    <p>The authors who contributed to the analysis and manuscript are:</p>

    <ul>

    <li>Edgar Garriga Nogales</li>

    <li>Paolo Di Tommaso</li>

    <li>Cedrik Magis</li>

    <li>Ionas Erb</li>

    <li>Hafid Laayouni</li>

    <li>Fyodor Kondrashov</li>

    <li>Evan Floden</li>

    <li>Cedric Notredame</li>

    </ul>

    <h3>

    <a id="user-content-notebooks" class="anchor" href="#notebooks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notebooks</h3>

    <p>This repository contains a series of <a href="http://jupyter.org/" rel="nofollow">Jupyter
    Notebooks</a> that contain

    the steps for replicating the analysis, tables and figures in the manuscript.</p>

    <p>The index jupyter notebook can be found <a href="notebook/00_StartHere.ipynb">here</a>.</p>

    <p>The notebook executes the pipeline, some steps of which require a lot of resources.</p>

    <h3>

    <a id="user-content-pipeline" class="anchor" href="#pipeline" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline</h3>

    <p>The pipeline for generating trees, alignments and performing the evaluations
    is built using

    <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>, a workflow tool
    to run tasks across

    multiple compute infrastructures in a very portable manner. It comes with a docker
    container

    making installation trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-pipeline-quick-start" class="anchor" href="#pipeline-quick-start"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    Quick Start</h3>

    <p>Make sure you have either docker/singularity installed or the required dependencies
    listed

    in the last section.</p>

    <p>Install the Nextflow runtime by running the following command:</p>

    <pre><code>$ curl -fsSL get.nextflow.io | bash

    </code></pre>

    <p>When done, you can launch the pipeline execution by entering the command shown
    below:</p>

    <pre><code>$ nextflow run evanfloden/dpa-analysis

    </code></pre>

    <p>By default the pipeline is executed against the provided example dataset.

    Check the <em>Pipeline parameters</em>  section below to see how enter your data
    on the program

    command line.</p>

    <h3>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Containers</h3>

    <p>All the methods above are available in a <a href="http://www.docker.com" rel="nofollow">Docker</a>
    image on DockerHub <a href="https://hub.docker.com/r/cbcrg/regressive-msa/" rel="nofollow">here</a>
    and the image is tested to be compatible with the <a href="http://singularity.lbl.gov/"
    rel="nofollow">Singularity</a>.</p>

    <p>The container also contains test data consisting of protein sequences, reference
    alignments and trees in the directory <code>/test_data</code>.</p>

    <p>To launch the container interactively with Docker run:</p>

    <p><code>docker run cbcrg/regressive-msa</code></p>

    <p>To launch the container interactivly with Singularity run:</p>

    <p><code>singularity shell docker://cbcrg/regressive-msa</code></p>

    <h3>

    <a id="user-content-pipeline-parameters" class="anchor" href="#pipeline-parameters"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    parameters</h3>

    <h4>

    <a id="user-content---seqs" class="anchor" href="#--seqs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--seqs</code>

    </h4>

    <ul>

    <li>Specifies the location of the input <em>fasta</em> file(s).</li>

    <li>Multiple files can be specified using the usual wildcards (*, ?), in this
    case make sure to surround the parameter string

    value by single quote characters (see the example below)</li>

    </ul>

    <p>Example:</p>

    <pre><code>$ nextflow run evanfloden/dpa-analysis --seqs ''/home/seqs/*.fasta''

    </code></pre>

    <p>This will handle each fasta file as a seperate sample.</p>

    <h4>

    <a id="user-content---refs" class="anchor" href="#--refs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--refs</code>

    </h4>

    <ul>

    <li>Specifies the location of the reference <em>aligned fasta</em> file(s).</li>

    </ul>

    <h4>

    <a id="user-content---trees" class="anchor" href="#--trees" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--trees</code>

    </h4>

    <ul>

    <li>Specifies the location of input tree file(s).</li>

    </ul>

    <h4>

    <a id="user-content---align_method" class="anchor" href="#--align_method" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--align_method</code>

    </h4>

    <ul>

    <li>Specifies which alignment methods should be used.</li>

    <li>Options include: "CLUSTALO,MAFFT-FFTNS1,MAFFT-SPARSECORE,MAFFT-GINSI,PROBCONS,UPP"</li>

    </ul>

    <h4>

    <a id="user-content---tree_method" class="anchor" href="#--tree_method" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--tree_method</code>

    </h4>

    <ul>

    <li>Specifies which guide-tree / clustering methods should be used.</li>

    <li>Options include: "CLUSTALO,MAFFT_PARTTREE"</li>

    </ul>

    <h4>

    <a id="user-content---regressive_align" class="anchor" href="#--regressive_align"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--regressive_align</code>

    </h4>

    <ul>

    <li>Flag to generate regressive MSAs.</li>

    <li>See <code>templates/dpa_align</code> for the specific commands executed.</li>

    </ul>

    <h4>

    <a id="user-content---stardard_align" class="anchor" href="#--stardard_align"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><code>--stardard_align</code>

    </h4>

    <ul>

    <li>Flag to perform standard MSAs.</li>

    <li>Standard MSA is alignment where the guide-tree is provided as input.</li>

    <li>See <code>templates/std_align</code> for the specific commands executed.</li>

    </ul>

    <h4>

    <a id="user-content---default_align" class="anchor" href="#--default_align" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--default_align</code>

    </h4>

    <ul>

    <li>Flag to perform default MSAs.</li>

    <li>Default MSA is alignment where the alignment software uses an internally generated
    guide-tree.</li>

    <li>See <code>templates/default_align</code> for the specific commands executed.</li>

    </ul>

    <h4>

    <a id="user-content---evaluate" class="anchor" href="#--evaluate" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--evaluate</code>

    </h4>

    <ul>

    <li>Flag to perform evaluation of the alignments.</li>

    <li>Requires reference sequences to be provided with the <code>--refs</code> parameter.</li>

    </ul>

    <h4>

    <a id="user-content---buckets" class="anchor" href="#--buckets" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--buckets</code>

    </h4>

    <ul>

    <li>List of bucket sizes or maximum size of the subMSAs in the regressive proceedure.</li>

    <li>Default value is "1000" sequences.</li>

    </ul>

    <h4>

    <a id="user-content---output" class="anchor" href="#--output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><code>--output</code>

    </h4>

    <ul>

    <li>Location of the results.</li>

    <li>Default locations is <code>results</code> directory.</li>

    </ul>

    '
  stargazers_count: 5
  subscribers_count: 0
  topics: []
  updated_at: 1620129144.0
faustus123/hdsingularity:
  data_format: 2
  description: Repository used to build Singularity containers of HD software
  filenames:
  - Singularity
  full_name: faustus123/hdsingularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-hdsingularity" class="anchor" href="#hdsingularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>hdsingularity</h1>

    <p>Repository used to build Singularity containers of HD software</p>

    <p>Checkout singularity-hub.org for details</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1501591637.0
frankwillmore/alcf-singularity:
  data_format: 2
  description: local settings
  filenames:
  - examples/multistage/Singularity
  - examples/asciinema/Singularity
  - examples/opensuse/Singularity
  - examples/scratch/Singularity.alpine
  - examples/scratch/Singularity.busybox
  - examples/busybox/Singularity
  - examples/self/Singularity
  - examples/raspbian/Singularity
  - examples/docker/Singularity
  - examples/ubuntu/Singularity
  - examples/arch/Singularity
  - examples/centos/Singularity
  - examples/shub/Singularity
  - examples/instances/Singularity
  - examples/debian/Singularity
  - examples/library/Singularity
  - examples/apps/Singularity
  - examples/apps/Singularity.cowsay
  - examples/scientific/Singularity
  full_name: frankwillmore/alcf-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <p><a href="https://travis-ci.org/sylabs/singularity" rel="nofollow"><img src="https://camo.githubusercontent.com/a1646c42a348a1331feb3842e34171e866c139adbae2608ba5fbd2c022c9c20f/68747470733a2f2f7472617669732d63692e6f72672f73796c6162732f73696e67756c61726974792e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/sylabs/singularity.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://circleci.com/gh/sylabs/singularity/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg"
    style="max-width:100%;"></a>

    <a href="https://goreportcard.com/report/github.com/sylabs/singularity" rel="nofollow"><img
    src="https://camo.githubusercontent.com/179d3d939b6a64c4f021860776fdc6243bc26409e966f1aa6bd7d35ca9593fea/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f73796c6162732f73696e67756c6172697479"
    alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/sylabs/singularity"
    style="max-width:100%;"></a></p>

    <ul>

    <li><a href="CONTRIBUTING.md">Guidelines for Contributing</a></li>

    <li><a href=".github/PULL_REQUEST_TEMPLATE.md">Pull Request Template</a></li>

    <li><a href="LICENSE.md">Project License</a></li>

    <li><a href="https://www.sylabs.io/docs/" rel="nofollow">Documentation</a></li>

    <li><a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459"
    rel="nofollow">Citation</a></li>

    </ul>

    <p>Singularity is an open source container platform designed to be simple, fast,
    and secure. Singularity is optimized for <a href="https://www.sylabs.io/2018/09/singularity-is-enterprise-performance-computing/"
    rel="nofollow">EPC</a> and HPC workloads, allowing untrusted users to run untrusted
    containers in a trusted way.</p>

    <p>Check out <a href="https://www.sylabs.io/singularity/whos-using-singularity/"
    rel="nofollow">who is using Singularity</a> and some <a href="https://www.sylabs.io/category/how-tos/"
    rel="nofollow">use cases of Singularity</a> on our website.</p>

    <h2>

    <a id="user-content-getting-started-with-singularity" class="anchor" href="#getting-started-with-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with Singularity</h2>

    <p>To install Singularity from source, see the <a href="INSTALL.md">installation
    instructions</a>. For other installation options, see <a href="https://www.sylabs.io/guides/3.0/user-guide/installation.html"
    rel="nofollow">our website</a>.</p>

    <p>For system administrators, see the <a href="https://www.sylabs.io/guides/3.0/admin-guide/"
    rel="nofollow">administrator documentation</a>.</p>

    <p>For users, see the <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow">user
    documentation</a>.</p>

    <h2>

    <a id="user-content-contributing-to-singularity" class="anchor" href="#contributing-to-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    to Singularity</h2>

    <p>Community contributions are always greatly appreciated. To start developing
    Singularity, check out the <a href="CONTRIBUTING.md">guidelines for contributing</a>.</p>

    <p>We also welcome contributions to our <a href="https://github.com/sylabs/singularity-userdocs">user
    docs</a> and <a href="https://github.com/sylabs/singularity-admindocs">admin docs</a>.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>To get help with Singularity, check out the <a href="https://www.sylabs.io/singularity/community/"
    rel="nofollow">Community Portal</a>.</p>

    <p>For additional support, <a href="https://www.sylabs.io/contact/" rel="nofollow">contact
    us</a> to receive more information.</p>

    <h2>

    <a id="user-content-cite-as" class="anchor" href="#cite-as" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cite as:</h2>

    <pre><code>Kurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers
    for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459

    </code></pre>

    <p>We also have a Zenodo citation:</p>

    <pre><code>Kurtzer, Gregory M.. (2016). Singularity 2.1.2 - Linux application
    and environment

    containers for science. 10.5281/zenodo.60736


    https://doi.org/10.5281/zenodo.60736

    </code></pre>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p><em>Unless otherwise noted, this project is licensed under a 3-clause BSD license
    found in the <a href="LICENSE.md">license file</a>.</em></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1558040154.0
fredjaya/rec-bench:
  data_format: 2
  description: automated benchmarking of recombination detection methods
  filenames:
  - Singularity
  - simg/Singularity.3seq
  full_name: fredjaya/rec-bench
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-rec-bench\" class=\"anchor\" href=\"#rec-bench\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>rec-bench</h1>\n<p>Automated benchmarking of recombination detection\
    \ methods</p>\n<p>Eternally a WIP - many things are hardcoded</p>\n<h2>\n<a id=\"\
    user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Dependencies</h2>\n\
    <p>Nextflow\nconda</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h2>\n<pre><code>git clone https://github.com/fredjaya/rec-bench.git\n\
    </code></pre>\n<p>Nextflow doesn't appear to create the conda environment properly.\
    \ Create manually.</p>\n<pre><code>conda env create -f environment.yml\nconda\
    \ activate fredjaya-rec-bench-0.1.0\n</code></pre>\n<p>Note: conda processes currently\
    \ hardcoded in <code>main.nf</code></p>\n<h2>\n<a id=\"user-content-usage\" class=\"\
    anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Usage</h2>\n<p><code>rec-bench</code> has five\
    \ modes that must be specified with <code>--mode</code> as follows:</p>\n<p><code>--mode\
    \ sim</code>\tGenerate simulation datasets\n<code>--mode sim_v</code>\tVisualise/summarise\
    \ simulation outputs\n<code>--mode div</code>\tBenchmark recombination detection\
    \ methods using simulated data\n<code>--mode emp</code>\tDetect recombination\
    \ in empirical sequence alignments\n<code>--mode class</code>\tCalculate classification\
    \ metrics</p>\n<p><code>nextflow run main.nf --help</code></p>\n<ul>\n<li>[ ]\
    \ Update readme</li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1617772984.0
funkelab/arid:
  data_format: 2
  description: Affinity Representing Instance Descriptors
  filenames:
  - singularity/Singularity
  full_name: funkelab/arid
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-connprep\" class=\"anchor\" href=\"#connprep\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>connprep</h1>\n<p>Produce preprocessed fMRI images ready for connectivity\
    \ analysis.</p>\n<h2>\n<a id=\"user-content-pipeline\" class=\"anchor\" href=\"\
    #pipeline\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pipeline</h2>\n<ol>\n<li>Drop initial or final volumes as specified.\
    \ Default: Analyze all volumes.</li>\n<li>Get the TR (volume acquisition time)\
    \ from pixdim[4] field of the Nifti header.</li>\n<li>Slice timing correction.\
    \ Default: none.</li>\n<li>Head motion realignment (SPM12 two-stage) and production\
    \ of mean fMRI.</li>\n<li>Rigid body coregistration of mean fMRI to T1 structural.</li>\n\
    <li>Compute volume quality metrics FD, DVARS.</li>\n<li>Reslice realigned fMRI\
    \ to native space, and also warp to MNI space using CAT12 transform.</li>\n<li>Remove\
    \ confounds from the native and MNI space fMRIs by simultaneous regression. Defaults:\n\
    <ul>\n<li>0.01 - 0.10 Hz bandpass filter</li>\n<li>6 estimated motion parameters\
    \ and their first differences</li>\n<li>6 principal components from the white\
    \ matter + CSF compartment</li>\n</ul>\n</li>\n<li>Repeat the confound removal,\
    \ additionally removing the mean signal of the gray matter compartment.</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Inputs</h2>\n\
    <pre><code>num_initial_vols_to_drop      0       Number of initial volumes to\
    \ drop\nnum_vols_to_analyze           all     Total number of volumes to analyze\n\
    bandpasslo_hz                 0.01    Low edge of bandpass filter in Hz\nbandpasshi_hz\
    \                 0.10    High edge of bandpass filter\nmot_PCs              \
    \         6       Number of PCs of motion params to remove\nmotderiv_PCs     \
    \             6       Same for motion derivatives\nwmcsf_PCs                 \
    \    6       Same for white matter/CSF compartment\nslorder                  \
    \     none    Slice timing correction, SPM12 nomenclature \nfmri_niigz       \
    \                     fMRI images, 4D Nifti\nmt1_niigz                       \
    \      T1 structural\ndeffwd_niigz                          Forward deformation\
    \ of T1 to MNI\ngray_niigz                            Gray matter volume fraction\n\
    white_niigz                           White matter volume fraction\ncsf_niigz\
    \                             CSF volume fraction\nproject                   \
    \            XNAT project label\nsubject                               XNAT subject\
    \ label\nsession                               XNAT session label\nscan      \
    \                            XNAT scan label\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<pre><code>connprep.pdf\
    \                               Processing report\nrp_adfmri.txt             \
    \                 Realignment parameters\nFD.txt                             \
    \        Framewise displacement\nDVARS.txt                                  Framewise\
    \ noise\nfiltered_keepgm_noscrub_nadfmri.nii.gz     Filtered data, native space,\
    \ gray matter signal retained\nfiltered_keepgm_noscrub_wadfmri.nii.gz     Filtered\
    \ data, MNI space, gray matter signal retained\nfiltered_removegm_noscrub_nadfmri.nii.gz\
    \   Filtered data, native space, gray matter signal removed\nfiltered_removegm_noscrub_wadfmri.nii.gz\
    \   Filtered data, MNI space, gray matter signal removed\nmeanadfmri.nii.gz  \
    \                        Mean fMRI, native space\nwmeanadfmri.nii.gz         \
    \                Mean fMRI, MNI space\nstats_keepgm_noscrub.txt              \
    \     Processing info when gray matter signal retained\nstats_removegm_noscrub.txt\
    \                 Processing info when gray matter signal removed\ngm_mask.nii.gz\
    \                             Native space gray matter mask\nwmcsf_mask.nii.gz\
    \                          Native space white matter/CSF mask\nconfounds_keepgm_noscrub.txt\
    \               Confounds matrix when gray matter signal retained\nconfounds_removegm_noscrub.txt\
    \             Confounds matrix  when gray matter signal removed\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1562764827.0
funkelab/funlib.run:
  data_format: 2
  description: Python wrapper for submitting jobs via bsub with the option to do so
    in a container environment.
  filenames:
  - singularity/Singularity
  full_name: funkelab/funlib.run
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-funlibrun\" class=\"anchor\" href=\"#funlibrun\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>funlib.run</h1>\n<p>Python wrapper for submitting jobs via bsub with\
    \ the option to do so in a container environment.</p>\n<h2>\n<a id=\"user-content-setup\"\
    \ class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Setup</h2>\n<pre><code>make install-full\n\
    </code></pre>\n<p>This creates a funlib.run config file ~/.funlib.run\nthat contains\
    \ default parameters that\ncan be overwritten for each specific run:</p>\n<pre><code>num_gpus\
    \ = 1\nmemory = 25600\nworking_directory = .\nsingularity = \"\"\nhost = \"\"\n\
    queue = \"normal\"\nenvironment = \"\"\nbatch = False\nmount_dirs = \"\"\n</code></pre>\n\
    <h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <p>There are three useful ways to use funlib.run:</p>\n<ol>\n<li>Direct usage\
    \ via command line arguments (overwrites config file defaults):</li>\n</ol>\n\
    <div class=\"highlight highlight-source-shell\"><pre>python run.py -p <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>python train.py<span class=\"pl-pds\">\"\
    </span></span> -c 5 -g 1 -q normal -s path-to-singularity-image\n\npython run_singularity.py\
    \ -p <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python mknet.py<span\
    \ class=\"pl-pds\">\"</span></span> -s path-to-singularity-image</pre></div>\n\
    <ol start=\"2\">\n<li>Indirect call via another script:</li>\n</ol>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span\
    \ class=\"pl-s1\">funlib</span>.<span class=\"pl-s1\">run</span> <span class=\"\
    pl-k\">import</span> <span class=\"pl-s1\">run</span>, <span class=\"pl-s1\">run_singularity</span>\n\
    \n<span class=\"pl-en\">run</span>(<span class=\"pl-s1\">command</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">\"python train.py\"</span>,\n    <span class=\"\
    pl-s1\">num_cpus</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">5</span>,\n\
    \    <span class=\"pl-s1\">num_gpus</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">1</span>,\n    <span class=\"pl-s1\">queue</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">\"normal\"</span>,\n    <span class=\"pl-s1\"\
    >execute</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n\
    \n<span class=\"pl-en\">run_singularity</span>(<span class=\"pl-s1\">command</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s\">\"python mknet.py\"</span>,\n \
    \               <span class=\"pl-s1\">singularity_image</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-s\">\"path_to_image\"</span>,\n                <span\
    \ class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >True</span>)</pre></div>\n<ol start=\"3\">\n<li>Command creation and subsequent\
    \ call:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-k\">from</span> <span class=\"pl-s1\">funlib</span>.<span class=\"\
    pl-s1\">run</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">run</span>,\
    \ <span class=\"pl-s1\">run_singularity</span>\n<span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">subprocess</span> <span class=\"pl-k\">import</span> <span\
    \ class=\"pl-s1\">check_call</span>\n\n<span class=\"pl-s1\">run_command</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">run</span>(<span class=\"\
    pl-s1\">command</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"python\
    \ train.py\"</span>,\n                  <span class=\"pl-s1\">num_cpus</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">5</span>,\n                  <span\
    \ class=\"pl-s1\">num_gpus</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">1</span>,\n                  <span class=\"pl-s1\">queue</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">\"normal\"</span>,\n                  <span\
    \ class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\"\
    >False</span>)\n\n<span class=\"pl-en\">check_call</span>(<span class=\"pl-s1\"\
    >run_command</span>,\n           <span class=\"pl-s1\">shell</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-s1\">run_singularity_command</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">run_singularity</span>(<span\
    \ class=\"pl-s1\">command</span><span class=\"pl-c1\">=</span><span class=\"pl-s\"\
    >\"python mknet.py\"</span>,\n                                          <span\
    \ class=\"pl-s1\">singularity_image</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s\">\"path_to_image\"</span>,\n                                 \
    \         <span class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-c1\">False</span>)\n\n<span class=\"pl-en\">check_call</span>(<span\
    \ class=\"pl-s1\">run_singularity_command</span>,\n           <span class=\"pl-s1\"\
    >shell</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n\
    <h2>\n<a id=\"user-content-usage-with-daisy\" class=\"anchor\" href=\"#usage-with-daisy\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage with Daisy</h2>\n<p>When used with daisy.call do not expand\
    \ the cmd to a string via setting expand=False:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-s1\">cmd</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-en\">run</span>(<span class=\"pl-s1\">command</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s1\">base_command</span>,\n          <span class=\"pl-s1\">queue</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s1\">queue</span>,\n          <span\
    \ class=\"pl-s1\">num_gpus</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">1</span>,\n          <span class=\"pl-s1\">num_cpus</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">num_cpus</span>,\n          <span class=\"\
    pl-s1\">singularity_image</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\"\
    >singularity_container</span>,\n          <span class=\"pl-s1\">mount_dirs</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s1\">mount_dirs</span>,\n         \
    \ <span class=\"pl-s1\">execute</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">False</span>,\n          <span class=\"pl-s1\">expand</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-s1\">daisy</span>.<span\
    \ class=\"pl-en\">call</span>(<span class=\"pl-s1\">cmd</span>, <span class=\"\
    pl-s1\">log_out</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">log_out</span>,\
    \ <span class=\"pl-s1\">log_err</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-s1\">log_err</span>)</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1594370552.0
funkelab/lsd:
  data_format: 2
  description: "\U0001F308"
  filenames:
  - singularity/Singularity
  full_name: funkelab/lsd
  latest_release: null
  readme: '<h1>

    <a id="user-content-local-shape-descriptors-for-neuron-segmentation" class="anchor"
    href="#local-shape-descriptors-for-neuron-segmentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Local Shape Descriptors
    (for Neuron Segmentation)</h1>

    <p><a href="https://camo.githubusercontent.com/616be86b75e53c18d0a1ca705fb3f62e97caa07dee7a7caa77cb26f3fd9bbefc/68747470733a2f2f6c6f63616c736861706564657363726970746f72732e6769746875622e696f2f6173736574732f696d672f33645f6d6573685f766563742e6a706567"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/616be86b75e53c18d0a1ca705fb3f62e97caa07dee7a7caa77cb26f3fd9bbefc/68747470733a2f2f6c6f63616c736861706564657363726970746f72732e6769746875622e696f2f6173736574732f696d672f33645f6d6573685f766563742e6a706567"
    alt="" data-canonical-src="https://localshapedescriptors.github.io/assets/img/3d_mesh_vect.jpeg"
    style="max-width:100%;"></a></p>

    <p>This repository contains code to compute Local Shape Descriptors (LSDs) from
    an instance segmentation. Those LSDs can then be used during training as an auxiliary
    target, which we found to improve boundary prediction and therefore segmentation
    quality. Read more about it in our <a href="https://www.biorxiv.org/content/10.1101/2021.01.18.427039v1"
    rel="nofollow">paper</a>.</p>

    <p>Here you find:</p>

    <ul>

    <li>LSD calculation from <code>numpy</code> arrays: <code>lsd/local_shape_descriptor.py</code>

    </li>

    <li>LSD <a href="http://funkey.science/gunpowder" rel="nofollow"><code>gunpowder</code></a>
    node: <code>lsd/gp/add_local_shape_descriptor.py</code>

    </li>

    </ul>

    <p>Soon (ETA February 1st), you will also find here:</p>

    <ul>

    <li>links to all training and testing data, as well as results</li>

    <li>scripts, examples, and tutorials for parallel inference and post-processing</li>

    </ul>

    '
  stargazers_count: 7
  subscribers_count: 4
  topics: []
  updated_at: 1617541983.0
funkelab/synful:
  data_format: 2
  description: Synaptic Partner Detection in 3D Microscopy Volumes
  filenames:
  - singularity/Singularity_py2.7.recipe
  - singularity/Singularity_py3.recipe
  full_name: funkelab/synful
  latest_release: v1.0
  readme: '<p><a href="https://zenodo.org/badge/latestdoi/166422086" rel="nofollow"><img
    src="https://camo.githubusercontent.com/e31cfa1af0774be894dee535edc05a6536309dc42e048f576dc489a330b1f8ec/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3136363432323038362e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/166422086.svg" style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-synful" class="anchor" href="#synful" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Synful</h1>

    <h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p>Synful: A project for the automated detection of synaptic partners in Electron
    Microscopy brain data using U-Nets (type of Convolutional Neural Network).</p>

    <p>This repository provides train and predict scripts for synaptic partner detection.
    For more details, see our <a href="https://www.biorxiv.org/content/10.1101/2019.12.12.874172v1"
    rel="nofollow">bioRxiv preprint</a>.</p>

    <p>We used the method to predict 244 Million synaptic partners in the full adult
    fly brain (FAFB) dataset.

    Please see <a href="https://github.com/funkelab/synful_fafb">https://github.com/funkelab/synful_fafb</a>
    for data dissemination and benchmark datasets.</p>

    <p>Please don''t hesitate to open

    an issue or write us an email (<a href="mailto:buhmannj@janelia.hhmi.org">Julia

    Buhmann</a> or <a href="mailto:funkej@janelia.hhmi.org">Jan

    Funke</a>) if you have any questions!</p>

    <ul>

    <li>[x] Add train scripts</li>

    <li>[x] Add inference scripts</li>

    <li>[x] Add download links for pretrained models</li>

    </ul>

    <h2>

    <a id="user-content-method" class="anchor" href="#method" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Method</h2>

    <p>The pipeline processes 3D raw data in two steps into synaptic partners:</p>

    <ol>

    <li>inference of a) <code>syn_indicator_mask</code> (postsynaptic locations) and
    b) <code>direction_vector</code> (vector pointing from postsynaptic location to
    its presynaptic partner)</li>

    <li>synapse extraction: a) locations extractions based on <code>syn_indicator_mask</code>
    and b) finding presynaptic partner based on <code>direction_vector</code>

    </li>

    </ol>

    <p><a href="docs/_static/method_overview.png" target="_blank" rel="noopener noreferrer"><img
    src="docs/_static/method_overview.png" alt="method_figure" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-system-requirements" class="anchor" href="#system-requirements"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>System
    Requirements</h2>

    <ul>

    <li>Hardware requirements

    <ul>

    <li>training and prediction requires at least one GPU with sufficient memory (12
    GB)</li>

    <li>For instance, we mostly used <code>GeForce GTX TITAN X 12 GB</code> for our
    project</li>

    </ul>

    </li>

    <li>Software requirements

    <ul>

    <li>Software has been tested on Linux (Ubuntu 16.04)</li>

    </ul>

    </li>

    </ul>

    <h2>

    <a id="user-content-installation-guide" class="anchor" href="#installation-guide"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation
    Guide</h2>

    <p>from source (creating a conda env is optional, but recommended).</p>

    <ul>

    <li>Clone this repository.</li>

    <li>In a terminal:</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>conda create -n <span class="pl-k">&lt;</span>conda_env_name<span
    class="pl-k">&gt;</span> python=3.6

    <span class="pl-c1">source</span> activate <span class="pl-k">&lt;</span>conda_env_name<span
    class="pl-k">&gt;</span>

    <span class="pl-c1">cd</span> synful

    pip install -r requirements.txt

    python setup.py install</pre></div>

    <p>If you are interested in using the package for training and prediction, additionally
    add tensorflow and funlib.learn.tensorflow to your conda env:</p>

    <div class="highlight highlight-source-shell"><pre>conda install tensorflow-gpu=1.14
    cudatoolkit=10.0

    pip install git+git://github.com/funkelab/funlib.learn.tensorflow@0712fee6b6c083c6bfc86e76f475b2e40b3c64f2

    </pre></div>

    <h4>

    <a id="user-content-install-time" class="anchor" href="#install-time" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install time</h4>

    <p>Installation should take around 5 mins (including 3 mins for the tensorflow
    installation).</p>

    <h2>

    <a id="user-content-training" class="anchor" href="#training" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Training</h2>

    <p>Training scripts are found in</p>

    <pre><code>train/&lt;setup&gt;

    </code></pre>

    <p>where <code>&lt;setup&gt;</code> is the name of a particular network configuration.

    In such a  directory, you will find two files:</p>

    <ul>

    <li>

    <code>generate_network.py</code> (generates a tensorflow network based on the
    parameter.json file in the same directoy)</li>

    <li>

    <code>train.py</code> (starts training)</li>

    </ul>

    <p>To get started, have a look at the train script in <a href="train/setup01">train/setup01/train.py</a>.</p>

    <p>To start training:</p>

    <div class="highlight highlight-source-shell"><pre>python generate_network.py
    parameter.json

    python train.py parameter.json</pre></div>

    <ul>

    <li>setup01: parameter.json is set to train a network on post-synaptic sites (single-task
    network)</li>

    <li>setup02: parameter.json is set to train on direction vectors (single-task
    network)</li>

    <li>setup03: parameter.json is set to train on both post-synaptic sites and direction
    vectors (multi-task network)</li>

    </ul>

    <h4>

    <a id="user-content-details-on-hyperparameters" class="anchor" href="#details-on-hyperparameters"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Details
    on hyperparameters</h4>

    <p>When training a network, you can set following hyperparameters in <code>scripts/train/&lt;setup01/02/03&gt;/parameter.json</code></p>

    <p>Parameters to set the architecture of the network (also see <a href="https://github.com/funkelab/funlib.learn.tensorflow/blob/master/funlib/learn/tensorflow/models/unet.py#L506">doc</a>
    where we create the U-Net)</p>

    <ul>

    <li>

    <code>input_size</code>: the dimensions of the cube that is used as input (called
    a mini-batch)</li>

    <li>

    <code>downsample_factor</code> = [[1, 3, 3], [1, 3, 3], [3, 3, 3]] creates a U-Net
    with four resolution levels

    <ul>

    <li>the first one being the original resolution, the second one with downsampled
    feature maps with factos [1, 3, 3] etc.</li>

    </ul>

    </li>

    <li>

    <code>fmap_num</code>: Number of feature maps in the first layer (we used 4 in
    the paper)</li>

    <li>

    <code>fmap_inc_factor</code>: In each layer, we use <code>fmap_inc_factor</code>
    to increase our number of feature maps (we used 5 and 12 in the paper)

    <ul>

    <li>Eg. if we have <code>fmap_num = 4</code> and <code>fmap_inc_factor = 5</code>
    , we have 20 in our first layer, 100 in our second layer ...</li>

    </ul>

    </li>

    <li>

    <code>unet_model</code>: vanilla, or dh_unet; vanille=single-task network, dh_unet=multitask
    network with two different upsampling paths</li>

    </ul>

    <p>Training parameters</p>

    <ul>

    <li>

    <code>learning_rate</code>: we used the AdamOptimizer across all experiments,
    with beta1=0.95,beta2=0.999,epsilon=1e-8</li>

    </ul>

    <p>ST / MT parameters</p>

    <ul>

    <li>

    <code>loss_comb_type</code>: in a multi-task setting, how to combine the two different
    losses</li>

    <li>

    <code>m_loss_scale</code> : loss weight for post-synaptic mask</li>

    <li>

    <code>d_loss_scale</code> : loss weight for direction vector field</li>

    </ul>

    <p>Balancing parameters needed to account for sparsity of synaptic sites</p>

    <ul>

    <li>

    <code>reject_probability</code> : 0.95 - p_rej in paper --&gt; reject empty mini-batches
    with probability <code>reject_probability</code>

    </li>

    <li>

    <code>clip_range</code> : the loss is scaled with the inverse class frequency
    ratio of foreground-and background voxels, clipping at <code>clip_range</code>

    </li>

    </ul>

    <h4>

    <a id="user-content-training-runtime" class="anchor" href="#training-runtime"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training
    runtime</h4>

    <p>Training takes between 3 and 10 days (depending on the size of the network),
    but you should see reasonable results within a day (after 90k iterations).</p>

    <h3>

    <a id="user-content-monitoring-training" class="anchor" href="#monitoring-training"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Monitoring
    Training</h3>

    <p>To visualize snapshots that are produced during training use this <a href="scripts/visualization/visualize_snapshot.py">script</a>:</p>

    <pre><code>python -i visualize_snapshot.py 300001 setup01

    </code></pre>

    <p>in order to load iteration <code>300001</code> of training setup <code>setup01</code>
    (use -1 to indicate most recent snapshot)</p>

    <h2>

    <a id="user-content-inference" class="anchor" href="#inference" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Inference</h2>

    <p>Once you trained a network, you can use this script to run inference:</p>

    <pre><code>cd scripts/predict/

    python predict_blockwise.py predict_template.json

    </code></pre>

    <p>Adapt following parameters in the configfile &lt;scripts/predict/predict_template.json&gt;:</p>

    <ul>

    <li>

    <code>db_host</code> --&gt; Put here the name of your running mongodb server (this
    is used to track which chunks are processed)</li>

    <li>

    <code>raw_file</code> --&gt; Put here the filepath of your raw data (as an example
    you can use the CREMI data that you can download from <a href="http://www.cremi.org"
    rel="nofollow">www.cremi.org</a>)</li>

    </ul>

    <p>For a full list of parameters and explanation, see: &lt;scripts/predict/predict_blockwise.py&gt;.</p>

    <h4>

    <a id="user-content-inference-runtime" class="anchor" href="#inference-runtime"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Inference
    runtime</h4>

    <p>Processing a CREMI cube (5 microns X 5 microns x 5 microns) takes ~4 minutes
    on a single GPU.</p>

    <h2>

    <a id="user-content-pretrained-models" class="anchor" href="#pretrained-models"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pretrained
    Models</h2>

    <p>We provide pretrained models, that we discuss in detail in our <a href="https://www.biorxiv.org/content/10.1101/2019.12.12.874172v2"
    rel="nofollow">bioRxiv preprint</a>. You will find the results of our gridsearch
    and the parameters that we used in Figure 3 <code>Validation results on CREMI
    dataset</code>.</p>

    <p>We provide four models that you can download from <a href="https://www.dropbox.com/s/301382766164ism/pretrained.zip?dl=0"
    rel="nofollow">here</a>.</p>

    <p>Please extract the zip file into &lt;scripts/train/&gt; of this repository,
    this will add for each model a setup directory with the necassary config files,
    tensorflow checkpoint and predict script.</p>

    <p>For instance for <code>p_setup52</code> (marked orange in Figure 3, one of
    the best performing models), you will get all relevant files in &lt;scripts/train/p_setup52&gt;.

    To run inference, you have to change the setup parameter in the predict config
    file to <code>p_setup52</code> and proceed according to <a href="#Inference">inference
    section</a>.</p>

    <h4>

    <a id="user-content-details-about-the-provided-models" class="anchor" href="#details-about-the-provided-models"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Details
    about the provided models</h4>

    <table>

    <thead>

    <tr>

    <th>setup</th>

    <th>specs</th>

    <th>f-score with seg</th>

    <th>f-score without</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>p_setup52 (+p_setup10)</td>

    <td>big, curriculum, CE, ST</td>

    <td>0.76</td>

    <td>0.74</td>

    </tr>

    <tr>

    <td>p_setup51</td>

    <td>big, curriculum, CE, MT_2</td>

    <td>0.76</td>

    <td>0.73</td>

    </tr>

    <tr>

    <td>p_setup54 (+p_setup05)</td>

    <td>small, curriculum, MSE, ST</td>

    <td>0.76</td>

    <td>0.7</td>

    </tr>

    <tr>

    <td>p_setup45 (+p_setup05)</td>

    <td>small, standard, MSE, MT2</td>

    <td>0.73</td>

    <td>0.68</td>

    </tr>

    </tbody>

    </table>

    <p>Note, that for the models that have an underlying ST architecture we also indicate
    the setup for the corresponding direction-vector-models (p_setup05+p_setup10).

    If you want to use the model with highest accuracy, pick <code>p_setup52</code>;
    If you want to use a model that gives reasonnable results, but also has fast inference
    runtime, pick <code>p_setup54</code>.</p>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1616581090.0
funkelab/synister:
  data_format: 2
  description: Neurotransmitter prediction from EM.
  filenames:
  - singularity/Singularity
  full_name: funkelab/synister
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"\
    #universal-data-tool\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Universal Data Tool</h1>\n<p><a href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ alt=\"Master Branch\" style=\"max-width:100%;\"></a>\n<a href=\"https://badge.fury.io/js/universal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    ><img src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\"\
    \ alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\"\
    \ alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://twitter.com/UniversalDataTl\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\"\
    \ alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Try it out at <a href=\"https://udt.dev\"\
    \ rel=\"nofollow\">udt.dev</a>, <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >download the desktop app</a> or <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">run on-premise</a>.</p>\n<p align=\"center\">\n  <a href=\"\
    https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p align=\"center\">\n  <b>\n  <a href=\"\
    https://docs.universaldatatool.com\" rel=\"nofollow\">Docs</a> \u2022 <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">Website</a> \u2022 <a href=\"\
    https://udt.dev\" rel=\"nofollow\">Playground</a> \u2022 <a href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\"\
    \ rel=\"nofollow\">Library Usage</a> \u2022 <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">On-Premise</a>\n  </b>\n</p>\n<p>The Universal Data Tool is\
    \ a web/desktop app for editing and annotating images, text, audio, documents\
    \ and to view and edit any data defined in the extensible <a href=\"https://github.com/UniversalDataTool/udt-format\"\
    >.udt.json and .udt.csv standard</a>.</p>\n<h2>\n<a id=\"user-content-supported-data\"\
    \ class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Supported Data</h2>\n<p align=\"\
    center\">\n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\"\
    \ rel=\"nofollow\">Image Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\"\
    \ rel=\"nofollow\">Image Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\"\
    \ rel=\"nofollow\">Text Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\"\
    \ rel=\"nofollow\">Named Entity Recognition</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\"\
    \ rel=\"nofollow\">Named Entity Relations / Part of Speech Tagging</a> \u2022\
    \ \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\"\
    \ rel=\"nofollow\">Audio Transcription</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\"\
    \ rel=\"nofollow\">Data Entry</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\"\
    \ rel=\"nofollow\">Video Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\"\
    \ rel=\"nofollow\">Landmark / Pose Annotation</a>\n</p>\n<h2>\n<a id=\"user-content-recent-updates\"\
    \ class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recent Updates</h2>\n<p><a href=\"\
    https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\">Follow\
    \ our development on Youtube!</a></p>\n\n<ul>\n<li><a href=\"https://youtu.be/q20WrCRcG4k\"\
    \ rel=\"nofollow\">Community Update Video 9</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\"\
    \ rel=\"nofollow\">Community Update Video 8</a></li>\n<li>\n<a href=\"https://youtu.be/glPPFgXibdw\"\
    \ rel=\"nofollow\">Community Update Video 7</a> <a href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\"\
    \ rel=\"nofollow\">(blog version)</a>\n\n</li>\n</ul>\n<h2>\n<a id=\"user-content-features\"\
    \ class=\"anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Features</h2>\n<ul>\n<li><strong>Collaborate\
    \ with others in real time, no sign up!</strong></li>\n<li>Usable on <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">web</a> or as <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\
    >Windows,Mac or Linux desktop application</a>\n</li>\n<li>Configure your project\
    \ with an easy-to-use GUI</li>\n<li><a href=\"https://universaldatatool.com/courses\"\
    \ rel=\"nofollow\">Easily create courses to train your labelers</a></li>\n<li>Download/upload\
    \ as easy-to-use CSV (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\
    >sample.udt.csv</a>) or JSON (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\
    >sample.udt.json</a>)</li>\n<li>Support for Images, Videos, PDFs, Text, Audio\
    \ Transcription and many other formats</li>\n<li>Can be <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\
    >easily integrated into a React application</a>\n</li>\n<li>Annotate images or\
    \ videos with classifications, tags, bounding boxes, polygons and points</li>\n\
    <li>Fast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly</li>\n\
    <li>Import data from Google Drive, Youtube, CSV, Clipboard and more</li>\n<li>Annotate\
    \ NLP datasets with Named Entity Recognition (NER), classification and Part of\
    \ Speech (PoS) tagging.</li>\n<li>Easily <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\
    >load into pandas</a> or <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\
    >use with fast.ai</a>\n</li>\n<li>Runs <a href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\"\
    \ rel=\"nofollow\">with docker</a> <code>docker run -p 3000:3000 universaldatatool/universaldatatool</code>\n\
    </li>\n<li>Runs <a href=\"https://singularity-hub.org/collections/4792\" rel=\"\
    nofollow\">with singularity</a> <code>singularity run universaldatatool/universaldatatool</code>\n\
    </li>\n</ul>\n<p align=\"center\"><kbd><a href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<h2>\n<a id=\"user-content-sponsors\"\
    \ class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sponsors</h2>\n<p><a href=\"\
    https://wao.ai\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\"\
    \ alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"></a>\n<a href=\"\
    https://momentum-tech.ca/\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\"\
    \ alt=\"momentum image\" style=\"max-width:100%;\"></a>\n<a href=\"https://www.enabledintelligence.net/\"\
    \ rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\"\
    \ alt=\"enabled intelligence image\" style=\"max-width:100%;\"></a></p>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <h3>\n<a id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Web\
    \ App</h3>\n<p>Just visit <a href=\"https://universaldatatool.com\" rel=\"nofollow\"\
    >universaldatatool.com</a>!</p>\n<p><em>Trying to run the web app locally? Run\
    \ <code>npm install</code> then <code>npm run start</code> after cloning this\
    \ repository to start the web server.</em></p>\n<h3>\n<a id=\"user-content-desktop-application\"\
    \ class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Desktop Application</h3>\n<p>Download\
    \ the latest release from the <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >releases page</a> and run the executable you downloaded.</p>\n<h2>\n<a id=\"\
    user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n\
    <ul>\n<li>(Optional) Say hi in the <a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\">Slack channel</a>!</li>\n<li>Read <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\
    >this guide to get started with development</a>.</li>\n</ul>\n<h2>\n<a id=\"user-content-contributors-\"\
    \ class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors <g-emoji class=\"\
    g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\
    >\u2728</g-emoji>\n</h2>\n<p>Thanks goes to these wonderful people (<a href=\"\
    https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\">emoji key</a>):</p>\n\
    \n\n\n<table>\n  <tr>\n    <td align=\"center\">\n<a href=\"https://twitter.com/seveibar\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Severin Ibarluzea</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://puskuruk.github.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Puskuruk</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/CedricJean\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>CedricJean</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://berupon.hatenablog.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>beru</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Ownmarc\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Marc</b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Wafaa-arbash\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Wafaa-arbash</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/pgrimaud\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Pierre Grimaud</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/sreevardhanreddi\"><img src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>sreevardhanreddi</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/mrdadah\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Mohammed Eldadah</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://x8795278.blogspot.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>x213212</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hysios\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>hysios </b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://congdv.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Cong Dao</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Renato Junior</b></sub></a><br><a\
    \ href=\"#translation-MrJunato\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://gitlab.com/rickstaa\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Rick</b></sub></a><br><a\
    \ href=\"#translation-rickstaa\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/anaplian\"><img src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>anaplian</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.behance.net/MiguelCarvalho13\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Miguel Carvalho</b></sub></a><br><a\
    \ href=\"#translation-miguelcarvalho13\" title=\"Translation\"><g-emoji class=\"\
    g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://kyleo.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Kyle OBrien</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hakkiyagiz\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Hakk\u0131 Ya\u011F\u0131z ERD\u0130\
    N\xC7</b></sub></a><br><a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/jvdavim\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Jo\xE3o Victor Davim</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n</table>\n\n\n\n<p>This project follows the <a\
    \ href=\"https://github.com/all-contributors/all-contributors\">all-contributors</a>\
    \ specification. Contributions of any kind welcome!</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1617292105.0
geoschem/Singularity_GC:
  data_format: 2
  description: Singularity Recipe for GEOS-Chem
  filenames:
  - Singularity
  full_name: geoschem/Singularity_GC
  latest_release: null
  readme: '<h1>

    <a id="user-content-multi_atlas_app" class="anchor" href="#multi_atlas_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Multi_Atlas_app</h1>

    <p>This includes everything required (except for the "full-multi-atlas" directory)
    to build a docker and corresponding singularity container for the Multi Atlas
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/multi_atlas/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://singularity-hub.org/collections/734" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Multi_Atlas_app.git

    cd Multi_Atlas_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have full-multi-atlas directory which contains atlases.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/multi_atlas

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Multi_Atlas_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - geos-chem
  - singularity-container
  - docker-image
  updated_at: 1564082038.0
geoschem/Singularity_GCHP:
  data_format: 2
  description: Singularity Recipe for High-Performance GEOS-Chem (GCHP)
  filenames:
  - Singularity
  full_name: geoschem/Singularity_GCHP
  latest_release: null
  readme: '<h1>

    <a id="user-content-apsim" class="anchor" href="#apsim" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>APSIM</h1>

    <p>The Agricultural Production Systems sIMulator (APSIM) is internationally recognised
    as a highly advanced simulator of agricultural systems. It contains a suite of
    modules which enable the simulation of systems that cover a range of plant, animal,
    soil, climate and management interactions. APSIM is undergoing continual development,
    with new capability added to regular releases of official versions. Its development
    and maintenance is underpinned by rigorous science and software engineering standards.
    The APSIM Initiative has been established to promote the development and use of
    the science modules and infrastructure software of APSIM.</p>

    <p>CI builds of this repository can be found <a href="https://apsimdev.apsim.info/APSIM.Builds.Portal/Bob.aspx"
    rel="nofollow">Here</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1531155094.0
gipert/Singularity.def:
  data_format: 2
  description: My Singularity recipe files
  filenames:
  - bat/Singularity.def
  - asciinema/Singularity.def
  - arch-base/Singularity.def
  - centos-base/Singularity.def
  - lilypond/Singularity.def
  - gerda-tgsend/Singularity.def
  - root-cern/Singularity.def
  - julia/Singularity.def
  - itunes/Singularity.def
  - texlive/Singularity.def
  full_name: gipert/Singularity.def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - singularity
  - containers
  updated_at: 1587858477.0
gnperdue/singularity_imgs:
  data_format: 2
  description: Singularity images for deep learning software
  filenames:
  - Singularity.py3_tf2gnt
  - Singularity.py3_fast2
  - Singularity.py2_tf110
  - Singularity.py3_trch
  - Singularity.py3_tf1gnt
  - Singularity.py2_tf17
  - Singularity.py3_tf
  - Singularity.py3_dmda
  full_name: gnperdue/singularity_imgs
  latest_release: null
  readme: '<p>Singularity containers (with inspiration from J. Simone, <a href="https://github.com/TomaszGolan/mlmpr">T.
    Golan</a>, and <a href="https://github.com/DeepLearnPhysics/larcv2-singularity">K.
    Terao</a>).</p>

    <p><a href="https://singularity-hub.org/collections/998" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ul>

    <li>Pull, e.g. <code>$ singularity pull shub://gnperdue/singularity_imgs:py2_tf17</code>

    </li>

    </ul>

    <h2>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <ul>

    <li>

    <code>Singularity.py2_tf110</code> - See <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu">TF</a>
    for base package definition.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - singularity-hub
  - singularity-container
  updated_at: 1593117348.0
gpuhackathons-org/gpubootcamp:
  data_format: 2
  description: This repository consists for gpu bootcamp material for HPC and AI
  filenames:
  - hpc/nways/Singularity
  - hpc/miniprofiler/Singularity
  - hpc/openacc/Singularity
  - misc/jupyter_lab_template/appName/Singularity
  - ai/DeepStream/Singularity
  - ai/DeepStream_Perf_Lab/Singularity
  - ai/RAPIDS/Singularity
  - hpc_ai/ai_science_cfd/Singularity
  - hpc_ai/ai_science_climate/Singularity
  full_name: gpuhackathons-org/gpubootcamp
  latest_release: null
  readme: '<h1>

    <a id="user-content-gpubootcamp-official-training-materials" class="anchor" href="#gpubootcamp-official-training-materials"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GPUBootcamp
    Official Training Materials</h1>

    <p>This repository consists of GPU bootcamp material for both HPC and AI:</p>

    <ul>

    <li>

    <p><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai">AI</a></p>

    </li>

    <li>

    <p><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc">HPC</a></p>

    </li>

    <li>

    <p><a href="https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc_ai">HPC_AI</a></p>

    </li>

    </ul>

    <h1>

    <a id="user-content-system-requirements" class="anchor" href="#system-requirements"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>System
    Requirements</h1>

    <p>Each lab contains docker and singularity definition files. Follow the readme
    files inside each on how to build the container and run the labs inside it.</p>


    <h2>

    <a id="user-content-questions" class="anchor" href="#questions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Questions?</h2>

    <p>Please join <a href="https://openacclang.slack.com/messages/openaccusergroup"
    rel="nofollow">OpenACC Slack Channel</a> for questions.</p>

    '
  stargazers_count: 62
  subscribers_count: 6
  topics: []
  updated_at: 1622636147.0
granek/singularity-rstudio-base:
  data_format: 2
  description: 'Singularity image to serve as base for all project images.  Defaults
    to starting up RStudio with an auto-selected port and password '
  filenames:
  - Singularity.mro.4.0.3
  - Singularity.3.6.1
  - Singularity.4.0.2
  - Singularity.3.6.0
  - Singularity.4.0.3
  full_name: granek/singularity-rstudio-base
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3197" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>This Singularity image is intended to serve as base for all project images.</p>

    <p>By default it starts up RStudio with an auto-selected port and password</p>

    <h1>

    <a id="user-content-running-singularity-image" class="anchor" href="#running-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    Singularity Image</h1>

    <p>Run a singularity-rstudio-base container with <code>singularity run shub://granek/singularity-rstudio-base</code></p>

    <h2>

    <a id="user-content-tmp-issues" class="anchor" href="#tmp-issues" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>/tmp issues</h2>

    <p>It is recommended to do one of the following when running this image. There
    is no need to do both:</p>

    <ol>

    <li>Set "mount tmp = no" in <code>/etc/singularity/singularity.conf</code>.</li>

    <li>If #1 is not an option, the following command can be used to bind mount <code>/tmp</code>
    in the container to a "private" tmp directory:</li>

    </ol>

    <pre><code>SINGTMP="/tmp/${USER}_$$_tmp"; mkdir -p $SINGTMP; singularity run --bind
    $SINGTMP:/tmp shub://granek/singularity-rstudio-base

    </code></pre>

    <h3>

    <a id="user-content-tmp-issues-tldr" class="anchor" href="#tmp-issues-tldr" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>/tmp issues TLDR</h3>

    <p>If a second user tries on the same server tries to run an RStudio container
    they will have permission issues with <code>/tmp/rstudio-server</code>, which
    will be owned by the user who first ran an RStudio container.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1609875369.0
grst/containerize-conda:
  data_format: 2
  description: Turn an existing conda environment into a Docker or Singularity container
  filenames:
  - Singularity
  full_name: grst/containerize-conda
  latest_release: null
  readme: '<h1>

    <a id="user-content-containerize-an-existing-conda-environment" class="anchor"
    href="#containerize-an-existing-conda-environment" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Containerize an existing conda environment</h1>

    <p>I use conda environments for working on data analysis projects.

    Sometimes I need to revert to install using <code>pip</code> or <code>R</code>''s

    <code>install.packages</code> if a package is not on bioconda or conda-forge.</p>

    <p>This makes it very hard to reproduce the environment, and therefore,

    the analysis, on another system. Even pure conda environments stored

    as an <code>environment.yml</code> file tend to <a href="https://github.com/conda/conda/issues/9257">break
    after a

    while</a>.</p>

    <p>Using the instructions below allows to package an existing environment

    into a Docker or Singularity container which should be more portable

    and can also easily be integrated into a <a href="https://grst.github.io/bioinformatics/2019/12/23/reportsrender.html"
    rel="nofollow">fully reproducible

    data analysis

    workflow</a>

    based on e.g. <a href="https://www.nextflow.io/" rel="nofollow">Nextflow</a>.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <ul>

    <li><a href="https://conda.github.io/conda-pack/" rel="nofollow">conda-pack</a></li>

    <li>either Docker, Podman or Singularity</li>

    <li>source conda environment needs to be on a linux x64 machine.</li>

    </ul>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ol>

    <li>Clone this repository (retrieve <code>Dockerfile</code>/<code>Singularity</code>)</li>

    </ol>

    <pre><code>git clone git@github.com:grst/containerize-conda.git

    cd containerize-conda

    </code></pre>

    <ol start="2">

    <li>Pack the environment</li>

    </ol>

    <pre><code>conda-pack -n &lt;MY_ENV&gt; -o packed_environment.tar.gz

    </code></pre>

    <ol start="3">

    <li>Build the container</li>

    </ol>

    <pre><code># With singularity

    singularity build --fakeroot &lt;OUTPUT_CONTAINER.sif&gt; Singularity


    # With Docker

    docker build . -t &lt;TAG&gt;


    # With Podman/Buildah

    podman build . -t &lt;TAG&gt;

    </code></pre>

    <h2>

    <a id="user-content-how-it-works" class="anchor" href="#how-it-works" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How it works</h2>

    <p>Conda environment can''t be just "moved" to another location, as some paths
    are

    hardcoded into the environment. <code>conda-pack</code> takes care of replacing
    these paths

    back to placeholders and creates a <code>.tar.gz</code> archive that contains
    the

    environment. This environment can be unpacked to another machine (or, in our

    case, a container). Running <code>conda-unpack</code> in the environment replaces
    the

    placeholders back to the actual paths matching the new location.</p>

    '
  stargazers_count: 9
  subscribers_count: 2
  topics: []
  updated_at: 1622029405.0
gwastro/pycbc_bench:
  data_format: 2
  description: Some benchmark singularity images for pycbc / pycbc inference
  filenames:
  - Singularity
  full_name: gwastro/pycbc_bench
  latest_release: null
  readme: '<h1>

    <a id="user-content-pycbc_bench" class="anchor" href="#pycbc_bench" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pycbc_bench</h1>

    <p>Some benchmark singularity images for pycbc / pycbc inference</p>

    <h1>

    <a id="user-content-build-singularity-image" class="anchor" href="#build-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>build
    singularity image</h1>

    <p>sudo singularity build pycbcb.img Singularity</p>

    <h1>

    <a id="user-content-run-pycbc-inspiral" class="anchor" href="#run-pycbc-inspiral"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>run
    pycbc inspiral</h1>

    <p>singularity run --cleanenv --app inspiral pycbcb.img</p>

    '
  stargazers_count: 1
  subscribers_count: 8
  topics: []
  updated_at: 1559569839.0
heathsc/gemBS:
  data_format: 2
  description: gemBS is a bioinformatics pipeline designed for high throughput analysis
    of DNA methylation from Whole Genome Bisulfite Sequencing data (WGBS).
  filenames:
  - Singularity
  - IHEC/Singularity.ihec
  full_name: heathsc/gemBS
  latest_release: v3.5.1_IHEC
  readme: "<h1>\n<a id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>News</h1>\n\
    <p>First release of gemBS-rs, a complete rewrite of the gemBS pipeline (apart\
    \ from the mapper) in Rust bringing increased\nstability while maintaining the\
    \ high performance of gemBS: <a href=\"https://github.com/heathsc/gemBS-rs.git\"\
    >https://github.com/heathsc/gemBS-rs.git</a></p>\n<h1>\n<a id=\"user-content-gembs\"\
    \ class=\"anchor\" href=\"#gembs\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>gemBS</h1>\n<p>gemBS is a high performance\
    \ bioinformatic pipeline designed for highthroughput analysis\nof DNA methylation\
    \ data from whole genome bisulfites sequencing data\n(WGBS). It combines GEM3,\
    \ a high performance read aligner and\nbs_call, a high performance variant and\
    \ methyation caller, into a streamlined and efficient pipeline for\nbisulfite\
    \ sueqnce analysis.</p>\n<p>The manuscript describing the pipeline is available\
    \ <a href=\"https://www.biorxiv.org/content/early/2017/10/11/201988\" rel=\"nofollow\"\
    >here</a></p>\n<hr>\n<h2>\n<a id=\"user-content-licensing\" class=\"anchor\" href=\"\
    #licensing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Licensing</h2>\n<p>gemBS is licensed under GPL.</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Download</h2>\n\
    <p>Use <code>git clone --recursive</code> to retrieve the complete source code\
    \ including the code from external projects such as <code>bs_call</code> and <code>gem3-mapper</code>.</p>\n\
    <pre><code>git clone --recursive https://github.com/heathsc/gemBS.git\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<ol>\n<li>Before starting the installation of gemBS,\
    \ you will need to install\nor check the installation of several packages.</li>\n\
    </ol>\n<p>a) gcc with development libraries\nb) python3, pip3, matplotlib, multiprocess\n\
    c) zlib, lzma, openssl, libcurl, libncurses, wget, pigz</p>\n<p>If you are working\
    \ on a clean (fairly recent) Ubuntu installation, you\ncan install everything\
    \ required with the followiwg commands:</p>\n<pre><code>sudo apt-get update\n\
    sudo apt-get install -y python3 build-essential git python3-pip wget pigz\nsudo\
    \ apt-get install -y zlib1g-dev libbz2-dev\nsudo apt-get install -y libncurses5-dev\
    \ liblzma-dev libssl-dev libcurl4-openssl-dev\npip3 install matplotlib multiprocess\n\
    </code></pre>\n<ol start=\"2\">\n<li>\n<p>Download the gemBS distribution if you\
    \ haven't already done so:</p>\n<p><code>git clone --recursive https://github.com/heathsc/gemBS.git</code></p>\n\
    </li>\n<li>\n<p>Use python install command:</p>\n</li>\n</ol>\n<p>To install to\
    \ the standard system location (i.e., so that all users\ncan use gemBS):</p>\n\
    <pre><code>``python3 setup.py install``\n</code></pre>\n<p>To install to the user's\
    \ home directory:</p>\n<pre><code>``python3 setup.py install --user``\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-check-your-installation\" class=\"anchor\" href=\"\
    #check-your-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Check your installation</h2>\n<p>For checking\
    \ your installation follow this\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/example.html\"\
    \ rel=\"nofollow\">worked example</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Documentation\
    \ can be found at\n<a href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/index.html\"\
    \ rel=\"nofollow\">gemBS</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-changelog\"\
    \ class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Changelog:</h2>\n<pre><code>3.5.5\
    \ Fix logging bug caused by trimming change in 3.5.3\n3.5.4 Fix bug in the output\
    \ of strand specific cpg txt files (not\n      encode Bed files) where the 'C'\
    \ entry was not being printed\n3.5.3 Allow for read end specific trimming in bs_call\n\
    3.5.3 Enable range checks and asserts in bs_call all target; add bs_call debug\
    \ target\n3.5.2 Correct problems with gcc10.  Move to htslib/samtools/bcftools\
    \ version 1.11\n3.5.1 Check if C compiler requires --std=c99 flag for standards\
    \ conformant behaviour\n3.5.1 Make sure bgzip is copied correctly during installation\n\
    3.5.0 Make bs_call process contig pools from largest to smallest (this change\
    \ alters the sqlite db format so\n      if you have a previously started gemBS\
    \ run you should (a) remove the .gemBS directory, (b) redo the\n      'gemBS prepare'\
    \ step to recreate the db file and (3) run 'gemBS db-sync'. \n3.5.0 Switch bs_call\
    \ and snpxtr to use the new dbSNP index format\n3.5.0 Add ability of dbSNP to\
    \ read the new JSON and VCF  dbSNP format files\n      that are now used for human\
    \ and non-human species respectively\n3.5.0 Add multithreading to dbSNP_idx\n\
    3.5.0 Change format of dbSNP index to allow (a) efficient loading\n      of SNP\
    \ data for individual contigs and (b) parallel index creation \n3.5.0 Rewrite\
    \ mextr and snpxtr as standalone tools rather than\n      bcftools plugins.  Now\
    \ multithreaded and (relatively) memoryefficient\n3.5.0 Replace bedToBigBed and\
    \ wigToBigWig to reduce memory usage\n      and improve speed\n3.4.5 Fix crash\
    \ when using the -k (keep-mismatch) flag, and fix rare hangs at end of processing\n\
    3.4.4 Sort input bcf files to bcftools concat stage to ensure reproducibility.\n\
    3.4.4 Add extra sort keys when generating pools to ensure stability of pool membership\
    \ in the event of multiple contigs\n      having the same size\n3.4.3 Remove calculation\
    \ of the goodness of filter (GOF) as this is expensive, non-standard and unreliable.\
    \  Removing this\n      removes the dependency on GSL.\n3.4.3 Add autodetection\
    \ of output format to bs_call (unless explicitly specified on the command line)\n\
    3.4.2 Add CRAM support (via make_cram option in configuration file)\n3.4.1 Add\
    \ benchmark-mode that does not write date or program version numbers into SAM/BAM\
    \ or VCF/BCF files\n      Switch to samtools, bcftools and htslib v1.10\n3.4.0\
    \ Move to new bs_call version (2.1.0) which is more efficient\n      in memory\
    \ use and can read BAMs and write BCFs natively.\n      The new bs_call requires\
    \ a faidx indexed reference, so gemBS\n      no creates this during indexing.\n\
    3.4.0 Add switches to give more control to threads and memory\n      usage in\
    \ mapping and calling stages\n3.3.3 Remove legacy pathway for config files with\
    \ no header line (fix issue 'error in gemBS index #65)\n3.3.2 Fix error where\
    \ header line for wig files could be omitted\n3.3.2 Fix generation of non_cpg\
    \ files\n3.3.1 Fix Attribute error bug due to not checking if conversion is a\
    \ list\n3.3.0 Make new release for IHEC\n3.3.0 Switch conversion default in IHEC_standard\
    \ configuration to 0.01,0.05 rather than auto, which can give odd results if conversion\
    \ controls not present or not working correctly\n3.3.0 Fix bug where conversion\
    \ parameters could be ignored\n3.2.13 Fix formatting bug in mextr with multiple\
    \ samples (not triggered in normal gemBS use)\n3.2.12 Ensure that conversion statistics\
    \ are correctly calculated for non-stranded or reverse conversion protocols\n\
    3.2.11 Introduce reverse_conversion option for mapping where read 1 is G2A converted\
    \ and read 2 is C2T converted\n3.2.10 Correct regex patch for single end reads\n\
    3.2.9 Update Singularity and Dockerfile recipes to allow kemp utils to be built\
    \ correctly\n3.2.9 Make setup.py and gemBS/commands.py read the version information\
    \ from gemBS/version.py, so ensuring consistency\n3.2.9 Fix bug added in last\
    \ version where options in config file were not being taken into account\n3.2.8\
    \ Fix mis specification errors in long options for mextr. \n3.2.8 Fix bug where\
    \ mextr (methyl extract plugin for bcftools) would crash if cpg output  option\
    \ was not set.\n3.2.7 Apply patches for bugs in handling single end reads (suggested\
    \ by I. Moghul)\n3.2.7 Changed regex for filenames to make it more general (suggested\
    \ by I. Moghul)\n3.2.7 Fixed bug (reported by chhylp123) where zero arguments\
    \ to some options were being ignored\n3.2.6 Cleaned up compilation and cleaning\
    \ of gemBS tools\n3.2.6 Fixed python error if either the over conversion reference\
    \ sequence was not defined\n3.2.6 Added check in bs_call that conversion parameters\
    \ are valid (between 0 and 1)\n3.2.6 Perform more stringent sanity checking on\
    \ conversion vaalues when autocomputed by gemBS\n3.2.6 Use --diasble-lzma configuration\
    \ flag for samtools and bcftools as we don't need it and it removes an unneccesary\
    \ dependency\n3.2.6 Add install options --disable-cuda (on by default) and --enable-cuda\
    \ that affect GEM3 comppilation\n3.2.6 Bug fix with incorrect handling of duplicate\
    \ reads\n3.2.5 Minor bug fix - correct error with non-paired end non-bisulfite\
    \ reads\n3.2.4 Modify the bisulfite processing in gem-mapper to be more efficient\
    \ (in particular for the non-stranded option)\n3.2.4 Modify gemBS to use the new\
    \ conversion options for gem-mapper\n3.2.4 Switch gem-mapper to use option --underconversion-sequence\
    \ and --overconversion-sequence rather than --underconversion_sequence to be consistent\
    \ with other options\n3.2.3 Fixed bug if conversion parameters were not set\n\
    3.2.2 Rework non-stranded mode so that both possible conversions are tried and\
    \ the results merged\n3.2.2 Fix bug where non-stranded flag was not being passed\
    \ to mapper in paired end mode\n3.2.1 Move warning message from bscall from stdout\
    \ to stderr\n3.2.1 Switch Singularity build to use Ubuntu 16.04 rather than 18.04\
    \ to allow the image to work in CentOS 6 (Docker build changed as well to keep\
    \ the two in sync)\n3.2.1 Fix undeclared variable bugs and missing --ignore-deps\
    \ option in merge-bcfs\n3.2.1 Add default for dbSNP_index if dbSNP_files is set\n\
    3.2.1 Add gsl-path install option\n3.2.0 Make new release\n3.1.0 Make installation\
    \ process more modular.  Allow for sub-installs\n3.1.0 Add support for reading\
    \ config from ${index_dir}/gemBS.json if it exists\n3.1.0 Add --reference-bias\
    \ option to mextr and gemBS extract\n3.1.0 Add support for non-bisulfite mapping\
    \ of individual datasets\n3.1.0 Allow white space in variable values\n3.1.0 Allow\
    \ fallback to gzip if pigz not present\n3.1.0 Add --dry-run, --json, --ignore-db\
    \ and --ignore-dep to extract command\n3.1.0 Add --ignore-dep option to call and\
    \ merge-bcfs commands\n3.1.0 Add SNP extraction function to extract command\n\
    3.0 Make v3.0 release\n3.0 Merge with master branch.\n3.0 Bump samtools sort memory\
    \ limit to 2G\n3.0 Add extra_references option for reference generation\n3.0 Allow\
    \ input files to mapping to be shell commands\n3.0 Add links to documentation\n\
    3.0 Upload new yeast example and add documentation\n3.0 Add --dir option to gemBS\n\
    3.0 Add --ignore-db options for --dry-run / --json\n3.0 Add --json output option\
    \ for dry runs\n3.0 Update help text to match new functions\n3.0 Introduce standard\
    \ analysis configurations stored within distribution\n3.0 Switch gem3-mapper distribution\
    \ to gembs branch on official gem3-mapper repo\n3.0 Removal of incomplete files\
    \ and roll back of db in the event of pipeline failure\n3.0 Automatic removal\
    \ of individual BAMs and BCFs after successful merging\n3.0 Prevent pipelines\
    \ hanging in event of failure\n3.0 Generate ENCODE bed and bigbed files\n3.0 Switch\
    \ to python 3\n3.0 Switch to mextr for BCF filtering\n3.0 Include fetch and build\
    \ of samtools / bcftools during build process\n3.0 Add dry-run capability to map\
    \ and call commands\n3.0 Introduce contig pools to automatically group small contigs\n\
    3.0 Automatic generation of contig.size files from index build\n3.0 Allow use\
    \ of in memory sqlite3 db as an option\n3.0 Allow multiple instances of gemBS\
    \ (possible on different hosts) to work \n    simultaneously on the same analysis\n\
    3.0 Reduce and simply commands\n3.0 Add Dockerfile\n3.0 Add multi-threading and\
    \ multi-processing options for most commands\n3.0 Use sqlite3 to track progress\
    \ of analyses, file paths etc.\n3.0 Added more flexible configuration options\
    \ (new csv format + new configuration file)\n3.0 Remove test dataset from distribution\
    \ (distribute from web site)\n2.1.0 Ensure commands run during pipeline come from\
    \ installation\n2.1.0 Added Singularity build recipe\n2.1.0 Add new command gemBS\
    \ direct-mapping\n2.1.0 Fixed Makefile clean in tools\n2.0.2 Fixed bug related\
    \ with the percentage of High Quality Variant in Variants summary report.\n2.0.2\
    \ Check temporary directory existence.\n2.0.2 Fixed QualityNonRefCpg sample name\
    \ in png image.\n2.0.2 Fixed mapper issues related with aligning performace.\n\
    2.0.2 Fixed arguments for Under/Over Conversion sequence name in gem3-mapper\n\
    2.0.1 On bscall repository, fixed argument -k about discarded reads that do not\
    \ form proper pairs.\n2.0 Check tmp folder before starting mapping process.\n\
    2.0 Added Left and Right Trimming optional arguments to gemBS bscall.\n2.0 Added\
    \ GC Coverage correlation value to BS Call Stats Summary.\n2.0 Fixed error when\
    \ reporting complete path to not found bam files.\n2.0 Fixed iteration over sampleBams\
    \ dictionary in MergeAll method.\n2.0 Updated: Avoid redo indexing when merging\
    \ just one file.\n2.0 Changed conversion formula.\n2.0 Added parameter for dbSNP.\n\
    2.0 Added threads to bscall.\n2.0 Removed CpGs reports. Already done from bscall\
    \ report.\n2.0 Fixed bs_call makefile for the gcc to be used.\n2.0 New bscall\
    \ version. Generates JSON report.\n2.0 Removed gemBS options snp-stats,cpg-report,cpg-stats.\n\
    2.0 Added summary report from the bs_call json stats\n2.0 New BSCall Report. From\
    \ bscall son file generates three types of reports:\n    Mapping and Coverage\
    \ Report\n    Bs-Genotypes Calls Report\n    Methylation Statistics report\n1.7\
    \ Added non stranded read conversion parameter\n1.7 Fixed SE crash when estimating\
    \ overlapped bases.\n1.7 Fixed gem-index (gem3) to follow fastq and SAM specifications.\
    \ \n    Modified gem3-mapper repository external module.\n    New external module\
    \ https://github.com/heathsc/gem3-mapper.git\n1.7 Fixed threads parameter to samtools\
    \ merge\n1.7 Fixed threads parameter to gem-mapper\n1.7 Removed Indels Field on\
    \ Variants Report.\n1.7 Added Overlapping Bases at Mapping Report\n1.7 Modified\
    \ Base Counts Overall, removed Base Counts general and Base Counts Overall\n1.7\
    \ New Dinucleotide CpGs Report\n    New table dinucleotide stats\n    New plots\
    \ for Informative Reads and CpGs\n    Methylation levels plots for different types\
    \ of CpGs\n    Summary Table\n1.7 New Readme file to inform about report test\n\
    1.7 New basic statis table for Variants Report\n1.7 Removed parameter -r (reference\
    \ length) parameter for mapping reports command (gemBS bsMap).\n1.6 New CpGs Density\
    \ plot, include box plos, bar plot and fitting curve\n1.6 Change name at CpG report:\n\
    \    \"Heterozygous\" for \"Alternative CX\"\n    \"De Novo CpGs Methylation Status\"\
    \ for \"Non Reference CpGs\"\n    \"CpGs with SNP\" for \"SNPs (CX) at Reference\
    \ CpGs\"\n1.6 CpGs Report Simplified to Q&gt;20\n1.6 BigWig Default parameters\
    \ for filtering CpG per a given quality and a total number of supported informative\
    \ reads   \n1.5 Initial Release  \n</code></pre>\n<hr>\n<h2>\n<a id=\"user-content-developers\"\
    \ class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Developers</h2>\n<p>gemBS:</p>\n\
    <ul>\n<li>Marcos Fernandez-Callejo - <a href=\"mailto:marcos.fernandez@cnag.crg.eu\"\
    >marcos.fernandez@cnag.crg.eu</a>\n</li>\n<li>Simon Heath - <a href=\"mailto:simon.heath@gmail.com\"\
    >simon.heath@gmail.com</a>\n</li>\n</ul>\n<p>gem mapper:</p>\n<ul>\n<li>Santiago\
    \ Marco-Sola - <a href=\"mailto:santiagomsola@gmail.com\">santiagomsola@gmail.com</a>\n\
    </li>\n</ul>\n<p>bisulfite caller and filtering:</p>\n<ul>\n<li>Simon Heath -\
    \ <a href=\"mailto:simon.heath@gmail.com\">simon.heath@gmail.com</a>\n</li>\n\
    </ul>\n"
  stargazers_count: 25
  subscribers_count: 4
  topics: []
  updated_at: 1618985852.0
heathsc/gemBS-rs:
  data_format: 2
  description: A re-write of the gemBS pipeline framework in Rust
  filenames:
  - Singularity
  - texlive/Singularity.tex
  full_name: heathsc/gemBS-rs
  latest_release: v4.0
  readme: "<h1>\n<a id=\"user-content-gembs-rs\" class=\"anchor\" href=\"#gembs-rs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>gemBS-rs</h1>\n<p>A rewrite of the <a href=\"https://github.com/heathsc/gemBS\"\
    >gemBS</a> pipeline\nframework from Python/C into Rust.</p>\n<p>gemBS is a high\
    \ performance bioinformatic pipeline designed for highthroughput analysis\nof\
    \ DNA methylation data from whole genome bisulfites sequencing data\n(WGBS). It\
    \ combines GEM3, a high performance read aligner and\nbs_call, a high performance\
    \ variant and methyation caller, into a streamlined and efficient pipeline for\n\
    bisulfite sequence analysis.</p>\n<p>The manuscript describing the original gemBS\
    \ pipeline is available\n<a href=\"https://doi.org/10.1093/bioinformatics/bty690\"\
    \ rel=\"nofollow\">here</a></p>\n<p>The rewrite of the pipeline into Rust has\
    \ two aims: (1) to have a more\nrobust pipeline and (2) to provide a more flexible\
    \ platform for future\ndevelopments.  All of the tools developed for the pipeline\
    \ except for the GEM3 mapper (being an external project that is also very stable!)\
    \ have now been re-written in Rust. These include bs_call, the methylation and\
    \ SNV-variant caller, and the methylation and SNP extractions tools mextr and\
    \ snpxtr.  In all cases the running times are comparable to the original C versions.</p>\n\
    <hr>\n<h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<p>The pipeline uses samtools for generating sorted\
    \ BAM/CRAM files from GEM3 and bcftools for merging and indexing BCF files produced\
    \ by bs_call.  In addition, many of the tools link to htslb to enable reading\
    \ of BAM/CRAM and reading/writing of BCF files.  Samtools and htslib are automatically\
    \ installed during the installation of the gemBS pipeline.   There is also an\
    \ optional dependency on TeXLive which is used to produce pdf versions of the\
    \ QC reports.  If requested by the user this is also installed with the pipeline.</p>\n\
    <hr>\n<h2>\n<a id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Licensing</h2>\n<p>gemBS is licensed under the GPL.</p>\n<hr>\n<h2>\n\
    <a id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Download</h2>\n\
    <p>Use <code>git clone --recursive</code> to retrieve the complete source code\
    \ including the code from external projects such as <code>gem3-mapper</code>.</p>\n\
    <pre><code>git clone --recursive https://github.com/heathsc/gemBS-rs.git\n</code></pre>\n\
    <hr>\n<h2>\n<a id=\"user-content-configure--install\" class=\"anchor\" href=\"\
    #configure--install\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Configure &amp; Install</h2>\n<p>Before starting\
    \ the installation of gemBS, you will need to install\nor check the installation\
    \ of several packages.</p>\n<p>a) gcc with development libraries</p>\n<p>b) rust\
    \ (for installation instructions see <a href=\"https://www.rust-lang.org/learn/get-started\"\
    \ rel=\"nofollow\">here</a>).  Note that if you have rust already installed you\
    \ should update it using <code>rustup update</code> before trying to compile gemBS.</p>\n\
    <p>c) zlib, libz2, lzma, openssl, libcurl, libncurses, wget, expat, ncurses, openssl,\
    \ freetype, fontconfig</p>\n<p>If you are working on a clean (fairly recent) Ubuntu\
    \ installation, you\ncan install everything required with the following commands:</p>\n\
    <pre><code>apt-get install -y build-essential git autoconf wget lbzip2 pkg-config\
    \ cmake\napt-get install -y zlib1g-dev libbz2-dev libexpat1-dev\napt-get install\
    \ -y libncurses5-dev liblzma-dev libssl-dev libcurl4-openssl-dev curl\napt-get\
    \ install -y libfreetype6-dev libfontconfig1-dev\ncurl https://sh.rustup.rs -sSf\
    \ &gt; rust.sh &amp;&amp; sh rust.sh -y\n</code></pre>\n<p>Download the gemBS\
    \ distribution if you haven't already done so:</p>\n<pre><code>git clone --recursive\
    \ https://github.com/heathsc/gemBS-rs.git\n</code></pre>\n<p>From the main gemBS-rs\
    \ directory type the following to make the default config file:</p>\n<pre><code>make\
    \ gemBS_config.mk\n</code></pre>\n<p>Then look at the file gemBS_config.mk and\
    \ make any changes that are required.  When the file is OK the pipeline and components\
    \ can be built and installed by typing:</p>\n<pre><code>make install\n</code></pre>\n\
    <p>If the make and install process is successful, a shell script called gemBS\
    \ will be created in the main gemBS-rs directory.  This file should be copied\
    \ to a directory that is in your PATH so that gemBS can be invoked from anywhere.</p>\n\
    <hr>\n<h2>\n<a id=\"user-content-check-your-installation\" class=\"anchor\" href=\"\
    #check-your-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Check your installation</h2>\n<p>For checking\
    \ your installation follow this\n<a href=\"http://statgen.cnag.cat/gemBS/UserGuide/_build/html/example.html\"\
    \ rel=\"nofollow\">worked example</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Documentation\
    \ can be found at\n<a href=\"http://statgen.cnag.cat/gemBS/\" rel=\"nofollow\"\
    >gemBS</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-changelog\" class=\"anchor\"\
    \ href=\"#changelog\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Changelog:</h2>\n<pre><code>4.0 First release\
    \ of gemBS-rs (4th release of gemBS)\n4.0.1 Correct bug preventing that caused\
    \ non-stranded mapping to fail\n4.0.2 Move to versions 1.12 of htslib/samtools/bcftools\n\
    4.0.2 Change way we iterate over SAM/BAM/CRAM files to the same way used in samtools\
    \ \n      view (the old way did not always work with cram files)\n</code></pre>\n"
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1621670204.0
hejm37/sysu-planner:
  data_format: 2
  description: sparkle planning challenge
  filenames:
  - Singularity
  full_name: hejm37/sysu-planner
  latest_release: null
  readme: '<h1>

    <a id="user-content-sysu-planner" class="anchor" href="#sysu-planner" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>sysu-planner</h1>

    <p>The SYSU-Planner is a two-stage planner designed to solve classical planning
    problems. It first performs the 1-BFWS (<a href="https://people.eng.unimelb.edu.au/nlipovetzky/papers/aaai17-BFWS-novelty-exploration.pdf"
    rel="nofollow">Nir and Hector 2017</a>) with very fast speed. If it fails to find
    a solution, it will then perform a modified online refinement algorithm named
    <a href="http://ada.liacs.nl/events/sparkle-planning-19/documents/solver_description/SYSU-planner-description.pdf"
    rel="nofollow">Forward-RHC</a> (see also <a href="https://ipc2018-classical.bitbucket.io/planner-abstracts/team8.pdf"
    rel="nofollow">Maximilian and Jorg 2018</a>).</p>

    <h2>

    <a id="user-content-build-and-run-with-container" class="anchor" href="#build-and-run-with-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    and run with container</h2>

    <p>Using the planner with <a href="https://sylabs.io/docs/#singularity" rel="nofollow">Singularity</a>
    is rather simple. First install Singularity following <a href="https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">this guide</a>. Then run the following script in CLI and you will
    have the plan file <em>sas_plan</em> under <em>$RUNDIR</em>.</p>

    <pre><code>sudo singularity build planner.img sysu-planner/Singularity

    mkdir rundir

    cp path/to/domain.pddl rundir

    cp path/to/problem.pddl rundir

    RUNDIR="$(pwd)/rundir"

    DOMAIN="$RUNDIR/domain.pddl"

    PROBLEM="$RUNDIR/problem.pddl"

    PLANFILE="$RUNDIR/sas_plan"

    singularity run -C -H $RUNDIR planner.img $DOMAIN $PROBLEM $PLANFILE $COSTBOUND

    </code></pre>

    <h3>

    <a id="user-content-supported-problems" class="anchor" href="#supported-problems"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Supported
    problems</h3>

    <p>The formulation of supported problems is the same as <a href="https://ipc2018-classical.bitbucket.io/#pddl"
    rel="nofollow">IPC 2018</a>. We also provide a set of supported domains and problems
    in <a href="https://github.com/hejm37/benchmark-domains">benchmark-domains</a>.</p>

    <h2>

    <a id="user-content-notes-on-playing-with-the-source-code" class="anchor" href="#notes-on-playing-with-the-source-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Notes
    on playing with the source code</h2>

    <p>The source code of the planner contains two part:</p>

    <ul>

    <li>BFWS-public and its dependency, LAPKT-public</li>

    <li>fast-downward-conjunctions</li>

    </ul>

    <p>Then planner should be invoked in the fast-downward-conjunctions part (using
    --dual option and it will call BFWS-public/fd-version/bfws.py to perform 1-BFWS,
    see <a href="https://github.com/hejm37/sysu-planner/blob/master/Singularity">the
    Singularity script</a> for more details).</p>

    <h3>

    <a id="user-content-potential-failures" class="anchor" href="#potential-failures"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Potential
    Failures</h3>

    <p>If the above build has failed, it may appears to be a cmake cache fail. In
    this case, remove the <em>builds</em> (if it exists) directory under fast-downward-conjunctions
    and rerun the singularity command shall solve the problem.</p>

    <p>Or it may appears to be a scons build fail. In this case, remove all the <em>.sconsign.dblite</em>
    files under the directory shall solve the problem.</p>

    <p>Both cases would occur if the planner was built outside a container.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1563536767.0
hexmek/container:
  data_format: 2
  description: null
  filenames:
  - Singularity.metaeuk
  - Singularity.antismash_standalone
  - Singularity.bamm
  - Singularity.METAMVGL
  - Singularity.metawrap
  - Singularity.seqtk
  - Singularity.iqtree
  - Singularity.raxml-ng
  - Singularity.dRep3
  - Singularity.art
  - Singularity.bioconvert
  - Singularity.spades_3.15
  - Singularity.BUSCO4
  - Singularity.spades_3.13
  - Singularity.megahit
  - Singularity.spades
  - Singularity.cmseq
  - Singularity.snakemake
  - Singularity.puntseq
  - Singularity.dRep
  - Singularity.mummer
  - Singularity.tree
  - Singularity.ete3
  - Singularity.krona
  - Singularity.trimal
  - Singularity.repeatmasker
  - Singularity.euk_decide
  - Singularity.biopython
  - Singularity.bioinfo
  - Singularity.deeptools
  - Singularity.famsa
  - Singularity.minimap2
  - Singularity.dbcan
  - Singularity.pysam
  - Singularity.metabat2
  - Singularity.eukcc_vanilla
  - Singularity.CAT
  - Singularity.sourmash
  - Singularity.R
  - Singularity.metawap_docker
  - Singularity.ploidyNGS
  - Singularity.VAMB_10.1
  - Singularity.nQuire
  - Singularity.bbmap
  - Singularity.kraken2
  - Singularity.cmseq_conda
  - Singularity.qiime2
  - Singularity.mash
  - Singularity.fastani
  - Singularity.CAT_update
  - Singularity.mmseq2
  - Singularity.pasta
  - Singularity.VAMB
  - Singularity.mafft
  - Singularity.comparem
  - Singularity.mashmap
  - Singularity.BUSCO414
  - Singularity.ncbi-downloader
  - Singularity.BUSCO5
  - Singularity.nanofilt
  - Singularity.sepp
  - Singularity.EukRep
  - Singularity.VAMP
  - Singularity.bwa
  full_name: hexmek/container
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"\
    #singularity-rnaseq\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-rnaseq</h1>\n<h2>\n<a id=\"user-content-running-jupyter\"\
    \ class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Jupyter</h2>\n<p>Run\
    \ this to start Jupyter:</p>\n<pre><code>singularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\
    </code></pre>\n<p>Then follow the instructions that Jupyter printed to the terminal\
    \ when you started it up to access Jupyter in your web browser</p>\n<h3>\n<a id=\"\
    user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Accessing Jupyter on a remote server</h3>\n<p>If you are running the\
    \ container on a remote server, you will need to set up port forwarding with ssh\
    \ to be able to access Jupyter.  Run this command to forward the default Jupyter\
    \ port (8888)</p>\n<pre><code>ssh -L 8888:localhost:8888 bug\n</code></pre>\n\
    <blockquote>\n<p>Note if the default Jupyter port is not available, Jupyter will\
    \ choose a different port.  In this case you will need to substitute the port\
    \ that Jupyter outputs for 8888 in the ssh port forwarding command above.</p>\n\
    </blockquote>\n<h2>\n<a id=\"user-content-running-on-a-slurm-cluster\" class=\"\
    anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running on a SLURM Cluster</h2>\n\
    <p>You can use this image interactively on a SLURM-managed cluster by running\
    \ launching RStudio or Jupyter. The following instructions work on the Duke Compute\
    \ Cluster (DCC).  Doing this on other cluster will require some modification and\
    \ may not work, depending on how the cluster is configured.</p>\n<h3>\n<a id=\"\
    user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RStudio</h3>\n\
    <ol>\n<li>ssh to DCC login node: <code>ssh NETID@dcc-login-01.rc.duke.edu</code>\n\
    </li>\n<li>run tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n\
    <li>Run this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty\
    \ bash -i</code>\n</li>\n<li>Run <code>hostname -A</code> on compute node and\
    \ record results</li>\n<li>Run on the following on a compute node and note the\
    \ port, username, and password that the command prints:</li>\n</ol>\n<pre><code>mkdir\
    \ -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\n\
    singularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind\
    \ /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\
    </code></pre>\n<ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the port returned\
    \ but the \"singularity run\" commmand</li>\n<li>Where COMPUTE_HOSTNAME is the\
    \ hostname returned by running \"hostname -A\" on the compute node</li>\n<li>Where\
    \ NETID is your NetID</li>\n</ul>\n</li>\n<li>Go to \"localhost:PORT\" in a webrowser\
    \ and enter the username and password printed by the \"singularity run\" commmand</li>\n\
    <li>Have fun!!</li>\n<li>At the end of an analysis you will probably want to copy\
    \ results to your directory in <code>/work</code> or <code>/hpc/group</code>\n\
    </li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Jupyter</h3>\n<ol>\n<li>ssh to dcc-login-01.rc.duke.edu</li>\n<li>run\
    \ tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n<li>Run\
    \ this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty bash -i</code>\n\
    </li>\n<li>Run on compute node:</li>\n</ol>\n<pre><code>mkdir -p /scratch/josh/rnaseq_demo/rawdata\
    \ /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\
    \n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace\
    \ \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n</code></pre>\n\
    <ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the number after\
    \ <code>http://127.0.0.1:</code> in the URL given by Jupyter (defaults to 8888,\
    \ but Jupyter will use a different one if the default is in use, or if a different\
    \ port is supplied as an argument using <code>--port</code> when running the singularity\
    \ container</li>\n<li>Where COMPUTE_HOSTNAME is the hostname returned by running\
    \ \"hostname -A\" on the compute node</li>\n<li>Where NETID is your NetID</li>\n\
    </ul>\n</li>\n<li>Copy the URL supplied by jupyter that starts <code>http://127.0.0.1:</code>\
    \ and paste it in a webbrowser</li>\n<li>Have fun!!</li>\n<li>At the end of an\
    \ analysis you will probably want to copy results to your directory in <code>/work</code>\
    \ or <code>/hpc/group</code>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter-on-gpu-node\"\
    \ class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Jupyter on GPU node</h3>\n<p>Same\
    \ as above, but the srun command should use the <code>chsi-gpu</code> partition\
    \ and request a gpu, but less CPUs and Memory:</p>\n<p><code>srun -A chsi -p chsi-gpu\
    \ --gres=gpu:1 --mem=15866 -c 2 --pty bash -i</code></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1619700602.0
housw/BlendIt:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity.BlendIt.def
  full_name: housw/BlendIt
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-corebedtools-intersect" class="anchor" href="#nf-corebedtools-intersect"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nf-core/bedtools-intersect</h1>

    <p><strong>Intersect lots of bed files with lots of other bed files</strong></p>

    <p><a href="https://travis-ci.org/nf-core/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/811368779316af4f70b4dd35fc2c24cebcc4dc194cd63234e130384ec38ac89f/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f626564746f6f6c732d696e746572736563742e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nf-core/bedtools-intersect.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/bedtools-intersect" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ca7e06b0d2929a9cba14da1892e90c6d4673a695806cb07ea82e89a1cbecef92/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f626564746f6f6c732d696e746572736563742e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/bedtools-intersect.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The nf-core/bedtools-intersect pipeline comes with documentation about the
    pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1594543109.0
humanconnectome/hcp-pipelines-singularity:
  data_format: 2
  description: The definition files for creating singularity containers that can run
    in the WashU HPC
  filenames:
  - Singularity.def
  full_name: humanconnectome/hcp-pipelines-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-definitions-for-hcp-pipelines" class="anchor"
    href="#singularity-definitions-for-hcp-pipelines" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Singularity Definitions for HCP Pipelines</h1>

    <p>The definition files for creating singularity containers for the XNAT pipelines

    wrapper code so that it can run in the WashU HPC.</p>

    <h2>

    <a id="user-content-cloning-with-submodules" class="anchor" href="#cloning-with-submodules"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cloning
    with Submodules</h2>

    <p>Don''t forget to pull down the submodules as well, with the <code>--recursive</code>
    flag.</p>

    <pre><code>git clone https://github.com/humanconnectome/hcp-pipelines-singularity
    --recursive

    </code></pre>

    <h2>

    <a id="user-content-development" class="anchor" href="#development" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Development</h2>

    <table>

    <thead>

    <tr>

    <th>Command</th>

    <th>Task</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><code>make clean</code></td>

    <td>Remove previous container image.</td>

    </tr>

    <tr>

    <td><code>make update</code></td>

    <td>Update all the git submodule repos.</td>

    </tr>

    <tr>

    <td><code>make build</code></td>

    <td>Generate a container image from .def file</td>

    </tr>

    <tr>

    <td><code>make upload</code></td>

    <td>Upload the container to correct location in the HPC.</td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610395015.0
iapalm/lc-builds:
  data_format: 2
  description: singularity lc builds
  filenames:
  - Singularity
  full_name: iapalm/lc-builds
  latest_release: null
  readme: '<p>This repository provides some boiler plate scripts for running ''pangeo''
    python ecosystem using singularity containers.</p>

    <p>Steps are:</p>

    <ol>

    <li>

    <p>Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a></p>

    <pre><code>docker pull pangeo/pangeo-notebook

    </code></pre>

    <p>The pangeo-notebook has a pretty diverse set of libraries for most cloud,

    dask, zarr, netCDF, analysis type tasks.</p>

    <ul>

    <li>

    <p>(Optional) Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a>

    If you need to customise, see minimal example in Dockerfile and requirements.txt
    and description here:</p>

    <ul>

    <li>

    <p>(Deprecated) <a href="https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants">https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants</a></p>

    </li>

    <li>

    <p>(<strong>Use this since 27-07-2020</strong>) <a href="https://github.com/pangeo-data/pangeo-docker-images">https://github.com/pangeo-data/pangeo-docker-images</a></p>

    <p>Then you would build a custom image along the lines of:</p>

    <pre><code>make pangeo-notebook

    </code></pre>

    </li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Convert docker image to singularity with a command such as:</p>

    <pre><code>singularity -d build pangeo-custom.sif docker-daemon://pangeo/pangeo-notebook:master

    </code></pre>

    </li>

    <li>

    <p>Copy the created <code>pangeo-custom.sif</code> singularity image to somewhere
    accessible on the HPC filesystem and edit the <code>container=</code> and <code>scheduler_file=</code>
    variables in the <code>start_jupyter.slurm</code> and <code>start_worker.slurm</code>
    scripts to point to the singularity image and the shared filesystem location to
    write the scheduler details, respectively.</p>

    </li>

    <li>

    <p>Start the jupyter lab and dask-scheduler, the first parameter is the working
    path you want to use for jupyter lab:</p>

    <pre><code>sbatch start_jupyter.slurm $HOME

    </code></pre>

    <p>This starts a scheduler and jupyterlab with 2 cores each and 8GB/core memory.
    These can be edited in the #SBATCH headers, also note you can set the default
    directory for jupyterlab with the notebook_dir - at present it defaults to $SCRATCH1DIR.</p>

    </li>

    <li>

    <p>Start dask-workers (where n is the number of workers you want - these are configures
    for &lt; 2 hour wall time limit so that they use the <code>h2</code> queue):</p>

    <pre><code>sbatch -n 10 start_worker.slurm

    </code></pre>

    <p>also note that this input argument to dask-worker <code>--local-directory $LOCALDIR</code>
    tells the worker the path to local disk storage on the node which can be used
    for spilling data, but not all HPC nodes/centres have attached local storage.</p>

    </li>

    <li>

    <p>See instruction printed to the slurm-######.out log file for connecting to
    the jupyter session running on the compute node, something like:</p>

    <pre><code>ssh -N -l pbranson -L 8888:compute-node123:8888 hpc-login.host.com

    </code></pre>

    <p>and take note of the randomly generated token printed to the slurm-######.out
    log file. You will need that to login to Jupyterlab.</p>

    </li>

    <li>

    <p>To connect to the dask-scheduler from a notebook use the following snippet:</p>

    <pre><code>import os

    from distributed import Client

    client=Client(scheduler_file=os.environ[''SCRATCH1DIR''] + ''/scheduler.json'')

    client

    </code></pre>

    </li>

    <li>

    <p>View the scheduler bokeh dashboard at <a href="http://localhost:8888/proxy/8787/status"
    rel="nofollow">http://localhost:8888/proxy/8787/status</a>. This can also be entered
    into the Jupyterlab dask widget as <code>/proxy/8787/status</code></p>

    </li>

    <li>

    <p>As a little cheat in jupyter lab I open up a terminal and then do</p>

    <pre><code>ssh localhost

    </code></pre>

    <p>to connect to the host running the jupyter container - this gives you access
    to the jobscheduler from that terminal and you can start workers  in there with:</p>

    <pre><code>sbatch start_worker.slurm

    </code></pre>

    <p>Also note that the dask worker specifications used in the <code>start_worker.slurm</code>
    script are based of the slurm environment variables, so you can alter the worker
    specification using the <code>#SBATCH</code> directives:</p>

    <pre><code>#SBATCH --ntasks=20

    #SBATCH --cpus-per-task=2

    #SBATCH --mem-per-cpu=10G

    #SBATCH --time=0:30:00

    </code></pre>

    <p>or at the command line when you submit the script:</p>

    <pre><code>sbatch -n 4 -c 4 --mem-per-cpu=16G start_worker.slurm

    </code></pre>

    <p>which would start 4 workers with 4 cores per worker and 16*4 = 64GB memory
    per dask-worker.</p>

    </li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1560984106.0
ifurther/flair-def:
  data_format: 2
  description: singularity def file for flair(fluka)
  filenames:
  - flair.def
  - flair-cern.def
  full_name: ifurther/flair-def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619686613.0
intel/HPC-containers-from-Intel:
  data_format: 2
  description: Intel HPC Containers using Singularity
  filenames:
  - definitionFiles/WRF/wrfRun.def
  - definitionFiles/WRF/wrfBuild.def
  - definitionFiles/gromacs/gromacsBuild.def
  - definitionFiles/gromacs/gromacsRun.def
  - definitionFiles/namd/namdBuild.def
  - definitionFiles/namd/namdRun.def
  - definitionFiles/lammps/lammpsRun.def
  - definitionFiles/lammps/lammpsBuild.def
  - definitionFiles/base/base.def
  full_name: intel/HPC-containers-from-Intel
  latest_release: null
  readme: '<h1>

    <a id="user-content-goal" class="anchor" href="#goal" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Goal:</h1>

    <p>Create containers using Singularity definition file for HPC apps and run them
    on the cloud or bare metal for Single and Cluster runs.</p>

    <p>This repo should have definition files only for few HPC applications. Users
    can utilize them to generate containers.</p>

    <h2>

    <a id="user-content-get-help" class="anchor" href="#get-help" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Get Help</h2>

    <ul>

    <li>

    <a href="https://github.com/intel/HPC-containers-from-Intel/issues">Post an issue</a>
    if you face any problem building or running a container</li>

    </ul>

    '
  stargazers_count: 16
  subscribers_count: 9
  topics:
  - hpc
  - cluster
  - singularity-containers
  - cloud
  updated_at: 1619711561.0
jendrikseipp/scorpion:
  data_format: 2
  description: Optimal classical planner based on saturated cost partitioning
  filenames:
  - misc/releases/20.06/Singularity.20.06
  - misc/releases/19.12/Singularity.19.12
  - misc/releases/19.06/Singularity.19.06
  - misc/releases/latest/Singularity
  full_name: jendrikseipp/scorpion
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-scorpion\" class=\"anchor\" href=\"#scorpion\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Scorpion</h1>\n<p>Scorpion is an optimal classical planner that uses\
    \ saturated cost\npartitioning to combine multiple abstraction heuristics. It\
    \ also contains\nimplementations of many other cost partitioning algorithms over\n\
    abstraction and landmark heuristics. Scorpion is based on the <a href=\"https://github.com/aibasel/downward\"\
    >Fast\nDownward planning system</a>, which is\ndescribed below. We regularly port\
    \ the latest changes from Fast Downward\nto Scorpion and also try to port Scorpion\
    \ features back to Fast Downward.</p>\n<p>Please use the following reference when\
    \ citing Scorpion:\nJendrik Seipp, Thomas Keller and Malte Helmert.\n<a href=\"\
    https://www.jair.org/index.php/jair/article/view/11673\" rel=\"nofollow\">Saturated\
    \ Cost Partitioning for Optimal Classical Planning</a>.\nJournal of Artificial\
    \ Intelligence Research 67, pp. 129-167. 2020.</p>\n<h2>\n<a id=\"user-content-instructions\"\
    \ class=\"anchor\" href=\"#instructions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Instructions</h2>\n<p>After installing\
    \ the requirements (see below), compile the planner with</p>\n<pre><code>./build.py\n\
    </code></pre>\n<p>and see the available options with</p>\n<pre><code>./fast-downward.py\
    \ --help  # driver\n./fast-downward.py --search -- --help  # search component\n\
    </code></pre>\n<p>For more details (including build instructions for Windows),\
    \ see the\ndocumentation about\n<a href=\"http://www.fast-downward.org/ObtainingAndRunningFastDownward\"\
    \ rel=\"nofollow\">compiling</a>\nand <a href=\"http://www.fast-downward.org/PlannerUsage\"\
    \ rel=\"nofollow\">running</a> the planner.</p>\n<h3>\n<a id=\"user-content-recommended-configuration\"\
    \ class=\"anchor\" href=\"#recommended-configuration\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Recommended Configuration</h3>\n\
    <p>We recommend the following configuration, which is similar to the one\nScorpion\
    \ used in the IPC 2018. It prunes irrelevant operators in a\npreprocessing step,\
    \ uses partial order reduction, and maximizes over\nmultiple diverse SCP heuristics\
    \ computed for projections and Cartesian\nabstractions:</p>\n<pre><code>./fast-downward.py\
    \ --transform-task preprocess-h2\n  ../benchmarks/gripper/prob01.pddl\n  --search\
    \ \"astar(scp([\n    projections(hillclimbing(max_time=100, random_seed=0)),\n\
    \    projections(systematic(2)),\n    cartesian()],\n    max_orders=infinity,\
    \ max_time=200, max_optimization_time=2, diversify=true,\n    orders=greedy_orders(random_seed=0),\
    \ random_seed=0),\n    pruning=atom_centric_stubborn_sets(min_required_pruning_ratio=0.2))\"\
    \n</code></pre>\n<p>(In <a href=\"https://lab.readthedocs.io/\" rel=\"nofollow\"\
    >Downward Lab</a> you can use the\n<code>driver_options</code> argument of <code>add_algorithm</code>\
    \ to specify the\n<code>--transform-task</code> argument.)</p>\n<p>If you want\
    \ to run exactly the same Scorpion version as in IPC 2018, we\nrecommend using\
    \ the <a href=\"https://bitbucket.org/ipc2018-classical/team44/src/ipc-2018-seq-opt/\"\
    \ rel=\"nofollow\">Scorpion IPC\nrepo</a>.\nIt also includes a <a href=\"https://github.com/hpcng/singularity\"\
    >Singularity</a>\nimage.</p>\n<h2>\n<a id=\"user-content-differences-between-scorpion-and-fast-downward\"\
    \ class=\"anchor\" href=\"#differences-between-scorpion-and-fast-downward\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Differences\
    \ between Scorpion and Fast Downward</h2>\n<ul>\n<li>Scorpion comes with the\n\
    <a href=\"https://ojs.aaai.org/index.php/ICAPS/article/view/13708\" rel=\"nofollow\"\
    >h\xB2-preprocessor</a>\nby Vidal Alc\xE1zar and \xC1lvaro Torralba that prunes\
    \ irrelevant operators.\nPass <code>--transform-task builds/release/bin/preprocess-h2</code>\
    \ to use it.</li>\n<li>The <code>--transform-task</code> command allows you to\
    \ run arbitrary preprocessing\ncommands that transform the SAS+ output from the\
    \ translator before\npassing it to the search.</li>\n<li>If <a href=\"https://ccache.dev/\"\
    \ rel=\"nofollow\">ccache</a> is installed, Scorpion uses it to cache\ncompilation\
    \ files.</li>\n</ul>\n<h3>\n<a id=\"user-content-new-plugin-options\" class=\"\
    anchor\" href=\"#new-plugin-options\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>New plugin options</h3>\n<ul>\n\
    <li>\n<p><code>cegar(..., search_strategy=incremental)</code>: use <a href=\"\
    https://ojs.aaai.org/index.php/ICAPS/article/view/6667\" rel=\"nofollow\">incremental\
    \ search for\nCartesian abstraction\nrefinement</a>\n(default).</p>\n</li>\n<li>\n\
    <p><code>hillclimbing(..., max_generated_patterns=200)</code>: limit the number\
    \ of\npatterns generated by hill climbing.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-new-cost-partitioning-algorithms-for-abstraction-heuristics\"\
    \ class=\"anchor\" href=\"#new-cost-partitioning-algorithms-for-abstraction-heuristics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>New cost partitioning algorithms for abstraction heuristics</h3>\n\
    <p>We use Cartesian abstractions in the example configurations below\n(<code>[cartesian()]</code>).\
    \ You can also use pattern database heuristics, e.g.,\n<code>[projections(systematic(2))]</code>,\
    \ or mix abstractions, e.g.,\n<code>[projections(systematic(3)), cartesian()]</code>.\
    \ Some of the algorithms are\nalso part of vanilla Fast Downward, but only for\
    \ PDB heuristics.</p>\n<ul>\n<li>Optimal cost partitioning:\n<code>optimal_cost_partitioning([cartesian()])</code>\n\
    </li>\n<li>Canonical heuristic:\n<code>canonical_heuristic([cartesian()])</code>\n\
    </li>\n<li>Post-hoc optimization:\n<code>operatorcounting([pho_abstraction_constraints([cartesian()],\
    \ saturated=false)])</code>\n</li>\n<li>Uniform cost partitioning:\n<code>uniform_cost_partitioning([cartesian()],\
    \ opportunistic=false)</code>\n</li>\n<li>Opportunistic uniform cost partitioning:\n\
    <code>uniform_cost_partitioning([cartesian()], ..., opportunistic=true)</code>\n\
    </li>\n<li>Greedy zero-one cost partitioning:\n<code>zero_one_cost_partitioning([cartesian()],\
    \ ...)</code>\n</li>\n<li>Saturated post-hoc optimization:\n<code>operatorcounting([pho_abstraction_constraints([cartesian()],\
    \ saturated=true)])</code>\n</li>\n</ul>\n<p>You can also compute the maximum\
    \ over abstraction heuristics:</p>\n<ul>\n<li><code>maximize([cartesian()])</code></li>\n\
    </ul>\n<h3>\n<a id=\"user-content-new-cost-partitioning-algorithms-for-landmark-heuristics\"\
    \ class=\"anchor\" href=\"#new-cost-partitioning-algorithms-for-landmark-heuristics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>New cost partitioning algorithms for landmark heuristics</h3>\n<p>Example\
    \ using A* search and saturated cost partitioning over BJOLP\nlandmarks:</p>\n\
    <pre><code>--evaluator\n  \"lmc=lmcount(lm_merged([lm_rhw(), lm_hm(m=1)]),\n \
    \ admissible=true, cost_partitioning=suboptimal, greedy=true,\n  reuse_costs=true,\
    \ scoring_function=max_heuristic_per_stolen_costs)\"\n--search\n  \"astar(lmc,\
    \ lazy_evaluator=lmc)\"\n</code></pre>\n<p>Different cost partitioning algorithms\
    \ (all need <code>admissible=true</code>):</p>\n<ul>\n<li>Optimal cost partitioning\
    \ (part of vanilla Fast Downward):\n<code>lmcount(..., cost_partitioning=optimal)</code>\n\
    </li>\n<li>Canonical heuristic:\n<code>lmcount(..., cost_partitioning=canonical)</code>\n\
    </li>\n<li>Post-hoc optimization:\n<code>lmcount(..., cost_partitioning=pho)</code>\n\
    </li>\n<li>Uniform cost partitioning:\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=false, reuse_costs=false)</code>\n</li>\n<li>Opportunistic uniform cost\
    \ partitioning (part of vanilla Fast Downward):\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=false, reuse_costs=true, scoring_function=min_stolen_costs)</code>\n\
    </li>\n<li>Greedy zero-one cost partitioning:\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=true, reuse_costs=false, scoring_function=max_heuristic)</code>\n</li>\n\
    <li>Saturated cost partitioning:\n<code>lmcount(..., cost_partitioning=suboptimal,\
    \ greedy=true, reuse_costs=true, scoring_function=max_heuristic_per_stolen_costs)</code>\n\
    </li>\n</ul>\n<h1>\n<a id=\"user-content-fast-downward\" class=\"anchor\" href=\"\
    #fast-downward\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Fast Downward</h1>\n<p>Fast Downward is a domain-independent\
    \ classical planning system.</p>\n<p>Copyright 2003-2020 Fast Downward contributors\
    \ (see below).</p>\n<p>For further information:</p>\n<ul>\n<li>Fast Downward website:\
    \ <a href=\"http://www.fast-downward.org\" rel=\"nofollow\">http://www.fast-downward.org</a>\n\
    </li>\n<li>Report a bug or file an issue: <a href=\"http://issues.fast-downward.org\"\
    \ rel=\"nofollow\">http://issues.fast-downward.org</a>\n</li>\n<li>Fast Downward\
    \ mailing list: <a href=\"https://groups.google.com/forum/#!forum/fast-downward\"\
    \ rel=\"nofollow\">https://groups.google.com/forum/#!forum/fast-downward</a>\n\
    </li>\n<li>Fast Downward main repository: <a href=\"https://github.com/aibasel/downward\"\
    >https://github.com/aibasel/downward</a>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-tested-software-versions\"\
    \ class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Tested software\
    \ versions</h2>\n<p>This version of Fast Downward has been tested with the following\
    \ software versions:</p>\n<table>\n<thead>\n<tr>\n<th>OS</th>\n<th>Python</th>\n\
    <th>C++ compiler</th>\n<th>CMake</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Ubuntu\
    \ 20.04</td>\n<td>3.8</td>\n<td>GCC 9, GCC 10, Clang 10, Clang 11</td>\n<td>3.16</td>\n\
    </tr>\n<tr>\n<td>Ubuntu 18.04</td>\n<td>3.6</td>\n<td>GCC 7, Clang 6</td>\n<td>3.10</td>\n\
    </tr>\n<tr>\n<td>macOS 10.15</td>\n<td>3.6</td>\n<td>AppleClang 12</td>\n<td>3.19</td>\n\
    </tr>\n<tr>\n<td>Windows 10</td>\n<td>3.6</td>\n<td>Visual Studio Enterprise 2017\
    \ (MSVC 19.16) and 2019 (MSVC 19.28)</td>\n<td>3.19</td>\n</tr>\n</tbody>\n</table>\n\
    <p>We test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu,\
    \ we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and\
    \ on macOS, we do not test LP solvers (yet).</p>\n<h2>\n<a id=\"user-content-contributors\"\
    \ class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors</h2>\n<p>The following\
    \ list includes all people that actively contributed to\nFast Downward, i.e. all\
    \ people that appear in some commits in Fast\nDownward's history (see below for\
    \ a history on how Fast Downward\nemerged) or people that influenced the development\
    \ of such commits.\nCurrently, this list is sorted by the last year the person\
    \ has been\nactive, and in case of ties, by the earliest year the person started\n\
    contributing, and finally by last name.</p>\n<ul>\n<li>2003-2020 Malte Helmert</li>\n\
    <li>2008-2016, 2018-2020 Gabriele Roeger</li>\n<li>2010-2020 Jendrik Seipp</li>\n\
    <li>2010-2011, 2013-2020 Silvan Sievers</li>\n<li>2012-2020 Florian Pommerening</li>\n\
    <li>2013, 2015-2020 Salome Eriksson</li>\n<li>2016-2020 Cedric Geissmann</li>\n\
    <li>2017-2020 Guillem Franc\xE8s</li>\n<li>2018-2020 Augusto B. Corr\xEAa</li>\n\
    <li>2018-2020 Patrick Ferber</li>\n<li>2015-2019 Manuel Heusner</li>\n<li>2017\
    \ Daniel Killenberger</li>\n<li>2016 Yusra Alkhazraji</li>\n<li>2016 Martin Wehrle</li>\n\
    <li>2014-2015 Patrick von Reth</li>\n<li>2015 Thomas Keller</li>\n<li>2009-2014\
    \ Erez Karpas</li>\n<li>2014 Robert P. Goldman</li>\n<li>2010-2012 Andrew Coles</li>\n\
    <li>2010, 2012 Patrik Haslum</li>\n<li>2003-2011 Silvia Richter</li>\n<li>2009-2011\
    \ Emil Keyder</li>\n<li>2010-2011 Moritz Gronbach</li>\n<li>2010-2011 Manuela\
    \ Ortlieb</li>\n<li>2011 Vidal Alc\xE1zar Saiz</li>\n<li>2011 Michael Katz</li>\n\
    <li>2011 Raz Nissim</li>\n<li>2010 Moritz Goebelbecker</li>\n<li>2007-2009 Matthias\
    \ Westphal</li>\n<li>2009 Christian Muise</li>\n</ul>\n<h2>\n<a id=\"user-content-history\"\
    \ class=\"anchor\" href=\"#history\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>History</h2>\n<p>The current\
    \ version of Fast Downward is the merger of three different\nprojects:</p>\n<ul>\n\
    <li>the original version of Fast Downward developed by Malte Helmert\nand Silvia\
    \ Richter</li>\n<li>LAMA, developed by Silvia Richter and Matthias Westphal based\
    \ on\nthe original Fast Downward</li>\n<li>FD-Tech, a modified version of Fast\
    \ Downward developed by Erez\nKarpas and Michael Katz based on the original code</li>\n\
    </ul>\n<p>In addition to these three main sources, the codebase incorporates\n\
    code and features from numerous branches of the Fast Downward codebase\ndeveloped\
    \ for various research papers. The main contributors to these\nbranches are Malte\
    \ Helmert, Gabi R\xF6ger and Silvia Richter.</p>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>The following\
    \ directory is not part of Fast Downward as covered by\nthis license:</p>\n<ul>\n\
    <li>./src/search/ext</li>\n</ul>\n<p>For the rest, the following license applies:</p>\n\
    <pre><code>Fast Downward is free software: you can redistribute it and/or modify\n\
    it under the terms of the GNU General Public License as published by\nthe Free\
    \ Software Foundation, either version 3 of the License, or (at\nyour option) any\
    \ later version.\n\nFast Downward is distributed in the hope that it will be useful,\
    \ but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY\
    \ or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for\
    \ more details.\n\nYou should have received a copy of the GNU General Public License\n\
    along with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;.\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621274654.0
jengelmann/FastqPuri:
  data_format: 2
  description: fastq quality assessment and filtering tool
  filenames:
  - Singularity
  - Singularity-Test
  full_name: jengelmann/FastqPuri
  latest_release: v1.0.6
  readme: "<h1>\n<a id=\"user-content-fastqpuri-an-fq-quality-control-and-filter-tool\"\
    \ class=\"anchor\" href=\"#fastqpuri-an-fq-quality-control-and-filter-tool\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>FastqPuri,\
    \ an fq quality control and filter tool</h1>\n<p>Software and source code of <code>FastqPuri</code>.\
    \ It creates quality reports of\n<code>fastq</code> files and filters them removing\
    \ low quality reads, reads\ncontaining too many N's or contamination reads (unwanted\
    \ rRNA reads,\nimpurities coming from another organism, ...).</p>\n<h2>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p>Clone the repository, or download the source. Make sure that\nyour system supplies\
    \ the following dependencies for FastqPuri.</p>\n<ul>\n<li>OS: Linux (clang, gcc),\
    \ Mac OS (clang, gcc), OpenBSD (clang)</li>\n<li>\n<code>cmake</code> (at least\
    \ version 2.8),</li>\n<li>a <code>C</code> compiler supporting the <code>c11</code>\
    \ standard\n(change the compiler flags otherwise),</li>\n<li>pandoc (optional,\
    \ see documentation in <code>PANDOC.md</code>),</li>\n<li>\n<code>Rscript</code>\
    \ (optional),</li>\n<li>Following <code>R</code> packages installed (optional):\n\
    <ul>\n<li><code>pheatmap</code></li>\n<li><code>knitr</code></li>\n<li><code>rmarkdown</code></li>\n\
    </ul>\n</li>\n</ul>\n<p><strong>NOTE:</strong>  FastqPuri will work without the\
    \ optional dependencies\nbut will skip creating html reports if they are not available.</p>\n\
    <pre><code>$ cmake -H. -Bbuild/ [-DRSCRIPT=/path/to/my/R/bin/Rscript] [-DCMAKE_INSTALL_PREFIX=/path/to/my/root]\
    \ ... \n$ cd build \n$ make \n$ sudo make install  \n</code></pre>\n<p>When running\
    \ <code>cmake</code>, there are some variables you can set\nusing the option -D\
    \ followed by the variable name. These variables are:</p>\n<ul>\n<li>\n<code>CMAKE_C_COMPILER</code>:\
    \ <code>C</code> compiler (default <code>gcc</code>)</li>\n<li>\n<code>CMAKE_C_FLAGS</code>:\
    \ compiler flags (default <code>-Wall -O3 -march=native -std=c11</code>).</li>\n\
    <li>\n<code>CMAKE_INSTALL_PREFIX</code>: root path for <code>make install</code>,\
    \ e.g. to\nredirect to a directory with user access (default /usr/local),</li>\n\
    <li>\n<code>PANDOC</code>: <code>pandoc</code> executable (default <code>pandoc</code>),</li>\n\
    <li>\n<code>RSCRIPT</code>: <code>Rscript</code> executable (default <code>Rscript</code>),</li>\n\
    <li>\n<code>READ_MAXLEN</code>: Maximum Illumina read length</li>\n<li>(default\
    \ 400),</li>\n</ul>\n<p>The executables will be created in the folder <code>bin</code>\
    \ and installed in <code>/usr/local/bin</code>.\n<code>R</code> scripts will be\
    \ installed in <code>/usr/local/share/FastqPuri/R</code>.</p>\n<p><strong>WARNING:</strong>\
    \ do not move the executables that depend on <code>R</code> scripts,\nanywhere\
    \ else, unless you also move the corresponding <code>R</code> scripts respecting\n\
    the local folder structure.</p>\n<h2>\n<a id=\"user-content-executables\" class=\"\
    anchor\" href=\"#executables\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Executables</h2>\n<ul>\n<li>\n<code>Qreport</code>:\
    \ creates a quality report in html format (see <code>README_Qreport.md</code>),</li>\n\
    <li>\n<code>Sreport</code>: creates a summary report in html format on a set of\
    \ samples,\nregarding either the original files or the filtering process\n(see\
    \ <code>README_Sreport.md</code>),</li>\n<li>\n<code>makeBloom</code>: creates\
    \ a  bloom filter from a fasta file of a certain size,\nand stores it in a file\
    \ (see <code>README_makeBloom.md</code>)</li>\n<li>\n<code>makeTree</code>: creates\
    \ a tree of a certain depth from a fasta file and stores\nit in a file (see <code>README_makeTree.md</code>),</li>\n\
    <li>\n<code>trimFilter</code>: performs the filtering process for single-end data\n\
    (see <code>README_trimFilter.md</code>).</li>\n<li>\n<code>trimFilterPE</code>:\
    \ performs the filtering process for double stranded data\n(see <code>README_trimFilterPE.md</code>).</li>\n\
    </ul>\n<p>An exemplar work flow could be:</p>\n<ul>\n<li><code>Qreport</code></li>\n\
    <li><code>Sreport</code></li>\n<li><code>makeBloom</code></li>\n<li>\n<code>trimFilter</code>\
    \ or <code>trimFilterPE</code>\n</li>\n<li><code>Qreport</code></li>\n<li><code>Sreport</code></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-documentation-of-the-code\" class=\"anchor\"\
    \ href=\"#documentation-of-the-code\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation of the code</h2>\n\
    <p>A Doxygen documentation of the code is available:</p>\n<ul>\n<li>\n<code>html</code>\
    \ version under the folder <code>html</code> (open <code>index.html</code> with\
    \ a browser).</li>\n<li>\n<code>pdf</code> version: <code>latex/refman.pdf</code>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-use-a-docker-container-to-run-fastqpuri\"\
    \ class=\"anchor\" href=\"#use-a-docker-container-to-run-fastqpuri\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use\
    \ a docker container to run FastqPuri</h2>\n<p>The file 'Dockerfile' documents\
    \ the exact linux installation we used\nfor testing. If you have a docker installation\
    \ ready on your machine,\nyou may want to use a docker container for easy installation\
    \ and\ncapsulated usage of FastqPuri. After cloning this project from github\n\
    and change to its main directory, you may install a docker container\nas follows:</p>\n\
    <pre><code>$ docker build -t fastqpuri .\n</code></pre>\n<p>This will create a\
    \ container based on the debian linux distribution\ncovering all dependencies\
    \ including R and pandoc.  As soon as such a\ncontainer is installed, you can\
    \ use it either interactively:</p>\n<pre><code>$ docker run -v $PWD:/tmp -it fastqpuri\n\
    </code></pre>\n<p>or by running a pipeline implemented in an executable bash script:</p>\n\
    <pre><code>$ docker run -v $PWD:/tmp fastqpuri ./pipeline.sh\n</code></pre>\n\
    <p>Note that this call generates results in the docker container\ndirectory <code>/tmp</code>\
    \ but also keeps them after closing the docker container\nlocally where the container\
    \ was started.</p>\n<p>Instead of generating the docker container yourself with\
    \ 'docker\nbuild', you can also pull a pre-built image from the docker hub as\n\
    follows:</p>\n<pre><code>$ docker pull clottaz/fastqpuri\n</code></pre>\n<p>You\
    \ can run such a pre-built image with 'docker run' by indicating the\nimages as\
    \ 'clottaz/fastqpuri'.</p>\n<h2>\n<a id=\"user-content-use-a-singularity-container-to-run-fastqpuri\"\
    \ class=\"anchor\" href=\"#use-a-singularity-container-to-run-fastqpuri\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Use\
    \ a singularity container to run FastqPuri</h2>\n<p>Alternativly, if you have\
    \ singularity installed on your machine, you\ncan call our docker container for\
    \ FastqPuri as follows:</p>\n<pre><code>$ singularity shell --bind .:/tmp docker://clottaz/fastqpuri\n\
    </code></pre>\n<p>This call opens a shell within the container.\nWith <code>--bind</code>\
    \ we  mount the current directory also in the container.\nThe syntax is as follows:\
    \ --bind src:dest; src is the source path on\nthe host and dest is the destination\
    \ path in the container, i.e. where\nyou would like to make the source path available\
    \ in your container.\nNote that this destination path in your container should\
    \ be an existing\ndirectory, the operation will fail if you do not create the\
    \ directory first.\nHence, when we call <code>singularity shell</code> like this,\
    \ the working directory\nin the container is <code>/tmp</code>.</p>\n<p>Alternatively,\
    \ in order to execute a script from the current\ndirectory, call singularity as\
    \ follows:</p>\n<pre><code>$ singularity run --bind .:/tmp docker://clottaz/fastqpuri\
    \ /tmp/pipeline.sh\n</code></pre>\n<p>Note that <code>/tmp/pipeline.sh</code>\
    \ relates to the call within the\ncontainer. Thus, <code>pipeline.sh</code> is\
    \ located in the directory where singularity\nrun is executed, but will be made\
    \ available to the container via the <code>--bind</code>\nparameter.</p>\n<p>If\
    \ you want to invoke a function of FastqPuri, you can use the 'exec'\ncommand\
    \ like so:</p>\n<pre><code>singularity exec docker://clottaz/fastqpuri Qreport\
    \ -h\n</code></pre>\n<p>or invoke a script located in your home directory (assuming\
    \ that\nrun_ex_TREE.sh is located in your home directory):</p>\n<pre><code>$ singularity\
    \ exec docker://clottaz/fastqpuri $HOME/run_ex_TREE.sh\n</code></pre>\n<p>Singularity\
    \ documentation can be found here: <a href=\"https://www.sylabs.io/docs/\" rel=\"\
    nofollow\">https://www.sylabs.io/docs/</a></p>\n<h2>\n<a id=\"user-content-installation-via-bioconda--under-construction\"\
    \ class=\"anchor\" href=\"#installation-via-bioconda--under-construction\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation\
    \ via bioconda <strong>-under construction</strong>.</h2>\n<p><em>We are currently\
    \ working on a bioconda environment for FastqPuri.\nIf you follow the instructions\
    \ below, it is quite likely that\nFastqPuri will not yet properly run from the\
    \ bioconda environment.\nSorry about that and please stay tuned!</em></p>\n<p>Bioconda\
    \ is a channel for the conda package manager specializing in\nbioinformatics software.\
    \ Have a look at the reference:</p>\n<ul>\n<li>Bjoern Gruening, Ryan Dale, Andreas\
    \ Sjoedin, Brad A. Chapman, Jillian\nRowe, Christopher H. Tomkins-Tinch, Renan\
    \ Valieris, the Bioconda\nTeam, and Johannes Koester. 2018. Bioconda: Sustainable\
    \ and\nComprehensive Software Distribution for the Life Sciences. Nature\nMethods,\
    \ 2018.</li>\n</ul>\n<p>To find out how to use bioconda, see <a href=\"https://bioconda.github.io\"\
    \ rel=\"nofollow\">https://bioconda.github.io</a>.\nFor installing FastqPuri in\
    \ a bioconda environment, you have to install\neither <code>miniconda</code> or\
    \ <code>anaconda</code> and register channels as follows:</p>\n<pre><code>$ conda\
    \ config --add channels defaults\n$ conda config --add channels bioconda\n$ conda\
    \ config --add channels conda-forge\n</code></pre>\n<p>Then you can install <code>fastqpuri</code>:</p>\n\
    <pre><code>$ conda install fastqpuri\n</code></pre>\n<p>Actually, you may also\
    \ want to use a specific environment for the\nsequencing quality control:</p>\n\
    <pre><code>$ conda create -n qc fastqpuri\n</code></pre>\n<p>This call installs\
    \ <code>FastqPuri</code> directly in a separate environment.</p>\n<h2>\n<a id=\"\
    user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributors</h2>\n\
    <p>Paula P\xE9rez Rubio,\nClaudio Lottaz,\nJulia Engelmann</p>\n<h2>\n<a id=\"\
    user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>GPL v3 (see LICENSE.txt)</p>\n"
  stargazers_count: 16
  subscribers_count: 4
  topics: []
  updated_at: 1622626216.0
jganong/singularity-test:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jganong/singularity-test
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606934860.0
jganong/ubuntu-bionic-R-4.0.3-foieGras:
  data_format: 2
  description: singularity container to run Ian Jonsen's foieGras package
  filenames:
  - Singularity
  full_name: jganong/ubuntu-bionic-R-4.0.3-foieGras
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607375064.0
jganong/ubuntu-focal-foiegras:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jganong/ubuntu-focal-foiegras
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-create-study-specific-roi-image-in-mni-space\"\
    \ class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Create\
    \ study-specific ROI image in MNI space</h1>\n<p>PMAT resting state connectivity\
    \ study.</p>\n<h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Inputs:</h2>\n<p>All should be matched to the same T1 image.</p>\n\
    <ul>\n<li>T1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)</li>\n\
    <li>Deformation from T1 subject space to atlas space (typically DEF_FWD resource\
    \ of cat12 assessor)</li>\n<li>SUBJECT directory of Freesurfer output (typically\
    \ SUBJECT resource of freesurfer_dev assessor)</li>\n<li>Temporal lobe segmentation\
    \ (typically SEG resource of Temporal_Lobe assessor)</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n\
    <pre><code>rois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv\
    \        Region labels and volumes\nmakerois-PMAT.pdf           Visual report\
    \ of final ROI image\n</code></pre>\n<h2>\n<a id=\"user-content-regions-of-interest\"\
    \ class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Regions of interest</h2>\n<h3>\n\
    <a id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Spheres (atlas space)</h3>\n<p>Source: <em>Libby LA, Ekstrom AD, Ragland\
    \ JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal\
    \ cortices within human hippocampal subregions revealed by high-resolution functional\
    \ imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012</em></p>\n\
    <p>Method: <em>Schr\xF6der TN, Haak K V., Jimenez NIZ, et al. Functional topography\
    \ of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"\
    #entorhinal-cortex-atlas-space\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Entorhinal cortex (atlas space)</h3>\n\
    <p>Anterior lateral and posterior medial sections. Source and method: <em>Schr\xF6\
    der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal\
    \ cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738</em></p>\n\
    <h3>\n<a id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\"\
    \ href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Temporal lobe (Subject space,\
    \ warped)</h3>\n<p>Head for anterior hippocampus; body and tail combined for posterior\
    \ hippocampus. Method: <em>Plassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale\
    \ Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE\
    \ Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017\
    \ Feb 24. PMID: 28781411; PMCID: PMC5544133.</em></p>\n<h3>\n<a id=\"user-content-parahippocampal-perirhinal-subject-space-warped\"\
    \ class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parahippocampal,\
    \ perirhinal (Subject space, warped)</h3>\n<p>Generated by Freesurfer 6. Parahippocampal\
    \ (1016,2016) and perirhinal (surface patch resampled to volume, overlap with\
    \ parahippocampus was assigned to perirhinal). Method: <em>Bruce Fischl, Andre\
    \ van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H.\
    \ Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne\
    \ Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating\
    \ the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.</em></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607375887.0
jkulhanek/a2cat-vn-pytorch:
  data_format: 2
  description: Target driven visual navigation using deep reinforcement learning implemented
    in Pytorch
  filenames:
  - Singularity
  full_name: jkulhanek/a2cat-vn-pytorch
  latest_release: null
  readme: '<h1>

    <a id="user-content-target-driven-visual-navigation" class="anchor" href="#target-driven-visual-navigation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>target-driven-visual-navigation</h1>

    <p>Target driven visual navigation using deep reinforcement learning implemented
    in Pytorch</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1607415822.0
jkulhanek/deep-rl-pytorch:
  data_format: 2
  description: Popular Deep RL algorithms implemented in PyTorch
  filenames:
  - Singularity
  full_name: jkulhanek/deep-rl-pytorch
  latest_release: null
  readme: '<h1>

    <a id="user-content-deep-rl-pytorch" class="anchor" href="#deep-rl-pytorch" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deep RL PyTorch</h1>

    <p><a href="https://singularity-hub.org/collections/2581" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>This repo contains implementation of popular Deep RL algorithms. Furthermore
    it contains unified interface for training and evaluation with unified model saving
    and visualization. It can be used as a good starting point when implementing new
    RL algorithm in PyTorch.</p>

    <h2>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h2>

    <p>If you want to base your algorithm on this repository, start by installing
    it as a package</p>

    <pre><code>pip install git+https://github.com/jkulhanek/deep-rl-pytorch.git

    </code></pre>

    <p>If you want to run attached experiments yourself, feel free to clone this repository.</p>

    <pre><code>git clone https://github.com/jkulhanek/deep-rl-pytorch.git

    </code></pre>

    <p>All dependencies are prepared in a docker container. If you have nvidia-docker
    enabled, you can use this image. To pull and start the image just run:</p>

    <pre><code>docker run --runtime=nvidia --net=host -it kulhanek/deep-rl-pytorch:latest
    bash

    </code></pre>

    <p>From there, you can either clone your own repository containing your experiments
    or clone this one.</p>

    <h2>

    <a id="user-content-concepts" class="anchor" href="#concepts" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Concepts</h2>

    <p>All algorithms are implemented as base classes. In your experiment your need
    to subclass from those base classes. The <code>deep_rl.core.AbstractTrainer</code>
    class is used for all trainers and all algorithms inherit this class. Each trainer
    can be wrapped in several wrappers (classes extending <code>deep_rl.core.AbstractWrapper</code>).
    Those wrappers are used for saving, logging, terminating the experiment and etc.
    All experiments should be registered using <code>@deep_rl.register_trainer</code>
    decorator. This decorator than wraps the trainer with default wrappers. This can
    be controlled by passing arguments to the decorator. All registered trainers (experiments)
    can be run by calling <code>deep_rl.make_trainer(&lt;&lt;name&gt;&gt;).run()</code>.</p>

    <h2>

    <a id="user-content-implemented-algorithms" class="anchor" href="#implemented-algorithms"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Implemented
    algorithms</h2>

    <h3>

    <a id="user-content-a2c" class="anchor" href="#a2c" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>A2C</h3>

    <p>A2C is a synchronous, deterministic variant of Asynchronous Advantage Actor
    Critic (A3C) [2] which according to OpenAI [1] gives equal performance. It is
    however more efficient for GPU utilization.</p>

    <p>Start your experiment by subclassing <code>deep_rl.a2c.A2CTrainer</code>.

    Several models are included in <code>deep_rl.a2c.model</code>. You may want to
    use at least some helper modules contained in this package when designing your
    own experiment.</p>

    <p>In most of the models, initialization is done according to [3].</p>

    <h3>

    <a id="user-content-asynchronous-advantage-actor-critic-a3c-2" class="anchor"
    href="#asynchronous-advantage-actor-critic-a3c-2" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Asynchronous Advantage Actor Critic (A3C)
    [2]</h3>

    <p>This implementation uses multiprocessing. It comes with two optimizers - RMSprop
    and Adam.</p>

    <h3>

    <a id="user-content-actor-critic-using-kronecker-factored-trust-region-acktr-1"
    class="anchor" href="#actor-critic-using-kronecker-factored-trust-region-acktr-1"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Actor
    Critic using Kronecker-Factored Trust Region (ACKTR) [1]</h3>

    <p>This is an improvement of A2C described in [1].</p>

    <h2>

    <a id="user-content-experiments" class="anchor" href="#experiments" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Experiments</h2>

    <blockquote>

    <p>Comming soon</p>

    </blockquote>

    <h2>

    <a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Requirements</h2>

    <p>Those packages must be installed before using the framework for your own algorithm:</p>

    <ul>

    <li>OpenAI baselines (can be installed by running <code>pip install git+https://github.com/openai/baselines.git</code>)</li>

    <li>PyTorch</li>

    <li>Visdom (<code>pip install visdom</code>)</li>

    <li>Gym (<code>pip install gym</code>)</li>

    <li>MatPlotLib</li>

    </ul>

    <p>Those packages must be installed prior running experiments:</p>

    <ul>

    <li>DeepMind Lab</li>

    <li>Gym[atari]</li>

    </ul>

    <h2>

    <a id="user-content-sources" class="anchor" href="#sources" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Sources</h2>

    <p>This repository is based on work of several other authors. We would like to
    express our thanks.</p>

    <ul>

    <li><a href="https://github.com/openai/baselines/tree/master/baselines">https://github.com/openai/baselines/tree/master/baselines</a></li>

    <li><a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/a2c_ppo_acktr">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/a2c_ppo_acktr</a></li>

    <li><a href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></li>

    <li><a href="https://github.com/openai/gym">https://github.com/openai/gym</a></li>

    </ul>

    <h2>

    <a id="user-content-references" class="anchor" href="#references" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

    <p>[1] Wu, Y., Mansimov, E., Grosse, R.B., Liao, S. and Ba, J., 2017. Scalable
    trust-region method for deep reinforcement learning using kronecker-factored approximation.
    In Advances in neural information processing systems (pp. 5279-5288).</p>

    <p>[2] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
    Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement
    learning. In International conference on machine learning (pp. 1928-1937).</p>

    <p>[3] Saxe, A.M., McClelland, J.L. and Ganguli, S., 2013. Exact solutions to
    the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint
    arXiv:1312.6120.</p>

    '
  stargazers_count: 4
  subscribers_count: 1
  topics: []
  updated_at: 1615406783.0
jncc/s2-ard-processor:
  data_format: 2
  description: Sentinel 2 ARD processor
  filenames:
  - mpi-base/Singularity
  - base/Singularity
  full_name: jncc/s2-ard-processor
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-s2-ard-processor\" class=\"anchor\" href=\"\
    #s2-ard-processor\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>S2 ARD Processor</h1>\n<p>Docker based sentinel 2\
    \ Analysis ready production system.</p>\n<h2>\n<a id=\"user-content-base\" class=\"\
    anchor\" href=\"#base\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Base</h2>\n<p>A base docker image packaging\
    \ Dr Pete Buntings Python Atmospheric and Radiometric Correction of Satellite\
    \ Imagery (ARCSI) software (<a href=\"https://www.arcsi.remotesensing.info/\"\
    \ rel=\"nofollow\">https://www.arcsi.remotesensing.info/</a>).</p>\n<p>Based on\
    \ the official ContinuumIO Miniconda3 release with python 3.5, base package contains\
    \ a minimal installaition of ARCSI and its dependencies using the conda package\
    \ manger, correct as of version 3.1.6 (conda reporting 3.6.1).</p>\n<h3>\n<a id=\"\
    user-content-build-or-pull-arcsi-base\" class=\"anchor\" href=\"#build-or-pull-arcsi-base\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Build or Pull arcsi-base</h3>\n<h4>\n<a id=\"user-content-build-image\"\
    \ class=\"anchor\" href=\"#build-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build image</h4>\n<p><code>docker\
    \ build -t jncc/arcsi-base ./base/</code></p>\n<p><strong>OR</strong></p>\n<h4>\n\
    <a id=\"user-content-pull-image-direction-from-docker-hub\" class=\"anchor\" href=\"\
    #pull-image-direction-from-docker-hub\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pull Image direction from docker\
    \ hub</h4>\n<p><code>docker pull jncc/arcsi-base</code></p>\n<h3>\n<a id=\"user-content-usage\"\
    \ class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h3>\n<h4>\n<a id=\"user-content-run-image-interactively\"\
    \ class=\"anchor\" href=\"#run-image-interactively\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run image interactively</h4>\n\
    <p><code>docker run -i -v &lt;local mount point&gt;:/data -t jncc/arcsi-base /bin/bash</code></p>\n\
    <p>To run a container and get help on ARCSI commandline options do:</p>\n<p><code>docker\
    \ run -t jncc/arcsi-base arcsi.py -h</code></p>\n<p>See below under \"Docker example\"\
    \ for a more detailed Sentinel-2 example.</p>\n<h3>\n<a id=\"user-content-docker-example\"\
    \ class=\"anchor\" href=\"#docker-example\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Docker example</h3>\n<div class=\"\
    highlight highlight-source-shell\"><pre>docker run -i -t -v <span class=\"pl-smi\"\
    >${local_data}</span>:/data jncc/arcsi-base \\\n    arcsi.py -s sen2 --stats -f\
    \ KEA --fullimgouts -p RAD SHARP SATURATE CLOUDS TOPOSHADOW STDSREF DOSAOTSGL\
    \ METADATA FOOTPRINT \\\n    --interp near --outwkt /data/<span class=\"pl-smi\"\
    >${PATH_TO_OUTPUT_PROJ_WKT}</span> --projabbv <span class=\"pl-smi\">${PROJ_ABBREVIATION}</span>\
    \ -t /data/tmp/ -o /data/output/ \\\n    --dem /data/<span class=\"pl-smi\">${PATH_TO_DEM}</span>\
    \ -i /data/inputs/<span class=\"pl-smi\">${SINGLE_INPUT_FILE}</span></pre></div>\n\
    <h3>\n<a id=\"user-content-see-also\" class=\"anchor\" href=\"#see-also\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>See\
    \ also</h3>\n<p>Thanks to Markus Neteler (<a href=\"https://github.com/mundialis/docker-arcsi\"\
    >https://github.com/mundialis/docker-arcsi</a>), Edward P. Morris and Angelos\
    \ Tzotsos for their work on the orignal ARCSI Dockerfile.</p>\n"
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1604315244.0
joaocaldeira/singularity_imgs:
  data_format: 2
  description: Singularity images to run on the cluster
  filenames:
  - Singularity.py3_tf114
  - Singularity.py3_tf112
  - Singularity.py3_tf114_lls
  - Singularity.py3_tf113
  - Singularity.py3_astro
  - Singularity.py3_tf112_plus
  - Singularity.py3_tf115
  full_name: joaocaldeira/singularity_imgs
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_imgs" class="anchor" href="#singularity_imgs"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_imgs</h1>

    <p><a href="https://singularity-hub.org/collections/2968" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity images to run on the cluster</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1590440777.0
kalibera/rchk:
  data_format: 2
  description: null
  filenames:
  - image/Singularity.bionic
  - image/Singularity.def
  full_name: kalibera/rchk
  latest_release: null
  readme: '<p>This project consists of several bug-finding tools that look for memory

    protection errors in C source code using R API, that is in the source code

    of <a href="http://www.r-project.org/" rel="nofollow">R</a> itself and packages.  The
    tools perform

    whole-program static analysis on LLVM bitcode and run on Linux.  About

    200-300 memory protection bugs have been found using rchk and fixed in R.

    rchk is now regularly used to check <a href="https://github.com/kalibera/cran-checks/tree/master/rchk">CRAN

    packages</a>.</p>

    <p>To use the tool, one needs to build R from source using a special compiler

    wrapper, which builds LLVM bitcode in addition to native code (both shared

    libraries and executables). R packages are then installed using this version

    of R, providing LLVM bitcode for their shared libraries as well. The core of

    rchk is implemented in C++ and analyzes the LLVM bitcode of R packages and R

    itself. Several installation options are provided, including containers.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>The tool is available in pre-built containers, Docker and Singularity, for

    <em>non-interactive</em> use. The container is invoked as a command to check a

    particular package:</p>

    <pre><code>docker pull kalibera/rchk:latest

    docker run kalibera/rchk:latest audio

    </code></pre>

    <pre><code>singularity pull shub://kalibera/rchk:def

    singularity run kalibera-rchk-master-def.simg jpeg

    </code></pre>

    <p>For more details, see <a href="doc/DOCKER.md">Docker rchk container</a> and

    <a href="doc/SINGULARITY.md">Singularity rchk container</a>. This setup is good
    for

    occasional checking of a single package. Docker clients are

    available for Linux, macOS and Windows. Singularity only for Linux.</p>

    <p>The tool can also be used interactively in a virtual machine running Ubuntu,

    which can be automatically installed using Vagrant scripts. This setup is

    good for Linux, Windows and macOS users and makes it faster to repeatedly

    check the same package and easier to customize the process. See

    <a href="doc/INSTALLATION.md">Automated installation (Docker/Virtualbox) for interactive
    use</a>.</p>

    <p>Finally, the tool can be installed natively on Linux, compiled from source.

    This setup is good for interactive use and reduces disk space overhead. The

    setup is not automated, but only requires several steps described for recent

    Linux distributions. See <a href="doc/INSTALLATION.md">Native installation on
    Linux for interactive use</a>.</p>

    <p>An alternative docker image is also available from third parties on R-hub

    (<code>rhub/ubuntu-rchk</code>,

    <a href="https://github.com/r-hub/rhub-linux-builders/tree/master/ubuntu-rchk">source</a>).</p>

    <h2>

    <a id="user-content-checking-the-first-package-interactive-use" class="anchor"
    href="#checking-the-first-package-interactive-use" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Checking the first package (interactive
    use)</h2>

    <p>This part applies to interactive installation of rchk (natively or automated

    install in Docker/Virtualbox).  For this that one also needs to install

    <code>subversion</code>, <code>rsync</code> (<code>apt-get install subversion
    rsync</code>, but already

    available in the automated install).  More importantly, one also needs any

    dependencies needed by that package.</p>

    <ol>

    <li>Build R producing also LLVM bitcode

    <ul>

    <li><code>svn checkout https://svn.r-project.org/R/trunk</code></li>

    <li><code>cd trunk</code></li>

    <li>

    <code>. ../scripts/config.inc</code> (<em>in automated install</em>, <code>. /opt/rchk/scripts/config.inc</code>)</li>

    <li>

    <code>. ../scripts/cmpconfig.inc</code> (<em>in automated install</em>, <code>.
    /opt/rchk/scripts/cmpconfig.inc</code>)</li>

    <li>

    <code>../scripts/build_r.sh</code> (<em>in automated install</em>, <code>/opt/rchk/scripts/build_r.sh</code>)</li>

    </ul>

    </li>

    <li>Install and check the package

    <ul>

    <li><code>echo ''install.packages("jpeg",repos="http://cloud.r-project.org")''
    |  ./bin/R --no-echo</code></li>

    <li>

    <code>../scripts/check_package.sh jpeg</code> (in VM install, <code>/opt/rchk/scripts/check_package.sh
    jpeg</code>)</li>

    </ul>

    </li>

    </ol>

    <p>The output of the checking is in files

    <code>packages/lib/jpeg/libs/jpeg.so.*check</code>. For version 0.1-8 of the package,

    <code>jpeg.so.maacheck</code> includes</p>

    <pre><code>WARNING Suspicious call (two or more unprotected arguments) to Rf_setAttrib
    at read_jpeg /rchk/trunk/packages/build/IsnsJjDm/jpeg/src/read.c:131

    </code></pre>

    <p>which is a true error. <code>bcheck</code> does not find any errors, <code>jpeg.so.bcheck</code>

    only contains something like</p>

    <pre><code>Analyzed 15 functions, traversed 1938 states.

    </code></pre>

    <p>To check the next package, just follow the same steps, installing it into

    this customized version of R.  When checking a tarball, one would typically

    first install the CRAN/BIOC version of the package to get all dependencies

    in, and then use <code>R CMD INSTALL</code> to install the newest version to check
    from

    the tarball.</p>

    <p>One can reduce the number of required R package dependencies by only

    installing LinkingTo dependencies of the package and then installing the

    package with <code>--libs-only</code> option (only shared libraries are built
    and

    installed). This is enough to build shared libraries of most but not all

    packages. Docker and singularity rchk containers for non-interactive use do

    this, see <code>scripts/utils.r</code> and definitions of the containers for more

    details.</p>

    <p>Further information:</p>

    <ul>

    <li>

    <a href="doc/INSTALLATION.md">Installation</a> - installation instructions.</li>

    <li>

    <a href="doc/USAGE.md">User documentation</a> - how to use the tools and what
    they check.</li>

    <li>

    <a href="doc/INTERNALS.md">Internals</a> - how the tools work internally.</li>

    <li>

    <a href="doc/BUILDING.md">Building</a> - how to get the necessary bitcode files
    for R/packages; this is now encapsulated in scripts, but the background is here</li>

    </ul>

    <p><a href="https://singularity-hub.org/collections/2534" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 94
  subscribers_count: 10
  topics: []
  updated_at: 1616664726.0
kapsakcj/singularities:
  data_format: 2
  description: Singularity recipes for bioinformatics software
  filenames:
  - quast/5.0.0/Singularity.quast.5.0.0
  - lyveset/1.1.4f/Singularity.lyveset.1.1.4f
  - seqsero2/1.0.0/Singularity.seqsero2.1.0.0
  - seqsero2/0.1/Singularity.seqsero2-0.1
  - spades/3.13.0/Singularity.spades.3.13.0
  full_name: kapsakcj/singularities
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularities\" class=\"anchor\" href=\"#singularities\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>singularities</h1>\n<p>Singularity recipes for bioinformatics software.\
    \ Build singularity images with these recipes (sudo required) or download/pull\
    \ the images from <a href=\"https://singularity-hub.org/collections/2778\" rel=\"\
    nofollow\">singularity-hub.org</a></p>\n<p>This repo is <strong>WORK IN PROGRESS</strong>.\
    \ Feel free to try the recipes/Singularity builds, but they are <strong>not tested\
    \ deeply and are in no way guaranteed to work. Proceed at your own risk</strong>.</p>\n\
    <p>It is somewhat modeled after <a href=\"https://github.com/StaPH-B/docker-builds\"\
    >https://github.com/StaPH-B/docker-builds</a> , but with Singularity recipes instead.</p>\n\
    <p>Sysadmins for High Performance Cluster computers almost always favor Sinularity\
    \ over Docker :) so I'm starting to learn the ways of Singularity.</p>\n<h2>\n\
    <a id=\"user-content-available-singularity-images\" class=\"anchor\" href=\"#available-singularity-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Available Singularity images</h2>\n<table>\n<thead>\n<tr>\n<th align=\"\
    center\">Software</th>\n<th align=\"center\">Version</th>\n<th align=\"center\"\
    >Link</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">SPAdes</td>\n\
    <td align=\"center\">3.13.0</td>\n<td align=\"center\"><a href=\"http://cab.spbu.ru/software/spades/\"\
    \ rel=\"nofollow\">http://cab.spbu.ru/software/spades/</a></td>\n</tr>\n<tr>\n\
    <td align=\"center\">QUAST</td>\n<td align=\"center\">5.0.0</td>\n<td align=\"\
    center\"><a href=\"https://github.com/ablab/quast\">https://github.com/ablab/quast</a></td>\n\
    </tr>\n<tr>\n<td align=\"center\">Lyve-SET</td>\n<td align=\"center\">1.1.4f</td>\n\
    <td align=\"center\"><a href=\"https://github.com/lskatz/lyve-SET\">https://github.com/lskatz/lyve-SET</a></td>\n\
    </tr>\n<tr>\n<td align=\"center\">SeqSero2</td>\n<td align=\"center\">0.1, 1.0.0</td>\n\
    <td align=\"center\"><a href=\"https://github.com/denglab/SeqSero2\">https://github.com/denglab/SeqSero2</a></td>\n\
    </tr>\n</tbody>\n</table>\n<p>These Singularity images can be built if you have\
    \ Singularity installed and <strong>have sudo/admin priveleges</strong></p>\n\
    <pre><code># build an image using a recipe (called Singularity in this example)\n\
    sudo singularity build my-new-singularity-image.simg /path/to/Singularity\n\n\
    # download the repo\ngit clone https://github.com/kapsakcj/singularities.git\n\
    # another example using the SPAdes recipe\nsudo singularity build my-new-spades-3.13.0-image.simg\
    \ /path/to/Singularity.spades.3.13.0\n</code></pre>\n<p>These Singularity images\
    \ are also available to download from singularity-hub.org if you <strong>don't\
    \ have sudo priveleges</strong> (no build necessary!). The badge below is a link\
    \ to the singularity-hub.org collection.</p>\n<p><a href=\"https://singularity-hub.org/collections/2778\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>The name of the Singularity hub collection\
    \ is <code>kapsakcj/singularities</code> and the tag is specified by the extenion\
    \ of the Singularity recipe file. For example the recipe, <code>/spades/3.13.0/Singularity.spades.3.13.0</code>,\
    \ can be downloaded like so:</p>\n<pre><code># download an image like so, and\
    \ name it whatever you want with the --name flag\nsingularity pull --name my-new-spades-3.13.0-image\
    \ shub://kapsakcj/singularities:spades.3.13.0\n</code></pre>\n<h2>\n<a id=\"user-content-useful-links-and-resources\"\
    \ class=\"anchor\" href=\"#useful-links-and-resources\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Useful links\
    \ and resources</h2>\n<ul>\n<li>Singularity v2.6 User guide <a href=\"https://www.sylabs.io/guides/2.6/user-guide/index.html\"\
    \ rel=\"nofollow\">https://www.sylabs.io/guides/2.6/user-guide/index.html</a>\n\
    </li>\n<li>SingularityHub <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >https://singularity-hub.org/</a>\n</li>\n<li>Excellent tutorial on Singularity\
    \ (using v2.5) from Sylabs, many other links within <a href=\"https://github.com/Singularity-tutorial/Singularity-tutorial.github.io\"\
    >https://github.com/Singularity-tutorial/Singularity-tutorial.github.io</a>\n\
    </li>\n<li>How to build a container using Singularity Hub, linked to a github\
    \ repo with Singularity recipes <a href=\"https://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container\"\
    >https://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container</a>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-tipstricksthings-to-remember-about-singularity--many-from-jake-garfin--\"\
    \ class=\"anchor\" href=\"#tipstricksthings-to-remember-about-singularity--many-from-jake-garfin--\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Tips/Tricks/Things-to-remember about Singularity [ many from Jake\
    \ Garfin :) ]</h3>\n<ul>\n<li>Singularity automatically brings your user &amp;\
    \ group into the container with you (ie. no <code>-u $(id -u):$(id -g)</code>\
    \ needed like in Docker)</li>\n<li>Singularity (by default) wants to mount your\
    \ entire home directory inside the container as well. Use <code>--cleanenv</code>\
    \ and <code>--containall</code> to keep things separate and bring in specific\
    \ directories you want with <code>-B /local-dir:/dir-in-container</code>\n</li>\n\
    <li>Docker images converted to Singularity that want to write to system directories\
    \ owned by root aren't going to work out of the box.</li>\n<li>If you are making\
    \ a container with something that uses perl, add this to the recipe in the <code>%environment</code>\
    \ section to prevent locale settings errors (see lyveset recipe)</li>\n</ul>\n\
    <pre><code>%environment\n    export LC_ALL=C\n</code></pre>\n<p>TO-DO</p>\n<ul>\n\
    <li>\n<p>How to: Singularity</p>\n<ul>\n<li>Links to docs for installing</li>\n\
    <li>How to download an image from singularity hub</li>\n<li>How to download an\
    \ image fromm dockerhub\n<ul>\n<li><code>singularity pull docker://staphb/skesa</code></li>\n\
    </ul>\n</li>\n<li>How to take a recipe and build locally (sudo required)</li>\n\
    <li>How to run a Singularity container\n<ul>\n<li>Different ways to run - <code>singularity\
    \ exec [...]</code> or ./name-of-singularity.simg [...]</li>\n<li>Mounting DIRs\
    \ - default way (mount entire <code>$HOME</code> DIR), or way to mount a specific\
    \ DIR and not entire <code>$HOME</code> DIR</li>\n</ul>\n</li>\n</ul>\n</li>\n\
    <li>\n<p>Create SingularityHub account</p>\n<ul>\n<li>link to this repo</li>\n\
    <li>create autobuilds for each recipe</li>\n</ul>\n</li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1583250575.0
kavonrtep/SeqGrapheR:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: kavonrtep/SeqGrapheR
  latest_release: v0.5.0.2.4
  readme: '<h1>

    <a id="user-content-batch-connect---osc-rstudio-server" class="anchor" href="#batch-connect---osc-rstudio-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch
    Connect - OSC RStudio Server</h1>

    <p><a href="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667"
    alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/osc/bc_osc_rstudio_server_pitzer.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>An interactive app designed for OSC OnDemand that launches an RStudio Server

    within an Pitzer batch job.</p>

    <h2>

    <a id="user-content-deprecated-application-warning" class="anchor" href="#deprecated-application-warning"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deprecated
    application warning</h2>

    <p>This application no longer works.  It raises an exception when users attempt
    to submit jobs.

    This is because we now have functionality to submit to multiple clusters and

    <a href="https://github.com/OSC/bc_osc_rstudio_server">the generic application</a>
    now submits

    to pitzer rendering this application useless.</p>

    <p>For historic versions, see the last released you can still view

    <a href="https://github.com/OSC/bc_osc_rstudio_server_pitzer/tree/v0.3.0">v0.3.0</a>
    as it was the last

    working version of this application.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1617867243.0
kbronik2017/nicpython36:
  data_format: 2
  description: modified version of nicMSlesions
  filenames:
  - Singularity
  full_name: kbronik2017/nicpython36
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-ms_cnn\" class=\"anchor\" href=\"#ms_cnn\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>MS_CNN</h1>\n\
    <p>[This is a modified version of nicMSlesions (<a href=\"https://github.com/NIC-VICOROB/nicMSlesions\"\
    >https://github.com/NIC-VICOROB/nicMSlesions</a>)]\n<br>\n<a href=\"CNN.jpeg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img height=\"300\" src=\"CNN.jpeg\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h1>\n<a id=\"user-content-this--version-support-additionally-the-following-functionalities\"\
    \ class=\"anchor\" href=\"#this--version-support-additionally-the-following-functionalities\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>This  version support additionally the following functionalities:</h1>\n\
    <dl>\n  <dt>(1) Runnable on a Mac system/computer</dt>\n  <dt>(2) Cold start and\
    \ warm start support:</dt>\n  <dd>- Allowing to re-create the architecture of\
    \ the model</dd>\n  <dd>- Allowing to use the saved weights of the model</dd>\n\
    \  <dd>- Allowing to use  the training configuration and avoiding to run preprocessing\
    \ again</dd>\n  <dd>- Allowing to resume training exactly where it left off(interrupting\
    \ the training is     \n    allowed throughout the training process)</dd>\n  <dd>-\
    \ Allowing to use pretrained model</dd>\n  <dt>(3) Supporting Python 3</dt>\n\
    \  <dt>(4) Integrated Tensorborad [to provide the measurements and visualisations\
    \ of TensorFlow execution (to understand, debug, and optimisation of  the TensorFlow\
    \ programs)]</dt>\n  <dt>(5) Checking whether a file or directory is relevant\
    \ for Training and Testing</dt> \n  <dt>(6) Easy HPC (High Performance Computing)\
    \ support</dt> \n  <dt>(7) Bias correction of masks using FSL</dt>\n  <dt>(8)\
    \ Registration, moving all images to the Flair, T1 or Standard space</dt>\n</dl>\n\
    <br>\n <p><a href=\"BR.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"><img\
    \ height=\"500\" src=\"BR.jpg\" style=\"max-width:100%;\"></a></p>\n \n<br>\n\
    \ <p><a href=\"note.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"><img height=\"\
    100\" src=\"note.jpg\" style=\"max-width:100%;\"></a></p>\n \n# Running the Program!\n\
    <p>This modified version can be run with or without a GUI (similar to original\
    \ version)</p>\n<p>After lunching the graphical user interface, user will need\
    \ to provide necessary information to start training/testing as follows:</p>\n\
    <br>\n <p><a href=\"GUI_NM.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img height=\"500\" src=\"GUI_NM.jpg\" style=\"max-width:100%;\"></a></p>\n \n\
    <h1></h1>\n<h1>\n<a id=\"user-content-running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\"\
    \ class=\"anchor\" href=\"#running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running the Program on the HPC cluster using NVIDIA GPUs(without any\
    \ additional library/dependency installation):</h1>\n<br>\n <p><a href=\"hpc.jpeg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img height=\"200\" src=\"hpc.jpeg\"\
    \ style=\"max-width:100%;\"></a></p>\n \n<p>First, user will need to be sure that\
    \ \"singularity\"\n<a href=\"https://singularity.lbl.gov/\" rel=\"nofollow\">https://singularity.lbl.gov/</a>\n\
    is available on local or remote machine.</p>\n<p>Then:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  - singularity pull docker://kbronik/ms_cnn_ucl:latest\
    \  </pre></div>\n<p>After running the above, a singularity image using docker\
    \ hub (docker://kbronik/ms_cnn_ucl:latest) will be generated:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>  - path to singularity//..///ms_cnn_ucl_latest.sif\
    \  </pre></div>\n<p>Finally:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>  - singularity run --nv   (path to singularity)//..///ms_cnn_ucl_latest.sif\
    \  python  (path to nicpython36)/nic_train_network_batch.py (or other nic-python\
    \ code)</pre></div>\n<br>\n <p><a href=\"note_HPC.jpg\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img height=\"120\" src=\"note_HPC.jpg\" style=\"max-width:100%;\"\
    ></a></p>\n \n<h1>\n<a id=\"user-content-for-an-interactive-session\" class=\"\
    anchor\" href=\"#for-an-interactive-session\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>For an interactive session:</h1>\n\
    <div class=\"highlight highlight-source-shell\"><pre>  - singularity shell   (path\
    \ to singularity)//..///ms_cnn_ucl_latest.sif </pre></div>\n<p>Then:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>  - <span class=\"pl-c1\">source</span>\
    \ activate idp\n  - python (path to nicpython36)/app.py</pre></div>\n<h1>\n<a\
    \ id=\"user-content-for-an-interactive-session-tensorflow-on-cpu-only\" class=\"\
    anchor\" href=\"#for-an-interactive-session-tensorflow-on-cpu-only\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>For\
    \ an interactive session (TensorFlow on CPU only):</h1>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  - singularity <span class=\"pl-c1\">exec</span>\
    \  docker://kbronik/ms-ucl-cnn-cpu:CPU_Latest  python  (path to nicpython36)/app.py\
    \ </pre></div>\n"
  stargazers_count: 1
  subscribers_count: 0
  topics:
  - convolutional-neural-networks
  - nicmslesions
  - nicmslesions-python3
  updated_at: 1587563855.0
kernsuite-debian/singularity-container:
  data_format: 2
  description: null
  filenames:
  - examples/asciinema/Singularity
  - examples/opensuse/Singularity
  - examples/busybox/Singularity
  - examples/self/Singularity
  - examples/raspbian/Singularity
  - examples/docker/Singularity
  - examples/ubuntu/Singularity
  - examples/arch/Singularity
  - examples/centos/Singularity
  - examples/shub/Singularity
  - examples/apps/Singularity
  - examples/apps/Singularity.cowsay
  - examples/scientific/Singularity
  full_name: kernsuite-debian/singularity-container
  latest_release: null
  readme: "<p>_Please note recent changes in the github repo branch structure.  If\
    \ you want\nto install a stable release of Singularity, please use a tag or a\
    \ <a href=\"https://github.com/singularityware/singularity/releases\">release\n\
    tarball</a>.  If you are\na developer who would like to contribute to Singularity\
    \ and you want to know\nwhich branch to submit your pull request to, please see\
    \ notes on the branch\nreorganization <a href=\"https://www.sylabs.io/2018/03/managing-singularity-branches/\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>Please also note that 2.6.0 is expected to\
    \ be the final feature release in the\n2.x series. While bug fixes may be added\
    \ via point releases (for example 2.6.1)\nno new features releases (for example\
    \ 2.7.0) are planned.</p>\n<p>Pull requests adding features to the 2.x series\
    \ will no longer be reviewed.<br>\nAny new features should be targeted to the\
    \ master branch (which used to be\ncalled development-3.0)._</p>\n<p><a href=\"\
    https://travis-ci.org/singularityware/singularity\" rel=\"nofollow\"><img src=\"\
    https://camo.githubusercontent.com/f9a86612d918b5d7b8615b4f1203222f491b2a672958652856370704a30742f9/68747470733a2f2f7472617669732d63692e6f72672f73696e67756c6172697479776172652f73696e67756c61726974792e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/singularityware/singularity.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a></p>\n<ul>\n<li><a href=\"CONTRIBUTING.md\">Guidelines\
    \ for Contributing</a></li>\n<li><a href=\".github/PULL_REQUEST_TEMPLATE.md\"\
    >Pull Request Template</a></li>\n<li><a href=\"LICENSE.md\">Project License</a></li>\n\
    <li><a href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\">Documentation</a></li>\n\
    <li><a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459\"\
    \ rel=\"nofollow\">Citation</a></li>\n</ul>\n<h1>\n<a id=\"user-content-singularity---enabling-users-to-have-full-control-of-their-environment\"\
    \ class=\"anchor\" href=\"#singularity---enabling-users-to-have-full-control-of-their-environment\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity - Enabling users to have full control of their environment.</h1>\n\
    <p>Starting a Singularity container \"swaps\" out the host\noperating system environment\
    \ for one the user controls!</p>\n<p>Let's say you are running Ubuntu on your\
    \ workstation or server, but you\nhave an application which only runs on Red Hat\
    \ Enterprise Linux 6.3.\nSingularity can instantly virtualize the operating system,\
    \ without having\nroot access, and allow you to run that application in its native\
    \ environment!</p>\n<h1>\n<a id=\"user-content-about\" class=\"anchor\" href=\"\
    #about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>About</h1>\n<p>Singularity is a container platform focused on supporting\
    \ \"Mobility of\nCompute\".</p>\n<p>Mobility of Compute encapsulates the development\
    \ to compute model where\ndevelopers can work in an environment of their choosing\
    \ and creation, and\nwhen the developer needs additional compute resources, this\
    \ environment\ncan easily be copied and executed on other platforms. Additionally,\
    \ as the\nprimary use case for Singularity is targeted towards computational portability.\n\
    Many of the barriers to entry of other container solutions do not apply to\nSingularity,\
    \ making it an ideal solution for users (both computational and\nnon-computational)\
    \ and HPC centers.</p>\n<h2>\n<a id=\"user-content-the-container\" class=\"anchor\"\
    \ href=\"#the-container\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>The Container</h2>\n<p>Singularity utilizes\
    \ container images, which means when you enter and\nwork within the Singularity\
    \ container, you are physically located inside\nof this image. The image grows\
    \ and shrinks in real time as you install\nor delete files within the container.\
    \ If you want to copy a container,\nyou copy the image.</p>\n<p>Using a single\
    \ image for the container format has added advantages\nespecially within the context\
    \ of HPC with large parallel file systems\nbecause all metadata operations within\
    \ the container occur within the\ncontainer image (and not on the metadata server!).</p>\n\
    <h2>\n<a id=\"user-content-mobility-of-compute\" class=\"anchor\" href=\"#mobility-of-compute\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Mobility of Compute</h2>\n<p>With Singularity, developers who like\
    \ to be able to easily control their\nown environment will love Singularity's\
    \ flexibility. Singularity does not\nprovide a pathway for escalation of privilege\
    \ (as do other container\nplatforms which are thus not appropriate for multi-tenant\
    \ resources) so\nyou must be able to become root on the host system (or virtual\
    \ machine)\nin order to modify the container.</p>\n<p>A Singularity container\
    \ can be launched in a variety of different ways\ndepending on what you wanted\
    \ to do with it. A simple method might be to\nlaunch an interactive shell within\
    \ the container image as follows:</p>\n<pre><code>[gmk@centos7-x64 demo]$ singularity\
    \ shell /tmp/Centos-7.img \ngmk@Centos-7.img demo&gt; echo \"Hello from within\
    \ the container\"\nHello from within the container\ngmk@Centos-7.img demo&gt;\
    \ whoami\ngmk\ngmk@Centos-7.img demo&gt; \n</code></pre>\n<p>And if you want to\
    \ do the same thing as root:</p>\n<pre><code>[gmk@centos7-x64 demo]$ sudo singularity\
    \ shell -w /tmp/Centos-7.img \nroot@Centos-7.img demo&gt; whoami\nroot\nroot@Centos-7.img\
    \ demo&gt; \n</code></pre>\n<p><em>note: By default, Singularity launches the\
    \ container image in read-only\nmode (so it can be easily launched in parallel).\
    \ The <code>-w</code> option used above\ntells Singularity to mount the image\
    \ in read/write mode, such that root\ncan now make changes to the container.</em></p>\n\
    <p>Additionally, relevant file systems on your host are shared, automatically,\n\
    within the context of your container. This can be demonstrated as\nfollows:</p>\n\
    <pre><code>[gmk@centos7-x64 demo]$ pwd\n/home/gmk/demo\n[gmk@centos7-x64 demo]$\
    \ echo \"world\" &gt; hello\n[gmk@centos7-x64 demo]$ singularity shell /tmp/Centos-7.img\
    \ \ngmk@Centos-7.img demo&gt; pwd\n/home/gmk/demo\ngmk@Centos-7.img demo&gt; cat\
    \ hello\nworld\n</code></pre>\n<p>Once the developer has completed their environment,\
    \ the image file can\nbe compressed and copied to any other system that has Singularity\
    \ installed.\nIf you do not have root on that system, you will not be able to\
    \ make any\nchanges to the image once on that system. But you will be able to\
    \ use the\ncontainer and access the data and files outside the container as\n\
    easily as you would on your development system or virtual machine.</p>\n<h2>\n\
    <a id=\"user-content-portability-of-singularity-container-images\" class=\"anchor\"\
    \ href=\"#portability-of-singularity-container-images\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Portability of\
    \ Singularity container images</h2>\n<p>Singularity images are highly portable\
    \ between Linux distributions (as\nlong as the binary format is the same). You\
    \ can generate your image on\nDebian or CentOS, and run it on Mint or Slackware.</p>\n\
    <p>Within a particular container, one can include their programs, data,\nscripts\
    \ and pipelines and thus port a workflow to any other architecture\ncompatible\
    \ Linux system or distribution.</p>\n<h2>\n<a id=\"user-content-bootstrapping-new-images\"\
    \ class=\"anchor\" href=\"#bootstrapping-new-images\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Bootstrapping\
    \ new images</h2>\n<p>Generally, when bootstrapping an image from scratch, you\
    \ must build it from\na compatible host. This is because you must use the distribution\
    \ specific\ntools it comes with (e.g. Red Hat does not provide Debian's debootstrap\
    \ by\ndefault). But once the image has been bootstrapped and includes the necessary\n\
    bits to be self-hosting (e.g. YUM on CentOS and apt-get on Debian/Ubuntu) then\n\
    the process of managing the container can be implemented from within the\ncontainer.</p>\n\
    <p>The process of building a bootstrap starts with a definition\nspecification.\
    \ The definition file describes how you want the operating\nsystem to be built,\
    \ what should go inside it and any additional\nmodifications necessary.</p>\n\
    <p>Here is an example of a very simple bootstrap definition file for CentOS:</p>\n\
    <pre><code>BootStrap: yum\nOSVersion: 7\nMirrorURL: http://mirror.centos.org/centos-%{OSVERSION}/%{OSVERSION}/os/$basearch/\n\
    Include: yum\n</code></pre>\n<p>Once you have created your bootstrap definition,\
    \ you can build your\nSingularity container image by first creating a blank image,\
    \ and then\nbootstrapping using your definition file:</p>\n<pre><code>[gmk@centos7-x64\
    \ demo]$ sudo singularity create /tmp/Centos-7.img\n[gmk@centos7-x64 demo]$ sudo\
    \ singularity bootstrap /tmp/Centos-7.img centos.def\n</code></pre>\n<p>From there\
    \ we can immediately start using the container:</p>\n<pre><code>[gmk@centos7-x64\
    \ demo]$ singularity exec /tmp/Centos-7.img cat /etc/redhat-release \nCentOS Linux\
    \ release 7.2.1511 (Core) \n[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-7.img\
    \ python --version\nPython 2.7.5\n[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-7.img\
    \ python hello.py \nhello world\n[gmk@centos7-x64 demo]$ \n</code></pre>\n<p>And\
    \ if I do this same process again, while changing the <strong>OSVersion</strong>\n\
    variable in the bootstrap definition to <strong>6</strong> (where previously it\
    \ was\nautomatically ascertained by querying the RPM database), we can\nessentially\
    \ build a CentOS-6 image in exactly the same manner as\nabove. Doing so reveals\
    \ this:</p>\n<pre><code>[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-6.img\
    \ cat /etc/redhat-release \nCentOS release 6.7 (Final)\n[gmk@centos7-x64 demo]$\
    \ singularity exec /tmp/Centos-6.img python --version\nPython 2.6.6\n[gmk@centos7-x64\
    \ demo]$ \n</code></pre>\n<p>And as expected, the Python version we now see is\
    \ what comes from by\ndefault in CentOS-6.</p>\n<h1>\n<a id=\"user-content-cite-as\"\
    \ class=\"anchor\" href=\"#cite-as\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Cite as:</h1>\n<pre><code>Kurtzer\
    \ GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for mobility\
    \ of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459\n\
    </code></pre>\n<p>We also have a Zenodo citation:</p>\n<pre><code>Kurtzer, Gregory\
    \ M.. (2016). Singularity 2.1.2 - Linux application and environment\ncontainers\
    \ for science. 10.5281/zenodo.60736\n\nhttp://dx.doi.org/10.5281/zenodo.60736\n\
    </code></pre>\n<h1>\n<a id=\"user-content-webpage\" class=\"anchor\" href=\"#webpage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Webpage</h1>\n<p>We have full documentation at <a href=\"https://www.sylabs.io/docs/\"\
    \ rel=\"nofollow\">https://www.sylabs.io/docs/</a>, and <a href=\"http://www.github.com/singularityware/singularityware.github.io\"\
    >welcome contributions</a>.</p>\n"
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1589975545.0
klm122/w2l:
  data_format: 2
  description: w2l
  filenames:
  - Singularity.gpu
  - Singularity
  full_name: klm122/w2l
  latest_release: null
  readme: '<h1>

    <a id="user-content-w2l" class="anchor" href="#w2l" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>w2l</h1>

    <p>w2l</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583870496.0
lamps24/neural_network_project:
  data_format: 2
  description: null
  filenames:
  - Singularity.gpu
  - Singularity
  - Singularity.devel
  full_name: lamps24/neural_network_project
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-csci5980\" class=\"anchor\" href=\"#csci5980\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>csci5980</h1>\n<p>Final project for CSci 5980: deep learning for automatic\
    \ music translation.</p>\n<p>Follow theses steps to install all package dependencies\
    \ for running the model:</p>\n<p>We first install software dependencies for manipulating\
    \ raw audio (<code>ffmpeg</code>):</p>\n<ol>\n<li>\n<p>Create a local software\
    \ directory\n<code>mkdir ~/software</code></p>\n</li>\n<li>\n<p>Install the NASM\
    \ assembler (dependency of ffmpeg):</p>\n</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span> <span class=\"\
    pl-k\">~</span>/software\nwget https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2\n\
    tar -xvf nasm-2.14.02.tar.bz2\n<span class=\"pl-c1\">cd</span> nasm-2.14.02\n\
    ./configure --prefix=<span class=\"pl-k\">~</span>/software/nasm/\nmake install\n\
    <span class=\"pl-k\">export</span> PATH=<span class=\"pl-smi\">$PATH</span>:<span\
    \ class=\"pl-k\">~</span>/software/nasm/bin/</pre></div>\n<ol start=\"3\">\n<li>Make\
    \ sure that NASM assembler installed correctly:</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>nasm -v</pre></div>\n<p>The output should look\
    \ something like:\n<code>NASM version 2.14.02 compiled on Mar 11 2020</code></p>\n\
    <ol start=\"4\">\n<li>Install ffmpeg:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/software\n\
    wget https://ffmpeg.org/releases/ffmpeg-4.2.2.tar.bz2\ntar -xvf ffmpeg-4.2.2.tar.bz2\n\
    <span class=\"pl-c1\">cd</span> ffmpeg-4.2.2\n./configure --prefix=<span class=\"\
    pl-k\">~</span>/software/ffmpeg/\nmake install\n<span class=\"pl-k\">export</span>\
    \ PATH=<span class=\"pl-smi\">$PATH</span>:<span class=\"pl-k\">~</span>/software/ffmpeg/bin/</pre></div>\n\
    <ol start=\"5\">\n<li>Make sure that ffmpeg installed correctly:</li>\n</ol>\n\
    <div class=\"highlight highlight-source-shell\"><pre>ffmpeg -version</pre></div>\n\
    <p>The output should look something like:</p>\n<pre><code>ffmpeg version 4.2.2\
    \ Copyright (c) 2000-2019 the FFmpeg developers\nbuilt with gcc 4.4.7 (GCC) 20120313\
    \ (Red Hat 4.4.7-23)\nconfiguration: --prefix=/home/csci5980/piehl008/software/ffmpeg/\n\
    libavutil      56. 31.100 / 56. 31.100\nlibavcodec     58. 54.100 / 58. 54.100\n\
    libavformat    58. 29.100 / 58. 29.100\nlibavdevice    58.  8.100 / 58.  8.100\n\
    libavfilter     7. 57.100 /  7. 57.100\nlibswscale      5.  5.100 /  5.  5.100\n\
    libswresample   3.  5.100 /  3.  5.100\n</code></pre>\n<ol start=\"6\">\n<li>Now,\
    \ we can make the virtual environment and install python packages.  First, create\
    \ the virtual environment by running:</li>\n</ol>\n<p><code>conda create --name\
    \ audio-proj python=3.7</code></p>\n<ol start=\"7\">\n<li>Next, install packages\
    \ by running</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/csci5980\nconda install\
    \ --name audio-proj --file requirements.txt --channel defaults --channel conda-forge</pre></div>\n\
    <p>(Note: this can take a while - and you need to say yes to installing everything\
    \ after it solves the environment)</p>\n<ol start=\"8\">\n<li>To activate the\
    \ virtual environment, you can now run <code>source activate audio-proj</code>.\
    \ Note: you should do this to test that you can activate the virtual evironment,\
    \ but you probably shouldn't run a lot unless you are submitting jobs to the queue.\
    \  If you want to use this virtual environment through the MSI notebooks, check\
    \ out the tutorial at <a href=\"https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\"\
    \ rel=\"nofollow\">https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html</a>.</li>\n\
    </ol>\n<h3>\n<a id=\"user-content-adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ class=\"anchor\" href=\"#adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Adding the Virtual Environment to Jupyter Notebooks</h3>\n<p>Now that\
    \ we have created the virtual environment, we can add it to the Jupyter notebook\
    \ kernels so that we can use the virtual environment through MSI's notebook server.\
    \ To do this, we have to add the kernel specifications to the known Jupyter kernels\
    \ for our user:</p>\n<ol start=\"9\">\n<li>If you haven't already, activate your\
    \ virtual environment by running <code>source activate audio-proj</code>. Then\
    \ enter</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>which\
    \ python</pre></div>\n<p>Your output should tell you where the python executable\
    \ for this virtual environment lives - the output for me displays <code>~/.conda/envs/audio-proj/bin/python</code>.\
    \  If you see something that looks like <code>/panfs/roc/msisoft/anaconda/anaconda3-2018.12/bin/python</code>,\
    \ go back and make sure that you have the virtual environment active and try again.\
    \ After you have an ouput that clearly has the name of the virtual environment\
    \ in the directory path (i.e. contains audio-proj in it), continue to the next\
    \ step.</p>\n<ol start=\"10\">\n<li>Now, we need to create the kernel configuration.\
    \ To do this run</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>mkdir <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj\n\
    nano <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj/kernel.json</pre></div>\n\
    <p>The nano command will open a very basic text editor that you can navigate with\
    \ the arrow keys. Enter the following:</p>\n<pre lang=\"text\"><code>{\n \"argv\"\
    : [\n  \"~/.conda/envs/audio-proj/bin/python\", #replace this with your path from\
    \ step 9 above! (and delete this comment)\n  \"-m\",\n  \"ipykernel_launcher\"\
    ,\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Audio Project\
    \ Kernel\",\n \"language\": \"python\"\n}\n</code></pre>\n<p>where you replace\
    \ the first line of the argv array with whatever executable path was output from\
    \ step 9 above (it likely will be identical to this). To exit the nano text editor,\
    \ type <code>Ctrl-x &lt;RETURN&gt;</code> and then type <code>Y &lt;RETURN&gt;</code>\
    \ to save the file.</p>\n<ol start=\"11\">\n<li>Now that you have saved the kernel\
    \ file, you should be able to go to <code>https://notebooks.msi.umn.edu/</code>\
    \ and when you click on the <code>New</code> tab to create a new file, you should\
    \ be able to select <code>Audio Project Kernel</code> as an available kernel to\
    \ run your newly created file in.</li>\n</ol>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1588946712.0
larosap/hackathon_intel_genci:
  data_format: 2
  description: hackathon_intel_genci
  filenames:
  - Sarek/Singularity
  - Sarek/ScLifeLab/Singularity
  full_name: larosap/hackathon_intel_genci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CNTdocker</h1>\n<h2>\n<a id=\"user-content-about\" class=\"anchor\"\
    \ href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>About</h2>\n<p>Dockerfiles to create Docker images\
    \ used by the CNT at the university of Pennsylvania</p>\n<h2>\n<a id=\"user-content-directory-contents-explanation\"\
    \ class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directory\
    \ contents explanation</h2>\n<h3>\n<a id=\"user-content-eeg\" class=\"anchor\"\
    \ href=\"#eeg\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>EEG</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common EEG analysis tools. Usually python 3</p>\n<p><strong>echobase</strong>:\
    \ Dockerfiles used to create images that can calculate functional connectivity\
    \ of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase\
    \ code is from <a href=\"https://github.com/andyrevell/paper001\">https://github.com/andyrevell/paper001</a></p>\n\
    <pre><code>Ubuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\n\
    scipy 1.4.1\n</code></pre>\n<h3>\n<a id=\"user-content-imaging\" class=\"anchor\"\
    \ href=\"#imaging\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>Imaging</strong>:</h3>\n<p>Dockerfiles used\
    \ to create images with common MRI analysis tools.</p>\n<pre><code>  Ubuntu 18.04\n\
    \  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n\
    \  FSL 6.0.1\n</code></pre>\n<h3>\n<a id=\"user-content-ml\" class=\"anchor\"\
    \ href=\"#ml\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><strong>ml</strong>:</h3>\n<p>Dockerfiles used to\
    \ create images with common machine learning tools.</p>\n<p><strong>wavenet</strong>:\
    \ Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet\
    \ paper\n<a href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\"\
    \ rel=\"nofollow\">Wavenet blog</a>\n<a href=\"https://arxiv.org/pdf/1609.03499.pdf\"\
    \ rel=\"nofollow\">Wavenet paper</a></p>\n<pre><code>  Ubuntu 18.04\n  tensorflow\
    \ 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n</code></pre>\n<p><strong>Tensorflow_2.1</strong>:\
    \ Dockerfile to create compatible dependencies to with tensorflow 2.1</p>\n<pre><code>\
    \  Ubuntu 18.04\n  tensorflow 2.1\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1573750055.0
lehtiolab/nf-deqms:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: lehtiolab/nf-deqms
  latest_release: null
  readme: '<h1>

    <a id="user-content-lehtiolabnf-deqms" class="anchor" href="#lehtiolabnf-deqms"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>lehtiolab/nf-deqms</h1>

    <p><strong>A small pipeline to re-run DEqMS on existing results</strong></p>

    <p><a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/0fcfc6847f4944e0c46cb62bb190c0110bafa56ce455c12dd23051df8d710a4a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413532302e30312e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A520.01.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/lehtiolab/nf-deqms" rel="nofollow"><img src="https://camo.githubusercontent.com/4068dc15ebffdfaa7d220510750dd7bcde75393d91d3fe2d05dc15190c515246/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6c656874696f6c61622f6e662d6465716d732e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/lehtiolab/nf-deqms.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>This workflow reruns DEqMS analysis on existing results, e.g. from the <a href="https://github.com/lehtiolab/ddamsproteomics">lehtiolab/ddamsproteomics</a>
    pipeline. It exists so one can use orthogonal sample groups (CTRL vs TREAT, old
    vs young) and rerun, or perhaps correct a mistake in the sample annotation, without
    having to re-search an entire set of spectra against a protein sequence database.</p>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h2>

    <a id="user-content-how-to-run" class="anchor" href="#how-to-run" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to run</h2>

    <ul>

    <li>install <a href="https://nextflow.io" rel="nofollow">Nextflow</a>

    </li>

    <li>install <a href="https://docs.docker.com/engine/installation/" rel="nofollow">Docker</a>,
    <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow">Singularity</a>,
    or <a href="https://conda.io/miniconda.html" rel="nofollow">Conda</a>

    </li>

    <li>run pipeline:</li>

    </ul>

    <pre><code>nextflow run lehtiolab/nf-deqms --proteins proteins.txt --peptides
    peptides.txt --genes genes.txt --ensg ensg.txt --sampletable samples.txt -profile
    standard,docker

    </code></pre>

    <p>You can leave out any accession that you do not have or are not interested
    in (e.g. <code>--ensg</code> in a Swissprot analysis).</p>

    <p>The lehtiolab/nf-deqms pipeline comes with documentation about the pipeline,
    found in the <code>docs/</code> directory:</p>

    <ul>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="https://nf-co.re/usage/troubleshooting" rel="nofollow">Troubleshooting</a></li>

    </ul>

    <p>There is more extensive documentation on the options inside the main.nf file.</p>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>lehtiolab/nf-deqms was originally written by Jorrit Boekel and tries to follow
    the <a href="https://nf-co.re" rel="nofollow">nf-core</a> best practices and templates.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605692054.0
letaylor/docker-letaylor:
  data_format: 2
  description: Docker images
  filenames:
  - images/sc_qc_cluster/Singularity.sc_qc_cluster
  full_name: letaylor/docker-letaylor
  latest_release: null
  readme: '<h1>

    <a id="user-content-docker-letaylor" class="anchor" href="#docker-letaylor" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>docker-letaylor</h1>

    <p>This repo contains Docker images that are automatically built using Travis
    CI. It is not designed to scale to many images as each image is updated if any
    one image changes.</p>

    <h1>

    <a id="user-content-automatically-push-images-to-docker-hub-using-travis-ci" class="anchor"
    href="#automatically-push-images-to-docker-hub-using-travis-ci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Automatically push
    images to Docker Hub using Travis CI</h1>

    <h2>

    <a id="user-content-1-edit-config-files" class="anchor" href="#1-edit-config-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.
    Edit config files</h2>

    <p>Edit the following files:</p>

    <ul>

    <li>

    <code>.travis.yml</code> : alter <code>$IMAGE_NAME</code>.</li>

    </ul>

    <h2>

    <a id="user-content-2-give-travis-ci-access-to-upload-to-docer-hub" class="anchor"
    href="#2-give-travis-ci-access-to-upload-to-docer-hub" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>2. Give Travis CI access
    to upload to Docer Hub</h2>

    <p>Store both <code>$DOCKER_PASSWORD</code> and <code>$DOCKER_USERNAME</code>
    securely in on Travis CI. These are used for authentication.</p>

    <ol>

    <li>Login to the account you want Travis to use to upload on <a href="https://hub.docker.com/"
    rel="nofollow">hub.docker.com</a>.</li>

    <li>Click on your username on the top left and go to ''Account Settings''.</li>

    <li>On the left hand panel, go to ''Security'' and enter your password as requested.</li>

    <li>Now we''ll create an API token. Name it Travis CI.</li>

    <li>Create the token and copy it.</li>

    <li>Login to your account on <a href="https://travis-ci.org" rel="nofollow">travis-ci.org</a>
    and go to the repository that you want to add this automatic functionality to.</li>

    <li>On the right next to ''More options'' go to ''Settings'' in the hamburger
    menu.</li>

    <li>Add an environment variable with the name <code>DOCKER_PASSWORD</code> and
    give it the value of the API token that you copied from <a href="https://hub.docker.com/"
    rel="nofollow">hub.docker.com</a>.</li>

    <li>Add an environment variable with the name <code>DOCKER_USERNAME</code> and
    give it your <a href="https://hub.docker.com/" rel="nofollow">hub.docker.com</a>
    user name.</li>

    </ol>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611328575.0
lkirk/nb-env:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: lkirk/nb-env
  latest_release: null
  readme: '<h1>

    <a id="user-content-nb-env" class="anchor" href="#nb-env" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nb-env</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1615079530.0
lxwgcool/singularity:
  data_format: 2
  description: singularity-recipe-share
  filenames:
  - Singularity.tensorflow-gpu-1.12.0
  full_name: lxwgcool/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-canopy" class="anchor" href="#coesra-singularity-canopy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-canopy</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 2
  subscribers_count: 0
  topics: []
  updated_at: 1574278904.0
maplesond/portcullis:
  data_format: 2
  description: Splice junction analysis and filtering from BAM files
  filenames:
  - Singularity
  full_name: maplesond/portcullis
  latest_release: 1.2.2
  readme: "<p><a href=\"doc/source/images/portcullis_logo.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"doc/source/images/portcullis_logo.png\"\
    \ alt=\"alt text\" title=\"Portcullis\" style=\"max-width:100%;\"></a></p>\n<h1>\n\
    <a id=\"user-content-portcullis\" class=\"anchor\" href=\"#portcullis\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Portcullis</h1>\n\
    <p><a href=\"https://github.com/maplesond/portcullis/releases\"><img src=\"https://camo.githubusercontent.com/b9c31b04d2671e6317cdfd9e4fdf893512936091302d1b1b56c99cb89ab43df7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7461672f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Version\" data-canonical-src=\"https://img.shields.io/github/tag/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://jenkins.sdlmapleson.net/job/portcullis/job/develop/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f696e3e0136cfb90f0e05f4f4e0a257ece7cd1e52ff19a0c8963b32df756d3a7/68747470733a2f2f6a656e6b696e732e73646c6d61706c65736f6e2e6e65742f6275696c645374617475732f69636f6e3f6a6f623d706f727463756c6c6973253246646576656c6f70\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://jenkins.sdlmapleson.net/buildStatus/icon?job=portcullis%2Fdevelop\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/ad4d6f3e16da4f0dddcd142fa3b6088042b13242787f5ad939d2db28282d3eb5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d627269676874677265656e2e737667\"\
    \ alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPL%20v3-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/maplesond/portcullis/issues\"\
    ><img src=\"https://camo.githubusercontent.com/d3bedf8e24750956939d66108f9ba197e72b83d1de8fc7305708ab2d67c20c17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues-raw/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Portcullis stands for PORTable CULLing\
    \ of Invalid Splice junctions from pre-aligned RNA-seq data.  It is known that\
    \ RNAseq mapping tools generate many invalid junction predictions, particularly\
    \ in deep datasets with high coverage over splice sites.  In order to address\
    \ this, instead for creating a new RNAseq mapper, with a focus on SJ accuracy\
    \ we created a tool that takes in a BAM file generated by an RNAseq mapper of\
    \ the user's own choice (e.g. Tophat2, Gsnap, STAR2 or HISAT2) as input (i.e.\
    \ it's portable).  It then, analyses and quantifies all splice junctions in the\
    \ file before, filtering (culling) those which are unlikely to be genuine.  Portcullis\
    \ output's junctions in a variety of formats making it suitable for downstream\
    \ analysis (such as differential splicing analysis and gene modelling) without\
    \ additional work.  Portcullis can also filter the original BAM file removing\
    \ alignments associated with <em>bad</em> junctions.  Both the filtered junctions\
    \ and BAM files are cleaner and more usable resources which can more effectively\
    \ be used to assist in downstream analyses such as gene prediction and genome\
    \ annotation.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h2>\n<p>We support multiple methods\
    \ for installing and running portcullis.  Hopefully your favourite container or\
    \ package manager is supported below.  If not let us know and we'll try to work\
    \ to get it integrated there.</p>\n<p><strong>Docker</strong></p>\n<p><a href=\"\
    https://hub.docker.com/r/maplesond/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/de966674ebe7a3dec2fed423683dd2c64e3630527fab6a691add53421292e384/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d61706c65736f6e642f706f727463756c6c69732e737667\"\
    \ alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/maplesond/portcullis.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code># Keep in mind you need to mount\
    \ in any working directories to the container with the `-v` option.\n# Ideally,\
    \ mount these into the /data directory which is the container's working directory.\n\
    docker run --it --rm -v /abspath/to/data/on/host:/data maplesond/portcullis:stable\
    \ portcullis --help\n</code></pre>\n<p><strong>Singularity</strong></p>\n<pre><code>#\
    \ First download the container:\nsingularity pull --name portcullis.img shub://maplesond/portcullis:master\n\
    \n# Then to execute commands in the container:\nsingularity exec portcullis.img\
    \ portcullis --help\n</code></pre>\n<p><strong>Conda</strong></p>\n<p><a href=\"\
    https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/381a7739b713a2bae02343a6ac934de39148a7866dbf4e52b597391b2a07fd4b/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f6c61746573745f72656c656173655f646174652e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/latest_release_date.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/77a7c650d2675de3588df907d8e8aec11957abc95bcfd87d3b1b07f78a2bc4ec/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f706c6174666f726d732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/platforms.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/bioconda/portcullis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/83781f462972e76ba4f2d046533fd48deb7cb72a0512481ff304f79c51bc01e3/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f646f776e6c6f6164732e737667\"\
    \ alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/downloads.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<pre><code>conda install portcullis --channel=bioconda\n\
    </code></pre>\n<p><strong>Brew</strong></p>\n<pre><code>brew install brewsci/bio/portcullis\n\
    </code></pre>\n<p><strong>From source</strong></p>\n<p><a href=\"https://github.com/maplesond/portcullis/releases\"\
    ><img src=\"https://camo.githubusercontent.com/3885a69f4777ec0c98cf3d0bee17eb7ca3d3eb69bbf850df2f36895b80168ade/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d61706c65736f6e642f706f727463756c6c69732f746f74616c2e737667\"\
    \ alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/github/downloads/maplesond/portcullis/total.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>If you wish to install from source please\
    \ first confirm that first you have these dependencies are installed and configured:</p>\n\
    <ul>\n<li>\n<strong>GCC</strong> V4.8+</li>\n<li>\n<strong>autoconf</strong> V2.53+</li>\n\
    <li>\n<strong>automake</strong> V1.11+</li>\n<li><strong>make</strong></li>\n\
    <li>\n<strong>libtool</strong> V2.4.2+</li>\n<li><strong>zlib-dev</strong></li>\n\
    <li><strong>pthreads</strong></li>\n<li>\n<strong>boost-dev</strong> V1.52+</li>\n\
    <li>\n<strong>samtools</strong> V1.2+</li>\n<li>\n<strong>Python3-dev</strong>\
    \ V3.5+ (Make sure the following packages are installed: <em>pandas</em>, <em>matplotlib</em>,\
    \ <em>setuptools</em>, <em>sphinx</em>, <em>tabulate</em>)</li>\n</ul>\n<p>Then\
    \ proceed with the following steps:</p>\n<pre><code># Clone the repo\ngit clone\
    \ git@github.com:maplesond/portcullis.git\n\n# Move into repo directory\ncd portcullis\n\
    \n# Generate configure script\n./autogen.sh\n\n# Confirm dependencies and generate\
    \ makefiles\n# Adding --prefix &lt;dir&gt; will tell make install to put everything\
    \ in a \n# particular directory.  Default is /usr/local.\n./configure\n\n# Compile\
    \ (increasing -j will make it go faster!\nmake -j 2\n\n# Run some unit tests (you\
    \ can increase -j here too)\nmake -j 2 check\n\n# Install to prefix dir\nmake\
    \ install\n</code></pre>\n<p><strong>Common problems</strong></p>\n<ul>\n<li>\n\
    <p>Many system python installations do not come with the C API immediately available,\
    \ which prevents Portcullis from embedding python code.  We typically would recommend\
    \ installing anaconda3 as this would include the latest version of python, all\
    \ required python packages as well as the C API.  If you are running a debian\
    \ system and the C libraries are not available by default and you wish to use\
    \ the system python installation the you can install them using: <code>sudo apt-get\
    \ install python-dev</code>.  Also, if you have installed python to a custom location\
    \ please verify that the <em>bin</em> directors on the <em>PATH</em> environment\
    \ variable, and the lib (or lib64) directory is on the <em>LD_LIBRARY_PATH</em>\
    \ or <em>LD_RUN_PATH</em> as appropriate.</p>\n</li>\n<li>\n<p>If Portcullis is\
    \ failing at the <code>./autogen.sh</code> step you will likely need to install\
    \ autotools.  The following command should do this on MacOS: <code>brew install\
    \ autoconf automake libtool</code>.  On a debian system this can be done with:\
    \ <code>sudo apt-get install autoconf automake libtool</code>.</p>\n</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quickstart</h2>\n<p>After portcullis has been installed, the <code>portcullis</code>\
    \ executable should be available.  Typing <code>portcullis</code> or <code>portcullis\
    \ --help</code> at the command line will present you with the portcullis help\
    \ message.</p>\n<p>These modes are available:</p>\n<ul>\n<li>\n<strong>prep</strong>\
    \    - Prepares input data so that it is suitable for junction analysis</li>\n\
    <li>\n<strong>junc</strong>    - Calculates junction metrics for the prepared\
    \ data</li>\n<li>\n<strong>filter</strong>  - Separates alignments based on whether\
    \ they are likely to represent genuine splice junctions or not</li>\n<li>\n<strong>bamfilt</strong>\
    \ - Filters a BAM to remove any reads associated with invalid junctions</li>\n\
    <li>\n<strong>full</strong>    - Runs prep, junc, filter and optionally bamfilt\
    \ as a complete pipeline</li>\n</ul>\n<p>Typing <code>portcullis &lt;mode&gt;\
    \ --help</code> will bring up help and usage information specific to that mode.</p>\n\
    <p>In addition to portcullis, we provide a tool-suite for manipulating junction\
    \ files called junctools.  Typing <code>junctools --help</code> will provide you\
    \ with the program options.</p>\n<p>For much more information about portcullis'\
    \ capabilities and how to configure and run it, an online version of the manual\
    \ can be found here: <a href=\"https://portcullis.readthedocs.org/en/latest/\"\
    \ rel=\"nofollow\">https://portcullis.readthedocs.org/en/latest/</a>.</p>\n<h2>\n\
    <a id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Licensing</h2>\n\
    <p>GNU GPL V3.  See COPYING file for more details.</p>\n<h2>\n<a id=\"user-content-authors\"\
    \ class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Authors</h2>\n<ul>\n<li>Daniel\
    \ Mapleson</li>\n<li>Luca Venturini</li>\n<li>David Swarbreck</li>\n</ul>\n<p>See\
    \ AUTHORS file for more details.</p>\n<h2>\n<a id=\"user-content-acknowledgements\"\
    \ class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Acknowledgements</h2>\n<p>Affiliation:\
    \ The Earlham Institute (EI)\nFunding: The Biotechnology and Biological Sciences\
    \ Research Council (BBSRC)</p>\n"
  stargazers_count: 23
  subscribers_count: 3
  topics:
  - portcullis
  - junction
  - splice-junctions
  - bam-files
  - filter
  updated_at: 1620067152.0
marcjwilliams1/rstudiosrvrV4:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: marcjwilliams1/rstudiosrvrV4
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <p><a href="https://singularity-hub.org/collections/4911" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for R studio server with Rv4.0.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605122458.0
markxiao/freesurfer:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: markxiao/freesurfer
  latest_release: null
  readme: '<h1>

    <a id="user-content-freesurfer" class="anchor" href="#freesurfer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>freesurfer</h1>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1618603672.0
markxiao/fsl:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: markxiao/fsl
  latest_release: null
  readme: '<h1>

    <a id="user-content-fsl" class="anchor" href="#fsl" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>fsl</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618603672.0
matmu/vep:
  data_format: 2
  description: Recipe for VEP + Cache
  filenames:
  - Singularityfiles/Singularity.99-GRCh38-merged
  - Singularityfiles/Singularity.99-GRCh37-merged
  full_name: matmu/vep
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-containerized-variant-effect-predictor-vep--cache\"\
    \ class=\"anchor\" href=\"#containerized-variant-effect-predictor-vep--cache\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Containerized Variant Effect Predictor (VEP) + Cache</h1>\n<p><a href=\"\
    https://twitter.com/intent/tweet?hashtags=Ensembl,VEP,Singularity,Docker&amp;url=https://github.com/matmu/vep\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/90bc908826728c0e4261acfff5619fd732c7be2b2a00624fce6363c9a3623c90/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c\"\
    \ alt=\"Twitter\" data-canonical-src=\"https://img.shields.io/twitter/url/http/shields.io.svg?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>\_+ <a href=\"#Introduction\">Introduction</a>\
    \ <br>\n\_+ <a href=\"#Building-image-with-Singularity\">Building image with Singularity</a>\
    \ <br>\n\_+ <a href=\"#Run-VEP\">Run VEP</a> <br>\n\_\_\_\_|-- <a href=\"#More-options\"\
    >More options</a> <br>\n\_\_\_\_|-- <a href=\"#Examples\">Examples</a> <br>\n\_\
    + <a href=\"#Post-processing\">Post-processing</a> <br>\n\_\_\_\_|-- <a href=\"\
    #Split-VEP\">Split VEP</a> <br>\n\_\_\_\_|-- <a href=\"#Filtering-by-VEP-annotations\"\
    >Filtering by VEP annotations</a> <br>\n\_+ <a href=\"#VEP-plugins\">VEP plugins</a>\
    \ <br>\n\_+ <a href=\"#Build-and-run-VEP-with-Docker\">Build &amp; run VEP with\
    \ Docker</a> <br>\n\_+ <a href=\"#Acknowledgments\">Acknowledgements</a></p>\n\
    <h2>\n<a id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Introduction</h2>\n<p>This documentation describes the usage of the\
    \ Docker image at <a href=\"https://hub.docker.com/r/matmu/vep\" rel=\"nofollow\"\
    >https://hub.docker.com/r/matmu/vep</a> which contains the bioinformatics tool\
    \ <strong>Ensembl Variant effect predictor (VEP)</strong> for annotating genetic\
    \ variants. The image comes with</p>\n<ul>\n<li>Merged cache including RefSeq\
    \ and Ensembl transcripts (VEP parameter --merged required)</li>\n<li>Reference\
    \ genome and index</li>\n<li>Plugins (annotation data is not included)</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-available-versions\" class=\"anchor\" href=\"\
    #available-versions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Available versions</h2>\n<p><strong>Human:</strong>\
    \ <a href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38-merged.yml\"\
    ><img src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38-merged.yml/badge.svg\"\
    \ alt=\"103-GRCh38-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38.yml\"\
    ><img src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38.yml/badge.svg\"\
    \ alt=\"103-GRCh38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/101-GRCh38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/101-GRCh38/badge.svg\"\
    \ alt=\"101-GRCh38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh38/badge.svg\"\
    \ alt=\"100-GRCh38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh38-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh38-merged/badge.svg\"\
    \ alt=\"100-GRCh38-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh37/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh37/badge.svg\"\
    \ alt=\"100-GRCh37\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCh37-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCh37-merged/badge.svg\"\
    \ alt=\"100-GRCh37-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/99-GRCh38-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/99-GRCh38-merged/badge.svg\"\
    \ alt=\"99-GRCh38-merged\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/99-GRCh37-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/99-GRCh37-merged/badge.svg\"\
    \ alt=\"99-GRCh37-merged\" style=\"max-width:100%;\"></a><br>\n<strong>Mouse:</strong>\
    \ <a href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCm39.yml\"\
    ><img src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCm39.yml/badge.svg\"\
    \ alt=\"103-GRCm39\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/101-GRCm38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/101-GRCm38/badge.svg\"\
    \ alt=\"101-GRCm38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCm38/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCm38/badge.svg\"\
    \ alt=\"100-GRCm38\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/matmu/vep/workflows/100-GRCm38-merged/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/matmu/vep/workflows/100-GRCm38-merged/badge.svg\"\
    \ alt=\"100-GRCm38-merged\" style=\"max-width:100%;\"></a></p>\n<p>The term <code>merged</code>\
    \ refers to the merged Ensembl/RefSeq cache. To be consistent with the Ensembl\
    \ website, chose Ensembl cache only (i.e. without the term <code>merged</code>).\
    \ Examples for available versions are <strong>99-GRCh38</strong> (VEP 99 with\
    \ Ensembl cache for reference GRCh38) or <strong>99-GRh37-merged</strong> (VEP\
    \ 99 with Ensembl/Refseq cache for reference GRCh37).</p>\n<p>You can also visit\
    \ <a href=\"https://hub.docker.com/r/matmu/vep/tags\" rel=\"nofollow\">https://hub.docker.com/r/matmu/vep/tags</a>\
    \ to get a list of available versions.</p>\n<p><strong>Note:</strong> If you require\
    \ a container for a species not mentioned above, feel free to contact us or even\
    \ better, create an issue.</p>\n<h2>\n<a id=\"user-content-build-image-with-singularity\"\
    \ class=\"anchor\" href=\"#build-image-with-singularity\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Build image\
    \ with Singularity</h2>\n<div class=\"highlight highlight-source-shell\"><pre>singularity\
    \ build vep.<span class=\"pl-k\">&lt;</span>version<span class=\"pl-k\">&gt;</span>.simg\
    \ docker://matmu/vep:<span class=\"pl-k\">&lt;</span>version<span class=\"pl-k\"\
    >&gt;</span></pre></div>\n<p><code>&lt;version&gt;</code> is a tag representing\
    \ the Ensembl version and the species + version of the reference genome.</p>\n\
    <h2>\n<a id=\"user-content-run-vep\" class=\"anchor\" href=\"#run-vep\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run\
    \ VEP</h2>\n<p>To run VEP execute</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.<span class=\"pl-k\">&lt;</span>version<span\
    \ class=\"pl-k\">&gt;</span>.simg vep [options]</pre></div>\n<p>whereby <code>&lt;version&gt;</code>\
    \ is replaced by a respective version (see above), e.g. <code>99-CRCh38</code>.\
    \ It is essential to add the VEP option <code>--merged</code> when using an image\
    \ with merged Ensembl/Refseq cache. For species except homo sapiens, also the\
    \ parameter <code>--species</code> (e.g. <code>--species mus_musculus</code>),\
    \ has to be set as well.</p>\n<h3>\n<a id=\"user-content-more-options\" class=\"\
    anchor\" href=\"#more-options\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>More options</h3>\n<p>The options\
    \ for base cache/plugin directories, species and assembly are set to the right\
    \ values by default and do not need to be set by the user.</p>\n<p>Visit <a href=\"\
    http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html\" rel=\"nofollow\"\
    >http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html</a> for detailed\
    \ information about all VEP options. Detailed information about <strong>input/output\
    \ formats</strong> can be found at <a href=\"https://www.ensembl.org/info/docs/tools/vep/vep_formats.html#defaultout\"\
    \ rel=\"nofollow\">https://www.ensembl.org/info/docs/tools/vep/vep_formats.html#defaultout</a>.</p>\n\
    <h3>\n<a id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Examples</h3>\n\
    <h4>\n<a id=\"user-content-minimum-output-format-compressed-tab-delimited\" class=\"\
    anchor\" href=\"#minimum-output-format-compressed-tab-delimited\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Minimum\
    \ (output format: compressed tab delimited)</h4>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.100-GRCh38-merged.simg\
    \ vep --dir /opt/vep/.vep --merged --offline --cache --input_file <span class=\"\
    pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz] --output_file\
    \ <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.txt.gz\
    \ --tab --compress_output bgzip</pre></div>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.100-GRCh38.simg vep --dir\
    \ /opt/vep/.vep --offline --cache --input_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.vcf[.gz] --output_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.txt.gz --tab --compress_output bgzip</pre></div>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity <span class=\"\
    pl-c1\">exec</span> vep.100-GRCm38.simg vep --dir /opt/vep/.vep --offline --cache\
    \ --input_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz]\
    \ --output_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.txt.gz\
    \ --tab --compress_output bgzip -species mus_musculus</pre></div>\n<h4>\n<a id=\"\
    user-content-minimum-output-format-compressed-vcf\" class=\"anchor\" href=\"#minimum-output-format-compressed-vcf\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Minimum (output format: compressed vcf)</h4>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ vep.100-GRCh38.simg vep --dir /opt/vep/.vep --offline --cache --input_file <span\
    \ class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz]\
    \ --output_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf.gz\
    \ --vcf --compress_output bgzip</pre></div>\n<h4>\n<a id=\"user-content-full-annotation\"\
    \ class=\"anchor\" href=\"#full-annotation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Full annotation</h4>\n<div class=\"\
    highlight highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ vep.100-GRCh38.simg vep --dir /opt/vep/.vep --offline --cache --input_file <span\
    \ class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf[.gz]\
    \ --output_file <span class=\"pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf.gz\
    \ --vcf --compress_output bgzip --everything --nearest symbol        </pre></div>\n\
    <h2>\n<a id=\"user-content-post-processing\" class=\"anchor\" href=\"#post-processing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Post-processing</h2>\n<h3>\n<a id=\"user-content-split-vep\" class=\"\
    anchor\" href=\"#split-vep\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Split VEP</h3>\n<p>There is a plugin for <code>bcftools</code>\
    \ that allows to split VEP annotations as well as sample information in a VCF\
    \ file and convert it to a text file: <a href=\"http://samtools.github.io/bcftools/howtos/plugin.split-vep.html\"\
    \ rel=\"nofollow\">http://samtools.github.io/bcftools/howtos/plugin.split-vep.html</a>.</p>\n\
    <h3>\n<a id=\"user-content-filtering-by-vep-annotations\" class=\"anchor\" href=\"\
    #filtering-by-vep-annotations\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Filtering by VEP annotations</h3>\n\
    <p>If you chose to output the VEP annotations as text file, any command line tool\
    \ (e.g. <code>awk</code>) or even <code>Excel</code> can be used for filtering\
    \ the results. For VCF files, the image includes a VEP filtering script which\
    \ can be executed by</p>\n<div class=\"highlight highlight-source-shell\"><pre>singularity\
    \ <span class=\"pl-c1\">exec</span> vep.<span class=\"pl-k\">&lt;</span>version<span\
    \ class=\"pl-k\">&gt;</span>.simg filter_vep [options]</pre></div>\n<h4>\n<a id=\"\
    user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Options</h4>\n\
    <p>Visit <a href=\"https://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html\"\
    \ rel=\"nofollow\">https://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html</a>\
    \ for detailed info about available options.</p>\n<h4>\n<a id=\"user-content-filtering-examples\"\
    \ class=\"anchor\" href=\"#filtering-examples\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Filtering examples</h4>\n<h5>\n\
    <a id=\"user-content-filter-for-rare-variants\" class=\"anchor\" href=\"#filter-for-rare-variants\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Filter for rare variants</h5>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity <span class=\"pl-c1\">exec</span> vep.<span class=\"pl-k\">&lt;</span>version<span\
    \ class=\"pl-k\">&gt;</span>.simg filter_vep --input_file <span class=\"pl-k\"\
    >&lt;</span>filename<span class=\"pl-k\">&gt;</span>.vcf --output_file <span class=\"\
    pl-k\">&lt;</span>filename<span class=\"pl-k\">&gt;</span>.filtered.vcf --only_matched\
    \ --filter <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>(IMPACT is HIGH\
    \ or IMPACT is MODERATE or IMPACT is LOW) and (BIOTYPE is protein_coding) and\
    \ ((PolyPhen &gt; 0.446) or (SIFT &lt; 0.05)) and (EUR_AF &lt; 0.001 or gnomAD_NFE_AF\
    \ &lt; 0.001 or (not EUR_AF and not gnomAD_NFE_AF))<span class=\"pl-pds\">\"</span></span>\
    \ </pre></div>\n<h2>\n<a id=\"user-content-vep-plugins\" class=\"anchor\" href=\"\
    #vep-plugins\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>VEP plugins</h2>\n<p>VEP allows several other annotations\
    \ sources (aka Plugins). Their respective Perl modules are included in the image,\
    \ the annotation files have to be added seperately, however. The list of plugins\
    \ as well as instructions on how to download and pre-process the annotation files\
    \ can be found at: <a href=\"http://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html\"\
    \ rel=\"nofollow\">http://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html</a>.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity <span class=\"\
    pl-c1\">exec</span> vep.100-GRCh38-merged.simg vep --dir /opt/vep/.vep --merged\
    \ --offline --cache --input_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.vcf[.gz] --output_file <span class=\"pl-k\">&lt;</span>filename<span\
    \ class=\"pl-k\">&gt;</span>.txt.gz --tab --compress_output bgzip --plugin CADD,/path/to/ALL.TOPMed_freeze5_hg38_dbSNP.tsv.gz</pre></div>\n\
    <h2>\n<a id=\"user-content-build-and-run-vep-with-docker\" class=\"anchor\" href=\"\
    #build-and-run-vep-with-docker\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Build and run VEP with Docker</h2>\n\
    <p>To pull the image and run the container with Docker use</p>\n<pre><code>docker\
    \ run matmu/vep:&lt;version&gt; vep [options]\n</code></pre>\n<p>Unlike Singularity,\
    \ the directories of <strong>Plugin</strong> annotation files (e.g. <code>/path/to/dir</code>)\
    \ have to be explicitely bound to a target directory (e.g. <code>/opt/data</code>)\
    \ within the container with option <code>-v</code>:</p>\n<pre><code>docker run\
    \ -v /path/to/dir:/opt/data matmu/vep:&lt;version&gt; vep [options]\n</code></pre>\n\
    <h2>\n<a id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Acknowledgments</h2>\n<p>This document has been created by <strong>Julia\
    \ Remes</strong> &amp; <strong>Matthias Munz</strong>, <strong>University of L\xFC\
    beck</strong>, <strong>Germany</strong>.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1614861605.0
mbhall88/Longitude_pipeline:
  data_format: 2
  description: Pipeline for analysing M. tuberculosis nanopore reads and getting drug
    susceptibility information.
  filenames:
  - containers/recipes/Singularity.nanoporeqc
  - containers/recipes/Singularity.mykrobe
  full_name: mbhall88/Longitude_pipeline
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>containers</h1>

    <p>recipes of Singularity</p>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics:
  - nanopore
  - tuberculosis
  - bioinformatics-pipeline
  updated_at: 1581419079.0
mbhall88/Singularity_recipes:
  data_format: 2
  description: Repository with all my Singularity recipes
  filenames:
  - Singularity.template
  - assembly/Singularity.haslr
  - assembly/Singularity.polish
  - assembly/Singularity.canu
  - assembly/Singularity.racon
  - nanopore/Singularity.puntseq
  - nanopore/Singularity.nanoporeqc
  - nanopore/Singularity.guppygpu
  - nanopore/Singularity.taiyaki
  - nanopore/Singularity.guppycpu
  - recipes/Singularity.centrifuge
  - recipes/Singularity.mccortex
  - recipes/Singularity.ngm
  - recipes/Singularity.pilon
  - recipes/Singularity.spades
  - recipes/Singularity.mummer
  - recipes/Singularity.bracken
  - recipes/Singularity.filtlong
  - recipes/Singularity.f5pack
  - recipes/Singularity.minimap2
  - recipes/Singularity.medaka
  - recipes/Singularity.mykrobe
  - recipes/Singularity.porechop
  - recipes/Singularity.vcftools
  - recipes/Singularity.kraken2
  - recipes/Singularity.nanopolish
  - recipes/Singularity.deepbinnergpu
  - recipes/Singularity.deepbinnercpu
  - recipes/Singularity.pandora
  - recipes/Singularity.clustalo
  - recipes/Singularity.samtools
  - recipes/Singularity.pistis
  - recipes/Singularity.bcftools
  - recipes/Singularity.krakenuniq
  full_name: mbhall88/Singularity_recipes
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipes" class="anchor" href="#singularity-recipes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Recipes</h1>

    <hr>

    <p><strong>NOTE</strong>: I am in the process of porting these recipes and images
    to

    singularity version 3 and <a href="https://cloud.sylabs.io/library" rel="nofollow">Singularity
    Library</a>. Any recipes that

    have been ported will have their images now hosted <a href="https://cloud.sylabs.io/library/mbhall88/default"
    rel="nofollow">here</a>.</p>

    <hr>

    <p><a href="https://singularity-hub.org/collections/685" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a><br>

    This repository is for all of my <a href="https://www.sylabs.io/singularity/"
    rel="nofollow">Singularity</a> recipes and is linked to Singularity

    Hub where they are built and available.</p>

    <p>You will find all the programs I have made Singularity recipes for inside the

    <a href="https://github.com/mbhall88/Singularity_recipes/tree/master/recipes"><code>recipes</code>
    directory</a>.</p>

    <p>I try to keep them up-to-date in terms of versions, but if you notice anything

    out of date, feel free to put in a pull request. Additionally, feel free to

    contribute recipes for programs I don''t already have in here, or request one
    and

    if I have time I will try and make one.</p>

    <h2>

    <a id="user-content-getting-a-pre-built-container" class="anchor" href="#getting-a-pre-built-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    a pre-built container</h2>

    <p>If you would like a pre-built version of any of the containers you can pull
    it

    down from <a href="https://www.singularity-hub.org/collections/685/usage" rel="nofollow">Singularity
    Hub</a>

    like so</p>

    <div class="highlight highlight-source-shell"><pre>tool=<span class="pl-s"><span
    class="pl-pds">"</span>samtools<span class="pl-pds">"</span></span>

    singularity pull --name <span class="pl-s"><span class="pl-pds">"</span><span
    class="pl-smi">$tool</span><span class="pl-pds">"</span></span>.simg shub://mbhall88/Singularity_recipes:<span
    class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$tool</span><span
    class="pl-pds">"</span></span></pre></div>

    <p>Note: The <code>--force</code> option will override any container that exists
    in that

    location with the same name. If you are trying to pull a container to update a

    pre-existing one, then the <code>--force</code> flag is necessary.</p>

    <p>For a full list of usage examples, check out the <a href="https://www.singularity-hub.org/collections/685/usage"
    rel="nofollow">Singularity Hub usage docs</a>.</p>

    <h2>

    <a id="user-content-template" class="anchor" href="#template" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Template</h2>

    <p>I have also included a template <a href="https://github.com/mbhall88/Singularity_recipes/blob/master/Singularity.template"><code>Singularity.template</code></a>

    which can be used for building up your own recipes.</p>

    '
  stargazers_count: 13
  subscribers_count: 1
  topics:
  - singularity-containers
  - singularity-hub
  - singularity
  - nanopore
  - bioinformatics
  updated_at: 1614220433.0
mbhall88/eipp-2019-singularity:
  data_format: 2
  description: Singularity group project for EIPP 2019
  filenames:
  - recipes/Singularity.shellcheck
  - recipes/Singularity.snakemake
  - recipes/Singularity.fun
  - recipes/Singularity.template
  - recipes/Singularity.jupyter
  - recipes/Singularity.nanopolish
  - recipes/Singularity.flye
  - recipes/sandbox-dev/Singularity.nanopolish
  full_name: mbhall88/eipp-2019-singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-eipp-2019-singularity-group-project\" class=\"\
    anchor\" href=\"#eipp-2019-singularity-group-project\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>EIPP 2019 Singularity\
    \ group project</h1>\n<p><a href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/4457c96be6834fd67756b9c0eab298334a5b948ab2234fbea89648e221e66af1/68747470733a2f2f73796c6162732e696f2f6775696465732f322e362f61646d696e2d67756964652f5f7374617469632f6c6f676f2e706e67\"\
    \ height=\"100\" data-canonical-src=\"https://sylabs.io/guides/2.6/admin-guide/_static/logo.png\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/0d44589c34845e74b1d32ae082d1f190828469fdc700fd026f3e4935eba669d2/68747470733a2f2f736369656e63652e736369656e63656d61672e6f72672f636f6e74656e742f7363692f3238372f353435372f313430312f46312e6d656469756d2e676966\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0d44589c34845e74b1d32ae082d1f190828469fdc700fd026f3e4935eba669d2/68747470733a2f2f736369656e63652e736369656e63656d61672e6f72672f636f6e74656e742f7363692f3238372f353435372f313430312f46312e6d656469756d2e676966\"\
    \ height=\"100\" data-canonical-src=\"https://science.sciencemag.org/content/sci/287/5457/1401/F1.medium.gif\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/3751\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<ul>\n<li>\n<a href=\"#introduction-to-containers\"\
    >Introduction to containers</a>\n<ul>\n<li><a href=\"#tldr\">tl;dr</a></li>\n\
    </ul>\n</li>\n<li><a href=\"#what-can-i-do-with-a-container\">What can I do with\
    \ a container?</a></li>\n<li>\n<a href=\"#how-do-i-get-a-container\">How do I\
    \ get a container?</a>\n<ul>\n<li>\n<a href=\"#remote\">Remote</a>\n<ul>\n<li><a\
    \ href=\"#docker-hub\">Docker Hub</a></li>\n<li><a href=\"#singularity-hub\">Singularity\
    \ Hub</a></li>\n<li><a href=\"#singularity-library\">Singularity Library</a></li>\n\
    <li><a href=\"#quay-and-biocontainers\">Quay and BioContainers</a></li>\n</ul>\n\
    </li>\n<li><a href=\"#build-locally\">Build locally</a></li>\n</ul>\n</li>\n<li>\n\
    <a href=\"#exercise-1\">Exercise 1</a>\n<ul>\n<li><a href=\"#task-1\">Task 1</a></li>\n\
    <li><a href=\"#task-2\">Task 2</a></li>\n<li><a href=\"#task-3\">Task 3</a></li>\n\
    </ul>\n</li>\n<li><a href=\"#sandbox-development\">Sandbox development</a></li>\n\
    <li><a href=\"#exercise-2\">Exercise 2</a></li>\n<li>\n<a href=\"#run-and-serving-applications\"\
    ><code>run</code> and serving applications</a>\n<ul>\n<li><a href=\"#singularity-run\"\
    ><code>singularity run</code></a></li>\n<li><a href=\"#serving-applications\"\
    >Serving applications</a></li>\n</ul>\n</li>\n<li><a href=\"#workflow-management-systems\"\
    >Workflow management systems</a></li>\n<li><a href=\"#programs-requiring-gpus\"\
    >Programs requiring GPUs</a></li>\n<li><a href=\"#bonus\">Bonus</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-introduction-to-containers\" class=\"anchor\" href=\"\
    #introduction-to-containers\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Introduction to containers</h2>\n\
    <h3>\n<a id=\"user-content-tldr\" class=\"anchor\" href=\"#tldr\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>tl;dr</h3>\n\
    <p>A container is a standard unit of software that packages up code and all its\n\
    dependencies, so the application runs quickly and reliably from one computing\n\
    environment to another. That includes files, environment variables, dependencies\
    \ and\nlibraries.</p>\n<p>For those who would like more detailed information about\
    \ what containers are, please\nrefer to <a href=\"https://github.com/titansmc/singularity-training-2019/raw/master/1.-singularity-training-what-are-containers.odp\"\
    >this fantastic slide deck from Josep Moscardo</a>.</p>\n<h2>\n<a id=\"user-content-what-can-i-do-with-a-container\"\
    \ class=\"anchor\" href=\"#what-can-i-do-with-a-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>What can\
    \ I do with a container?</h2>\n<p>In it's most basic form, you can execute a software\
    \ program, via a container, even\nthough you may not have that program installed\
    \ on the system you are running it on.</p>\n<p>Examples are the best teachers!</p>\n\
    <p>Firstly, let's clone this repository (and call it <code>eipp-singularity</code>)\
    \ as we will use some files from it throughout this\nproject.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>project=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>eipp-singularity<span class=\"pl-pds\">\"</span></span>\ngit\
    \ clone https://github.com/mbhall88/eipp-2019-singularity.git <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$project</span><span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-c1\">cd</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$project</span><span class=\"\
    pl-pds\">\"</span></span></pre></div>\n<p>Now, there is a <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\"\
    \ rel=\"nofollow\">BAM</a> file in the repository that we sadly can't view as\
    \ we do not have <a href=\"https://github.com/samtools/samtools\"><code>samtools</code></a>\
    \ installed (let's pretend). Thanks to Singularity we\ndon't have to worry about\
    \ trying to install <code>samtools</code> and can instead use a pre-built container\
    \ to view our BAM file with <code>samtools</code>.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>img=<span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>docker://quay.io/biocontainers/samtools:1.9--h10a08f8_12<span class=\"\
    pl-pds\">\"</span></span>\nsingularity <span class=\"pl-c1\">exec</span> <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$img</span><span\
    \ class=\"pl-pds\">\"</span></span> samtools view data/toy.bam</pre></div>\n<p>Magic\
    \ <g-emoji class=\"g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\
    >\u2728</g-emoji></p>\n<p>So what's going on here?</p>\n<p>Let's work our way\
    \ through the command.</p>\n<ol>\n<li>\n<a href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#executing-commands\"\
    \ rel=\"nofollow\"><code>singularity exec</code></a> tells Singularity to execute\
    \ a given command inside a\ngiven container.</li>\n<li>\n<code>\"$img\"</code>\
    \ specifies the container for Singularity to operate on. We will look at this\
    \ component in more detail later.</li>\n<li>\n<code>samtools view data/toy.bam</code>\
    \ This is the command we want Singularity to execute inside the container. Notice\
    \ how we can specify files that exist on our local file system?!</li>\n</ol>\n\
    <h2>\n<a id=\"user-content-how-do-i-get-a-container\" class=\"anchor\" href=\"\
    #how-do-i-get-a-container\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>How do I get a container?</h2>\n<h3>\n<a id=\"\
    user-content-remote\" class=\"anchor\" href=\"#remote\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Remote</h3>\n\
    <p>In the above example, the container we used for <code>samtools</code> was remote.</p>\n\
    <p>Remote containers are containers that have been pre-built and stored in \"\
    the cloud\".\nThere are many benefits to this kind of set up. Firstly, it makes\
    \ sharing containers\neasy. Secondly, it saves users (and yourself) a lot of time\
    \ in the future. As the\ncontainer is pre-built, we don't need to spend time waiting\
    \ for the build to happen (more on this later). The only wait time we have is\
    \ for the download of the remote\ncontainer to finish. Lastly, remote services\
    \ are convenient for building images if we\ndon't have <code>sudo</code> access\
    \ on the machine we are using. We will look at building containers\nlocally very\
    \ soon, but for now, it suffices to know that to build them locally, you need\n\
    <code>sudo</code> access.</p>\n<p>Now you might have noticed in the example above\
    \ that the <a href=\"https://en.wikipedia.org/wiki/Uniform_Resource_Identifier\"\
    \ rel=\"nofollow\">URI</a> for the <code>samtools</code>\ncontainer has the work\
    \ 'docker' in it. This is one of the coolest things about Singularity: <a href=\"\
    https://sylabs.io/guides/3.4/user-guide/singularity_and_docker.html\" rel=\"nofollow\"\
    >it can convert Docker containers into Singularity containers</a>! We now have\n\
    access to any Docker container <em>plus</em> any Singularity container.</p>\n\
    <p>Let's take a look at some remote container registries in a little more detail\
    \ and see\nhow we can use containers from them.</p>\n<h4>\n<a id=\"user-content-docker-hub\"\
    \ class=\"anchor\" href=\"#docker-hub\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"https://hub.docker.com/\"\
    \ rel=\"nofollow\">Docker Hub</a>\n</h4>\n<p>The official registry for Docker\
    \ containers. Let's search for <a href=\"http://conda.pydata.org/miniconda.html\"\
    \ rel=\"nofollow\"><code>miniconda3</code></a> on <a href=\"https://hub.docker.com/\"\
    \ rel=\"nofollow\">Docker Hub</a> and select the option <a href=\"https://hub.docker.com/r/continuumio/miniconda3\"\
    \ rel=\"nofollow\"><code>continuumio/miniconda3</code></a>. On the right, there\
    \ is a section <strong>Docker Pull Command</strong>. It\nsays <code>docker pull\
    \ continuumio/miniconda3</code>. If we were using Docker, this would be the\n\
    command we would use to pull that container to our local machine. To use it in\
    \ Singularity\nwe need to tweak it just a little. For <code>miniconda3</code>\
    \ we would use the URI <code>docker://continuumio/miniconda3</code>. As we can\
    \ see, you need to add <code>docker://</code> to the\nbeginning of the <code>repository/tag</code>.<br>\n\
    We can go one step further and unlock another great benefit of using remote containers.\
    \ We're reproducibility warriors, right?! Of course, we are. So let's be specific\n\
    about the version of <code>miniconda3</code> we want to use. On the <a href=\"\
    https://hub.docker.com/r/continuumio/miniconda3\" rel=\"nofollow\"><code>miniconda3</code>\
    \ Docker Hub page</a>, select the <a href=\"https://hub.docker.com/r/continuumio/miniconda3/tags\"\
    \ rel=\"nofollow\"><strong>Tags</strong></a> heading. On this\npage, we see a\
    \ whole bunch of different versions of <code>miniconda3</code> we can choose from.\
    \ Any\nversion of this container that has been built is kept. If we wanted to\
    \ use version <code>4.6.14</code>, then all we have to do is append this, with\
    \ a colon, to our original URI</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker://continuumio/miniconda3:4.6.14</pre></div>\n<p>Now, as we saw earlier,\
    \ we can directly execute a container from it's URI. However, it\nis likely you\
    \ may want to use a container multiple times. In these circumstances, it is\n\
    more \"economical\" to pull a copy of the container onto our local machine, so\
    \ we don't\nhave to try and retrieve it from the registry each time (images are\
    \ usually cached though). To pull the <code>miniconda3</code> container from Docker\
    \ Hub, we use Singularity's <a href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#download-pre-built-images\"\
    \ rel=\"nofollow\"><code>pull</code></a>\ncommand and optionally specify a name.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity pull docker://continuumio/miniconda3:4.6.14</pre></div>\n\
    <p>The above command will pull the container into the current directory and name\
    \ it <code>miniconda3-4.6.14.sif</code>. If we wanted to call it instead <code>miniconda3.sif</code>\
    \ we would use the <code>--name</code> argument</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull --name miniconda3.sif docker://continuumio/miniconda3:4.6.14</pre></div>\n\
    <p>When we want to use this image again in the future, rather than specifying\
    \ the URI we\njust point Singularity at our local copy</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ miniconda3.sif <span class=\"pl-k\">&lt;</span>command<span class=\"pl-k\">&gt;</span></pre></div>\n\
    <h4>\n<a id=\"user-content-singularity-hub\" class=\"anchor\" href=\"#singularity-hub\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity\
    \ Hub</a>\n</h4>\n<p>Set up and maintained by a collaboration between Stanford\
    \ University and Singularity,\nSingularity Hub is Singularity's \"semi-official\"\
    \ version of Docker Hub. We will dig\ninto how to set this up for yourself a little\
    \ later in <a href=\"#Exercise-1\">Exercise 1</a>.</p>\n<p>As with Docker Hub,\
    \ we can search for containers uploaded by users and then use them in\nthe same\
    \ way. However, it will ask us to log in using GitHub first. Login with your\n\
    GitHub account and then search for <a href=\"https://github.com/DaehwanKimLab/centrifuge\"\
    ><code>centrifuge</code></a>. The first result should\nbe for <a href=\"https://singularity-hub.org/collections/685\"\
    \ rel=\"nofollow\"><code>mbhall88/Singularity_recipes</code></a> - click on this.\
    \ This will take\nyou to a page listing all of the Singularity containers I maintain\
    \ in a <a href=\"https://github.com/mbhall88/Singularity_recipes\">recipes repository\
    \ on GitHub</a>. Scroll through these and look for the\n<a href=\"https://singularity-hub.org/containers/5461\"\
    \ rel=\"nofollow\"><code>centrifuge</code></a> one and then click on the green\
    \ <strong>Complete</strong> button.\nThe resulting screen will have the Build\
    \ Specs (more on this soon) plus a bunch of\nbuild metrics. Additionally, at the\
    \ top of this screen, you will see the core piece of\nthe URI that we need: <code>mbhall88/Singularity_recipes:centrifuge</code>.\
    \ So to use this container,\nwe add the <code>shub://</code> scheme to the front.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>uri=<span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>shub://mbhall88/Singularity_recipes:centrifuge<span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity pull --name centrifuge.sif <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$uri</span><span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity <span class=\"pl-c1\">exec</span>\
    \ centrifuge.sif centrifuge --help</pre></div>\n<p>Due to Singularity Hub be generously\
    \ hosted as no charge by Google Cloud, and also due\nto a recent malicious attack,\
    \ it is <a href=\"https://singularityhub.github.io/singularityhub-docs/docs/interact\"\
    \ rel=\"nofollow\">recommended</a> to <code>pull</code> containers from Singularity\
    \ and\nthen execute them, rather than running directly from the URI.</p>\n<p>Again,\
    \ we can go one step further and specify a particular build of the container we\n\
    want to use. In the <strong>Build Metrics</strong> section, there is a field called\
    \ 'Version (file hash)'. For reproducibility purposes, it is advisable to use\
    \ this hash as it makes it\nclear to others who may read your code exactly which\
    \ container you used. So to pull the\nlatest centrifuge container, we would do\
    \ the following (<strong>don't run this if you already\npulled the container above</strong>).</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>hash=<span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>13bc12f41b20001f17e6f8811dc3eeea<span class=\"\
    pl-pds\">\"</span></span>\nuri=<span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>shub://mbhall88/Singularity_recipes:centrifuge@<span class=\"pl-smi\">${hash}</span><span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity pull --name centrifuge.sif <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$uri</span><span\
    \ class=\"pl-pds\">\"</span></span>\nsingularity <span class=\"pl-c1\">exec</span>\
    \ centrifuge.sif centrifuge --help</pre></div>\n<h4>\n<a id=\"user-content-singularity-library\"\
    \ class=\"anchor\" href=\"#singularity-library\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><a href=\"https://cloud.sylabs.io/library\"\
    \ rel=\"nofollow\">Singularity Library</a>\n</h4>\n<p>This is the official container\
    \ registry for Singularity. However, all images built on\nthis service are Singularity\
    \ v3+ compatible. At EBI we only have Singularity v2.6, but\nEMBL Heidelberg's\
    \ cluster does use Singularity v3+. This service works similarly to Singularity\
    \ and Docker Hubs, using the scheme <code>library://</code> for its URIs.</p>\n\
    <p>One additional feature that Singularity Library has is a <a href=\"https://cloud.sylabs.io/builder\"\
    \ rel=\"nofollow\">remote builder</a>. This builder allows you to dump a recipe\
    \ for a container, it will build the\ncontainer for you, and then you can download\
    \ it on to your local machine. Very handy\nwhen working on a computer you do not\
    \ have <code>sudo</code> access on.</p>\n<p>See the slides <em>below</em> <a href=\"\
    https://slides.com/mbhall88/remote-container-systems#/2/1\" rel=\"nofollow\">this</a>\
    \ for more information about Singularity\nLibrary.</p>\n<h4>\n<a id=\"user-content-quay-and-biocontainers\"\
    \ class=\"anchor\" href=\"#quay-and-biocontainers\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://quay.io/\"\
    \ rel=\"nofollow\">Quay</a> and <a href=\"https://biocontainers.pro/\" rel=\"\
    nofollow\">BioContainers</a>\n</h4>\n<p>Quay is a container registry for Docker\
    \ and <a href=\"https://coreos.com/rkt/\" rel=\"nofollow\">rkt</a> containers.\
    \ We won't talk much\nabout this service outside how to use the BioContainers\
    \ builds hosted on it.</p>\n<p>BioContainers is an open-source and community-driven\
    \ framework for reproducibility in\nbioinformatics<a href=\"https://doi.org/10.1093/bioinformatics/btx192\"\
    \ rel=\"nofollow\"><sup>1</sup></a>. They build and maintain containers for a\
    \ large suite of bioinformatics\ntools. In particular, any tool that has a <a\
    \ href=\"https://bioconda.github.io/\" rel=\"nofollow\">Bioconda</a> recipe automatically\
    \ has\na BioContainers image built and stored on Quay.</p>\n<p>To see an example\
    \ of how to find and use these BioContainers images check out the slides\nbelow\
    \ <a href=\"https://slides.com/mbhall88/remote-container-systems#/4/1i\" rel=\"\
    nofollow\">here</a>.</p>\n<hr>\n<p>For more details on remote container systems,\
    \ refer to <a href=\"https://slides.com/mbhall88/remote-container-systems\" rel=\"\
    nofollow\">my slides</a> from a one-day\n<a href=\"https://git.embl.de/grp-bio-it/singularity-training-2019\"\
    \ rel=\"nofollow\">Singularity course</a> I was involved in running at EMBL in\
    \ early 2019.</p>\n<h3>\n<a id=\"user-content-build-locally\" class=\"anchor\"\
    \ href=\"#build-locally\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Build locally</h3>\n<p>We've talked a lot about\
    \ how to use containers that others have been kind enough to\nconstruct for us.\
    \ But what happens if an image doesn't exist for the software tool you\nwant to\
    \ use? Or if you want to combine multiple programs into a single container? You\n\
    guessed it; we can build containers locally from definition/recipe files.</p>\n\
    <p>Rather than reinvent the wheel, please refer to (and work your way through)\
    \ <a href=\"https://slides.com/mbhall88/making-containers#/\" rel=\"nofollow\"\
    >these slides</a> from the <a href=\"https://git.embl.de/grp-bio-it/singularity-training-2019\"\
    \ rel=\"nofollow\">Singularity course</a> I was involved in running at EMBL in\
    \ early 2019. Once you get to slide titled <a href=\"https://slides.com/mbhall88/making-containers#/2/4\"\
    \ rel=\"nofollow\">\"Playing in a sandbox with a shell\"</a> you can move on to\
    \ <a href=\"#Exercise-1\">Exercise 1</a>.</p>\n<p><strong>Note:</strong> As the\
    \ course was aimed at users of Singularity v3+ you will see the container\nextension\
    \ <code>.sif</code> used. This was a new container file format introduced in v3\
    \ that is\nnot usable with v2. The container extension for v2 was <code>.simg</code>,\
    \ so you may see this sometimes.\nFor instance, the cluster at EBI is still on\
    \ v2 (the training VMs are v3). For those using\nthe Heidelberg cluster, your\
    \ cluster has v3. Singularity v2 containers, with the <code>.simg</code> extension,\n\
    can be executed by Singularity v3. You will also find all of the recipe\nfiles\
    \ in that presentation in the <a href=\"https://github.com/mbhall88/eipp-2019-singularity/tree/master/recipes\"\
    ><code>recipes/</code></a> directory of this repository.</p>\n<h2>\n<a id=\"user-content-exercise-1\"\
    \ class=\"anchor\" href=\"#exercise-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Exercise 1</h2>\n<p>Form two\
    \ groups and complete the following tasks.</p>\n<h3>\n<a id=\"user-content-task-1\"\
    \ class=\"anchor\" href=\"#task-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Task 1</h3>\n<p><a href=\"https://help.github.com/en/github/getting-started-with-github/fork-a-repo\"\
    >Fork</a> this repository on GitHub.</p>\n<h3>\n<a id=\"user-content-task-2\"\
    \ class=\"anchor\" href=\"#task-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Task 2</h3>\n<p><a href=\"https://slides.com/mbhall88/remote-container-systems#/1/6\"\
    \ rel=\"nofollow\">Enable Singularity Hub</a> on your fork of this repository.</p>\n\
    <h3>\n<a id=\"user-content-task-3\" class=\"anchor\" href=\"#task-3\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Task\
    \ 3</h3>\n<p>Each group should choose one of the following two GitHub issues to\
    \ close:</p>\n<ul>\n<li>\n<code>snakemake</code> recipe: <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/1\"\
    ><img src=\"https://camo.githubusercontent.com/26d3e148ca179ea5b34cb0255936905ed487432faa4027a512640b8f92a68ea7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f31\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/1\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>\n<code>shellcheck</code> recipe:\
    \ <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/2\"><img\
    \ src=\"https://camo.githubusercontent.com/a43a776c8cd5a471e5293ecd213c14f9452745fe9c75b850bd1986cf79d0d70a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f32\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/2\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-sandbox-development\"\
    \ class=\"anchor\" href=\"#sandbox-development\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sandbox development</h2>\n<p>During\
    \ the previous exercise, you may have noticed that errors in your build recipe\
    \ require you to rerun the build all over again. When installing simple programs,\
    \ this isn't too costly. However, when we want to build more complicated containers,\
    \ it becomes time-consuming to rerun the entire build continually. In this section,\
    \ we will look at how we can use Singularity's <a href=\"https://sylabs.io/guides/3.4/user-guide/build_a_container.html#creating-writable-images-and-sandbox-directories\"\
    \ rel=\"nofollow\"><code>--sandbox</code></a> option to speed up the container\
    \ recipe development cycle.</p>\n<p>So what is a sandbox? Think of it as a directory\
    \ that mimics the inside of a container. You can then start an interactive shell\
    \ session in this sandbox and run commands in the same environment that they will\
    \ run in when building the container. In this way, you can test out what commands\
    \ you need to run to get your program(s) installed and executing correctly. This\
    \ massively reduces your turnaround time for creating containers. In addition,\
    \ as we make the sandbox writeable, any changes we make will stay saved.</p>\n\
    <p>Let's get into the sandbox and play!</p>\n<p>Create a new directory where we\
    \ will do our sandbox development.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>mkdir sandbox-dev\n<span class=\"pl-c1\">cd</span> sandbox-dev</pre></div>\n\
    <p>Next, we will use the <a href=\"https://github.com/mbhall88/eipp-2019-singularity/blob/master/recipes/Singularity.template\"\
    >template recipe</a> in this repository to build our sandbox from.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sudo singularity build --sandbox playground\
    \ ../recipes/Singularity.template</pre></div>\n<p>You should now see a directory\
    \ called <code>playground</code>. I've named the sandbox <code>playground</code>,\
    \ but you can name it whatever you want.</p>\n<p>Now we will start an interactive\
    \ shell within the sandbox/container image.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity shell --writable playground</pre></div>\n<p><em>Note: If\
    \ you don't use <a href=\"https://sylabs.io/guides/3.4/user-guide/build_a_container.html#writable\"\
    \ rel=\"nofollow\"><code>--writable</code></a> you won't be able to install anything\
    \ or do anything that changes the size of the container.</em></p>\n<p>You should\
    \ now see the prompt change to something like</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>Singularity playground:<span class=\"pl-k\">~</span><span class=\"pl-k\"\
    >&gt;</span></pre></div>\n<p><strong>IMPORTANT:<br>\nThe directory <code>/root</code>\
    \ from your local machine will be mounted in the sandbox. So anything you do in\
    \ the sandbox in that directory will also be reflected in the <code>/root</code>\
    \ directory locally.\nEnsure you move out of <code>/root</code> within the sandbox\
    \ and do all of your work there. I tend to use <code>/usr/local</code>, but you\
    \ could create a new directory altogether (but outside <code>/root</code>) e.g.\
    \ <code>/sandbox</code>.</strong></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> /usr/local</pre></div>\n<p>Now we'll try\
    \ and <a href=\"https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent\"\
    \ rel=\"nofollow\">install <code>conda</code></a> inside the sandbox.</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\
    \ -O miniconda3.sh</pre></div>\n<p>This will give us: <code>bash: wget: command\
    \ not found</code>. A perfect example of why these sandboxes\nare so useful. The\
    \ OS installation is <em>very</em> minimal and doesn't include a lot of programs.</p>\n\
    <p>Let's install <code>wget</code> in our sandbox and try again.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>apt install -y wget\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\
    \ -O miniconda3.sh</pre></div>\n<p>Now we'll install <code>conda</code>, specifying\
    \ the prefix (<code>-p</code>) as a directory in <code>/usr/local</code>\ncalled\
    \ <code>miniconda</code>.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>bash miniconda3.sh -b -p <span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pwd<span\
    \ class=\"pl-pds\">)</span></span>/miniconda</pre></div>\n<p>In order to run <code>conda</code>\
    \ now, we need to ensure it's binary is in our <code>PATH</code>.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> PATH=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-s\"><span class=\"\
    pl-pds\">$(</span>realpath miniconda/bin<span class=\"pl-pds\">)</span></span>:<span\
    \ class=\"pl-smi\">${PATH}</span><span class=\"pl-pds\">\"</span></span></pre></div>\n\
    <p><em>Remember from the <a href=\"https://slides.com/mbhall88/making-containers#/1/7\"\
    \ rel=\"nofollow\"><code>%environment</code> slide</a> that when writing the recipe\
    \ for\nthis <code>conda</code> installation we would need to write the <code>export</code>\
    \ line as:</em></p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>export\
    \ PATH=<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>realpath miniconda/bin<span\
    \ class=\"pl-pds\">)</span></span>:<span class=\"pl-smi\">${PATH}</span><span\
    \ class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">&gt;&gt;</span> <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$SINGULARITY_ENVIRONMENT</span><span\
    \ class=\"pl-pds\">\"</span></span></pre></div>\n<p>Lastly, we need to test <code>conda</code>\
    \ is executable.</p>\n<div class=\"highlight highlight-source-shell\"><pre>conda\
    \ list</pre></div>\n<p>In order to convert these commands into a recipe I generally\
    \ keep a text file open where\nI paste (successful) commands into as I go so I\
    \ don't have to search back through my\nshell history later.</p>\n<h2>\n<a id=\"\
    user-content-exercise-2\" class=\"anchor\" href=\"#exercise-2\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Exercise\
    \ 2</h2>\n<p>Similar to <a href=\"#exercise-1\">Exercise 1</a>, form two groups\
    \ (can be different groups) and put\nin a pull request each to close the following\
    \ two issues:</p>\n<ul>\n<li>\n<code>flye</code> recipe: <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/3\"\
    ><img src=\"https://camo.githubusercontent.com/5bdb30d6ea7dece3a9a4cfc16de03ce988f6197b0363cb987ad5506c879a57eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f33\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/3\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>\n<code>nanopolish</code> recipe:\
    \ <a href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/4\"><img\
    \ src=\"https://camo.githubusercontent.com/b1fb67d7045f5f2f26d02ed8c2d5b5423da330dfc0b19efa70dbfd53ca698f5f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f34\"\
    \ alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/4\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n</ul>\n<p>I chose more complicated programs\
    \ this time so you can get some experience using a sandbox.</p>\n<h2>\n<a id=\"\
    user-content-run-and-serving-applications\" class=\"anchor\" href=\"#run-and-serving-applications\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><code>run</code> and serving applications</h2>\n<h3>\n<a id=\"user-content-singularity-run\"\
    \ class=\"anchor\" href=\"#singularity-run\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><code>singularity run</code>\n\
    </h3>\n<p>The <a href=\"https://sylabs.io/guides/3.4/user-guide/cli/singularity_run.html\"\
    \ rel=\"nofollow\"><code>run</code></a> directive will execute the <a href=\"\
    https://slides.com/mbhall88/making-containers#/1/10\" rel=\"nofollow\"><code>%runscript</code></a>\
    \ and\npass along all arguments to this script. The <code>run</code> directive\
    \ is handy for when you want\nto automate some common tasks using the programs\
    \ installed within the container and be\nable to handle user options. Refer to\
    \ <a href=\"https://slides.com/mbhall88/making-containers#/1/10\" rel=\"nofollow\"\
    >the slide on <code>%runscript</code></a>,\nfrom the earlier section on <a href=\"\
    #build-locally\">buiding containers locally</a>, for\nan example of using <code>singularity\
    \ run</code>.</p>\n<h3>\n<a id=\"user-content-serving-applications\" class=\"\
    anchor\" href=\"#serving-applications\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Serving applications</h3>\n<p>It\
    \ is also possible to serve applications through a port from a container. As an\
    \ example\nwe will build a container to run a <a href=\"https://jupyter.org/\"\
    \ rel=\"nofollow\"><code>jupyter notebook</code></a> that we can access on\nour\
    \ local machine.</p>\n<p>The recipe to do this can be found in the <code>recipe/</code>\
    \ directory as <a href=\"https://github.com/mbhall88/eipp-2019-singularity/blob/master/recipes/Singularity.jupyter\"\
    ><code>Singularity.jupyter</code></a>.\nOf particular interest for this example,\
    \ see the <code>%runscript</code> section.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>%runscript\n    PORT=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span\
    \ class=\"pl-smi\">${1<span class=\"pl-k\">:-</span>8888}</span><span class=\"\
    pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">echo</span> <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>Starting notebook...<span class=\"pl-pds\"\
    >\"</span></span>\n    <span class=\"pl-c1\">echo</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>Open browser to localhost:<span class=\"pl-smi\"\
    >${PORT}</span><span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\"\
    >exec</span> /usr/local/bin/jupyter notebook  --ip=<span class=\"pl-s\"><span\
    \ class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span> --port=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$PORT</span><span\
    \ class=\"pl-pds\">\"</span></span> --no-browser</pre></div>\n<p>We take the first\
    \ option passed by the user and store it in a variable <code>PORT</code>, or use\
    \ <code>8888</code>\nif nothing is given. We print some logging to the screen\
    \ with <code>echo</code> and then start\na <code>jupyter</code> session, passing\
    \ the <code>PORT</code> to <code>jupyter</code>.</p>\n<p>Let's build this image\
    \ and then fire it up.</p>\n<div class=\"highlight highlight-source-shell\"><pre>sudo\
    \ singularity build jupyter.sif recipes/Singularity.jupyter\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> we will use the default port 8888</span>\nsingularity\
    \ run jupyter.sif  </pre></div>\n<p>You should get some output from <code>jupyter</code>\
    \ indicating it has started running the notebook\nand providing a location, which\
    \ should look something like:</p>\n<pre><code>[I 11:40:28.948 NotebookApp] Serving\
    \ notebooks from local directory: /home/vagrant/container-dev\n[I 11:40:28.949\
    \ NotebookApp] The Jupyter Notebook is running at:\n[I 11:40:28.949 NotebookApp]\
    \ http://dev-vm:8888/?token=c8fe88de778120e5ccd42850d6d13712e27b125b0481d5b0\n\
    [I 11:40:28.949 NotebookApp]  or http://127.0.0.1:8888/?token=c8fe88de778120e5ccd42850d6d13712e27b125b0481d5b0\n\
    [I 11:40:28.949 NotebookApp] Use Control-C to stop this server and shut down all\
    \ kernels (twice to skip confirmation).\n[C 11:40:28.953 NotebookApp]\n</code></pre>\n\
    <p>Copy the URL (either one), and paste it into a web browser. You should now\
    \ see the home\npage for the notebook. Select the example notebook at <code>notebooks/plot.ipynb</code>.</p>\n\
    <p>Run the two cells in the notebook, and you should see some toy data plotted.</p>\n\
    <p>This is quite a simple use case for serving applications. You can do far more\
    \ complicated\nthings like <a href=\"https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/\"\
    \ rel=\"nofollow\">running an RStudio server</a> from a container and access it\
    \ locally.</p>\n<h2>\n<a id=\"user-content-workflow-management-systems\" class=\"\
    anchor\" href=\"#workflow-management-systems\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Workflow management systems</h2>\n\
    <p>Containers and workflow management systems (WMSs), such as <code>snakemake</code>\
    \ and <a href=\"https://www.nextflow.io/\" rel=\"nofollow\"><code>nextflow</code></a>,\n\
    are a match made in heaven. Containers add a crucial layer of reproducibility\
    \ to these systems.</p>\n<p>Though this is not a project to teach you how to use\
    \ WMSs, I would\nencourage you to take a look at <a href=\"https://slides.com/mbhall88/singularity-and-workflow-management-systems#/\"\
    \ rel=\"nofollow\">this short slide deck</a> from the Singularity course I ran\n\
    as it shows you how easy it is to integrate Singularity containers into WMSs.</p>\n\
    <h2>\n<a id=\"user-content-programs-requiring-gpus\" class=\"anchor\" href=\"\
    #programs-requiring-gpus\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Programs requiring GPUs</h2>\n<p>Singularity\
    \ also provides the ability to utilise GPU cards, without needing to install\n\
    the GPU drivers into your container. Currently, it can only use NVIDIA GPUs. To\
    \ allow a\ncontainer to use the local GPU card and drivers all you need to do\
    \ it pass the\n<a href=\"https://sylabs.io/guides/2.6/user-guide/appendix.html#a-gpu-example\"\
    \ rel=\"nofollow\"><code>--nv</code></a> option. For example, to get a python\
    \ shell with the GPU version of <code>tensorflow</code>\navailable, you would\
    \ run the following (on a machine with an NVIDIA GPU).</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>singularity <span class=\"pl-c1\">exec</span>\
    \ --nv docker://tensorflow/tensorflow:latest-gpu python</pre></div>\n<h2>\n<a\
    \ id=\"user-content-bonus\" class=\"anchor\" href=\"#bonus\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Bonus</h2>\n\
    <p>If you have gotten to this point, then have a go at creating a container for\
    \ a piece of\nsoftware you have had difficulties installing in the past. Alternatively,\
    \ you could try\nand reduce the size of the containers we have already produced\
    \ by using <a href=\"https://www.alpinelinux.org/\" rel=\"nofollow\">Alpine</a>\
    \ as the\nbase OS.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics:
  - singularity
  - containers
  - bioinformatics
  - phd
  - embl-ebi
  - embl
  updated_at: 1573137725.0
mbhall88/head_to_head_pipeline:
  data_format: 2
  description: Snakemake pipeline to run analysis for the Illumina vs. Nanopore comparison.
  filenames:
  - analysis/assembly/containers/Singularity.canu
  full_name: mbhall88/head_to_head_pipeline
  latest_release: null
  readme: '<p>This repository holds the pipelines/scripts used for our project analysing
    Illumina

    and Nanopore for Mtb drug resistance calling and epidemiological clustering.</p>

    <p>It is currently in progress.</p>

    <p>See subdirectories for more specific information about different pipelines.</p>

    <ul>

    <li><a href="analysis/assembly">Assembly</a></li>

    <li><a href="https://github.com/mbhall88/tubby">Basecall training</a></li>

    <li><a href="data/H37Rv_PRG">H37Rv PRG construction</a></li>

    <li><a href="data/QC">Quality Control</a></li>

    <li><a href="analysis/baseline_variants">Baseline variant analysis</a></li>

    <li><a href="analysis/pandora_variants">Pandora variant analysis</a></li>

    <li><a href="analysis/transmission_clustering">Transmission clustering</a></li>

    <li><a href="analysis/resistance_prediction">Drug Resistance Prediction</a></li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1621647335.0
mbhall88/pandora_analysis_pipeline:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity.make_prg_dependencies
  - containers/Singularity.subsample
  full_name: mbhall88/pandora_analysis_pipeline
  latest_release: null
  readme: "<p><a href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\"\
    \ alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HEMnet - Haematoxylin &amp; Eosin and Molecular neural network</h1>\n\
    <h2>\n<a id=\"user-content-description\" class=\"anchor\" href=\"#description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Description</h2>\n<p>A deep learning automated cancer diagnosis software\
    \ using molecular labelling to improve pathological annotation of\nHaematoxylin\
    \ and Eosin (H&amp;E) stained tissue.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<ol>\n<li>\n\
    <p>Docker</p>\n<p>You can download and run the docker image using the following\
    \ commands:</p>\n<pre><code>docker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\
    </code></pre>\n</li>\n<li>\n<p>Conda</p>\n<p>Install Openslide (this is necessary\
    \ to open whole slide images) - download it <a href=\"https://openslide.org/download/\"\
    \ rel=\"nofollow\">here</a></p>\n<p>Create a conda environment from the <code>environment.yml</code>\
    \ file</p>\n<pre><code>conda env create -f environment.yml\nconda activate HEMnet\n\
    </code></pre>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-slide-preparation\"\
    \ class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Slide Preparation</h3>\n<p>Name\
    \ slides in the format: <code>slide_id_TP53</code> for TP53 slides and <code>slide_id_HandE</code>\
    \ for H&amp;E slides\nThe <code>TP53</code> and <code>HandE</code> suffix is used\
    \ by HEMnet to identify the stain used.</p>\n<h3>\n<a id=\"user-content-1-generate-training-and-testing-datasets\"\
    \ class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.\
    \ Generate training and testing datasets</h3>\n<p>a. Generate train dataset</p>\n\
    <p><code>python HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides\
    \ -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs\
    \ -v</code></p>\n<p>b. Generate test dataset</p>\n<p><code>python HEMnet_test_dataset.py\
    \ -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory\
    \ -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh\
    \ -n non_cancer_thresh</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is the relative path to the template slide from which all other slides will\
    \ be normalised against. The template\nslide should be the same for each step.</li>\n\
    <li>\n<code>-m</code> is the tile magnification. e.g. if  the input is <code>10</code>\
    \ then the tiles will be output at 10x</li>\n<li>\n<code>-a</code> is the align\
    \ magnification. Paired TP53 and H&amp;E slides will be registered at this magnification.\n\
    To reduce computation time we recommend this be less than the tile magnification\
    \ - a five times downscale generally works well.</li>\n<li>\n<code>-c</code> cancer\
    \ threshold to apply to the DAB channel. DAB intensities less than this threshold\
    \ indicate cancer.</li>\n<li>\n<code>-n</code> non-cancer threshold to apply to\
    \ the DAB channel. DAB intensities greater than this threshold indicate no cancer.</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\"\
    \ href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2. Train and evaluate model</h3>\n\
    <p>a. Training model</p>\n<p><code>python train.py -b /path/to/base/directory\
    \ -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory\
    \ -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size\
    \ -s -w -f -v</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-m</code>\
    \ is CNN base model. eg. <code>resnet50</code>, <code>vgg16</code>, <code>vgg19</code>,\
    \ <code>inception_v3</code> and <code>xception</code>.</li>\n<li>\n<code>-g</code>\
    \ is number of GPUs for training.</li>\n<li>\n<code>-e</code> is training epochs.\
    \ Default is <code>100</code> epochs.</li>\n<li>\n<code>-a</code> is batch size.\
    \ Default is <code>32</code>\n</li>\n<li>\n<code>-s</code> is option to save the\
    \ trained model weights.</li>\n<li>\n<code>-w</code> is option to used transfer\
    \ learning. Model will used pre-trained weights from ImageNet at the initial stage.</li>\n\
    <li>\n<code>-f</code> is fine-tuning option. Model will re-train CNN base.</li>\n\
    </ul>\n<p>b. Test model prediction</p>\n<p><code>python test.py  -b /path/to/base/directory\
    \ -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory\
    \ -w model_weights -m cnn_base -g num_gpus -v</code></p>\n<p>Other parameters:</p>\n\
    <ul>\n<li>\n<code>-w</code> is path to trained model. eg. <code>trained_model.h5</code>.</li>\n\
    <li>\n<code>-m</code> is CNN base model (same to training step).</li>\n<li>\n\
    <code>-g</code> is number of GPUs for prediction.</li>\n</ul>\n<p>c. Evaluate\
    \ model performance and visualise model prediction</p>\n<p><code>python visualisation.py\
    \ -b /path/to/base/directory -t /relative/path/to/training_output_directory -p\
    \ /relative/path/to/test_output_directory  -o /relative/path/to/output/directory\
    \ -i sample</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is path to training outputs.</li>\n<li>\n<code>-p</code> is path to test outputs.</li>\n\
    <li>\n<code>-i</code> is name of Whole Slide Image for visualisation.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\"\
    \ href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3. Apply model to diagnose new\
    \ images</h3>\n<p><code>python HEMnet_inference.py -s '/path/to/new/HE/Slides/'\
    \ -o '/path/to/output/directory/' -t '/path/to/template/slide/' -nn '/path/to/trained/model/'\
    \ -v</code></p>\n<p>Predict on TCGA images with our pretrained model for colorectal\
    \ cancer using <a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\">google colab</a></p>\n<h2>\n<a id=\"user-content-results\"\
    \ class=\"anchor\" href=\"#results\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Results</h2>\n<h2>\n<a id=\"\
    user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citing\
    \ HEMnet</h2>\n<h2>\n<a id=\"user-content-the-team\" class=\"anchor\" href=\"\
    #the-team\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>The Team</h2>\n<p>Please contact Dr Quan Nguyen (<a href=\"mailto:quan.nguyen@uq.edu.au\"\
    >quan.nguyen@uq.edu.au</a>), Andrew Su (<a href=\"mailto:a.su@uqconnect.edu.au\"\
    >a.su@uqconnect.edu.au</a>),\nand Xiao Tan (<a href=\"mailto:xiao.tan@uqconnect.edu.au\"\
    >xiao.tan@uqconnect.edu.au</a>) for issues, suggestions,\nand we are very welcome\
    \ to collaboration opportunities.</p>\n"
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1604591459.0
mbhall88/pistis:
  data_format: 2
  description: Quality control plotting for long reads
  filenames:
  - Singularity
  full_name: mbhall88/pistis
  latest_release: v0.3.0
  readme: "<h1>\n<a id=\"user-content-pistis\" class=\"anchor\" href=\"#pistis\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Pistis</h1>\n\
    <h3>\n<a id=\"user-content-quality-control-plotting-for-long-reads\" class=\"\
    anchor\" href=\"#quality-control-plotting-for-long-reads\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Quality\
    \ control plotting for long reads.</h3>\n<p><a href=\"https://pypi.python.org/pypi/pistis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0d377fd7c4560ba9ce5e50da718cfcda6af8bfe6e63362d9c8741335e20fec6c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7069737469732e737667\"\
    \ alt=\"PyPI status\" data-canonical-src=\"https://img.shields.io/pypi/v/pistis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.org/mbhall88/pistis\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/460505d13dbbc44006c446a195f753c22160192229624c04b719693986845945/68747470733a2f2f7472617669732d63692e6f72672f6d6268616c6c38382f7069737469732e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/mbhall88/pistis.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/a5606fdcd10a7afc202cdcc307f242a27a106834bebba2be192225e4315fb774/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6268616c6c38382f7069737469732e737667\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/mbhall88/pistis.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://twitter.com/mbhall88\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/899e87a3d856d3491f29644236afe87260be498a45240bd9acde07d48634d9fd/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6d6268616c6c38382e7376673f7374796c653d736f6369616c266c6f676f3d74776974746572266c6162656c3d466f6c6c6f77\"\
    \ alt=\"Twitter Follow\" data-canonical-src=\"https://img.shields.io/twitter/follow/mbhall88.svg?style=social&amp;logo=twitter&amp;label=Follow\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/2402\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>This package provides plotting designed\
    \ to give you an idea of how your long read\nsequencing data looks. It was conceived\
    \ of and developed with nanopore reads in\nmind, but there is no reason why PacBio\
    \ reads can't be used.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"\
    anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Installation</h2>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>pip3 install pistis</pre></div>\n<p>You can also\
    \ use <code>pip</code> if you are running with python2.<br>\nOr using a virtual\n\
    environment manager such as <a href=\"https://conda.io/docs/\" rel=\"nofollow\"\
    >conda</a> or\n<a href=\"https://docs.pipenv.org/\" rel=\"nofollow\">pipenv</a>.</p>\n\
    <p>You should now be able to run <code>pistis</code> from the command line</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis --help</pre></div>\n\
    <h3>\n<a id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity</h3>\n<p>There is a built image maintained with this repository\
    \ that can be used. For the latest release you can use the URI <code>shub://mbhall88/pistis</code><br>\n\
    For example</p>\n<div class=\"highlight highlight-source-shell\"><pre>singularity\
    \ <span class=\"pl-c1\">exec</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>shub://mbhall88/pistis<span class=\"pl-pds\">\"</span></span> pistis\
    \ --help\nsingularity pull --name pistis.simg <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>shub://mbhall88/pistis<span class=\"pl-pds\">\"</span></span></pre></div>\n\
    <h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <p>The main use case for <code>pistis</code> is as a command-line interface (CLI),\
    \ but it can also be\nused in an interactive way, such as with a <a href=\"https://jupyter.org/\"\
    \ rel=\"nofollow\">Jupyter Notebook</a>.</p>\n<h4>\n<a id=\"user-content-cli-usage\"\
    \ class=\"anchor\" href=\"#cli-usage\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CLI Usage</h4>\n<p>After installing\
    \ and running the help menu you should see the following usage\noptions</p>\n\
    <pre><code>pistis -h\n\nUsage: pistis [OPTIONS]\n\n  A package for sanity checking\
    \ (quality control) your long read data.\n  Feed it a fastq file and in return\
    \ you will receive a PDF with four plots:\n\n          1. GC content histogram\
    \ with distribution curve for sample.\n\n          2. Jointplot showing the read\
    \ length vs. phred quality score for\n          each         read. The interior\
    \ representation of this plot can be\n          altered with the         --kind\
    \ option.\n\n          3. Box plot of the phred quality score at positional bins\
    \ across\n          all reads. The reads are binned into read positions 1, 2,\
    \ 3, 4, 5,\n          6, 7, 8, 9, 10, 11-20, 21-50, 51-100, 101-200, 201-300.\
    \ Plots from\n          the start of reads.\n\n          4. Same as 3, but plots\
    \ from the end of the read.\n\n  Additionally, if you provide a BAM/SAM file a\
    \ histogram of the read\n  percent identity will be added to the report.\n\nOptions:\n\
    \  -f, --fastq PATH                Fastq file to plot. This can be gzipped.\n\
    \  -o, --output PATH               Path to save the plot PDF as. If name is not\n\
    \                                  specified, will use the name of the fastq\n\
    \                                  (or bam) file with .pdf extension.\n  -k, --kind\
    \ [kde|scatter|hex]    The kind of representation to use for the\n           \
    \                       jointplot of quality score vs read length.\n         \
    \                         Accepted kinds are 'scatter', 'kde'\n              \
    \                    (default), or 'hex'. For examples refer to h\n          \
    \                        ttps://seaborn.pydata.org/generated/seaborn.\n      \
    \                            jointplot.html\n  --log_length / --no_log_length\
    \  Plot the read length as a log10\n                                  transformation\
    \ on the quality vs read length\n                                  plot\n  -b,\
    \ --bam PATH                  SAM/BAM file to produce read percent\n         \
    \                         identity histogram from.\n  -d, --downsample INTEGER\
    \        Down-sample the sequence files to a given\n                         \
    \         number of reads. Set to 0 for no\n                                 \
    \ subsampling. Default: 50000\n  -h, --help                      Show this message\
    \ and exit.\n</code></pre>\n<p>Note the <code>--downsample</code> option is set\
    \ to 50000 by default. That is, <code>pistis</code> will\nonly plot 50000 reads\
    \ (sampled from a uniform distribution). You can set this to\n0 if you want to\
    \ plot every read, or select another number of your choosing. Be aware\nthat if\
    \ you try to plot too many reads you may run into memory issues, so try\ndownsampling\
    \ if this happens.</p>\n<p>There are three different use cases - currently - for\
    \ producing plots:</p>\n<p><strong>Fastq only</strong> - This will return four\
    \ plots:</p>\n<ul>\n<li>A distribution plot of the GC content for each read.</li>\n\
    <li>A bivariate jointplot with read length on the y-axis and mean read quality\n\
    score on the x-axis.</li>\n<li>Two boxplots that show the distribution of quality\
    \ scores at select positions\nand positional ranges. One plot shows the scores\
    \ from the beginning of the\nread and the other from the end of the read.</li>\n\
    </ul>\n<p>To use <code>pistis</code> in this way you just need a fastq file.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis -f /path/to/my.fastq\
    \ -o /save/as/report.pdf</pre></div>\n<p>This will save the four plots to a file\
    \ called <code>report.pdf</code> in directory <code>/save/as/</code>.\nIf you\
    \ don't provide a <code>--output/-o</code> option the file will be saved in the\
    \ current\ndirectory with the basename of the fastq file. So in the above example\
    \ it would be\nsaved as <code>my.pdf</code>.<br>\nIf you would prefer the read\
    \ lengths in the bivariate plot of read length vs.\nmean quality score then you\
    \ can indicate this like so</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pistis -f /path/to/my.fastq -o /save/as/report.pdf --no_log_length</pre></div>\n\
    <p>Additionally, you can change the way the data is represented in the bivariate\
    \ plot.\nThe default is a kernel density estimation plot (as in the below image),\
    \ however you can\nchoose to use a <a href=\"https://seaborn.pydata.org/generated/seaborn.jointplot.html\"\
    \ rel=\"nofollow\">hex bin or scatter plot version instead</a>.\nIn the running\
    \ example, to use a scatter plot you would run the following</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>pistis -f /path/to/my.fastq -o /save/as/report.pdf\
    \ --kind scatter</pre></div>\n<p>You can also provide a <code>gzip</code>ed fastq\
    \ file without any extra steps</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>pistis -f /path/to/my.fastq.gz -o /save/as/report.pdf</pre></div>\n<p><strong>Examples</strong><br>\n\
    GC content:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_gc_plot.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_gc_plot.png\"\
    \ alt=\"gc content plot\" style=\"max-width:100%;\"></a></p>\n<p>Read length vs.\
    \ mean read quality score:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_v_len.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_v_len.png\"\
    \ alt=\"read length vs quality plot\" style=\"max-width:100%;\"></a></p>\n<p>Base\
    \ quality from the start of each read:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_start.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_start.png\"\
    \ alt=\"base quality from start plot\" style=\"max-width:100%;\"></a></p>\n<p>Base\
    \ quality from the end of each read:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_end.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_end.png\"\
    \ alt=\"base quality from end plot\" style=\"max-width:100%;\"></a></p>\n<hr>\n\
    <p><strong>Fastq and BAM/SAM</strong> - This will return the above four plots,\
    \ plus a distribution\nplot of each read's percent identity with the reference\
    \ it is aligned to in the\n[BS]AM file. Reads which are flagged as supplementary\
    \ or secondary are not included.\nThe plot also includes a dashed vertical red\
    \ line indicating the median\npercent identity.<br>\nNote: If using a BAM file,\
    \ it must be sorted and indexed (i.e <code>.bai</code> file). See <a href=\"http://www.htslib.org/doc/samtools.html\"\
    \ rel=\"nofollow\"><code>samtools</code></a>\nfor instructions on how to do this.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis -f /path/to/my.fastq\
    \  -b /path/to/my.bam -o /save/as/report.pdf\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> or</span>\npistis -f /path/to/my.fastq  -b /path/to/my.sam -o\
    \ /save/as/report.pdf</pre></div>\n<p><strong>Example</strong><br>\nDistribution\
    \ of aligned read percent identity:<br>\n<a href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_perc_id.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_perc_id.png\"\
    \ alt=\"percent identity plot\" style=\"max-width:100%;\"></a></p>\n<hr>\n<p><strong>BAM/SAM\
    \ only</strong> - At this stage you will receive only the distribution\nplot of\
    \ each read's percent identity with the reference it is aligned to. In a\nfuture\
    \ release I aim to allow you to also get the other four fastq-only plots.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>pistis -b /path/to/my.bam\
    \ -o /save/as/report.pdf</pre></div>\n<p>As with the fastq-only method, if you\
    \ don't provide a <code>--output/-o</code> option the file will be saved in the\
    \ current\ndirectory with the basename of the [BS]AM file. So in the above example\
    \ it would be\nsaved as <code>my.pdf</code>.</p>\n<h4>\n<a id=\"user-content-usage-in-a-development-environment\"\
    \ class=\"anchor\" href=\"#usage-in-a-development-environment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage\
    \ in a development environment</h4>\n<p>If you would like to use <code>pistis</code>\
    \ within a development environment such as a\n<code>jupyter notebook</code> or\
    \ just a plain ol' python shell then take a look at <a href=\"https://github.com/mbhall88/pistis/blob/master/examples/example_usage.ipynb\"\
    >this example notebook</a>\nfor all the details.</p>\n<h2>\n<a id=\"user-content-credits\"\
    \ class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Credits</h2>\n<ul>\n<li>This\
    \ package was created with <a href=\"https://github.com/audreyr/cookiecutter\"\
    >Cookiecutter</a> and the <a href=\"https://github.com/audreyr/cookiecutter-pypackage\"\
    ><code>audreyr/cookiecutter-pypackage</code> project template</a>.</li>\n<li>The\
    \ two test data files (fastq and BAM) that I have used in this repository were\n\
    taken from <a href=\"https://github.com/wdecoster/nanotest\">Wouter De Coster's\
    \ <code>nanotest</code> repository</a>.</li>\n<li>Which in turn comes from <a\
    \ href=\"http://lab.loman.net/2017/03/09/ultrareads-for-nanopore/\" rel=\"nofollow\"\
    >Nick Loman and Josh Quick</a>.</li>\n<li>The example plots in this <code>README</code>\
    \ were made using the entire fastq of basecalled\nreads from the experiment in\
    \ that <a href=\"http://lab.loman.net/2017/03/09/ultrareads-for-nanopore/\" rel=\"\
    nofollow\">blog on \"whale hunting\"</a>.</li>\n<li>The plot for the BAM file\
    \ was obtained by running <code>pistis</code> on a BAM file generated\nby mapping\
    \ the fastq file to <em>E. coli</em> reference <a href=\"https://www.ncbi.nlm.nih.gov/nuccore/NC_000913.3\"\
    \ rel=\"nofollow\">NC_000913.3</a>\nusing Heng Li's <a href=\"https://github.com/lh3/minimap2\"\
    ><code>minimap2</code></a> and <code>-x map-ont</code> option.</li>\n</ul>\n<h1>\n\
    <a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing</h1>\n\
    <p>If you would like to contribute to this package you are more than welcome.<br>\n\
    <strong>Please read through the <a href=\"https://github.com/mbhall88/pistis/blob/master/CONTRIBUTING.rst\"\
    >contributing guidelines</a> first</strong>.</p>\n"
  stargazers_count: 5
  subscribers_count: 2
  topics:
  - nanopore
  - oxford-nanopore
  - bioinformatics
  - bioinformatics-analysis
  - plotting
  - quality-control
  - pacbio
  updated_at: 1614735802.0
melnel000/Sarek_CBIO:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: melnel000/Sarek_CBIO
  latest_release: null
  readme: "<h1>\n<a id=\"\" class=\"anchor\" href=\"#\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"http://sarek.scilifelab.se/\"\
    \ rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/Sarek_logo.png\"\
    \ alt=\"Sarek\" title=\"Sarek\" style=\"max-width:100%;\"></a>\n</h1>\n<h4>\n\
    <a id=\"user-content-an-open-source-analysis-pipeline-to-detect-germline-or-somatic-variants-from-whole-genome-or-targeted-sequencing\"\
    \ class=\"anchor\" href=\"#an-open-source-analysis-pipeline-to-detect-germline-or-somatic-variants-from-whole-genome-or-targeted-sequencing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>An open-source analysis pipeline to detect germline or somatic variants\
    \ from whole genome or targeted sequencing</h4>\n<p><a href=\"https://www.nextflow.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.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\"\
    \ alt=\"Nextflow version\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg?logo=data:image/svg+xml;base64,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\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.org/SciLifeLab/Sarek\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/63575514d0146dcd6226c111d1293fade62aa93f4946f2afcbb55718f3be5d86/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d747261766973\"\
    \ alt=\"Travis build status\" data-canonical-src=\"https://img.shields.io/travis/SciLifeLab/Sarek.svg?logo=travis\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://gitter.im/SciLifeLab/Sarek\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/419eea0b1c13bbf50717b250a81b0de7616141e86a5fc88eee2360f003f4c4e6/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d676974746572266c6f676f436f6c6f723d776869746526636f6c6f72423d346662393961\"\
    \ alt=\"Join the chat on https://gitter.im/SciLifeLab/Sarek\" data-canonical-src=\"\
    https://img.shields.io/gitter/room/SciLifeLab/Sarek.svg?logo=gitter&amp;logoColor=white&amp;colorB=4fb99a\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/e041bce107aca31eca6dd63a962556bf70d6cef2508c777604eedb99ca049c4b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f5363694c6966654c61622f536172656b2e737667\"\
    \ alt=\"MIT License\" data-canonical-src=\"https://img.shields.io/github/license/SciLifeLab/Sarek.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/SciLifeLab/Sarek/releases/latest\"\
    ><img src=\"https://camo.githubusercontent.com/720a0b93892db5c772d24eb7dc2fd6fefb2b556eff92ee7ae6a2963a40a8dd5a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465\"\
    \ alt=\"Sarek version\" data-canonical-src=\"https://img.shields.io/github/release/SciLifeLab/Sarek.svg?logo=github&amp;logoColor=white\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://zenodo.org/badge/latestdoi/54024046\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2794ec0225017cde71e3ed51dd8393510fe23a950955ef03f7439d7c0f288f83/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f35343032343034362e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/54024046.svg\" style=\"\
    max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.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\"\
    \ alt=\"Install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?logo=data:image/png;base64,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\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/maxulysse/sarek\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/bc3bec2ef3bf857d42e0bff8df09f0e81595bbd7dbc2681d0feadd729acb4bc0/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6d6178756c797373652f736172656b2e7376673f6c6f676f3d646f636b6572\"\
    \ alt=\"Docker Container available\" data-canonical-src=\"https://img.shields.io/docker/automated/maxulysse/sarek.svg?logo=docker\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h2>\n<p><a href=\"\
    https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/CAW_logo.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img align=\"right\" title=\"CAW\" src=\"\
    https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/CAW_logo.png\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Previously known as the Cancer Analysis\
    \ Workflow (CAW),\nSarek is a workflow designed to run analyses on WGS data from\
    \ regular samples or tumour / normal pairs, including relapse samples if required.</p>\n\
    <p>It's built using <a href=\"https://www.nextflow.io/\" rel=\"nofollow\">Nextflow</a>,\
    \ a domain specific language for workflow building.\nSoftware dependencies are\
    \ handled using <a href=\"https://www.docker.com\" rel=\"nofollow\">Docker</a>\
    \ or <a href=\"https://www.sylabs.io/singularity/\" rel=\"nofollow\">Singularity</a>\
    \ - container technologies that provide excellent reproducibility and ease of\
    \ use.\nSingularity has been designed specifically for high-performance computing\
    \ environments.\nThis means that although Sarek has been primarily designed for\
    \ use with the Swedish <a href=\"https://www.uppmax.uu.se\" rel=\"nofollow\">UPPMAX\
    \ HPC systems</a>, it should be able to run on any system that supports these\
    \ two tools.</p>\n<p>Sarek was developed at the <a href=\"https://ngisweden.scilifelab.se/\"\
    \ rel=\"nofollow\">National Genomics Infastructure</a> and <a href=\"https://www.nbis.se/\"\
    \ rel=\"nofollow\">National Bioinformatics Infastructure Sweden</a> which are\
    \ both platforms at <a href=\"https://www.scilifelab.se/\" rel=\"nofollow\">SciLifeLab</a>.\n\
    It is listed on the <a href=\"https://bio.tools/Sarek\" rel=\"nofollow\">Elixir\
    \ - Tools and Data Services Registry</a>.</p>\n<h2>\n<a id=\"user-content-workflow-steps\"\
    \ class=\"anchor\" href=\"#workflow-steps\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Workflow steps</h2>\n<p>Sarek\
    \ is built with several workflow scripts.\nA wrapper script contained within the\
    \ repository makes it easy to run the different workflow scripts as a single job.\n\
    To test your installation, follow the <a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/TESTS.md\"\
    >tests documentation.</a></p>\n<p>Raw FastQ files or aligned BAM files (with or\
    \ without realignment &amp; recalibration) can be used as inputs.\nYou can choose\
    \ which variant callers to use, plus the pipeline is capable of accommodating\
    \ additional variant calling software or CNV callers if required.</p>\n<p>The\
    \ worflow steps and tools used are as follows:</p>\n<ol>\n<li>\n<strong>Preprocessing</strong>\
    \ - <code>main.nf</code> <em>(based on <a href=\"https://software.broadinstitute.org/gatk/best-practices/\"\
    \ rel=\"nofollow\">GATK best practices</a>)</em>\n<ul>\n<li>Map reads to Reference\n\
    <ul>\n<li><a href=\"http://bio-bwa.sourceforge.net/\" rel=\"nofollow\">BWA</a></li>\n\
    </ul>\n</li>\n<li>Mark Duplicates\n<ul>\n<li><a href=\"https://github.com/broadinstitute/gatk\"\
    >GATK MarkDuplicates</a></li>\n</ul>\n</li>\n<li>Base (Quality Score) Recalibration\n\
    <ul>\n<li><a href=\"https://github.com/broadinstitute/gatk\">GATK BaseRecalibrator</a></li>\n\
    <li><a href=\"https://github.com/broadinstitute/gatk\">GATK ApplyBQSR</a></li>\n\
    </ul>\n</li>\n</ul>\n</li>\n<li>\n<strong>Germline variant calling</strong> -\
    \ <code>germlineVC.nf</code>\n<ul>\n<li>SNVs and small indels\n<ul>\n<li><a href=\"\
    https://github.com/broadinstitute/gatk\">GATK HaplotyeCaller</a></li>\n<li><a\
    \ href=\"https://github.com/Illumina/strelka\">Strelka2</a></li>\n</ul>\n</li>\n\
    <li>Structural variants\n<ul>\n<li><a href=\"https://github.com/Illumina/manta\"\
    >Manta</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<strong>Somatic variant calling</strong>\
    \ - <code>somaticVC.nf</code> <em>(optional)</em>\n<ul>\n<li>SNVs and small indels\n\
    <ul>\n<li><a href=\"https://github.com/broadinstitute/gatk\">MuTect2</a></li>\n\
    <li><a href=\"https://github.com/ekg/freebayes\">Freebayes</a></li>\n<li><a href=\"\
    https://github.com/Illumina/strelka\">Strelka2</a></li>\n</ul>\n</li>\n<li>Structural\
    \ variants\n<ul>\n<li><a href=\"https://github.com/Illumina/manta\">Manta</a></li>\n\
    </ul>\n</li>\n<li>Sample heterogeneity, ploidy and CNVs\n<ul>\n<li><a href=\"\
    https://github.com/Crick-CancerGenomics/ascat\">ASCAT</a></li>\n</ul>\n</li>\n\
    </ul>\n</li>\n<li>\n<strong>Annotation</strong> - <code>annotate.nf</code> <em>(optional)</em>\n\
    <ul>\n<li>Variant annotation\n<ul>\n<li><a href=\"http://snpeff.sourceforge.net/\"\
    \ rel=\"nofollow\">SnpEff</a></li>\n<li><a href=\"https://www.ensembl.org/info/docs/tools/vep/index.html\"\
    \ rel=\"nofollow\">VEP (Variant Effect Predictor)</a></li>\n</ul>\n</li>\n</ul>\n\
    </li>\n<li>\n<strong>Reporting</strong> - <code>runMultiQC.nf</code>\n<ul>\n<li>Reporting\n\
    <ul>\n<li><a href=\"http://multiqc.info\" rel=\"nofollow\">MultiQC</a></li>\n\
    </ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>The Sarek\
    \ pipeline comes with documentation in the <code>docs/</code> directory:</p>\n\
    <ol>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL.md\"\
    >Installation documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL_RACKHAM.md\"\
    >Installation documentation specific for UPPMAX <code>rackham</code></a></li>\n\
    <li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL_BIANCA.md\"\
    >Installation documentation specific for UPPMAX <code>bianca</code></a></li>\n\
    <li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/TESTS.md\"\
    >Tests documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/REFERENCES.md\"\
    >Reference files documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/CONFIG.md\"\
    >Configuration and profiles documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INTERVALS.md\"\
    >Intervals documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/USAGE.md\"\
    >Running the pipeline</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/PARAMETERS.md\"\
    >Command line parameters</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/USE_CASES.md\"\
    >Examples</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INPUT.md\"\
    >Input files documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/PROCESS.md\"\
    >Processes documentation</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/CONTAINERS.md\"\
    >Documentation about containers</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/ASCAT.md\"\
    >More information about ASCAT</a></li>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/OUTPUT.md\"\
    >Output documentation structure</a></li>\n</ol>\n<h2>\n<a id=\"user-content-contributions--support\"\
    \ class=\"anchor\" href=\"#contributions--support\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributions\
    \ &amp; Support</h2>\n<p>If you would like to contribute to this pipeline, please\
    \ see the <a href=\"https://github.com/SciLifeLab/Sarek/blob/master/.github/CONTRIBUTING.md\"\
    >contributing guidelines</a>.</p>\n<p>For further information or help, don't hesitate\
    \ to get in touch on <a href=\"https://gitter.im/SciLifeLab/Sarek\" rel=\"nofollow\"\
    >Gitter</a> or contact us: <a href=\"mailto:maxime.garcia@scilifelab.se\">maxime.garcia@scilifelab.se</a>,\
    \ <a href=\"mailto:szilveszter.juhos@scilifelab.se\">szilveszter.juhos@scilifelab.se</a></p>\n\
    <h2>\n<a id=\"user-content-changelog\" class=\"anchor\" href=\"#changelog\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CHANGELOG</h2>\n\
    <ul>\n<li><a href=\"https://github.com/SciLifeLab/Sarek/blob/master/CHANGELOG.md\"\
    >CHANGELOG</a></li>\n</ul>\n<h2>\n<a id=\"user-content-credits\" class=\"anchor\"\
    \ href=\"#credits\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Credits</h2>\n<p>Main authors:</p>\n<ul>\n<li><a href=\"\
    https://github.com/MaxUlysse\">Maxime Garcia</a></li>\n<li><a href=\"https://github.com/szilvajuhos\"\
    >Szilveszter Juhos</a></li>\n</ul>\n<p>Helpful contributors:</p>\n<ul>\n<li><a\
    \ href=\"https://github.com/alneberg\">Johannes Alneberg</a></li>\n<li><a href=\"\
    https://github.com/Sebastian-D\">Sebastian DiLorenzo</a></li>\n<li><a href=\"\
    https://github.com/J35P312\">Jesper Eisfeldt</a></li>\n<li><a href=\"https://github.com/ewels\"\
    >Phil Ewels</a></li>\n<li><a href=\"https://github.com/gulfshores\">Max K\xE4\
    ller</a></li>\n<li><a href=\"https://github.com/malinlarsson\">Malin Larsson</a></li>\n\
    <li><a href=\"https://github.com/marcelm\">Marcel Martin</a></li>\n<li><a href=\"\
    https://github.com/bjornnystedt\">Bj\xF6rn Nystedt</a></li>\n<li><a href=\"https://github.com/pallolason\"\
    >Pall Olason</a></li>\n<li><a href=\"https://github.com/arontommi\">Aron Skaftason</a></li>\n\
    </ul>\n<hr>\n<p><a href=\"https://www.scilifelab.se/\" rel=\"nofollow\"><img src=\"\
    https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/SciLifeLab_logo.png\"\
    \ alt=\"SciLifeLab\" title=\"SciLifeLab\" style=\"max-width:100%;\"></a>\n<a href=\"\
    https://ngisweden.scilifelab.se/\" rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/NGI_logo.png\"\
    \ alt=\"NGI\" title=\"NGI\" style=\"max-width:100%;\"></a>\n<a href=\"https://www.nbis.se/\"\
    \ rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/NBIS_logo.png\"\
    \ alt=\"NBIS\" title=\"NBIS\" style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1541579046.0
mesnardo/petibm-decoupledibpm:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity.petibm0.5-xenial
  - singularity/Singularity.petibm0.5.1-xenial
  - singularity/Singularity.petibm0.4.2-xenial
  full_name: mesnardo/petibm-decoupledibpm
  latest_release: null
  readme: '<h1>

    <a id="user-content-decoupled-immersed-boundary-projection-method-with-petibm"
    class="anchor" href="#decoupled-immersed-boundary-projection-method-with-petibm"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Decoupled
    Immersed Boundary Projection Method with PetIBM</h1>

    <p><a href="https://github.com/mesnardo/petibm-decoupledibpm/raw/master/LICENSE"><img
    src="https://camo.githubusercontent.com/8ccf186e7288af6d88a1f6a930c0fcc4e7a8a9936b34e07629d815d1eab4d977/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667"
    alt="License" data-canonical-src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://cloud.docker.com/u/mesnardo/repository/docker/mesnardo/petibm-decoupledibpm"
    rel="nofollow"><img src="https://camo.githubusercontent.com/b8d9674ae17bb539afa71ecc4169a1ee5a6a9242d8f9e12a10f4583093ba57c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65722d2d6875622d696e666f726d6174696f6e616c2e737667"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/badge/hosted-docker--hub-informational.svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3171" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="Singularity Hub" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-flow-over-a-stationary-circular-cylinder-re40-and-100" class="anchor"
    href="#flow-over-a-stationary-circular-cylinder-re40-and-100" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Flow over a stationary
    circular cylinder ($Re=40$ and $100$)</h2>

    <p><a href="runs/cylinder2dRe40/189_markers/figures/wz_0005000.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/cylinder2dRe40/189_markers/figures/wz_0005000.png"
    alt="cylinderRe40_vorticity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Vorticity contours around the cylinder at Reynolds number
    $40$. (Contour levels between $-3D/U_\infty$ and $3D/U_\infty$ with increments
    of $0.4$.)</p>

    <p><a href="runs/cylinder2dRe100/189_markers/figures/wz_0020000.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/cylinder2dRe100/189_markers/figures/wz_0020000.png"
    alt="cylinderRe100_vorticity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Vorticity contours around the cylinder at Reynolds number
    $100$ after $200$ time units of flow simulation. (Contour levels between $-3D/U_\infty$
    and $3D/U_\infty$ with increments of $0.4$.)</p>

    <p><a href="runs/cylinder2dRe40/189_markers/figures/cp_0005000.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/cylinder2dRe40/189_markers/figures/cp_0005000.png"
    alt="cylinderRe40_pressure_coefficient" style="max-width:100%;"></a>

    <strong>Figure:</strong> Pressure coefficient along the upper and lower surfaces
    of the cylinder at Reynolds number $40$. We compare with the results from Li et
    al. (2016).</p>

    <p><a href="runs/cylinder2dRe100/189_markers/figures/pressure_coefficient.png"
    target="_blank" rel="noopener noreferrer"><img src="runs/cylinder2dRe100/189_markers/figures/pressure_coefficient.png"
    alt="cylinderRe100_pressure_coefficient" style="max-width:100%;"></a>

    <strong>Figure:</strong> Pressure coefficient along the upper and lower surfaces
    of the cylinder at Reynolds number $100$. We compare with the results from Li
    et al. (2016).</p>

    <h2>

    <a id="user-content-flow-around-an-inline-oscillating-circular-cylinder-re100"
    class="anchor" href="#flow-around-an-inline-oscillating-circular-cylinder-re100"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Flow
    around an inline oscillating circular cylinder ($Re=100$)</h2>

    <p><a href="runs/oscillatingcylinderRe100/algo1/figures/vorticity.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/algo1/figures/vorticity.png"
    alt="oscillatingcylinderRe100_vorticity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Contours of the vorticity field around an inline oscillating
    cylinder at different phase angles ($\phi = 2 \pi f t$): $\phi = 0^o$ (left) and
    $\phi = 288^o$ (right). (Contour levels between $-20 U_m / D$ and $20 U_m / D$
    using $30$ increments.)</p>

    <p><a href="runs/oscillatingcylinderRe100/algo1/figures/pressure.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/algo1/figures/pressure.png"
    alt="oscillatingcylinderRe100_pressure" style="max-width:100%;"></a>

    <strong>Figure:</strong> Contours of the pressure field around an inline oscillating
    cylinder at different phase angles ($\phi = 2 \pi f t$): $\phi = 0^o$ (left) and
    $\phi = 288^o$ (right). (Contour levels between $-1 \rho U_m^2$ and $1 \rho U_m^2$
    using $50$ increments.)</p>

    <p><a href="runs/oscillatingcylinderRe100/algo1/figures/velocity_profiles.png"
    target="_blank" rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/algo1/figures/velocity_profiles.png"
    alt="oscillatingcylinderRe100_velocity" style="max-width:100%;"></a>

    <strong>Figure:</strong> Profile of the velocity components ($u$: left, $v$: right)
    at four locations along the centerline for various phase angles $\phi$.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/drag_coefficient.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient.png"
    alt="oscillatingcylinderRe100_drag_coefficient" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient of the inline oscillating
    cylinder obtained using different algorithms. We also show zooms at early and
    developed stages.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/drag_coefficient_dt.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient_dt.png"
    alt="oscillatingcylinderRe100_drag_coefficient_dt" style="max-width:100%;"></a>

    <a href="runs/oscillatingcylinderRe100/figures/drag_coefficient_dx.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient_dx.png"
    alt="oscillatingcylinderRe100_drag_coefficient_dx" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient obtained with Algorithm
    1 for different time-step sizes and different grid sizes.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/temporal_error.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/temporal_error.png"
    alt="oscillatingcylinderRe100_temporal_error" style="max-width:100%;"></a>

    <strong>Figure:</strong> Variations of the $L_\infty$ and $L_2$ norm errors of
    the streamwise velocity as a function of the computational time-step size.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/spatial_error.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/spatial_error.png"
    alt="oscillatingcylinderRe100_temporal_error" style="max-width:100%;"></a>

    <strong>Figure:</strong> Variations of the $L_\infty$ and $L_2$ norm errors of
    the streamwise velocity as a function of the computational grid spacing.</p>

    <p><a href="runs/oscillatingcylinderRe100/figures/drag_coefficient_lag.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/oscillatingcylinderRe100/figures/drag_coefficient_lag.png"
    alt="oscillatingcylinderRe100_cd_lag" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient using Algorithm 3 with
    force-prediction scheme 3. We compared the history obtained with different Lagrangian
    mesh resolutions: $N_b = 500$ Lagrangian markers on the boundary and $N_b = 202$
    markers (the latter one corresponding to the same resolution as the Eulerian background
    grid).</p>

    <h2>

    <a id="user-content-flow-around-an-impulsively-started-circular-cylinder-re40"
    class="anchor" href="#flow-around-an-impulsively-started-circular-cylinder-re40"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Flow
    around an impulsively started circular cylinder (Re=40)</h2>

    <p><a href="runs/translatingcylinder2dRe40/figures/drag_coefficients.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/translatingcylinder2dRe40/figures/drag_coefficients.png"
    alt="translatingcylinder2dRe40_cd" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the drag coefficient of the impulsively started
    cylinder. Comparison with the analytical solution of Bar-Lev &amp; Yang (1997)
    and the numerical results from Taira &amp; Colonius (2007).</p>

    <p><a href="runs/translatingcylinder2dRe40/dt=0.0005/figures/vorticity.png" target="_blank"
    rel="noopener noreferrer"><img src="runs/translatingcylinder2dRe40/dt=0.0005/figures/vorticity.png"
    alt="translatingcylinder2dRe40_wz" style="max-width:100%;"></a>

    <strong>Figure:</strong> Vorticity contours around the impulsively started circular
    cylinder at $t=1.0$ (left) and $t=3.5$ (right). Contour levels between $-3 \omega_z
    D / U_o$ and $3 \omega_z D / U_o$ with increments of $0.4$.</p>

    <p><a href="runs/translatingcylinder2dRe40/figures/recirculation_lengths.png"
    target="_blank" rel="noopener noreferrer"><img src="runs/translatingcylinder2dRe40/figures/recirculation_lengths.png"
    alt="translatingcylinder2dRe40_lw" style="max-width:100%;"></a>

    <strong>Figure:</strong> History of the recirculation length measured in the reference
    frame of the impulsively start cylinder at Reynolds number 40 and for different
    time-step sizes.</p>

    <h2>

    <a id="user-content-three-dimensional-flow-around-an-inline-oscillating-sphere-re7854"
    class="anchor" href="#three-dimensional-flow-around-an-inline-oscillating-sphere-re7854"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Three-dimensional
    flow around an inline oscillating sphere ($Re=78.54$)</h2>

    <p><a href="runs/oscillatingsphere/figures/pressure.png" target="_blank" rel="noopener
    noreferrer"><img src="runs/oscillatingsphere/figures/pressure.png" alt="sphere_pressure"
    style="max-width:100%;"></a>

    <strong>Figure:</strong> Contours of the pressure field in the $x$/$y$ at $z=0$
    at three phase angles. Contour levels between $-2 p / \rho U_m^2$ and $2 p / \rho
    U_m^2$ with $30$ increments.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1581529613.0
miquelramirez/hybrid-fs:
  data_format: 2
  description: 'Hybrid-FS: A planner for controlling hybrid systems specified in Functional
    STRIPS'
  filenames:
  - Singularity
  full_name: miquelramirez/hybrid-fs
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-the-fs-functional-strips-planner\" class=\"\
    anchor\" href=\"#the-fs-functional-strips-planner\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>The FS Functional\
    \ STRIPS planner</h1>\n<p><code>FS</code> is a classical planner that works with\
    \ the Functional STRIPS planning language <a href=\"#ref-geffner-fstrips-2000\"\
    >[Geffner, 2000]</a>,\na modeling language based on the quantifier-free\nfragment\
    \ of first-order logic that includes constant, function and predicate symbols,\
    \ but no variable symbols. The increased expressiveness\nof the Functional STRIPS\
    \ language with respect to propositional languages such as standard STRIPS (which\
    \ is indeed subsumed by Functional STRIPS)\noften results in problem encodings\
    \ which are more compact, more readable, have fewer ground actions\nand preserve\
    \ the structural properties of the problem in a manner which allows the derivation\
    \ of more effective heuristics.</p>\n<p>Along with the core of the Functional\
    \ STRIPS language, the <code>FS</code> planner supports certain extensions which\
    \ are useful both\nfrom the expressive <em>and</em> the computational point of\
    \ view. These include <em>existential quantification</em>,\n<em>state constraints</em>,\
    \ a fairly large library of <em>global constraints</em>, and the possibility of\
    \ using <em>externally-defined symbols</em>\nand <em>built-in arithmetic symbols</em>.</p>\n\
    <p>This documentation covers a number of practical issues related to the use of\
    \ the <code>FS</code> planner. The planner, however, has\nbeen used and described\
    \ in a number of academic publications that <a href=\"http://gfrances.github.io/pubs/\"\
    \ rel=\"nofollow\">can be found here</a>,\nthe most recent of which are <a href=\"\
    #ref-frances-modeling-2015\">[Franc\xE8s and Geffner, 2015]</a> and <a href=\"\
    #ref-frances-existential-2016\">[Franc\xE8s and Geffner, 2016a]</a>\nand <a href=\"\
    #ref-frances-effective-2016\">[Franc\xE8s and Geffner, 2016b]</a>.</p>\n<ol>\n\
    <li><a href=\"#installation\">Installation</a></li>\n<li><a href=\"#usage\">Usage</a></li>\n\
    <li><a href=\"#credits\">Credits</a></li>\n<li><a href=\"#references\">References</a></li>\n\
    </ol>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a name=\"user-content-references\"></a>Installation</h2>\n<p>The\
    \ easiest way to use the planner is by <a href=\"doc/installation.md\">manually\
    \ compiling the planner source code</a>.\nThis procedure is complemented by the\
    \ <a href=\"doc/hybrid.md\">instructions specific for setting up the hybrid module</a>\
    \ of <code>FS</code>.</p>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><a name=\"user-content-usage\"></a>Usage</h2>\n<p>You\
    \ can find a high-level overview of the planner usage options <a href=\"doc/usage.md\"\
    >here</a></p>\n<h2>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a name=\"user-content-credits\"></a>Credits</h2>\n<p>The <code>FS</code>\
    \ planner is partially built upon the <a href=\"http://www.lapkt.org\" rel=\"\
    nofollow\">Lightweight Automated Planning Toolkit</a>\nand the PDDL parser from\
    \ the <a href=\"http://www.fast-downward.org\" rel=\"nofollow\">Fast Downward</a>\
    \ distribution.</p>\n<h2>\n<a id=\"user-content-references\" class=\"anchor\"\
    \ href=\"#references\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a name=\"user-content-references\"></a>References</h2>\n\
    <ul>\n<li>\n<p><a name=\"user-content-ref-frances-modeling-2015\">Franc\xE8s,\
    \ G., and Geffner, H. (2015)</a>,\n<a href=\"http://gfrances.github.io/pubs/2015-icaps-better-heuristics-more-expressive-languages/\"\
    \ rel=\"nofollow\"><em>Modeling and Computation in Planning: Better Heuristics\
    \ from More Expressive Languages</em></a>, ICAPS 2015.</p>\n</li>\n<li>\n<p><a\
    \ name=\"user-content-ref-frances-existential-2016\">Franc\xE8s, G., and Geffner,\
    \ H. (2016a)</a>,\n<a href=\"http://gfrances.github.io/pubs/2016-ijcai-existential-quantification-planning-csp/\"\
    \ rel=\"nofollow\"><em>E-STRIPS: Existential Quantification in Planning and Constraint\
    \ Satisfaction</em></a>, IJCAI 2016.</p>\n</li>\n<li>\n<p><a name=\"user-content-ref-frances-effective-2016\"\
    >Franc\xE8s, G., and Geffner, H. (2016b)</a>,\n<a href=\"http://gfrances.github.io/pubs/2016-ijcai-effective-planning-more-expressive-languages/\"\
    \ rel=\"nofollow\"><em>Effective Planning with More Expressive Languages</em></a>,\
    \ IJCAI 2016.</p>\n</li>\n<li>\n<p><a name=\"user-content-ref-geffner-fstrips-2000\"\
    >Geffner, H. (2000)</a>,\n<a href=\"http://www.tecn.upf.es/~hgeffner/\" rel=\"\
    nofollow\"><em>Functional STRIPS: A more flexible lan-\nguage for planning and\
    \ problem solving</em></a>.\nIn Minker, J., ed., Logic-Based Artificial Intelligence.\
    \ Kluwer. 187\u2013205.</p>\n</li>\n</ul>\n"
  stargazers_count: 2
  subscribers_count: 3
  topics:
  - cplusplus-14
  - ai
  - planning
  - hybrid-systems
  - kinodynamic-planning
  updated_at: 1570694552.0
ml4ai/UA-hpc-containers:
  data_format: 2
  description: Container recipes for use with the U of A HPC resources
  filenames:
  - Singularity.keras
  - Singularity.delphi
  - Singularity.cuda9_py36
  - Singularity.pytorch
  - Singularity.numba
  - Singularity.nvidia_docker
  - Singularity.tensorflow
  - Singularity.dynet
  - Singularity.caffe2
  - Singularity.pytorch_skimage
  - Singularity.im2markup
  - Singularity.torch
  - openmnt/Singularity.opennmt
  - word2vec/Singularity.w2v
  full_name: ml4ai/UA-hpc-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-ml4ai-lab-singularity-container-repository" class="anchor"
    href="#ml4ai-lab-singularity-container-repository" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>ML4AI Lab Singularity Container Repository</h1>

    <p><a href="https://singularity-hub.org/collections/2086" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>This repository holds singularity containers that are commonly used by members
    of the ML4AI lab to run projects on the University of Arizona''s high-performance
    computing environment.</p>

    <p><strong>See Also</strong>:

    <a href="https://github.com/clulab/hpc-ml">https://github.com/clulab/hpc-ml</a></p>

    '
  stargazers_count: 2
  subscribers_count: 20
  topics: []
  updated_at: 1617312549.0
ml4ai/automates:
  data_format: 2
  description: 'AutoMATES: Automated Model Assembly from Text, Equations, and Software'
  filenames:
  - automates/equation_reading/equation_extraction/containers/Singularity.pytorch_skimage
  - automates/equation_reading/equation_extraction/containers/Singularity.im2markup
  full_name: ml4ai/automates
  latest_release: v0.1.0
  readme: "<h1 align=\"center\">\n<a id=\"user-content-automated-model-assemblyfrom-text-equations-and-software\"\
    \ class=\"anchor\" href=\"#automated-model-assemblyfrom-text-equations-and-software\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Automated Model Assembly<br>from Text, Equations, and Software</h1>\n\
    <p align=\"center\">\n  \n  \n  <a href=\"https://github.com/ml4ai/automates/actions\"\
    >\n    <img src=\"https://camo.githubusercontent.com/aca4ff329fcbc8477eb69ee3754d654a8f27d1b798178530242c11421121ca79/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f6d6c3461692f6175746f6d617465732f436f6e74696e756f7573253230496e746567726174696f6e3f6c6162656c3d7465737473\"\
    \ alt=\"GH Actions build status\" data-canonical-src=\"https://img.shields.io/github/workflow/status/ml4ai/automates/Continuous%20Integration?label=tests\"\
    \ style=\"max-width:100%;\">\n  </a>\n  <a href=\"https://codecov.io/gh/ml4ai/automates\"\
    \ rel=\"nofollow\">\n   <img src=\"https://camo.githubusercontent.com/f7328d200bac8b4c5c0473f8c3e3d1b870c0811e5802f20ef544cfc68259abbd/68747470733a2f2f636f6465636f762e696f2f67682f6d6c3461692f6175746f6d617465732f6272616e63682f6d61737465722f67726170682f62616467652e737667\"\
    \ data-canonical-src=\"https://codecov.io/gh/ml4ai/automates/branch/master/graph/badge.svg\"\
    \ style=\"max-width:100%;\">\n  </a>\n  <a href=\"https://www.codefactor.io/repository/github/ml4ai/automates\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ce091de26720098ad92a2ee617c9d8975b7bf53366140d5119b22f0025f8e740/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6d6c3461692f6175746f6d617465732f6261646765\"\
    \ alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/ml4ai/automates/badge\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p>This repository holds the source code\
    \ for the AutoMATES documentation\nand several component pipelines.</p>\n<p>For\
    \ documentation: <a href=\"https://ml4ai.github.io/automates\" rel=\"nofollow\"\
    >https://ml4ai.github.io/automates</a></p>\n<h2>\n<a id=\"user-content-installation-instructions\"\
    \ class=\"anchor\" href=\"#installation-instructions\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation\
    \ instructions</h2>\n<p>For all operating systems, the first step of the installation\
    \ process is to clone the AutoMATES repository.</p>\n<h3>\n<a id=\"user-content-linux-and-macos\"\
    \ class=\"anchor\" href=\"#linux-and-macos\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linux and macOS</h3>\n<ul>\n\
    <li>Create a new <a href=\"https://docs.python.org/3/library/venv.html\" rel=\"\
    nofollow\">Python virtualenv</a>\n</li>\n<li>Activate your new Python virtualenv</li>\n\
    <li>Install Graphviz as defined below</li>\n<li>Run <code>pip install -e .</code>\
    \ from the root of the AutoMATES directory</li>\n</ul>\n<h4>\n<a id=\"user-content-graphviz-installation\"\
    \ class=\"anchor\" href=\"#graphviz-installation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>GraphViz installation</h4>\n\
    <h5>\n<a id=\"user-content-debian-flavored-linux\" class=\"anchor\" href=\"#debian-flavored-linux\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Debian flavored linux</h5>\n<ul>\n<li>Use the command: <code>sudo\
    \ apt-get install graphviz libgraphviz-dev pkg-config</code>\n</li>\n</ul>\n<h5>\n\
    <a id=\"user-content-macos-with-homebrew\" class=\"anchor\" href=\"#macos-with-homebrew\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>macOS with Homebrew</h5>\n<ul>\n<li>Use the command: <code>brew install\
    \ graphviz</code>\n</li>\n<li>Install PyGraphviz to your virtualenv with: <code>pip\
    \ install --install-option=\"--include-path=/usr/local/include/\" --install-option=\"\
    --library-path=/usr/local/lib\" pygraphviz</code>\n</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-windows\" class=\"anchor\" href=\"#windows\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Windows</h3>\n\
    <ul>\n<li>Download and install <a href=\"https://www.anaconda.com/products/individual\"\
    \ rel=\"nofollow\">Anaconda</a>\n</li>\n<li>Edit the <code>PYTHONPATH</code> variable\
    \ in <code>environment.yml</code> to be your local path to your checkout of the\
    \ AutoMATES repo</li>\n<li>Run <code>conda env create --file environment.yml</code>\
    \ from the root of the AutoMATES directory</li>\n</ul>\n"
  stargazers_count: 17
  subscribers_count: 10
  topics: []
  updated_at: 1620238275.0
mmirko/singularitytest:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: mmirko/singularitytest
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularitytest" class="anchor" href="#singularitytest" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>singularitytest</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605257877.0
mosoriob/pegasus_montage-workflow-v2:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: mosoriob/pegasus_montage-workflow-v2
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-montage-workflow-v2\" class=\"anchor\" href=\"\
    #montage-workflow-v2\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>montage-workflow-v2</h1>\n<p>A new Python DAX\
    \ generator version of the classic Montage workflow. This workflow uses the <a\
    \ href=\"http://montage.ipac.caltech.edu\" rel=\"nofollow\">Montage\ntoolkit</a>\
    \ to re-project, background correct and add astronomical\nimages into custom mosaics.</p>\n\
    <h2>\n<a id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h2>\n<ul>\n<li>\n<a href=\"http://montage.ipac.caltech.edu\"\
    \ rel=\"nofollow\">Montage</a> - version 4.0 or later</li>\n<li>\n<a href=\"http://www.astropy.org/\"\
    \ rel=\"nofollow\">AstroPy</a> - version 1.0 or later</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-plan-a-montage-workflow\" class=\"anchor\" href=\"#plan-a-montage-workflow\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Plan a Montage Workflow</h2>\n<p>The <em>./montage-workflow.py</em>\
    \ Python script sets up a <em>data/</em> directory with a Pegasus DAX,\nimage\
    \ tables and region headers. For example:</p>\n<pre><code>./montage-workflow.py\
    \ --center \"56.7 24.0\" --degrees 2.0 \\\n          --band dss:DSS2B:blue --band\
    \ dss:DSS2R:green --band dss:DSS2IR:red\n</code></pre>\n<p>This will create a\
    \ 2x2 degree mosaic centered on 56.7 24.0, with 3 bands making up the\nred, green,\
    \ and blue channels for the final JPEG output. A 2 degree workflow has a lot\n\
    of input images and thus the workflow becomes wide. I simplified version of the\
    \ workflow\nlooks like:</p>\n<p><a href=\"docs/images/dax1.png?raw=true\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"docs/images/dax1.png?raw=true\"\
    \ alt=\"DAX 1\" title=\"DAX 1\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"\
    user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Examples</h2>\n\
    <p>The quickest way to get started is to use the <em>./example-dss.sh</em>\nscript.\
    \ It shows how to use the <em>montage-workflow.py</em> DAX generator to set up\
    \ and plan\n2 degree workflows as described above. Example:</p>\n<pre><code>$\
    \ ./example-dss.sh \n\nAdding band 1 (dss DSS2B -&gt; blue)\nRunning sub command:\
    \ mArchiveList dss DSS2B \"56.7 24.00\" 2.2 2.2 data/1-images.tbl\n[struct stat=\"\
    OK\", count=\"16\"]\nRunning sub command: cd data &amp;&amp; mDAGTbls 1-images.tbl\
    \ region-oversized.hdr 1-raw.tbl 1-projected.tbl 1-corrected.tbl\n[struct stat=\"\
    OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data &amp;&amp; mOverlaps\
    \ 1-raw.tbl 1-diffs.tbl\n[struct stat=\"OK\", count=120]\n\nAdding band 2 (dss\
    \ DSS2R -&gt; green)\nRunning sub command: mArchiveList dss DSS2R \"56.7 24.00\"\
    \ 2.2 2.2 data/2-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command:\
    \ cd data &amp;&amp; mDAGTbls 2-images.tbl region-oversized.hdr 2-raw.tbl 2-projected.tbl\
    \ 2-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub\
    \ command: cd data &amp;&amp; mOverlaps 2-raw.tbl 2-diffs.tbl\n[struct stat=\"\
    OK\", count=120]\n\nAdding band 3 (dss DSS2IR -&gt; red)\nRunning sub command:\
    \ mArchiveList dss DSS2IR \"56.7 24.00\" 2.2 2.2 data/3-images.tbl\n[struct stat=\"\
    OK\", count=\"16\"]\nRunning sub command: cd data &amp;&amp; mDAGTbls 3-images.tbl\
    \ region-oversized.hdr 3-raw.tbl 3-projected.tbl 3-corrected.tbl\n[struct stat=\"\
    OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data &amp;&amp; mOverlaps\
    \ 3-raw.tbl 3-diffs.tbl\n[struct stat=\"OK\", count=120]\n2016.06.02 21:46:32.455\
    \ PDT:    \n2016.06.02 21:46:32.461 PDT:   -----------------------------------------------------------------------\
    \ \n2016.06.02 21:46:32.466 PDT:   File for submitting this DAG to HTCondor  \
    \         : montage-0.dag.condor.sub \n2016.06.02 21:46:32.471 PDT:   Log of DAGMan\
    \ debugging messages                 : montage-0.dag.dagman.out \n2016.06.02 21:46:32.476\
    \ PDT:   Log of HTCondor library output                     : montage-0.dag.lib.out\
    \ \n2016.06.02 21:46:32.481 PDT:   Log of HTCondor library error messages    \
    \         : montage-0.dag.lib.err \n2016.06.02 21:46:32.487 PDT:   Log of the\
    \ life of condor_dagman itself          : montage-0.dag.dagman.log \n2016.06.02\
    \ 21:46:32.492 PDT:    \n2016.06.02 21:46:32.497 PDT:   -no_submit given, not\
    \ submitting DAG to HTCondor.  You can do this with: \n2016.06.02 21:46:32.507\
    \ PDT:   -----------------------------------------------------------------------\
    \ \n2016.06.02 21:46:33.387 PDT:   Your database is compatible with Pegasus version:\
    \ 4.6.1 \n2016.06.02 21:46:33.392 PDT:   \n\nI have concretized your abstract\
    \ workflow. The workflow has been entered \ninto the workflow database with a\
    \ state of \"planned\". The next step is \nto start or execute your workflow.\
    \ The invocation required is\n\npegasus-run  /data/scratch/rynge/montage2/montage-workflow-v2/work/1464929190\n\
    \n2016.06.02 21:46:33.419 PDT:   Time taken to execute is 2.961 seconds \n</code></pre>\n\
    <p>Running the workflow produces fits and jpeg mosaics for each band, as well\
    \ as a combined color one:</p>\n<p><a href=\"docs/images/pleiades.jpg?raw=true\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"docs/images/pleiades.jpg?raw=true\"\
    \ alt=\"Pleiades\" title=\"Pleiades\" style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1535257330.0
mvdoc/singularity-def:
  data_format: 2
  description: Singularity definition files for various projects
  filenames:
  - hauntedhouse_freesurfer/Singularity
  - hauntedhouse/Singularity
  - miniconda/Singularity
  full_name: mvdoc/singularity-def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1495630055.0
nbcrrolls/electrostatics-singularity:
  data_format: 2
  description: Molecular electrostatics singularity image
  filenames:
  - Singularity
  full_name: nbcrrolls/electrostatics-singularity
  latest_release: v2.1
  readme: "<h1>\n<a id=\"user-content-singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ class=\"anchor\" href=\"#singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity container for molecular electrostatic calculations using\
    \ PDB2PQR/APBS and Brownian dynamics with BrownDye.</h1>\n<p>This singularity\
    \ image contains a complete software environment for running <a href=\"http://browndye.ucsd.edu/\"\
    \ rel=\"nofollow\">BrownDye (version 1 and 2)</a> simulations. It also includes\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">PDB2PQR</a> and\
    \ <a href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\">APBS</a>.</p>\n\
    <p>Please <a href=\"http://eepurl.com/by4eQr\" rel=\"nofollow\">register</a> your\
    \ use of APBS and PDB2PQR.</p>\n<p>The image has been verified to work on XSEDE\
    \ <a href=\"https://portal.xsede.org/sdsc-comet\" rel=\"nofollow\">comet</a> and\
    \ <a href=\"https://www.sdsc.edu/support/user_guides/tscc-quick-start.html\" rel=\"\
    nofollow\">TSCC</a> shared cluster at SDSC. It will automatically bind <code>/cvmfs</code>\
    \ <code>/oasis</code> <code>/projects</code> <code>/scratch</code> directories,\
    \ if available on the host.</p>\n<h2>\n<a id=\"user-content-using-the-container\"\
    \ class=\"anchor\" href=\"#using-the-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using the container</h2>\n<p>Pull\
    \ the singularity image:</p>\n<pre><code>singularity pull shub://nbcrrolls/electrostatics-singularity\n\
    </code></pre>\n<p>Start bash shell in the container:</p>\n<pre><code>singularity\
    \ shell nbcrrolls-electrostatics-singularity-master-latest.simg\n</code></pre>\n\
    <p>Now the container is running and we can start a BrownDye2 job (using the Thrombin\
    \ example):</p>\n<pre><code>module load browndye2\ncp -ai $BD2_PATH/examples/thrombin\
    \ .\ncd thrombin\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n_trajectories&gt; 10000\
    \ /&lt;n_trajectories&gt; 1000 /' t_m_simulation.xml.bak\nmake all # takes about\
    \ min to run\nmodule unload browndye2\n</code></pre>\n<p>And if you want to use\
    \ BrownDye version 1:</p>\n<pre><code>module load browndye1\ncp -ai $BD1_PATH/thrombin-example\
    \ .\ncd thrombin-example\nsed -i 's/-PE0//g' *\nsed -i 's/&lt;n-trajectories&gt;\
    \ 10000 /&lt;n-trajectories&gt; 1000 /' input.xml.bak # limit the number of calculated\
    \ trajectories\nmake all\nbd_top input.xml\nnam_simulation t-m-simulation.xml\
    \ # this takes about 3 min to run\ncat results.xml\nmodule unload browndye1\n\
    </code></pre>\n<p>After we are finished we can quit the container:</p>\n<pre><code>exit\n\
    </code></pre>\n<p>You can also access individual applications from the electrostatics\
    \ container.</p>\n<p>To list available applications:</p>\n<pre><code>$ singularity\
    \ apps nbcrrolls-electrostatics-singularity-master-latest.simg \napbs\npdb2pqr\n\
    nam_simulation\nwe_simulation\n</code></pre>\n<p>To run, for example, apbs calculation:</p>\n\
    <pre><code>singularity exec nbcrrolls-electrostatics-singularity-master-latest.simg\
    \ apbs input.in\n</code></pre>\n<p>or</p>\n<pre><code>singularity run --app apbs\
    \ nbcrrolls-electrostatics-singularity-master-latest.simg input.in\n</code></pre>\n\
    <p>This Singularity image is hosted on Singularity Hub: <a href=\"https://singularity-hub.org/collections/2497\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h6>\n<a id=\"user-content-this-project-is-supported-by-nbcr\"\
    \ class=\"anchor\" href=\"#this-project-is-supported-by-nbcr\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>This\
    \ project is supported by <a href=\"http://nbcr.ucsd.edu\" rel=\"nofollow\">NBCR</a>.</h6>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1556048171.0
nealplatt/sH_hybridization:
  data_format: 2
  description: null
  filenames:
  - config/Singularity
  - scripts/unused/Singularity
  - scripts/unused/Singularity_newhybrids
  full_name: nealplatt/sH_hybridization
  latest_release: v1.0
  readme: '<p><a href="https://zenodo.org/badge/latestdoi/124456755" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4641d504e79e577f2add43b190e60f3910a1688ac8f26f972d799fd6f3f4b213/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3132343435363735352e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/124456755.svg" style="max-width:100%;"></a>
    <a href="https://github.com/ambv/black"><img src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667"
    alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    class="anchor" href="#ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ancient
    hybridization and adaptive introgression of an invadolysin gene in <em>Schistosoma
    haematobium</em>.</h1>

    <p>Roy N. Platt II, Marina McDew-White, Winka Le Clec''h, Frederic D. Chevalier,
    Fiona Allan, Aidan M. Emery, Amadou Garba, Shaali M. Ame, Joanne P. Webster, David
    Rollinson, Bonnie L. Webster, Timothy J. C. Anderson.</p>

    <p>The parasitic blood fluke <em>Schistosoma</em> <em>haematobium</em> causes
    urogenital schistosomiasis in humans and is a major cause of morbidity and mortality
    across sub-Saharan Africa. <em>S</em>. <em>haematobium</em> can hybridize with
    closely-related livestock schistosomes, including <em>S</em>. <em>bovis</em>,
    however the frequency, direction, age and genomic consequences of hybridization
    in nature are unknown. We sequenced 96 <em>S</em>. <em>haematobium</em> exomes
    from Niger and the Zanzibar archipelago. We found evidence of an ancient, adaptive
    introgression event between Nigerien <em>S</em>. <em>haematobium</em> and <em>S</em>.
    <em>bovis</em> occurring 108-613 generations ago. Introgressed S. bovis alleles
    constitute 3.3-8.2% of Nigerien <em>S</em>. <em>haematobium</em> genomes. Some
    <em>S</em>. <em>bovis</em> alleles have reached high frequency and show signatures
    of directional selection; the strongest signal spans a single gene in the invadolysin
    gene family, an M8 metalloprotease associated with parasitic life-history traits.</p>

    <h4>

    <a id="user-content-biorxiv-pre-print" class="anchor" href="#biorxiv-pre-print"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="https://doi.org/10.1101/539353" rel="nofollow">bioRxiv pre-print</a>

    </h4>

    <hr>

    <h3>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTES:</h3>

    <p>All analyses were conducted on a HPCC in a <code>singularity</code> container
    or in a <code>conda</code> managed environment. The singularity recipe and conda
    environmental yaml are in the <code>config</code> dir.</p>

    <p>Raw code is found in the <code>scripts</code> dir</p>

    <p>Data that is not readily available through the SRA is in the <code>data</code>
    dir.  These will be housed in an online repository (ex. Dryad), but provided here
    for documentation purposes.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1618309153.0
nextgenusfs/funannotate:
  data_format: 2
  description: Eukaryotic Genome Annotation Pipeline
  filenames:
  - Singularity
  full_name: nextgenusfs/funannotate
  latest_release: v1.8.7
  readme: '<p><a href="https://github.com/nextgenusfs/funannotate/releases/latest"><img
    src="https://camo.githubusercontent.com/77ba51f3f201259675cc5dd53ada0a361559748add45bf62437f21ab6ba102a6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6e65787467656e757366732f66756e616e6e6f746174652e737667"
    alt="Latest Github release" data-canonical-src="https://img.shields.io/github/release/nextgenusfs/funannotate.svg"
    style="max-width:100%;"></a>

    <a href="https://zenodo.org/badge/latestdoi/48254740" rel="nofollow"><img src="https://camo.githubusercontent.com/60682e17fa8054e0d7609e1eccfc1226867f83f55d9f19a2d9ddf6aadb09ba70/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f34383235343734302e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/48254740.svg" style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/6dfb6879b08d70d126676bad7d9ba75f1600e35f7ffbe85772b833121a2d0897/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f62696f636f6e64612f66756e616e6e6f74617465"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/6dfb6879b08d70d126676bad7d9ba75f1600e35f7ffbe85772b833121a2d0897/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f62696f636f6e64612f66756e616e6e6f74617465"
    alt="Conda" data-canonical-src="https://img.shields.io/conda/dn/bioconda/funannotate"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/025770f53027c0679a57f4dcce8795b4fdc998bc94e79ea907db5b3e44cc321c/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f6e65787467656e757366732f66756e616e6e6f746174652f6c6174657374"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/025770f53027c0679a57f4dcce8795b4fdc998bc94e79ea907db5b3e44cc321c/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f6e65787467656e757366732f66756e616e6e6f746174652f6c6174657374"
    alt="Docker Image Size (tag)" data-canonical-src="https://img.shields.io/docker/image-size/nextgenusfs/funannotate/latest"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/528f0577834915a1e28ec7423f521cf14aac5496340487bf67cf4abacf31aad3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6e65787467656e757366732f66756e616e6e6f74617465"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/528f0577834915a1e28ec7423f521cf14aac5496340487bf67cf4abacf31aad3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6e65787467656e757366732f66756e616e6e6f74617465"
    alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/nextgenusfs/funannotate"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/5068" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p><a href="funannotate-logo.png?raw=true" target="_blank" rel="noopener noreferrer"><img
    src="funannotate-logo.png?raw=true" alt="Alt text" title="Funannotate" style="max-width:100%;"></a></p>

    <p>funannotate is a pipeline for genome annotation (built specifically for fungi,
    but will also work with higher eukaryotes). Installation, usage, and more information
    can be found at <a href="http://funannotate.readthedocs.io" rel="nofollow">http://funannotate.readthedocs.io</a></p>

    <h4>

    <a id="user-content-quickest-start-docker" class="anchor" href="#quickest-start-docker"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Quickest
    start Docker:</h4>

    <p>You can use docker to run <code>funannotate</code>. Caveats are that GeneMark
    is not included in the docker image (see licensing below and you can complain
    to the developers for making it difficult to distribute/use). I''ve also written
    a bash script that can run the docker image and auto-detect/include the proper
    user/volume bindings.  This docker image is built off of the latest code in master,
    so it will be ahead of the tagged releases. The image includes the required databases
    as well, if you want just funannotate without the databases then that is located
    on docker hub as well <code>nextgenusfs/funannotate-slim</code>. So this route
    can be achieved with:</p>

    <pre><code># download/pull the image from docker hub

    $ docker pull nextgenusfs/funannotate


    # download bash wrapper script (optional)

    $ wget -O funannotate-docker https://raw.githubusercontent.com/nextgenusfs/funannotate/master/funannotate-docker


    # might need to make this executable on your system

    $ chmod +x /path/to/funannotate-docker


    # assuming it is in your PATH, now you can run this script as if it were the funannotate
    executable script

    $ funannotate-docker test -t predict --cpus 12

    </code></pre>

    <h4>

    <a id="user-content-quickstart-bioconda-install" class="anchor" href="#quickstart-bioconda-install"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Quickstart
    Bioconda install:</h4>

    <p>The pipeline can be installed with conda (via <a href="https://bioconda.github.io/"
    rel="nofollow">bioconda</a>):</p>

    <pre><code>#add appropriate channels

    conda config --add channels defaults

    conda config --add channels bioconda

    conda config --add channels conda-forge


    #then create environment

    conda create -n funannotate funannotate

    </code></pre>

    <p>If <code>conda</code> is taking forever to solve the environment, I would recommend
    giving <a href="https://github.com/mamba-org/mamba">mamba</a> a try:</p>

    <pre><code>#install mamba into base environment

    conda install -n base mamba


    #then use mamba as drop in replacmeent

    mamba create -n funannotate funannotate

    </code></pre>

    <p>If you want to use GeneMark-ES/ET you will need to install that manually following
    developers instructions:

    <a href="http://topaz.gatech.edu/GeneMark/license_download.cgi" rel="nofollow">http://topaz.gatech.edu/GeneMark/license_download.cgi</a></p>

    <p>Note that you will need to change the shebang line for all perl scripts in
    GeneMark to use <code>/usr/bin/env perl</code>.

    You will then also need to add <code>gmes_petap.pl</code> to the $PATH or set
    the environmental variable $GENEMARK_PATH to the gmes_petap directory.</p>

    <p>To install just the python funannotate package, you can do this with pip:</p>

    <pre><code>python -m pip install funannotate

    </code></pre>

    <p>To install the most updated code in master you can run:</p>

    <pre><code>python -m pip install git+https://github.com/nextgenusfs/funannotate.git

    </code></pre>

    '
  stargazers_count: 157
  subscribers_count: 13
  topics:
  - genome-annotation
  - gene-models
  - comparative-genomics
  - ncbi-submission
  updated_at: 1622528541.0
nhoffman/dada2-nf:
  data_format: 2
  description: A Nextflow pipeline for processing 16S rRNA sequences using dada2
  filenames:
  - singularity/Singularity
  full_name: nhoffman/dada2-nf
  latest_release: null
  readme: '<h1>

    <a id="user-content-dada2-nextflow-pipeline" class="anchor" href="#dada2-nextflow-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dada2
    Nextflow pipeline</h1>

    <h2>

    <a id="user-content-local-execution-quickstart-for-the-truly-impatient" class="anchor"
    href="#local-execution-quickstart-for-the-truly-impatient" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Local execution quickstart
    for the truly impatient</h2>

    <p>Install Docker and make sure that the Docker daemon is running.</p>

    <p>Install the nextflow binary in this directory</p>

    <pre><code>wget -qO- https://get.nextflow.io | bash

    </code></pre>

    <p>Execute locally, using the minimal data set.</p>

    <pre><code>./nextflow run main.nf -params-file params-minimal.json

    </code></pre>

    <h2>

    <a id="user-content-execution-on-aws-batch" class="anchor" href="#execution-on-aws-batch"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execution
    on AWS Batch</h2>

    <p>Details will depend on your AWS batch configuration. General instructions TBD.</p>

    <h3>

    <a id="user-content-infernal-16s-filtering" class="anchor" href="#infernal-16s-filtering"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Infernal
    16s filtering</h3>

    <p>Coveriance model used for Infernal sequence filtering obtained from the Rfam
    database:</p>

    <p><a href="https://rfam.xfam.org/family/RF00177" rel="nofollow">https://rfam.xfam.org/family/RF00177</a></p>

    <p>To cite Rfam see latest web site instructions:</p>

    <p><a href="https://rfam.xfam.org/" rel="nofollow">https://rfam.xfam.org/</a></p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1617385346.0
nicspalla/openmpi_centos_x86_64:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: nicspalla/openmpi_centos_x86_64
  latest_release: null
  readme: '<h1>

    <a id="user-content-openmpi_centos_x86_64" class="anchor" href="#openmpi_centos_x86_64"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>openmpi_centos_x86_64</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605260984.0
nservant/HiC-Pro:
  data_format: 2
  description: 'HiC-Pro: An optimized and flexible pipeline for Hi-C data processing'
  filenames:
  - Singularity
  full_name: nservant/HiC-Pro
  latest_release: v3.0.0
  readme: "<h1>\n<a id=\"user-content-hic-pro\" class=\"anchor\" href=\"#hic-pro\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HiC-Pro</h1>\n<h3>\n<a id=\"user-content-an-optimized-and-flexible-pipeline-for-hi-c-data-processing\"\
    \ class=\"anchor\" href=\"#an-optimized-and-flexible-pipeline-for-hi-c-data-processing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>An optimized and flexible pipeline for Hi-C data processing</h3>\n\
    <p><a href=\"https://travis-ci.com/nservant/HiC-Pro\" rel=\"nofollow\"><img src=\"\
    https://camo.githubusercontent.com/73a86528f082d212ef4ce88c8e2cf0c58768f77ffcca0a20820f724928dd04f1/68747470733a2f2f7472617669732d63692e636f6d2f6e73657276616e742f4869432d50726f2e7376673f6272616e63683d646576656c5f707933\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/nservant/HiC-Pro.svg?branch=devel_py3\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://camo.githubusercontent.com/fa5e0167fc052efefc929acb2c1c877388fffa8a486163becac51ff00b7ccadc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e64612d6275696c642d627269676874677265656e2e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fa5e0167fc052efefc929acb2c1c877388fffa8a486163becac51ff00b7ccadc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e64612d6275696c642d627269676874677265656e2e737667\"\
    \ alt=\"Conda\" data-canonical-src=\"https://img.shields.io/badge/Conda-build-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a0a5d2fc20aeae0c34909c3cb3c096a3f2c0592c8e47dc0c0e9c6bb1f0014110/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53696e67756c61726974792d6275696c642d627269676874677265656e2e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a0a5d2fc20aeae0c34909c3cb3c096a3f2c0592c8e47dc0c0e9c6bb1f0014110/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53696e67756c61726974792d6275696c642d627269676874677265656e2e737667\"\
    \ alt=\"Singularity\" data-canonical-src=\"https://img.shields.io/badge/Singularity-build-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/repository/docker/nservant/hicpro\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a946c73292a5c2f469ea28505b0533d98efa33c02af4137ce82a41ad0a9d96c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f636b65722d6d616e75616c2d79656c6c6f772e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/badge/Docker-manual-yellow.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://camo.githubusercontent.com/5224ef8b916118d25eaf98ae2a4b258283c163a2354a008c7eed94dcba0635e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d756c746951432d312e382d626c75652e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5224ef8b916118d25eaf98ae2a4b258283c163a2354a008c7eed94dcba0635e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d756c746951432d312e382d626c75652e737667\"\
    \ alt=\"MultiQC\" data-canonical-src=\"https://img.shields.io/badge/MultiQC-1.8-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://groups.google.com/forum/#!forum/hic-pro\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1b9d743d52863763181e70393d3998508fea708ce52d4e2c6cb4c9d8d43f3d50/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47726f7570732d2532306a6f696e253230636861742532302545322538362539322d3466623939612e7376673f7374796c653d666c61742d737175617265\"\
    \ alt=\"Forum\" data-canonical-src=\"https://img.shields.io/badge/Groups-%20join%20chat%20%E2%86%92-4fb99a.svg?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://doi.org/10.1186/s13059-015-0831-x\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/27e7387e5cf05b69efa15a349127425e83ef45fd07d906c440f0b14d68df2d4e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f492d31302e313138362532467331333035392d2d3031352d2d303833312d2d782d6c69676874677265792e7376673f7374796c653d666c61742d737175617265\"\
    \ alt=\"DOI\" data-canonical-src=\"https://img.shields.io/badge/DOI-10.1186%2Fs13059--015--0831--x-lightgrey.svg?style=flat-square\"\
    \ style=\"max-width:100%;\"></a></p>\n<hr>\n<p>Find documentation and examples\
    \ at <a href=\"http://nservant.github.io/HiC-Pro/\" rel=\"nofollow\">http://nservant.github.io/HiC-Pro/</a></p>\n\
    <p>For any question about HiC-Pro, please contact <a href=\"mailto:nicolas.servant@curie.fr\"\
    >nicolas.servant@curie.fr</a> or use the <a href=\"https://groups.google.com/forum/#!forum/hic-pro\"\
    \ rel=\"nofollow\">HiC-Pro forum</a></p>\n<h2>\n<a id=\"user-content-what-is-hic-pro-\"\
    \ class=\"anchor\" href=\"#what-is-hic-pro-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>What is HiC-Pro ?</h2>\n<p>HiC-Pro\
    \ was designed to process Hi-C data, from raw fastq files (paired-end Illumina\
    \ data) to normalized contact maps. It supports the main Hi-C protocols, including\
    \ digestion protocols as well as protocols that do not require restriction enzymes\
    \ such as DNase Hi-C. In practice, HiC-Pro was successfully applied to many data-sets\
    \ including dilution Hi-C, in situ Hi-C, DNase Hi-C, Micro-C, capture-C, capture\
    \ Hi-C or HiChip data.<br>\nThe pipeline is flexible, scalable and optimized.\
    \ It can operate either on a single laptop or on a computational cluster. HiC-Pro\
    \ is sequential and each step of the workflow can be run independantly.<br>\n\
    HiC-Pro includes a fast implementatation of the iterative correction method (see\
    \ the <a href=\"https://github.com/hiclib/iced\">iced python package</a> for more\
    \ information).\nFinally, HiC-Pro can use phasing data to build <a href=\"doc/AS.md\"\
    >allele-specific contact maps</a>.</p>\n<p>If you use HiC-Pro, please cite :</p>\n\
    <p><em>Servant N., Varoquaux N., Lajoie BR., Viara E., Chen CJ., Vert JP., Dekker\
    \ J., Heard E., Barillot E.</em> HiC-Pro: An optimized and flexible pipeline for\
    \ Hi-C processing. Genome Biology 2015, 16:259 <a href=\"https://doi.org/10.1186/s13059-015-0831-x\"\
    \ rel=\"nofollow\">doi:10.1186/s13059-015-0831-x</a></p>\n<h2>\n<a id=\"user-content-containers\"\
    \ class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Containers</h2>\n<h3>\n<a id=\"\
    user-content-using-hic-pro-through-conda\" class=\"anchor\" href=\"#using-hic-pro-through-conda\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using HiC-Pro through <code>conda</code>\n</h3>\n<p>In order to ease\
    \ the installation of HiC-Pro dependancies, we provide a <code>yml</code> file\
    \ for conda with all required tools.\nIn order to build your conda environment,\
    \ first install <a href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"\
    nofollow\">miniconda</a> and use :</p>\n<pre><code>conda env create -f MY_INSTALL_PATH/HiC-Pro/environment.yml\
    \ -p WHERE_TO_INSTALL_MY_ENV\nconda activate WHERE_TO_INSTALL_MY_ENV\n</code></pre>\n\
    <h3>\n<a id=\"user-content-using-the-hic-pro-docker-image\" class=\"anchor\" href=\"\
    #using-the-hic-pro-docker-image\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Using the HiC-Pro <code>Docker</code>\
    \ image</h3>\n<p>A docker image is automatically build and available on <a href=\"\
    https://hub.docker.com/repository/docker/nservant/hicpro\" rel=\"nofollow\">Docker\
    \ Hub</a>\nTo pull a Docker image, simply use :</p>\n<pre><code>docker pull nservant/hicpro:latest\n\
    </code></pre>\n<p>Note that the <code>tag</code> may depend on the HiC-Pro version.</p>\n\
    <p>You can also build your own image from the root folder using</p>\n<pre><code>docker\
    \ build -t hicpro:3.0.0 .\n</code></pre>\n<h3>\n<a id=\"user-content-using-hic-pro-through-singularity\"\
    \ class=\"anchor\" href=\"#using-hic-pro-through-singularity\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using\
    \ HiC-Pro through <code>Singularity</code>\n</h3>\n<p>HiC-Pro provides a Singularity\
    \ container to ease its installation process.\nA ready-to-use container is available\
    \ <a href=\"https://zerkalo.curie.fr/partage/HiC-Pro/singularity_images/hicpro_latest_ubuntu.img\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>In order to build you own Singularity image;</p>\n\
    <p>1- Install singularity</p>\n<ul>\n<li>Linux : <a href=\"http://singularity.lbl.gov/install-linux\"\
    \ rel=\"nofollow\">http://singularity.lbl.gov/install-linux</a>\n</li>\n<li>MAC\
    \ : <a href=\"http://singularity.lbl.gov/install-mac\" rel=\"nofollow\">http://singularity.lbl.gov/install-mac</a>\n\
    </li>\n<li>Windows : <a href=\"http://singularity.lbl.gov/install-windows\" rel=\"\
    nofollow\">http://singularity.lbl.gov/install-windows</a>\n</li>\n</ul>\n<p>2-\
    \ Build the singularity HiC-Pro image using the 'Singularity' file available in\
    \ the HiC-Pro root directory.</p>\n<pre><code>sudo singularity build hicpro_latest_ubuntu.img\
    \ MY_INSTALL_PATH/HiC-Pro/envs/Singularity\n</code></pre>\n<p>3- Run HiC-pro</p>\n\
    <p>You can then either use HiC-Pro using the 'exec' command ;</p>\n<pre><code>singularity\
    \ exec hicpro_latest_ubuntu.img HiC-Pro -h\n</code></pre>\n<p>Or directly use\
    \ HiC-Pro within the Singularity shell</p>\n<pre><code>singularity shell hicpro_latest_ubuntu.img\n\
    HiC-Pro -h\n</code></pre>\n<h2>\n<a id=\"user-content-how-to-install-it-\" class=\"\
    anchor\" href=\"#how-to-install-it-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How to install it ?</h2>\n<p>The\
    \ HiC-Pro pipeline requires the following dependencies :</p>\n<ul>\n<li>The <a\
    \ href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\" rel=\"nofollow\"\
    >bowtie2</a> mapper</li>\n<li>Python (&gt;3.7) with <em>pysam (&gt;=0.15.4)</em>,\
    \ <em>bx-python(&gt;=0.8.8)</em>, <em>numpy(&gt;=1.18.1)</em>, and <em>scipy(&gt;=1.4.1)</em>\
    \ libraries.<br>\n<strong>Note that the current version no longer supports python\
    \ 2</strong>\n</li>\n<li>R with the <em>RColorBrewer</em> and <em>ggplot2 (&gt;2.2.1)</em>\
    \ packages</li>\n<li>g++ compiler</li>\n<li>samtools (&gt;1.9)</li>\n<li>Unix\
    \ sort (<strong>which support -V option</strong>) is required ! For Mac OS user,\
    \ please install the GNU core utilities !</li>\n</ul>\n<p>Note that Bowtie &gt;2.2.2\
    \ is strongly recommanded for allele specific analysis.</p>\n<p>To install HiC-Pro,\
    \ be sure to have the appropriate rights and run :</p>\n<pre><code>tar -zxvf HiC-Pro-master.tar.gz\n\
    cd HiC-Pro-master\n## Edit config-install.txt file if necessary\nmake configure\n\
    make install\n</code></pre>\n<p>Note that if some of these dependencies are not\
    \ installed (i.e. not detected in the $PATH), HiC-Pro will try to install them.<br>\n\
    You can also edit the <em>config-install.txt</em> file and manually defined the\
    \ paths to dependencies.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>SYSTEM CONFIGURATION</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>PREFIX</td>\n<td>Path to installation folder</td>\n\
    </tr>\n<tr>\n<td>BOWTIE2_PATH</td>\n<td>Full path the bowtie2 installation directory</td>\n\
    </tr>\n<tr>\n<td>SAMTOOLS_PATH</td>\n<td>Full path to the samtools installation\
    \ directory</td>\n</tr>\n<tr>\n<td>R_PATH</td>\n<td>Full path to the R installation\
    \ directory</td>\n</tr>\n<tr>\n<td>PYTHON_PATH</td>\n<td>Full path to the python\
    \ installation directory</td>\n</tr>\n<tr>\n<td>CLUSTER_SYS</td>\n<td>Scheduler\
    \ to use for cluster submission. Must be TORQUE, SGE, SLURM or LSF</td>\n</tr>\n\
    </tbody>\n</table>\n<h2>\n<a id=\"user-content-annotation-files\" class=\"anchor\"\
    \ href=\"#annotation-files\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Annotation Files</h2>\n<p>In order to process\
    \ the raw data, HiC-Pro requires three annotation files. Note that the pipeline\
    \ is provided with some Human and Mouse annotation files.<br>\n<strong>Please\
    \ be sure that the chromosome names are the same than the ones used in your bowtie\
    \ indexes !</strong></p>\n<ul>\n<li>\n<strong>A BED file</strong> of the restriction\
    \ fragments after digestion. This file depends both of the restriction enzyme\
    \ and the reference genome. See the <a href=\"doc/FAQ.md\">FAQ</a> and the <a\
    \ href=\"doc/UTILS.md\">HiC-Pro utilities</a> for details about how to generate\
    \ this file. A few annotation files are provided with the HiC-Pro sources as examples.</li>\n\
    </ul>\n<pre><code>   chr1   0       16007   HIC_chr1_1    0   +\n   chr1   16007\
    \   24571   HIC_chr1_2    0   +\n   chr1   24571   27981   HIC_chr1_3    0   +\n\
    \   chr1   27981   30429   HIC_chr1_4    0   +\n   chr1   30429   32153   HIC_chr1_5\
    \    0   +\n   chr1   32153   32774   HIC_chr1_6    0   +\n   chr1   32774   37752\
    \   HIC_chr1_7    0   +\n   chr1   37752   38369   HIC_chr1_8    0   +\n   chr1\
    \   38369   38791   HIC_chr1_9    0   +\n   chr1   38791   39255   HIC_chr1_10\
    \   0   +\n   (...)\n</code></pre>\n<ul>\n<li>\n<strong>A table file</strong>\
    \ of chromosomes' size. This file can be easily find on the UCSC genome browser.\
    \ Of note, pay attention to the contigs or scaffolds, and be aware that HiC-pro\
    \ will generate a map per chromosomes pair. For model organisms such as Human\
    \ or Mouse, which are well annotated, we usually recommand to remove all scaffolds.</li>\n\
    </ul>\n<pre><code>   chr1    249250621\n   chr2    243199373\n   chr3    198022430\n\
    \   chr4    191154276\n   chr5    180915260\n   chr6    171115067\n   chr7   \
    \ 159138663\n   chr8    146364022\n   chr9    141213431\n   chr10   135534747\n\
    \   (...)\n</code></pre>\n<ul>\n<li>\n<strong>The bowtie2 indexes</strong>. See\
    \ the <a href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\" rel=\"\
    nofollow\">bowtie2 manual page</a> for details about how to create such indexes.</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-how-to-use-it-\" class=\"anchor\" href=\"#how-to-use-it-\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to use it ?</h2>\n<p>First have a look at the help message !</p>\n\
    <pre><code>  HiC-Pro --help\n  usage : HiC-Pro -i INPUT -o OUTPUT -c CONFIG [-s\
    \ ANALYSIS_STEP] [-p] [-h] [-v]\n  Use option -h|--help for more information\n\
    \n  HiC-Pro 3.0.0\n  ---------------\n  OPTIONS\n\n   -i|--input INPUT : input\
    \ data folder; Must contains a folder per sample with input files\n   -o|--output\
    \ OUTPUT : output folder\n   -c|--conf CONFIG : configuration file for Hi-C processing\n\
    \   [-p|--parallel] : if specified run HiC-Pro on a cluster\n   [-s|--step ANALYSIS_STEP]\
    \ : run only a subset of the HiC-Pro workflow; if not specified the complete workflow\
    \ is run\n      mapping: perform reads alignment - require fast files\n      proc_hic:\
    \ perform Hi-C filtering - require BAM files\n      quality_checks: run Hi-C quality\
    \ control plots\n      merge_persample: merge multiple inputs and remove duplicates\
    \ if specified - require .validPairs files\n      build_contact_maps: Build raw\
    \ inter/intrachromosomal contact maps - require .allValidPairs files\n      ice_norm\
    \ : run ICE normalization on contact maps - require .matrix files\n   [-h|--help]:\
    \ help\n   [-v|--version]: version\n</code></pre>\n<ul>\n<li>\n<p>Copy and edit\
    \ the configuration file <em>'config-hicpro.txt'</em> in your local folder. See\
    \ the <a href=\"doc/MANUAL.md\">manual</a> for details about the configuration\
    \ file</p>\n</li>\n<li>\n<p>Put all input files in a rawdata folder. The input\
    \ files have to be organized with <strong>one folder per sample</strong>, such\
    \ as;</p>\n</li>\n</ul>\n<pre><code>   + PATH_TO_MY_DATA\n     + sample1\n   \
    \    ++ file1_R1.fastq.gz\n       ++ file1_R2.fastq.gz\n       ++ ...\n     +\
    \ sample2\n       ++ file1_R1.fastq.gz\n       ++ file1_R2.fastq.gz\n     *...\n\
    </code></pre>\n<ul>\n<li>Run HiC-Pro on your laptop in standalone model</li>\n\
    </ul>\n<pre><code>    MY_INSTALL_PATH/bin/HiC-Pro -i FULL_PATH_TO_DATA_FOLDER\
    \ -o FULL_PATH_TO_OUTPUTS -c MY_LOCAL_CONFIG_FILE\n</code></pre>\n<ul>\n<li>Run\
    \ HiC-Pro on a cluster (TORQUE/SGE/SLURM/LSF)</li>\n</ul>\n<pre><code>   MY_INSTALL_PATH/bin/HiC-Pro\
    \ -i FULL_PATH_TO_DATA_FOLDER -o FULL_PATH_TO_OUTPUTS -c MY_LOCAL_CONFIG_FILE\
    \ -p\n</code></pre>\n<p>In the latter case, you will have the following message\
    \ :</p>\n<pre><code>  Please run HiC-Pro in two steps :\n  1- The following command\
    \ will launch the parallel workflow through 12 torque jobs:\n  qsub HiCPro_step1.sh\n\
    \  2- The second command will merge all outputs to generate the contact maps:\n\
    \  qsub HiCPro_step2.sh\n</code></pre>\n<p>Execute the displayed command from\
    \ the output folder:</p>\n<pre><code>  qsub HiCPro_step1.sh\n</code></pre>\n<p>Once\
    \ executed succesfully (may take several hours), run the step using:</p>\n<pre><code>\
    \  qsub HiCPro_step2.sh\n</code></pre>\n<h2>\n<a id=\"user-content-test-dataset\"\
    \ class=\"anchor\" href=\"#test-dataset\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Test Dataset</h2>\n<p>The test\
    \ dataset and associated results are available <a href=\"https://zerkalo.curie.fr/partage/HiC-Pro/\"\
    \ rel=\"nofollow\">here</a>.\nSmall fastq files (2M reads) extracted from the\
    \ Dixon et al. 2012 paper are available for test.</p>\n<pre><code> ## Get the\
    \ data. Will download a test_data folder and a configuration file\n wget https://zerkalo.curie.fr/partage/HiC-Pro/HiCPro_testdata.tar.gz\
    \ &amp;&amp; tar -zxvf HiCPro_testdata.tar.gz\n\n ## Edit the configuration file\
    \ and set the path to Human bowtie2 indexes\n\n ## Run HiC-Pro\n time HICPRO_INSTALL_DIR/bin/HiC-Pro\
    \ -c config_test_latest.txt -i test_data -o hicpro_latest_test\n\nRun HiC-Pro\
    \ 3.0.0\n--------------------------------------------\nThu Mar 19, 12:18:10 (UTC+0100)\n\
    Bowtie2 alignment step1 ...\nLogs: logs/dixon_2M_2/mapping_step1.log\nLogs: logs/dixon_2M/mapping_step1.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:18:57 (UTC+0100)\n\
    Bowtie2 alignment step2 ...\nLogs: logs/dixon_2M_2/mapping_step2.log\nLogs: logs/dixon_2M/mapping_step2.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:19:08 (UTC+0100)\n\
    Combine R1/R2 alignment files ...\nLogs: logs/dixon_2M_2/mapping_combine.log\n\
    Logs: logs/dixon_2M/mapping_combine.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:19:13 (UTC+0100)\nMapping statistics for R1 and R2 tags ...\nLogs:\
    \ logs/dixon_2M_2/mapping_stats.log\nLogs: logs/dixon_2M/mapping_stats.log\n\n\
    --------------------------------------------\nThu Mar 19, 12:19:15 (UTC+0100)\n\
    Pairing of R1 and R2 tags ...\nLogs: logs/dixon_2M_2/mergeSAM.log\nLogs: logs/dixon_2M/mergeSAM.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:19:25 (UTC+0100)\n\
    Assign alignments to restriction fragments ...\nLogs: logs/dixon_2M_2/mapped_2hic_fragments.log\n\
    Logs: logs/dixon_2M/mapped_2hic_fragments.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:20:10 (UTC+0100)\nMerge chunks from the same sample ...\nLogs:\
    \ logs/dixon_2M/merge_valid_interactions.log\nLogs: logs/dixon_2M_2/merge_valid_interactions.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:20:11 (UTC+0100)\n\
    Merge stat files per sample ...\nLogs: logs/dixon_2M/merge_stats.log\nLogs: logs/dixon_2M_2/merge_stats.log\n\
    \n--------------------------------------------\nThu Mar 19, 12:20:11 (UTC+0100)\n\
    Run quality checks for all samples ...\nLogs: logs/dixon_2M/make_Rplots.log\n\
    Logs: logs/dixon_2M_2/make_Rplots.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:20:22 (UTC+0100)\nGenerate binned matrix files ...\nLogs: logs/dixon_2M/build_raw_maps.log\n\
    Logs: logs/dixon_2M_2/build_raw_maps.log\n\n--------------------------------------------\n\
    Thu Mar 19, 12:20:22 (UTC+0100)\nRun ICE Normalization ...\nLogs: logs/dixon_2M/ice_500000.log\n\
    Logs: logs/dixon_2M/ice_1000000.log\nLogs: logs/dixon_2M_2/ice_500000.log\nLogs:\
    \ logs/dixon_2M_2/ice_1000000.log\n\nreal\t2m15,736s\nuser\t4m3,277s\nsys\t0m24,423s\n\
    \n</code></pre>\n"
  stargazers_count: 241
  subscribers_count: 20
  topics: []
  updated_at: 1622545610.0
openPMD/openPMD-api:
  data_format: 2
  description: ':floppy_disk: C++ & Python API for Scientific I/O'
  filenames:
  - Singularity
  full_name: openPMD/openPMD-api
  latest_release: 0.13.4
  readme: "<h1>\n<a id=\"user-content-c--python-api-for-scientific-io-with-openpmd\"\
    \ class=\"anchor\" href=\"#c--python-api-for-scientific-io-with-openpmd\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>C++\
    \ &amp; Python API for Scientific I/O with openPMD</h1>\n<p><a href=\"https://github.com/openPMD/openPMD-standard/releases\"\
    ><img src=\"https://camo.githubusercontent.com/5957dc11cb68f6705ef9ef6683152c6c4fcc057a24dff340e1e8bada1bee0d01/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e504d442d312e302e302d2d312e312e302d626c7565\"\
    \ alt=\"Supported openPMD Standard\" data-canonical-src=\"https://img.shields.io/badge/openPMD-1.0.0--1.1.0-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.openpmd.org/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2d54fa5adcf7ca50d2cdb45823be5064379b613d526fa06f6e193ba2ce616318/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4150492d446f787967656e2d626c7565\"\
    \ alt=\"Doxygen\" data-canonical-src=\"https://img.shields.io/badge/API-Doxygen-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://gitter.im/openPMD/API\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/f551141eea2176c34aa163092549d6fdde9a220d605d6efe9128b024c6e5932b/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6f70656e504d442f415049\"\
    \ alt=\"Gitter chat\" data-canonical-src=\"https://img.shields.io/gitter/room/openPMD/API\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"Supported Platforms\" title=\"Supported Platforms\" data-canonical-src=\"\
    https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://www.gnu.org/licenses/lgpl-3.0.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4fc8091716dbd967fa2a82eef3d29227110e5081f3128aaa38663e547c24f812/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4c47504c76332d626c7565\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/license-LGPLv3-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://doi.org/10.14278/rodare.27\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/104f6901ae04bff8385acc8273bb276ab84656a927a418108b940d09fd6e5d7c/68747470733a2f2f726f646172652e687a64722e64652f62616467652f444f492f31302e31343237382f726f646172652e32372e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://rodare.hzdr.de/badge/DOI/10.14278/rodare.27.svg\"\
    \ style=\"max-width:100%;\"></a><br>\n<a href=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0d568047fbbd300af56b766d9a4235a20534485f5827194e859d4f5c20e58334/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6f70656e706d642f6f70656e706d642d6170692f6261646765\"\
    \ alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api/badge\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:cpp\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/63a7f9e783999e3afc03ef38ee82e2048017e4e6d279ff4120ad8b8718480ccd/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6370702f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: C/C++\" data-canonical-src=\"https://img.shields.io/lgtm/grade/cpp/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:python\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5046bf66a4612476a030d38de817c23fa03990183d2d74fa92c5f1379feb5d09/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: Python\" data-canonical-src=\"https://img.shields.io/lgtm/grade/python/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/alerts/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/85e32deb8face392eea9bfa2be4da4c11ca7c0f834fa069223fbc63758b68c4f/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f616c657274732f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\"\
    \ alt=\"LGTM: Total alerts\" data-canonical-src=\"https://img.shields.io/lgtm/alerts/g/openPMD/openPMD-api?logo=lgtm&amp;logoWidth=18\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://coveralls.io/github/openPMD/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/f0487a8fc14d269210ae8ce80b556d4afcd93684f02e61386d2d02daa4423cbd/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6f70656e504d442f6f70656e504d442d6170692f6261646765\"\
    \ alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/openPMD/openPMD-api/badge\"\
    \ style=\"max-width:100%;\"></a><br>\n<a href=\"https://openpmd-api.readthedocs.io/en/latest/?badge=latest\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8f438ca4eaa308f982d0a28c48f05bf63ca1a86e52c3d2817f6d7559d1dee68e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6f70656e706d642d6170692f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/openpmd-api/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.com/openPMD/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8278d89e3634e25a818a2d673fe997c2aa5a09fd5995f716aeffa743388030ee/68747470733a2f2f7472617669732d63692e636f6d2f6f70656e504d442f6f70656e504d442d6170692e7376673f6272616e63683d646576\"\
    \ alt=\"Linux/OSX Build Status dev\" data-canonical-src=\"https://travis-ci.com/openPMD/openPMD-api.svg?branch=dev\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://ci.appveyor.com/project/ax3l/openpmd-api/branch/dev\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/30679331f3c6a5ae54970eda85f36a30347dee8a9111448d7aa48587fd524a1d/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f78393571346e36323070716b306530742f6272616e63682f6465763f7376673d74727565\"\
    \ alt=\"Windows Build Status dev\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/x95q4n620pqk0e0t/branch/dev?svg=true\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/openPMD/openPMD-api/actions?query=workflow%3Awheels\"\
    ><img src=\"https://github.com/openPMD/openPMD-api/workflows/wheels/badge.svg?branch=wheels&amp;event=push\"\
    \ alt=\"PyPI Wheel Release\" style=\"max-width:100%;\"></a>\n<a href=\"https://dev.azure.com/axelhuebl/openPMD-api/_build/latest?definitionId=1&amp;branchName=azure_install\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1fc9c860bb1fc8e7e145ea9dd6723b9ac6ac2743c195e5e899f6cfa3d38a5a69/68747470733a2f2f6465762e617a7572652e636f6d2f6178656c687565626c2f6f70656e504d442d6170692f5f617069732f6275696c642f7374617475732f6f70656e504d442e6f70656e504d442d6170693f6272616e63684e616d653d617a7572655f696e7374616c6c266c6162656c3d6e696768746c792532307061636b61676573\"\
    \ alt=\"Nightly Packages Status\" data-canonical-src=\"https://dev.azure.com/axelhuebl/openPMD-api/_apis/build/status/openPMD.openPMD-api?branchName=azure_install&amp;label=nightly%20packages\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://scan.coverity.com/projects/openpmd-openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0b068d7b5718aafccb46e2ee35f8249938ef189a36ba353c15222ed5e6f65e49/68747470733a2f2f7363616e2e636f7665726974792e636f6d2f70726f6a656374732f31373630322f62616467652e737667\"\
    \ alt=\"Coverity Scan Build Status\" data-canonical-src=\"https://scan.coverity.com/projects/17602/badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>openPMD is an open meta-data schema that\
    \ provides meaning and self-description for data sets in science and engineering.\n\
    See <a href=\"https://github.com/openPMD/openPMD-standard\">the openPMD standard</a>\
    \ for details of this schema.</p>\n<p>This library provides a reference API for\
    \ openPMD data handling.\nSince openPMD is a schema (or markup) on top of portable,\
    \ hierarchical file formats, this library implements various backends such as\
    \ HDF5, ADIOS1, ADIOS2 and JSON.\nWriting &amp; reading through those backends\
    \ and their associated files is supported for serial and <a href=\"https://www.mpi-forum.org/docs/\"\
    \ rel=\"nofollow\">MPI-parallel</a> workflows.</p>\n<h2>\n<a id=\"user-content-usage\"\
    \ class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-c\"\
    \ class=\"anchor\" href=\"#c\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>C++</h3>\n<p><a href=\"https://isocpp.org/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/042b5af19c304a2d4f876865d00baa90a284f2d35056ed9728c944befbb07733/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231342d79656c6c6f77677265656e\"\
    \ alt=\"C++14\" title=\"C++14 API\" data-canonical-src=\"https://img.shields.io/badge/language-C%2B%2B14-yellowgreen\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ alt=\"C++14 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-c++\"\
    ><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"\
    pl-pds\">&lt;</span>openPMD/openPMD.hpp<span class=\"pl-pds\">&gt;</span></span>\n\
    #<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"\
    pl-c\"><span class=\"pl-c\">//</span> ...</span>\n\n<span class=\"pl-k\">auto</span>\
    \ s = openPMD::Series(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>samples/git-sample/data%T.h5<span\
    \ class=\"pl-pds\">\"</span></span>, openPMD::Access::READ_ONLY);\n\n<span class=\"\
    pl-k\">for</span>( <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp;\
    \ i : s.iterations ) {\n    std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>Iteration: <span class=\"pl-pds\">\"</span></span> &lt;&lt;\
    \ i.<span class=\"pl-smi\">first</span> &lt;&lt; <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>;\n\
    \n    <span class=\"pl-k\">for</span>( <span class=\"pl-k\">auto</span> <span\
    \ class=\"pl-k\">const</span>&amp; m : i.<span class=\"pl-smi\">second</span>.<span\
    \ class=\"pl-smi\">meshes</span> ) {\n        std::cout &lt;&lt; <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>  Mesh '<span class=\"pl-pds\">\"</span></span>\
    \ &lt;&lt; m.<span class=\"pl-smi\">first</span> &lt;&lt; <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>' attributes:<span class=\"pl-cce\">\\n</span><span\
    \ class=\"pl-pds\">\"</span></span>;\n        <span class=\"pl-k\">for</span>(\
    \ <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp; val\
    \ : m.<span class=\"pl-smi\">second</span>.<span class=\"pl-c1\">attributes</span>()\
    \ )\n            std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>    <span class=\"pl-pds\">\"</span></span> &lt;&lt; val &lt;&lt; <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span\
    \ class=\"pl-pds\">'</span></span>;\n    }\n\n    <span class=\"pl-k\">for</span>(\
    \ <span class=\"pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp; p :\
    \ i.<span class=\"pl-smi\">second</span>.<span class=\"pl-smi\">particles</span>\
    \ ) {\n        std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>  Particle species '<span class=\"pl-pds\">\"</span></span> &lt;&lt;\
    \ p.<span class=\"pl-smi\">first</span> &lt;&lt; <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>' attributes:<span class=\"pl-cce\">\\n</span><span class=\"\
    pl-pds\">\"</span></span>;\n        <span class=\"pl-k\">for</span>( <span class=\"\
    pl-k\">auto</span> <span class=\"pl-k\">const</span>&amp; val : p.<span class=\"\
    pl-smi\">second</span>.<span class=\"pl-c1\">attributes</span>() )\n         \
    \   std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> \
    \   <span class=\"pl-pds\">\"</span></span> &lt;&lt; val &lt;&lt; <span class=\"\
    pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"\
    pl-pds\">'</span></span>;\n    }\n}</pre></div>\n<h3>\n<a id=\"user-content-python\"\
    \ class=\"anchor\" href=\"#python\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Python</h3>\n<p><a href=\"https://www.python.org/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/acd285afab5d7ddd4942e5215ade53e84551c9d7d635642ba92c19fde7d4345b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e332d79656c6c6f77677265656e\"\
    \ alt=\"Python3\" title=\"Python3 API\" data-canonical-src=\"https://img.shields.io/badge/language-Python3-yellowgreen\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\"\
    \ alt=\"Python3 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">openpmd_api</span>\
    \ <span class=\"pl-k\">as</span> <span class=\"pl-s1\">io</span>\n\n<span class=\"\
    pl-c\"># ...</span>\n\n<span class=\"pl-s1\">series</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-s1\">io</span>.<span class=\"pl-v\">Series</span>(<span\
    \ class=\"pl-s\">\"samples/git-sample/data%T.h5\"</span>, <span class=\"pl-s1\"\
    >io</span>.<span class=\"pl-v\">Access</span>.<span class=\"pl-s1\">read_only</span>)\n\
    \n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">k_i</span>, <span class=\"\
    pl-s1\">i</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">series</span>.<span\
    \ class=\"pl-s1\">iterations</span>.<span class=\"pl-en\">items</span>():\n  \
    \  <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"Iteration: {0}\"\
    </span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">k_i</span>))\n\
    \n    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">k_m</span>, <span\
    \ class=\"pl-s1\">m</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\"\
    >i</span>.<span class=\"pl-s1\">meshes</span>.<span class=\"pl-en\">items</span>():\n\
    \        <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"  Mesh '{0}'\
    \ attributes:\"</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\"\
    >k_m</span>))\n        <span class=\"pl-k\">for</span> <span class=\"pl-s1\">a</span>\
    \ <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">m</span>.<span class=\"\
    pl-s1\">attributes</span>:\n            <span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s\">\"    {0}\"</span>.<span class=\"pl-en\">format</span>(<span\
    \ class=\"pl-s1\">a</span>))\n\n    <span class=\"pl-k\">for</span> <span class=\"\
    pl-s1\">k_p</span>, <span class=\"pl-s1\">p</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">i</span>.<span class=\"pl-s1\">particles</span>.<span\
    \ class=\"pl-en\">items</span>():\n        <span class=\"pl-en\">print</span>(<span\
    \ class=\"pl-s\">\"  Particle species '{0}' attributes:\"</span>.<span class=\"\
    pl-en\">format</span>(<span class=\"pl-s1\">k_p</span>))\n        <span class=\"\
    pl-k\">for</span> <span class=\"pl-s1\">a</span> <span class=\"pl-c1\">in</span>\
    \ <span class=\"pl-s1\">p</span>.<span class=\"pl-s1\">attributes</span>:\n  \
    \          <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"    {0}\"\
    </span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s1\">a</span>))</pre></div>\n\
    <h3>\n<a id=\"user-content-more\" class=\"anchor\" href=\"#more\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>More!</h3>\n\
    <p>Curious?\nOur manual shows full <a href=\"https://openpmd-api.readthedocs.io/en/latest/usage/firstwrite.html\"\
    \ rel=\"nofollow\">read &amp; write examples</a>, both serial and MPI-parallel!</p>\n\
    <h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<p>Required:</p>\n<ul>\n<li>CMake 3.15.0+</li>\n\
    <li>C++14 capable compiler, e.g. g++ 5.0+, clang 5.0+, VS 2017+</li>\n</ul>\n\
    <p>Shipped internally in <code>share/openPMD/thirdParty/</code>:</p>\n<ul>\n<li>\n\
    <a href=\"https://github.com/mpark/variant\">MPark.Variant</a> 1.4.0+ (<a href=\"\
    https://github.com/mpark/variant/blob/master/LICENSE.md\">BSL-1.0</a>)</li>\n\
    <li>\n<a href=\"https://github.com/catchorg/Catch2\">Catch2</a> 2.13.4+ (<a href=\"\
    https://github.com/catchorg/Catch2/blob/master/LICENSE.txt\">BSL-1.0</a>)</li>\n\
    <li>\n<a href=\"https://github.com/pybind/pybind11\">pybind11</a> 2.6.2+ (<a href=\"\
    https://github.com/pybind/pybind11/blob/master/LICENSE\">new BSD</a>)</li>\n<li>\n\
    <a href=\"https://github.com/nlohmann/json\">NLohmann-JSON</a> 3.9.1+ (<a href=\"\
    https://github.com/nlohmann/json/blob/develop/LICENSE.MIT\">MIT</a>)</li>\n</ul>\n\
    <p>I/O backends:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/JSON\"\
    \ rel=\"nofollow\">JSON</a></li>\n<li>\n<a href=\"https://support.hdfgroup.org/HDF5\"\
    \ rel=\"nofollow\">HDF5</a> 1.8.13+ (optional)</li>\n<li>\n<a href=\"https://www.olcf.ornl.gov/center-projects/adios\"\
    \ rel=\"nofollow\">ADIOS1</a> 1.13.1+ (optional)</li>\n<li>\n<a href=\"https://github.com/ornladios/ADIOS2\"\
    >ADIOS2</a> 2.7.0+ (optional)</li>\n</ul>\n<p>while those can be built either\
    \ with or without:</p>\n<ul>\n<li>MPI 2.1+, e.g. OpenMPI 1.6.5+ or MPICH2</li>\n\
    </ul>\n<p>Optional language bindings:</p>\n<ul>\n<li>Python:\n<ul>\n<li>Python\
    \ 3.6 - 3.9</li>\n<li>pybind11 2.6.2+</li>\n<li>numpy 1.15+</li>\n<li>mpi4py 2.1+\
    \ (optional, for MPI)</li>\n<li>pandas 1.0+ (optional, for dataframes)</li>\n\
    <li>dask 2021+ (optional, for dask dataframes)</li>\n</ul>\n</li>\n</ul>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p><a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/580cdb6331254f66680bd967326d9630f5124291efd5ace18549fbb93cbae6db/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737061636b2e696f2d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Spack Package\" data-canonical-src=\"https://img.shields.io/badge/spack.io-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3c3728308a9e416589fa8b3b8168967287c515037bfbd4b40f1b14f266de56fb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612e696f2d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Conda Package\" data-canonical-src=\"https://img.shields.io/badge/conda.io-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/openPMD/homebrew-openPMD\"\
    ><img src=\"https://camo.githubusercontent.com/92b7b7bb69b2b17d1981ae5fdcdb32f971ee57f54a98ea4804e93fd0a2eb58ef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772e73682d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"Brew Package\" data-canonical-src=\"https://img.shields.io/badge/brew.sh-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4eeb2c48c0e554ea8dbe8e90f73a6f2d217e775e0bd5e5cb90ddf4619ef3a6a0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707970692e6f72672d6f70656e706d642d2d6170692d627269676874677265656e\"\
    \ alt=\"PyPI Package\" data-canonical-src=\"https://img.shields.io/badge/pypi.org-openpmd--api-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://cmake.org\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/2babf79954ea524c24f2f9b1cfa05728fdaccbe0a80c2da6a7a7595cc131894c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66726f6d5f736f757263652d434d616b652d627269676874677265656e\"\
    \ alt=\"From Source\" data-canonical-src=\"https://img.shields.io/badge/from_source-CMake-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Our community loves to help each other.\n\
    Please <a href=\"https://github.com/openPMD/openPMD-api/issues/new?labels=install&amp;template=install_problem.md\"\
    >report installation problems</a> in case you should get stuck.</p>\n<p>Choose\
    \ <em>one</em> of the install methods below to get started:</p>\n<h3>\n<a id=\"\
    user-content-spack\" class=\"anchor\" href=\"#spack\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://spack.io\"\
    \ rel=\"nofollow\">Spack</a>\n</h3>\n<p><a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/becffbecb6a50502286f02ec86c4e62f239f3671148d11341152e0dc695a2fc9/68747470733a2f2f696d672e736869656c64732e696f2f737061636b2f762f6f70656e706d642d617069\"\
    \ alt=\"Spack Version\" data-canonical-src=\"https://img.shields.io/spack/v/openpmd-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://spack.io\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\"\
    \ alt=\"Spack Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b29fc54abf92a2a6d1d2c70159eb276df53b67a1294ae3f82b1b0bf22a3c586b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392c5f646576656c6f706d656e742c5f4850432d627269676874677265656e\"\
    \ alt=\"Spack Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29,_development,_HPC-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:           \
    \    +python +adios1 -adios2 -hdf5 -mpi</span>\nspack install openpmd-api\nspack\
    \ load -r openpmd-api</pre></div>\n<h3>\n<a id=\"user-content-conda\" class=\"\
    anchor\" href=\"#conda\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"https://conda.io\" rel=\"nofollow\"\
    >Conda</a>\n</h3>\n<p><a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3ad2b345bb3589aa2d733ca20df72938ed54a48342b69f7cbe9eacab43d84549/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f6f70656e706d642d617069\"\
    \ alt=\"Conda Version\" data-canonical-src=\"https://img.shields.io/conda/vn/conda-forge/openpmd-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"Conda Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\"\
    \ alt=\"Conda Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/conda-forge/openpmd-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a51d3cd2bfa3c6b01bd0f2e36abc206fb9db5700105fac2acd2c2105fced2ec4/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f636f6e64612d666f7267652f6f70656e706d642d617069\"\
    \ alt=\"Conda Downloads\" data-canonical-src=\"https://img.shields.io/conda/dn/conda-forge/openpmd-api\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:           \
    \           OpenMPI support  =*=mpi_openmpi*</span>\n<span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> optional:                        MPICH support  =*=mpi_mpich*</span>\n\
    conda create -n openpmd -c conda-forge openpmd-api\nconda activate openpmd</pre></div>\n\
    <h3>\n<a id=\"user-content-brew\" class=\"anchor\" href=\"#brew\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"https://brew.sh\" rel=\"nofollow\">Brew</a>\n</h3>\n<p><a href=\"https://github.com/openPMD/homebrew-openPMD\"\
    ><img src=\"https://camo.githubusercontent.com/d873c8c473fece8abf1c34a8299a50b7ee0da9772eb50cce8649e7ca32468a6c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772d6c61746573745f76657273696f6e2d6f72616e6765\"\
    \ alt=\"Brew Version\" data-canonical-src=\"https://img.shields.io/badge/brew-latest_version-orange\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://docs.brew.sh/Homebrew-on-Linux\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\"\
    \ alt=\"Brew Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://brew.sh\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/7b767b67facc26db7d850ae5c3155fa949048991dde7a0f5471c1e6862f0a586/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392d627269676874677265656e\"\
    \ alt=\"Brew Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>brew tap openpmd/openpmd\nbrew install openpmd-api</pre></div>\n<h3>\n<a\
    \ id=\"user-content-pypi\" class=\"anchor\" href=\"#pypi\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"\
    https://pypi.org\" rel=\"nofollow\">PyPI</a>\n</h3>\n<p><a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fc28bd368523d0b8adab5f4b414aebb304743130eef42bca203f4e9eed428ae7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f70656e504d442d617069\"\
    \ alt=\"PyPI Version\" data-canonical-src=\"https://img.shields.io/pypi/v/openPMD-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api/#files\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\"\
    \ alt=\"PyPI Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\"\
    \ alt=\"PyPI Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7cef25e887c028f65d5d7e20f3e7abe394ab328ecb499e34e47460afd2c78558/68747470733a2f2f696d672e736869656c64732e696f2f707970692f666f726d61742f6f70656e504d442d617069\"\
    \ alt=\"PyPI Format\" data-canonical-src=\"https://img.shields.io/pypi/format/openPMD-api\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/openPMD-api\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/82acdd95515851a5ba4a3006871151f76cfe4d6583f6c24e80bd0fd29d391845/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6f70656e504d442d617069\"\
    \ alt=\"PyPI Downloads\" data-canonical-src=\"https://img.shields.io/pypi/dm/openPMD-api\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>On very old macOS versions (&lt;10.9)\
    \ or on exotic processor architectures, this install method <em>compiles from\
    \ source</em> against the found installations of HDF5, ADIOS1, ADIOS2, and/or\
    \ MPI (in system paths, from other package managers, or loaded via a module system,\
    \ ...).</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> we need pip 19 or newer</span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> optional:                   --user</span>\n\
    python3 -m pip install -U pip\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ optional:                        --user</span>\npython3 -m pip install openpmd-api</pre></div>\n\
    <p>If MPI-support shall be enabled, we always have to recompile:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> optional:                                    --user</span>\npython3\
    \ -m pip install -U pip setuptools wheel\npython3 -m pip install -U cmake\n\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> optional:                 \
    \                                                  --user</span>\nopenPMD_USE_MPI=ON\
    \ python3 -m pip install openpmd-api --no-binary openpmd-api</pre></div>\n<p>For\
    \ some exotic architectures and compilers, you might need to disable a compiler\
    \ feature called <a href=\"https://en.wikipedia.org/wiki/Interprocedural_optimization\"\
    \ rel=\"nofollow\">link-time/interprocedural optimization</a> if you encounter\
    \ linking problems:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-k\">export</span> CMAKE_INTERPROCEDURAL_OPTIMIZATION=OFF\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> optional:                               \
    \                 --user</span>\npython3 -m pip install openpmd-api --no-binary\
    \ openpmd-api</pre></div>\n<h3>\n<a id=\"user-content-from-source\" class=\"anchor\"\
    \ href=\"#from-source\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>From Source</h3>\n<p><a href=\"https://cmake.org\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0a40953107090abff3b564ec3436668756b873cd4d9e021738a046781a5a0e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d646576656c6f706d656e742d627269676874677265656e\"\
    \ alt=\"Source Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-development-brightgreen\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>openPMD-api can also be built and installed\
    \ from source using <a href=\"https://cmake.org/\" rel=\"nofollow\">CMake</a>:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/openPMD/openPMD-api.git\n\
    \nmkdir openPMD-api-build\n<span class=\"pl-c1\">cd</span> openPMD-api-build\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optional: for full tests,\
    \ with unzip</span>\n../openPMD-api/share/openPMD/download_samples.sh\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> for own install prefix append:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DCMAKE_INSTALL_PREFIX=$HOME/somepath</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for options append:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DopenPMD_USE_...=...</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> e.g. for python support add:</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>   -DopenPMD_USE_PYTHON=ON</span>\n\
    cmake ../openPMD-api\n\ncmake --build <span class=\"pl-c1\">.</span>\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> optional</span>\nctest\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> sudo might be required for system\
    \ paths</span>\ncmake --build <span class=\"pl-c1\">.</span> --target install</pre></div>\n\
    <p>The following options can be added to the <code>cmake</code> call to control\
    \ features.\nCMake controls options with prefixed <code>-D</code>, e.g. <code>-DopenPMD_USE_MPI=OFF</code>:</p>\n\
    <table>\n<thead>\n<tr>\n<th>CMake Option</th>\n<th>Values</th>\n<th>Description</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td><code>openPMD_USE_MPI</code></td>\n<td>\n\
    <strong>AUTO</strong>/ON/OFF</td>\n<td>Parallel, Multi-Node I/O for clusters</td>\n\
    </tr>\n<tr>\n<td><code>openPMD_USE_HDF5</code></td>\n<td>\n<strong>AUTO</strong>/ON/OFF</td>\n\
    <td>HDF5 backend (<code>.h5</code> files)</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_ADIOS1</code></td>\n\
    <td>\n<strong>AUTO</strong>/ON/OFF</td>\n<td>ADIOS1 backend (<code>.bp</code>\
    \ files up to version BP3)</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_ADIOS2</code></td>\n\
    <td>\n<strong>AUTO</strong>/ON/OFF</td>\n<td>ADIOS2 backend (<code>.bp</code>\
    \ files in BP3, BP4 or higher)</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_PYTHON</code></td>\n\
    <td>\n<strong>AUTO</strong>/ON/OFF</td>\n<td>Enable Python bindings</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_INVASIVE_TESTS</code></td>\n<td>ON/<strong>OFF</strong>\n\
    </td>\n<td>Enable unit tests that modify source code <sup>1</sup>\n</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_VERIFY</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Enable internal VERIFY (assert) macro independent of build type <sup>2</sup>\n\
    </td>\n</tr>\n<tr>\n<td><code>openPMD_INSTALL</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Add installation targets</td>\n</tr>\n<tr>\n<td><code>Python_EXECUTABLE</code></td>\n\
    <td>(newest found)</td>\n<td>Path to Python executable</td>\n</tr>\n</tbody>\n\
    </table>\n<p><sup>1</sup> <em>e.g. changes C++ visibility keywords, breaks MSVC</em>\n\
    <sup>2</sup> <em>this includes most pre-/post-condition checks, disabling without\
    \ specific cause is highly discouraged</em></p>\n<p>Additionally, the following\
    \ libraries are shipped internally.\nThe following options allow to switch to\
    \ external installs:</p>\n<table>\n<thead>\n<tr>\n<th>CMake Option</th>\n<th>Values</th>\n\
    <th>Library</th>\n<th>Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>openPMD_USE_INTERNAL_VARIANT</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>MPark.Variant</td>\n<td>1.4.0+</td>\n\
    </tr>\n<tr>\n<td><code>openPMD_USE_INTERNAL_CATCH</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Catch2</td>\n<td>2.13.4+</td>\n</tr>\n<tr>\n<td><code>openPMD_USE_INTERNAL_PYBIND11</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>pybind11</td>\n<td>2.6.2+</td>\n</tr>\n\
    <tr>\n<td><code>openPMD_USE_INTERNAL_JSON</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>NLohmann-JSON</td>\n<td>3.9.1+</td>\n</tr>\n</tbody>\n</table>\n<p>By default,\
    \ this will build as a shared library (<code>libopenPMD.[so|dylib|dll]</code>)\
    \ and installs also its headers.\nIn order to build a static library, append <code>-DBUILD_SHARED_LIBS=OFF</code>\
    \ to the <code>cmake</code> command.\nYou can only build a static or a shared\
    \ library at a time.</p>\n<p>By default, the <code>Release</code> version is built.\n\
    In order to build with debug symbols, pass <code>-DCMAKE_BUILD_TYPE=Debug</code>\
    \ to your <code>cmake</code> command.</p>\n<p>By default, tests, examples and\
    \ command line tools are built.\nIn order to skip building those, pass <code>OFF</code>\
    \ to these <code>cmake</code> options:</p>\n<table>\n<thead>\n<tr>\n<th>CMake\
    \ Option</th>\n<th>Values</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td><code>openPMD_BUILD_TESTING</code></td>\n<td>\n<strong>ON</strong>/OFF</td>\n\
    <td>Build tests</td>\n</tr>\n<tr>\n<td><code>openPMD_BUILD_EXAMPLES</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Build examples</td>\n</tr>\n<tr>\n<td><code>openPMD_BUILD_CLI_TOOLS</code></td>\n\
    <td>\n<strong>ON</strong>/OFF</td>\n<td>Build command-line tools</td>\n</tr>\n\
    </tbody>\n</table>\n<h2>\n<a id=\"user-content-linking-to-your-project\" class=\"\
    anchor\" href=\"#linking-to-your-project\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Linking to your project</h2>\n\
    <p>The install will contain header files and libraries in the path set with <code>-DCMAKE_INSTALL_PREFIX</code>.</p>\n\
    <h3>\n<a id=\"user-content-cmake\" class=\"anchor\" href=\"#cmake\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CMake</h3>\n\
    <p>If your project is using CMake for its build, one can conveniently use our\
    \ provided <code>openPMDConfig.cmake</code> package which is installed alongside\
    \ the library.</p>\n<p>First set the following environment hint if openPMD-api\
    \ was <em>not</em> installed in a system path:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> optional: only needed\
    \ if installed outside of system paths</span>\n<span class=\"pl-k\">export</span>\
    \ CMAKE_PREFIX_PATH=<span class=\"pl-smi\">$HOME</span>/somepath:<span class=\"\
    pl-smi\">$CMAKE_PREFIX_PATH</span></pre></div>\n<p>Use the following lines in\
    \ your project's <code>CMakeLists.txt</code>:</p>\n<div class=\"highlight highlight-source-cmake\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> supports:           \
    \            COMPONENTS MPI NOMPI HDF5 ADIOS1 ADIOS2</span>\n<span class=\"pl-c1\"\
    >find_package</span>(openPMD 0.9.0 <span class=\"pl-k\">CONFIG</span>)\n\n<span\
    \ class=\"pl-k\">if</span>(openPMD_FOUND)\n    <span class=\"pl-c1\">target_link_libraries</span>(YourTarget\
    \ <span class=\"pl-k\">PRIVATE</span> openPMD::openPMD)\n<span class=\"pl-k\"\
    >endif</span>()</pre></div>\n<p><em>Alternatively</em>, add the openPMD-api repository\
    \ source directly to your project and use it via:</p>\n<div class=\"highlight\
    \ highlight-source-cmake\"><pre><span class=\"pl-c1\">add_subdirectory</span>(<span\
    \ class=\"pl-s\">\"path/to/source/of/openPMD-api\"</span>)\n\n<span class=\"pl-c1\"\
    >target_link_libraries</span>(YourTarget <span class=\"pl-k\">PRIVATE</span> openPMD::openPMD)</pre></div>\n\
    <p>For development workflows, you can even automatically download and build openPMD-api\
    \ from within a depending CMake project.\nJust replace the <code>add_subdirectory</code>\
    \ call with:</p>\n<div class=\"highlight highlight-source-cmake\"><pre><span class=\"\
    pl-c1\">include</span>(FetchContent)\n<span class=\"pl-c1\">set</span>(CMAKE_POLICY_DEFAULT_CMP0077\
    \ <span class=\"pl-k\">NEW</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_CLI_TOOLS\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_EXAMPLES\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c1\">set</span>(openPMD_BUILD_TESTING\
    \ <span class=\"pl-k\">OFF</span>)\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> set(openPMD_BUILD_SHARED_LIBS OFF)  # precedence over BUILD_SHARED_LIBS\
    \ if needed; or:</span>\n<span class=\"pl-c1\">set</span>(openPMD_INSTALL <span\
    \ class=\"pl-smi\">${BUILD_SHARED_LIBS}</span>)  <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> only install if used as shared a library</span>\n<span class=\"\
    pl-c1\">set</span>(openPMD_USE_PYTHON <span class=\"pl-k\">OFF</span>)\nFetchContent_Declare(openPMD\n\
    \  GIT_REPOSITORY <span class=\"pl-s\">\"https://github.com/openPMD/openPMD-api.git\"\
    </span>\n  GIT_TAG        <span class=\"pl-s\">\"dev\"</span>)\nFetchContent_MakeAvailable(openPMD)</pre></div>\n\
    <h3>\n<a id=\"user-content-manually\" class=\"anchor\" href=\"#manually\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Manually</h3>\n\
    <p>If your (Linux/OSX) project is build by calling the compiler directly or uses\
    \ a manually written <code>Makefile</code>, consider using our <code>openPMD.pc</code>\
    \ helper file for <code>pkg-config</code> which are installed alongside the library.</p>\n\
    <p>First set the following environment hint if openPMD-api was <em>not</em> installed\
    \ in a system path:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> optional: only needed if installed\
    \ outside of system paths</span>\n<span class=\"pl-k\">export</span> PKG_CONFIG_PATH=<span\
    \ class=\"pl-smi\">$HOME</span>/somepath/lib/pkgconfig:<span class=\"pl-smi\"\
    >$PKG_CONFIG_PATH</span></pre></div>\n<p>Additional linker and compiler flags\
    \ for your project are available via:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> switch to check if openPMD-api\
    \ was build as static library</span>\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> (via BUILD_SHARED_LIBS=OFF) or as shared library (default)</span>\n\
    <span class=\"pl-k\">if</span> [ <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span><span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pkg-config --variable=static\
    \ openPMD<span class=\"pl-pds\">)</span></span><span class=\"pl-pds\">\"</span></span>\
    \ <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>true<span class=\"pl-pds\">\"</span></span> ]\n<span class=\"pl-k\">then</span>\n\
    \    pkg-config --libs --static openPMD\n    <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lopenPMD\
    \ -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so\
    \ /usr/lib/libmpi.so /usr/lib/x86_64-linux-gnu/hdf5/openmpi/libhdf5.so /usr/lib/x86_64-linux-gnu/libsz.so\
    \ /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libdl.so /usr/lib/x86_64-linux-gnu/libm.so\
    \ -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so\
    \ /usr/lib/libmpi.so</span>\n<span class=\"pl-k\">else</span>\n    pkg-config\
    \ --libs openPMD\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> -L${HOME}/somepath/lib\
    \ -lopenPMD</span>\n<span class=\"pl-k\">fi</span>\n\npkg-config --cflags openPMD\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> -I${HOME}/somepath/include</span></pre></div>\n\
    <h2>\n<a id=\"user-content-author-contributions\" class=\"anchor\" href=\"#author-contributions\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Author Contributions</h2>\n<p>openPMD-api is developed by many people.\n\
    It was initially started by the <a href=\"https://hzdr.de/crp\" rel=\"nofollow\"\
    >Computational Radiation Physics Group</a> at <a href=\"https://www.hzdr.de/\"\
    \ rel=\"nofollow\">HZDR</a> as successor to <a href=\"https://github.com/ComputationalRadiationPhysics/libSplash/\"\
    >libSplash</a>, generalizing the <a href=\"https://arxiv.org/abs/1706.00522\"\
    \ rel=\"nofollow\">successful HDF5 &amp; ADIOS1 implementations</a> in <a href=\"\
    https://github.com/ComputationalRadiationPhysics/picongpu\">PIConGPU</a>.\nThe\
    \ following people and institutions <a href=\"https://github.com/openPMD/openPMD-api/graphs/contributors\"\
    >contributed</a> to openPMD-api:</p>\n<ul>\n<li>\n<a href=\"https://github.com/ax3l\"\
    >Axel Huebl (HZDR, now LBNL)</a>:\nproject lead, releases, documentation, automated\
    \ CI/CD, Python bindings, Dask, installation &amp; packaging, prior reference\
    \ implementations</li>\n<li>\n<a href=\"https://github.com/C0nsultant\">Fabian\
    \ Koller (HZDR)</a>:\ninitial library design and implementation with HDF5 &amp;\
    \ ADIOS1 backend</li>\n<li>\n<a href=\"https://github.com/franzpoeschel\">Franz\
    \ Poeschel (CASUS)</a>:\nadded JSON &amp; ADIOS2 backend, data staging/streaming</li>\n\
    <li>\n<a href=\"https://github.com/guj\">Junmin Gu (LBNL)</a>:\nnon-collective\
    \ parallel I/O fixes, ADIOS improvements, benchmarks</li>\n</ul>\n<p>Further thanks\
    \ go to improvements and contributions from:</p>\n<ul>\n<li>\n<a href=\"https://github.com/CFGrote\"\
    >Carsten Fortmann-Grote (EU XFEL GmbH, now MPI-EvolBio)</a>:\ndraft of our Python\
    \ unit tests</li>\n<li>\n<a href=\"https://github.com/StanczakDominik\">Dominik\
    \ Sta\u0144czak (Warsaw University of Technology)</a>:\ndocumentation improvements</li>\n\
    <li>\n<a href=\"https://github.com/mingwandroid\">Ray Donnelly (Anaconda, Inc.)</a>:\n\
    support on conda packaging and libc++ quirks</li>\n<li>\n<a href=\"https://github.com/amundson\"\
    >James Amundson (FNAL)</a>:\ncompile fix for newer compilers</li>\n<li>\n<a href=\"\
    https://github.com/psychocoderHPC\">Ren\xE9 Widera (HZDR)</a>:\ndesign improvements\
    \ for initial API design</li>\n<li>\n<a href=\"https://github.com/erikzenker\"\
    >Erik Zenker (HZDR)</a>:\ndesign improvements for initial API design</li>\n<li>\n\
    <a href=\"https://github.com/sbastrakov\">Sergei Bastrakov (HZDR)</a>:\ndocumentation\
    \ improvements (windows)</li>\n<li>\n<a href=\"https://github.com/RemiLehe\">R\xE9\
    mi Lehe (LBNL)</a>:\npackage integration testing on macOS and Linux</li>\n<li>\n\
    <a href=\"https://github.com/LDAmorim\">L\xEDgia Diana Amorim (LBNL)</a>:\npackage\
    \ integration testing on macOS</li>\n<li>\n<a href=\"https://github.com/KseniaBastrakova\"\
    >Kseniia Bastrakova (HZDR)</a>:\ncompatibility testing</li>\n<li>\n<a href=\"\
    https://github.com/PrometheusPi\">Richard Pausch (HZDR)</a>:\ncompatibility testing,\
    \ documentation improvements</li>\n<li>\n<a href=\"https://github.com/pordyna\"\
    >Pawe\u0142 Ordyna (HZDR)</a>:\nreport on NVCC warnings</li>\n<li>\n<a href=\"\
    https://github.com/dmitry-ganyushin\">Dmitry Ganyushin (ORNL)</a>:\nDask dataframe\
    \ support</li>\n<li>\n<a href=\"https://github.com/jakirkham\">John Kirkham (NVIDIA)</a>:\n\
    Dask guidance &amp; reviews</li>\n</ul>\n<h3>\n<a id=\"user-content-grants\" class=\"\
    anchor\" href=\"#grants\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Grants</h3>\n<p>The openPMD-api authors acknowledge\
    \ support via the following programs.\nThis project has received funding from\
    \ the European Unions Horizon 2020 research and innovation programme under grant\
    \ agreement No 654220.\nSupported by the Consortium for Advanced Modeling of Particles\
    \ Accelerators (CAMPA), funded by the U.S. DOE Office of Science under Contract\
    \ No. DE-AC02-05CH11231.\nSupported by the Exascale Computing Project (17-SC-20-SC),\
    \ a collaborative effort of two U.S. Department of Energy organizations (Office\
    \ of Science and the National Nuclear Security Administration).\nThis work was\
    \ partially funded by the Center of Advanced Systems Understanding (CASUS), which\
    \ is financed by Germany's Federal Ministry of Education and Research (BMBF) and\
    \ by the Saxon Ministry for Science, Culture and Tourism (SMWK) with tax funds\
    \ on the basis of the budget approved by the Saxon State Parliament.</p>\n<h3>\n\
    <a id=\"user-content-transitive-contributions\" class=\"anchor\" href=\"#transitive-contributions\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Transitive Contributions</h3>\n<p>openPMD-api stands on the shoulders\
    \ of giants and we are grateful for the following projects included as direct\
    \ dependencies:</p>\n<ul>\n<li>\n<a href=\"https://github.com/ornladios/ADIOS\"\
    >ADIOS1</a> and <a href=\"https://github.com/ornladios/ADIOS2\">ADIOS2</a> by\
    \ <a href=\"https://csmd.ornl.gov/adios\" rel=\"nofollow\">S. Klasky (ORNL), team,\
    \ collaborators</a> and <a href=\"https://github.com/ornladios/ADIOS2/graphs/contributors\"\
    >contributors</a>\n</li>\n<li>\n<a href=\"https://github.com/catchorg/Catch2\"\
    >Catch2</a> by <a href=\"https://github.com/philsquared\">Phil Nash</a>, <a href=\"\
    https://github.com/horenmar\">Martin Ho\u0159e\u0148ovsk\xFD</a> and <a href=\"\
    https://github.com/catchorg/Catch2/graphs/contributors\">contributors</a>\n</li>\n\
    <li>HDF5 by <a href=\"https://www.hdfgroup.org\" rel=\"nofollow\">the HDF group</a>\
    \ and community</li>\n<li>\n<a href=\"https://github.com/nlohmann/json\">json</a>\
    \ by <a href=\"https://github.com/nlohmann\">Niels Lohmann</a> and <a href=\"\
    https://github.com/nlohmann/json/graphs/contributors\">contributors</a>\n</li>\n\
    <li>\n<a href=\"https://github.com/pybind/pybind11\">pybind11</a> by <a href=\"\
    https://github.com/wjakob\">Wenzel Jakob (EPFL)</a> and <a href=\"https://github.com/pybind/pybind11/graphs/contributors\"\
    >contributors</a>\n</li>\n<li>all contributors to the evolution of modern C++\
    \ and early library preview developers, e.g. <a href=\"https://github.com/mpark\"\
    >Michael Park (Facebook)</a>\n</li>\n<li>the <a href=\"https://cmake.org\" rel=\"\
    nofollow\">CMake build system</a> and <a href=\"https://github.com/Kitware/CMake/blob/master/Copyright.txt\"\
    >contributors</a>\n</li>\n<li>packaging support by the <a href=\"https://conda-forge.org\"\
    \ rel=\"nofollow\">conda-forge</a>, <a href=\"https://pypi.org\" rel=\"nofollow\"\
    >PyPI</a> and <a href=\"https://spack.io\" rel=\"nofollow\">Spack</a> communities,\
    \ among others</li>\n<li>the <a href=\"https://github.com/openPMD/openPMD-standard\"\
    >openPMD-standard</a> by <a href=\"https://github.com/ax3l\">Axel Huebl (HZDR,\
    \ now LBNL)</a> and <a href=\"https://github.com/openPMD/openPMD-standard/blob/latest/AUTHORS.md\"\
    >contributors</a>\n</li>\n</ul>\n"
  stargazers_count: 67
  subscribers_count: 9
  topics:
  - openpmd
  - openscience
  - hdf5
  - adios
  - mpi
  - hpc
  - research
  - file-handling
  - python3
  - meta-data
  - opendata
  - cpp14
  updated_at: 1622195402.0
openhpc/ohpc:
  data_format: 2
  description: OpenHPC Integration, Packaging, and Test Repo
  filenames:
  - containers/Singularity.1.3.3.el7
  full_name: openhpc/ohpc
  latest_release: v2.2.GA
  readme: '<h3>

    <a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon
    octicon-link"></span></a><a href="https://github.com/openhpc/ohpc/blob/master/docs/recipes/install/common/figures/ohpc_logo.png"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/openhpc/ohpc/raw/master/docs/recipes/install/common/figures/ohpc_logo.png"
    width="170" valign="middle" hspace="5" alt="OpenHPC" style="max-width:100%;"></a>

    </h3>

    <h3>

    <a id="user-content-community-building-blocks-for-hpc-systems" class="anchor"
    href="#community-building-blocks-for-hpc-systems" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Community building blocks for HPC systems</h3>

    <h4>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h4>

    <p>This stack provides a variety of common, pre-built ingredients required to

    deploy and manage an HPC Linux cluster including provisioning tools, resource

    management, I/O clients, runtimes, development tools, containers, and a variety
    of

    scientific libraries.</p>

    <p>There are currently two release series:

    <a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> and

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>, which target different
    major

    Linux OS distributions. The 1.3.x series targets CentOS7 and SLES12 while the

    2.x series targets CentOS8 and Leap15.</p>

    <h4>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h4>

    <p>OpenHPC provides pre-built binaries via repositories for use with standard

    Linux package manager tools (e.g. <code>yum</code> or <code>zypper</code>). To
    get started,

    you can enable an OpenHPC repository locally through installation of an

    <code>ohpc-release</code> RPM which includes gpg keys for package signing and
    defines

    the URL locations for [base] and [update] package repositories. Installation

    guides tailored for each supported provisioning system and resource manager

    with detailed example instructions for installaing a cluster are also available.

    Copies of the <code>ohpc-release</code> package and installation guides along
    with

    more information is available on the relevant release series pages

    (<a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> or

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>).</p>

    <hr>

    <h4>

    <a id="user-content-questions-comments-or-bug-reports" class="anchor" href="#questions-comments-or-bug-reports"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Questions,
    Comments, or Bug Reports?</h4>

    <p>Subscribe to the users email list at <a href="https://groups.io/g/openhpc-users"
    rel="nofollow">https://groups.io/g/openhpc-users</a> or see

    the <a href="http://openhpc.community" rel="nofollow">http://openhpc.community</a>
    page for more pointers.</p>

    <h4>

    <a id="user-content-additional-software-requests" class="anchor" href="#additional-software-requests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Additional
    Software Requests?</h4>

    <p>Please see the component submission page at

    <a href="https://github.com/openhpc/submissions">https://github.com/openhpc/submissions</a>
    for more information regarding new

    software inclusion requests.</p>

    <h4>

    <a id="user-content-register-your-system" class="anchor" href="#register-your-system"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Register
    your system</h4>

    <p>If you are using elements of OpenHPC, please consider registering your

    system(s) using the <a href="https://drive.google.com/open?id=1KvFM5DONJigVhOlmDpafNTDDRNTYVdolaYYzfrHkOWI"
    rel="nofollow">System Registration

    Form</a>.</p>

    '
  stargazers_count: 560
  subscribers_count: 93
  topics:
  - hpc
  - scientific-computing
  - package-repository
  - clusters-management
  - devtools
  - mpi
  - linuxfoundation
  updated_at: 1622601901.0
pbranson/pangeo-hpc-singularity:
  data_format: 2
  description: Scripts to run dask and jupyter lab on Singularity using the pangeo-notebook
    image
  filenames:
  - Singularity.pangeo-notebook
  full_name: pbranson/pangeo-hpc-singularity
  latest_release: null
  readme: '<p>This repository provides some boiler plate scripts for running ''pangeo''
    python ecosystem using singularity containers.</p>

    <p>Steps are:</p>

    <ol>

    <li>

    <p>Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a></p>

    <pre><code>docker pull pangeo/pangeo-notebook

    </code></pre>

    <p>The pangeo-notebook has a pretty diverse set of libraries for most cloud,

    dask, zarr, netCDF, analysis type tasks.</p>

    <ul>

    <li>

    <p>(Optional) Obtain docker image curated at <a href="https://github.com/pangeo-data/pangeo-stacks">https://github.com/pangeo-data/pangeo-stacks</a>

    If you need to customise, see minimal example in Dockerfile and requirements.txt
    and description here:</p>

    <ul>

    <li>

    <p>(Deprecated) <a href="https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants">https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants</a></p>

    </li>

    <li>

    <p>(<strong>Use this since 27-07-2020</strong>) <a href="https://github.com/pangeo-data/pangeo-docker-images">https://github.com/pangeo-data/pangeo-docker-images</a></p>

    <p>Then you would build a custom image along the lines of:</p>

    <pre><code>make pangeo-notebook

    </code></pre>

    </li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Convert docker image to singularity with a command such as:</p>

    <pre><code>singularity -d build pangeo-custom.sif docker-daemon://pangeo/pangeo-notebook:master

    </code></pre>

    </li>

    <li>

    <p>Copy the created <code>pangeo-custom.sif</code> singularity image to somewhere
    accessible on the HPC filesystem and edit the <code>container=</code> and <code>scheduler_file=</code>
    variables in the <code>start_jupyter.slurm</code> and <code>start_worker.slurm</code>
    scripts to point to the singularity image and the shared filesystem location to
    write the scheduler details, respectively.</p>

    </li>

    <li>

    <p>Start the jupyter lab and dask-scheduler, the first parameter is the working
    path you want to use for jupyter lab:</p>

    <pre><code>sbatch start_jupyter.slurm $HOME

    </code></pre>

    <p>This starts a scheduler and jupyterlab with 2 cores each and 8GB/core memory.
    These can be edited in the #SBATCH headers, also note you can set the default
    directory for jupyterlab with the notebook_dir - at present it defaults to $SCRATCH1DIR.</p>

    </li>

    <li>

    <p>Start dask-workers (where n is the number of workers you want - these are configures
    for &lt; 2 hour wall time limit so that they use the <code>h2</code> queue):</p>

    <pre><code>sbatch -n 10 start_worker.slurm

    </code></pre>

    <p>also note that this input argument to dask-worker <code>--local-directory $LOCALDIR</code>
    tells the worker the path to local disk storage on the node which can be used
    for spilling data, but not all HPC nodes/centres have attached local storage.</p>

    </li>

    <li>

    <p>See instruction printed to the slurm-######.out log file for connecting to
    the jupyter session running on the compute node, something like:</p>

    <pre><code>ssh -N -l pbranson -L 8888:compute-node123:8888 hpc-login.host.com

    </code></pre>

    <p>and take note of the randomly generated token printed to the slurm-######.out
    log file. You will need that to login to Jupyterlab.</p>

    </li>

    <li>

    <p>To connect to the dask-scheduler from a notebook use the following snippet:</p>

    <pre><code>import os

    from distributed import Client

    client=Client(scheduler_file=os.environ[''SCRATCH1DIR''] + ''/scheduler.json'')

    client

    </code></pre>

    </li>

    <li>

    <p>View the scheduler bokeh dashboard at <a href="http://localhost:8888/proxy/8787/status"
    rel="nofollow">http://localhost:8888/proxy/8787/status</a>. This can also be entered
    into the Jupyterlab dask widget as <code>/proxy/8787/status</code></p>

    </li>

    <li>

    <p>As a little cheat in jupyter lab I open up a terminal and then do</p>

    <pre><code>ssh localhost

    </code></pre>

    <p>to connect to the host running the jupyter container - this gives you access
    to the jobscheduler from that terminal and you can start workers  in there with:</p>

    <pre><code>sbatch start_worker.slurm

    </code></pre>

    <p>Also note that the dask worker specifications used in the <code>start_worker.slurm</code>
    script are based of the slurm environment variables, so you can alter the worker
    specification using the <code>#SBATCH</code> directives:</p>

    <pre><code>#SBATCH --ntasks=20

    #SBATCH --cpus-per-task=2

    #SBATCH --mem-per-cpu=10G

    #SBATCH --time=0:30:00

    </code></pre>

    <p>or at the command line when you submit the script:</p>

    <pre><code>sbatch -n 4 -c 4 --mem-per-cpu=16G start_worker.slurm

    </code></pre>

    <p>which would start 4 workers with 4 cores per worker and 16*4 = 64GB memory
    per dask-worker.</p>

    </li>

    </ol>

    '
  stargazers_count: 6
  subscribers_count: 2
  topics: []
  updated_at: 1613348612.0
perambluate/singularity-definition-files-for-HPC:
  data_format: 2
  description: To build hpc benchmark and mpi with cuda support sif
  filenames:
  - hpcc_intel.def
  - bert.def
  - hpc_mpi_cuda.def
  - hpl_intel_cuda.def
  full_name: perambluate/singularity-definition-files-for-HPC
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc_mpi_cuda_singu_def_file" class="anchor" href="#hpc_mpi_cuda_singu_def_file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>hpc_mpi_cuda_singu_def_file</h1>

    <p>A collect of definition files to build images for singularity containers, which
    includes hpc benchmarks and mpis with cuda support.</p>

    <p><a href="https://singularity-hub.org/collections/4181" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1588998487.0
peterk87/nf-villumina:
  data_format: 2
  description: Generic viral Illumina sequence analysis pipeline
  filenames:
  - Singularity
  - singularity/Singularity.2.0.0
  full_name: peterk87/nf-villumina
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-peterk87nf-villumina\" class=\"anchor\" href=\"\
    #peterk87nf-villumina\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>peterk87/nf-villumina</h1>\n<p><strong>Generic\
    \ viral Illumina sequence analysis pipeline</strong></p>\n<p><a href=\"https://travis-ci.org/peterk87/nf-villumina\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c95912a5b97ffebe518b92d2612faba172f193f3aec7d85e0b8ee6a88db89b94/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f6e662d76696c6c756d696e612e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/nf-villumina.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.nextflow.io/\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\"\
    \ alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\"\
    \ alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/peterk87/nf-villumina\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/14da47af1d6b7d4d6e7909986afbce060794df560644ce6b565495a43df45b94/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f70657465726b38372f6e662d76696c6c756d696e612e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/peterk87/nf-villumina.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/2925\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h3>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h3>\n<p>The pipeline\
    \ is built using <a href=\"https://www.nextflow.io\" rel=\"nofollow\">Nextflow</a>,\
    \ a workflow tool to run tasks across multiple compute infrastructures in a very\
    \ portable manner. It comes with a <a href=\"https://sylabs.io/\" rel=\"nofollow\"\
    >Singularity</a> container making installation trivial and results highly reproducible.</p>\n\
    <p><code>nf-villumina</code> will</p>\n<ul>\n<li>remove low quality reads (<a\
    \ href=\"https://github.com/OpenGene/fastp\">fastp</a>)</li>\n<li>filter for reads\
    \ from a taxonomic group of interest (by default superkingdom <a href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;id=10239&amp;lvl=3&amp;lin=f&amp;keep=1&amp;srchmode=1&amp;unlock\"\
    \ rel=\"nofollow\">Viruses</a> (taxid=10239)) using <a href=\"https://ccb.jhu.edu/software/kraken2/\"\
    \ rel=\"nofollow\">Kraken2</a> and <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\"\
    \ rel=\"nofollow\">Centrifuge</a> classification results</li>\n<li>perform <em>de\
    \ novo</em> assembly with [Unicycler] and [Shovill] on the taxonomic classification\
    \ filtered reads</li>\n<li>search all contig sequences using NCBI nucleotide <a\
    \ href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\">BLAST</a>\
    \ against a database of your choice (we recommend the version 5 NCBI nt DB)</li>\n\
    </ul>\n<p><strong>NOTE:</strong> You will need to create/download databases for\
    \ <a href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\">Kraken2</a>,\
    \ <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\"\
    >Centrifuge</a> and <a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"\
    nofollow\">BLAST</a> in order to get the most out of this workflow!</p>\n<h3>\n\
    <a id=\"user-content-pre-requisites\" class=\"anchor\" href=\"#pre-requisites\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pre-requisites</h3>\n<h4>\n<a id=\"user-content-taxonomic-classification-for-kraken2-and-centrifuge\"\
    \ class=\"anchor\" href=\"#taxonomic-classification-for-kraken2-and-centrifuge\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Taxonomic Classification for <a href=\"https://ccb.jhu.edu/software/kraken2/\"\
    \ rel=\"nofollow\">Kraken2</a> and <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\"\
    \ rel=\"nofollow\">Centrifuge</a>\n</h4>\n<p>For taxonomic classification with\
    \ <a href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\">Kraken2</a>\
    \ and <a href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"\
    nofollow\">Centrifuge</a>, you will need to download (or build) databases for\
    \ these programs so that you may use them within the <code>nf-villumina</code>\
    \ workflow.</p>\n<p>You can point to the Kraken2 and Centrifuge database with\
    \ <code>export KRAKEN2_DB=/path/to/kraken2/database</code> and <code>export CENTRIFUGE_DB=/path/to/centrifuge/database/prefix</code>\
    \ in your <code>~/.bashrc</code> so you don't need to specify it each time you\
    \ run the workflow with <code>--kraken2_db /path/to/kraken2/standard2 --centrifuge_db\
    \ /path/to/centrifuge/nt-2018-03-03/nt</code></p>\n<h4>\n<a id=\"user-content-kraken2-dbs\"\
    \ class=\"anchor\" href=\"#kraken2-dbs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Kraken2 DBs</h4>\n<ul>\n<li>MiniKraken2_v2_8GB:\
    \ (5.5GB) 8GB Kraken 2 Database built from the Refseq bacteria, archaea, and viral\
    \ libraries and the GRCh38 human genome</li>\n<li>\n<a href=\"https://monash.figshare.com/articles/GTDB_r89_54k/8956970\"\
    \ rel=\"nofollow\">GTDB_r89_54k Kraken2 DBs</a>: There are multiple Kraken2 DBs\
    \ of various sizes available for download. For more info, see <a href=\"https://github.com/rrwick/Metagenomics-Index-Correction\"\
    >https://github.com/rrwick/Metagenomics-Index-Correction</a> and the manuscript:\
    \ M\xE9ric, Wick et al. (2019) Correcting index databases improves metagenomic\
    \ studies. doi: <a href=\"https://doi.org/10.1101/712166\" rel=\"nofollow\">https://doi.org/10.1101/712166</a>\n\
    </li>\n</ul>\n<h4>\n<a id=\"user-content-centrifuge-dbs\" class=\"anchor\" href=\"\
    #centrifuge-dbs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Centrifuge DBs</h4>\n<ul>\n<li>NCBI nucleotide non-redundant\
    \ sequences (2018-03-03) (64 GB)</li>\n<li>\n<a href=\"https://monash.figshare.com/ndownloader/files/16378439\"\
    \ rel=\"nofollow\">GTDB_r89_54k Centrifuge DB (108 GB tar file)</a>: For more\
    \ info, see <a href=\"https://github.com/rrwick/Metagenomics-Index-Correction\"\
    >https://github.com/rrwick/Metagenomics-Index-Correction</a> and the manuscript:\
    \ M\xE9ric, Wick et al. (2019) Correcting index databases improves metagenomic\
    \ studies. doi: <a href=\"https://doi.org/10.1101/712166\" rel=\"nofollow\">https://doi.org/10.1101/712166</a>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-blast-dbs\" class=\"anchor\" href=\"\
    #blast-dbs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\"\
    >BLAST</a> DBs</h3>\n<p>For nf-villumina, you must have a version 5 BLAST DB with\
    \ embedded taxonomic information installed, e.g. version 5 <code>nt</code> DB\
    \ (see <a href=\"https://ftp.ncbi.nlm.nih.gov/blast/db/v5/\" rel=\"nofollow\"\
    >https://ftp.ncbi.nlm.nih.gov/blast/db/v5/</a>)</p>\n<p>You can download pre-built\
    \ <a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\">BLAST</a>\
    \ DBs like <code>nt</code> and <code>nr</code> from <a href=\"https://ftp.ncbi.nlm.nih.gov/blast/db/\"\
    \ rel=\"nofollow\">the NCBI FTP site</a> using the <code>update_blastdb.pl</code>\
    \ script included with your install of BLAST+ to download and/or update your local\
    \ BLAST databases.</p>\n<p>Show all available databases:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ update_blastdb.pl --showall</pre></div>\n<p>Download\
    \ the BLASTDB version 5 \"nt\" database to your current directory decompressing\
    \ files and deleting original compressed archives:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>update_blastdb.pl --blastdb_version 5 nt --decompress</pre></div>\n\
    <p><strong>NOTE:</strong> For ease of use, all databases should be downloaded\
    \ to the same directory (e.g. <code>/opt/DB/blast</code> set in <code>$BLASTDB</code>\
    \ environment variable in your <code>~/.bashrc</code>)</p>\n<p>Check that your\
    \ database has been downloaded properly and has taxids associated with the sequences\
    \ contained within it:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$\
    \ blastdbcheck -db nt -must_have_taxids -verbosity 3</pre></div>\n<h3>\n<a id=\"\
    user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Documentation</h3>\n\
    <p>The peterk87/nf-villumina pipeline comes with documentation about the pipeline,\
    \ found in the <code>docs/</code> directory:</p>\n<ol>\n<li><a href=\"docs/installation.md\"\
    >Installation</a></li>\n<li>Pipeline configuration\n<ul>\n<li><a href=\"docs/configuration/local.md\"\
    >Local installation</a></li>\n<li><a href=\"docs/configuration/adding_your_own.md\"\
    >Adding your own system</a></li>\n</ul>\n</li>\n<li><a href=\"docs/usage.md\"\
    >Running the pipeline</a></li>\n<li><a href=\"docs/output.md\">Output and how\
    \ to interpret the results</a></li>\n<li><a href=\"docs/troubleshooting.md\">Troubleshooting</a></li>\n\
    </ol>\n\n<h3>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Credits</h3>\n<p>peterk87/nf-villumina was originally written by Peter\
    \ Kruczkiewicz.</p>\n<p>Bootstrapped with <a href=\"https://github.com/nf-core/tools\"\
    >nf-core/tools</a> <code>nf-core create</code>.</p>\n<p>Thank you to the <a href=\"\
    https://github.com/nf-core/tools\">nf-core/tools</a> team for a great tool for\
    \ bootstrapping creation of a production ready Nextflow workflows.</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1603317815.0
peterk87/nf-virontus:
  data_format: 2
  description: Oxford Nanopore reference mapping, taxonomic classification, de novo
    assembly workflow primarily for viral sequence data
  filenames:
  - singularity/Singularity.1.0.0
  - singularity/Singularity.1.1.0
  full_name: peterk87/nf-virontus
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-peterk87nf-virontus\" class=\"anchor\" href=\"\
    #peterk87nf-virontus\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>peterk87/nf-virontus</h1>\n<p><strong>Virontus\
    \ viral Oxford Nanopore sequence analysis pipeline</strong></p>\n<p><a href=\"\
    https://travis-ci.org/peterk87/nf-virontus\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ca2382eaedc143481936b2847287dfadcc9737054d5f078007bb7dcc0f5474bb/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f6e662d7669726f6e7475732e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/nf-virontus.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.nextflow.io/\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\"\
    \ alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\"\
    \ alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/peterk87/nf-virontus\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/701f9cda36830e80f60b8cb5e6108a4b9fdfcb6f09698b97e11a87e15dd71a93/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f70657465726b38372f6e662d7669726f6e7475732e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/peterk87/nf-virontus.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/4297\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><strong>Table of Contents</strong></p>\n\
    <ul>\n<li>\n<a href=\"#peterk87nf-virontus\">peterk87/nf-virontus</a>\n<ul>\n\
    <li><a href=\"#introduction\">Introduction</a></li>\n<li>\n<a href=\"#installation\"\
    >Installation</a>\n<ul>\n<li>\n<a href=\"#1-install-nextflow\">1) Install </a><a\
    \ href=\"https://www.nextflow.io\" rel=\"nofollow\">Nextflow</a>\n</li>\n<li>\n\
    <a href=\"#2-install-singularity\">2) Install </a><a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a>\n</li>\n<li><a href=\"#3-install-virontus\"\
    >3) Install Virontus</a></li>\n</ul>\n</li>\n<li>\n<a href=\"#usage\">Usage</a>\n\
    <ul>\n<li><a href=\"#preparing-your-data\">Preparing your data</a></li>\n<li><a\
    \ href=\"#recommended-steps\">Recommended Steps</a></li>\n<li><a href=\"#example\"\
    >Example</a></li>\n</ul>\n</li>\n<li><a href=\"#credits\">Credits</a></li>\n</ul>\n\
    </li>\n</ul>\n<blockquote>\n<p>TOC created by <a href=\"https://github.com/ekalinin/github-markdown-toc\"\
    >gh-md-toc</a></p>\n</blockquote>\n<h3>\n<a id=\"user-content-introduction\" class=\"\
    anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Introduction</h3>\n<p>The Virontus\
    \ pipeline is for the analysis of viral shotgun and amplicon  Oxford Nanopore\
    \ sequence data. Given basecalled (and demultiplexed) Nanopore reads, Virontus\
    \ produces one or more consensus sequences from read mapping with <a href=\"https://github.com/lh3/minimap2\"\
    >Minimap2</a> and variant calling with <a href=\"https://github.com/nanoporetech/medaka\"\
    >Medaka</a> and <a href=\"https://www.nature.com/articles/s41467-019-12493-y\"\
    \ rel=\"nofollow\">Longshot</a> results with respect to one or more reference\
    \ sequences. For amplicon sequencing, the user should provide a BED file containing\
    \ primer coordinates with respect to a reference sequence so that the primer sequences\
    \ can be trimmed using <a href=\"https://github.com/andersen-lab/ivar\">iVar</a>.</p>\n\
    <p>Optionally, Virontus will perform taxonomic classification with <a href=\"\
    https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\">Kraken2</a> and <a href=\"\
    https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\">Centrifuge</a>\
    \ if index paths are provided. Reads can be filtered by taxonomic classification.\
    \ By default viral and unclassified reads are filtered.</p>\n<p>De novo assembly\
    \ with <a href=\"https://github.com/rrwick/Unicycler\">Unicycler</a> can be optionally\
    \ performed if desired (specify <code>--do_unicycler_assembly</code> when running\
    \ Virontus). If taxonomic classification is performed then taxonomically filtered\
    \ reads will be assembled, otherwise all reads will be used for assembly.</p>\n\
    <p>The Virontus pipeline is built using <a href=\"https://www.nextflow.io\" rel=\"\
    nofollow\">Nextflow</a>, a workflow tool to run tasks across multiple compute\
    \ infrastructures in a very portable manner. It comes with <a href=\"https://www.docker.com/\"\
    \ rel=\"nofollow\">Docker</a> and <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a> containers making installation trivial and\
    \ results highly reproducible.</p>\n<h3>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h3>\n<p>You will\
    \ need to install <a href=\"https://www.nextflow.io\" rel=\"nofollow\">Nextflow</a>\
    \ in order to run the Virontus pipeline.</p>\n<p><a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a> is recommended for portable and reproducible\
    \ execution of the pipeline with the <code>-profile singularity</code> command-line\
    \ argument.</p>\n<h4>\n<a id=\"user-content-1-install-nextflow\" class=\"anchor\"\
    \ href=\"#1-install-nextflow\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>1) Install <a href=\"https://www.nextflow.io\"\
    \ rel=\"nofollow\">Nextflow</a>\n</h4>\n<p>If you have <a href=\"https://conda.io/\"\
    \ rel=\"nofollow\">Conda</a> installed, you can install <a href=\"https://www.nextflow.io\"\
    \ rel=\"nofollow\">Nextflow</a> with the following command:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>conda install -c bioconda -c conda-forge\
    \ nextflow</pre></div>\n<h4>\n<a id=\"user-content-2-install-singularity\" class=\"\
    anchor\" href=\"#2-install-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2) Install <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a>\n</h4>\n<p>Installing <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity</a> is optional but recommended for portability\
    \ and reproducibility of results.</p>\n<h4>\n<a id=\"user-content-3-install-virontus\"\
    \ class=\"anchor\" href=\"#3-install-virontus\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3) Install Virontus</h4>\n<p>Nextflow\
    \ will automatically download the latest version of Virontus. You can show the\
    \ Virontus help message with usage information with:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>nextflow run peterk87/nf-virontus --help</pre></div>\n\
    <h3>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h3>\n\
    <p>Show usage information with</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>nextflow run peterk87/nf-virontus --help</pre></div>\n<p>You should see\
    \ the following</p>\n<pre><code>N E X T F L O W  ~  version 20.01.0\nLaunching\
    \ `main.nf` [awesome_pauling] - revision: 9aeb19496b\nWARN: DSL 2 IS AN EXPERIMENTAL\
    \ FEATURE UNDER DEVELOPMENT -- SYNTAX MAY CHANGE IN FUTURE RELEASE\n==================================================================\n\
    peterk87/nf-virontus   ~  version 1.1.0\n==================================================================\n\
    \n  Git info: null - null [null]\n\nUsage:\nGiven some barcoded and demultiplexed\
    \ reads, the typical command for running the pipeline is as follows:\n\n  nextflow\
    \ run peterk87/nf-virontus \\\n    --reads \"reads/*.fastq\" \\\n    --outdir\
    \ results \\\n    --ref_fasta refs.fa \\\n    -profile singularity # recommended\
    \ to run with Singularity\n\nThe above assumes that you have a Centrifuge DB and\
    \ Kraken2 DB located at\n/opt/DB/centrifuge/nt-2018-03-03/nt and /opt/DB/kraken2/standard2,\n\
    respectively, OR that you have set $CENTRIFUGE_DB and $KRAKEN2_DB env\nvariables.\
    \ It also assumes that you have Singularity installed on your\nlocal machine and\
    \ will automatically pull and use the Singularity image for\nthis workflow from\
    \ Singularity-Hub.org.\n\nNOTE: For best results, please ensure you have Singularity\
    \ installed prior to running this workflow.(https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-installation-steps)\n\
    \nNote:\nThe argument supplied to \"--reads\" must be quoted if using \"*\" and\
    \ other\ncharacters and symbols that could be shell expanded!\n\nMandatory Options:\n\
    \  --reads   Input reads directory and pattern (default: \"reads/*.fastq\")\n\
    \  --ref_fasta      Reference genomes multiFASTA file (one or more references\n\
    \                   in a single file) (default: \"./refs.fasta\")\nAmplicon Sequencing\
    \ Options:\n  --bedfile        BED format file with amplicon sequencing primers\
    \ info (optional).\n                   Produced as output from PrimalScheme.\n\
    Consensus Generation Options:\n  --low_coverage   Low coverage threshold (default=3).\n\
    \                   Replace consensus sequence positions below this depth\n  \
    \                 threshold with a low coverage character\n                  \
    \ (see --low_cov_char)\n  --no_coverage    No coverage threshold (default=0).\n\
    \                   Replace consensus sequence positions with less than or\n \
    \                  equal this depth with a no coverage character\n           \
    \        (see --no_cov_char)\n  --low_cov_char   Low coverage character (default=\"\
    N\")\n  --no_cov_char    No coverage character (default=\"-\")\n\nCluster Options:\n\
    \  --slurm_queue     Name of SLURM queue to run workflow on; use with -profile\
    \ slurm\n\n\nTaxonomic Classification Options:\n  --centrifuge_db   Path to Centrifuge\
    \ DB and prefix. If not specified, will\n                    try to get from $CENTRIFUGE_DB\
    \ env variable or see if\n                    \"/opt/DB/centrifuge/nt-2018-03-03/nt\"\
    \ exists.\n                    (default: null)\n  --kraken2_db      Path to Kraken2\
    \ DB directory. . If not specified, will\n                    try to get from\
    \ $KRAKEN2_DB env variable or see if\n                    \"/opt/DB/kraken2/standard2\"\
    \ exists.\n                    (default: null)\n  --taxids          Taxonomic\
    \ IDs to filter reads by. Multiple taxids should\n                    be delimited\
    \ by commas (`--taxids 1,2,3`). To disable\n                    filtering of reads\
    \ based on taxids, do not provide a\n                    value for the `--taxids`\
    \ argument:\n                    `nextflow run ... --taxids --reads ...`\n   \
    \                 (default: 10239 (Viruses))\n  --exclude_unclassified_reads \
    \ Exclude unclassified reads from taxonomic\n                                classification\
    \ filtered reads (default: false)\n\nDe Novo Assembly Options:\n  --do_unicycler_assembly\
    \       Assemble filtered reads using Unicycler? (default: false)\n\nOther Options:\n\
    \  --outdir          The output directory where the results will be saved\n  \
    \                  (default: results)\n  -w/--work-dir     The temporary directory\
    \ where intermediate data will be\n                    saved (default: ./work)\n\
    \  -profile          Configuration profile to use. [standard, singularity,\n \
    \                   conda, slurm] (default 'standard')\n  --tracedir        Pipeline\
    \ run info output directory (default:\n                    results/pipeline_info)\n\
    \nNote:\nIt is recommended that this workflow be executed with Singularity using\
    \ the\nSingularity profile (`-profile singularity`) for maximum reproducibility\
    \ and\nease of execution on different platforms.\n</code></pre>\n<h4>\n<a id=\"\
    user-content-preparing-your-data\" class=\"anchor\" href=\"#preparing-your-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Preparing your data</h4>\n<p>It is assumed that your data has been\
    \ basecalled using the latest version of ONT Guppy (<code>guppy_basecaller</code>/<code>guppy_basecall_server</code>)\
    \ and barcode demultiplexed using <code>guppy_barcoder</code> with the appropriate\
    \ settings for the kits used.</p>\n<p>After basecalling and demultiplexing, it\
    \ is recommended that all reads belonging to a particular barcode be concatenated\
    \ together and optionally renamed to represent the sample to which the reads belong.\
    \ Virontus will extract the sample name for each input reads FASTQ file from the\
    \ base filename of the FASTQ file (e.g. sample name will be <code>sample</code>\
    \ from filename <code>sample1.fastq</code>).</p>\n<p>Below is an example <code>guppy_barcoder</code>\
    \ command for more lenient barcode demultiplexing:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>guppy_barcoder \\\n  -q 0 \\\n  --min_score 30\
    \ \\\n  --detect_mid_strand_barcodes \\\n  --allow_inferior_barcodes \\\n  --trim_barcodes\
    \ \\\n  -i basecalled-reads/ \\\n  -s demuxed-reads \\\n  --arrangements_files\
    \ barcode_arrs_nb12.cfg</pre></div>\n<ul>\n<li>\n<code>-q 0</code> to output less\
    \ files per barcode</li>\n<li>\n<code>--min_score 30</code> for a lower barcode\
    \ score threshold (default: 60)</li>\n<li>\n<code>--detect_mid_strand_barcodes</code>\
    \ to detect mid strand barcodes</li>\n<li>\n<code>--trim_barcodes</code> to trim\
    \ barcodes from read sequences</li>\n<li>\n<code>--arrangements_files</code> to\
    \ specify the barcodes used</li>\n</ul>\n<p><strong>NOTE:</strong> It's recommended\
    \ to use the default setting if possible to avoid misassigning reads into the\
    \ incorrect barcodes.</p>\n<h5>\n<a id=\"user-content-recommended-steps\" class=\"\
    anchor\" href=\"#recommended-steps\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recommended Steps</h5>\n<ol>\n\
    <li>Basecall reads using Guppy</li>\n<li>Demultiplex reads using <code>guppy_barcoder</code>\n\
    </li>\n<li>Concatenate reads belonging to the same barcode into a single file\
    \ (<code>cat barcode01/*.fastq &gt; concat-reads/barcode01.fastq</code>)</li>\n\
    <li>[Optionally] rename concatenated barcoded reads with appropriate sample name\
    \ (<code>mv concat-reads/barcode01.fastq concat-reads/sample1.fastq</code>)</li>\n\
    </ol>\n<h4>\n<a id=\"user-content-example\" class=\"anchor\" href=\"#example\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Example</h4>\n<p>Example command</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ nextflow run peterk87/nf-virontus \\\n    -resume \\\n    -profile singularity\
    \ \\\n    --reads <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>reads/*.fq<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --ref_fasta MN908947.3.fa \\\n   \
    \ --low_coverage 3 \\\n    --bedfile nCoV-2019.bed</pre></div>\n<p>What you will\
    \ see in the terminal:</p>\n<pre><code>N E X T F L O W  ~  version 20.01.0\nLaunching\
    \ `../main.nf` [ecstatic_davinci] - revision: 9aeb19496b\nWARN: DSL 2 IS AN EXPERIMENTAL\
    \ FEATURE UNDER DEVELOPMENT -- SYNTAX MAY CHANGE IN FUTURE RELEASE\n=======================================================\n\
    peterk87/nf-virontus v1.1.0\n=======================================================\n\
    Pipeline Name         : peterk87/nf-virontus\nPipeline Version      : 1.1.0\n\
    Run Name              : ecstatic_davinci\nReads                 : reads/*.fq\n\
    Ref Sequences FASTA   : MN908947.3.fa\nPrimer Scheme         : nCoV-2019.bed\n\
    Consensus No Coverage : &lt;=0X positions replaced with '-'\nConsensus Low Coverage:\
    \ &lt;3X positions replaced with 'N'\nCentrifuge DB         : null\nKraken2 DB\
    \            : null\nTaxids                : Filtering for taxids belonging to\
    \ 10239\nUnicycler Assembly?   : No\nMax Memory            : 256 GB\nMax CPUs\
    \              : 48\nMax Time              : 10d\nOutput dir            : results\n\
    Working dir           : ./work\nContainer Engine      : singularity\nContainer\
    \             : virontus.simg\nCurrent home          : /home/pkruczkiewicz\nCurrent\
    \ user          : pkruczkiewicz\nCurrent path          : ./\nScript dir      \
    \      : ./nf-virontus\nConfig Profile        : standard\nCommand-Line       \
    \   : nextflow run peterk87/nf-virontus -profile singularity -resume --reads 'reads/*.fq'\
    \ --ref_fasta MN908947.3.fa --low_coverage 3 --bedfile nCoV-2019.bed\nNextflow\
    \ version      : 20.01.0\n=========================================\nexecutor\
    \ &gt;  local (18)\n[0a/142458] process &gt; REC2FASTA  [100%] 1 of 1 \u2714\n\
    [a3/3168c5] process &gt; MAP        [100%] 3 of 3 \u2714\n[0d/8a698f] process\
    \ &gt; IVAR_TRIM  [100%] 3 of 3 \u2714\n[76/f82320] process &gt; MAP_STATS  [100%]\
    \ 3 of 3 \u2714\n[cc/de6b36] process &gt; MEDAKA     [100%] 3 of 3 \u2714\n[74/058b57]\
    \ process &gt; LONGSHOT   [100%] 3 of 3 \u2714\n[b4/5ed366] process &gt; BCF_FILTER\
    \ [100%] 3 of 3 \u2714\n[a3/ae8e3a] process &gt; CONSENSUS  [ 100%] 3 of 3 \u2714\
    \n[e3/f75ddb] process &gt; COVERAGE_PLOT [100%] 3 of 3 \u2714\n\nPipeline execution\
    \ summary\nCompleted at: 30-Apr-2020 14:00:11\nDuration    : 1m 40s\nCPU hours\
    \   : 0.1 (58.9% cached)\nSucceeded   : 18\nCached      : 4\n</code></pre>\n<p>Example\
    \ output file tree structure:</p>\n<pre><code>results/\n\u251C\u2500\u2500 consensus\n\
    \u2502\_\_ \u251C\u2500\u2500 NB02-MN908947.3.consensus.fasta\n\u2502\_\_ \u251C\
    \u2500\u2500 NB04-MN908947.3.consensus.fasta\n\u2502\_\_ \u2514\u2500\u2500 unclassified-MN908947.3.consensus.fasta\n\
    \u251C\u2500\u2500 mapping\n\u2502\_\_ \u251C\u2500\u2500 NB02\n\u2502\_\_ \u2502\
    \_\_ \u251C\u2500\u2500 bamfiles\n\u2502\_\_ \u2502\_\_ \u2502\_\_ \u251C\u2500\
    \u2500 NB02-MN908947.3.bam \n\u2502\_\_ \u2502\_\_ \u2502\_\_ \u2514\u2500\u2500\
    \ NB02-MN908947.3.trim.bam \n\u2502\_\_ \u2502\_\_ \u251C\u2500\u2500 NB02-MN908947.3-depths.tsv\n\
    \u2502\_\_ \u2502\_\_ \u251C\u2500\u2500 NB02-MN908947.3.flagstat\n\u2502\_\_\
    \ \u2502\_\_ \u2514\u2500\u2500 NB02-MN908947.3.idxstats\n\u2502\_\_ \u251C\u2500\
    \u2500 NB04\n\u2502\_\_ \u2502\_\_ \u251C\u2500\u2500 bamfiles\n\u2502\_\_ \u2502\
    \_\_ \u2502\_\_ \u251C\u2500\u2500 NB04-MN908947.3.bam \n\u2502\_\_ \u2502\_\_\
    \ \u2502\_\_ \u2514\u2500\u2500 NB04-MN908947.3.trim.bam \n\u2502\_\_ \u2502\_\
    \_ \u251C\u2500\u2500 NB04-MN908947.3-depths.tsv\n\u2502\_\_ \u2502\_\_ \u251C\
    \u2500\u2500 NB04-MN908947.3.flagstat\n\u2502\_\_ \u2502\_\_ \u2514\u2500\u2500\
    \ NB04-MN908947.3.idxstats\n\u2502\_\_ \u2514\u2500\u2500 unclassified\n\u2502\
    \_\_     \u251C\u2500\u2500 bamfiles\n\u2502\_\_     \u2502\_\_ \u251C\u2500\u2500\
    \ unclassified-MN908947.3.bam \n\u2502\_\_     \u2502\_\_ \u2514\u2500\u2500 unclassified-MN908947.3.trim.bam\
    \ \n\u2502\_\_     \u251C\u2500\u2500 unclassified-MN908947.3-depths.tsv\n\u2502\
    \_\_     \u251C\u2500\u2500 unclassified-MN908947.3.flagstat\n\u2502\_\_     \u2514\
    \u2500\u2500 unclassified-MN908947.3.idxstats\n\u251C\u2500\u2500 pipeline_info\n\
    \u2502\_\_ \u251C\u2500\u2500 execution_dag.dot\n\u2502\_\_ \u251C\u2500\u2500\
    \ execution_report.html\n\u2502\_\_ \u251C\u2500\u2500 execution_timeline.html\n\
    \u2502\_\_ \u2514\u2500\u2500 execution_trace.txt\n\u251C\u2500\u2500 plots\n\u2502\
    \_\_ \u251C\u2500\u2500 coverage_plot-NB02-VS-MN908947.3-log_scale.pdf\n\u2502\
    \_\_ \u251C\u2500\u2500 coverage_plot-NB02-VS-MN908947.3.pdf\n\u2502\_\_ \u251C\
    \u2500\u2500 coverage_plot-NB04-VS-MN908947.3-log_scale.pdf\n\u2502\_\_ \u2514\
    \u2500\u2500 coverage_plot-NB04-VS-MN908947.3.pdf\n\u251C\u2500\u2500 refs\n\u2502\
    \_\_ \u2514\u2500\u2500 MN908947.3.fa\n\u2514\u2500\u2500 vcf\n    \u251C\u2500\
    \u2500 NB02-MN908947.3.longshot.filt.vcf\n    \u251C\u2500\u2500 NB02-MN908947.3.longshot.vcf\n\
    \    \u251C\u2500\u2500 NB02-MN908947.3.medaka.vcf\n    \u251C\u2500\u2500 NB04-MN908947.3.longshot.filt.vcf\n\
    \    \u251C\u2500\u2500 NB04-MN908947.3.longshot.vcf\n    \u251C\u2500\u2500 NB04-MN908947.3.medaka.vcf\n\
    \    \u251C\u2500\u2500 unclassified-MN908947.3.longshot.filt.vcf\n    \u251C\u2500\
    \u2500 unclassified-MN908947.3.longshot.vcf\n    \u2514\u2500\u2500 unclassified-MN908947.3.medaka.vcf\n\
    </code></pre>\n<h3>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Credits</h3>\n<p>peterk87/nf-virontus was originally written by Peter\
    \ Kruczkiewicz.</p>\n<p>Bootstrapped with <a href=\"https://github.com/nf-core/tools\"\
    >nf-core/tools</a> <code>nf-core create</code>.</p>\n<p>Thank you to the <a href=\"\
    https://github.com/nf-core/tools\">nf-core/tools</a> team for a great tool for\
    \ bootstrapping creation of a production ready Nextflow workflows.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1603317652.0
peterk87/viral-ampliseq-assembly:
  data_format: 2
  description: Snakemake workflow for analysis and assembly of viral genomes from
    IonTorrent AmpliSeq data.
  filenames:
  - Singularity
  full_name: peterk87/viral-ampliseq-assembly
  latest_release: v1.0.0
  readme: "<h1>\n<a id=\"user-content-snakemake-workflow-viral-ampliseq-assembly\"\
    \ class=\"anchor\" href=\"#snakemake-workflow-viral-ampliseq-assembly\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Snakemake\
    \ workflow: viral-ampliseq-assembly</h1>\n<p><a href=\"https://snakemake.bitbucket.io\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/de7b3ae9d2ddd7970750ed14a267d738217987e5635a19380de6f3b2ec3216e6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d254532253839254135352e352e342d627269676874677265656e2e737667\"\
    \ alt=\"Snakemake\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%E2%89%A55.5.4-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.com/peterk87/viral-ampliseq-assembly\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9ca62ba99cb6a38032432759aa450c99bf81b9671bab9e21e2492c47bf7cf065/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f766972616c2d616d706c697365712d617373656d626c792e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/viral-ampliseq-assembly.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/3359\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://snakemake.readthedocs.io/en/stable/\"\
    \ rel=\"nofollow\">Snakemake</a> workflow for analysis and assembly of viral genomes\
    \ such as Classical Swine Fever Virus (<a href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=11096\"\
    \ rel=\"nofollow\">CSFV</a>) from IonTorrent AmpliSeq data.</p>\n<h2>\n<a id=\"\
    user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Overview</h2>\n\
    <ul>\n<li>\n<p>Preprocessing</p>\n<ul>\n<li>Duplicate reads were removed using\
    \ <a href=\"https://broadinstitute.github.io/picard/\" rel=\"nofollow\">Picard</a>\n\
    </li>\n<li>Reads were trimmed with <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\"\
    \ rel=\"nofollow\">Trimmomatic</a> prior to <a href=\"http://cab.spbu.ru/software/spades/\"\
    \ rel=\"nofollow\">SPAdes</a> assembly</li>\n<li>BAM file stats computed using\
    \ <a href=\"https://samtools.github.io/\" rel=\"nofollow\">Samtools</a> (coverage\
    \ depth, extent, extent per genome, # of reads mapped)</li>\n</ul>\n</li>\n<li>\n\
    <p>Reference Genome Selection</p>\n<ul>\n<li>Downloading of all Classical swine\
    \ fever virus (<a href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=11096\"\
    \ rel=\"nofollow\">CSFV</a>) (or FMDV, Ebola, Zika) virus genomes from <a href=\"\
    https://www.ncbi.nlm.nih.gov/books/NBK25501/\" rel=\"nofollow\">NCBI Entrez API</a>\
    \ using <a href=\"https://biopython.org/\" rel=\"nofollow\">BioPython</a>\n</li>\n\
    <li>\n<a href=\"https://mash.readthedocs.io/en/latest/\" rel=\"nofollow\">Mash</a>\
    \ screen of deduplicated reads against all reference genomes with sketch size\
    \ of 10000 and sketch k-mer size of 16, sorting by Mash screen identity to find\
    \ top reference genome for read mapping and variant calling</li>\n</ul>\n</li>\n\
    <li>\n<p>Read Mapping &amp; Variant Calling</p>\n<ul>\n<li>Read mapping with <a\
    \ href=\"https://github.com/lh3/bwa\">BWA MEM</a>\n</li>\n<li>Removal of duplicate\
    \ reads with <a href=\"https://samtools.github.io/\" rel=\"nofollow\">Samtools</a>\n\
    </li>\n<li>Variant calling with <a href=\"https://github.com/ekg/freebayes\">FreeBayes</a>\n\
    </li>\n<li>\n<a href=\"http://snpeff.sourceforge.net/SnpEff.html\" rel=\"nofollow\"\
    >SnpEff</a> was used to predict and report variant effects using reference genome\
    \ annotation</li>\n</ul>\n</li>\n<li>\n<p>De Novo Assembly</p>\n<ul>\n<li>\n<a\
    \ href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\">SPAdes</a> de\
    \ novo assembly of trimmed deduplicated reads.</li>\n<li>\n<a href=\"http://quast.sourceforge.net/quast.html\"\
    \ rel=\"nofollow\">QUAST</a> quality assessment of assemblies</li>\n</ul>\n</li>\n\
    <li>\n<p>Quality Control</p>\n<ul>\n<li>\n<a href=\"https://multiqc.info/\" rel=\"\
    nofollow\">MultiQC</a> interactive report of <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\"\
    \ rel=\"nofollow\">FastQC</a>, <a href=\"https://samtools.github.io/\" rel=\"\
    nofollow\">Samtools</a>, <a href=\"http://quast.sourceforge.net/quast.html\" rel=\"\
    nofollow\">QUAST</a>, <a href=\"http://snpeff.sourceforge.net/SnpEff.html\" rel=\"\
    nofollow\">SnpEff</a>\n</li>\n</ul>\n</li>\n<li>\n<p>Phylogenetic Tree</p>\n<ul>\n\
    <li>Phylogenetic tree constructed with <a href=\"http://www.iqtree.org/\" rel=\"\
    nofollow\">IQ-TREE</a> (or <a href=\"http://bioinformatics.hungry.com/clearcut/\"\
    \ rel=\"nofollow\">Clearcut</a> if a quick and dirty tree is okay)</li>\n<li>Interactive\
    \ HTML phylogenetic tree visualization with <a href=\"http://phylocanvas.org/\"\
    \ rel=\"nofollow\">PhyloCanvas</a> using <a href=\"https://github.com/peterk87/shiptv\"\
    >shiptv</a>\n</li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-authors\"\
    \ class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Authors</h2>\n<ul>\n<li>Peter\
    \ Kruczkiewicz (@peterk87)</li>\n</ul>\n<h2>\n<a id=\"user-content-usage\" class=\"\
    anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-step-0-install-pre-requisites\"\
    \ class=\"anchor\" href=\"#step-0-install-pre-requisites\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 0:\
    \ Install pre-requisites</h3>\n<p>Running this workflow with <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> is recommended, but you can use <a href=\"\
    https://conda.io/en/latest/\" rel=\"nofollow\">Conda</a> if you prefer. The Singularity\
    \ image will come with all the dependencies bundled together in a single file.</p>\n\
    <h4>\n<a id=\"user-content-install-singularity-recommended\" class=\"anchor\"\
    \ href=\"#install-singularity-recommended\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> (recommended)</h4>\n<p>Follow the instructions\
    \ for installing Singularity <a href=\"https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-start\"\
    \ rel=\"nofollow\">here</a></p>\n<h4>\n<a id=\"user-content-setup-and-activate-the-conda-environment-if-not-using-singularity-optional\"\
    \ class=\"anchor\" href=\"#setup-and-activate-the-conda-environment-if-not-using-singularity-optional\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Setup and activate the <a href=\"https://conda.io/en/latest/\" rel=\"\
    nofollow\">Conda</a> environment if not using <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> (optional)</h4>\n<p>Install <a href=\"https://conda.io/en/latest/\"\
    \ rel=\"nofollow\">Conda</a> if you haven't already following <a href=\"https://conda.io/en/latest/miniconda.html\"\
    \ rel=\"nofollow\">these instructions</a> and setup the <a href=\"https://bioconda.github.io/user/install.html#set-up-channels\"\
    \ rel=\"nofollow\">BioConda channel</a>.</p>\n<p>Download or <code>git clone</code>\
    \ this repo</p>\n<div class=\"highlight highlight-source-shell\"><pre>git clone\
    \ https://github.com/peterk87/viral-ampliseq-assembly.git\n<span class=\"pl-c1\"\
    >cd</span> viral-ampliseq-assembly\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> create a conda environment named \"viral-ampliseq-assembly-1.0.0\"</span>\n\
    conda env create -f environment.yml\nconda activate viral-ampliseq-assembly-1.0.0\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span> install snakemake into this\
    \ env</span>\nconda install -y snakemake\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> run Snakemake on the test directory</span>\nsnakemake --directory\
    \ test/</pre></div>\n<h3>\n<a id=\"user-content-step-1-install-workflow\" class=\"\
    anchor\" href=\"#step-1-install-workflow\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 1: Install workflow</h3>\n\
    <p>If you simply want to use this workflow, download and extract the <a href=\"\
    https://github.com/peterk87/viral-ampliseq-assembly/releases\">latest release</a>.\n\
    If you intend to modify and further develop this workflow, fork this repository.\
    \ Please consider providing any generally applicable modifications via a pull\
    \ request.</p>\n<p>In any case, if you use this workflow in a paper, don't forget\
    \ to give credits to the authors by citing the URL of this repository and, if\
    \ available, its DOI (see above).</p>\n<h3>\n<a id=\"user-content-step-2-configure-workflow\"\
    \ class=\"anchor\" href=\"#step-2-configure-workflow\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 2: Configure\
    \ workflow</h3>\n<p>Create an analysis directory, copy and modify the example\
    \ <code>config.yaml</code> and <code>samples.tsv</code> files to suit your needs.</p>\n\
    <p>e.g.</p>\n<pre><code>mkdir ~/my-ampliseq-analysis\ncp viral-ampliseq-assembly/config.yaml\
    \ ~/my-ampliseq-analysis/\ncp viral-ampliseq-assembly/samples.tsv ~/my-ampliseq-analysis/\n\
    </code></pre>\n<p>Edit your <code>config.yaml</code> as needed.</p>\n<p>Add sample\
    \ entries to your <code>samples.tsv</code></p>\n<pre><code>sample  bam_file\n\
    Sample1 bams/Sample1.bam\nSample2 bams/Sample2.bam\nSample3 bams/Sample3.bam\n\
    ... &lt;more sample entries&gt;\n</code></pre>\n<p>where <code>bam_file</code>\
    \ can be the relative or absolute path to a sample's BAM file.</p>\n<h4>\n<a id=\"\
    user-content-iq-tree-maximum-likelihood-or-clearcut-rnj-tree\" class=\"anchor\"\
    \ href=\"#iq-tree-maximum-likelihood-or-clearcut-rnj-tree\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"\
    http://www.iqtree.org/\" rel=\"nofollow\">IQ-TREE</a> maximum-likelihood or <a\
    \ href=\"http://bioinformatics.hungry.com/clearcut/\" rel=\"nofollow\">Clearcut</a>\
    \ RNJ tree</h4>\n<p>In your <code>config.yaml</code> the <code>fast_tree</code>\
    \ parameter controls which method (ML or RNJ) is used for phylogenetic tree construction.</p>\n\
    <p>If you want a quick and dirty tree, set</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre><span class=\"pl-ent\">fast_tree</span>: <span class=\"pl-c1\">true</span></pre></div>\n\
    <p>in your <code>config.yaml</code> to generate a Relaxed Neighbor Joining (RNJ)\
    \ tree.</p>\n<p>Otherwise, if you want a high accuracy phylogenetic tree and are\
    \ willing to wait for it, then set</p>\n<div class=\"highlight highlight-source-yaml\"\
    ><pre><span class=\"pl-ent\">fast_tree</span>: <span class=\"pl-c1\">false</span></pre></div>\n\
    <p>to use <a href=\"http://www.iqtree.org/\" rel=\"nofollow\">IQ-TREE</a> to generate\
    \ a maximum-likelihood phylogenetic tree with 1000 ultrafast bootstraps (UFBoot)\
    \ (see <a href=\"http://dx.doi.org/10.1093/molbev/mst024\" rel=\"nofollow\">Minh\
    \ et al., 2016</a> for more info on UFBoot).</p>\n<h3>\n<a id=\"user-content-step-3-execute-workflow\"\
    \ class=\"anchor\" href=\"#step-3-execute-workflow\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 3: Execute\
    \ workflow</h3>\n<p><em>If you do not have <a href=\"https://sylabs.io/docs/\"\
    \ rel=\"nofollow\">Singularity</a> installed then remove the <code>--use-singularity</code>\
    \ flag</em></p>\n<p>Test your configuration by performing a dry-run via</p>\n\
    <pre><code>snakemake --use-singularity -n\n</code></pre>\n<p>Execute the workflow\
    \ locally via</p>\n<pre><code>snakemake --use-singularity --cores $N\n</code></pre>\n\
    <p>using <code>$N</code> cores.</p>\n<h4>\n<a id=\"user-content-cluster-execution\"\
    \ class=\"anchor\" href=\"#cluster-execution\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Cluster execution</h4>\n<p><em>Note:\
    \ You may need to install the <code>drmaa</code> Python library (<code>pip install\
    \ drmaa</code>)</em></p>\n<p>You can execute the workflow on a SLURM/DRMAA cluster\
    \ environment with</p>\n<pre><code>snakemake --directory test --use-singularity\
    \ --drmaa \" -c 4 -p YourClusterQueueName --mem=4096 \" -j 8 -w 60\n</code></pre>\n\
    <p>This will run the workflow on the test data in the <code>test/</code> directory\
    \ with 4 CPUs and 4G memory per job and 8 jobs at once (<code>-j 8</code>) while\
    \ waiting 60 seconds for output files to appear on the shared filesystem (<code>-w\
    \ 60</code>).</p>\n<p>The cluster partition or queue to schedule jobs to is specified\
    \ with <code>-p YourClusterQueueName</code>.</p>\n<p>The above will run each rule\
    \ or job with 4 CPUs and 4GB memory each, which may be way more than needed or\
    \ not enough so you could create a YAML (or JSON) file to specify default and\
    \ specific resource requirements for some steps:</p>\n<p>Example <code>cluster-config.yaml</code>:</p>\n\
    <div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">__default__</span>:\n\
    \    <span class=\"pl-ent\">cpu</span>: <span class=\"pl-c1\">1</span>\n    <span\
    \ class=\"pl-ent\">partition</span>: <span class=\"pl-s\">YourClusterQueueName</span>\n\
    \    <span class=\"pl-ent\">memory</span>: <span class=\"pl-c1\">1024</span>\n\
    <span class=\"pl-ent\">samtools_index_bam_initial</span>:\n    <span class=\"\
    pl-ent\">cpu</span>: <span class=\"pl-c1\">32</span>\n    <span class=\"pl-ent\"\
    >memory</span>: <span class=\"pl-c1\">16384</span>\n<span class=\"pl-ent\">spades_assembly</span>:\n\
    \    <span class=\"pl-ent\">cpu</span>: <span class=\"pl-c1\">32</span>\n    <span\
    \ class=\"pl-ent\">memory</span>: <span class=\"pl-c1\">16384</span>\n<span class=\"\
    pl-ent\">bwa_mem</span>:\n    <span class=\"pl-ent\">cpu</span>: <span class=\"\
    pl-c1\">32</span>\n    <span class=\"pl-ent\">memory</span>: <span class=\"pl-c1\"\
    >4096</span>\n<span class=\"pl-ent\">mafft_msa</span>:\n    <span class=\"pl-ent\"\
    >cpu</span>: <span class=\"pl-c1\">32</span>\n    <span class=\"pl-ent\">memory</span>:\
    \ <span class=\"pl-c1\">4096</span>\n<span class=\"pl-ent\">iqtree</span>:\n \
    \   <span class=\"pl-ent\">cpu</span>: <span class=\"pl-c1\">8</span>\n    <span\
    \ class=\"pl-ent\">memory</span>: <span class=\"pl-c1\">4096</span>\n<span class=\"\
    pl-ent\">snpeff</span>:\n    <span class=\"pl-ent\">memory</span>: <span class=\"\
    pl-c1\">4096</span></pre></div>\n<p>With the <code>cluster-config.yaml</code>,\
    \ run the workflow in a cluster environment via</p>\n<pre><code>snakemake --directory\
    \ test --use-singularity --cluster-config cluster-config.yaml --drmaa \" -c {cluster.cpu}\
    \ -p {cluster.partition} --mem={cluster.memory} \" -j 8 -w 60\n</code></pre>\n\
    <p>With the above command and <code>cluster-config.yaml</code>, by default, a\
    \ rule or step in the workflow will only use 1 CPU and request 1G of memory, while\
    \ the rules like <code>iqtree</code> or <code>spades_assembly</code> will request\
    \ more CPUs and memory from the SLURM/DRMAA scheduler.</p>\n<p>See the <a href=\"\
    https://snakemake.readthedocs.io\" rel=\"nofollow\">Snakemake documentation</a>\
    \ for further details.</p>\n<h2>\n<a id=\"user-content-testing\" class=\"anchor\"\
    \ href=\"#testing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Testing</h2>\n<p>Tests cases are in the subfolder\
    \ <code>test</code>. They should be executed via continuous integration with Travis\
    \ CI.</p>\n<h2>\n<a id=\"user-content-output\" class=\"anchor\" href=\"#output\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Output</h2>\n<p>If you were to copy the files in <code>test</code>\
    \ (<code>samples.tsv</code>, <code>bam/</code> and <code>config.yaml</code>) to\
    \ a new directory <code>my-analysis-directory</code> and run the workflow on that\
    \ directory, i.e.</p>\n<div class=\"highlight highlight-source-shell\"><pre>snakemake\
    \ --directory my-analysis-directory/ <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> other args</span></pre></div>\n<p>The contents of <code>my-analysis-directory</code>\
    \ should look like:</p>\n<div class=\"highlight highlight-source-shell\"><pre>my-analysis-directory\n\
    \u251C\u2500\u2500 phylogeny <span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Phylogenetic Tree Output</span>\n\u2502   \u251C\u2500\u2500 genome-metadata.tsv\n\
    \u2502   \u2514\u2500\u2500 tree.html\n\u251C\u2500\u2500 config.yaml <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> INPUT: Workflow Execution Config File </span>\n\
    \u251C\u2500\u2500 qc <span class=\"pl-c\"><span class=\"pl-c\">#</span> Quality\
    \ Control Output</span>\n\u2502   \u251C\u2500\u2500 multiqc.html <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> MultiQC report file</span>\n\u2502   \u251C\
    \u2500\u2500 fastqc <span class=\"pl-c\"><span class=\"pl-c\">#</span> FastQC\
    \ Output</span>\n\u2502   \u2502   \u251C\u2500\u2500 Sample1.html\n\u2502   \u2502\
    \   \u2514\u2500\u2500 Sample1_fastqc.zip\n\u2502   \u251C\u2500\u2500 multiqc_data\n\
    \u2502   \u2502   \u251C\u2500\u2500 [Text files]\n\u2502   \u2514\u2500\u2500\
    \ quast <span class=\"pl-c\"><span class=\"pl-c\">#</span> QUAST Output</span>\n\
    \u2502       \u251C\u2500\u2500 report.tex\n\u2502       \u251C\u2500\u2500 icarus_viewers\n\
    \u2502       \u2502   \u2514\u2500\u2500 contig_size_viewer.html\n\u2502     \
    \  \u251C\u2500\u2500 report.html\n\u2502       \u251C\u2500\u2500 basic_stats\n\
    \u2502       \u2502   \u251C\u2500\u2500 [QUAST PDFs]\n\u2502       \u251C\u2500\
    \u2500 icarus.html\n\u2502       \u251C\u2500\u2500 transposed_report.tex\n\u2502\
    \       \u251C\u2500\u2500 quast.log\n\u2502       \u251C\u2500\u2500 report.pdf\n\
    \u2502       \u251C\u2500\u2500 report.txt\n\u2502       \u251C\u2500\u2500 .snakemake_timestamp\n\
    \u2502       \u251C\u2500\u2500 report.tsv\n\u2502       \u251C\u2500\u2500 transposed_report.tsv\n\
    \u2502       \u2514\u2500\u2500 transposed_report.txt\n\u251C\u2500\u2500 variant_calling\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> Variant Calling Output</span>\n\
    \u2502   \u251C\u2500\u2500 Sample1-filtered.vcf <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Filtered variants for Sample1 in VCF format</span>\n\u2502   \u251C\
    \u2500\u2500 Sample1.vcf <span class=\"pl-c\"><span class=\"pl-c\">#</span> Unfiltered\
    \ variants for Sample1 in VCF format</span>\n\u2502   \u251C\u2500\u2500 snpeff\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> SnpEff Output</span>\n\u2502\
    \   \u2502   \u251C\u2500\u2500 Sample1\n\u2502   \u2502   \u2502   \u251C\u2500\
    \u2500 [SnpEff specific files]\n\u2502   \u2502   \u251C\u2500\u2500 Sample1.vcf\n\
    \u2502   \u2502   \u251C\u2500\u2500 Sample1.csv\n\u2502   \u2502   \u251C\u2500\
    \u2500 Sample1.html <span class=\"pl-c\"><span class=\"pl-c\">#</span> SnpEff\
    \ report for Sample1</span>\n\u2502   \u2502   \u2514\u2500\u2500 Sample1.genes.txt\n\
    \u2502   \u2514\u2500\u2500 Sample1-vcf.tsv <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> SnpEff annotated variants in a tab-delimited table</span>\n\u251C\
    \u2500\u2500 mapping <span class=\"pl-c\"><span class=\"pl-c\">#</span> Read Mapping\
    \ Output</span>\n\u2502   \u2514\u2500\u2500 Sample1 <span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> Read mapping output and summary files for Sample1</span>\n\
    \u2502       \u251C\u2500\u2500 Sample1-extent.tsv\n\u2502       \u251C\u2500\u2500\
    \ Sample1-genome_extent.tsv\n\u2502       \u251C\u2500\u2500 Sample1-idxstats.tsv\n\
    \u2502       \u251C\u2500\u2500 Sample1.bam\n\u2502       \u251C\u2500\u2500 Sample1-depth.tsv\n\
    \u2502       \u251C\u2500\u2500 Sample1-idxstats-sorted.tsv\n\u2502       \u251C\
    \u2500\u2500 Sample1-idxstats-top_mapped.txt\n\u2502       \u2514\u2500\u2500\
    \ Sample1.bam.bai\n\u251C\u2500\u2500 bam <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Input directory with Sample1 BAM file specified in config.yaml</span>\n\
    \u2502   \u2514\u2500\u2500 a.bam\n\u251C\u2500\u2500 consensus <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Consensus Sequence Output</span>\n\u2502\
    \   \u2514\u2500\u2500 Sample1.fasta <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Consensus sequence for Sample1 from reference mapping and variant calling</span>\n\
    \u251C\u2500\u2500 logs <span class=\"pl-c\"><span class=\"pl-c\">#</span> Log\
    \ files for various tools</span>\n\u2502   \u251C\u2500\u2500 <span class=\"pl-k\"\
    >&lt;</span>tool name<span class=\"pl-k\">&gt;</span>\n\u2502   \u2502   \u2514\
    \u2500\u2500 Sample1.log\n\u251C\u2500\u2500 samples.tsv <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> INPUT: tab-delimited table with 2 fields: \"sample\"\
    \ and \"bam_file\"</span>\n\u251C\u2500\u2500 references <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Reference Genomes Downloaded From NCBI</span>\n\
    \u2502   \u251C\u2500\u2500 Sample1 <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Top Reference Genome</span>\n\u2502   \u2502   \u251C\u2500\u2500 reference.gff\n\
    \u2502   \u2502   \u251C\u2500\u2500 reference-no_ambig.fasta.bwt\n\u2502   \u2502\
    \   \u251C\u2500\u2500 reference-no_ambig.fasta.pac\n\u2502   \u2502   \u251C\u2500\
    \u2500 reference.genbank\n\u2502   \u2502   \u251C\u2500\u2500 reference-no_ambig.fasta.amb\n\
    \u2502   \u2502   \u251C\u2500\u2500 reference-no_ambig.fasta.ann\n\u2502   \u2502\
    \   \u251C\u2500\u2500 reference-no_ambig.fasta\n\u2502   \u2502   \u251C\u2500\
    \u2500 reference-no_ambig.fasta.sa\n\u2502   \u2502   \u251C\u2500\u2500 reference.fasta\n\
    \u2502   \u2502   \u2514\u2500\u2500 reference-no_ambig.fasta.fai\n\u2502   \u251C\
    \u2500\u2500 csf.msh <span class=\"pl-c\"><span class=\"pl-c\">#</span> Mash sketch\
    \ database from \"csf.fasta\"</span>\n\u2502   \u251C\u2500\u2500 csf.genbank\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> CSFV genomes downloaded from\
    \ NCBI in GenBank format</span>\n\u2502   \u2514\u2500\u2500 csf.fasta <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> CSFV genomes downloaded from NCBI in FASTA\
    \ format</span>\n\u251C\u2500\u2500 assembly <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Assembly Output</span>\n\u2502   \u251C\u2500\u2500 spades <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> SPAdes assembly outputs for each\
    \ input sample</span>\n\u2502   \u2502   \u2514\u2500\u2500 Sample1 <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> SPAdes assembly output for Sample1</span>\n\
    \u2502   \u2502       \u251C\u2500\u2500 before_rr.fasta\n\u2502   \u2502    \
    \   \u251C\u2500\u2500 params.txt\n\u2502   \u2502       \u251C\u2500\u2500 contigs.paths\n\
    \u2502   \u2502       \u251C\u2500\u2500 input_dataset.yaml\n\u2502   \u2502 \
    \      \u251C\u2500\u2500 <span class=\"pl-k\">&lt;</span>SPAdes specific output\
    \ directories<span class=\"pl-k\">&gt;</span>\n\u2502   \u2502       \u251C\u2500\
    \u2500 scaffolds.paths\n\u2502   \u2502       \u251C\u2500\u2500 contigs.fasta\n\
    \u2502   \u2502       \u251C\u2500\u2500 spades.log\n\u2502   \u2502       \u251C\
    \u2500\u2500 assembly_graph.fastg\n\u2502   \u2502       \u251C\u2500\u2500 dataset.info\n\
    \u2502   \u2502       \u251C\u2500\u2500 scaffolds.fasta\n\u2502   \u2502    \
    \   \u2514\u2500\u2500 assembly_graph_with_scaffolds.gfa\n\u2502   \u2514\u2500\
    \u2500 spades-Sample1.fasta\n\u251C\u2500\u2500 benchmarks <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Benchmark runtime info for tools in workflow</span>\n\
    \u2502   \u251C\u2500\u2500 <span class=\"pl-k\">&lt;</span>benchmark tab-delimited\
    \ files <span class=\"pl-k\">for</span> <span class=\"pl-smi\">various tools</span>\
    \ <span class=\"pl-k\">in</span> workflow<span class=\"pl-k\">&gt;</span>\n\u251C\
    \u2500\u2500 msa <span class=\"pl-c\"><span class=\"pl-c\">#</span> Multiple sequence\
    \ alignment (MSA) output and IQ-TREE/Clearcut phylogenetic tree</span>\n\u2502\
    \   \u251C\u2500\u2500 alignment.fasta\n\u2502   \u251C\u2500\u2500 samples-pre-aln.fasta\n\
    \u2502   \u2514\u2500\u2500 alignment.fasta.treefile\n\u2514\u2500\u2500 preprocess\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> Preprocessing Output of Input\
    \ BAM Files </span>\n    \u251C\u2500\u2500 samtools <span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> Initial BAM file stats output</span>\n    \u2502   \u251C\
    \u2500\u2500 depth\n    \u2502   \u2502   \u251C\u2500\u2500 Sample1-extent.tsv\n\
    \    \u2502   \u2502   \u251C\u2500\u2500 Sample1-genome_extent.tsv\n    \u2502\
    \   \u2502   \u2514\u2500\u2500 Sample1.tsv\n    \u2502   \u251C\u2500\u2500 flagstat\n\
    \    \u2502   \u2502   \u2514\u2500\u2500 Sample1.flagstat\n    \u2502   \u251C\
    \u2500\u2500 index\n    \u2502   \u2502   \u2514\u2500\u2500 Sample1.done\n  \
    \  \u2502   \u2514\u2500\u2500 idxstats\n    \u2502       \u251C\u2500\u2500 Sample1-top_mapped.txt\n\
    \    \u2502       \u251C\u2500\u2500 Sample1.tsv\n    \u2502       \u2514\u2500\
    \u2500 Sample1-sorted.tsv\n    \u251C\u2500\u2500 fastqs <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Deduplicated reads in FASTQ format</span>\n   \
    \ \u2502   \u2514\u2500\u2500 Sample1.fastq\n    \u251C\u2500\u2500 mash <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Mash Screen results</span>\n  \
    \  \u2502   \u251C\u2500\u2500 Sample1-screen_references-sorted.tsv\n    \u2502\
    \   \u2514\u2500\u2500 Sample1-screen_references.tsv\n    \u251C\u2500\u2500 trimmed_fastqs\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> Trimmomatic trimmed reads</span>\n\
    \    \u2502   \u2514\u2500\u2500 Sample1.fastq\n    \u2514\u2500\u2500 dedup <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Deduplicated BAM files</span>\n\
    \        \u251C\u2500\u2500 Sample1.bam\n        \u251C\u2500\u2500 Sample1.metrics.txt\n\
    \        \u2514\u2500\u2500 Sample1.bam.bai</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1566573045.0
pmitev/Teoroo-singularity:
  data_format: 2
  description: Singularity images and recipes
  filenames:
  - clease/Singularity.clease
  - deal.II/Singularity.deal
  - graphics/Singularity.gnuplot_5.4a
  - graphics/Singularity.gnuplot_4.6a
  - graphics/Singularity.gnuplot_5.4
  - graphics/Singularity.gnuplot_4.6
  - graphics/Singularity.graphics
  - graphics/Singularity.gnuplot_alpine
  - Atom/Singularity.atom
  - xcrysden/Singularity.xcrysden_1.5.60
  - xcrysden/Singularity.xcrysden
  - ase-twistd/Singularity.ase-twistd
  - tesseract/Singularity.tesseract
  - gromacs/Singularity.gromacs
  - jupyter/Singularity.jupyter
  - gdis/Singularity.gdis
  - rstudio-server/Singularity.rstudio-server
  - mongodb/Singularity.mongodb
  - MD2-lab/Singularity.md2-lab
  - pp/Singularity.pp2
  - tools/Singularity.vim
  - tools/Singularity.mc
  - tools/Singularity.gawk
  - tools/Singularity.gnuplot
  - tools/Singularity.meld
  - jmol/Singularity.jmol
  - ubuntu/Singularity.2004
  - ubuntu/Singularity.1804
  - texlive/Singularity.texlive
  - AMPE/Singularity.ampe
  - kmos/Singularity.kmos
  - kmos/Singularity.kmos3_9
  - lammps/Singularity.lammps
  - lammps/Singularity.lammps_ase
  - lammps/Singularity.lammps_prophet
  - lammps/Singularity.lammps_ase_kim
  - cuda/Singularity.u18.04_cuda9.2
  - VESTA/Singularity.vesta
  - obabel/Singularity.obabel
  - acroread/Singularity.acroread
  full_name: pmitev/Teoroo-singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2338" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-teoroo-singularity" class="anchor" href="#teoroo-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Teoroo-singularity</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - singularity-containers
  updated_at: 1622557404.0
pmonnahan/AncInf:
  data_format: 2
  description: Local Ancestry Inference
  filenames:
  - singularity/Singularity_defs.def
  full_name: pmonnahan/AncInf
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-ancestry-inference\" class=\"anchor\" href=\"\
    #ancestry-inference\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Ancestry Inference</h1>\n<p>Human local ancestry\
    \ inference using <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3738819/\"\
    \ rel=\"nofollow\">RFmix</a>.  The basic workflow is to parse the input PLINK\
    \ file by chromosome, perform reference-based haplotype phasing on the data using\
    \ <a href=\"https://odelaneau.github.io/shapeit4/\" rel=\"nofollow\">ShapeIt4</a>,\
    \ and, finally, perform local ancestry inference with RFMix.  More information\
    \ is provided in the <em>Pipeline Overview</em> below.  With the RFMix output,\
    \ admixture mapping (i.e. associating local ancestry with phenotype) can be accomplished\
    \ via a separate pipeline found <a href=\"https://github.com/pmonnahan/admixMap\"\
    >here</a>.</p>\n\n\n<p><strong>Table of Contents</strong></p>\n<ul>\n<li>\n<a\
    \ href=\"#requirements\">Requirements</a>\n<ul>\n<li><a href=\"#snakemake\">Snakemake</a></li>\n\
    <li><a href=\"#singularity\">Singularity</a></li>\n</ul>\n</li>\n<li>\n<a href=\"\
    #running-the-workflow\">Running the workflow</a>\n<ul>\n<li><a href=\"#other-notes\"\
    >Other Notes</a></li>\n<li><a href=\"#debugging-and-error-reports\">Debugging\
    \ and error reports</a></li>\n</ul>\n</li>\n<li>\n<a href=\"#pipeline-overview\"\
    >Pipeline Overview</a>\n<ul>\n<li><a href=\"#input-data\">Input Data</a></li>\n\
    <li><a href=\"#output\">Output</a></li>\n<li><a href=\"#reference-population\"\
    >Reference population</a></li>\n<li><a href=\"#phasing\">Phasing</a></li>\n<li><a\
    \ href=\"#local-ancestry-inference\">Local Ancestry Inference</a></li>\n</ul>\n\
    </li>\n</ul>\n\n<p><a href=\"https://github.com/pmonnahan/AncInf/blob/master/Pipeline_DAG.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pmonnahan/AncInf/raw/master/Pipeline_DAG.png\"\
    \ alt=\"Pipeline DAG\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<h3>\n<a id=\"\
    user-content-snakemake\" class=\"anchor\" href=\"#snakemake\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Snakemake</h3>\n\
    <p>The pipeline is coordinated and run on an HPC (or locally) using <em>Snakemake</em>.\
    \  To install snakemake, first create a virtual environment via:</p>\n<pre><code>module\
    \ load python3/3.6.3_anaconda5.0.1\nconda install -c conda-forge mamba\nmamba\
    \ create -c conda-forge -c bioconda -n &lt;your_environment_name&gt; snakemake\n\
    </code></pre>\n<p>This will create a new virtual environment and install <code>snakemake</code>.\
    \  Then, activate this environment and perform following installations:</p>\n\
    <pre><code>conda activate &lt;your_environment_name&gt;\nconda install numpy yaml\
    \ pandas\n</code></pre>\n<p>Anytime you need to run the pipeline, activate this\
    \ environment beforehand via:</p>\n<pre><code>conda activate &lt;environment_name&gt;\n\
    </code></pre>\n<p>If you choose not to create an environment, you must ensure\
    \ that these packages are installed and available for your python installation.</p>\n\
    <h3>\n<a id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity</h3>\n<p>The installation of the individual programs used\
    \ throughout this pipeline can be completely avoid by utilizing a Singularity\
    \ image.  This image is too large to be hosted on Github, although you can find\
    \ the definitions file used to create the image <a href=\"https://github.com/pmonnahan/AncInf/blob/master/singularity/Singularity_defs.def\"\
    >here</a>.  Building of images is still not currently supported at MSI, so I used\
    \ a Vagrant virtual machine, which comes with Singularity pre-configured/installed\
    \ (<a href=\"https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\"\
    \ rel=\"nofollow\">https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4</a>).\
    \  I can also share the img file directly upon request.</p>\n<p>However, in order\
    \ to utilize the singularity image, <em>singularity</em> must be installed on\
    \ the HPC.  Currently, the pipeline assumes that <em>singularity</em> will be\
    \ available as a module and can be loaded into the environment via the command\
    \ specified in the config.yml file, in the <code>module</code> entry under the\
    \  <code>singularity</code> section.  The default setting will work for MSI at\
    \ UMN.</p>\n<p>Singularity settings in config.yml</p>\n<pre><code>singularity:\n\
    \  use_singularity: 'true'\n  image: '/home/pmonnaha/pmonnaha/singularity/AncestryInference.sif\n\
    \  module: 'module load singularity'\n</code></pre>\n<h2>\n<a id=\"user-content-running-the-workflow\"\
    \ class=\"anchor\" href=\"#running-the-workflow\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running the workflow</h2>\n<p>Clone\
    \ this repository to the location where you want to store the output of the pipeline.</p>\n\
    <pre><code>git clone https://github.com/pmonnahan/AncInf.git rfmix_test\ncd rfmix_test\n\
    </code></pre>\n<p>The critical files responsible for executing the pipeline are\
    \ contained in the <em>./workflow</em> subdirectory contained within the cloned\
    \ repo.  They are:</p>\n<ul>\n<li>Snakefile</li>\n<li>config.yml</li>\n<li>cluster.yaml</li>\n\
    </ul>\n<p>The <em>Snakefile</em> is the primary workhouse of snakemake, which\
    \ specifies the dependencies of various parts of the pipeline and coordinates\
    \ execution.  No modifications to the <em>Snakefile</em> are necessary.</p>\n\
    <p>In order for the <em>Snakefile</em> to locate all of the necessary input and\
    \ correctly submit jobs to the cluster, <strong>both</strong> the <em>config.yaml</em>\
    \ and <em>cluster.yaml</em> need to be modified. Open these files and change the\
    \ required entries that are indicated with 'MODIFY'.  Other fields do not require\
    \ modification, although this may be desired given the particulars of the run\
    \ you wish to implement.  Details on each entry in the config file (e.g. what\
    \ the program expects in each entry as well as the purpose of the entry) are provided\
    \ in the <em>Pipeline Overview</em> at the bottom.</p>\n<p>The entire pipeline\
    \ can be executed on a local machine (not recommended) or on an HPC, and the <em>cluster.yaml</em>\
    \ file is required only for the latter.  For a local run, change the <code>local_run</code>\
    \ entry to <code>true</code> under the <code>run_settings</code> section of the\
    \ config file, and launch snakemake from within the parent directory by the simple\
    \ command:</p>\n<pre><code>snakemake\n</code></pre>\n<p>However, multiple steps\
    \ in the pipeline have high resource demands, and so are unlikely to be able to\
    \ be run locally.  This option exists primarily for testing and troubleshooting,\
    \ so the remainder of the  documentation assumes that the pipeline will be executed\
    \ on an HPC.  In order to coordinate the use of the HPC, the following modifications\
    \ to the snakemake command are required:</p>\n<pre><code>snakemake --cluster \"\
    sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 32\n</code></pre>\n<p>where -j specifies the number of jobs that can be submitted\
    \ at once.</p>\n<p>One additional setting in the <em>config.yml</em> is needed\
    \ in order to correctly submit jobs to the HPC.  The relevant entries are under\
    \ the <code>run_settings</code> section of the config file:</p>\n<pre><code>run_settings:\n\
    \  local_run: 'false'\n  cluster_config: 'workflow/cluster_slurm.yaml'\n  scheduler:\
    \ 'slurm'\n</code></pre>\n<p>Here, it is necessary that the <code>cluster_config</code>\
    \ entry is set to the path of the cluster_slurm.yaml file that will be used in\
    \ the snakemake command.  Also, the scheduler must correspond to the syntax used\
    \ in the snakemake command and cluster.yaml file.  I should point out that these\
    \ additional changes are needed for responsibly using PLINK within a snakemake\
    \ framework, and are not directly needed for snakemake.  PLINK will attempt to\
    \ auto-detect available resources upon running regardless of the resources that\
    \ were requested when the job was submitted.  Therefore, we have to read and parse\
    \ the requested resources in the cluster config file in order for them to be communicated\
    \ to PLINK from within the Snakefile.</p>\n<h3>\n<a id=\"user-content-other-notes\"\
    \ class=\"anchor\" href=\"#other-notes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Other notes</h3>\n<p>It is recommended\
    \ that <em>snakemake</em> is run as an interactive session on an HPC.  <em>Snakemake</em>\
    \ will launch the specified number (via the -j flag) of jobs, and then will hang\
    \ and wait for them to finish.  As jobs finish (and assuming no errors), <em>snakemake</em>\
    \ will launch additional jobs keeping the total running jobs at whatever -j is\
    \ set for.  Although <em>snakemake</em> should not use a lot of memory, it could\
    \ have long run times, which is generally not advisable on login nodes.</p>\n\
    <p>One attractive feature of <em>snakemake</em> is its ability to keep track of\
    \ the progress and dependencies of the different stages of the pipeline.  Specifically,\
    \ if an error is encountered or the pipeline otherwise stops before the final\
    \ step, <em>snakemake</em> can resume the pipeline where it left off, avoiding\
    \ redundant computation for previously completed tasks.  To do so, simply resubmit\
    \ the original <em>snakemake</em> command.</p>\n<p>To run a specific part of the\
    \ pipeline, do:</p>\n<pre><code>snakemake -R &lt;rule_name&gt; --cluster \"sbatch\
    \ --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 20 --rerun-incomplete\n</code></pre>\n<p>where <em>rule_name</em> indicates\
    \ the 'rule' (i.e. job) in the Snakefile that you wish to run.  Or, you can request\
    \ a specific file by providing the filename at the end of the command.  You may\
    \ need to include the -F (i.e. force) if the output file already exists and you\
    \ want to overwrite it.</p>\n<p>Also, it is often very helpful to do a 'dry-run'\
    \ of the pipeline in which the different steps and dependencies are printed to\
    \ screen, but no actual jobs are executed.  This can be helpful to ensure that\
    \ config entries are correct, etc.  To perform a dry-run, do:</p>\n<pre><code>snakemake\
    \ -nrp\n</code></pre>\n<p>NOTE: It is convenient to make an alias in your ~/.bashrc\
    \ file to run snakemake on the cluster without having to type the --cluster...\
    \ part of the command every time.  For me, it looked like this:</p>\n<pre><code>alias\
    \ snakeslurm=\"snakemake -k --cluster 'sbatch --no-requeue --partition={cluster.p}\
    \ --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --job-name={cluster.job-name}\
    \ --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}' --cluster-config workflow/cluster_slurm.yaml\"\
    \n</code></pre>\n<p>This way, I can just do:</p>\n<pre><code>snakeslurm -j 25\n\
    </code></pre>\n<p>To launch snakemake on the cluster.</p>\n<h4>\n<a id=\"user-content-unlocking-the-working-directory\"\
    \ class=\"anchor\" href=\"#unlocking-the-working-directory\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Unlocking\
    \ the working directory</h4>\n<p>When <em>snakemake</em> is launched it will place\
    \ a lock on the working directory, such that other <em>snakemake</em> runs are\
    \ prohibited from starting.  When <em>snakemake</em> finishes or errors out, it\
    \ will remove this lock.  However, sometimes this lock is not correctly removed.\
    \  This can occur, for example, if the VPN drops connection while <em>snakemake</em>\
    \ is running.  If you receive a \"Directory cannot be locked...\" error message\
    \ from <em>snakemake</em> and you are sure that no other <em>snakemake</em> processes\
    \ are currently running, you can unlock the directory by:</p>\n<pre><code>snakemake\
    \ --unlock\n</code></pre>\n<p>Then, you can run the usual <em>snakemake</em> command\
    \ to restart the pipeline.</p>\n<h4>\n<a id=\"user-content-debugging-and-error-reports\"\
    \ class=\"anchor\" href=\"#debugging-and-error-reports\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Debugging\
    \ and error reports</h4>\n<p>Should an error be encountered in a job, snakemake\
    \ will halt the pipeline and indicate in the terminal that an error has occurred.\
    \  The offending job will also be printed in red in the terminal window.  More\
    \ information on why the job failed can be found in the 'stdout' and 'stderr'\
    \ files that are output to the <em>'OandE'</em> directory and will be labelled\
    \ with the jobname.</p>\n<h2>\n<a id=\"user-content-pipeline-overview\" class=\"\
    anchor\" href=\"#pipeline-overview\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Overview</h2>\n<h3>\n\
    <a id=\"user-content-input-data\" class=\"anchor\" href=\"#input-data\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Input\
    \ Data</h3>\n<p>The pipeline expects as input a single set of PLINK files (.bed,\
    \ .fam, .bim) that has gone through basic QC steps (missingness, hwe, maf, etc).\
    \  I have written QC pipelines for non-imputed and imputed data, which are available\
    \ <a href=\"https://github.com/pmonnahan/DataPrep\">here</a> and <a href=\"https://github.com/pmonnahan/DataPrep/tree/master/postImpute\"\
    >here</a>, respectively.  It is technically possible to use imputed data in ancestry\
    \ inference, although this is not widely seen throughout the literature.</p>\n\
    <p>The input PLINK files are specified in the <code>query</code> entry within\
    \ the config file.</p>\n<pre><code>query: \"PATH_TO_PLINK_PREFIX\" \nsamples:\
    \ \"all\"  \n</code></pre>\n<p>The user can also provide a path to a file in the\
    \ <code>samples</code> entry, in which case the program will subset the <code>query</code>\
    \ dataset to include only the samples in the file (one sample per line).</p>\n\
    <p>It is assumed that the query coordinates and chromosome names are consistent\
    \ with those used in the reference VCF (see below).</p>\n<h3>\n<a id=\"user-content-output\"\
    \ class=\"anchor\" href=\"#output\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Output</h3>\n<p>All output is\
    \ labelled using the prefix specified in the <code>outname</code> entry in the\
    \ config file.</p>\n<pre><code>outname: \"AncInf\"\n</code></pre>\n<p>The RFMix\
    \ results will be output to the <em>rfmix</em> directory that is automatically\
    \ created. RFMix outputs a number of files, but the most relevant files are those\
    \ ending in <em>.Q</em> (which contain the global ancestry percentage estimates\
    \ for each individual) and the files ending in <em>.msp.tsv</em> (which contain\
    \ the maximum-likelihood ancestry state in each window analyzed; i.e. local ancestry).\
    \  The <em>.Q</em> files can be easily filtered to isolate individuals of a given\
    \ ethnicity, based on user-provided thresholds.</p>\n<p>A set of phased BCF files\
    \ (separated by chromosome) are generated as an intermediate step and are saved\
    \ to the <em>phased</em> directory.  This directory will also contain the phased\
    \ BCF of the individuals from the reference population.</p>\n<p>A good initial\
    \ check that the results make sense is to simply look at the average local ancestry\
    \ along a chromosome.  A full collection of these images (one for each chromosome)\
    \ will be created and output into the <em>chrom_plots</em> folder within the master\
    \ run directory.  These averages should remain fairly stable across the chromosome.\
    \  Any large, sudden changes in the dominant ancestral component are indicative\
    \ of issues in phasing or ancestry inference.  Furthermore, these chromosome plots\
    \ should be inspected to identify areas of suspect inference.  For example, drastic\
    \ changes in average ancestry is often observed near centromeres or telomeres.\
    \  These can also likely be flagged by low SNP counts in the inferred windows\
    \ (which is reported in the <em>.msp.tsv</em> files).</p>\n<h3>\n<a id=\"user-content-reference-population\"\
    \ class=\"anchor\" href=\"#reference-population\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Reference population</h3>\n<p>The\
    \ reference VCF to be used for phasing as well as for ancestry inference is provided\
    \ under the <code>reference</code> section of the config file.  The pipeline is\
    \ currently set up to use the 1000Genomes VCF (available <a href=\"https://www.internationalgenome.org/\"\
    \ rel=\"nofollow\">here</a> or by request) for the reference population.  However,\
    \ any VCF should work in theory as long as the necessary accessory files are provided.</p>\n\
    <pre><code>reference:\n  vcf: \"PATH_TO_REFERENCE_VCF\"\n  subpops: \"accessory/1000G_PopLabels.txt\"\
    \n  genmap: \"PATH_TO_DATA_SUBDIRECTORY/genetic_map_hg19.txt\"\n  phased_bcf:\
    \ 'none`  \n</code></pre>\n<p>There are two required files that need to accompany\
    \ the reference VCF, and these are provided at the <code>subpops</code> and <code>genmap</code>\
    \ entries.  The <code>subpops</code> file should be a text file with two columns:\
    \ sample ID as it appears in the VCF in the first column and the subpopulation\
    \ label for that sample in the second column.  If using the 1000Genomes VCF, then\
    \ the <code>subpop</code> file was automatically downloaded to the <em>accessory</em>\
    \ subdirectory.  The <code>genmap</code> file specifies the genetic map for the\
    \ reference genome and is too large to be hosted on GitHub.  However, the hg19\
    \ genetic map is available <a href=\"https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.html\"\
    \ rel=\"nofollow\">here</a> or by request.  The file contains 3 space-delimited\
    \ columns: chromosome, base position, genetic position.</p>\n<p>It is assumed\
    \ that the reference VCF file has been filtered, phased, and indexed. The VCF\
    \ does NOT need to be subsetted to include only the individuals from the desired\
    \ reference subpopulations.  This is accomplished by the initial steps of the\
    \ pipeline, using the <code>subpops</code> file described above along with the\
    \ comma-separated lists (no spaces!) in the <code>ref_pops</code> and <code>pop_names</code>\
    \ entries under the <code>rfmix</code> section of the config file.</p>\n<pre><code>rfmix:\n\
    \  ref_pops: \"YRI,GWD,ESN,CEU,IBS,TSI\" # No spaces!!\n  pop_names: \"AFR,AFR,AFR,EUR,EUR,EUR\"\
    \ \n</code></pre>\n<p>Based on the information contained in the <code>subpops</code>\
    \ file described above, individuals corresponding to the subpopulation names provided\
    \ in <code>ref_pops</code> entry are extracted from the reference VCF.  In addition,\
    \ a new file is created at:</p>\n<pre><code> accessory/Population_Map_File.txt\n\
    </code></pre>\n<p>, which re-labels the subsetted individuals with the corresponding\
    \ value in the <code>pop_names</code>.</p>\n<p>There is expected to be a 1:1 ordered\
    \ correspondence between the subpopulation labels <code>ref_pops</code> and the\
    \ superpopulation names in <code>pop_names</code>.  In this example where we are\
    \ interesting in inferring 2-way admixture between AFR and EUR populations, all\
    \ YRI, GWD, and ESN individuals would be extracted and re-labelled as AFR individuals,\
    \ while the CEU, IBS, and TSI individuals would be labelled as EUR individuals.\
    \  This scheme was developed to allow for flexibility in the inclusion/exclusion\
    \ of particular subpopulations.</p>\n<p>RFMix will sample randomly from within\
    \ these superpopulations to generate the training/test sets needed for the machine\
    \ learning algorithm.  It is best if the reference individuals from a superpopulation\
    \ are evenly distributed across subpopulations, so that a single subpopulation\
    \ does not dominate during the resampling.</p>\n<h3>\n<a id=\"user-content-phasing\"\
    \ class=\"anchor\" href=\"#phasing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Phasing</h3>\n<p>The config file\
    \ has the following options for modifying the behavior of haplotype phasing in\
    \ ShapeIt4:</p>\n<pre><code>phase:\n  threads: \"12\"\n  pbwt_depth: \"4\"\n \
    \ sequence_data: 'true'\n</code></pre>\n<p>Increasing the <code>pbwt_depth</code>\
    \ may increase the phasing accuracy, but comes at a substantial computational\
    \ cost.  The <code>sequence_data</code> entry should be set to false if the data\
    \ comes from an array.</p>\n<h3>\n<a id=\"user-content-local-ancestry-inference\"\
    \ class=\"anchor\" href=\"#local-ancestry-inference\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Local Ancestry\
    \ Inference</h3>\n<p>In addition to the <code>ref_pops</code> and <code>pop_names</code>,\
    \ the <code>rfmix</code> section of the config file provides a number of options\
    \ for modifying the behavior of RFMix.</p>\n<pre><code>rfmix:\n  ref_pops: \"\
    YRI,GWD,ESN,CEU,IBS,TSI\" # No spaces!!\n  pop_names: \"AFR,AFR,AFR,EUR,EUR,EUR\"\
    \ \n  generations: \"8\"\n  reanalyze_reference: \"true\" \n  window_size: \"\
    0.02\" \n  threads: \"12\"\n</code></pre>\n<p>The <code>generations</code> entry\
    \ specifies the number of generations in the past when admixture between the superpopulations\
    \ is assumed to have begun.  Values used in the literature are typically approximations\
    \ based off of historical events or genomic dating methods.  <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4289685/#:~:text=Patterns%20of%20Genetic%20Ancestry%20of%20Self%2DReported%20Latinos&amp;text=On%20average%2C%20we%20estimate%20that,%2C%20and%206.2%25%20African%20ancestry\"\
    \ rel=\"nofollow\">Bryc et al 2015</a> provide a good reference for African American\
    \ and Latinx ancestry inference.  For both scenarios, they modelled admixture\
    \ between Europeans and Native Americans at 11-12 generations ago and subsequent\
    \ admixture with Africans 6-8 generations ago.  Unfortunately, RFMix only allows\
    \ the user to specify a single value, so I have used '8' for African Americans\
    \ (modelling 2-way admixture between AFR and EUR) and '12' for Latinx individuals\
    \ (modelling 3-way admixture between AFR, EUR, and AMR)</p>\n<p>In the case that\
    \ a set of reference haplotypes may not be of \"pure\" ancestry and may themselves\
    \ be somewhat admixed, the option --reanalyze-reference will cause the program\
    \ to iteratively analyze the reference haplotypes as if they were query haplotypes,\
    \ in addition to analyzing the query input (see the <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3738819/\"\
    \ rel=\"nofollow\">RFmix</a> paper for a more thorough explanation of this procedure).\
    \  This is often advised for inferring local ancestry in Latinx populations, where\
    \ a 3-way AFR, EUR, and AMR admixture is modelled.  However, it is likely not\
    \ necessary for inferring ancestry in African American populations, where the\
    \ ancestral populations likely do not contain any admixed individuals.</p>\n<p>The\
    \ last relevant option is the window size in which ancestry is to be inferred.\
    \  This value is specified in centiMorgans (cM).  Default is 0.2 cM, which corresponds\
    \ to ~100 - 150 kb windows.  For a given window, there is a minimum requirement\
    \ on the number of SNPs, and windows will be expanded to meet this requirement\
    \ regardless of the specified window size.</p>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1616969120.0
pmonnahan/DataPrep:
  data_format: 2
  description: start with raw plink, end with standardized QCed plink
  filenames:
  - workflow/Singularity_defs.def
  full_name: pmonnahan/DataPrep
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-pre-imputation-qc-pipeline\" class=\"anchor\"\
    \ href=\"#pre-imputation-qc-pipeline\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pre-imputation QC pipeline</h1>\n\
    <p>The purpose of this pipeline is to perform the following for a set of input\
    \ PLINK datasets:</p>\n<ul>\n<li>basic QC (genotype/variant missingness, HWE,\
    \ and minor allele frequency)</li>\n<li>harmonize allele specifications with the\
    \ GRCh37 reference genome</li>\n<li>produce a set of VCF files (separated by chromosome)\
    \ for imputation</li>\n<li>merge filtered datasets into a single dataset consisting\
    \ only of overlapping sites.</li>\n</ul>\n<p>A companion pipeline, which performs\
    \ post-imputation QC, will download alongside the pre-imputation pipeline.  To\
    \ use the post-imputation pipeline, see the README in the postImpute directory.</p>\n\
    \n\n<p><strong>Table of Contents</strong></p>\n<ul>\n<li>\n<a href=\"#requirements\"\
    >Requirements</a>\n<ul>\n<li><a href=\"#snakemake\">Snakemake</a></li>\n<li><a\
    \ href=\"#singularity\">Singularity</a></li>\n</ul>\n</li>\n<li>\n<a href=\"#running-the-workflow\"\
    >Running the workflow</a>\n<ul>\n<li><a href=\"#other-notes\">Other Notes</a></li>\n\
    <li><a href=\"#debugging-and-error-reports\">Debugging and error reports</a></li>\n\
    </ul>\n</li>\n<li>\n<a href=\"#pipeline-overview\">Pipeline Overview</a>\n<ul>\n\
    <li><a href=\"#input-data\">Input Data</a></li>\n<li><a href=\"#output\">Output</a></li>\n\
    <li><a href=\"#dataset-harmonization\">Data Harmonization</a></li>\n<li><a href=\"\
    #reference-allele-fixing\">Reference allele fixing</a></li>\n<li><a href=\"#basic-qc\"\
    >Basic QC</a></li>\n<li><a href=\"#merging-inputs-optional\">Merging Inputs (Optional)</a></li>\n\
    <li><a href=\"#imputaton-preparation\">Imputation Preparation</a></li>\n</ul>\n\
    </li>\n</ul>\n\n<p><a href=\"https://github.com/pmonnahan/DataPrep/blob/master/Pipeline_DAG.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/pmonnahan/DataPrep/raw/master/Pipeline_DAG.png\"\
    \ alt=\"Pipeline DAG\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<h3>\n<a id=\"\
    user-content-snakemake\" class=\"anchor\" href=\"#snakemake\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Snakemake</h3>\n\
    <p>The pipeline is coordinated and run on an HPC (or locally) using <em>Snakemake</em>.\
    \  To install snakemake, first create a virtual environment via:</p>\n<pre><code>module\
    \ load python3/3.6.3_anaconda5.0.1\nconda install -c conda-forge mamba\nmamba\
    \ create -c conda-forge -c bioconda -n &lt;your_environment_name&gt; snakemake\n\
    </code></pre>\n<p>This will create a new virtual environment and install <code>snakemake</code>.\
    \  Then, activate this environment and perform following installations:</p>\n\
    <pre><code>conda activate &lt;your_environment_name&gt;\nconda install numpy yaml\
    \ pandas\n</code></pre>\n<p>Anytime you need to run the pipeline, activate this\
    \ environment beforehand via:</p>\n<pre><code>conda activate &lt;environment_name&gt;\n\
    </code></pre>\n<p>If you choose not to create an environment, you must ensure\
    \ that these packages are installed and available for your python installation.</p>\n\
    <h3>\n<a id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity</h3>\n<p>The installation of the individual programs \
    \ used throughout this pipeline can be completely avoid by utilizing a Singularity\
    \ image.  This image is too large to be hosted on Github, although you can find\
    \ the definitions file used to create the image <a href=\"https://github.com/pmonnahan/AncInf/blob/master/singularity/Singularity_defs.def\"\
    >here</a>.  Building of images is still not currently supported at MSI, so I used\
    \ a Vagrant virtual machine, which comes with Singularity pre-configured/installed\
    \ (<a href=\"https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\"\
    \ rel=\"nofollow\">https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4</a>).\
    \  I can also share the img file directly upon request.</p>\n<p>However, in order\
    \ to utilize the singularity image, <em>Singularity</em> must be installed on\
    \ the HPC.  Currently, the pipeline assumes that <em>Singularity</em> will be\
    \ available as a module and can be loaded into the environment via the command\
    \ specified in the config.yml file, where it says 'singularity_module'.  The default\
    \ setting will work for MSI at UMN.</p>\n<p>Singularity settings in config.yml</p>\n\
    <pre><code>singularity:\n  use_singularity: 'true'\n  image: '/home/pmonnaha/pmonnaha/singularity/AncestryInference.sif\n\
    </code></pre>\n<h2>\n<a id=\"user-content-running-the-workflow\" class=\"anchor\"\
    \ href=\"#running-the-workflow\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Running the workflow</h2>\n<p>First,\
    \ activate the virtual environment into which snakemake was installed:</p>\n<pre><code>conda\
    \ activate &lt;environment_name&gt;\n</code></pre>\n<p>Clone the parent repository\
    \ to the location where you want to store the output of the pipeline.</p>\n<pre><code>git\
    \ clone https://github.com/pmonnahan/DataPrep.git preImputeQC\ncd preImputeQC\n\
    </code></pre>\n<p>The critical files responsible for executing the pipeline are\
    \ contained in the <em>./workflow</em> subdirectory contained within the cloned\
    \ repo.  They are:</p>\n<ul>\n<li>Snakefile</li>\n<li>config.yml</li>\n<li>cluster.yaml</li>\n\
    </ul>\n<p>The <em>Snakefile</em> is the primary workhouse of snakemake, which\
    \ specifies the dependencies of various parts of the pipeline and coordinates\
    \ execution.  No modifications to the <em>Snakefile</em> are necessary.</p>\n\
    <p>In order for the <em>Snakefile</em> to locate all of the necessary input and\
    \ correctly submit jobs to the cluster, <strong>both</strong> the <em>config.yaml</em>\
    \ and <em>cluster.yaml</em> need to be modified. Open these files and change the\
    \ required entries that are indicated with 'MODIFY'.  Other fields do not require\
    \ modification, although this may be desired given the particulars of the run\
    \ you wish to implement.  Details on each entry in the config file (e.g. what\
    \ the program expects in each entry as well as the purpose of the entry) are provided\
    \ in the <em>Pipeline Overview</em> at the bottom.  Note: Only use letters and\
    \ numbers when naming output files or datasets as this may cause issues with the\
    \ report creation.</p>\n<p>The entire pipeline can be executed on a local machine\
    \ (not recommended) or on an HPC, and the <em>cluster.yaml</em> file is required\
    \ only for the latter.  For a local run, change the <code>local_run</code> entry\
    \ to <code>true</code> under the <code>run_settings</code> section of the config\
    \ file, and launch snakemake from within the parent directory by the simple command:</p>\n\
    <pre><code>snakemake\n</code></pre>\n<p>However, multiple steps in the pipeline\
    \ have high resource demands, and so are unlikely to be able to be run locally.\
    \  This option exists primarily for testing and troubleshooting, so the remainder\
    \ of the  documentation assumes that the pipeline will be executed on an HPC.\
    \  In order to coordinate the use of the HPC, the following modifications to the\
    \ snakemake command are required:</p>\n<pre><code>snakemake --cluster \"sbatch\
    \ --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 32\n</code></pre>\n<p>where -j specifies the number of jobs that can be submitted\
    \ at once.</p>\n<p>One additional setting in the <em>config.yml</em> is needed\
    \ in order to correctly submit jobs to the HPC.  The relevant entries are under\
    \ the <code>run_settings</code> section of the config file:</p>\n<pre><code>run_settings:\n\
    \  local_run: 'false'\n  cluster_config: 'workflow/cluster_slurm.yaml'\n  scheduler:\
    \ 'slurm'\n</code></pre>\n<p>Here, it is necessary that the <code>cluster_config</code>\
    \ entry is set to the path of the cluster_slurm.yaml file that will be used in\
    \ the snakemake command.  Also, the scheduler must correspond to the syntax used\
    \ in the snakemake command and cluster.yaml file.  I should point out that these\
    \ additional changes are needed for responsibly using PLINK within a snakemake\
    \ framework, and are not directly needed for snakemake.  PLINK will attempt to\
    \ auto-detect available resources upon running regardless of the resources that\
    \ were requested when the job was submitted.  Therefore, we have to read and parse\
    \ the requested resources in the cluster config file in order for them to be communicated\
    \ to PLINK from within the Snakefile.</p>\n<h3>\n<a id=\"user-content-other-notes\"\
    \ class=\"anchor\" href=\"#other-notes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Other notes</h3>\n<p>It is recommended\
    \ that <em>snakemake</em> is run as an interactive session on an HPC.  <em>Snakemake</em>\
    \ will launch the specified number (via the -j flag) of jobs, and then will hang\
    \ and wait for them to finish.  As jobs finish (and assuming no errors), <em>snakemake</em>\
    \ will launch additional jobs keeping the total running jobs at whatever -j is\
    \ set for.  Although <em>snakemake</em> should not use a lot of memory, it could\
    \ have long run times, which is generally not advisable on login nodes.</p>\n\
    <p>One attractive feature of <em>snakemake</em> is its ability to keep track of\
    \ the progress and dependencies of the different stages of the pipeline.  Specifically,\
    \ if an error is encountered or the pipeline otherwise stops before the final\
    \ step, <em>snakemake</em> can resume the pipeline where it left off, avoiding\
    \ redundant computation for previously completed tasks.  To do so, simply resubmit\
    \ the original <em>snakemake</em> command.</p>\n<p>To run a specific part of the\
    \ pipeline, do:</p>\n<pre><code>snakemake -R &lt;rule_name&gt; --cluster \"sbatch\
    \ --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem}\
    \ --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml\
    \ -j 20 --rerun-incomplete\n</code></pre>\n<p>where <em>rule_name</em> indicates\
    \ the 'rule' (i.e. job) in the Snakefile that you wish to run.  Or, you can request\
    \ a specific file by providing the filename at the end of the command.  You may\
    \ need to include the -F (i.e. force) if the output file already exists and you\
    \ want to overwrite it.</p>\n<p>Also, it is often very helpful to do a 'dry-run'\
    \ of the pipeline in which the different steps and dependencies are printed to\
    \ screen, but no actual jobs are executed.  This can be helpful to ensure that\
    \ config entries are correct, etc.  To perform a dry-run, do:</p>\n<pre><code>snakemake\
    \ -nrp\n</code></pre>\n<p>NOTE: It is convenient to make an alias in your ~/.bashrc\
    \ file to run snakemake on the cluster without having to type the --cluster...\
    \ part of the command every time.  For me, it looked like this:</p>\n<pre><code>alias\
    \ snakeslurm=\"snakemake -k --cluster 'sbatch --no-requeue --partition={cluster.p}\
    \ --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --job-name={cluster.job-name}\
    \ --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type}\
    \ -o {cluster.o} -e {cluster.e} -A {cluster.A}' --cluster-config workflow/cluster_slurm.yaml\"\
    \n</code></pre>\n<p>This way, I can just do:</p>\n<pre><code>snakeslurm -j 25\n\
    </code></pre>\n<p>To launch snakemake on the cluster.</p>\n<h4>\n<a id=\"user-content-debugging-and-error-reports\"\
    \ class=\"anchor\" href=\"#debugging-and-error-reports\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Debugging\
    \ and error reports</h4>\n<p>Should an error be encountered in a job, snakemake\
    \ will halt the pipeline and indicate in the terminal that an error has occurred.\
    \  The offending job will also be printed in red in the terminal window.  More\
    \ information on why the job failed can be found in the 'stdout' and 'stderr'\
    \ files that are output to the <em>'OandE'</em> directory and will be labelled\
    \ with the jobname.</p>\n<h2>\n<a id=\"user-content-pipeline-overview\" class=\"\
    anchor\" href=\"#pipeline-overview\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Overview</h2>\n<h3>\n\
    <a id=\"user-content-input-data\" class=\"anchor\" href=\"#input-data\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Input\
    \ Data</h3>\n<p>Under the 'query' section, you can specify the inputs for one\
    \ or more datasets.  Each dataset should be uniquely named (Note: avoid using\
    \ periods or underscores when naming output files or datasets as this may cause\
    \ issues with the report creation.) with values specified for the following \"\
    keys\":</p>\n<ul>\n<li>\n<strong>data</strong>: path to the PLINK files (just\
    \ the PLINK prefix).</li>\n<li>\n<strong>chrom_key</strong>: tab-delimited text\
    \ file with 2 columns (no header).  The first column contains the old chromosome\
    \ names, and the second column contains the new names.\n<ul>\n<li>Used for converting\
    \ to numeric names. e.g chr10 to 10.</li>\n</ul>\n</li>\n<li>\n<strong>allele_key</strong>:\
    \ tab-delimited text file with 5 columns (no header).  First column is snpID and\
    \ following columns are: old_allele1 old_allele2 new_allele1 new_allele2.\n<ul>\n\
    <li>Used for converting alleles with A/B specification to ACGT.  Oftentimes provided\
    \ in the dbGaP download.  If alleles are already specified in ACGT format, this\
    \ field can be set to 'none'</li>\n</ul>\n</li>\n<li>\n<strong>ID_key</strong>:\
    \ tab-delimited text file with 2 columns (no header).  First column is old SNP\
    \ ID and second column is new SNP ID.\n<ul>\n<li>Used for converting to rsID format.\
    \  If SNP IDs are already in rs-format, this field can be set to 'none'</li>\n\
    </ul>\n</li>\n<li>\n<strong>flip_key</strong>: text file with single column containing\
    \ SNP rsIDs that need to be flipped in order to align strand to the hg19 reference\
    \ genome.\n<ul>\n<li>Used to harmonize strand across datasets to the hg19 reference\
    \ genome.  Set this field to 'none' if all alleles are already on the same strand\
    \ as the target reference genome.</li>\n</ul>\n</li>\n</ul>\n<p>Each of these\
    \ fields are optional and providing 'none' as the entry will disable the steps\
    \ associated with each key.  However, these fields should only be set to 'none'\
    \ if you are sure that they are not necessary (e.g. you have already fixed any\
    \ existing strand issues across datasets).</p>\n<p>Example of input specifications\
    \ in the config file:</p>\n<pre><code>query:\n  \"dataset1\":\n    data: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\
    \n    chrom_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    allele_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\
    \n    ID_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    flip_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\
    \n  \"dataset2\":\n    data: \"PATH/TO/PLINK/PREFIX/FOR/DATASET2\"\n    chrom_key:\
    \ \"none\"\n    allele_key: \"none\"\n    ID_key: \"none\"\n    flip_key: \"none\"\
    \n</code></pre>\n<p>Phenotypes of the samples must be specified by a tab-delimited\
    \ text file where the first column contains the sample IDs (as they appear in\
    \ the imputed VCF file) and the second column contains the phenotype. The path\
    \ to this file can be provided in the field labelled 'phenotype_file' under the\
    \ 'phenotype_data' field in the config.yml file.</p>\n<p>Sex of the samples must\
    \ also be specified in a tab-delimited text file where the first column is sample\
    \ ID and the second column is the sex specification according to PLINK.  The path\
    \ to this file can be provided in the field labelled 'sex_file' under the 'phenotype_data'\
    \ field in the config.yml file.</p>\n<pre><code>phenotype_data: \n  pheno_file:\
    \ \"none\"\n  sex_file: \"/path/to/sex/file\"\n</code></pre>\n<h3>\n<a id=\"user-content-output\"\
    \ class=\"anchor\" href=\"#output\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Output</h3>\n<p>The output is\
    \ a set of PLINK files in the parent directory labelled with the value provided\
    \ in the 'outname' entry of the config file.  However, if 'merge' is set to 'false'\
    \ in the config file, this final merge step is skipped, and the final output would\
    \ be the set of QC'ed plink files within each subdirectory labelled with the dataset\
    \ names.  Within each of these subdirectories, there will also be a set of VCF\
    \ files, which are suitable for use in either the Michigan or TOPMed imputation\
    \ servers.</p>\n<p>The other primary output is a PDF report containing a summary\
    \ of various steps in the pipeline.  It is <strong>highly recommended</strong>\
    \ that the user carefully review this report to confirm that everything seems\
    \ in order.  Particular attention should be paid to whether specific steps have\
    \ resulted in major loss of markers as well as whether there is a positive correlation\
    \ between allele frequencies in the 1000Genomes dataset and allele frequencies\
    \ in each of the query datasets.  These scatter plots are provided towards the\
    \ end of the report, and if a substantial subset of the points exhibit an anti-correlation,\
    \ this is indicative of a preponderance of strand errors that ought to be corrected\
    \ (via the 'flip_key') prior to proceeding.</p>\n<h3>\n<a id=\"user-content-dataset-harmonization\"\
    \ class=\"anchor\" href=\"#dataset-harmonization\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Dataset harmonization</h3>\n\
    <p>The first step(s) in the pipeline aims to harmonize the naming of chromosomes,\
    \ alleles, and variant IDs.  This is accomplished via the 4 keys described above.\
    \  While this pipeline generally attempts to simplify the QC process, it is extremely\
    \ important that the user is acquainted well enough with each individual dataset\
    \ to ensure that the appropriate keys are specified (or not specified).</p>\n\
    <h3>\n<a id=\"user-content-reference-allele-fixing\" class=\"anchor\" href=\"\
    #reference-allele-fixing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Reference allele fixing</h3>\n<p>In contrast\
    \ to a VCF, where alleles are specified with respect to a specified reference\
    \ genome (reference versus alternative alleles), PLINK-formatted files often specify\
    \ alleles as major/minor alleles based on the frequency in the dataset.  Furthermore,\
    \ many commonly used arrays will contain a mixture of SNPs genotyped on either\
    \ the + or - strand.  Lastly, the default behavior of PLINK is to automatically\
    \ set the minor to A1 and the major allele to A2, which can unintentionally generate\
    \ inconsistencies in allele specifications across datasets.</p>\n<p>With respect\
    \ to a reference genome, two possible types of errors can occur:</p>\n<ul>\n<li>Flipped\
    \ strand:  The genotype is specified with respect to the opposite strand relative\
    \ to the reference genome.</li>\n<li>Swapped allele:  The genotype is specified\
    \ on the same strand as the reference genome, but the A1 (minor) allele has been\
    \ set to equal the 'reference' allele when it ought to be set to equal the non-reference/'alternative'\
    \ allele</li>\n</ul>\n<p>To identify these errors, we use the bcftools plugin\
    \ '+fixref', which requires not only the reference sequence (fasta) file, but\
    \ also a VCF file containing variant sites that are used to identify mismatching\
    \ alleles in the query dataset.  Importantly, if the program determines that no\
    \ strand issues exist and that the reference/alternative alleles have simply been\
    \ swapped, then program will swap the major/minor alleles to match the reference.\
    \  It will not perform any strand flipping, where it converts genotypes to be\
    \ specified with respect to the nucleotide on the opposite strand.  Although the\
    \ program will attempt to identify these strand flips, it doesn't make the correction\
    \ as the authors consider this a risky move that should not be handled in an automated\
    \ fashion.  Thus, flip-strand mismatches are ultimately removed.  If there are\
    \ a large number of these, the user should attempt to understand and resolve the\
    \ source of the issue and rerun this pipeline.</p>\n<p>By default, the pipeline\
    \ will download the following files for the hg19 reference genome:</p>\n<p>Reference\
    \ fasta:\n<a href=\"ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz\"\
    \ rel=\"nofollow\">ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz</a></p>\n\
    <p>Reference VCF (1000Genomes):\n<a href=\"ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz\"\
    \ rel=\"nofollow\">ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz</a></p>\n\
    <p>An indication of whether alleles are now specified correctly is to plot frequency\
    \ of an allele in the query population against the frequency in the reference\
    \ population and look for an obviously positive correlation.  Such plots are automatically\
    \ produced in the PDF report as the final step in the pipeline.</p>\n<h3>\n<a\
    \ id=\"user-content-basic-qc\" class=\"anchor\" href=\"#basic-qc\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Basic\
    \ QC</h3>\n<p>After alleles have been fixed as described above, a series of basic\
    \ QC steps are performed on each dataset by the script <em>'scripts/QC.py'</em>,\
    \ with the filtering thresholds specified in the config file (see below).</p>\n\
    <pre><code>perform_QC: 'true'\nQC:\n  vm1: \"0.2\" # Initial variant missingness\
    \ filter\n  gm: \"0.1\" # Individual missingness filter\n  vm2: \"0.05\"  # Ultimate\
    \ call rate for variants after removing low-callrate samples\n  maf: \"0.01\"\
    \  # mimimum Minor allele frequency\n  hwe: \"0.0000001\"  # p-value threshold\
    \ for whether site follows hardy-weinberg\n  mbs: \"0.0000001\"  # p-value treshold\
    \ for test of whether missingness varies by sex\n</code></pre>\n<p>We first wish\
    \ to identify and remove individual samples that show high missingess across markers\
    \ (specified by 'gm').  However, to identify these individuals, we first need\
    \ to remove variants that imputed poorly across all individuals (specified by\
    \ 'vm1').  After removing these individuals, we then remove variants with high\
    \ missingness (specified by 'vm2').  Since poor imputation will result in missing\
    \ genotypes, this missingness filter indirectly filters for low quality imputation\
    \ sites.  Variants are also filtered based whether or not they show significant\
    \ departures from Hardy-Weinberg Equilibrium ('hwe' entry) and whether there is\
    \ a significant association between missingness and sex ('mbs' entry).  We also\
    \ remove rare variants based on the 'maf' value.  Lastly, we remove indels, duplicate\
    \ SNPs, and multi-allelic variants.</p>\n<p>Note that testing for missigness by\
    \ case/control status is generally recommended as well if the user wishes to proceed\
    \ straight to SNP-based analyses such as GWAS.  However, if the data is to be\
    \ used for ancestry inference, it may make more sense to retain these SNPs.</p>\n\
    <h3>\n<a id=\"user-content-merging-inputs-optional\" class=\"anchor\" href=\"\
    #merging-inputs-optional\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Merging inputs (Optional)</h3>\n<p>If multiple\
    \ input datasets were provided, an optional final step is to create a single merged\
    \ dataset consisting of only the sites that overlap (i.e. passed filters) across\
    \ all component datasets.  This behavior is controlled by the 'merge' entry in\
    \ the config file.  To enable the merging behavior, set this to:</p>\n<pre><code>merge:\
    \ 'true'\n</code></pre>\n<h3>\n<a id=\"user-content-imputaton-preparation\" class=\"\
    anchor\" href=\"#imputaton-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Imputaton preparation</h3>\n\
    <p>Another optional, final feature is to create a set of of VCF files (parsed\
    \ by chromosome) for each of the input datasets.  These VCFs can be used directly\
    \ as input into either the Michigan Imputation Server or the TOPMed Imputation\
    \ Server.  The output of the imputation servers can then be used as input into\
    \ the post-imputation QC pipeline (see README.md in the 'postImpute' directory).</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1617574369.0
powellgenomicslab/SingularityBaseImages:
  data_format: 2
  description: Singularity base images that will be build on singularity hub and can
    be used to build other images
  filenames:
  - Singularity.TxnDoubletDetection
  - Singularity.AllSoftwares
  - Singularity.R363_python368
  - Singularity.DemultiplexingSoftwares
  - Singularity.R4_python368
  - Singularity.DoubletDetection
  - Singularity.Anne_demultiplexing_test
  full_name: powellgenomicslab/SingularityBaseImages
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularitybaseimages" class="anchor" href="#singularitybaseimages"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SingularityBaseImages</h1>

    <p>A repo for singularity images. This is linked to singularity hub and all results
    can be pulled from there.</p>

    <h2>

    <a id="user-content-singularity-hub-images" class="anchor" href="#singularity-hub-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Hub Images</h2>

    <ul>

    <li>

    <p>Singularity.R363_python368</p>

    <ul>

    <li>To pull: <code>singularity pull shub://drneavin/SingularityBaseImages:r363_python368</code>

    </li>

    <li>Contains:

    <ul>

    <li>R 3.6.3</li>

    <li>python 3.6.8</li>

    <li>conda</li>

    <li>Some basic R packages (see the definition file to see all installed)</li>

    <li>Some basic python package (see the definition file to see all installed)</li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Singularity.R4_python368</p>

    <ul>

    <li>To pull: <code>singularity pull shub://drneavin/SingularityBaseImages:r4_python368</code>

    </li>

    <li>Contains:

    <ul>

    <li>R 4.0.3</li>

    <li>python 3.6.8</li>

    <li>conda</li>

    <li>Some basic R packages (see the definition file to see all installed)</li>

    <li>Some basic python package (see the definition file to see all installed)</li>

    </ul>

    </li>

    </ul>

    </li>

    <li>

    <p>Singularity.TxnDoubletDetection</p>

    <ul>

    <li>To pull: <code>singularity pull shub://drneavin/SingularityBaseImages:txndoubletdetection</code>

    </li>

    <li>Built on top of <code>Singularity.R4_python368</code> image</li>

    <li>Also contains:

    <ul>

    <li>DoubletDetection</li>

    <li>DoubletDecon</li>

    <li>DoubletFinder</li>

    <li>scds</li>

    <li>scrublet</li>

    <li>scDoubletFinder</li>

    <li>solo</li>

    </ul>

    </li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1611115640.0
powerPlant/3d-dna-srf:
  data_format: 2
  description: Singularity recipe files for 3D DNA (https://github.com/theaidenlab/3d-dna)
  filenames:
  - Singularity.180922
  - Singularity
  full_name: powerPlant/3d-dna-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2286" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the 3D de novo assembly (3D DNA) pipeline</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549335251.0
powerPlant/angsd-srf:
  data_format: 2
  description: Singularity recipe files for angsd (https://github.com/ANGSD/angsd)
  filenames:
  - Singularity
  - Singularity.0.921
  - Singularity.0.919
  - Singularity.0.925
  - Singularity.0.917
  - Singularity.0.922
  - Singularity.0.923
  - Singularity.0.918
  full_name: powerPlant/angsd-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2300" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the angsd program for analysing NGS data</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549507443.0
powerPlant/apsim-srf:
  data_format: 2
  description: Singularity recipe files for APSIM Classic (https://github.com/APSIMInitiative/APSIMClassic)
  filenames:
  - Singularity
  - Singularity.7.9-r4047
  - Singularity.7.10-r49ace54f9c8a670190aef9d8d0fb9d5477bb1534
  full_name: powerPlant/apsim-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the APSIM Classic version of the Agricultural
    Production Systems sIMulator</p>

    <h2>

    <a id="user-content-maintainer-notes" class="anchor" href="#maintainer-notes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Maintainer
    Notes</h2>

    <ul>

    <li>Recipes for APSIM 7.9 use the upstream SVN repository (no longer available)</li>

    <li>Please see comments inside the recipes for the reasons why some upstream files
    are overwritten during the build process</li>

    <li>The Cotton Model requires a password, which needs to be obtained by the model
    owner and placed under <code>files/CottonPassword.txt</code>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1586904956.0
powerPlant/apsimx-srf:
  data_format: 2
  description: Singularity recipe files for ApsimX (https://github.com/APSIMInitiative/ApsimX)
  filenames:
  - Singularity.2019.10.04.4236
  - Singularity.2020.08.04.5350
  - Singularity.2020.04.09.5012
  - Singularity.2019.01.08.3392
  - Singularity.2019.06.05.3920
  - Singularity.2021.04.15.6139
  - Singularity.2020.09.17.5665
  - Singularity
  - Singularity.2019.01.30.3436
  - Singularity.2020.10.21.5755
  - Singularity.2019.04.03.3693
  - Singularity.2020.11.27.5887
  - Singularity.2019.07.18.4025
  - Singularity.2018.09.28.3099
  - Singularity.2018.01.30.2253
  full_name: powerPlant/apsimx-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2271" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for ApsimX, the next generation of the Agricultural
    Production Systems sIMulator (APSIM)</p>

    <h2>

    <a id="user-content-maintainer-notes" class="anchor" href="#maintainer-notes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Maintainer
    notes</h2>

    <p>This container downloads the ApsimX <code>deb</code> file built by the <a href="https://github.com/APSIMInitiative/APSIM.Builds">BOB
    Build Service</a> at CSIRO and installs it on a Ubuntu:Bionic container.</p>

    <p>There are two useful endpoints from the Build Service:</p>

    <ul>

    <li>

    <a href="http://apsimdev.apsim.info/APSIM.Builds.Service/Builds.svc/GetLatestVersion"
    rel="nofollow">Get Latest Version</a>: shows the full version number of the latest
    release</li>

    <li>

    <a href="http://apsimdev.apsim.info/APSIM.Builds.Service/Builds.svc/GetURLOfLatestVersion?operatingSystem=Debian"
    rel="nofollow">Get URL of Latest Version</a>: shows the download URL for the latest
    version of the specified OS</li>

    </ul>

    <p><code>ApsimSetup*.deb</code> files are named using the <a href="https://github.com/APSIMInitiative/APSIM.Builds/blob/21a8ac85a1d868a45f60a994a35a5d07dc04562a/APSIM.Builds.Service/Builds.svc.cs#L278">build.issueNumber</a>
    instead of the full version number, so we must hardcode the URLs inside the recipe
    files, while using the full version number for the recipe file names.</p>

    <p>To facilitate running the container, a <code>/usr/local/bin/apsimmodels</code>
    file is created as the entrypoint using <code>/usr/local/bin/apsim</code> as a
    template.</p>

    <h2>

    <a id="user-content-container-specific-notes" class="anchor" href="#container-specific-notes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container-specific
    notes</h2>

    <ul>

    <li>ApsimX apparently needs <code>/etc/localtime</code> to run so we have to install
    <code>tzdata</code> as a dependency. We configure our local timezone, but this
    can be overridden at run time by exporting the <code>TZ</code> environment variable.</li>

    </ul>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1618541722.0
powerPlant/aws-cli-srf:
  data_format: 2
  description: Singularity recipe files for aws-cli (https://github.com/aws/aws-cli)
  filenames:
  - Singularity
  - Singularity.2.0.43
  full_name: powerPlant/aws-cli-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the AWS CLI v2 tool</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1598486009.0
powerPlant/bedops-srf:
  data_format: 2
  description: Singularity recipe files for bedops (https://github.com/bedops/bedops)
  filenames:
  - Singularity.2.4.39
  - Singularity
  full_name: powerPlant/bedops-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the BEDOPS open-source command-line toolkit
    that performs highly efficient and scalable Boolean and other set operations,
    statistical calculations, archiving, conversion and other management of genomic
    data of arbitrary scale.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1596773368.0
powerPlant/bismark-srf:
  data_format: 2
  description: Singularity recipe files for Bismark (https://github.com/FelixKrueger/Bismark)
  filenames:
  - Singularity.0.20.0
  - Singularity
  - Singularity.0.19.1
  full_name: powerPlant/bismark-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2263" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Bismark bisulfite mapping and methylation
    calling program</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549335455.0
powerPlant/busco-srf:
  data_format: 2
  description: Singularity recipe files for busco (https://gitlab.com/ezlab/busco)
  filenames:
  - Singularity.4.0.6
  - Singularity.4.0.4
  - Singularity.4.0.0
  - Singularity.4.0.5
  - Singularity.4.1.1
  - Singularity
  - Singularity.4.0.1
  - Singularity.4.0.2
  - Singularity.4.1.2
  - Singularity.5.1.2
  - Singularity.4.1.0
  - Singularity.4.1.4
  full_name: powerPlant/busco-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the BUSCO tool for Benchmarking Universal
    Single-Copy Ortholog assessment</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1618269021.0
powerPlant/cdo-srf:
  data_format: 2
  description: Singularity recipe files for cdo (https://www.mpimet.mpg.de/cdo/)
  filenames:
  - Singularity.1.9.5
  - Singularity.1.9.3
  - Singularity
  - Singularity.1.7.0
  full_name: powerPlant/cdo-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2262" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Climate Data Operators toolset</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549335527.0
powerPlant/checkm-srf:
  data_format: 2
  description: Singularity recipe files for checkm (http://ecogenomics.github.io/CheckM)
  filenames:
  - Singularity.1.0.11
  - Singularity.1.0.10
  - Singularity.1.1.3
  - Singularity.1.0.12
  - Singularity.1.0.7
  - Singularity.1.0.13
  - Singularity.1.0.8
  - Singularity
  full_name: powerPlant/checkm-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2464" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the CheckM set of tools for assessing the quality
    of genomes recovered from isolates, single cells, or metagenomes</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1598504920.0
powerPlant/crema-srf:
  data_format: 2
  description: Singularity recipe files for crema (https://github.com/gbgolding/crema)
  filenames:
  - Singularity
  - Singularity.fe4cf7a
  full_name: powerPlant/crema-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2320" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the crema tool to classify RNAs by Ensemble Machine
    learning Algorithms</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1550200930.0
powerPlant/diamond-srf:
  data_format: 2
  description: Singularity recipe files for DIAMOND (https://github.com/bbuchfink/diamond)
  filenames:
  - Singularity.v0.9.16
  - Singularity
  - Singularity.v0.9.23
  - Singularity.v0.9.19
  - Singularity.v0.9.17
  - Singularity.v0.9.18
  - Singularity.v0.9.20
  - Singularity.v0.9.15
  - Singularity.v0.9.24
  - Singularity.v0.9.21
  - Singularity.v0.9.22
  full_name: powerPlant/diamond-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2322" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the DIAMOND Accelerated BLAST compatible local
    sequence aligner</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1549857110.0
powerPlant/eddypro-engine-srf:
  data_format: 2
  description: Singularity recipe files for EddyPro Engine (https://github.com/LI-COR/eddypro-engine)
  filenames:
  - Singularity.5.2.1
  - Singularity.5.2.0
  - Singularity.6.2.0
  - Singularity.6.2.1
  - Singularity
  - Singularity.6.0.0
  - Singularity.5.1.1
  - Singularity.6.1.0
  full_name: powerPlant/eddypro-engine-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2272" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the EddyPro eddy covariance data processing software</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549923689.0
powerPlant/edta-srf:
  data_format: 2
  description: Singularity recipe files for edta (https://github.com/oushujun/EDTA)
  filenames:
  - Singularity.1.9.0
  - Singularity
  - Singularity.1.8.3
  full_name: powerPlant/edta-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the Extensive de novo TE Annotator tool</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1603071842.0
powerPlant/entrez-direct-srf:
  data_format: 2
  description: Singularity recipe files for entrez-direct (https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/)
  filenames:
  - Singularity
  - Singularity.13.8.20200819
  full_name: powerPlant/entrez-direct-srf
  latest_release: null
  readme: '<p>Singularity recipe files for Entrez Direct: E-utilities on the Unix
    Command Line to provide access to the NCBI''s suite of interconnected databases</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1598244762.0
powerPlant/getorganelle-srf:
  data_format: 2
  description: Singularity recipe files for getorganelle (https://github.com/Kinggerm/GetOrganelle)
  filenames:
  - Singularity
  - Singularity.v1.6.2e
  full_name: powerPlant/getorganelle-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the GetOrganelle toolkit to assembly organelle
    genomes from genome skimming data</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1579837325.0
powerPlant/groimp-srf:
  data_format: 2
  description: Singularity recipe files for GroIMP (http://www.grogra.de/software/groimp)
  filenames:
  - Singularity.1.6-cuda
  - Singularity
  - Singularity.1.6-jre8-cuda
  full_name: powerPlant/groimp-srf
  latest_release: null
  readme: '<p>Singularity recipe files for GroIMP, a 3D-modelling platform</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1583894106.0
powerPlant/gromacs-srf:
  data_format: 2
  description: Singularity recipe files for GROMACS (http://www.gromacs.org/)
  filenames:
  - Singularity.2018
  - Singularity.2019.1
  - Singularity.2018.3
  - Singularity.2020.1
  - Singularity.2019.4
  - Singularity.2019.6
  - Singularity.2018.1
  - Singularity.2019
  - Singularity
  - Singularity.2019.5
  - Singularity.2018.5
  - Singularity.2018.2
  - Singularity.2020.2
  - Singularity.2019.2
  - Singularity.2020
  - Singularity.2019.3
  - Singularity.2018.4
  full_name: powerPlant/gromacs-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2264" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the GROMACS molecular dynamics package</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1591670800.0
powerPlant/hapcol-srf:
  data_format: 2
  description: Singularity recipe files for hapcol (https://github.com/AlgoLab/HapCol)
  filenames:
  - Singularity
  - Singularity.97d4a5e
  full_name: powerPlant/hapcol-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the HapCol tool, a fast and memory-efficient
    method for haplotype assembly from long gapless reads</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1579837367.0
powerPlant/mandalorion-episode-ii-srf:
  data_format: 2
  description: Singularity recipe files for Mandalorion-Episode-II (https://github.com/rvolden/Mandalorion-Episode-II)
  filenames:
  - Singularity
  - Singularity.6219d58
  full_name: powerPlant/mandalorion-episode-ii-srf
  latest_release: null
  readme: '<p>Singularity recipe files for Mandalorion Episode II, Attack of the Isoforms</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583274107.0
powerPlant/mapgd-srf:
  data_format: 2
  description: Singularity recipe files for MAPGD (https://github.com/LynchLab/MAPGD)
  filenames:
  - Singularity
  - Singularity.0.4.38-d3edee2
  full_name: powerPlant/mapgd-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2319" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the MAPGD series of related programs for the analysis
    of low coverage population genomic data or for the analysis of pooled data</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549853299.0
powerPlant/metabat2-srf:
  data_format: 2
  description: Singularity recipe files for metabat2 (https://bitbucket.org/berkeleylab/metabat/src/master/)
  filenames:
  - Singularity.2.15-3-g367a7ef
  - Singularity
  - Singularity.2.15
  full_name: powerPlant/metabat2-srf
  latest_release: null
  readme: '<p>Singularity recipe files for MetaBAT: A robust statistical framework
    for reconstructing genomes from metagenomic data</p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1618472328.0
powerPlant/mrbayes-srf:
  data_format: 2
  description: Singularity recipe files for MrBayes (http://nbisweden.github.io/MrBayes)
  filenames:
  - Singularity.3.2.7a
  - Singularity
  - Singularity.3.2.7a-gpu
  full_name: powerPlant/mrbayes-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3808" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the MrBayes program for Bayesian inference and
    model choice across a wide range of phylogenetic and evolutionary models</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1574325488.0
powerPlant/novograph-srf:
  data_format: 2
  description: Singularity recipe files for NovoGraph (https://github.com/NCBI-Hackathons/NovoGraph)
  filenames:
  - Singularity.1.0.0
  - Singularity
  full_name: powerPlant/novograph-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2342" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the NovoGraph tool to construct a genome graph
    representation of long-read-based de novo sequence assemblies</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1550014659.0
powerPlant/opendronemap-srf:
  data_format: 2
  description: Singularity recipe files for OpenDroneMap (https://www.opendronemap.org/)
  filenames:
  - Singularity.0.4.1
  - Singularity.0.4.0
  - Singularity
  full_name: powerPlant/opendronemap-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2266" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the OpenDroneMap Drone Mapping Software</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549336324.0
powerPlant/paml-srf:
  data_format: 2
  description: Singularity recipe files for paml (http://abacus.gene.ucl.ac.uk/software/paml.html)
  filenames:
  - Singularity
  - Singularity.4.9i
  full_name: powerPlant/paml-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3399" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PAML tool for phylogenetic analyses of DNA
    or protein sequences using maximum likelihood.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1565742033.0
powerPlant/pblat-srf:
  data_format: 2
  description: Singularity recipe files for Pblat (http://icebert.github.io/pblat/)
  filenames:
  - Singularity.2.0
  - Singularity
  - Singularity.2.1
  full_name: powerPlant/pblat-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2380" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for Pblat, the parallelized blat with multi-threads
    support</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1550562816.0
powerPlant/pcl-srf:
  data_format: 2
  description: Singularity recipe files for pcl (https://github.com/PointCloudLibrary/pcl)
  filenames:
  - Singularity.1.9.0
  - Singularity
  - Singularity.1.9.1
  - Singularity.1.8.1
  full_name: powerPlant/pcl-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2329" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the pcl Point Cloud Library</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1550093426.0
powerPlant/pinfish-srf:
  data_format: 2
  description: Singularity recipe files for pinfish (https://github.com/nanoporetech/pinfish)
  filenames:
  - Singularity
  - Singularity.0.1.0
  full_name: powerPlant/pinfish-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the pinfish collection of tools helping
    to make sense of long transcriptomics data (long cDNA reads, direct RNA reads)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1583274123.0
powerPlant/plink2-srf:
  data_format: 2
  description: Singularity recipe files for plink2 (https://www.cog-genomics.org/plink/2.0/)
  filenames:
  - Singularity
  - Singularity.v2.00a2LM
  full_name: powerPlant/plink2-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3722" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the PLINK association analysis toolset</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1572401216.0
powerPlant/portcullis-srf:
  data_format: 2
  description: Singularity recipe files for Portcullis (https://github.com/maplesond/portcullis)
  filenames:
  - Singularity.1.1.0
  - Singularity
  - Singularity.1.1.2
  - Singularity.1.1.1
  full_name: powerPlant/portcullis-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2267" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for Portcullis, a program for PORTable CULLing of
    Invalid Splice junctions from pre-aligned RNA-seq data</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1549336366.0
powerPlant/qiime2-srf:
  data_format: 2
  description: Singularity recipe files for QIIME 2 (https://docs.qiime2.org/)
  filenames:
  - Singularity.2019.1
  - Singularity.2019.7
  - Singularity.2019.4
  - Singularity
  - Singularity.2020.6
  - Singularity.2019.1-picrust2
  - Singularity.2020.8
  - Singularity.2018.2
  - Singularity.2018.11
  - Singularity.2019.7-picrust2
  - Singularity.2020.2
  - Singularity.2019.10
  full_name: powerPlant/qiime2-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2268" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the QIIME 2 microbiome analysis package</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1603334435.0
powerPlant/racon-srf:
  data_format: 2
  description: Singularity recipe files for Racon (https://github.com/isovic/racon/)
  filenames:
  - Singularity.1.3.0
  - Singularity.1.3.2
  - Singularity.1.3.1
  - Singularity.1.4.7
  - Singularity
  - Singularity.1.4.3
  - Singularity.1.4.0
  - Singularity.1.3.3
  - Singularity.1.4.2
  full_name: powerPlant/racon-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2269" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Racon consensus module for raw de novo DNA
    assembly of long uncorrected reads</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1590711591.0
powerPlant/ragoo-srf:
  data_format: 2
  description: Singularity recipe files for RaGOO (https://github.com/malonge/RaGOO)
  filenames:
  - Singularity.1.01
  - Singularity.1.02
  - Singularity
  full_name: powerPlant/ragoo-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2341" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the RaGOO tool to order and orient genome assembly
    contigs via Minimap2 alignments to a reference genome</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1550774761.0
powerPlant/repet-srf:
  data_format: 2
  description: Singularity recipe files for REPET (https://urgi.versailles.inra.fr/Tools/REPET)
  filenames:
  - Singularity.3.0
  - Singularity
  full_name: powerPlant/repet-srf
  latest_release: null
  readme: '<p>Singularity recipe files for REPET

    (<a href="https://urgi.versailles.inra.fr/Tools/REPET" rel="nofollow">https://urgi.versailles.inra.fr/Tools/REPET</a>),
    used to detect, annotate and

    analyse repeats in genomic sequences, specifically designed for transposable

    elements (TEs).</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1602104190.0
powerPlant/sex-detector-plusplus-srf:
  data_format: 2
  description: Singularity recipe files for sex-detector-plusplus (https://gitlab.in2p3.fr/sex-det-family/sex-detector-plusplus)
  filenames:
  - Singularity.00f7d723
  - Singularity
  full_name: powerPlant/sex-detector-plusplus-srf
  latest_release: null
  readme: '<p>Singularity recipe files for SEX-DETector, a tool for the statistical
    inferrence of sex-linked genes from RNA / DNA reads from a cross (parents and
    set of childrens)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1600917082.0
powerPlant/sga-srf:
  data_format: 2
  description: Singularity recipe files for sga (https://github.com/jts/sga)
  filenames:
  - Singularity.0.10.15
  - Singularity
  full_name: powerPlant/sga-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/3984" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the SGA tool, a de novo genome assembler based
    on the concept of string graphs</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1579231330.0
powerPlant/slim-srf:
  data_format: 2
  description: Singularity recipe files for slim (https://github.com/MesserLab/SLiM)
  filenames:
  - Singularity.3.5
  - Singularity
  - Singularity.3.4+1c85d00
  full_name: powerPlant/slim-srf
  latest_release: null
  readme: '<p>Singularity recipe files for Selection on Linked Mutations: A forward
    population genetic simulation for studying linkage effects, such as hitchhiking,
    background selection, and Hill-Robertson interference</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1607459916.0
powerPlant/sortmerna-srf:
  data_format: 2
  description: Singularity recipe files for sortmerna (https://github.com/biocore/sortmerna)
  filenames:
  - Singularity.3.0.3
  - Singularity
  - Singularity.4.2.0
  - Singularity.4.3.2
  full_name: powerPlant/sortmerna-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the SortMeRNA local sequence alignment
    tool for filtering, mapping and clustering.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1618542140.0
powerPlant/spades-srf:
  data_format: 2
  description: Singularity recipe files for spades (git@github:powerPlant/spades-srf.git)
  filenames:
  - Singularity.v0.5-recomb
  - Singularity.metaplasmid-paper
  - Singularity.spaligner-paper
  - Singularity.v3.9.0
  - Singularity.cloudspades-paper
  - Singularity.v3.13.1
  - Singularity.v3.8.2
  - Singularity.cami2-submission
  - Singularity.v3.14.0
  - Singularity.v3.13.0
  - Singularity.v3.8.1
  - Singularity.template
  - Singularity.v3.8.0
  - Singularity.v3.10.1
  - Singularity.v3.10.0
  - Singularity.v3.11.0
  - Singularity.v3.11.1
  - Singularity.v3.12.0
  - templates/Singularity.template
  full_name: powerPlant/spades-srf
  latest_release: null
  readme: '<p>Singularity recipe files for spades</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1580700253.0
powerPlant/squeezemeta-srf:
  data_format: 2
  description: Singularity recipe files for SqueezeMeta (https://github.com/jtamames/SqueezeMeta)
  filenames:
  - Singularity.0.4.4
  - Singularity
  - Singularity.1.0.0-beta
  full_name: powerPlant/squeezemeta-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2930" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the SqueezeMeta fully automated metagenomics pipeline</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1557458055.0
powerPlant/stacks-srf:
  data_format: 2
  description: Singularity recipe files for Stacks (http://catchenlab.life.illinois.edu/stacks/)
  filenames:
  - Singularity.2.0
  - Singularity.2.2
  - Singularity
  - Singularity.2.1
  full_name: powerPlant/stacks-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2270" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the Stacks software pipeline for building loci
    from short-read sequences</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1611661899.0
powerPlant/swan-srf:
  data_format: 2
  description: Singularity recipe files for SWAN (http://bitbucket.org/charade/swan)
  filenames:
  - Singularity.3516c2f
  full_name: powerPlant/swan-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2354" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the SWAN tool for SV detection</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1550114140.0
powerPlant/trinityrnaseq-srf:
  data_format: 2
  description: Singularity recipe files for trinityrnaseq (https://github.com/trinityrnaseq/trinityrnaseq)
  filenames:
  - Singularity.2.9.0
  - Singularity.2.8.6
  - Singularity
  - Singularity.2.10.0
  - Singularity.2.9.1
  full_name: powerPlant/trinityrnaseq-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the Trinity RNA-Seq de novo transcriptome
    assembly</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1591576526.0
powerPlant/vg-srf:
  data_format: 2
  description: Singularity recipe files for vg (https://github.com/vgteam/vg)
  filenames:
  - Singularity.1.8.0
  - Singularity.1.9.0
  - Singularity.1.12.1
  - Singularity
  - Singularity.1.12.0
  - Singularity.1.11.0
  - Singularity.1.10.0
  - Singularity.1.13.0
  full_name: powerPlant/vg-srf
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2311" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity recipe files for the vg tools for working with genome variation
    graphs</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1549578706.0
pranithavangala/singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity_recipev1.0_addR.3.4.3
  - Singularity_recipe0_part1
  - Singularity_hicpro_v1
  - Singularity_recipe_MMARGE
  - Singularity_recipe_R.3.4.1
  - Singularity_recipev1.R-3-4-3
  - Singularity.add_g2gtools
  - Singularity_add.R_packages
  - Singularity.add_python_packages
  - Singularity_recipev1.0
  full_name: pranithavangala/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-multi_atlas_app" class="anchor" href="#multi_atlas_app" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Multi_Atlas_app</h1>

    <p>This includes everything required (except for the "full-multi-atlas" directory)
    to build a docker and corresponding singularity container for the Multi Atlas
    pipeline.</p>

    <p><a href="https://hub.docker.com/r/vuiiscci/multi_atlas/tags/" rel="nofollow">Docker
    Hub</a></p>

    <p><a href="https://singularity-hub.org/collections/734" rel="nofollow">Singularity
    Hub</a></p>

    <h1>

    <a id="user-content-build-instructions" class="anchor" href="#build-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Instructions:</h1>

    <p>Just clone and run <code>build.sh</code>:</p>

    <pre><code>git clone https://github.com/vuiiscci/Multi_Atlas_app.git

    cd Multi_Atlas_app/

    ./build.sh

    </code></pre>

    <p>NOTE that you must have full-multi-atlas directory which contains atlases.</p>

    <h1>

    <a id="user-content-run-instructions" class="anchor" href="#run-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    Instructions:</h1>

    <p>For docker:</p>

    <pre><code>sudo docker run --rm \

    -v $(pwd)/INPUTS/:/INPUTS/ \

    -v $(pwd)/OUTPUTS:/OUTPUTS/ \

    --user $(id -u):$(id -g) \

    vuiiscci/multi_atlas

    </code></pre>

    <p>For singularity:</p>

    <pre><code>singularity run -e \

    -B INPUTS/:/INPUTS \

    -B OUTPUTS/:/OUTPUTS \

    shub://vuiiscci/Multi_Atlas_app

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1609299433.0
qbicsoftware-archive/microarray-qc-workflow:
  data_format: 2
  description: Quality Control plots and data normalisation for Microarray data
  filenames:
  - Singularity
  full_name: qbicsoftware-archive/microarray-qc-workflow
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-work-in-progress\" class=\"anchor\" href=\"\
    #work-in-progress\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Work in progress</h1>\n<h1>\n<a id=\"user-content-microarray-qc-workflow\"\
    \ class=\"anchor\" href=\"#microarray-qc-workflow\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>microarray-qc-workflow</h1>\n\
    <p>Takes .cel files and creates qc plots as well as normalising the data</p>\n\
    <p><a href=\"https://travis-ci.org/qbicsoftware/microarray-qc-workflow\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/16f4a8e65783fd7014126125a1c351cf1ac4d34d672b6cb4e6cab27706d0c85f/68747470733a2f2f7472617669732d63692e6f72672f71626963736f6674776172652f6d6963726f61727261792d71632d776f726b666c6f772e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/qbicsoftware/microarray-qc-workflow.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.nextflow.io/\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/17df893666bfa5cad487466d4476ab773ea5def560c04c50f45795017865e81c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33302e302d627269676874677265656e2e737667\"\
    \ alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.30.0-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><a href=\"http://bioconda.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\"\
    \ alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/microarray-qc-workflow\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8af3143d98534fd47758128af0ea9ed413467e1151e5ab095c16968f578fcdd3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6d6963726f61727261792d71632d776f726b666c6f772e737667\"\
    \ alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/microarray-qc-workflow.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\"\
    \ alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h3>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h3>\n<p>microarray-qc-workflow:\
    \ Takes .cel files and creates qc plots as well as normalising the data</p>\n\
    <p>The pipeline is built using <a href=\"https://www.nextflow.io\" rel=\"nofollow\"\
    >Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures\
    \ in a very portable manner. It comes with docker / singularity containers making\
    \ installation trivial and results highly reproducible.</p>\n<h3>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h3>\n<p>The microarray-qc-workflow\
    \ pipeline comes with documentation about the pipeline, found in the <code>docs/</code>\
    \ directory:</p>\n<ol>\n<li><a href=\"docs/installation.md\">Installation</a></li>\n\
    <li>Pipeline configuration\n<ul>\n<li><a href=\"docs/configuration/local.md\"\
    >Local installation</a></li>\n<li><a href=\"docs/configuration/adding_your_own.md\"\
    >Adding your own system</a></li>\n</ul>\n</li>\n<li><a href=\"docs/usage.md\"\
    >Running the pipeline</a></li>\n<li><a href=\"docs/output.md\">Output and how\
    \ to interpret the results</a></li>\n<li><a href=\"docs/troubleshooting.md\">Troubleshooting</a></li>\n\
    </ol>\n<h3>\n<a id=\"user-content-credits\" class=\"anchor\" href=\"#credits\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Credits</h3>\n<p>This pipeline was written by Timo Lucas (<a href=\"\
    https://github.com/lucass122\">lucass122</a>) at <a href=\"http://www.qbic.uni-tuebingen.de/\"\
    \ rel=\"nofollow\">QBiC T\xFCbingen</a>.\nR script based on script by Stefan Czemmel\
    \ [qbicStefanC]:\n<a href=\"https://github.com/qbicsoftware/qbic-wf-microarrayQC\"\
    >https://github.com/qbicsoftware/qbic-wf-microarrayQC</a></p>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1600940492.0
radio1988/OneStopRNAseq:
  data_format: 2
  description: null
  filenames:
  - snakemake/workflow/envs/Singularity
  full_name: radio1988/OneStopRNAseq
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-csci5980\" class=\"anchor\" href=\"#csci5980\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>csci5980</h1>\n<p>Final project for CSci 5980: deep learning for automatic\
    \ music translation.</p>\n<p>Follow theses steps to install all package dependencies\
    \ for running the model:</p>\n<p>We first install software dependencies for manipulating\
    \ raw audio (<code>ffmpeg</code>):</p>\n<ol>\n<li>\n<p>Create a local software\
    \ directory\n<code>mkdir ~/software</code></p>\n</li>\n<li>\n<p>Install the NASM\
    \ assembler (dependency of ffmpeg):</p>\n</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span> <span class=\"\
    pl-k\">~</span>/software\nwget https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2\n\
    tar -xvf nasm-2.14.02.tar.bz2\n<span class=\"pl-c1\">cd</span> nasm-2.14.02\n\
    ./configure --prefix=<span class=\"pl-k\">~</span>/software/nasm/\nmake install\n\
    <span class=\"pl-k\">export</span> PATH=<span class=\"pl-smi\">$PATH</span>:<span\
    \ class=\"pl-k\">~</span>/software/nasm/bin/</pre></div>\n<ol start=\"3\">\n<li>Make\
    \ sure that NASM assembler installed correctly:</li>\n</ol>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>nasm -v</pre></div>\n<p>The output should look\
    \ something like:\n<code>NASM version 2.14.02 compiled on Mar 11 2020</code></p>\n\
    <ol start=\"4\">\n<li>Install ffmpeg:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/software\n\
    wget https://ffmpeg.org/releases/ffmpeg-4.2.2.tar.bz2\ntar -xvf ffmpeg-4.2.2.tar.bz2\n\
    <span class=\"pl-c1\">cd</span> ffmpeg-4.2.2\n./configure --prefix=<span class=\"\
    pl-k\">~</span>/software/ffmpeg/\nmake install\n<span class=\"pl-k\">export</span>\
    \ PATH=<span class=\"pl-smi\">$PATH</span>:<span class=\"pl-k\">~</span>/software/ffmpeg/bin/</pre></div>\n\
    <ol start=\"5\">\n<li>Make sure that ffmpeg installed correctly:</li>\n</ol>\n\
    <div class=\"highlight highlight-source-shell\"><pre>ffmpeg -version</pre></div>\n\
    <p>The output should look something like:</p>\n<pre><code>ffmpeg version 4.2.2\
    \ Copyright (c) 2000-2019 the FFmpeg developers\nbuilt with gcc 4.4.7 (GCC) 20120313\
    \ (Red Hat 4.4.7-23)\nconfiguration: --prefix=/home/csci5980/piehl008/software/ffmpeg/\n\
    libavutil      56. 31.100 / 56. 31.100\nlibavcodec     58. 54.100 / 58. 54.100\n\
    libavformat    58. 29.100 / 58. 29.100\nlibavdevice    58.  8.100 / 58.  8.100\n\
    libavfilter     7. 57.100 /  7. 57.100\nlibswscale      5.  5.100 /  5.  5.100\n\
    libswresample   3.  5.100 /  3.  5.100\n</code></pre>\n<ol start=\"6\">\n<li>Now,\
    \ we can make the virtual environment and install python packages.  First, create\
    \ the virtual environment by running:</li>\n</ol>\n<p><code>conda create --name\
    \ audio-proj python=3.7</code></p>\n<ol start=\"7\">\n<li>Next, install packages\
    \ by running</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c1\">cd</span> <span class=\"pl-k\">~</span>/csci5980\nconda install\
    \ --name audio-proj --file requirements.txt --channel defaults --channel conda-forge</pre></div>\n\
    <p>(Note: this can take a while - and you need to say yes to installing everything\
    \ after it solves the environment)</p>\n<ol start=\"8\">\n<li>To activate the\
    \ virtual environment, you can now run <code>source activate audio-proj</code>.\
    \ Note: you should do this to test that you can activate the virtual evironment,\
    \ but you probably shouldn't run a lot unless you are submitting jobs to the queue.\
    \  If you want to use this virtual environment through the MSI notebooks, check\
    \ out the tutorial at <a href=\"https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\"\
    \ rel=\"nofollow\">https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html</a>.</li>\n\
    </ol>\n<h3>\n<a id=\"user-content-adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ class=\"anchor\" href=\"#adding-the-virtual-environment-to-jupyter-notebooks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Adding the Virtual Environment to Jupyter Notebooks</h3>\n<p>Now that\
    \ we have created the virtual environment, we can add it to the Jupyter notebook\
    \ kernels so that we can use the virtual environment through MSI's notebook server.\
    \ To do this, we have to add the kernel specifications to the known Jupyter kernels\
    \ for our user:</p>\n<ol start=\"9\">\n<li>If you haven't already, activate your\
    \ virtual environment by running <code>source activate audio-proj</code>. Then\
    \ enter</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>which\
    \ python</pre></div>\n<p>Your output should tell you where the python executable\
    \ for this virtual environment lives - the output for me displays <code>~/.conda/envs/audio-proj/bin/python</code>.\
    \  If you see something that looks like <code>/panfs/roc/msisoft/anaconda/anaconda3-2018.12/bin/python</code>,\
    \ go back and make sure that you have the virtual environment active and try again.\
    \ After you have an ouput that clearly has the name of the virtual environment\
    \ in the directory path (i.e. contains audio-proj in it), continue to the next\
    \ step.</p>\n<ol start=\"10\">\n<li>Now, we need to create the kernel configuration.\
    \ To do this run</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>mkdir <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj\n\
    nano <span class=\"pl-k\">~</span>/.local/share/jupyter/kernels/audio-proj/kernel.json</pre></div>\n\
    <p>The nano command will open a very basic text editor that you can navigate with\
    \ the arrow keys. Enter the following:</p>\n<pre lang=\"text\"><code>{\n \"argv\"\
    : [\n  \"~/.conda/envs/audio-proj/bin/python\", #replace this with your path from\
    \ step 9 above! (and delete this comment)\n  \"-m\",\n  \"ipykernel_launcher\"\
    ,\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Audio Project\
    \ Kernel\",\n \"language\": \"python\"\n}\n</code></pre>\n<p>where you replace\
    \ the first line of the argv array with whatever executable path was output from\
    \ step 9 above (it likely will be identical to this). To exit the nano text editor,\
    \ type <code>Ctrl-x &lt;RETURN&gt;</code> and then type <code>Y &lt;RETURN&gt;</code>\
    \ to save the file.</p>\n<ol start=\"11\">\n<li>Now that you have saved the kernel\
    \ file, you should be able to go to <code>https://notebooks.msi.umn.edu/</code>\
    \ and when you click on the <code>New</code> tab to create a new file, you should\
    \ be able to select <code>Audio Project Kernel</code> as an available kernel to\
    \ run your newly created file in.</li>\n</ol>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1619146376.0
rohitfarmer/singularity-defs:
  data_format: 2
  description: Definition (recipe) files for singularity containers.
  filenames:
  - H5N1/Singularity.def
  - H5N1/Singularity_R_3.6.def
  - cite-seq/Singularity_xenial.def
  - cite-seq/Singularity_publish.def
  - cite-seq/Singularity_rocker.def
  - cite-seq/Singularity_3.def
  - tf-gpu-chemistry/Singularity
  - generic/Singularity.def
  - bittersweet/Singularity.def
  - cytof-workflow-v3/Singularity.def
  - comet/Singularity.def
  - cytof-deep-cnn/Singularity.def
  full_name: rohitfarmer/singularity-defs
  latest_release: null
  readme: '<h1>

    <a id="user-content-definitionrecipe-files-for-singularity-containers" class="anchor"
    href="#definitionrecipe-files-for-singularity-containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Definition/Recipe Files
    for Singularity Containers</h1>

    <p>Some of the containers are available to download from <a href="https://cloud.sylabs.io/library/rohitfarmer"
    rel="nofollow">https://cloud.sylabs.io/library/rohitfarmer</a></p>

    <p>For feedback and collaboration write to me at <a href="mailto:rohit.farmer@gmail.com">rohit.farmer@gmail.com</a></p>

    <h1>

    <a id="user-content-install-singularity-on-linux" class="anchor" href="#install-singularity-on-linux"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Singularity on Linux</h1>

    <h2>

    <a id="user-content-singularity-version-34" class="anchor" href="#singularity-version-34"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Version 3.4</h2>

    <p>Follow the instructions on <a href="https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps</a></p>

    <h1>

    <a id="user-content-building-a-singularity-container" class="anchor" href="#building-a-singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    a Singularity Container</h1>

    <h2>

    <a id="user-content-readonly-container" class="anchor" href="#readonly-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Readonly
    Container</h2>

    <p>To build a read-only SquashFS Singularity container on a local machine using
    a recipe/definition file.</p>

    <p><code>sudo singularity build &lt;container-name.sif&gt; &lt;Singularity.def&gt;</code></p>

    <h2>

    <a id="user-content-writable-sandbox" class="anchor" href="#writable-sandbox"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Writable
    Sandbox</h2>

    <p>To build a writable sandbox (essentially a folder) on a local machine using
    a recipe/definition file.</p>

    <p><code>sudo singularity build --sandbox  &lt;sandbox-folder-name/&gt; &lt;Singularity.def&gt;</code></p>

    <p><em>Note: The advantage of building a writable sandbox is that it can be used
    to install and configure packages as you go, and once you are satisfied with the
    requirements, the sandbox can be converted into a read-only SquashFS container.
    To build a sandbox quickly, it''s better to install a minimal set of packages
    via the definition file.</em></p>

    <h3>

    <a id="user-content-installconfigure-packages-in-a-writable-sandbox" class="anchor"
    href="#installconfigure-packages-in-a-writable-sandbox" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install/Configure Packages
    in a Writable Sandbox</h3>

    <p>Once a writable sandbox is created to execute it to invoke the shell of the
    operating installed in the container in the "writable" mode. If the shell is not
    invoked in the "writable" mode, all the changes will be lost once you exit from
    the container environment.</p>

    <p><code>sudo singularity shell --writable &lt;sandbox-folder-name/&gt;</code></p>

    <p>Install packages as you would, for example, in Ubuntu from the command line.</p>

    <h2>

    <a id="user-content-convert-a-writable-sandbox-to-a-readonly-container" class="anchor"
    href="#convert-a-writable-sandbox-to-a-readonly-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Convert a Writable
    Sandbox to a Readonly Container</h2>

    <p><code>sudo singularity build &lt;container-name.sif&gt; &lt;sandbox-folder-name/&gt;</code></p>

    <h1>

    <a id="user-content-execute-a-container" class="anchor" href="#execute-a-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execute
    a Container</h1>

    <h2>

    <a id="user-content-invoke-a-shell" class="anchor" href="#invoke-a-shell" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Invoke a shell</h2>

    <p>The command below can be used for both read-only/writable containers/sandbox.</p>

    <p><code>singularity shell &lt;container-name.sif&gt;</code></p>

    <p><em>Note: By default, Singularity binds to your current working and home directory.
    Therefore, you do not need to do anything else to execute a script that is in
    your current working directory. It can also pull, for example, Vim settings from
    the .vimrc file in your home directory. Therefore, if Vim installed in the container,
    it can be used with the same settings from inside the container as it would from
    outside.</em></p>

    <h2>

    <a id="user-content-execute-a-command-via-container" class="anchor" href="#execute-a-command-via-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execute
    a Command via Container</h2>

    <p><code>singularity exec &lt;container-name.sif&gt; &lt;command&gt;</code></p>

    <p>For example: <code>singularity exec &lt;container-name.sif&gt; Rscript --vanilla
    hello.R</code></p>

    <h1>

    <a id="user-content-running-jupyter-notebooks-from-within-a-container" class="anchor"
    href="#running-jupyter-notebooks-from-within-a-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running Jupyter Notebooks
    from Within a Container</h1>

    <p>This section is for containers that have Jupyter notebook installed (e.g. cite-seq).</p>

    <p>A generic command that should work on a personal computer. <code>singularity
    exec container-name.sif jupyter notebook --no-browser --ip=127.0.0.1 --port=8888</code><br>

    <em>Note: The IP address and the port number mentioned in the command are the
    jupyter defaults. They can be changed as per need.</em><br>

    Copy the URL generated by jupyter daemon and paste it in your browser; this should
    open Jupyter with the list of the files in your current working directory on the
    host computer.</p>

    <h2>

    <a id="user-content-running-with-r-as-a-kernel" class="anchor" href="#running-with-r-as-a-kernel"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    with R as a Kernel</h2>

    <p>Sometimes if you already have an R kernel installed in your home directory,
    it conflicts with what you have inside the container. Therefore, it would require
    you to re-install the kernel specs in your home directory via the container.</p>

    <pre><code>singularity exec container-name.sif R --quiet --slave -e ''IRkernel::installspec()''


    # Screen log.

    # [InstallKernelSpec] Removing existing kernelspec in /home/user/.local/share/jupyter/kernels/ir

    # [InstallKernelSpec] Installed kernelspec ir in /home/user/.local/share/jupyter/kernels/ir

    </code></pre>

    <h2>

    <a id="user-content-running-on-an-hpc" class="anchor" href="#running-on-an-hpc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    on an HPC</h2>

    <ol>

    <li>SSH to the HPC.</li>

    <li>Claim an interactive node.</li>

    <li>Navigate to your project directory. Singularity container should be in your
    project directory.</li>

    <li><code>singularity exec container-name.sif jupyter notebook --no-browser --ip=''0.0.0.0''</code></li>

    <li>Keep the SSH session and Jupyter notebook session running. Copy the URL on
    your local browser.</li>

    </ol>

    <p><em>Note: On some HPCs, you may have to initiate an additional SSH tunnel connecting
    your local machine to the interactive node on the HPC. In that case, follow some
    generic instructions here <a href="https://rohitfarmer.github.io/docs/docs/HPC/jupyter/"
    rel="nofollow">https://rohitfarmer.github.io/docs/docs/HPC/jupyter/</a> or ask
    your system administrator.</em></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1598393438.0
sailfish009/openstructure:
  data_format: 2
  description: Open-Source Computational Structural Biology Framework
  filenames:
  - singularity/Singularity
  full_name: sailfish009/openstructure
  latest_release: null
  readme: '<h1>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h1>

    <p>This README is pulled from a default template for workflows.</p>

    <h1>

    <a id="user-content-workflow-template-setup" class="anchor" href="#workflow-template-setup"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Workflow
    template setup</h1>

    <h2>

    <a id="user-content-lib" class="anchor" href="#lib" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>lib</h2>

    <ul>

    <li>The <code>lib</code> directory contains general libraries that may be referenced
    by multiple workflows, for instance cromwell configs and python configs. Currently
    nothing in this directory is used.</li>

    </ul>

    <h2>

    <a id="user-content-pipelines" class="anchor" href="#pipelines" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pipelines</h2>

    <ul>

    <li>

    <p>Each pipeline is a full analysis. Think of it like the heading of a methods
    section in a paper. For instance if this were genetic summary statistics workflow,
    a pipeline might be "fine-mapping" that does both conditional and credible set
    analysis. Another pipeline may be "colocalization".</p>

    </li>

    <li>

    <p>Pipelines may have numbers prior to their name (e.g., <code>example_pipeline_1</code>
    to <code>0025-example_pipeline_1</code>). These numbers do not mean anything,
    but merely used to keep pipelines in their general order of execution. These are
    optional.</p>

    </li>

    <li>

    <p>A pipeline consists of :</p>

    </li>

    </ul>

    <ol>

    <li>A workflow.</li>

    <li>A <code>scripts</code> directory with <em>all</em> scripts referenced by that
    workflow (unless a general lib script is called). Scripts may have numbers prior
    to their name. These numbers do not mean anything, but merely used to keep scripts
    in their general order of execution. These are optional.</li>

    <li>A <code>docs</code> directory that contains a documentation of the default
    parameters written in a style that is publishable as methods in a paper (including
    citations). Within the <code>docs</code> directory there may be a <code>reference</code>
    with any additional reference materials.</li>

    <li>An <code>example_runtime_setup</code> directory contains files that give an
    example of actual config files and any other files used to run the pipeline.</li>

    </ol>

    <h2>

    <a id="user-content-studies" class="anchor" href="#studies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>studies</h2>

    <ul>

    <li>A studies directory should either exist within the workflow repo or be a separate
    repo that has the same name as the workflow repo, but with <code>studies</code>
    appended to it (e.g. <code>template-workflow</code> becomes <code>template-workflow-studies</code>).</li>

    <li>If there is a standard set of plots that will always look the same way, a
    pipeline should generate such plots. Otherwise, all code to analyze the results
    of a pipeline run should be in the <code>studies</code> directory. For instance
    if this were genetic summary statistics workflow, <code>studies</code> may contain
    a <code>t2d</code> directory and a <code>weight</code> directory.</li>

    <li>Within a study is either an Jupyter notebook (either python or R kernel) or
    an R markdown file. Nearly all plots / analysis of the results of running the
    various pipelines should be done in the notebook / markdown file.</li>

    <li>A study may also contain a scripts directory with scripts to aggregate data
    for a one off analysis (if the analysis is going to be repeated, consider making
    a new pipeline or adding it to an existing pipeline) or for special plots that
    cannot be done in the notebook / markdown file.</li>

    </ul>

    <h1>

    <a id="user-content-new-workflow-reminders" class="anchor" href="#new-workflow-reminders"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>New
    workflow reminders</h1>

    <ul>

    <li>[ ] Documentation</li>

    <li>[ ] Environment version control</li>

    <li>[ ] Pipeline version control</li>

    <li>[ ] Git branches</li>

    <li>[ ] Code review</li>

    </ul>

    <h1>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h1>

    <p>Be sure to document your code!</p>

    <h1>

    <a id="user-content-environment-version-control" class="anchor" href="#environment-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Environment
    version control</h1>

    <p>Analysis environment is controlled using conda. Each pipeline should have an
    <code>environment.yml</code> file with all of the packages used. If a required
    package or library is missing from conda (and therefore not in the <code>environment.yml</code>),
    it should be noted in the <code>README.md</code> of the pipeline.</p>

    <div class="highlight highlight-source-shell"><pre>conda env <span class="pl-k">export</span>
    --no-builds <span class="pl-k">|</span> grep -v prefix <span class="pl-k">|</span>
    grep -v name <span class="pl-k">&gt;</span> environment.yml</pre></div>

    <h1>

    <a id="user-content-pipeline-version-control" class="anchor" href="#pipeline-version-control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    version control</h1>

    <p>Each pipeline within this workflow uses <a href="https://pypi.org/project/bumpversion"
    rel="nofollow">bumpversion</a> for automatic <a href="https://semver.org" rel="nofollow">semantic
    versioning</a>.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    bump the appropriate increment</span>

    bumpversion patch --verbose --dry-run

    bumpversion minor --verbose --dry-run

    bumpversion major --verbose --dry-run


    <span class="pl-c"><span class="pl-c">#</span> commit with tags</span>

    git push --tags</pre></div>

    <h1>

    <a id="user-content-github-forks" class="anchor" href="#github-forks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GitHub forks</h1>

    <p>Forking the repository allows developers to work independently while retaining
    well-maintained code on the master fork. For instructions on how to fork, follow
    the <a href="https://help.github.com/en/articles/fork-a-repo">Fork a repo</a>
    instructions.</p>

    <p>After forking the repo, clone the repo to your local desktop:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    to use SSH</span>

    git clone git@github.com:<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git


    <span class="pl-c"><span class="pl-c">#</span> to use Https</span>

    git clone https://github.com/<span class="pl-k">&lt;</span>username<span class="pl-k">&gt;</span>/template-workflow.git</pre></div>

    <p>This creates a replica of the remote repository on your local desktop. <em>Note</em>:
    When you create your local repository, it will also make a local clone of the
    remote repository (typically as <code>origin</code>). So, your local master branch
    would simply be <code>master</code>. But, your remote master branch will be <code>origin/master</code>.
    You can also add multiple remote repositories. For instance, let us say our main
    repository is under the remote repository <code>my_repo</code>. We will want to
    add it as a remote repository, so we can fetch the most up-to-date code. You could
    add it by:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Add the my_repo remote repo to your local desktop -- this will allow you to pull
    and push to branches on the my_repo repository</span>

    git remote add my_repo git@github.com:my_repo/template-workflow.git</pre></div>

    <h1>

    <a id="user-content-git-branches" class="anchor" href="#git-branches" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Git branches</h1>

    <p>Branching is how git actually tracks code development. For more information,
    see the <a href="https://www.atlassian.com/git/tutorials/using-branches" rel="nofollow">Git
    Branch Tutorial</a> on Atlassian. If you want to add a new feature, pipeline,
    or fix a bug, a common work flow would look like this:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Update your local copy of the master branch to make sure you are getting the most
    up-to-date code</span>

    git pull


    <span class="pl-c"><span class="pl-c">#</span> Create the branch on your local
    machine and switch in this branch </span>

    git checkout -b [name_of_your_new_branch]


    <span class="pl-c"><span class="pl-c">#</span> Push the branch on github</span>

    git push origin [name_of_your_new_branch]</pre></div>

    <p>As you develop, you want to commit your work to your branch, so you don''t
    lose it all if something happens!</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Confirm we''re on the right branch</span>

    git branch -a


    <span class="pl-c"><span class="pl-c">#</span> Add all your work to be tracked
    (Note: there are many ways to add specific files, etc. See https://git-scm.com/docs/git-add
    for more information). The following command adds everything in your currently
    directory.</span>

    git add <span class="pl-c1">.</span>


    <span class="pl-c"><span class="pl-c">#</span> Commit your work to the branch
    with a message describing what''s in the commit</span>

    git commit -m <span class="pl-s"><span class="pl-pds">"</span>Created the scATAC-seq
    pipeline!<span class="pl-pds">"</span></span>


    <span class="pl-c"><span class="pl-c">#</span> You can add a -u parameter to set-upstream
    for a push</span>

    <span class="pl-c"><span class="pl-c">#</span> Alternatively, git will also automatically
    query you when you do your first push.</span>

    <span class="pl-c"><span class="pl-c">#</span> You can also set this manually
    by adding a new remote for your branch:</span>

    <span class="pl-c"><span class="pl-c">#</span>git remote add [name_of_your_remote]
    [name_of_your_new_branch]</span>


    <span class="pl-c"><span class="pl-c">#</span> Here is another push where we specify
    HEAD</span>

    <span class="pl-c"><span class="pl-c">#</span>git push origin HEAD # HEAD pushes
    everything up to the most recent commit</span></pre></div>

    <h1>

    <a id="user-content-code-review" class="anchor" href="#code-review" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Code review</h1>

    <p>Create a <a href="https://help.github.com/en/articles/creating-a-pull-request">GitHub
    Pull Request</a>. A PR allows other developers a chance to go through and comment
    on lines of code they believe can be improved. In addition, it will tell you if
    the code you are trying to merge into the <code>my_repo</code> branch actually
    conflicts with code that already exists in the branch, so you don''t overwrite
    someone else''s work.</p>

    <p>Once another developer approves the PR, you have the go-ahead to merge your
    code! Congrats, you finished your feature!</p>

    <p><em>Note</em>: There are some cases where you may just want to push directly
    to the my_repo fork, thereby avoiding code reviews. For instance, if you''re working
    on a one-off project that you want people to be able to see, but no one else is
    necessarily working on, you can always push directly to the branches on my_repo
    fork. Or, you could also still go through the steps of a PR, but simply merge
    your own code without CR.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1597367982.0
sajjadtorabian/singularity_recipes:
  data_format: 2
  description: null
  filenames:
  - Singularity.nipype-plus-jupyter-plus-seaborn
  full_name: sajjadtorabian/singularity_recipes
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-recipes\" class=\"anchor\" href=\"\
    #singularity-recipes\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Singularity Recipes</h1>\n<h2>\n<a id=\"user-content-nipype-plus-jupyter\"\
    \ class=\"anchor\" href=\"#nipype-plus-jupyter\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Nipype Plus Jupyter</h2>\n<p>We\
    \ first need to download the Docker layers. Normally this happens automatically\
    \ with <code>singularity build</code> or <code>singularity pull</code>, but if\
    \ you aren't using the development version 3.0 branch of Singularity that has\
    \ a fixed bug with respect to whiteout files, you will have issue when you do\
    \ these commands with nipype (note that it has whiteout files). What we did (because\
    \ didn't feel like installing another version of Singularity) was to do a pull\
    \ of nipype with the <a href=\"https://singularityhub.github.io/sregistry-cli\"\
    \ rel=\"nofollow\">Singularity Global Client</a> that will download the fixed\
    \ layers and then put them in the same cache that Singularity uses. Then we will\
    \ have what we need :)  Here is how you can install and use the client:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ pip install sregistry\n\
    $ sregistry pull nipype/nipype:latest</pre></div>\n<p>Now we want to debug the\
    \ build and find the missing path! To do this, you can build a completely empty\
    \ image to look around in. The recipe looks like this:</p>\n<pre><code># Singularity.nipype-plus-jupyter-empty\n\
    Bootstrap: docker\nFrom: nipype/nipype:latest\n</code></pre>\n<p>The above you\
    \ can save to whatever file you want, we're calling ours <code>Singularity.nipype-plus-jupyter-empty</code>\
    \ we can then build like this:</p>\n<pre><code>sudo singularity build npjup.simg\
    \ Singularity.nipype-plus-jupyter-empty\n</code></pre>\n<p>Then we can shell inside\
    \ and find locations of things.</p>\n<pre><code>singularity shell npjup.simg\n\
    </code></pre>\n<p>We are able to see what will be exported in the environment\
    \ at runtime, and then source it to add these locations to the path (so we can\
    \ find executables there!)</p>\n<pre><code>cat /environment\n$ cat /.singularity.d/env/10-docker.sh\
    \ \nexport PATH=\"/opt/conda/bin:/usr/lib/ants:/opt/afni:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\
    \nexport LANG=\"en_US.UTF-8\"\nexport LC_ALL=\"C.UTF-8\"\nexport ND_ENTRYPOINT=\"\
    /neurodocker/startup.sh\"\nexport MATLABCMD=\"/opt/mcr/v92/toolbox/matlab\"\n\
    export FORCE_SPMMCR=\"1\"\nexport LD_LIBRARY_PATH=\"/usr/lib/x86_64-linux-gnu:/opt/mcr/v92/runtime/glnxa64:/opt/mcr/v92/bin/glnxa64:/opt/mcr/v92/sys/os/glnxa64:\"\
    \nexport FREESURFER_HOME=\"/opt/freesurfer\"\nexport ANTSPATH=\"/usr/lib/ants\"\
    \nexport MKL_NUM_THREADS=\"1\"\nexport OMP_NUM_THREADS=\"1\"\nexport CONDA_DIR=\"\
    /opt/conda\"\n</code></pre>\n<p>There we see the location of conda! Strange that\
    \ it wasn't where we expected. Let's add to the path and then we have pip</p>\n\
    <pre><code>export PATH=/opt/conda/bin:$PATH\nwhich pip\n/opt/conda/bin/pip\n</code></pre>\n\
    <p>Now we can update the recipe <a href=\"Singularity.nipype-plus-jupyter\">Singularity.nipype-plus-jupyter</a>\
    \ with our found pip.</p>\n<pre><code>Bootstrap: docker\nFrom: nipype/nipype:latest\n\
    \n%labels\n  Maintainer Sajjad\n  Version v1.0\n\n%post\n    export PATH=/opt/conda/bin:$PATH\n\
    \    pip install --upgrade pip\n    pip install jupyter\n</code></pre>\n<p>And\
    \ build the image</p>\n<div class=\"highlight highlight-source-shell\"><pre>sudo\
    \ singularity build npjup.simg Singularity.nipype-plus-jupyter</pre></div>\n<p>:)</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1534761419.0
sbutcher/container-R:
  data_format: 2
  description: singularity container for use with singularity hub
  filenames:
  - Singularity
  full_name: sbutcher/container-R
  latest_release: null
  readme: '<h1>

    <a id="user-content-container-r" class="anchor" href="#container-r" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>container-R</h1>

    <p>singularity container for use with singularity hub</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1525440620.0
sbutcher/container-python:
  data_format: 2
  description: Python from source for use with singularity
  filenames:
  - Singularity
  full_name: sbutcher/container-python
  latest_release: null
  readme: '<h1>

    <a id="user-content-container-python" class="anchor" href="#container-python"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>container-python</h1>

    <p>Python from source for use with singularity</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1525427896.0
sbutcher/container-setc:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: sbutcher/container-setc
  latest_release: null
  readme: '<h1>

    <a id="user-content-container-setc" class="anchor" href="#container-setc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>container-setc</h1>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1538491698.0
sbutcher/game-container:
  data_format: 2
  description: Containers for game AI
  filenames:
  - Singularity
  full_name: sbutcher/game-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-game-container" class="anchor" href="#game-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>game-container</h1>

    <p>Containers for game AI</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1547647598.0
sbutcher/minigym-container:
  data_format: 2
  description: Container to run various Game AI workloads
  filenames:
  - Singularity
  full_name: sbutcher/minigym-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-minigym-container" class="anchor" href="#minigym-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>minigym-container</h1>

    <p>Container to run various Game AI workloads</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1548062559.0
seasite-project/CGO21_YaskSite_AD:
  data_format: 2
  description: "This is the Artifact Description repository for the CGO21 paper: YaskSite\
    \ \u2013 Stencil Optimization Techniques Applied to Explicit ODE Methods on Modern\
    \ Architectures"
  filenames:
  - Singularity
  full_name: seasite-project/CGO21_YaskSite_AD
  latest_release: CGO21v0.3
  readme: "<h1>\n<a id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"\
    #-cgo21_yasksite_ad-\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><ins> CGO21_YaskSite_AD </ins>\n</h1>\n<h1>\n\
    <a id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Setup\
    \ phase</h1>\n<p>Steps 1 to 3 guide you through setting up.</p>\n<h2>\n<a id=\"\
    user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1.1</h2>\n\
    <p>Clone this repository and go to the cloned directory.</p>\n<pre><code>git clone\
    \ https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\n\
    git checkout CGO21v0.3\n</code></pre>\n<h2>\n<a id=\"user-content-step-12\" class=\"\
    anchor\" href=\"#step-12\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Step 1.2</h2>\n<p>For the next steps we need\
    \ singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install\
    \ singularity with the following script if you have root access.</p>\n<pre><code>./install_singularity.sh\n\
    </code></pre>\n<h2>\n<a id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2</h2>\n<p>Download the singularity container.</p>\n<p>The pre-build\
    \ container is available under the following link <a href=\"https://doi.org/10.5281/zenodo.4415558\"\
    \ rel=\"nofollow\">https://doi.org/10.5281/zenodo.4415558</a>\nand can be installed\
    \ using:</p>\n<pre><code>wget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1\
    \ -O YS_CGO.sif\n</code></pre>\n<h2>\n<a id=\"user-content-step-3\" class=\"anchor\"\
    \ href=\"#step-3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Step 3</h2>\n<p>Once singularity image is downloaded\
    \ on the benchmarking system the first step is to run the app called build.\n\
    This installs YaskSite. It should be done at runtime since the YaskSite does machine\
    \ specific configuration\nat build time. Run the following to do this:</p>\n<pre><code>singularity\
    \ run --app build YS_CGO.sif \n</code></pre>\n<h1>\n<a id=\"user-content-run-phase\"\
    \ class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run phase</h1>\n<p>Step 4 illustrates\
    \ how to run the app to reproduce results.\nIt is recommended the settings in\
    \ the paper are followed to get comparable results.</p>\n<h2>\n<a id=\"user-content-step-4\"\
    \ class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Step 4</h2>\n<p>Run the apps\
    \ corresponding to YaskSite and Offsite. There are also pre-configured apps that\
    \ helps to\nreproduce data in figures of the paper. To see the list of available\
    \ apps use:</p>\n<pre><code>singularity run-help YS_CGO.sif\n</code></pre>\n<p>The\
    \ method to run each apps are described in corresponding app's help. For example\
    \ help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be\
    \ obtained using:</p>\n<pre><code>singularity run-help --app Fig4 YS_CGO.sif\n\
    </code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1609764345.0
seb-mueller/singularity_dropSeqPipe:
  data_format: 2
  description: Singularity container for dropSeqPipe
  filenames:
  - Singularity.v04
  - Singularity
  full_name: seb-mueller/singularity_dropSeqPipe
  latest_release: null
  readme: "<p><a href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a>\n<a href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\"\
    \ alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"\
    \ alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HEMnet - Haematoxylin &amp; Eosin and Molecular neural network</h1>\n\
    <h2>\n<a id=\"user-content-description\" class=\"anchor\" href=\"#description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Description</h2>\n<p>A deep learning automated cancer diagnosis software\
    \ using molecular labelling to improve pathological annotation of\nHaematoxylin\
    \ and Eosin (H&amp;E) stained tissue.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<ol>\n<li>\n\
    <p>Docker</p>\n<p>You can download and run the docker image using the following\
    \ commands:</p>\n<pre><code>docker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\
    </code></pre>\n</li>\n<li>\n<p>Conda</p>\n<p>Install Openslide (this is necessary\
    \ to open whole slide images) - download it <a href=\"https://openslide.org/download/\"\
    \ rel=\"nofollow\">here</a></p>\n<p>Create a conda environment from the <code>environment.yml</code>\
    \ file</p>\n<pre><code>conda env create -f environment.yml\nconda activate HEMnet\n\
    </code></pre>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-slide-preparation\"\
    \ class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Slide Preparation</h3>\n<p>Name\
    \ slides in the format: <code>slide_id_TP53</code> for TP53 slides and <code>slide_id_HandE</code>\
    \ for H&amp;E slides\nThe <code>TP53</code> and <code>HandE</code> suffix is used\
    \ by HEMnet to identify the stain used.</p>\n<h3>\n<a id=\"user-content-1-generate-training-and-testing-datasets\"\
    \ class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.\
    \ Generate training and testing datasets</h3>\n<p>a. Generate train dataset</p>\n\
    <p><code>python HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides\
    \ -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs\
    \ -v</code></p>\n<p>b. Generate test dataset</p>\n<p><code>python HEMnet_test_dataset.py\
    \ -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory\
    \ -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh\
    \ -n non_cancer_thresh</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is the relative path to the template slide from which all other slides will\
    \ be normalised against. The template\nslide should be the same for each step.</li>\n\
    <li>\n<code>-m</code> is the tile magnification. e.g. if  the input is <code>10</code>\
    \ then the tiles will be output at 10x</li>\n<li>\n<code>-a</code> is the align\
    \ magnification. Paired TP53 and H&amp;E slides will be registered at this magnification.\n\
    To reduce computation time we recommend this be less than the tile magnification\
    \ - a five times downscale generally works well.</li>\n<li>\n<code>-c</code> cancer\
    \ threshold to apply to the DAB channel. DAB intensities less than this threshold\
    \ indicate cancer.</li>\n<li>\n<code>-n</code> non-cancer threshold to apply to\
    \ the DAB channel. DAB intensities greater than this threshold indicate no cancer.</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\"\
    \ href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2. Train and evaluate model</h3>\n\
    <p>a. Training model</p>\n<p><code>python train.py -b /path/to/base/directory\
    \ -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory\
    \ -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size\
    \ -s -w -f -v</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-m</code>\
    \ is CNN base model. eg. <code>resnet50</code>, <code>vgg16</code>, <code>vgg19</code>,\
    \ <code>inception_v3</code> and <code>xception</code>.</li>\n<li>\n<code>-g</code>\
    \ is number of GPUs for training.</li>\n<li>\n<code>-e</code> is training epochs.\
    \ Default is <code>100</code> epochs.</li>\n<li>\n<code>-a</code> is batch size.\
    \ Default is <code>32</code>\n</li>\n<li>\n<code>-s</code> is option to save the\
    \ trained model weights.</li>\n<li>\n<code>-w</code> is option to used transfer\
    \ learning. Model will used pre-trained weights from ImageNet at the initial stage.</li>\n\
    <li>\n<code>-f</code> is fine-tuning option. Model will re-train CNN base.</li>\n\
    </ul>\n<p>b. Test model prediction</p>\n<p><code>python test.py  -b /path/to/base/directory\
    \ -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory\
    \ -w model_weights -m cnn_base -g num_gpus -v</code></p>\n<p>Other parameters:</p>\n\
    <ul>\n<li>\n<code>-w</code> is path to trained model. eg. <code>trained_model.h5</code>.</li>\n\
    <li>\n<code>-m</code> is CNN base model (same to training step).</li>\n<li>\n\
    <code>-g</code> is number of GPUs for prediction.</li>\n</ul>\n<p>c. Evaluate\
    \ model performance and visualise model prediction</p>\n<p><code>python visualisation.py\
    \ -b /path/to/base/directory -t /relative/path/to/training_output_directory -p\
    \ /relative/path/to/test_output_directory  -o /relative/path/to/output/directory\
    \ -i sample</code></p>\n<p>Other parameters:</p>\n<ul>\n<li>\n<code>-t</code>\
    \ is path to training outputs.</li>\n<li>\n<code>-p</code> is path to test outputs.</li>\n\
    <li>\n<code>-i</code> is name of Whole Slide Image for visualisation.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\"\
    \ href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3. Apply model to diagnose new\
    \ images</h3>\n<p><code>python HEMnet_inference.py -s '/path/to/new/HE/Slides/'\
    \ -o '/path/to/output/directory/' -t '/path/to/template/slide/' -nn '/path/to/trained/model/'\
    \ -v</code></p>\n<p>Predict on TCGA images with our pretrained model for colorectal\
    \ cancer using <a href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\"\
    \ rel=\"nofollow\">google colab</a></p>\n<h2>\n<a id=\"user-content-results\"\
    \ class=\"anchor\" href=\"#results\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Results</h2>\n<h2>\n<a id=\"\
    user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Citing\
    \ HEMnet</h2>\n<h2>\n<a id=\"user-content-the-team\" class=\"anchor\" href=\"\
    #the-team\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>The Team</h2>\n<p>Please contact Dr Quan Nguyen (<a href=\"mailto:quan.nguyen@uq.edu.au\"\
    >quan.nguyen@uq.edu.au</a>), Andrew Su (<a href=\"mailto:a.su@uqconnect.edu.au\"\
    >a.su@uqconnect.edu.au</a>),\nand Xiao Tan (<a href=\"mailto:xiao.tan@uqconnect.edu.au\"\
    >xiao.tan@uqconnect.edu.au</a>) for issues, suggestions,\nand we are very welcome\
    \ to collaboration opportunities.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1569595505.0
sghignone/Rnnotator:
  data_format: 2
  description: Rnnotator is an automated software pipeline that generates transcript
    models by de novo assembly of RNA-Seq data without the need for a reference genome.
  filenames:
  - Singularity
  full_name: sghignone/Rnnotator
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-rnnotator\" class=\"anchor\" href=\"#rnnotator\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Rnnotator</h1>\n<p>Rnnotator is an automated software pipeline that\
    \ generates transcript models by de novo assembly of RNA-Seq data without the\
    \ need for a reference genome.</p>\n<p>Rnnotator must be run on a 64-bit Linux\
    \ architecture. Before running Rnnotator the\nfollowing prerequisites must be\
    \ installed:</p>\n<ul>\n<li>Blat v. 34 (<a href=\"http://genome.ucsc.edu/FAQ/FAQblat.html#blat3\"\
    \ rel=\"nofollow\">http://genome.ucsc.edu/FAQ/FAQblat.html#blat3</a>) -- DONE</li>\n\
    <li>Velvet 1.0.15 (<a href=\"http://www.ebi.ac.uk/~zerbino/velvet/\" rel=\"nofollow\"\
    >http://www.ebi.ac.uk/~zerbino/velvet/</a>) -- DONE</li>\n<li>AMOS (<a href=\"\
    http://sourceforge.net/apps/mediawiki/amos/index.php\" rel=\"nofollow\">http://sourceforge.net/apps/mediawiki/amos/index.php</a>)\
    \ -- DONE</li>\n<li>Vmatch 2.0 (<a href=\"http://www.vmatch.de/\" rel=\"nofollow\"\
    >http://www.vmatch.de/</a>) -- DONE</li>\n<li>bwa 0.5.8c (<a href=\"http://bio-bwa.sourceforge.net/\"\
    \ rel=\"nofollow\">http://bio-bwa.sourceforge.net/</a>) -- DONE</li>\n<li>MUMmer\
    \ (<a href=\"http://sourceforge.net/projects/mummer/\" rel=\"nofollow\">http://sourceforge.net/projects/mummer/</a>)\
    \ -- DONE</li>\n<li>BioPerl (<a href=\"http://www.bioperl.org\" rel=\"nofollow\"\
    >http://www.bioperl.org</a>) -- base system</li>\n<li>Perl modules: Parallel::ForkManager,\
    \ Tree (<a href=\"http://search.cpan.org/\" rel=\"nofollow\">http://search.cpan.org/</a>)\
    \ -- DONE</li>\n</ul>\n<p>Optional prerequisites are:</p>\n<ul>\n<li>Oases 0.1.18\
    \ (<a href=\"http://www.ebi.ac.uk/~zerbino/oases/\" rel=\"nofollow\">http://www.ebi.ac.uk/~zerbino/oases/</a>)\
    \ -- DONE</li>\n<li>Bambus 2.33 (<a href=\"http://www.cbcb.umd.edu/software/bambus/\"\
    \ rel=\"nofollow\">http://www.cbcb.umd.edu/software/bambus/</a>)</li>\n<li>Sopra\
    \ 1.0 (<a href=\"mailto:dayarian@physics.rutgers.edu\">dayarian@physics.rutgers.edu</a>)\
    \ x1 \u2013 x4 scripts</li>\n</ul>\n<p>sg</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - pipeline
  - singularity
  - singularity-recipe
  - rnaseq
  - docker
  - dockerfile
  updated_at: 1612716290.0
shreyaskamathkm/Cluster_Images:
  data_format: 2
  description: This contains the latest docker and singularity images
  filenames:
  - Singularity_Ubuntu_20_04_Cuda_11_1
  - Singularity_Ubuntu_18_04_Cuda_10_2
  - Singularity_Ubuntu_16_04
  - Singularity_Ubuntu_18_04_Cuda_11_1
  full_name: shreyaskamathkm/Cluster_Images
  latest_release: null
  readme: '<h1>

    <a id="user-content-examples-with-drake-and-ros-2" class="anchor" href="#examples-with-drake-and-ros-2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Examples
    with Drake and ROS 2</h1>

    <p>This repo shows examples of using <a href="https://drake.mit.edu/" rel="nofollow">Drake</a>
    and <a href="https://www.ros.org/" rel="nofollow">ROS 2</a> together.

    It uses the pydrake API and is of prototype quality.

    For a similar effort in ROS 1, see <a href="https://github.com/EricCousineau-TRI/repro/tree/master/ros/drake_ros1_hacks">EricCousineau-TRI/repro
    <code>drake_ros1_hacks</code></a>.</p>

    <h2>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

    <ul>

    <li>Ubuntu Focal (20.04)</li>

    <li><a href="https://index.ros.org/doc/ros2/Installation/Rolling/" rel="nofollow">ROS
    2 Rolling</a></li>

    <li><a href="https://drake.mit.edu/from_binary.html" rel="nofollow">Some Drake
    binary installation from October 2020</a></li>

    </ul>

    <p>Install ROS 2 Rolling using the <a href="https://index.ros.org/doc/ros2/Installation/Rolling/Linux-Install-Debians/"
    rel="nofollow">Linux binary instructions</a> and <a href="https://index.ros.org/doc/ros2/Installation/Prerelease-Testing/"
    rel="nofollow">enable the ROS 2 testing apt repo</a>.

    Install the apt packages <code>ros-rolling-desktop</code> and <code>ros-rolling-sdformat-urdf</code>.

    Extract the Drake binary installation, install it''s prerequisites, and <a href="https://drake.mit.edu/python_bindings.html#inside-virtualenv"
    rel="nofollow">use this Python virutalenv trick</a>.</p>

    <h2>

    <a id="user-content-setup" class="anchor" href="#setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h2>

    <p>Once the prerequisites are met, install <code>drake_ros</code> into the Drake
    virtualenv.</p>

    <pre><code>. path/to/drake/bin/activate

    cd path/to/this/repo

    cd drake_ros/

    python setup.py develop

    </code></pre>

    <h2>

    <a id="user-content-ros-2-tf-and-robot-model-demo" class="anchor" href="#ros-2-tf-and-robot-model-demo"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ROS
    2 tf and Robot Model Demo</h2>

    <p>This demo shows RViz visualizing a single UR10 robot being simulated by Drake.

    Set up two terminals: one for launching RViz, and another for launching the Drake
    simulation.</p>

    <pre><code>. /opt/ros/rolling/setup.bash

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" rviz2 -d view.rviz

    </code></pre>

    <pre><code>. /opt/ros/rolling/setup.bash

    . path/to/drake/bin/activate

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" ./ros2_demo.py

    </code></pre>

    <p><a href="https://user-images.githubusercontent.com/4175662/90415417-e7976980-e065-11ea-9564-96c820f51680.gif"
    target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/4175662/90415417-e7976980-e065-11ea-9564-96c820f51680.gif"
    alt="ur10_rviz_drake" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-interactive-markers-demo" class="anchor" href="#interactive-markers-demo"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interactive
    Markers Demo</h2>

    <p>This demonstrates using interactive markers to control an iiwa14 being simulated
    by Drake.

    Set up two terminals: one for launching RViz, and another for launching the Drake
    simulation.</p>

    <pre><code>. /opt/ros/rolling/setup.bash

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" rviz2 -d interactive_demo.rviz

    </code></pre>

    <pre><code>. /opt/ros/rolling/setup.bash

    . path/to/drake/bin/activate

    cd path/to/this/repo

    AMENT_PREFIX_PATH="$AMENT_PREFIX_PATH:$(pwd)" ./interactive_demo.py

    </code></pre>

    <p><a href="https://user-images.githubusercontent.com/4175662/96510753-dcea8380-1212-11eb-89ca-4a9019a8a9cd.gif"
    target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/4175662/96510753-dcea8380-1212-11eb-89ca-4a9019a8a9cd.gif"
    alt="iiwa14_interactive_drake" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-using-a-container" class="anchor" href="#using-a-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    a Container</h2>

    <p>There is a definition file for a <a href="https://sylabs.io/singularity/" rel="nofollow">Singularity</a>
    container.</p>

    <p>First <a href="https://sylabs.io/guides/3.7/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">build and install Singularity</a>.

    Afterwards, build a Singularity sandbox from the definition file.</p>

    <pre><code>singularity build --fakeroot --sandbox ~/drake-ros2-demos.sandbox demos.singularity.def

    </code></pre>

    <p>Create a shell with access to an NVidia graphics card and run one of the RViz
    configs for your chosen demo.</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    shell --nv --writable-tmpfs <span class="pl-k">~</span>/drake-ros2-demos.sandbox</span>

    <span class="pl-c1">Singularity&gt; rviz2 -d view.rviz</span></pre></div>

    <p>Create a shell into the sandbox and run one of the demos.</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    shell --writable <span class="pl-k">~</span>/drake-ros2-demos.sandbox</span>

    <span class="pl-c1">Singularity&gt; ./ros2_demo.py</span></pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616693700.0
shreyaskamathkm/singularity_meshroom:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: shreyaskamathkm/singularity_meshroom
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_meshroom" class="anchor" href="#singularity_meshroom"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_meshroom</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602807348.0
simarocchi/openmpi_centos7_x86_64:
  data_format: 2
  description: a Singularity recipe with openmpi 2.1.1 on base centos 7 to run on
    the Cineca clusters x86_64 based
  filenames:
  - Singularity
  full_name: simarocchi/openmpi_centos7_x86_64
  latest_release: null
  readme: '<h1>

    <a id="user-content-openmpi_centos7_x86_64" class="anchor" href="#openmpi_centos7_x86_64"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>openmpi_centos7_x86_64</h1>

    <p>a Singularity recipe with openmpi 2.1.1 on base centos 7 to run on the Cineca
    clusters x86_64 based</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605098444.0
sina-ehsani/hpc-singularity:
  data_format: 2
  description: Singularity for HPC
  filenames:
  - Singularity.centos7-python3.7-transformers3.0.2-ImageCrawl
  - Singularity.centos7-python3.7-transformers2.11.0-ImageCrawl
  full_name: sina-ehsani/hpc-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc-singularity" class="anchor" href="#hpc-singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>hpc-singularity</h1>

    <p>Singularity for HPC</p>

    <p>Make sure the sigularity is built on <a href="https://singularity-hub.org"
    rel="nofollow">https://singularity-hub.org</a></p>

    <p>if ready use:</p>

    <p><code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers3.0.2-imagecrawl</code></p>

    <p>Transformer 2.11.0:

    <code>singularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers2.11.0-imagecrawl</code></p>

    <p>Make sure the imagecrawl is updated (latest commit)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1601682764.0
slaclab/folding-at-home-gpu:
  data_format: 2
  description: gpu image for folding at home
  filenames:
  - Singularity
  full_name: slaclab/folding-at-home-gpu
  latest_release: null
  readme: '<h1>

    <a id="user-content-folding-at-home-gpu" class="anchor" href="#folding-at-home-gpu"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>folding-at-home-gpu</h1>

    <p>gpu image for folding at home</p>

    <p>simple merge of nvidia cl image with folding at home v7.5.1 to enable gpu processing.</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1584940583.0
slaclab/singularity-modules:
  data_format: 2
  description: Singularity Container Recipes
  filenames:
  - images/Singularity.lsf
  - images/cryolo/1.5.4/Singularity
  - images/ctffind/Singularity
  - images/ctffind/4.1.12/Singularity
  - images/ctffind/4.1.10/Singularity
  - images/ctffind/4.1.13/Singularity
  - images/emClarity/Singularity
  - images/protomo/Singularity
  - images/cdms-jupyterlab/Singularity
  - images/eman2/Singularity
  - images/eman2/20190324/Singularity
  - images/eman2/20190805/Singularity
  - images/eman2/20190917/Singularity
  - images/eman2/2.31/Singularity
  - images/eman2/20190418/Singularity
  - images/eman2/20200319.0/Singularity
  - images/eman2/20200419/Singularity
  - images/eman2/20200330/Singularity
  - images/eman2/20190603/Singularity
  - images/slac-ml/Singularity
  - images/slac-ml/20200227.0/Singularity
  - images/slac-ml/20200211.0/Singularity
  - images/slac-ml/20190712.2/Singularity
  - images/slac-ml/20200618.0/Singularity
  - images/relion/Singularity.docker
  - images/relion/Singularity.old
  - images/relion/Singularity
  - images/relion/ver3.1/Singularity.docker
  - images/relion/3.0.7/Singularity
  - images/relion/3.0.7/Singularity.orig
  - images/relion/3.0.2/Singularity.docker
  - images/relion/3.0.2/Singularity
  - images/relion/3.0.4/Singularity.docker
  - images/relion/3.0.4/Singularity
  - images/relion/2.1/Singularity.docker
  - images/relion/2.1/Singularity
  - images/relion/3.0.6/Singularity.docker
  - images/relion/3.0.6/Singularity
  - images/relion/3.0.8/Singularity.docker
  - images/relion/3.1.0-beta/Singularity.docker
  - images/relion/3.1.0-beta/Singularity
  - images/tem-simulator/Singularity
  - images/fah/7.5.1/Singularity
  - images/rosetta/Singularity
  - images/rosetta/2018.48/Singularity
  - images/phenix/Singularity
  - images/git/Singularity
  - images/topaz/0.2.4/Singularity
  - images/topaz/0.2.2/Singularity
  - images/chimera/Singularity
  - images/pymol/Singularity
  - images/imagemagick/Singularity
  - images/rclone/Singularity
  - images/cryosparc/2.13.2/Singularity
  - images/cryosparc/2.12.4/Singularity
  - images/cryosparc/2.14.2/Singularity
  - images/xds/Singularity
  - images/motioncor2/Singularity
  - images/motioncor2/1.2.3-intpix/Singularity
  - images/motioncor2/1.2.1/Singularity
  - images/motioncor2/1.3.0/Singularity
  - images/motioncor2/1.2.2/Singularity
  - images/motioncor2/1.2.6/Singularity
  - images/motioncor2/1.2.3/Singularity
  - images/motioncor2/1.3.2/Singularity
  - images/resmap/Singularity
  - images/openmbir/2.3.5/Singularity
  - images/amira/6.7.0/Singularity
  - images/scipion/Singularity
  - images/imod/Singularity
  - images/imod/4.9.12/Singularity
  - images/imod/4.10.42/Singularity
  - images/imod/4.9.11/Singularity
  - images/imod/4.9.10/Singularity
  - images/imod/4.10.38/Singularity
  - images/appion-protomo/Singularity
  - images/openmpi/Singularity.centos7
  - images/openmpi/Singularity.ubuntu1810
  - images/openmpi/Singularity
  - images/openmpi/Singularity.ubuntu1804
  - images/matlab/R2020a/Singularityfile
  - images/icon-gpu/Singularity
  full_name: slaclab/singularity-modules
  latest_release: null
  readme: '<h3>

    <a id="user-content-simnibs-singularity-recipe" class="anchor" href="#simnibs-singularity-recipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SimNIBS
    singularity recipe</h3>

    <p>Before building, place the SimNIBS source tarball in the /tmp directory. (recipe
    version 2.1.1)</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1592808314.0
sleeepyjack/variant_calling:
  data_format: 2
  description: null
  filenames:
  - Singularity.gpu
  - Singularity.cpu
  full_name: sleeepyjack/variant_calling
  latest_release: null
  readme: '<h1>

    <a id="user-content-variant-calling" class="anchor" href="#variant-calling" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Variant Calling</h1>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies:</h2>

    <pre><code>- make

    - Singularity (&gt;= v3.2)

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1589243145.0
slhogle/singularity_def_files:
  data_format: 2
  description: Singularity definition files for building various software to run on
    HPC systems
  filenames:
  - checkm.def
  - instrain.def
  - octopus.def
  - torstyverse.def
  full_name: slhogle/singularity_def_files
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-definition-files" class="anchor" href="#singularity-definition-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    definition files</h1>

    <p>Collection of def files for building some bioinformatics software I commonly
    use.</p>

    <h2>

    <a id="user-content-instrain-v1214" class="anchor" href="#instrain-v1214" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>inStrain v1.2.14</h2>

    <p><a href="https://github.com/MrOlm/inStrain">https://github.com/MrOlm/inStrain</a></p>

    <p>Also contains these functioning binaries:</p>

    <ul>

    <li><a href="https://github.com/samtools/samtools">samtools v1.10</a></li>

    <li><a href="https://github.com/hyattpd/Prodigal">prodigal v2.6.3</a></li>

    <li><a href="https://github.com/lh3/bwa">bwa v0.7.17-r1198-dirty</a></li>

    <li><a href="https://github.com/lh3/minimap2">minimap2 v2.17 (r941)</a></li>

    <li><a href="https://github.com/dnbaker/dashing">Dashing v0.4.8-1-g47e6</a></li>

    <li><a href="https://github.com/ParBLiSS/FastANI">FastANI v1.3</a></li>

    <li><a href="https://github.com/wwood/CoverM">CoverM v0.4.0</a></li>

    </ul>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/instrain" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/instrain</code></p>

    <h2>

    <a id="user-content-octopus-development-branch-version-v070-develop-2bde0433"
    class="anchor" href="#octopus-development-branch-version-v070-develop-2bde0433"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Octopus
    development branch version v0.7.0 (develop 2bde0433)</h2>

    <p><a href="https://github.com/luntergroup/octopus">https://github.com/luntergroup/octopus</a></p>

    <p>Built with:</p>

    <ul>

    <li>patchelf v0.10</li>

    <li>openssl v1.1.1g</li>

    <li>pkg-config v0.29.2</li>

    <li>gpatch v2.7.6</li>

    <li>ncurses v6.2</li>

    <li>cmake v3.17.3</li>

    <li>htslib v1.10</li>

    <li>boost v1.72.0</li>

    <li>GNU C/C++ compiler v9.3.0</li>

    </ul>

    <p>Target: x86_64 Linux 5.3.0-7642-generic<br>

    SIMD extension: AVX2</p>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/octopus" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/octopus</code></p>

    <h2>

    <a id="user-content-torstyverse" class="anchor" href="#torstyverse" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Torstyverse</h2>

    <p>Bundle of useful packages from <a href="https://github.com/tseemann">Torsten
    Seeman</a></p>

    <ul>

    <li><a href="https://github.com/tseemann/samclip">sampclip v0.4.0</a></li>

    <li><a href="https://github.com/tseemann/any2fasta">any2fasta v0.4.2</a></li>

    <li><a href="https://github.com/tseemann/barrnap">barrnap v0.9</a></li>

    <li><a href="https://github.com/tseemann/prokka">prokka v1.14.6</a></li>

    <li><a href="https://github.com/tseemann/shovill">shovill v1.1.0</a></li>

    <li><a href="https://github.com/tseemann/abricate">abricate v1.0.1</a></li>

    <li><a href="https://github.com/tseemann/snippy">snippy v4.6.0</a></li>

    </ul>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/torstyverse" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/torstyverse</code></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1614942669.0
solvcon/solvcon:
  data_format: 2
  description: A software framework of conservation-law solvers that use the space-time
    Conservation Element and Solution Element (CESE) method.
  filenames:
  - contrib/singularity/Singularity
  - contrib/singularity/Singularity.1.0.0-0.1.4+
  - contrib/singularity/Singularity.0.1.0
  full_name: solvcon/solvcon
  latest_release: 0.1.4
  readme: '<h3>

    <a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon
    octicon-link"></span></a><a href="https://github.com/openhpc/ohpc/blob/master/docs/recipes/install/common/figures/ohpc_logo.png"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/openhpc/ohpc/raw/master/docs/recipes/install/common/figures/ohpc_logo.png"
    width="170" valign="middle" hspace="5" alt="OpenHPC" style="max-width:100%;"></a>

    </h3>

    <h3>

    <a id="user-content-community-building-blocks-for-hpc-systems" class="anchor"
    href="#community-building-blocks-for-hpc-systems" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Community building blocks for HPC systems</h3>

    <h4>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h4>

    <p>This stack provides a variety of common, pre-built ingredients required to

    deploy and manage an HPC Linux cluster including provisioning tools, resource

    management, I/O clients, runtimes, development tools, containers, and a variety
    of

    scientific libraries.</p>

    <p>There are currently two release series:

    <a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> and

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>, which target different
    major

    Linux OS distributions. The 1.3.x series targets CentOS7 and SLES12 while the

    2.x series targets CentOS8 and Leap15.</p>

    <h4>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h4>

    <p>OpenHPC provides pre-built binaries via repositories for use with standard

    Linux package manager tools (e.g. <code>yum</code> or <code>zypper</code>). To
    get started,

    you can enable an OpenHPC repository locally through installation of an

    <code>ohpc-release</code> RPM which includes gpg keys for package signing and
    defines

    the URL locations for [base] and [update] package repositories. Installation

    guides tailored for each supported provisioning system and resource manager

    with detailed example instructions for installaing a cluster are also available.

    Copies of the <code>ohpc-release</code> package and installation guides along
    with

    more information is available on the relevant release series pages

    (<a href="https://github.com/openhpc/ohpc/wiki/1.3.X">1.3.x</a> or

    <a href="https://github.com/openhpc/ohpc/wiki/2.x">2.x</a>).</p>

    <hr>

    <h4>

    <a id="user-content-questions-comments-or-bug-reports" class="anchor" href="#questions-comments-or-bug-reports"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Questions,
    Comments, or Bug Reports?</h4>

    <p>Subscribe to the users email list at <a href="https://groups.io/g/openhpc-users"
    rel="nofollow">https://groups.io/g/openhpc-users</a> or see

    the <a href="http://openhpc.community" rel="nofollow">http://openhpc.community</a>
    page for more pointers.</p>

    <h4>

    <a id="user-content-additional-software-requests" class="anchor" href="#additional-software-requests"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Additional
    Software Requests?</h4>

    <p>Please see the component submission page at

    <a href="https://github.com/openhpc/submissions">https://github.com/openhpc/submissions</a>
    for more information regarding new

    software inclusion requests.</p>

    <h4>

    <a id="user-content-register-your-system" class="anchor" href="#register-your-system"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Register
    your system</h4>

    <p>If you are using elements of OpenHPC, please consider registering your

    system(s) using the <a href="https://drive.google.com/open?id=1KvFM5DONJigVhOlmDpafNTDDRNTYVdolaYYzfrHkOWI"
    rel="nofollow">System Registration

    Form</a>.</p>

    '
  stargazers_count: 15
  subscribers_count: 8
  topics:
  - computational-science
  updated_at: 1618917999.0
soudabeh19/centos7-reprozip.fslbuild-centos5:
  data_format: 2
  description: PreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)
  filenames:
  - Singularity
  full_name: soudabeh19/centos7-reprozip.fslbuild-centos5
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7-reprozipfslbuild-centos5" class="anchor" href="#centos7-reprozipfslbuild-centos5"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>centos7-reprozip.fslbuild-centos5</h1>

    <p>PreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1521572666.0
soycoder/nemo:
  data_format: 2
  description: HPC-AI 2020 | Training Project NEMO - Nucleus for European Modelling
    of the Ocean
  filenames:
  - Slurm Script/Singularity.CENTOS-7.7-NEMO-MOFED
  - Slurm Script/Singularity.nemo.apps
  full_name: soycoder/nemo
  latest_release: null
  readme: "<h1>\n<a id=\"user-content--nemo---ocean\" class=\"anchor\" href=\"#-nemo---ocean\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><g-emoji class=\"g-emoji\" alias=\"ocean\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png\"\
    >\U0001F30A</g-emoji> NEMO - ocean</h1>\n<p>HPC-AI 2020 | Training Project - NEMO:\
    \ Nucleus for European Modelling of the Ocean</p>\n<h2>\n<a id=\"user-content--docker-images---centos\"\
    \ class=\"anchor\" href=\"#-docker-images---centos\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><g-emoji class=\"\
    g-emoji\" alias=\"floppy_disk\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png\"\
    >\U0001F4BE</g-emoji> Docker Images - CentOS</h2>\n<p>Thank you for an image \
    \ (<a href=\"https://hub.docker.com/r/wangyoucao577/centos7-gcc7.4\" rel=\"nofollow\"\
    >wangyoucao577/centos7-gcc7.4</a>)</p>\n<h2>\n<a id=\"user-content--tag\" class=\"\
    anchor\" href=\"#-tag\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><g-emoji class=\"g-emoji\" alias=\"bookmark\"\
    \ fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f516.png\"\
    >\U0001F516</g-emoji> Tag</h2>\n<ul>\n<li><a href=\"https://hub.docker.com/layers/soycoder/centos7/nemo-ocean/images/sha256-c7bdaa3614e1fc1bbef31bdb05ac997e64b11abff716d00315807b1b79ad13c3\"\
    \ rel=\"nofollow\">:nemo-ocean</a></li>\n</ul>\n<h2>\n<a id=\"user-content--environment\"\
    \ class=\"anchor\" href=\"#-environment\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a><g-emoji class=\"g-emoji\" alias=\"\
    sunrise_over_mountains\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f304.png\"\
    >\U0001F304</g-emoji> Environment</h2>\n<ol>\n<li>HPC-X to build an out-of-box\
    \ MPI environment</li>\n<li>Boost library</li>\n<li>HDF5 Parallellibrary</li>\n\
    <li>NETCDF Parallel library with HDF5</li>\n<li>NETCDF-FortranParallel library\
    \ with NETCDF Parallel</li>\n<li>XIOS</li>\n<li>GYREwith GNUgfortran + HPC-X OpenMPI</li>\n\
    </ol>\n<div class=\"highlight highlight-text-html-basic\"><pre>/usr/mpi/gcc/openmpi-3.1.1rc1/bin/mpirun\
    \ \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 \\\n-mca mpi_show_mca_params 1\
    \ -mca pml_ucx_verbose 9 \\\n/usr/mpi/gcc/openmpi-3.1.1rc1/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n\n/usr/mpi/gcc/openmpi-3.1.1rc1/bin/mpirun -n 2 \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1\
    \ \\\n/usr/mpi/gcc/openmpi-3.1.1rc1/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n/usr/bin/time -p mpirun -n 2 \\\n-mca pml ucx -x UCX_TLS=rc UCX_NET_DEVICES=mlx5_0:1\
    \ ./nemo\n\n/usr/bin/time -p mpirun -n 2 \\\n-mca -x UCX_TLS=rc -x UCX_NET_DEVICES=mlx5_0:1\
    \ ./nemo\n\n/usr/bin/time -p mpirun -n 2 \\\n-mca -x UCX_TLS=rc -x UCX_NET_DEVICES=ib0\
    \ \\\n/home/hpc/nemo/apps/hpcx-v2.6.0-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-redhat7.7-x86_64/ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \nibstat\n\n\nNow step into the container and install MOFED:\n\n$ sudo singularity\
    \ exec -w u16.04-sandbox/ bash\n(singularity)# cd MOFED/MLNX_OFED_LINUX-4.3-1.0.1.0-ubuntu16.04-x86_64\n\
    (singularity)# ./mlnxofedinstall\n\n\n! -- (nemo) singularity exec -w nemo.sif\
    \ bash\n\n\n## Run container\nTo use Singularity in Mellanox/HPCX need to load\
    \ env module: `module load tools/singularity`\n.\n\nRun `osu_latency` test:\n\
    ```sh\n$ mpirun -np 2 --map-by node -mca btl self singularity exec hpcx-u16.04.simg\
    \ /hpcx/ompi-a7df\nd94/tests/osu-micro-benchmarks-5.3.2/osu_latency\n# OSU MPI\
    \ Latency Test v5.3.2\n# Size          Latency (us)\n0                       1.55\n\
    1                       1.55\n2                       1.55\n4                \
    \       1.55\n8                       1.54\n16                      1.55\n32 \
    \                     1.55\n64                      1.65\n128                \
    \     2.19\n256                     2.23\n512                     2.35\n1024 \
    \                   2.64\n2048                    2.89\n4096                 \
    \   3.51\n8192                    5.00\n16384                   6.44\n32768  \
    \                 8.91\n65536                  14.12\n131072                 25.05\n\
    262144                 27.31\n524288                 49.03\n1048576          \
    \      92.53\n2097152               178.95\n4194304               351.24\n\n\n\
    \n$hpcx_mpi_dir/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\ncd /home/hpc/nemo/apps/hpcx-v2.6.0-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-redhat7.7-x86_64\n\
    \nmpirun \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 \\\n-mca mpi_show_mca_params\
    \ 1 -mca pml_ucx_verbose 9 \\\n./ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n\nmpirun \\\n-mca mpi_show_mca_params 1 -mca pml_ucx_verbose 9 \\\n./ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\
    \n\n\n/usr/bin/time -p mpirun -np 4 \\\n--map-by core -report-bindings \\\n-mca\
    \ io ompio -x UCX_NET_DEVICES=mlx5_0:1 ./nemo</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1603363757.0
stevekm/IGV-snapshot-nf:
  data_format: 2
  description: Nextflow workflow for automated IGV snapshots
  filenames:
  - containers/IGV/Singularity.IGV
  full_name: stevekm/IGV-snapshot-nf
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-igv-snapshot-nf\" class=\"anchor\" href=\"#igv-snapshot-nf\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>IGV-snapshot-nf</h1>\n<p>An example Nextflow workflow for creating\
    \ automated IGV snapshots of .bam files based on a list of target regions.</p>\n\
    <p>This workflow is designed to show how to integrate <a href=\"https://github.com/stevekm/IGV-snapshot-automator\"\
    >automated IGV snapshotting</a> into a Nextflow workflow.</p>\n<h2>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p>First, clone this repository:</p>\n<pre><code>git clone https://github.com/stevekm/IGV-snapshot-automator.git\n\
    cd IGV-snapshot-automator\n</code></pre>\n<h3>\n<a id=\"user-content-containers\"\
    \ class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Containers</h3>\n<p>Docker and/or\
    \ Singularity containers are used to package IGV, X11, and <code>xvfb</code> required\
    \ for functionality. Docker is required to build Singularity containers</p>\n\
    <p>To build the Docker container for IGV:</p>\n<pre><code>cd containers\nmake\
    \ docker-build VAR=IGV\n</code></pre>\n<p>To test out the IGV Docker container:</p>\n\
    <pre><code>make docker-test VAR=IGV\n</code></pre>\n<p>(optional) To build a Singuarity\
    \ container for IGV, first build the Singularity Docker container:</p>\n<pre><code>make\
    \ docker-build VAR=Singularity-2.4\n</code></pre>\n<ul>\n<li>This container is\
    \ used to build Singularity containers</li>\n</ul>\n<p>To build the Singularity\
    \ container for IGV:</p>\n<pre><code>make singularity-build VAR=IGV\n\n# test\
    \ the container:\nmake singularity-test VAR=IGV\n</code></pre>\n<ul>\n<li>The\
    \ Singularity container will be saved to <code>containers/IGV/IGV.simg</code>,\
    \ which you can upload to your remote server for usage</li>\n</ul>\n<h1>\n<a id=\"\
    user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h1>\n\
    <p>Run the included demo workflow (from the parent repo directory):</p>\n<pre><code>make\
    \ run\n</code></pre>\n<p>Should look something like this:</p>\n<pre><code>IGV-snapshot-nf$\
    \ make run\n./nextflow run main.nf -profile \"docker\"\nN E X T F L O W  ~  version\
    \ 19.04.1\nLaunching `main.nf` [kickass_cray] - revision: 1823b32e4f\n~~~~~~~\
    \ IGV Pipeline ~~~~~~~\n* Project dir:        /Users/steve/projects/IGV-snapshot-nf\n\
    * Launch dir:         /Users/steve/projects/IGV-snapshot-nf\n* Work dir:     \
    \      /Users/steve/projects/IGV-snapshot-nf/work\n* Profile:            docker\n\
    * Script name:        main.nf\n* Script ID:          1823b32e4f4fbc1caa63b0c12b2d4340\n\
    * Container engine:   docker\n* Workflow session:   843f9541-9cc2-46c8-9005-89659c67ed80\n\
    * Nextflow run name:  kickass_cray\n* Nextflow version:   19.04.1, build 5072\
    \ (03-05-2019 12:29 UTC)\n* Launch command:\nnextflow run main.nf -profile docker\n\
    [warm up] executor &gt; local\nexecutor &gt;  local (1)\n[91/852794] process &gt;\
    \ run_IGV [100%] 1 of 1 \u2714\nCompleted at: 22-May-2019 15:27:46\nDuration \
    \   : 1m 20s\nCPU hours   : (a few seconds)\nSucceeded   : 1\n</code></pre>\n\
    <p>Example snapshot output:</p>\n<p><a href=\"https://raw.githubusercontent.com/stevekm/IGV-snapshot-nf/output/output/snapshots/chr13_113976596_113976736.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/stevekm/IGV-snapshot-nf/output/output/snapshots/chr13_113976596_113976736.png\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-software\"\
    \ class=\"anchor\" href=\"#software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Software</h1>\n<ul>\n<li>\n<p>Tested\
    \ with macOS 10.12.6 and RHEL 7</p>\n</li>\n<li>\n<p>Nextflow (requires Java 8+\
    \ and <code>bash</code>)</p>\n</li>\n<li>\n<p>IGV 2.4.10</p>\n</li>\n<li>\n<p>Python</p>\n\
    </li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - igv
  - nextflow
  updated_at: 1558554996.0
stevekm/MuTect2_target_chunking:
  data_format: 2
  description: demo pipeline for testing different data chunking methods for MuTect2
  filenames:
  - containers/annovar-150617/Singularity.annovar-150617
  - containers/variant-calling-0.0.2/Singularity.variant-calling-0.0.2
  full_name: stevekm/MuTect2_target_chunking
  latest_release: null
  readme: '<h1>

    <a id="user-content-mutect2-target-chunking" class="anchor" href="#mutect2-target-chunking"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MuTect2
    Target Chunking</h1>

    <p>Demo pipeline for testing different data chunking methods for MuTect2.</p>

    <p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_cancer_m2_MuTect2.php"
    rel="nofollow">MuTect2</a> is a common tool used for variant calling of tumor-normal
    pairs. However, it is limited to running only in single-threaded mode, which can
    lead to extremely long execution times.</p>

    <p>This demo pipeline uses different techniques to chunk the included list of
    target regions (<code>targets.bed</code>) into smaller segments to run in parallel,
    then aggregate all results for comparison to ensure that variant calls are the
    same across all chunking methods.</p>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <p>This pipeline comes pre-configured for usage on NYULMC''s Big Purple HPC cluster
    using pre-built Singularity containers and pre-downloaded reference files.</p>

    <p>In order to use this pipeline on your system you will need to update the file
    paths saved in <code>nextflow.config</code> for your system.</p>

    <p>Singularity and Docker container recipes are included in the <code>containers</code>
    directory.</p>

    <p>Paths to input .bam files for tumor and normal samples are read from the file
    <code>samples.analysis.tsv</code>.</p>

    <p>Once correctly configured, the pipeline can be run with:</p>

    <pre><code>make run

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics:
  - nextflow
  - mutect2
  - variant-calling
  updated_at: 1562090008.0
stevekm/NYU-phoenix-docker-singularity-nextflow-demo:
  data_format: 2
  description: Nextflow + Singularity/Docker demo for CentOS 6.8 without OverlayFS
  filenames:
  - containers/demo1/Singularity.demo1
  - containers/base/Singularity.base
  full_name: stevekm/NYU-phoenix-docker-singularity-nextflow-demo
  latest_release: null
  readme: '<h1>

    <a id="user-content-nyu-phoenix-hpc-dockersingularity-nextflow-demo" class="anchor"
    href="#nyu-phoenix-hpc-dockersingularity-nextflow-demo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NYU phoenix HPC Docker/Singularity
    Nextflow Demo</h1>

    <p>Demo on how to run a Nextflow pipeline on the HPC using Singularity containers
    built from Docker.</p>

    <h1>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <p>Clone this repository</p>

    <pre><code>git clone https://github.com/stevekm/NYU-phoenix-docker-singularity-nextflow-demo.git

    cd NYU-phoenix-docker-singularity-nextflow-demo

    </code></pre>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <h2>

    <a id="user-content-remote-hpc-phoenix" class="anchor" href="#remote-hpc-phoenix"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Remote
    HPC (phoenix)</h2>

    <p>To run this workflow on the NYU phoenix HPC system, use the following command:</p>

    <pre><code>make run-p

    </code></pre>

    <p>This will:</p>

    <ul>

    <li>

    <p>install Nextflow to the current directory</p>

    </li>

    <li>

    <p>extract a pre-built demo Singularity image from this repository</p>

    </li>

    <li>

    <p>run the Nextflow pipeline using the Singularity image</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-local" class="anchor" href="#local" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Local</h2>

    <p>To run this workflow on your local computer (Docker required), use the following
    command:</p>

    <pre><code>make run-l

    </code></pre>

    <p>This will:</p>

    <ul>

    <li>

    <p>install Nextflow to the current directory</p>

    </li>

    <li>

    <p>build the Docker containers included in this repository</p>

    </li>

    <li>

    <p>run the Nextflow pipeline using the Docker containers</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h1>

    <ul>

    <li>

    <p><code>Makefile</code>: shortcuts to common actions used in the demo</p>

    </li>

    <li>

    <p><code>main.nf</code>: main Nextflow pipeline file</p>

    </li>

    <li>

    <p><code>nextflow.config</code>: Nextflow configuration file</p>

    </li>

    <li>

    <p><code>bin</code>: directory for scripts to use inside the Nextflow pipeline;
    its contents will be prepended to your <code>PATH</code> when pipeline tasks are
    executed</p>

    </li>

    <li>

    <p><code>containers</code>: directory containing Docker and Singularity container
    files, along with documentation on their setup &amp; usage</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-software-requirements" class="anchor" href="#software-requirements"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
    Requirements</h1>

    <h2>

    <a id="user-content-local--remote-hpc-server" class="anchor" href="#local--remote-hpc-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>local
    &amp; remote HPC server</h2>

    <ul>

    <li>

    <p>Java 8 (for Nextflow)</p>

    </li>

    <li>

    <p>GraphViz Dot (to compile flowchart)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-local-only" class="anchor" href="#local-only" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>local only</h2>

    <ul>

    <li>

    <p>Docker version 17.12.0-ce, build c97c6d6</p>

    </li>

    <li>

    <p>Vagrant version 2.0.1 (for tesing Singularity containers)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-remote-hpc-server-only" class="anchor" href="#remote-hpc-server-only"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>remote
    HPC server only</h2>

    <ul>

    <li>Singularity version 2.4.2</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1521145930.0
stevekm/bwa-bench:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity
  full_name: stevekm/bwa-bench
  latest_release: null
  readme: ''
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1549319905.0
stevekm/nextflow-demos:
  data_format: 2
  description: Example Nextflow pipelines and programming techniques
  filenames:
  - Singularity/Singularity
  full_name: stevekm/nextflow-demos
  latest_release: null
  readme: '<h1>

    <a id="user-content-nextflow-demos" class="anchor" href="#nextflow-demos" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nextflow-demos</h1>

    <p>Demonstrations of various programming techniques for use inside <a href="https://www.nextflow.io/"
    rel="nofollow">Nextflow</a> pipelines. This repository is meant to be a supplement
    to the <a href="https://www.nextflow.io/docs/latest/getstarted.html" rel="nofollow">official
    Nextflow documentation</a> (links below).</p>

    <ul>

    <li>

    <p>an overview presentation about Nextflow can be found <a href="https://github.com/stevekm/nextflow-demos/blob/docs/docs/Nextflow_presentation.pdf">here</a>
    (view <a href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/stevekm/nextflow-demos/docs/docs/Nextflow_presentation.pdf"
    rel="nofollow">here</a>)</p>

    </li>

    <li>

    <p>Nextflow HTML report examples can be found here:</p>

    <ul>

    <li>

    <p><a href="https://htmlpreview.github.io/?https://github.com/stevekm/nextflow-demos/blob/report-output/reporting/nextflow-report.html"
    rel="nofollow">pipeline report</a></p>

    </li>

    <li>

    <p><a href="https://htmlpreview.github.io/?https://github.com/stevekm/nextflow-demos/blob/report-output/reporting/timeline-report.html"
    rel="nofollow">timeline report</a></p>

    </li>

    </ul>

    </li>

    </ul>

    <p><em>NOTE</em>: Some of the techniques demonstrated here may be deprecated by
    the new <a href="https://www.nextflow.io/docs/latest/dsl2.html" rel="nofollow">DSL2</a>
    syntax offered by Nextflow. Be sure to check that out as well.</p>

    <h1>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h1>

    <p>Clone this repo:</p>

    <div class="highlight highlight-source-shell"><pre>git clone git@github.com:stevekm/nextflow-demos.git

    <span class="pl-c1">cd</span> nextflow-demos</pre></div>

    <h1>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h1>

    <p>Each subdirectory contains files to run sample Nextflow pipelines.</p>

    <h2>

    <a id="user-content-files" class="anchor" href="#files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Files</h2>

    <ul>

    <li>

    <p><code>Makefile</code>: shortcut to commands to install and clean up Nextflow
    and its pipeline output</p>

    </li>

    <li>

    <p><code>main.nf</code>: Nextflow pipeline file</p>

    </li>

    <li>

    <p><code>nextflow.config</code>: config file for Nextflow pipeline (optional)</p>

    </li>

    </ul>

    <h2>

    <a id="user-content-sample-pipeline-directories" class="anchor" href="#sample-pipeline-directories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sample
    Pipeline Directories</h2>

    <p>(listed in recommended order for new users)</p>

    <ul>

    <li>

    <p><code>print-samples</code>: Prints samples from a list to the terminal</p>

    </li>

    <li>

    <p><code>make-files</code>: Creates files based on sample ID inputs</p>

    </li>

    <li>

    <p><code>output-files</code>: Same as <code>make-files</code> but includes custom
    file output options</p>

    </li>

    <li>

    <p><code>async</code>: demonstration of asynchronous process execution</p>

    </li>

    <li>

    <p><code>custom-email-output</code>: Creates files from sample ID''s then sends
    the user an email with a pipeline summary and files attached</p>

    </li>

    <li>

    <p><code>output-variable-name</code>: Same as <code>output-files</code> but includes
    inline variable definition of output file names</p>

    </li>

    <li>

    <p><code>R-Python</code>: methods for using other scripting languages inside the
    Nextflow pipeline</p>

    </li>

    <li>

    <p><code>join-pairs</code>: joining pairs of samples based on ID across input
    channels</p>

    </li>

    <li>

    <p><code>parse-samplesheet</code>: parsing of a samplesheet as input for Nextflow
    pipeline</p>

    </li>

    <li>

    <p><code>reporting</code>: execution of Nextflow pipeline with reporting and config
    features enabled.</p>

    </li>

    <li>

    <p><code>profiles-Docker-module</code>: usage of ''profiles'' to change process
    execution behavior to use Docker or environment modules</p>

    </li>

    <li>

    <p><code>Groovy-code</code>: example of using inline Groovy code inside the Nextflow
    pipeline</p>

    </li>

    </ul>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <p>You can use the following commands inside the provided demo subdirs to run
    the demo pipelines.</p>

    <h2>

    <a id="user-content-install-nextflow" class="anchor" href="#install-nextflow"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Nextflow</h2>

    <pre><code># in a subdir in this repo

    make

    </code></pre>

    <h2>

    <a id="user-content-run-pipeline" class="anchor" href="#run-pipeline" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run pipeline</h2>

    <pre><code>./nextflow run main.nf

    </code></pre>

    <p>or</p>

    <pre><code>make run

    </code></pre>

    <h2>

    <a id="user-content-cleanup" class="anchor" href="#cleanup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cleanup</h2>

    <pre><code>make clean

    </code></pre>

    <h1>

    <a id="user-content-resources" class="anchor" href="#resources" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h1>

    <ul>

    <li>

    <p>Nextflow Homepage: <a href="https://www.nextflow.io/" rel="nofollow">https://www.nextflow.io/</a></p>

    </li>

    <li>

    <p>Nextflow Docs: <a href="https://www.nextflow.io/docs/latest/getstarted.html"
    rel="nofollow">https://www.nextflow.io/docs/latest/getstarted.html</a></p>

    </li>

    <li>

    <p>Nextflow Patterns: <a href="https://nextflow-io.github.io/patterns/index.html"
    rel="nofollow">https://nextflow-io.github.io/patterns/index.html</a></p>

    </li>

    <li>

    <p>Nextflow GitHub: <a href="https://github.com/nextflow-io/nextflow">https://github.com/nextflow-io/nextflow</a></p>

    </li>

    <li>

    <p>Nextflow Google Group: <a href="https://groups.google.com/forum/#!forum/nextflow"
    rel="nofollow">https://groups.google.com/forum/#!forum/nextflow</a></p>

    </li>

    </ul>

    <h2>

    <a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Examples</h2>

    <ul>

    <li>

    <p>Nextflow tutorial: <a href="https://github.com/nextflow-io/hack17-tutorial">https://github.com/nextflow-io/hack17-tutorial</a></p>

    </li>

    <li>

    <p>Nextflow examples: <a href="https://github.com/nextflow-io/examples">https://github.com/nextflow-io/examples</a></p>

    </li>

    <li>

    <p>Pipeline examples: <a href="https://github.com/nextflow-io/awesome-nextflow">https://github.com/nextflow-io/awesome-nextflow</a></p>

    </li>

    <li>

    <p>Boilerplate example for writing pipelines: <a href="https://github.com/stevekm/nextflow-boilerplate">https://github.com/stevekm/nextflow-boilerplate</a></p>

    </li>

    <li>

    <p>NYU pipelines:</p>

    <ul>

    <li>

    <p>exome sequencing: <a href="https://github.com/NYU-Molecular-Pathology/NGS580-nf">https://github.com/NYU-Molecular-Pathology/NGS580-nf</a></p>

    </li>

    <li>

    <p>demultiplexing: <a href="https://github.com/NYU-Molecular-Pathology/demux-nf">https://github.com/NYU-Molecular-Pathology/demux-nf</a></p>

    </li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 57
  subscribers_count: 5
  topics:
  - nextflow
  updated_at: 1622124990.0
stevekm/singularity-samtools-demo:
  data_format: 2
  description: 'Singularity container for samtools '
  filenames:
  - Singularity
  - old/Singularity.v1.6
  full_name: stevekm/singularity-samtools-demo
  latest_release: null
  readme: '<h1>

    <a id="user-content-setup" class="anchor" href="#setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h1>

    <p>This assumes you are building a Singularity container locally on a Mac</p>

    <p>Make sure you''ve already installed Vagrant, since its needed to run Singularity
    on a Mac</p>

    <pre><code>brew cask install virtualbox

    brew cask install vagrant

    brew cask install vagrant-manager

    </code></pre>

    <p>If you have trouble install Vagrant with homebrew, try using <a href="https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg"
    rel="nofollow">this</a>.</p>

    <h1>

    <a id="user-content-creating-the-container" class="anchor" href="#creating-the-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Creating
    the Container</h1>

    <p>The workflow for creating a Singularity container on a Mac through Vagrant
    is saved in the included <code>Makefile</code>.</p>

    <p>Make the container by running:</p>

    <div class="highlight highlight-source-shell"><pre>make container</pre></div>

    <p>And run a test on the created container with</p>

    <div class="highlight highlight-source-shell"><pre>make <span class="pl-c1">test</span></pre></div>

    <h2>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h2>

    <p>If everything worked, the following files should be created:</p>

    <ul>

    <li>

    <p><code>singularity-vm/image/singularity-container-samtools</code>: the Singularity
    container file for samtools</p>

    </li>

    <li>

    <p><code>singularity-vm/image/samtools-version.txt</code>: the output from running
    samtools inside the container, should look like this:</p>

    </li>

    </ul>

    <pre><code>samtools 1.6

    Using htslib 1.6

    Copyright (C) 2017 Genome Research Ltd.

    </code></pre>

    <h1>

    <a id="user-content-resources" class="anchor" href="#resources" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h1>

    <p><a href="http://singularity.lbl.gov/install-mac" rel="nofollow">http://singularity.lbl.gov/install-mac</a></p>

    <p><a href="https://app.vagrantup.com/singularityware/boxes/singularity-2.4" rel="nofollow">https://app.vagrantup.com/singularityware/boxes/singularity-2.4</a></p>

    <p><a href="https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg"
    rel="nofollow">https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg</a></p>

    <p><a href="http://singularity.lbl.gov/docs-build-container" rel="nofollow">http://singularity.lbl.gov/docs-build-container</a></p>

    <p><a href="http://singularity.lbl.gov/docs-recipes" rel="nofollow">http://singularity.lbl.gov/docs-recipes</a></p>

    <p><a href="https://github.com/qbicsoftware/qbic-singularity-samtools">https://github.com/qbicsoftware/qbic-singularity-samtools</a></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - singularity
  - singularity-container
  updated_at: 1521728818.0
stevekm/vep-annotation-nf:
  data_format: 2
  description: variant annotation workflow with VEP
  filenames:
  - container/Singularity.vep-96.0
  full_name: stevekm/vep-annotation-nf
  latest_release: null
  readme: '<h1>

    <a id="user-content-vep-annotation-nf" class="anchor" href="#vep-annotation-nf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>vep-annotation-nf</h1>

    <p>Demo pipeline for annotating variants in .vcf files using <a href="https://useast.ensembl.org/info/docs/tools/vep/index.html"
    rel="nofollow">Variant Effect Predictor</a> (VEP).</p>

    <h1>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <p>Clone this repo:</p>

    <pre><code>git clone https://github.com/stevekm/vep-annotation-nf.git

    cd vep-annotation-nf

    </code></pre>

    <h2>

    <a id="user-content-nextflow" class="anchor" href="#nextflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Nextflow</h2>

    <p>Install <code>nextflow</code> in the current directory with the command in
    the Makefile.</p>

    <pre><code>make install

    </code></pre>

    <h2>

    <a id="user-content-vep-docker" class="anchor" href="#vep-docker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>VEP: Docker</h2>

    <p>To install VEP using Docker, run the Makefile command in the <code>container</code>
    directory.</p>

    <pre><code>cd container

    make docker-build

    </code></pre>

    <h2>

    <a id="user-content-vep-conda" class="anchor" href="#vep-conda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>VEP: conda</h2>

    <p>To install VEP using <code>conda</code> (for NYULMC Big Purple HPC), instead
    run the <code>conda-install</code> recipe from the Makefile in the parent repo
    directory.</p>

    <pre><code>make conda-install

    </code></pre>

    <h2>

    <a id="user-content-reference-files" class="anchor" href="#reference-files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Reference Files</h2>

    <p>VEP reference files will be downloaded automatically by the pipeline. However
    the hg19 genome fasta, fasta.fai, and fasta.dict files must also be obtained (not
    included; try <a href="https://support.illumina.com/sequencing/sequencing_software/igenome.html"
    rel="nofollow">these</a>). On NYULMC Big Purple, all required files are already
    cached and no download should be needed. On other systems, the command line arguments
    specifying the genome fasta files should be provided separately when running,
    or place the files <code>genome.fa</code>, <code>genome.fa.fai</code>, and <code>genome.dict</code>
    inside the included <code>ref</code> directory.</p>

    <h1>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h1>

    <p>The Makefile includes shortcuts to help run the pipeline easier on NYULMC Big
    Purple HPC.</p>

    <pre><code>make run

    </code></pre>

    <p>The command can also be used to run on other systems, it will simply invoke
    the command:</p>

    <pre><code>./nextflow run main.nf -resume

    </code></pre>

    <p>Nextflow <code>params</code> values can be passed on the command line:</p>

    <pre><code>./nextflow run main.nf -resume --ref_fa /path/to/genome.fa --ref_fai
    /path/to/genome.fa.fai --ref_dict /path/to/genome.dict

    </code></pre>

    <h1>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h1>

    <p>Output files will be collected in the <code>output</code> directory.</p>

    <h1>

    <a id="user-content-software" class="anchor" href="#software" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Software</h1>

    <p>Tested on RHEL 7, macOS 10.12</p>

    <ul>

    <li>

    <p>Nextflow (Java 8+)</p>

    </li>

    <li>

    <p><code>bash</code></p>

    </li>

    <li>

    <p>GNU <code>make</code></p>

    </li>

    <li>

    <p>Python 2.7+</p>

    </li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - vcf
  - annotation
  - vep
  - nextflow
  updated_at: 1617871975.0
sylabs/singularity:
  data_format: 2
  description: SingularityCE is the Community Edition of Singularity, an open source
    container platform designed to be simple, fast, and secure.
  filenames:
  - examples/legacy/2.2/arch.def
  - examples/legacy/2.2/scientific.def
  - examples/legacy/2.2/busybox.def
  - examples/legacy/2.2/ubuntu.def
  - examples/legacy/2.2/debian.def
  - examples/legacy/2.2/docker.def
  - examples/legacy/2.2/centos.def
  - examples/legacy/2.2/contrib/ubuntu-bio.def
  - examples/legacy/2.2/contrib/debian85-tensorflow-0.10.def
  - examples/legacy/2.2/contrib/linuxbrew_and_non-root_software_example.def
  - examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1.def
  - examples/legacy/2.2/contrib/ubuntu-openfoam.def
  - examples/legacy/2.2/contrib/centos7-ompi_master.def
  - examples/legacy/2.2/contrib/centos-minimal.def
  - examples/legacy/2.2/contrib/r_python_julia.def
  - examples/legacy/2.2/contrib/centos7-ompi_cuda.def
  - examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1-gpu.def
  - examples/legacy/2.2/contrib/ubuntu-root.def
  - examples/legacy/2.2/contrib/fedora.def
  - examples/legacy/2.3/contrib/raspbian.def
  - examples/build-singularity/build-singularity.def
  - e2e/testdata/inspecter_container.def
  - e2e/testdata/Docker_registry.def
  - e2e/testdata/sshfs.def
  - e2e/testdata/regressions/issue_5315.def
  - e2e/testdata/regressions/issue_4203.def
  - e2e/testdata/regressions/issue_4583.def
  - e2e/testdata/regressions/issue_4969.def
  - e2e/testdata/regressions/issue_5399.def
  - e2e/testdata/regressions/issue_5250.def
  - e2e/testdata/regressions/issue_4820.def
  - e2e/testdata/regressions/issue_4967.def
  full_name: sylabs/singularity
  latest_release: v3.8.0
  readme: '<h1>

    <a id="user-content-singularityce" class="anchor" href="#singularityce" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SingularityCE</h1>

    <p><a href="https://circleci.com/gh/sylabs/singularity/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li><a href="CONTRIBUTING.md">Guidelines for Contributing</a></li>

    <li><a href=".github/PULL_REQUEST_TEMPLATE.md">Pull Request Template</a></li>

    <li><a href="LICENSE.md">Project License</a></li>

    <li><a href="https://www.sylabs.io/docs/" rel="nofollow">Documentation</a></li>

    <li><a href="#support">Support</a></li>

    <li><a href="#citing-singularity">Citation</a></li>

    </ul>

    <p>SingularityCE is the Community Edition of Singularity, an open source container

    platform designed to be simple, fast, and secure. Singularity is optimized

    for compute focused enterprise and HPC workloads, allowing untrusted users

    to run untrusted containers in a trusted way.</p>

    <p>Check out <a href="https://www.sylabs.io/videos" rel="nofollow">talks about
    Singularity</a> and some <a href="https://sylabs.io/case-studies" rel="nofollow">use

    cases of Singularity</a> on our website.</p>

    <h2>

    <a id="user-content-getting-started-with-singularityce" class="anchor" href="#getting-started-with-singularityce"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with SingularityCE</h2>

    <p>To install SingularityCE from source, see the <a href="INSTALL.md">installation

    instructions</a>. For other installation options, see <a href="https://www.sylabs.io/guides/latest/admin-guide/"
    rel="nofollow">our

    guide</a>.</p>

    <p>System administrators can learn how to configure SingularityCE, and get an

    overview of its architecture and security features in the <a href="https://www.sylabs.io/guides/latest/admin-guide/"
    rel="nofollow">administrator

    guide</a>.</p>

    <p>For users, see the <a href="https://www.sylabs.io/guides/latest/user-guide/"
    rel="nofollow">user

    guide</a> for details on how to use

    and build Singularity containers.</p>

    <h2>

    <a id="user-content-contributing-to-singularityce" class="anchor" href="#contributing-to-singularityce"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    to SingularityCE</h2>

    <p>Community contributions are always greatly appreciated. To start developing

    SingularityCE, check out the <a href="CONTRIBUTING.md">guidelines for contributing</a>.</p>

    <p>We also welcome contributions to our <a href="https://github.com/sylabs/singularity-userdocs">user

    guide</a> and <a href="https://github.com/sylabs/singularity-admindocs">admin

    guide</a>.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>To get help with SingularityCE, check out the community spaces

    detailed at our <a href="https://www.sylabs.io/singularity/community/" rel="nofollow">Community

    Portal</a>.</p>

    <p>See also our <a href="SUPPORT.md">Support Guidelines</a> for further

    information about the best place, and how, to raise different kinds of

    issues and questions.</p>

    <p>For additional support, <a href="https://www.sylabs.io/contact/" rel="nofollow">contact
    us</a> to receive

    more information.</p>

    <h2>

    <a id="user-content-citing-singularity" class="anchor" href="#citing-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Citing
    Singularity</h2>

    <pre><code>Kurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers
    for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459

    </code></pre>

    <p>We also have a Zenodo citation:</p>

    <pre><code>Kurtzer, Gregory M. et. al. Singularity - Linux application and environment

    containers for science. 10.5281/zenodo.1310023

    </code></pre>

    <p><a href="https://doi.org/10.5281/zenodo.1310023" rel="nofollow">https://doi.org/10.5281/zenodo.1310023</a></p>

    <p>This is an ''all versions'' DOI. Follow the link to Zenodo to obtain a DOI
    specific

    to a particular version of Singularity.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p><em>Unless otherwise noted, this project is licensed under a 3-clause BSD license

    found in the <a href="LICENSE.md">license file</a>.</em></p>

    '
  stargazers_count: 33
  subscribers_count: 7
  topics:
  - containers
  - hpc
  - linux
  updated_at: 1622651677.0
sysmso/singularity-multinest:
  data_format: 2
  description: A container for PyMultinest
  filenames:
  - Singularity
  full_name: sysmso/singularity-multinest
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-multinest" class="anchor" href="#singularity-multinest"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-multinest</h1>

    <p>A container for PyMultinest</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602594100.0
tanhnhn/singularityhub-sregistry:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: tanhnhn/singularityhub-sregistry
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-registry" class="anchor" href="#singularity-registry"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Registry</h1>

    <p><a href="http://joss.theoj.org/papers/050362b7e7691d2a5d0ebed8251bc01e" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4cb65855144c475cbe5584c579404a17e3e6984f958da24427dbe46b6202eb3c/687474703a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f30353033363262376537363931643261356430656265643832353162633031652f7374617475732e737667"
    alt="status" data-canonical-src="http://joss.theoj.org/papers/050362b7e7691d2a5d0ebed8251bc01e/status.svg"
    style="max-width:100%;"></a>

    <a href="https://doi.org/10.5281/zenodo.1012531" rel="nofollow"><img src="https://camo.githubusercontent.com/411f713db9ba01edfcb60386aaa1dff3e4ed4464707b95d889900a88d8f54936/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313031323533312e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.1012531.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li><a href="https://singularityhub.github.io/sregistry" rel="nofollow">Documentation</a></li>

    </ul>

    <h2>

    <a id="user-content-what-is-singularity-registry" class="anchor" href="#what-is-singularity-registry"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What
    is Singularity Registry</h2>

    <p>Singularity Registry is a management and storage of Singularity images for
    an institution or user to deploy locally. It does not manage building, but serves
    endpoints to obtain and save containers. The Registry is expected to be available
    for use in the Fall.</p>

    <h2>

    <a id="user-content-images-included" class="anchor" href="#images-included" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Images Included</h2>

    <p>Singularity Registry consists of several Docker images, and they are integrated
    to work together using <a href="docker-compose.yml">docker-compose.yml</a>. The
    images are the following:</p>

    <ul>

    <li>

    <strong>vanessa/sregistry</strong>: is the main uwsgi application, which serves
    a Django (python-based) application.</li>

    <li>

    <strong>nginx</strong>: pronounced (engine-X) is the webserver. The starter application
    is configured for http, however you should follow the instructions to set up https
    properly.</li>

    <li>

    <strong>worker</strong>: is the same uwsgi image, but with a running command that
    is specialized to perform tasks. The tasks are run via <a href="http://www.celeryproject.org/"
    rel="nofollow">celery</a>, a distributed job queue that fits nicely into Django.
    The celery worker uses a</li>

    <li>

    <strong>redis</strong>: database to organize the jobs themselves.</li>

    </ul>

    <p>For more information about Singularity Registry, please reference the <a href="https://singularityhub.github.io/sregistry"
    rel="nofollow">docs</a>. If you have any issues, please <a href="https://github.com/singularityhub/sregistry/issues">let
    me know</a>!</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>This code is licensed under the Affero GPL, version 3.0 or later <a href="LICENSE">LICENSE</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1513562903.0
ternaustralia/coesra-singularity-canopy:
  data_format: 2
  description: null
  filenames:
  - Singularity.canopy
  full_name: ternaustralia/coesra-singularity-canopy
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-canopy" class="anchor" href="#coesra-singularity-canopy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-canopy</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610425023.0
ternaustralia/coesra-singularity-dropbox:
  data_format: 2
  description: null
  filenames:
  - Singularity.dropbox
  full_name: ternaustralia/coesra-singularity-dropbox
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-dropbox" class="anchor" href="#coesra-singularity-dropbox"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-dropbox</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1610425054.0
ternaustralia/coesra-singularity-jupyter:
  data_format: 2
  description: null
  filenames:
  - Singularity.jupyter
  full_name: ternaustralia/coesra-singularity-jupyter
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-jupyter" class="anchor" href="#coesra-singularity-jupyter"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-jupyter</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610425229.0
ternaustralia/coesra-singularity-kepler:
  data_format: 2
  description: null
  filenames:
  - Singularity.kepler
  full_name: ternaustralia/coesra-singularity-kepler
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-kepler" class="anchor" href="#coesra-singularity-kepler"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-kepler</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610425796.0
ternaustralia/coesra-singularity-knime:
  data_format: 2
  description: Knime
  filenames:
  - Singularity.knime
  full_name: ternaustralia/coesra-singularity-knime
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-knime" class="anchor" href="#coesra-singularity-knime"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-knime</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610426074.0
ternaustralia/coesra-singularity-macroecodesktop:
  data_format: 2
  description: null
  filenames:
  - Singularity.macroecodesktop
  full_name: ternaustralia/coesra-singularity-macroecodesktop
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-openrefine" class="anchor" href="#coesra-singularity-openrefine"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-openrefine</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610426323.0
ternaustralia/coesra-singularity-openrefine:
  data_format: 2
  description: null
  filenames:
  - Singularity.openrefine
  full_name: ternaustralia/coesra-singularity-openrefine
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-openrefine" class="anchor" href="#coesra-singularity-openrefine"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-openrefine</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610426463.0
ternaustralia/coesra-singularity-owncloud:
  data_format: 2
  description: Owncloud
  filenames:
  - Singularity.owncloud
  full_name: ternaustralia/coesra-singularity-owncloud
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-owncloud" class="anchor" href="#coesra-singularity-owncloud"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-owncloud</h1>

    <p>Author: Hoang Nguyen

    Created: 22 July 2019

    This will create a image with Singularity 2.5.1</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610426521.0
ternaustralia/coesra-singularity-panoply:
  data_format: 2
  description: null
  filenames:
  - Singularity.panoply
  full_name: ternaustralia/coesra-singularity-panoply
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-panoply" class="anchor" href="#coesra-singularity-panoply"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-panoply</h1>

    <p>Hoang Nguyen

    25 July 2019</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610426866.0
ternaustralia/coesra-singularity-qgis:
  data_format: 2
  description: null
  filenames:
  - Singularity.qgis
  full_name: ternaustralia/coesra-singularity-qgis
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-qgis" class="anchor" href="#coesra-singularity-qgis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-qgis</h1>

    <p>Hoang Nguyen 24 July 2019</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610427940.0
ternaustralia/coesra-singularity-rstudio:
  data_format: 2
  description: null
  filenames:
  - Singularity.rstudio
  full_name: ternaustralia/coesra-singularity-rstudio
  latest_release: null
  readme: '<h1>

    <a id="user-content-coesra-singularity-rstudio" class="anchor" href="#coesra-singularity-rstudio"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>coesra-singularity-rstudio</h1>

    <p>Hoang Nguyen

    25 July 2019</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - coesra
  updated_at: 1610424737.0
tgac-vumc/BLADE:
  data_format: 2
  description: 'BLADE: Bayesian Log-normAl DEconvolution for enhanced in silico microdissection
    of bulk gene expression data'
  filenames:
  - Singularity
  full_name: tgac-vumc/BLADE
  latest_release: null
  readme: "<p align=\"center\">\n  <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/logo_final_small.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"254\" height=\"281\"\
    \ src=\"https://github.com/tgac-vumc/BLADE/raw/master/logo_final_small.png\" style=\"\
    max-width:100%;\"></a>\n</p>\n<h1>\n<a id=\"user-content-blade-bayesian-log-normal-deconvolution\"\
    \ class=\"anchor\" href=\"#blade-bayesian-log-normal-deconvolution\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>BLADE:\
    \ Bayesian Log-normAl DEconvolution</h1>\n<p><a href=\"https://www.python.org/downloads/release/python-360/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8e26ba5220a7019a30342315ff5cc4989f91e698fdfe73a41476dd57524385d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e737667\"\
    \ alt=\"Python 3.6\" data-canonical-src=\"https://img.shields.io/badge/python-3.6-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://badge.fury.io/py/BLADE-Deconvolution\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/09a5fb429236fe0ed8858f2cd7a0d424aa6a58af4fa0bdfda86aa97e8409c118/68747470733a2f2f62616467652e667572792e696f2f70792f424c4144452d4465636f6e766f6c7574696f6e2e737667\"\
    \ alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/BLADE-Deconvolution.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/4861\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://mybinder.org/v2/gh/tgac-vumc/BLADE/master\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\"\
    \ alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"\
    max-width:100%;\"></a></p>\n<p>BLADE (Bayesian Log-normAl DEconvolution) was designed\
    \ to jointly estimate cell type composition and gene expression profiles per cell\
    \ type in a single-step while accounting for the observed gene expression variability\
    \ in single-cell RNA-seq data.</p>\n<p align=\"center\">\n  <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/framework.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"100%\" height=\"\
    100%\" src=\"https://github.com/tgac-vumc/BLADE/raw/master/framework.png\" style=\"\
    max-width:100%;\"></a>\n</p>\n<p>BLADE framework. To construct a prior knowledge\
    \ of BLADE, we used single-cell sequencing data. Cell are subject to phenotyping,\
    \ clustering and differential gene expression analysis. Then, for each cell type,\
    \ we retrieve average expression profiles (red cross and top heatmap), and standard\
    \ deviation per gene (blue circle and bottom heatmap). This prior knowledge is\
    \ then used in the hierarchical Bayesian model (bottom right) to deconvolute bulk\
    \ gene expression data.</p>\n<h4>\n<a id=\"user-content-demo-notebook-is-available-here-you-can-also-run-the-demo-using-binder\"\
    \ class=\"anchor\" href=\"#demo-notebook-is-available-here-you-can-also-run-the-demo-using-binder\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Demo notebook is available <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/jupyter/BLADE%20-%20Demo%20script.ipynb\"\
    >here</a>. You can also run the demo using <a href=\"https://mybinder.org/v2/gh/tgac-vumc/BLADE/master\"\
    \ rel=\"nofollow\">Binder</a>.</h4>\n<p>Note that for the testing on Binder, parallel\
    \ processing has to be disabled by setting <code>Njob</code> to 1. BLADE significantly\
    \ performs better with high number of cores, epecially when <code>Nsample</code>,\
    \ <code>Ngene</code> and <code>Ncell</code> is high. In case of Binder, we recommend\
    \ the following setting:</p>\n<ul>\n<li><code>Ncell=3</code></li>\n<li><code>Ngene=50</code></li>\n\
    <li><code>Nsample=10</code></li>\n</ul>\n<p>It takes about 30 minutes to complete\
    \ the demo execution on Binder.</p>\n<h2>\n<a id=\"user-content-system-requirements\"\
    \ class=\"anchor\" href=\"#system-requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>System Requirements</h2>\n<h3>\n\
    <a id=\"user-content-hardware-requirements\" class=\"anchor\" href=\"#hardware-requirements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Hardware Requirements</h3>\n<p>BLADE can run on the minimal computer\
    \ spec, such as Binder (1 CPU, 2GB RAM on Google Cloud), when data size is small.\
    \ However, BLADE can significantly benefit from the larger amount of CPUs and\
    \ RAM. Empirical Bayes procedure of BLADE runs independent optimization procedure\
    \ that can be parallelized. In our evaluation, we used a computing node with the\
    \ following spec:</p>\n<ul>\n<li>40 threads (Xeon 2.60GHz)</li>\n<li>128 GB RAM</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-os-requirements\" class=\"anchor\" href=\"#os-requirements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>OS Requirements</h3>\n<p>The package development version is tested\
    \ on Linux operating systems. (CentOS 7 and Ubuntu 16.04).</p>\n<h2>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <h3>\n<a id=\"user-content-using-pip\" class=\"anchor\" href=\"#using-pip\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using\
    \ pip</h3>\n<p>The python package of BLADE is available on pip.\nYou can simply\
    \ (takes only &lt;1min):</p>\n<pre><code>pip install BLADE_Deconvolution\n</code></pre>\n\
    <p>We tested BLADE with <code>python =&gt; 3.6</code>.</p>\n<h3>\n<a id=\"user-content-using-conda\"\
    \ class=\"anchor\" href=\"#using-conda\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Conda</h3>\n<p>One can\
    \ create a conda environment contains BLADE and also other dependencies to run\
    \ <a href=\"https://github.com/tgac-vumc/BLADE/blob/master/jupyter/BLADE%20-%20Demo%20script.ipynb\"\
    >Demo</a>.\nThe environment definition is in <a href=\"https://github.com/tgac-vumc/BLADE/environment.yml\"\
    >environment.yml</a>.</p>\n<h3>\n<a id=\"user-content-step-1-installing-miniconda-3\"\
    \ class=\"anchor\" href=\"#step-1-installing-miniconda-3\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1:\
    \ Installing Miniconda 3</h3>\n<p>First, please open a terminal or make sure you\
    \ are logged into your Linux VM. Assuming that you have a 64-bit system, on Linux,\
    \ download and install Miniconda 3 with:</p>\n<pre><code>wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
    bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>\n<p>On MacOS X, download\
    \ and install with:</p>\n<pre><code>curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\
    \ -o Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\
    </code></pre>\n<h3>\n<a id=\"user-content-step-2-create-a-conda-environment\"\
    \ class=\"anchor\" href=\"#step-2-create-a-conda-environment\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 2: Create a conda environment</h3>\n<p>You can install all the necessary dependency\
    \ using the following command (may takes few minutes).</p>\n<pre><code>conda env\
    \ create --file environment.yml\n</code></pre>\n<p>Then, the <code>BLADE</code>\
    \ environment can be activate by:</p>\n<pre><code>conda activate BLADE\n</code></pre>\n\
    <h3>\n<a id=\"user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Singularity</h3>\n<p>If you have Singularity, you can simply\
    \ pull the singularity container with all dependency resolved (in few minutes,\
    \ depends on the network speed).</p>\n<pre><code>singularity pull shub://tgac-vumc/BLADE\n\
    </code></pre>\n<h2>\n<a id=\"user-content-overview-of-blade\" class=\"anchor\"\
    \ href=\"#overview-of-blade\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Overview of BLADE</h2>\n<p>In the\
    \ BLADE package, you can load the following functions and modules.</p>\n<ul>\n\
    <li>\n<p><code>BLADE</code>: A class object contains core algorithms of <code>BLADE</code>.\
    \ Users can reach internal variables (<code>Nu</code>, <code>Omega</code>, and\
    \ <code>Beta</code>) and functions for calculating objective functions (ELBO function)\
    \ and gradients with respect to the variational parameters. There also is an optimization\
    \ function (<code>BLADE.Optimize()</code>) for performing L-BFGS optimization.\
    \ Though this is the core, we also provide a more accessible function (<code>BLADE_framework</code>)\
    \ that performs deconvolution. See below to obtain the current estimate of cellualr\
    \ fractions, gene expression profiles per cell type and per sample:</p>\n<ul>\n\
    <li>\n<code>ExpF(self.Beta)</code> : returns a <code>Nsample</code> by <code>Ngene</code>\
    \ matrix contains estimated fraction of each cell type in each sample.</li>\n\
    <li>\n<code>self.Nu</code>: a <code>Nsample</code> by <code>Ngene</code> by <code>Ncell</code>\
    \ multidimensional array contains estimated gene expression levels of each gene\
    \ in each cell type for each sample.</li>\n<li>\n<code>numpy.mean(self.Nu,0)</code>:\
    \ To obtain a estimated gene expression profile per cell type, we can simply take\
    \ an average across the samples.</li>\n</ul>\n</li>\n<li>\n<p><code>Framework</code>:\
    \ A framework based on the <code>BLADE</code> class module above. Users need to\
    \ provide the following input/output arguments.</p>\n<ul>\n<li>Input arguments\n\
    <ul>\n<li>\n<code>X</code>: a <code>Ngene</code> by <code>Ncell</code> matrix\
    \ contains average gene expression profiles per cell type (a signature matrix)\
    \ in log-scale.</li>\n<li>\n<code>stdX</code>: a <code>Ngene</code> by <code>Ncell</code>\
    \ matrix contains standard deviation per gene per cell type (a signature matrix\
    \ of gene expression variability).</li>\n<li>\n<code>Y</code>: a <code>Ngene</code>\
    \ by <code>Nsample</code> matrix contains bulk gene expression data. This should\
    \ be in linear-scale data without log-transformation.</li>\n<li>\n<code>Ind_Marker</code>:\
    \ Index for marker genes. By default, <code>[True]*Ngene</code> (all genes used\
    \ without filtering). For the genes with <code>False</code> they are excluded\
    \ in the first phase (Empirical Bayes) for finidng the best hyperparameters.</li>\n\
    <li>\n<code>Ind_sample</code>: Index for the samples used in the first phase (Empirical\
    \ Bayes). By default, <code>[True]*Nsample</code> (all samples used).</li>\n<li>\n\
    <code>Alphas</code>, <code>Alpha0s</code>, <code>Kappa0s</code> and <code>SYs</code>:\
    \ all possible hyperparameters considered in the phase of Empirical Bayes. A default\
    \ parameters are offered as described in the manuscript (to appear): <code>Alphas=[1,10]</code>,\
    \ <code>Alpha0s=[0.1, 1, 5]</code>, <code>Kappa0s=[1,0.5,0.1]</code> and <code>SYs=[1,0.3,0.5]</code>.</li>\n\
    <li>\n<code>Nrep</code>: Number of repeat for evaluating each parameter configuration\
    \ in Empirical Bayes phase. By default, <code>Nrep=3</code>.</li>\n<li>\n<code>Nrepfinal</code>:\
    \ Number of repeated optimizations for the final parameter set. By default, <code>Nrepfinal=10</code>.</li>\n\
    <li>\n<code>Njob</code>: Number of jobs executed in parallel. By default, <code>Njob=10</code>.</li>\n\
    </ul>\n</li>\n<li>Output values\n<ul>\n<li>\n<code>final_obj</code>: A final <code>BLADE</code>\
    \ object with optimized variational parameters and hyperparameters.</li>\n<li>\n\
    <code>best_obj</code>: The best object form Empirical Bayes step. If no genes\
    \ and samples are filtered, <code>best_obj</code> is the same as <code>final_obj</code>.</li>\n\
    <li>\n<code>best_set</code>: A list contains the hyperparameters selected in the\
    \ Empirical Bayes step.</li>\n<li>\n<code>All_out</code>: A list of <code>BLADE</code>\
    \ objects from the Empirical Bayes step.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n\
    <p><code>BLADE_job</code>/<code>Optimize</code>: Internal functions used by <code>Framework</code>.</p>\n\
    </li>\n</ul>\n"
  stargazers_count: 4
  subscribers_count: 7
  topics: []
  updated_at: 1620139641.0
tgac-vumc/QDNAseq.snakemake:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: tgac-vumc/QDNAseq.snakemake
  latest_release: null
  readme: "<p align=\"center\">\n  <a href=\"https://github.com/tgac-vumc/QDNAseq.snakemake/blob/master/DAG_all.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"100%\" height=\"\
    100%\" src=\"https://github.com/tgac-vumc/QDNAseq.snakemake/raw/master/DAG_all.svg\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>For the\
    \ installation of this pipeline any Python install compatable Conda is required.</p>\n\
    <p>The pipeline itself will run on Python 3.8.5 and R 3.6.3. For exact dependencies\
    \ view <code>environment.yaml</code> and <code>r-dependencies.R</code>.</p>\n\
    <h3>\n<a id=\"user-content-using-condamamba\" class=\"anchor\" href=\"#using-condamamba\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Conda/Mamba</h3>\n<p>for easy installation you need (Mini)Conda.</p>\n\
    <p>Miniconda installation from folder where you want to install Miniconda:</p>\n\
    <pre><code>cd &lt;/path/to/files/dir/&gt;\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
    bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>\n<p>follow the instructions\
    \ of the installation process, give the location where you want Miniconda to be\
    \ installed and answer YES to add Miniconda to your path.</p>\n<p>go to the directory\
    \ where the analysis need to be performed</p>\n<pre><code>cd &lt;/path/to/analysis/dir&gt;\n\
    git clone https://github.com/tgac-vumc/QDNAseq.snakemake/\ncd QDNAseq.snakemake\n\
    </code></pre>\n<p>install Mamba as drop-in replacement for Conda with Mamba's\
    \ improved installation-performance:</p>\n<pre><code>conda install -c conda-forge\
    \ mamba\n</code></pre>\n<p>create  the environment using Mamba:</p>\n<pre><code>mamba\
    \ env create --name QDNAseq-snakemake --file environment.yaml\n</code></pre>\n\
    <p>activate the environment by:</p>\n<pre><code>conda activate QDNAseq-snakemake\n\
    </code></pre>\n\n<p>Then run the R-script r-dependencies.R in the terminal to\
    \ install the non-conda R dependencies in the environment:</p>\n<pre><code>Rscript\
    \ r-dependencies.R\n</code></pre>\n<h3>\n<a id=\"user-content-using-singularity\"\
    \ class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Using Singularity</h3>\n<p>Under\
    \ development</p>\n\n<h2>\n<a id=\"user-content-preparing-analysis\" class=\"\
    anchor\" href=\"#preparing-analysis\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Preparing analysis</h2>\n<h3>\n\
    <a id=\"user-content-prepare-the-data\" class=\"anchor\" href=\"#prepare-the-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prepare the data</h3>\n<p>go to analysis dir and prepare analysis\
    \ by copy or create links to fastq.gz files:</p>\n<pre><code>cd &lt;/path/to/analysis/dir&gt;\n\
    \nmkdir fastq\ncd fastq\n</code></pre>\n<p>to link a single file:</p>\n<pre><code>ln\
    \ -s &lt;path/to/file&gt;\n</code></pre>\n<p>to link all files from a folder:</p>\n\
    <pre><code>for file in &lt;path/to/fastq/files&gt;/*.fastq.gz\ndo ln -s $file\n\
    done\n</code></pre>\n<h3>\n<a id=\"user-content-prepare-the-snakemake-settings\"\
    \ class=\"anchor\" href=\"#prepare-the-snakemake-settings\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Prepare\
    \ the snakemake settings</h3>\n<p>Open the configuration file <code>config.yaml</code>\
    \ to check the settings that snakemake will use and change according to your needs.\n\
    For providing service-analysis, set <code>setting</code> to <code>'service'</code>.\
    \ For research purposes, set <code>setting</code> to <code>'research'</code>.\
    \ For all settings set <code>setting</code> to <code>'all'</code>.</p>\n<p>One\
    \ of the options in the configfile is <code>dewaving</code>, if set to <code>'true'</code>\
    \ QNDAseq objects will be dewaved before segmentation.</p>\n<p>These options change\
    \ the rules performed in the pipeline, see the rule-graph in the next section.</p>\n\
    <h2>\n<a id=\"user-content-running-analysis\" class=\"anchor\" href=\"#running-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running analysis</h2>\n<p>Make sure that snakemake is able to find\
    \ the excecutive file Snakefile by performing a dry-run:</p>\n<pre><code>cd ../QDNAseq.snakemake\n\
    snakemake -n\n</code></pre>\n<p>Check the rules that are planned to be performed,\
    \ conform the rule-graph.</p>\n<p>An visualization of the order of rules to be\
    \ performed can be viewed by running the following command and opening the DAG-file</p>\n\
    <pre><code>snakemake --forceall --rulegraph | dot -Tsvg &gt; DAG.svg\n</code></pre>\n\
    <p>Rulegraphs for the intial settings <code>'service'</code>, <code>'research'</code>\
    \ and <code>'all'</code> are commited to this repro in the files <code>DAG_&lt;setting&gt;.svg</code>.</p>\n\
    <p>When ready, run the analysis</p>\n<pre><code>snakemake\n</code></pre>\n<p>Useful\
    \ snakemake options</p>\n<p><code>-j , --cores, --jobs</code> : Use at most N\
    \ cores in parallel (default: 1). If N is omitted, the limit is set to the number\
    \ of available cores.</p>\n<p><code>-n , --dryrun</code> : Do not execute anything.\
    \ but show rules which are planned to be performed.</p>\n<p><code>-k , --keep-going</code>\
    \ : Go on with independent jobs if a job fails.</p>\n<p><code>-f , --force</code>\
    \ : Force the execution of the selected target or the first rule regardless of\
    \ already created output.</p>\n<p><code>-R , --forcerun</code> : Force the re-execution\
    \ or creation of the given rules or files. Use this option if you changed a rule\
    \ and want to have all its output in your workflow updated.</p>\n<p><code>-U ,\
    \ --until</code> : Runs the pipeline until it reaches the specified rules or files.\
    \ Only runs jobs that are dependencies of the specified rule or files, does not\
    \ run sibling DAGs.</p>\n<p>for all options go to <a href=\"https://snakemake.readthedocs.io/en/v5.31.1/executing/cli.html#all-options\"\
    \ rel=\"nofollow\">https://snakemake.readthedocs.io/en/v5.31.1/executing/cli.html#all-options</a></p>\n"
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1617979919.0
tgac-vumc/RNA-seq:
  data_format: 2
  description: RNA-seq analysis pipeline based on Snakemake
  filenames:
  - Singularity
  full_name: tgac-vumc/RNA-seq
  latest_release: v1.0.0
  readme: "<h1>\n<a id=\"user-content-rna-seq-analysis-pipeline\" class=\"anchor\"\
    \ href=\"#rna-seq-analysis-pipeline\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>RNA-seq analysis pipeline</h1>\n\
    <p><a href=\"https://snakemake.bitbucket.io\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9e0a726dc69516d51067fd9fc2074a9f2dc9d44eb069ae05434a36f580af32f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b653d3d352e32352e302d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265\"\
    \ alt=\"Snakemake\" data-canonical-src=\"https://img.shields.io/badge/snakemake==5.25.0-brightgreen.svg?style=flat-square\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://singularity-hub.org/collections/3066\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c9d2afb620129b7ba0f4d918b77bfdb2b91c595cd6c6d013e950ee6e3c2bbc55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d73696e67756c61726974792d2d6875622d7265642e737667\"\
    \ alt=\"singularity-hub\" data-canonical-src=\"https://img.shields.io/badge/install%20with-singularity--hub-red.svg\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://docs.conda.io/en/latest/miniconda.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e225eb3891735f81d51e8e6aa377429328cfd43656973ff807bffe9234bc28c7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d636f6e64612d677265656e2e737667\"\
    \ alt=\"miniconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-conda-green.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>This is a <a href=\"https://snakemake.readthedocs.io/en/stable/\"\
    \ rel=\"nofollow\">Snakemake</a> based pipeline for RNA-seq used in the <a href=\"\
    http://www.tgac.nl/\" rel=\"nofollow\">Tumor Genome Core Analysis</a> housed in\
    \ the <a href=\"https://www.vumc.com/departments/cancer-center-amsterdam.htm\"\
    \ rel=\"nofollow\">Cancer Center Amsterdam</a>, at <a href=\"https://www.vumc.nl/\"\
    \ rel=\"nofollow\">Amsterdam UMC location VUmc</a> and part of the Department\
    \ of Pathology.</p>\n<p>The pipeline processes raw data from FastQ inputs (<a\
    \ href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"nofollow\"\
    >FastQC</a>, <a href=\"http://www.usadellab.org/cms/?page=trimmomatic\" rel=\"\
    nofollow\">Trimmomatic</a>), aligns the reads (<a href=\"https://github.com/alexdobin/STAR\"\
    >STAR</a>), generates gene counts (<a href=\"http://bioinf.wehi.edu.au/featureCounts/\"\
    \ rel=\"nofollow\">featureCounts</a>) and performs quality-control on the results\
    \ (<a href=\"https://multiqc.info/\" rel=\"nofollow\">MultiQC</a>). Paired-end\
    \ (PE) and single read (SR) are supported.</p>\n<p align=\"center\">\n  <a href=\"\
    https://github.com/tgac-vumc/RNA-seq/blob/master/DAG_RNAseq.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img width=\"850\" height=\"483\" src=\"https://github.com/tgac-vumc/RNA-seq/raw/master/DAG_RNAseq.png\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>The pipeline\
    \ is preliminary used in linux environment with conda/singularity available.</p>\n\
    <h3>\n<a id=\"user-content-using-conda\" class=\"anchor\" href=\"#using-conda\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Conda</h3>\n<h3>\n<a id=\"user-content-step-1-installing-miniconda-3\"\
    \ class=\"anchor\" href=\"#step-1-installing-miniconda-3\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 1:\
    \ Installing Miniconda 3</h3>\n<p>First, please open a terminal or make sure you\
    \ are logged into your Linux VM. Assuming that you have a 64-bit system, on Linux,\
    \ download and install Miniconda 3 with:</p>\n<pre><code>wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\
    bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>\n<p>On MacOS X, download\
    \ and install with:</p>\n<pre><code>curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\
    \ -o Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\
    </code></pre>\n<h3>\n<a id=\"user-content-step-2-downloading-repository--creating-environment\"\
    \ class=\"anchor\" href=\"#step-2-downloading-repository--creating-environment\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2: Downloading repository &amp; creating environment</h3>\n<pre><code>mkdir\
    \ snakemake_RNAseq\ncd snakemake_RNAseq\ngit clone https://github.com/tgac-vumc/RNA-seq\n\
    conda env create --name RNAseq --file env.yaml\n</code></pre>\n<h3>\n<a id=\"\
    user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using Singularity</h3>\n<p>The singularity container holds a virtual\
    \ environment of CentOS 7 and it's available with:</p>\n<pre><code>singularity\
    \ pull shub://tgac-vumc/RNA-seq\n</code></pre>\n<h2>\n<a id=\"user-content-path-configuration--running-the-pipeline\"\
    \ class=\"anchor\" href=\"#path-configuration--running-the-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Path\
    \ Configuration &amp; Running the pipeline</h2>\n<p>Before attempting to run the\
    \ pipeline, please open <em>config.yaml</em>. Inside, you will encounter <strong>Path\
    \ Configuration</strong> and <strong>Software Options</strong>.</p>\n<ol>\n<li>On\
    \ <strong>Path configuration</strong>, first, you have to choose whether your\
    \ data is PE or SR and after change the fastq path to the path where your fastq\
    \ files are actually stored.</li>\n<li>On <strong>Software Options</strong>, you\
    \ will find several options that can be modified by the user. Please, have a look\
    \ at it before running the pipeline.</li>\n</ol>\n<p>All the software used in\
    \ the pipeline is installed by conda or executed in a wrapper. We recommend to\
    \ run the pipeline from a different location than the pipeline path, like the\
    \ example below:</p>\n<pre><code>snakemake -s PATH_TO_PIPELINE/Snakefile --use-conda\
    \ --cores=24\n</code></pre>\n<p>With --use-conda option, the pipeline will create\
    \ environments to run rules based on <em>env.yaml</em>.\n<strong>Note</strong>\
    \ the pipeline assumes that <em>config.yaml</em> is available at the location\
    \ where the pipeline is executed.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1604920129.0
thehyve/singularity-jupyter:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: thehyve/singularity-jupyter
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-jupyter" class="anchor" href="#singularity-jupyter"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-jupyter</h1>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1611165393.0
thomas-robinson/fms_containers:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity.FMS-gcc10-openmpi-netcdf4.6.3-ubuntu-compile
  - Singularity.FMS-gcc10-openmpi-netcdf4.6.3-ubuntu
  full_name: thomas-robinson/fms_containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-fms_containers" class="anchor" href="#fms_containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>fms_containers</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604411747.0
tpall/geo-htseq-paper:
  data_format: 2
  description: Code used to generate summaries, models and figures for article "A
    field-wide assessment of differential high throughput sequencing reveals widespread
    bias".
  filenames:
  - Singularity
  full_name: tpall/geo-htseq-paper
  latest_release: null
  readme: '<p><a href="https://github.com/tpall/geo-htseq-paper/workflows/CI/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/tpall/geo-htseq-paper/workflows/CI/badge.svg"
    alt="CI" style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-geo-htseq-paper" class="anchor" href="#geo-htseq-paper" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Geo-htseq-paper</h1>

    <p>We analyzed the field of expression profiling by high throughput sequencing,
    or RNA-seq, in terms of replicability and reproducibility, using data from the
    GEO (Gene Expression Omnibus) repository. Our work puts an upper bound of 56%
    to field-wide reproducibility, based on the types of files submitted to GEO.</p>

    <h2>

    <a id="user-content-getting-data" class="anchor" href="#getting-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting data</h2>

    <p>Got to <a href="https://zenodo.org/record/3754095" rel="nofollow">https://zenodo.org/record/3754095</a>
    and download data archive, let''s say, to your Downloads folder.</p>

    <p>Then create new folder, e.g. "geo-htseq" and enter this folder</p>

    <div class="highlight highlight-source-shell"><pre>mkdir geo-htseq

    <span class="pl-c1">cd</span> geo-htseq</pre></div>

    <p>Copy downloaded dataset to your working directory and uncompress:</p>

    <div class="highlight highlight-source-shell"><pre>cp <span class="pl-k">~</span>/Downloads/geo-htseq-until-2019-12-31.tar.gz
    <span class="pl-c1">.</span>

    tar -xzvf geo-htseq-until-2019-12-31.tar.gz</pre></div>

    <p>Remove tar.gz archive from working directory:</p>

    <div class="highlight highlight-source-shell"><pre>rm geo-htseq-until-2019-12-31.tar.gz</pre></div>

    <p>Now you should have dataset in "output" subdirectory ready for analysis.</p>

    <h2>

    <a id="user-content-workflow-graph" class="anchor" href="#workflow-graph" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Workflow graph</h2>

    <p><a href="images/rulegraph.svg" target="_blank" rel="noopener noreferrer"><img
    src="images/rulegraph.svg" alt="rulegraph" style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619076381.0
tpall/htseq-paper-singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: tpall/htseq-paper-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-htseq-paper-singularity" class="anchor" href="#htseq-paper-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>htseq-paper-singularity</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604657436.0
tpall/singularity-stan:
  data_format: 2
  description: null
  filenames:
  - Singularity.3.6.3
  - Singularity
  full_name: tpall/singularity-stan
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7_aci" class="anchor" href="#centos7_aci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos7_aci</h1>

    <p>Centos 7 base image for ACI Singualarity recipe<br>

    This recipe may include unnecessary packages for certain software installation.<br>

    Size of CPU-only container: ~1 GB<br>

    Size of GPU container: ~2.6 GB</p>

    <p>More packages will be added in the future</p>

    <p>2019/2/17

    <strong>Centos 7</strong> with <strong>GCC 8</strong><br>

    To enable GCC 8,</p>

    <pre><code>&gt; source /opt/rh/devtoolset-8/enable

    </code></pre>

    <p>2019/3/1<br>

    OpenMPI is added to <code>$PATH</code></p>

    <p>2019/3/11<br>

    OpenMPI is updated to version 2.1.6</p>

    <p>2019/4/12<br>

    Boost 1.70.0 in added</p>

    <p>2019/7/19<br>

    <del>Python 2 and 3 are updated to version 2.7.16 and version 3.7.4</del><br>

    OpenMPI is updated to version 4.0.1</p>

    <p>2019/7/21<br>

    <del>Few Python packages are added</del></p>

    <p>2019/7/22<br>

    <del>Few corrections are made including Python</del></p>

    <p>2019/7/23<br>

    Pythons are replaced with packages<br>

    To enable Python 2.7.16,</p>

    <pre><code>&gt; source /opt/rh/python27/enable

    </code></pre>

    <p>System version of python is 3.6.8</p>

    <p>2019/7/30<br>

    devtoolset-7 GCC is added (some software can''t be built with GCC 8)</p>

    <p>2019/11/9<br>

    CMake 3.15.5 is added</p>

    <p>2019/11/22<br>

    OpenMPI is downgraded to 1.10.1 to match version on ACI</p>

    <p>2020/2/12<br>

    Boost is upgraded to 1.72.0 and CMake is upgraded to 3.16.4</p>

    <p>2020/3/2<br>

    GPU version is added</p>

    <p>2020/9/21<br>

    Minor updates are made (regarding libxkb)</p>

    <p>2020/9/28<br>

    Recipe for CUDA 9.1 is added (for FSL with CUDA)</p>

    <p>2020/10/11<br>

    Boost is upgraded to 1.74.0 and CMake is upgraded to 3.18.4<br>

    R 4.0.3 is added (Curl 7.72.0 and XZ 5.2.5 are added for R)<br>

    VirtualGL is downgraded to 2.5.2 to match system version</p>

    <p>2020/10/18<br>

    UDUNITS 2.2.26 is added</p>

    <p>2020/10/20<br>

    Tix-devel, Tx-devel, TkInter-devel, LAPACK-devel, and BLAS-devel are added</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1603529584.0
tpall/singularity-tidyverse:
  data_format: 2
  description: Singularity image running R tidyverse + some other libraries
  filenames:
  - Singularity.3.6.3
  - Singularity
  full_name: tpall/singularity-tidyverse
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/2366" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-singularity-tidyverse" class="anchor" href="#singularity-tidyverse"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    tidyverse</h2>

    <p>This will run R tidyverse + some other packages, like <em>here</em>, <em>readxl</em>,
    <em>lubridate</em>, <em>bookdown</em>, etc.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1608284812.0
uri-ai-lab/singularity-images:
  data_format: 2
  description: Setups for various images used on the dgx.
  filenames:
  - Singularity-PyTorch
  full_name: uri-ai-lab/singularity-images
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-images" class="anchor" href="#singularity-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-images</h1>

    <p>Setups for various images used on the dgx.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1573772605.0
vibaotram/singularity-container:
  data_format: 2
  description: null
  filenames:
  - Singularity.guppy3.6.0gpu-conda-api
  - Singularity.guppy4.2.2gpu-conda-api
  - Singularity.cpu-guppy3.4-conda-api
  - Singularity.guppy4.0.14gpu-conda-api
  - Singularity.myR_4-0-2_rstudio_1.3
  - Singularity.guppy-cpu-conda
  - Singularity.deepbinner-api
  - Singularity.guppy3.6.0cpu-conda-api
  - Singularity.guppy3.4gpu-conda-api
  - Singularity.guppy4.5.4gpu-conda-api
  - Singularity.myR_3-6-3
  full_name: vibaotram/singularity-container
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4054" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-singularity-container" class="anchor" href="#singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-container</h1>

    <h2>

    <a id="user-content-singularity-images-supporting-basedmux-workflow" class="anchor"
    href="#singularity-images-supporting-basedmux-workflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity images
    supporting <a href="https://github.com/vibaotram/baseDmux.git">baseDmux workflow</a>

    </h2>

    <p><strong>Singularity.guppy-cpu-conda</strong></p>

    <p>containing GUPPY version 3.4 CPU, Miniconda3</p>

    <pre><code>shub://vibaotram/singularity-container:guppy-cpu-conda

    </code></pre>

    <p><strong>Singularity.cpu-guppy3.4-conda-api</strong></p>

    <p>containing GUPPY version 3.4 CPU, Miniconda3, ONT_FAST5_API</p>

    <pre><code>shub://vibaotram/singularity-container:cpu-guppy3.4-conda-api

    </code></pre>

    <p><strong>Singularity.guppy3.4gpu-conda-api</strong></p>

    <p>containing GUPPY version 3.4 GPU, Miniconda3, ONT_FAST5_API</p>

    <pre><code>shub://vibaotram/singularity-container:guppy3.4gpu-conda-api

    </code></pre>

    <p><strong>Singularity.deepbinner-api</strong></p>

    <p>containing deepbinner 2.0.0, ONT_FAST5_API, python3</p>

    <pre><code>shub://vibaotram/singularity-container:deepbinner-api

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics:
  - singularity
  updated_at: 1621378901.0
vincent-noel/pc4ecm:
  data_format: 2
  description: PhysiCell Invasion Model
  filenames:
  - src/addons/PhysiBoSSa/MaBoSS-env-2.0/containers/singularity/Singularity
  full_name: vincent-noel/pc4ecm
  latest_release: null
  readme: '<h1>

    <a id="user-content-folding-at-home-gpu" class="anchor" href="#folding-at-home-gpu"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>folding-at-home-gpu</h1>

    <p>gpu image for folding at home</p>

    <p>simple merge of nvidia cl image with folding at home v7.5.1 to enable gpu processing.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1588946949.0
vivekkatial/qaoa-three-sat:
  data_format: 2
  description: An implementation for solving 3SAT (Exact Cover) using the Quantum
    Approximate Optimization Algorithm
  filenames:
  - SingularityFile.def
  full_name: vivekkatial/qaoa-three-sat
  latest_release: null
  readme: '<h1>

    <a id="user-content-qaoa-3sat--" class="anchor" href="#qaoa-3sat--" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>QAOA 3SAT <a href="https://camo.githubusercontent.com/4beb7225857c50a9391b71fbe998bc23c33b4d87ee15e3da9b7c1b7dfdc67a11/68747470733a2f2f7472617669732d63692e636f6d2f766976656b6b617469616c2f71616f612d74687265652d7361742e7376673f6272616e63683d6d6173746572"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/4beb7225857c50a9391b71fbe998bc23c33b4d87ee15e3da9b7c1b7dfdc67a11/68747470733a2f2f7472617669732d63692e636f6d2f766976656b6b617469616c2f71616f612d74687265652d7361742e7376673f6272616e63683d6d6173746572"
    alt="" data-canonical-src="https://travis-ci.com/vivekkatial/qaoa-three-sat.svg?branch=master"
    style="max-width:100%;"></a> <a href="https://qaoa-three-sat.readthedocs.io/en/latest/?badge=latest"
    rel="nofollow"><img src="https://camo.githubusercontent.com/253d508d956ec9315fd5509c8d9cb82640904ab96c15672f2c65c9ec5c2de390/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f71616f612d74687265652d7361742f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/qaoa-three-sat/badge/?version=latest"
    style="max-width:100%;"></a>

    </h1>

    <p>An implementation for solving 3SAT (Exact Cover) using the Quantum Approximate
    Optimization Algorithm</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607596883.0
weatherlab/metview:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: weatherlab/metview
  latest_release: null
  readme: '<h1>

    <a id="user-content-metview" class="anchor" href="#metview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>metview</h1>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1523286570.0
willgpaik/centos7_aci:
  data_format: 2
  description: Centos 7 base image for ACI
  filenames:
  - Singularity.test
  - Singularity.gpu
  - Singularity
  - Singularity.cuda9.1
  full_name: willgpaik/centos7_aci
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos7_aci" class="anchor" href="#centos7_aci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos7_aci</h1>

    <p>Centos 7 base image for ACI Singualarity recipe<br>

    This recipe may include unnecessary packages for certain software installation.<br>

    Size of CPU-only container: ~1 GB<br>

    Size of GPU container: ~2.6 GB</p>

    <p>More packages will be added in the future</p>

    <p>2019/2/17

    <strong>Centos 7</strong> with <strong>GCC 8</strong><br>

    To enable GCC 8,</p>

    <pre><code>&gt; source /opt/rh/devtoolset-8/enable

    </code></pre>

    <p>2019/3/1<br>

    OpenMPI is added to <code>$PATH</code></p>

    <p>2019/3/11<br>

    OpenMPI is updated to version 2.1.6</p>

    <p>2019/4/12<br>

    Boost 1.70.0 in added</p>

    <p>2019/7/19<br>

    <del>Python 2 and 3 are updated to version 2.7.16 and version 3.7.4</del><br>

    OpenMPI is updated to version 4.0.1</p>

    <p>2019/7/21<br>

    <del>Few Python packages are added</del></p>

    <p>2019/7/22<br>

    <del>Few corrections are made including Python</del></p>

    <p>2019/7/23<br>

    Pythons are replaced with packages<br>

    To enable Python 2.7.16,</p>

    <pre><code>&gt; source /opt/rh/python27/enable

    </code></pre>

    <p>System version of python is 3.6.8</p>

    <p>2019/7/30<br>

    devtoolset-7 GCC is added (some software can''t be built with GCC 8)</p>

    <p>2019/11/9<br>

    CMake 3.15.5 is added</p>

    <p>2019/11/22<br>

    OpenMPI is downgraded to 1.10.1 to match version on ACI</p>

    <p>2020/2/12<br>

    Boost is upgraded to 1.72.0 and CMake is upgraded to 3.16.4</p>

    <p>2020/3/2<br>

    GPU version is added</p>

    <p>2020/9/21<br>

    Minor updates are made (regarding libxkb)</p>

    <p>2020/9/28<br>

    Recipe for CUDA 9.1 is added (for FSL with CUDA)</p>

    <p>2020/10/11<br>

    Boost is upgraded to 1.74.0 and CMake is upgraded to 3.18.4<br>

    R 4.0.3 is added (Curl 7.72.0 and XZ 5.2.5 are added for R)<br>

    VirtualGL is downgraded to 2.5.2 to match system version</p>

    <p>2020/10/18<br>

    UDUNITS 2.2.26 is added</p>

    <p>2020/10/20<br>

    Tix-devel, Tx-devel, TkInter-devel, LAPACK-devel, and BLAS-devel are added</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1603227322.0
willgpaik/centos8_roar:
  data_format: 2
  description: Centos 8 base image for Roar
  filenames:
  - Singularity.gpu
  - Singularity
  full_name: willgpaik/centos8_roar
  latest_release: null
  readme: '<h1>

    <a id="user-content-centos8_roar" class="anchor" href="#centos8_roar" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>centos8_roar</h1>

    <p>Centos 8 base image for Roar</p>

    <h3>

    <a id="user-content-note" class="anchor" href="#note" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NOTE</h3>

    <ul>

    <li>This recipe may include unnecessary packages for certain software installation</li>

    <li>More packages will be added in the future</li>

    </ul>

    <h2>

    <a id="user-content-updates" class="anchor" href="#updates" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Updates</h2>

    <ul>

    <li>

    <p>2020/11/13</p>

    <ul>

    <li>Initial recipe added</li>

    </ul>

    </li>

    <li>

    <p>2021/03/22</p>

    <ul>

    <li>Default Python3 is updated to Python 3.8</li>

    <li>Lapack, BLAS, OpenBLAS, ATLAS, and NetCDF are added</li>

    <li>CMake 3.19.7, Boost 1.75.0, and R 4.0.4 are added</li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616617880.0
willgpaik/deformetrica_aci:
  data_format: 2
  description: Singularity recipe for Deformetrica on Centos 7
  filenames:
  - Singularity
  full_name: willgpaik/deformetrica_aci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-deformetrica_aci\" class=\"anchor\" href=\"\
    #deformetrica_aci\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>deformetrica_aci</h1>\n<p>Singularity recipe for Deformetrica\
    \ on Centos 7 for ACI-ICS clusters</p>\n<p>2019/2/14<br>\nAnaconda3 ver. 2018.12<br>\n\
    Deformetrica 4.1<br>\nGUI can be used through EoD</p>\n<p>Commands:</p>\n<pre><code>&gt;\
    \ source activate deformetrica  \n&gt; deformetrica  \nOr,  \n&gt; deformetrica\
    \ gui\n</code></pre>\n<p>2020/9/21<br>\nGPU support is added<br>\nAnaconda, Python,\
    \ and Deformetrica are updated</p>\n<p>2020/10/9<br>\nPyTorch and PyKeOps are\
    \ added</p>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1602342657.0
willgpaik/qt5_aci:
  data_format: 2
  description: Singularity recipe for Qt5 on Centos 7 and Ubuntu 16.04
  filenames:
  - Singularity
  - Singularity.qt5
  - Singularity.ubuntu
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_ants
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_fsl
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_centos8
  - dsistudio_mrtrix3/Singularity.dsi_mrtrix3_ants_fsl_fmriprep
  full_name: willgpaik/qt5_aci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-qt5_aci\" class=\"anchor\" href=\"#qt5_aci\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>qt5_aci</h1>\n<p>Singularity recipe for Qt5 on Centos 7 and Ubuntu\
    \ 16.04 For ICS</p>\n<p><strong>NOTE: DO NOT rebuild \"Singularity.dsi_mrtrix3\"\
    \ image.</strong><br>\n(Last successful build was Mar 12 2019)</p>\n<p>Singularity\
    \ recipe for DSI Studio and MRtrix3 is updated on <strong>dsistudio_mrtrix3</strong>\
    \ folder</p>\n<p>If you want to install DSI Studio and MRtrix3 on Basic Qt5 Container,<br>\n\
    downlaod \"dsistudio_mrtrix3_install.sh\" to preferred location\nand follow commands\
    \ inside Singularity environment:</p>\n<pre><code>&gt; chmod +x dsistudio_mrtrix3_install.sh\
    \  \n&gt; ./dsistudio_mrtrix3_install.sh\n</code></pre>\n<p>2019/2/21<br>\nUnable\
    \ to use <strong>GCC 8.2.1</strong> due to build failure =&gt; Going back to <strong>GCC\
    \ 7.3.1</strong><br>\n(Failed to resolve the issue at this moment)</p>\n<p><del>2019/5/13<br>\n\
    Updated dsistudio_mrtrix3_install.sh due to Qt version issue<br>\n(Requires Qt\
    \ 5.12.2 or above: <a href=\"https://github.com/frankyeh/DSI-Studio/issues/34\"\
    >https://github.com/frankyeh/DSI-Studio/issues/34</a>)</del></p>\n<p>2019/5/24<br>\n\
    Reverted changes made on 2019/5/13</p>\n<p>2019/6/24<br>\n<del>Newer version qt5\
    \ installation recipe added (in progress)</del></p>\n<p>2019/7/22<br>\nQt is updated\
    \ to 5.12 with Qt Charts (for DSI Studio)</p>\n<p>2019/7/24<br>\nQt SVG is added\
    \ (for MRtrix 3)<br>\n32-bit EoD graphics libraries are disable (to aviod warnings)</p>\n\
    <p>2019/7/29<br>\nNVIDIA driver is added to DSI Studio MRtrix3 container</p>\n\
    <p>2019/11/10<br>\nQt version 5.12.5 is used</p>\n<p>2020/4/24<br>\nUbuntu 16.04\
    \ version added with Qt 5.14.2</p>\n<p>2020/6/20<br>\nQt5 container is updated\
    \ to have nvidia driver</p>\n<p>2020/7/27<br>\nUbuntu container is updated to\
    \ have NVIDIA driver (Ubuntu 16.04 based)</p>\n<p>2020/9/28<br>\nQt5 container\
    \ is updated to use CUDA 9.1 version (for FSL with CUDA)<br>\n(Reference: <a href=\"\
    https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GPU\" rel=\"nofollow\">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GPU</a>)</p>\n\
    <p>2020/10/20<br>\nQt5X11Extras is added to the Qt5 recipe<br>\n(Ubuntu container\
    \ will not be updated unless necessary)</p>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1618004326.0
yee379/uresnet-tomo-seg:
  data_format: 2
  description: uresnet based deep neutral network for the segmentation of high resolution
    cryo-EM tomographs
  filenames:
  - Singularity
  full_name: yee379/uresnet-tomo-seg
  latest_release: null
  readme: '<h1>

    <a id="user-content-uresnet-tomo-seg" class="anchor" href="#uresnet-tomo-seg"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>uresnet-tomo-seg</h1>

    <p>uresnet based deep neutral network for the segmentation of high resolution
    cryo-EM tomographs</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1577150888.0
zenotech/paraview-superbuild:
  data_format: 2
  description: https://gitlab.kitware.com/paraview/paraview-superbuild.git
  filenames:
  - Scripts/singularity/Singularity.osmesa
  - Scripts/singularity/Singularity.egl
  full_name: zenotech/paraview-superbuild
  latest_release: null
  readme: "<p><a href=\"Documentation/img/paraview100.png\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img src=\"Documentation/img/paraview100.png\" alt=\"ParaView-Superbuild\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-introduction\"\
    \ class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Introduction</h1>\n<p>ParaView-Superbuild,\
    \ henceforth referred to as \"superbuild\", is a project to\nbuild ParaView and\
    \ its dependencies. ParaView itself can be easily built using\nCMake as long as\
    \ the required external dependencies are available on the build\nmachine. However,\
    \ ParaView's several external dependencies, e.g. Qt, CGNS,\nFFMPEG, etc. can be\
    \ very tedious to build. Also, if you want to generate\nredistributable binaries,\
    \ you need to take extra care when building and\npackaging these dependencies.\
    \ To make our lives easier in supporting both these\nuse-cases, the superbuild\
    \ project was born.</p>\n<p>Although primarily designed to build the official\
    \ ParaView binaries, the\nsuperbuild has since been regularly used to build and\
    \ install ParaView\non various supercomputing systems.</p>\n<h1>\n<a id=\"user-content-obtaining-the-source\"\
    \ class=\"anchor\" href=\"#obtaining-the-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Obtaining the source</h1>\n<p>To\
    \ obtain the superbuild source locally, clone this repository using\n<a href=\"\
    https://git-scm.org\" rel=\"nofollow\">Git</a>.</p>\n<pre><code>$ git clone --recursive\
    \ https://gitlab.kitware.com/paraview/paraview-superbuild.git\n</code></pre>\n\
    <h1>\n<a id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building</h1>\n\
    <p>The superbuild can be built with a Makefiles or Ninja CMake generator. The\
    \ IDE\ngenerators (Xcode and Visual Studio) are not supported.</p>\n<h2>\n<a id=\"\
    user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\
    <p>The superbuild tries to provide all of its own dependencies, but some tooling\n\
    is assumed to be available on the host machine.</p>\n<ul>\n<li>Compiler toolchain\n\
    <ul>\n<li>GCC 4.9 or newer</li>\n<li>Xcode 10 or newer (older is probably supported,\
    \ but untested)</li>\n<li>MSVC 2017 or newer</li>\n<li>ICC (minimum version unknown)</li>\n\
    </ul>\n</li>\n<li>Tools\n<ul>\n<li>\n<code>pkg-config</code> is used on non-Windows\
    \ platforms to find dependencies in\nsome projects</li>\n<li>\n<code>ninja</code>\
    \ (or <code>make</code>) for building</li>\n<li>Python (if not built by the superbuild)\
    \ for building packages</li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-building-a-specific-version\"\
    \ class=\"anchor\" href=\"#building-a-specific-version\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ a specific version</h2>\n<p>The superbuild project uses the same versioning\
    \ scheme as ParaView,\nand gets tagged for every release of ParaView.  For example,\
    \ to build\nParaView version 5.7.1, checkout the <code>v5.7.0</code> tag of ParaView\
    \ and\nsuperbuild.</p>\n<p>Currently available tags are shown\n<a href=\"https://gitlab.kitware.com/paraview/paraview-superbuild/-/tags\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>To checkout a specific tag from the superbuild\
    \ git repository:</p>\n<pre><code>$ cd paraview-superbuild\n$ git fetch origin\
    \ # ensure you have the latest state from the main repo\n$ git checkout v5.7.0\
    \ # replace `v5.7.0` with tag name of your choice\n$ git submodule update\n</code></pre>\n\
    <p>At this point, your superbuild has all of the <em>rules</em> that were used\n\
    when building the selected version of ParaView. Also, note that it's\npossible\
    \ to build a version of ParaView using a different superbuild\nversion.  For example,\
    \ you could use superbuild <code>v5.7.0</code>, to build the\nlatest master (i.e.,\
    \ development) version of ParaView, or a custom\nbranch.  This is done by first\
    \ checking out the superbuild for the\nappropriate version and then setting the\
    \ CMake variables that affect\nwhich ParaView source is to be used.  There are\
    \ several ways to\ncontrol how superbuild finds its source packages:</p>\n<ol>\n\
    <li>If you want to use git to checkout ParaView source (default), then set\n<code>paraview_SOURCE_SELECTION</code>\
    \ to <code>git</code>, ensure <code>paraview_GIT_REPOSITORY</code> is\npointing\
    \ to the ParaView git repository you want to clone (by default it is\nset to the\
    \ offical ParaView repository) and then set the <code>paraview_GIT_TAG</code>\n\
    to be a specific tagname or branch available for the selected git\nrepository.\
    \ Use <code>master</code> for latest development code, <code>v5.7.0</code> for\
    \ the\n5.7.0 release, <code>release</code> for latest stable release, or a specific\
    \ ParaView\ncommit SHA. In this setup, when building the superbuild, it will clone\
    \ and\ncheckout the appropriate revision from the ParaView git repository automatically.</li>\n\
    <li>Instead of letting superbuild do the cloning and updating of the ParaView\n\
    source, you can also manually check it out and keep it updated as needed.\nTo\
    \ use this configuration, set <code>paraview_SOURCE_SELECTION</code> to <code>source</code>,\
    \ and\nset <code>paraview_SOURCE_DIR</code> to point to a custom ParaView source\
    \ tree. See 'offline\nbuilds' below for instructions to download needed dependency\
    \ packages.</li>\n<li>Another option is to use a source tarball of a ParaView\
    \ release. For that,\nset <code>paraview_SOURCE_SELECTION</code> to the version\
    \ to build such as <code>5.7.0</code>.\nThe superbuild offers the lastest stable\
    \ release as well as release\ncandidate in preparation for the release. This is\
    \ the best way to build a\nreleased version of ParaView.</li>\n</ol>\n<p><strong>NOTE:</strong>\
    \ If you switch to a superbuild version older than 5.2, the instructions\ndescribed\
    \ on this page are not relevant since the superbuild was refactored and\nchanged\
    \ considerably for 5.2. For older versions, refer to instructions on the\n<a href=\"\
    http://www.paraview.org/Wiki/index.php?title=ParaView/Superbuild&amp;oldid=59804\"\
    \ rel=\"nofollow\">Wiki</a>.</p>\n<p><strong>ALSO NOTE:</strong> Since this README\
    \ is expected to be updated for each version,\nonce you checkout a specfic version,\
    \ you may want to refer to the README for\nthat specific version.</p>\n<h2>\n\
    <a id=\"user-content-incremental-builds\" class=\"anchor\" href=\"#incremental-builds\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Incremental builds</h2>\n<p>The superbuild is kind of na\xEFve for\
    \ changes to project sources within the\nsuperbuild. This is due to the superbuild\
    \ not tracking all source files for\neach project and instead only \"stamp files\"\
    \ to indicate the steps performed.</p>\n<p>When changing the source of a subproject,\
    \ the best solution is to delete the\n\"stamp file\" for the build step of that\
    \ project:</p>\n<pre><code>$ rm superbuild/$project/stamp/$project-build\n</code></pre>\n\
    <p>and to rerun the superbuild's build step.</p>\n<h2>\n<a id=\"user-content-projects-and-features\"\
    \ class=\"anchor\" href=\"#projects-and-features\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Projects and\
    \ Features</h2>\n<p>The superbuild contains multiple projects which may be used\
    \ to enable\ndifferent features within the resulting ParaView build. Most projects\
    \ involve\ndownloading and adding the feature to the resulting package, but there\
    \ are a\nfew which are used just to enable features within ParaView itself.</p>\n\
    <p>The <code>paraview</code> project must be enabled to build ParaView.</p>\n\
    <p>The <code>paraviewsdk</code> project enables the building of a package which\
    \ includes\nheaders and libraries suitable for developing against ParaView. It\
    \ is only available\non Linux (at the moment).</p>\n<p>The <code>paraviewweb</code>\
    \ project adds web services into the resulting package.</p>\n<p>The <code>paraviewgettingstartedguide</code>,\
    \ and <code>paraviewtutorialdata</code> packages add\nstartup documentation and\
    \ example data to the package.</p>\n<p>ParaView supports multiple rendering engines\
    \ including <code>egl</code>, <code>mesa</code>,\n<code>osmesa</code>, and <code>qt5</code>.\
    \ All of these are incompatible with each other. If none of\nthese are chosen,\
    \ a UI-less ParaView will be built (basically just\n<code>pvpython</code>). On\
    \ Windows and macOS, only the <code>qt5</code> rendering engine is\navailable.</p>\n\
    <p>The <code>python</code> package is available to enable Python support in the\
    \ package. In\naddition, the <code>matplotlib</code> and <code>numpy</code> packages\
    \ are available.</p>\n<p>The following packages enable other features within ParaView:</p>\n\
    <ul>\n<li>\n<code>adios</code>: Enable readers and writers for visualization data\
    \ in the ADIOS\nfile format.</li>\n<li>\n<code>las</code>: Enable reading the\
    \ LAS file format</li>\n<li>\n<code>cosmotools</code>: Enables Cosmo file format\
    \ readers and related filters and\nalgorithms.</li>\n<li>\n<code>ffmpeg</code>:\
    \ Video encoding library for macOS and Linux.</li>\n<li>\n<code>ospray</code>:\
    \ A ray tracing rendering backend from Intel.</li>\n<li>\n<code>silo</code>: Support\
    \ reading the silo file format.</li>\n<li>\n<code>tbb</code>: Improved parallel\
    \ processing support within various VTK and\nParaView filters and algorithms.</li>\n\
    <li>\n<code>visitbridge</code>: Enables readers for file formats provided from\
    \ the VisIt\nproject.</li>\n<li>\n<code>vortexfinder2</code>: A collection of\
    \ tools to visualize and analyze vortices.</li>\n<li>\n<code>vrpn</code>: Virtual\
    \ reality support through the VRPN interface.</li>\n<li>\n<code>vtkm</code>: VTK-m\
    \ Accelerator Filters</li>\n<li>\n<code>xdmf3</code>: A meta file format built\
    \ on top of HDF5.</li>\n</ul>\n<h2>\n<a id=\"user-content-offline-builds\" class=\"\
    anchor\" href=\"#offline-builds\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Offline builds</h2>\n<p>The superbuild\
    \ has a <code>download-all</code> target that will download all of\nthe files\
    \ from the network that are necessary for the currently\nconfigured build. By\
    \ default, they are placed into the <code>downloads</code>\ndirectory of the build\
    \ tree.  This superbuild-plus-downloads tree may\nthen be copied to a non-networked\
    \ machine and pointed at using the\n<code>superbuild_download_location</code>\
    \ variable (or placed in the default\nlocation).</p>\n<p>Note that the <code>nvidiaoptix</code>\
    \ and <code>nvidiamdl</code> project sources are not available\nat their URLs\
    \ in the superbuild outside of Kitware due to their sources being\nbehind click-wrapping.\
    \ They may be manually downloaded from these web pages:</p>\n<ul>\n<li>\n<code>nvidiaoptix</code>:\
    \ <a href=\"https://developer.nvidia.com/designworks/optix/download\" rel=\"nofollow\"\
    >https://developer.nvidia.com/designworks/optix/download</a>\nThough older versions\
    \ are available here:\n<a href=\"https://developer.nvidia.com/designworks/optix/downloads/legacy\"\
    \ rel=\"nofollow\">https://developer.nvidia.com/designworks/optix/downloads/legacy</a>\n\
    </li>\n<li>\n<code>nvidiamdl</code>: <a href=\"https://developer.nvidia.com/mdl-sdk\"\
    \ rel=\"nofollow\">https://developer.nvidia.com/mdl-sdk</a>\n</li>\n</ul>\n<h3>\n\
    <a id=\"user-content-overriding-downloaded-archives\" class=\"anchor\" href=\"\
    #overriding-downloaded-archives\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Overriding downloaded archives</h3>\n\
    <p>On rare occasions, you may want to replace a downloaded archive with a different\n\
    version. You may replace the archive with a newer version preserving its\nname,\
    \ however, on doing so, the hash verification will most likely fail during\nthe\
    \ build step. To skip the hash verification for archives that have been\nmanually\
    \ changed, set the <code>xxx_SKIP_VERIFICATION</code> option, where <code>xxx</code>\n\
    is the name of the project. <code>xxx_SKIP_VERIFICATION</code> must be passed\
    \ on command line\nwhen invoking CMake using <code>-Dxxx_SKIP_VERIFICATION:BOOL=TRUE</code>.</p>\n\
    <p>Alternatively, you can edit the <code>versions.cmake</code> files in the source\
    \ repository\nand modify the <code>URL_MDF5</code> or <code>URL_HASH</code> values\
    \ for the specific project with\nupdated hashes.</p>\n<h2>\n<a id=\"user-content-installing\"\
    \ class=\"anchor\" href=\"#installing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installing</h2>\n<p>The superbuild\
    \ supports the <code>install</code> target by selecting a template package\nusing\
    \ the <code>SUPERBUILD_DEFAULT_INSTALL</code> variable. The default and availability\n\
    depends on the platform and selected projects, but valid values for this\ninclude:</p>\n\
    <ul>\n<li><code>paraview/ZIP</code></li>\n<li><code>paraview/DragNDrop</code></li>\n\
    <li><code>paraview/TGZ</code></li>\n<li><code>paraview/TXZ</code></li>\n<li><code>paraviewsdk/TGZ</code></li>\n\
    <li><code>paraviewsdk/TXZ</code></li>\n</ul>\n<p>The CMake cache editors (<code>ccmake</code>\
    \ and <code>cmake-gui</code>) have dropdown options for\nthe supported options.</p>\n\
    <p>The selected package logic will be used to install ParaView and its\ndependencies\
    \ into <code>CMAKE_INSTALL_PREFIX</code> rather than being placed into a\npackage.\
    \ For example, the <code>DragNDrop</code> generator creates <code>.app</code>\
    \ bundles which\nwill be created whereas the <code>TGZ</code>, <code>TXZ</code>,\
    \ and <code>ZIP</code> generators use the standard\n<code>bin/</code>, <code>lib/</code>,\
    \ etc. directories.</p>\n<h3>\n<a id=\"user-content-caveats\" class=\"anchor\"\
    \ href=\"#caveats\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Caveats</h3>\n<p>If using the <code>git</code> source\
    \ selection for ParaView, the build will rerun when\nusing the <code>install</code>\
    \ target due to limitations in the external project\nmechanisms and the way CPack\
    \ works. There are two ways to avoid this:</p>\n<ul>\n<li>the <code>SUPERBUILD_OFFLINE_BUILD</code>\
    \ option may be set to <code>ON</code> to unlink the git\nupdate step from the\
    \ configure/build steps; or</li>\n<li>the initial build can just be run using\
    \ the <code>install</code> target instead of\nthe usual <code>make &amp;&amp;\
    \ make install</code> pattern.</li>\n</ul>\n<h2>\n<a id=\"user-content-external-plugins\"\
    \ class=\"anchor\" href=\"#external-plugins\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>External plugins</h2>\n<p>The\
    \ superbuild supports building more plugins into ParaView using the\n<code>paraviewexternalplugins</code>\
    \ project. As an example, to build two external\nplugins <code>a</code> and <code>b</code>,\
    \ the following settings should be used:</p>\n<ul>\n<li>\n<code>ENABLE_paraviewexternalplugins:BOOL=ON</code>:\
    \ Enables building using external\nplugins.</li>\n<li>\n<code>paraview_PLUGINS_EXTERNAL:STRING=a;b</code>:\
    \ The list of plugins to build.</li>\n<li>\n<code>paraview_PLUGIN_a_PATH:PATH=/path/to/plugin/a</code>:\
    \ The path to plugin <code>a</code>'s\nsource directory. It must contain a <code>plugins.cmake</code>\
    \ to be picked up by\nParaView.</li>\n<li>\n<code>paraview_PLUGIN_b_PATH:PATH=/path/to/plugin/b</code>:\
    \ Same as above, but for\nplugin <code>b</code>.</li>\n</ul>\n<h2>\n<a id=\"user-content-cmake-variables\"\
    \ class=\"anchor\" href=\"#cmake-variables\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CMake Variables</h2>\n<h3>\n\
    <a id=\"user-content-style-guide\" class=\"anchor\" href=\"#style-guide\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Style\
    \ Guide</h3>\n<p>Note that currently not all project and configuration variables\
    \ follow this\nstyle guide but any new projects should use this convention while\
    \ any\nexisting projects and configuration variables will transition to this over\n\
    time.</p>\n<ul>\n<li>All references to a given project name will be lowercase.</li>\n\
    <li>Underscores will be used as word seperators in variable names.</li>\n<li>All\
    \ project specific configuration variables will be lower-case project\nname followed\
    \ by upper-case setting name.\nExamples include:\n<ul>\n<li>\n<code>mesa_USE_SWR</code>\
    \ : Enable the OpenSWR driver for (OS)Mesa.</li>\n<li>\n<code>ospray_BUILD_ISA</code>\
    \ : Select the SIMD architecture used to build OSPray.</li>\n</ul>\n</li>\n<li>Internal\
    \ variables used within a given project's projectname.cmake file\nwill be all\
    \ lower-case.</li>\n<li>Multiple versions:\n<ul>\n<li>Use the <code>superbuild_set_selectable_source</code>\
    \ macro to allow multiple\nversions of a given project.</li>\n<li>Specify source\
    \ selection versions as numeric, i.e. without any \"v\" or\n\"V\" prefix.</li>\n\
    <li>If the project is going through a release candidate cycle, add the\navailable\
    \ RCs as additional sources as they become availabe.  Once\na final release is\
    \ made, replace all the RCs with the updated release.</li>\n</ul>\n</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-build-variables\" class=\"anchor\" href=\"#build-variables\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Build Variables</h3>\n<ul>\n<li>\n<code>superbuild_download_location</code>\
    \ (default <code>${CMAKE_BINARY_DIR}/downloads</code>):\nThe location to store\
    \ downloaded source artifacts. Usually, it is changed\nso that it is preserved\
    \ across a wipe of the build directory.</li>\n<li>\n<code>SUPERBUILD_PROJECT_PARALLELISM</code>\
    \ (default based on the number of available\nprocessors): When using a Makefiles\
    \ generator, subproject builds use <code>-j</code>\nexplicitly with this number.</li>\n\
    <li>\n<code>ENABLE_xxx</code> (generally, default <code>OFF</code>): If selected,\
    \ the <code>xxx</code> project\nwill be built within the superbuild. See above\
    \ for descriptions of the\nvarious projects. <code>ENABLE_</code> flags are not\
    \ shown for projects which must be\nenabled due to a project depending on it (e.g.,\
    \ <code>visitbridge</code> requires\n<code>boost</code>, so enabling <code>visitbridge</code>\
    \ will hide the <code>ENABLE_boost</code> option).</li>\n<li>\n<code>USE_SYSTEM_xxx</code>\
    \ (default <code>OFF</code>): If selected, the <code>xxx</code> project from the\n\
    build environment is used instead of building it within the superbuild.\nNot all\
    \ projects support system copies (the flag is not available if so).</li>\n<li>\n\
    <code>SUPERBUILD_DEBUG_CONFIGURE_STEPS</code> (default <code>OFF</code>): If set,\
    \ the superbuild\nwill log configure steps for each <code>xxx</code> project into\n\
    <code>superbuild/xxx/stamp/xxx-configure-*.log</code> files.</li>\n<li>\n<code>CMAKE_BUILD_TYPE</code>\
    \ (default <code>Release</code>): The build type to use for the\nbuild. Can be\
    \ <code>Release</code>, <code>RelWithDebInfo</code>, or (on not-Windows) <code>Debug</code>.</li>\n\
    </ul>\n<p>The following flags affect ParaView directly:</p>\n<ul>\n<li>\n<p><code>paraview_SOURCE_SELECTION</code>\
    \ (default <code>5.9.0</code>): The source to use for\nParaView itself. The version\
    \ numbers use the source tarballs from the\nwebsite for the release. The <code>source</code>\
    \ selection uses the\n<code>paraview_SOURCE_DIR</code> variable to look at a checked\
    \ out ParaView source\ndirectory. The <code>git</code> selection has the superbuild\
    \ clone and builds a\ncheckout of ParaView from git repository controlled by the\n\
    <code>paraview_GIT_REPOSITORY</code> and <code>paraview_GIT_TAG</code> variables.\
    \ By default, the\n<code>master</code> branch of the main repository is used.</p>\n\
    <p><strong>Note</strong>: When using the <code>source</code> selection, incremental\
    \ builds to the\nsuperbuild may not rebuild ParaView even if the source tree has\
    \ changed.\nThis is because the superbuild is \"blind\" to the source tree other\
    \ than\nits existence.</p>\n</li>\n<li>\n<p><code>CMAKE_BUILD_TYPE_paraview</code>\
    \ (default is the same as the superbuild):\nParaView may be built with a different\
    \ build type (e.g., <code>Release</code> vs.\n<code>RelWithDebInfo</code>) as\
    \ the rest of the superbuild using this variable. In\naddition to <code>&lt;SAME&gt;</code>\
    \ which uses <code>CMAKE_BUILD_TYPE</code>, any valid value for\n<code>CMAKE_BUILD_TYPE</code>\
    \ is also valid.</p>\n</li>\n<li>\n<p><code>BUILD_SHARED_LIBS_paraview</code>\
    \ (default is the same as the superbuild):\nParaView may be built with a different\
    \ selection for BUILD_SHARED_LIBS flag\nthan the rest of the superbuild using\
    \ this variable. For example,\nto build ParaView static while building other projects\
    \ in the superbuild\n(e.g. MPI, Python, etc.) as shared, set <code>BUILD_SHARED_LIBS</code>\
    \ to <code>ON</code>\nand <code>BUILD_SHARED_LIBS_paraview</code> to <code>OFF</code>.</p>\n\
    </li>\n<li>\n<p><code>PARAVIEW_BUILD_WEB_DOCUMENTATION</code> (default <code>OFF</code>):\
    \ Have ParaView build\nits HTML documentation.</p>\n</li>\n<li>\n<p><code>mesa_USE_SWR</code>\
    \ (default <code>ON</code>): If <code>mesa</code> is enabled, this enables\nIntel's\
    \ software rasterization backend (x86 only).</p>\n</li>\n<li>\n<p><code>PARAVIEW_INITIALIZE_MPI_ON_CLIENT</code>\
    \ (default <code>ON</code>): If <code>mpi</code> is enabled, this\nenables MPI\
    \ to be initialized automatically when running the GUI or pvpython.\nSome readers\
    \ use MPI IO and thus must have MPI initialized in order to be\nused so this is\
    \ the default for general ease of use. For some MPI implementations,\na code that\
    \ initializes MPI must be run with the appropriate mpi launcher\n(e.g. mpirun)\
    \ which in this case it may be desirable to disable this option.\nNote that the\
    \ <code>--mpi</code> or <code>--no-mpi</code> command line options to paraview\
    \ and\npvpython can be used to override this option.</p>\n</li>\n<li>\n<p><code>PARAVIEW_EXTRA_CMAKE_ARGUMENTS</code>\
    \ (default <code>\"\"</code>: Extra CMake arguments to\npass to ParaView's configure\
    \ step. This can be used to set CMake variables\nfor the build that are otherwise\
    \ not exposed in the superbuild itself.</p>\n</li>\n<li>\n<p><code>PARAVIEW_ENABLE_VRPLUGIN</code>\
    \ (default <code>ON</code>): Enables the VRPlugin. If\n<code>vrpn</code> is enabled,\
    \ the VRPlugin will support input devices through a VRPN\nconnection. VRUI support\
    \ is enabled unconditionally on Linux.</p>\n</li>\n</ul>\n<h4>\n<a id=\"user-content-paraview-editions\"\
    \ class=\"anchor\" href=\"#paraview-editions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>ParaView editions</h4>\n<p>A\
    \ typical ParaView build includes several modules and dependencies. While these\n\
    are necessary for a fully functional application, there are cases (e.g. in situ\n\
    use-cases) where a build with limited set of features is adequate. ParaView build\
    \ supports\nthis using the <code>PARAVIEW_BUILD_EDITION</code> setting. Supported\
    \ values for this setting are:</p>\n<ul>\n<li>\n<code>CORE</code>: Build modules\
    \ necessary for core ParaView functionality.\nThis does not include rendering.</li>\n\
    <li>\n<code>RENDERING</code>: Build modules necessary for supporting rendering\
    \ including views\nand representations. This includes everything in <code>CORE</code>.</li>\n\
    <li>\n<code>CATALYST</code>: Build all modules necessary for in situ use cases\
    \ without\nrendering and optional components like NetCDF- and HDF5-based readers\
    \ and\nwriters.</li>\n<li>\n<code>CATALYST_RENDERING</code>: Same as <code>CATALYST</code>\
    \ but with rendering supported added.</li>\n<li>\n<code>CANONICAL</code> (default):\
    \ Build modules necessary for standard ParaView build.</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-packaging-variables\" class=\"anchor\" href=\"#packaging-variables\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Packaging Variables</h3>\n<ul>\n<li>\n<code>PARAVIEW_PACKAGE_SUFFIX</code>\
    \ (default based on selected options): The suffix\nfor the name generated by the\
    \ package.</li>\n<li>\n<code>paraview_PLUGINS_AUTOLOAD</code>: List of plugins\
    \ to autoload in the packaged\nParaView.</li>\n</ul>\n<h1>\n<a id=\"user-content-packaging\"\
    \ class=\"anchor\" href=\"#packaging\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Packaging</h1>\n<p>The packages\
    \ may be built using the <code>cpack-paraview</code> tests via <code>ctest</code>.\
    \ The\neasiest way to build all available packages is to run <code>ctest -R cpack</code>.</p>\n\
    <h1>\n<a id=\"user-content-learning-resources\" class=\"anchor\" href=\"#learning-resources\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Learning Resources</h1>\n<ul>\n<li>\n<p>General information is available\
    \ at the <a href=\"http://www.paraview.org\" rel=\"nofollow\">ParaView Homepage</a>.</p>\n\
    </li>\n<li>\n<p>Community discussion takes place on the <a href=\"http://www.paraview.org/mailing-lists/\"\
    \ rel=\"nofollow\">ParaView Mailing Lists</a>.</p>\n</li>\n<li>\n<p>Commercial\
    \ <a href=\"http://www.kitware.com/products/support.html\" rel=\"nofollow\">support</a>\
    \ and <a href=\"http://www.kitware.com/products/protraining.php\" rel=\"nofollow\"\
    >training</a>\nare available from <a href=\"http://www.kitware.com/\" rel=\"nofollow\"\
    >Kitware</a>.</p>\n</li>\n</ul>\n<h1>\n<a id=\"user-content-reporting-bugs\" class=\"\
    anchor\" href=\"#reporting-bugs\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Reporting Bugs</h1>\n<p>If you have\
    \ found a bug:</p>\n<ol>\n<li>\n<p>If you have a patch, please read the <a href=\"\
    CONTRIBUTING.md\">CONTRIBUTING.md</a> document.</p>\n</li>\n<li>\n<p>Otherwise,\
    \ please join one of the <a href=\"http://www.paraview.org/mailing-lists/\" rel=\"\
    nofollow\">ParaView Mailing Lists</a> and ask\nabout the expected and observed\
    \ behaviors to determine if it is\nreally a bug.</p>\n</li>\n<li>\n<p>Finally,\
    \ if the issue is not resolved by the above steps, open\nan entry in the <a href=\"\
    http://www.paraview.org/Bug\" rel=\"nofollow\">ParaView Issue Tracker</a>.</p>\n\
    </li>\n</ol>\n<h1>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h1>\n<p>Like ParaView, ParaView-Superbuild is distributed\
    \ under the OSI-approved BSD\n3-clause License. See <a href=\"Copyright.txt\"\
    >Copyright.txt</a> for details. For additional licenses,\nrefer to <a href=\"\
    http://www.paraview.org/paraview-license/\" rel=\"nofollow\">ParaView Licenses</a>.</p>\n\
    <h1>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contributing</h1>\n<p>See <a href=\"CONTRIBUTING.md\">CONTRIBUTING.md</a>\
    \ for instructions to contribute.</p>\n"
  stargazers_count: 1
  subscribers_count: 5
  topics:
  - paraview-superbuild
  - cmake
  - superbuild
  updated_at: 1612294938.0
