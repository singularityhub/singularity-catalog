14renus/sdsc:
  data_format: 2
  description: null
  filenames:
  - tensorflow-gpu/from-ubuntu/Singularity
  full_name: 14renus/sdsc
  latest_release: null
  readme: '<h1>

    <a id="user-content-rstudio-tensorflow-singularity" class="anchor" href="#rstudio-tensorflow-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>rstudio-tensorflow-singularity</h1>

    <p>Singularity recipe file for running RStudio-Server wiht Tensorflow</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1554168191.0
20akshay00/HPC_practice:
  data_format: 2
  description: Practice codes for high-performance computing
  filenames:
  - OpenACC_Bootcamp/nways/Singularity
  - OpenACC_Bootcamp/nways_cfd/Singularity
  full_name: 20akshay00/HPC_practice
  latest_release: null
  readme: '<h4>

    <a id="user-content-open-acc-gpu-bootcamp" class="anchor" href="#open-acc-gpu-bootcamp"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Open
    ACC; GPU bootcamp:</h4>

    <p><a href="https://www.gpuhackathons.org/events-overview" rel="nofollow">https://www.gpuhackathons.org/events-overview</a></p>

    <h4>

    <a id="user-content-mpi-workshop" class="anchor" href="#mpi-workshop" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MPI Workshop:</h4>

    <p><a href="https://sites.google.com/view/mpi-workshop" rel="nofollow">https://sites.google.com/view/mpi-workshop</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619722913.0
APPIAN-PET/APPIAN:
  data_format: 2
  description: 'APPIAN is an open-source automated software pipeline for analyzing
    PET images in conjunction with MRI. The goal of APPIAN is to make PET tracer kinetic
    data analysis easy for users with moderate computing skills and to facilitate
    reproducible research. '
  filenames:
  - build/Singularity
  - build/Singularity.base
  full_name: APPIAN-PET/APPIAN
  latest_release: v2.0.2
  readme: "<h1>\n<a id=\"user-content-appian\" class=\"anchor\" href=\"#appian\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>APPIAN</h1>\n\
    <h1>\n<a id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Table of Contents</h1>\n<ol>\n<li><a href=\"#introduction\">Introduction</a></li>\n\
    <li><a href=\"#installation\">Installation</a></li>\n<li>\n<a href=\"#documentation\"\
    >Documentation</a><br>\n3.1 <a href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/USERGUIDE.md\"\
    >User Guide</a><br>\n3.2 <a href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/CONTRIBUTING.md\"\
    >Developer Guide</a>\n</li>\n<li><a href=\"#publications\">Publications</a></li>\n\
    <li><a href=\"#getting-help\">Getting Help</a></li>\n<li><a href=\"#about-us\"\
    >About us</a></li>\n<li><a href=\"#terms-and-conditions\">Terms and Conditions</a></li>\n\
    </ol>\n<h2>\n<a id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Introduction</h2>\n<p>The APPIAN pipeline is implemented in Python\
    \ using the <a href=\"http://nipype.readthedocs.io/en/latest/\" rel=\"nofollow\"\
    >Nipype</a> library. Although the core of the code is written in Python, the pipeline\
    \ can use tools or incorporate modules written in any programming language. The\
    \ only condition is that the tools must be capable of being run from a command\
    \ line with well-defined inputs and outputs. In this sense, APPIAN is  language\
    \ agnostic.</p>\n<h4>\n<a id=\"user-content-cost\" class=\"anchor\" href=\"#cost\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Cost</h4>\n<p>APPIAN is 100% free and open-source, but in exchange\
    \ we would greatly appreciate your feedback, whether it be as bug reports, pull\
    \ requests to add new features, questions on our <a href=\"https://groups.google.com/forum/#!forum/appian-users\"\
    \ rel=\"nofollow\">mailing list</a>, or suggestions on how to improve the documentation\
    \ or the code. You can even just send us an email to let us know what kind of\
    \ project you are working on!</p>\n<h2>\n<a id=\"user-content-installation\" class=\"\
    anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p><code>APPIAN</code>\
    \ is currently only available through <a href=\"https://docs.docker.com/\" rel=\"\
    nofollow\">Docker</a>. Docker is a platform for creating containers that package\
    \ a given software in a complete filesystem that contains everything it needs\
    \ to run, and ensures that the software can always be run in the same environment.\
    \ This means that all of the dependencies required by <code>APPIAN</code> are\
    \ within its Docker container (no need to fumble about trying to compile obscure\
    \ libraries). However, it also means that you will need to install Singularity\
    \ or Docker before proceeding. Don\u2019t worry it\u2019s very easy (except maybe\
    \ for Windows). For a guide on how to install Docker on Ubuntu, Debian, Mac, Windows,\
    \ or other operating system, please <a href=\"https://docs.docker.com/install/\"\
    \ rel=\"nofollow\">visit this link</a> and [Singularity][link_singularityinstall].</p>\n\
    <p>The pipeline is implemented in Python using the <a href=\"https://nipype.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">Nipype</a> library. Although the core is coded in Python, the\
    \ pipeline can use tools or incorporate modules written in any programming language.\
    \ The only condition is that these tools must be run from a command line, with\
    \ well-defined inputs and outputs. In this sense, <code>APPIAN</code> is  language\
    \ agnostic.\nOnce Docker or Singularity is installed, simply run the following\
    \ command line on your terminal:</p>\n<pre><code>docker pull tffunck/appian:latest-dev\n\
    \nsingularity pull shub://APPIAN-PET/APPIAN:latest\n</code></pre>\n<p>That\u2019\
    s it, <code>APPIAN</code> is installed on your computer.</p>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<h3>\n<a\
    \ id=\"user-content-developers\" class=\"anchor\" href=\"#developers\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Developers</h3>\n\
    <p>For those interested in extending or contributing to APPIAN please check out\
    \ our <a href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/CONTRIBUTING.md\"\
    >developer guide</a>.</p>\n<h3>\n<a id=\"user-content-users\" class=\"anchor\"\
    \ href=\"#users\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Users</h3>\n<p>For more information please read our\
    \ <a href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/USERGUIDE.md\"\
    >user guide</a>.</p>\n<h3>\n<a id=\"user-content-developers-1\" class=\"anchor\"\
    \ href=\"#developers-1\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Developers</h3>\n<p>For those interested in\
    \ extending or contributing to APPIAN please check out our <a href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/CONTRIBUTING.md\"\
    >contributors guidelines</a>.</p>\n<h2>\n<a id=\"user-content-publications\" class=\"\
    anchor\" href=\"#publications\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Publications</h2>\n<ol>\n<li>\n<p>Funck\
    \ T, Larcher K, Toussaint PJ, Evans AC, Thiel A (2018) APPIAN: Automated Pipeline\
    \ for PET Image Analysis. <em>Front Neuroinform</em>. PMCID: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6178989/\"\
    \ rel=\"nofollow\">PMC6178989</a>, DOI: <a href=\"https://doi.org/10.3389/fninf.2018.00064\"\
    \ rel=\"nofollow\">10.3389/fninf.2018.00064</a></p>\n</li>\n<li>\n<p>APPIAN automated\
    \ QC (<em>in preparation</em>)</p>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-getting-help\"\
    \ class=\"anchor\" href=\"#getting-help\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting help</h2>\n<p>If you\
    \ get stuck or don't know how to get started please send a mail to the APPIAN\
    \ mailing list :\n<a href=\"https://groups.google.com/forum/#!forum/appian-users\"\
    \ rel=\"nofollow\">https://groups.google.com/forum/#!forum/appian-users</a></p>\n\
    <p>For bugs, please post <a href=\"#https://github.com/APPIAN-PET/APPIAN/issues\"\
    >here</a> on the Github repository.</p>\n<p>To join the discussion for APPIAN\
    \ development, join our developers mailing list :\n<a href=\"https://groups.google.com/forum/#!forum/appian-dev\"\
    \ rel=\"nofollow\">https://groups.google.com/forum/#!forum/appian-dev</a></p>\n\
    <h2>\n<a id=\"user-content-about-us\" class=\"anchor\" href=\"#about-us\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>About\
    \ us</h2>\n<p>Thomas Funck, PhD Candidate (<a href=\"mailto:thomas.funck@mail.mcgill.ca\"\
    >thomas.funck@mail.mcgill.ca</a>)<br>\nKevin Larcher, MSc Eng.<br>\nPaule-Joanne\
    \ Toussaint, PhD</p>\n<h2>\n<a id=\"user-content-terms-and-conditions\" class=\"\
    anchor\" href=\"#terms-and-conditions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Terms and Conditions</h2>\n<p>Copyright\
    \ 2017 Thomas Funck, Kevin Larcher</p>\n<p>Permission is hereby granted, free\
    \ of charge, to any person obtaining a copy of this software and associated documentation\
    \ files (the \"Software\"), to deal in the Software without restriction, including\
    \ without limitation the rights to use, copy, modify, merge, publish, distribute,\
    \ sublicense, and/or sell copies of the Software, and to permit persons to whom\
    \ the Software is furnished to do so, subject to the following conditions:</p>\n\
    <p>The above copyright notice and this permission notice shall be included in\
    \ all copies or substantial portions of the Software.</p>\n<p>THE SOFTWARE IS\
    \ PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\
    \ BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\
    \ PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\
    \ BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\
    \ CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\
    \ SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>\n<pre><code>\n</code></pre>\n"
  stargazers_count: 23
  subscribers_count: 5
  topics:
  - neuroscience
  - neuroimaging
  - pipeline
  - automation
  - reproducible-research
  - openscience
  updated_at: 1619163843.0
ARPA-SIMC/smnd:
  data_format: 2
  description: Software for Meteorology, Normally Distributed
  filenames:
  - Singularity.smnd-run
  - Singularity.mistral
  full_name: ARPA-SIMC/smnd
  latest_release: v2.11
  readme: '<h1>

    <a id="user-content-smnd" class="anchor" href="#smnd" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SMND</h1>

    <h2>

    <a id="user-content-software-for-meteorology-normally-distributed" class="anchor"
    href="#software-for-meteorology-normally-distributed" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Software for Meteorology,
    Normally Distributed</h2>

    <blockquote>

    <p>the software for sure, the meteorological data not really.</p>

    </blockquote>

    <p>SMND is a helper package for simplifying the build and the deployment

    of a collection of meteorological software packages, mainly developed

    by <a href="http://www.arpa.emr.it/sim" rel="nofollow">Arpae-SIMC</a>. The current
    version is

    relatively stable, including the universal binary package.</p>

    <p>The software packages involved, all open source and freely

    redistributable, are:</p>

    <ul>

    <li>

    <a href="https://confluence.ecmwf.int/display/ECC/ecCodes+Home" rel="nofollow">eccodes</a>

    from ECMWF</li>

    <li><a href="https://github.com/ARPA-SIMC/wreport">wreport</a></li>

    <li><a href="https://github.com/ARPA-SIMC/bufr2netcdf">bufr2netcdf</a></li>

    <li><a href="https://github.com/ARPA-SIMC/dballe">DB.All-e</a></li>

    <li><a href="https://github.com/ARPA-SIMC/arkimet">arkimet</a></li>

    <li><a href="https://github.com/ARPA-SIMC/libsim">libsim</a></li>

    </ul>

    <h3>

    <a id="user-content-building-the-software-from-source" class="anchor" href="#building-the-software-from-source"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    the software from source</h3>

    <p>For building autonomously the software collection you can follow the

    guidelines in the <a href="doc/buildfromsource.md">corresponding page</a>.</p>

    <h3>

    <a id="user-content-deploying-the-software" class="anchor" href="#deploying-the-software"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deploying
    the software</h3>

    <p>If you do not want to build the packages on your own, different

    approaches are possible for quickly deploying precompiled binaries:</p>

    <ul>

    <li>The quick and universal way, <a href="doc/unibin.md">the universal binary

    package</a> (no need to be the system administrator).</li>

    <li>Running from a <a href="doc/singularity.md">singularity container</a>

    (requires agreement with the system administrator).</li>

    <li>Installing in a supported distribution (CentOS/Fedora) from <a href="doc/copr.md">copr

    repository</a> (requires to BE the system administrator).</li>

    </ul>

    <h3>

    <a id="user-content-notes-for-cosmo-model-users" class="anchor" href="#notes-for-cosmo-model-users"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Notes
    for COSMO model users</h3>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1619542108.0
ATNF/yandasoft:
  data_format: 2
  description: Astronomical Calibration and Imaging Software
  filenames:
  - deploy/singularity/Singularity.openmpi
  full_name: ATNF/yandasoft
  latest_release: null
  readme: '<h1>

    <a id="user-content-yandasoft" class="anchor" href="#yandasoft" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>YandaSoft</h1>

    <p>Yandasoft is a suite of applications and software developed by CSIRO for the
    calibration and imaging of Interferometric Radio Telescope data.</p>

    <h2>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>A git submodule for the full calibration and imaging documentation is included

    in the docs sibdirectory. On an initial clone of this repository you have to

    run, <code>git submodule init</code> and <code>git submodule update</code> to
    obtain the latest

    versions. Also you must have the sphinx document tool installed and run <code>make
    html</code> to generate the documentation.</p>

    <h2>

    <a id="user-content-in-this-version" class="anchor" href="#in-this-version" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>In this version</h2>

    <p>This release of the software is the first and consequently <em>beta</em> release.
    It contains the (at least) the following applications:</p>

    <h3>

    <a id="user-content-measurement-set-creation-and-manipulation" class="anchor"
    href="#measurement-set-creation-and-manipulation" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Measurement Set creation and manipulation</h3>

    <ul>

    <li>

    <code>csimulator</code>: simulation of visibilities.</li>

    <li>

    <code>ccontsubtract</code>: continuum subtraction.</li>

    <li>

    <code>mslist</code>: measurement set interogation.</li>

    <li>

    <code>msconcat</code>: concatenation of measurement sets.</li>

    <li>

    <code>msmerge</code>: merging of measurement sets.</li>

    </ul>

    <h3>

    <a id="user-content-calibration-tools" class="anchor" href="#calibration-tools"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Calibration
    tools</h3>

    <ul>

    <li>

    <code>cbpcalibrator</code>: bandpass calibrator.</li>

    <li>

    <code>ccalibrator</code>: for performing gain calibration</li>

    <li>

    <code>ccalapply</code>: for the application of calibration solutions.</li>

    </ul>

    <h3>

    <a id="user-content-imaging-tasks" class="anchor" href="#imaging-tasks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Imaging tasks</h3>

    <ul>

    <li>

    <code>cimager</code>: Original ASKAP imager.</li>

    <li>

    <code>imager</code>: New imager - permits more parallisation options.</li>

    <li>

    <code>linmos</code>: Linear mosaicking of images</li>

    </ul>

    <h3>

    <a id="user-content-pipeline-and-analysis-tasks" class="anchor" href="#pipeline-and-analysis-tasks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    and Analysis tasks</h3>

    <ul>

    <li>

    <code>msplit</code>: Manipulate measurement sets</li>

    <li>

    <code>cmodel</code>: Generate model images from component lists</li>

    <li>

    <code>selavy</code>: Source detection tools</li>

    </ul>

    <h2>

    <a id="user-content-about-yandasoft" class="anchor" href="#about-yandasoft" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>About Yandasoft</h2>

    <p>These tasks were originally developed to form the real-time calibration and
    imaging pipeline for the ASKAP telescope. In order to distribute this software
    more widely we have extracted these tools from the main codebase of the telescope
    system and distributed it separately.</p>

    <h3>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h3>

    <p>The dependencies are listed in detail in the INSTALL.txt file. But it should
    be noted that there are internal "ASKAP" dependencies that are required. They
    are all public and are automatically pulled from their respective repositories
    by the included <code>build_all.sh</code> script.</p>

    <h3>

    <a id="user-content-how-to-get-it" class="anchor" href="#how-to-get-it" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>How to get it</h3>

    <p>Yadasoft and its required ASKAP dependencies are available from the CSIRO bitbucket
    server at:</p>

    <ul>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/lofar-common.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/lofar-common.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/lofar-blob.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/lofar-blob.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/base-askap.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/base-askap.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/base-logfilters.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/base-logfilters.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/base-imagemath.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/base-imagemath.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/base-scimath.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/base-scimath.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/base-askapparallel.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/base-askapparallel.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/base-accessors.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/base-accessors.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/yandasoft.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/yandasoft.git</a></li>

    </ul>

    <p>There are some extra tasks available from:</p>

    <ul>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/askap-pipelinetasks.git"
    rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/askap-pipelinetasks.git</a></li>

    <li><a href="https://bitbucket.csiro.au/scm/askapsdp/askap-analysis.git" rel="nofollow">https://bitbucket.csiro.au/scm/askapsdp/askap-analysis.git</a></li>

    </ul>

    '
  stargazers_count: 9
  subscribers_count: 13
  topics:
  - astronomy
  - astronomical-algorithms
  updated_at: 1619096304.0
AdamWilsonLab/docker_geospatial_plus:
  data_format: 2
  description: Docker image
  filenames:
  - Singularity.latest
  full_name: AdamWilsonLab/docker_geospatial_plus
  latest_release: null
  readme: '<h1>

    <a id="user-content-geospatial-plus" class="anchor" href="#geospatial-plus" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Geospatial Plus</h1>

    <p>Building on the versioned geospatial Rocker image.</p>

    <h1>

    <a id="user-content-github-actions" class="anchor" href="#github-actions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Github Actions</h1>

    <p>This repository uses GitHub Actions to test the docker image prior to making
    it available as a GitHub package.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620074754.0
AmpSeq/AmpSeq:
  data_format: 2
  description: Computational analysis of AmpSeq data for targeted comparative genomics
  filenames:
  - Singularity/Singularity.AmpSeq
  full_name: AmpSeq/AmpSeq
  latest_release: null
  readme: "<p><a href=\"https://singularity-hub.org/collections/2390\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-ampseq\" class=\"\
    anchor\" href=\"#ampseq\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>AmpSeq</h1>\n<p>Amplicon Sequencing (AmpSeq)\
    \ is a practical, intuitive strategy with a semi-automated computational pipeline\
    \ for analysis of highly multiplexed PCR-derived sequences. Amplicons can target\
    \ from a single nucleotide to the upper limit of the sequencing platform. The\
    \ flexibility of AmpSeq\u2019s wet lab methods make it a tool of broad interest\
    \ for diverse species, and AmpSeq excels in flexibility, high-throughput, low-cost,\
    \ accuracy, and semi-automated analysis. Here we describe the procedure to output\
    \ data out of an AmpSeq project, with emphasis on the bioinformatics pipeline\
    \ to generate SNPs, haplotypes and presence/absence variants in a set of diverse\
    \ genotypes.</p>\n"
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1566940831.0
Aphoh/temp_tc:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: Aphoh/temp_tc
  latest_release: null
  readme: '<h1>

    <a id="user-content-transactive_control" class="anchor" href="#transactive_control"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>transactive_control</h1>

    <p>Code meant to support and simulate the Social Game that will be launched in
    2020. Elements of transactive control and behavioral engineering will be tested
    and designed here</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <ol>

    <li>Clone the repo</li>

    <li>Install <a href="https://dvc.org/doc/install" rel="nofollow">dvc</a> (with
    google drive support)</li>

    </ol>

    <ul>

    <li>On linux this is <code>pip install ''dvc[gdrive]''</code>

    </li>

    </ul>

    <ol start="3">

    <li>Install Docker, if you have not already.</li>

    <li>Run <code>python3 -m dvc remote add -d gdrive gdrive://1qaTn6IYd3cpiyJegDwwEhZ3LwrujK3_x</code>

    </li>

    <li>Run <code>python3 -m dvc pull</code>

    </li>

    </ol>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ol>

    <li>Run <code>./run.sh</code> from the root of the repo. This will put you in
    a shell in the docker container with the <code>rl_algos/logs</code> directory
    mounted</li>

    <li>Run <code>python3 StableBaselines.py sac test_experiment</code> in the docker
    container to start an experiment with the name <code>test_experiment</code>

    </li>

    <li>Run <code>tensorboard --logdir rl_algos/logs</code> from outside the docker
    container to view the logs</li>

    </ol>

    <hr>

    <h3>

    <a id="user-content-12202020" class="anchor" href="#12202020" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>12/20/2020</h3>

    <p>This repository has been cleaned and updated for use. It contains: (1) The
    OpenAI gym environment "OfficeLearn", in the "gym-socialgame" folder, and (2)
    implementations of Reinforcement learning algorithms in "rl_algos" folder. In
    the "simulations" folder are various datasets for setting up and training models
    associated with the gym simulation.</p>

    <h3>

    <a id="user-content-912020" class="anchor" href="#912020" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>9/1/2020</h3>

    <p>The most recent running of the code involves navigating to the rl_algos/ directory,
    then running the python command for the vanilla version:</p>

    <p>python StableBaselines.py sac</p>

    <p>Adding in the planning model can be done with the following flags:</p>

    <p>python StableBaselines.py sac --planning_steps=10 --planning_model=Oracle --num_steps=10000</p>

    <p>Please see transactive_control/gym-socialgame/gym_socialgame/envs for files
    pertaining to the setup of the environment. The socialgame_env.py contains a lot
    of the information necessary for understanding how the agent steps through the
    environment. The reward.py file contains information on the variety of reward
    types available for testing. agents.py contains information on the deterministic
    people that we created for our simulation.</p>

    <h3>

    <a id="user-content-gym-socialgame" class="anchor" href="#gym-socialgame" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>gym-socialgame</h3>

    <p>OpenAI Gym environment for a social game.</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1621784680.0
ArjitM/CellMorphology:
  data_format: 2
  description: Python code to identify/analyze cells from microscopic stack images.
  filenames:
  - Singularity.cellprof2
  full_name: ArjitM/CellMorphology
  latest_release: null
  readme: '<h1>

    <a id="user-content-cellmorphology" class="anchor" href="#cellmorphology" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>CellMorphology</h1>

    <p><strong>Under development</strong></p>

    <p>Python code to binarize noisy input images of microscopic images for further
    cell morphology analysis.</p>

    <p>Attempted localized edge detection, noise elimination, and edge propagation.

    Clustering functional

    segmentation needs touch-ups, constrictions identified OK, bends pending

    Stack image collation needs review</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - python
  - image-processing
  - cell-morphology-analysis
  - matlab
  updated_at: 1620661441.0
ArnaudBelcour/deepec-singularity:
  data_format: 2
  description: Singularity recipe for DeepEC.
  filenames:
  - Singularity
  full_name: ArnaudBelcour/deepec-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-deepec-singularity-recipe" class="anchor" href="#deepec-singularity-recipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>DeepEC
    Singularity recipe</h1>

    <p>Singularity recipe for <a href="https://bitbucket.org/kaistsystemsbiology/deepec/src/master/"
    rel="nofollow">DeepEC</a>.</p>

    <p>First download the recipe, for example with a git clone of this repository.</p>

    <p>Singularity container can be created with the command (this command needs admin
    right):</p>

    <pre><code>singularity build deepec.sif Singularity

    </code></pre>

    <p>The deepec.sif file size is around 1.3 GB.</p>

    <p>Then DeepEC can be called with:</p>

    <pre><code>singularity exec deepec.sif python /programs/deepec/deepec.py -i fasta_file
    -o output_folder

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1575494893.0
ArnaudBelcour/metage2metabo-metacom_singularity:
  data_format: 2
  description: Singularity recipe for Metage2Metabo metacom.
  filenames:
  - Singularity
  full_name: ArnaudBelcour/metage2metabo-metacom_singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-mlcontainer" class="anchor" href="#mlcontainer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MLContainer</h1>

    <p>Singularity container and conda environments for ML based analysis @ HEPHY</p>

    <h2>

    <a id="user-content-machine-learning-hats" class="anchor" href="#machine-learning-hats"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>machine-learning-hats</h2>

    <p><a href="https://github.com/FNALLPC/machine-learning-hats">https://github.com/FNALLPC/machine-learning-hats</a></p>

    <p>On HEPGPU01 its easier to use the conda environment</p>

    <pre><code>conda env create --file environment-gpu.yml

    </code></pre>

    <p>On CLIP its better to use the container. It will

    be installed after the shutdown.</p>

    <pre><code>build_ml-hats.sh

    </code></pre>

    <p>Run the shell container</p>

    <pre><code>run_ml-hats.sh

    </code></pre>

    <p>Run a script</p>

    <pre><code>run_ml-hats.sh &lt;scripts&gt;

    </code></pre>

    <h2>

    <a id="user-content-deepjetcore" class="anchor" href="#deepjetcore" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DeepJetCore</h2>

    <p><a href="https://github.com/DL4Jets/DeepJetCore">https://github.com/DL4Jets/DeepJetCore</a></p>

    <h1>

    <a id="user-content-container-for-deepjetcore" class="anchor" href="#container-for-deepjetcore"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    for DeepJetCore</h1>

    <p>Build the container (only on  HEPGPU01)</p>

    <pre><code>build_deepjet3.sh

    </code></pre>

    <p>Run the container</p>

    <pre><code>run_deepjet3.sh

    </code></pre>

    <h2>

    <a id="user-content-todo" class="anchor" href="#todo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TODO</h2>

    <p>Try container on CLIP</p>

    <h2>

    <a id="user-content-open-points" class="anchor" href="#open-points" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Open Points</h2>

    <h3>

    <a id="user-content-conda-environments-on-clip" class="anchor" href="#conda-environments-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conda
    Environments on CLIP</h3>

    <p>CLIP provides already installations of Conda</p>

    <pre><code>ml add Anaconda3/19.10

    </code></pre>

    <p>Nevertheless I could not build reliable environments due to limited

    quota. Also trying to move the correspondig directories to /scratch-cbe/users

    was not successfull.</p>

    <p>At this point it seems better to use the conda environment inside a container.</p>

    <h3>

    <a id="user-content-building-container-on-clip" class="anchor" href="#building-container-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on CLIP</h3>

    <p>Building containers on CLIP has two problems</p>

    <ul>

    <li>root rights required to build container (possible solution fakeroot)</li>

    <li>docker container could be transformed in singularity container, but

    the filesystems (BeeGEFS) do not fullfill the singularity requirements</li>

    </ul>

    <p>The best way seems to use CI with Jenkis ans the local singularity hub. This

    has to be understood in the future.</p>

    <h3>

    <a id="user-content-building-container-on-hepgpu01" class="anchor" href="#building-container-on-hepgpu01"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on HEPGPU01</h3>

    <p>Root rights are required in case container are build from scratch. Fakeroot

    would be a possible workaround. Has to be tried.</p>

    <p>Another possibility is to use "sudo".</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616028008.0
ArnaudBelcour/phylogeny-singularity:
  data_format: 2
  description: Singularity recipe for some phylogeny tools.
  filenames:
  - Singularity
  full_name: ArnaudBelcour/phylogeny-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-phylogeny" class="anchor" href="#singularity-recipe-for-phylogeny"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe for phylogeny</h1>

    <p>This singularity recipe contains:</p>

    <ul>

    <li>BEAGLE library v3.2.0 (PRE-RELEASE)</li>

    <li>BEAST v1.10.4</li>

    <li>FastME 2.1.6.1</li>

    <li>FastTree 2.1</li>

    <li>IQ-TREE version 2.0-rc1 (November 21, 2019)</li>

    <li>MrBayes 3.2.7</li>

    <li>PhyML v3.3.20190909</li>

    <li>RAxML-NG v0.9.0</li>

    </ul>

    <p>Using the Singularity recipe, you can create a Singularity container:</p>

    <pre><code>sudo singularity build phylogeny.sif Singularity

    </code></pre>

    <p>These tools can be called using:</p>

    <pre><code>singularity exec phylogeny.sif tool_exec

    </code></pre>

    <p>The <strong>tool_exec</strong> to use:</p>

    <ul>

    <li><strong>beast</strong></li>

    <li><strong>fastme</strong></li>

    <li>

    <strong>FastTree</strong>, <strong>FastTreeDbl</strong> and <strong>FastTreeMP</strong>
    for FastTree</li>

    <li><strong>iqtree</strong></li>

    <li>

    <strong>mb</strong> for MrBayes</li>

    <li>

    <strong>phyml</strong> and <strong>phyml-mpi</strong> for PhyML</li>

    <li><strong>raxml-ng</strong></li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1578956805.0
AuReMe/metage2metabo:
  data_format: 2
  description: From annotated genomes to metabolic screening in large scale microbiotas
  filenames:
  - recipes/Singularity
  full_name: AuReMe/metage2metabo
  latest_release: 1.5.0
  readme: "<p><a href=\"https://pypi.org/project/Metage2Metabo/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/68c19eb988f7da820e489e7d773438373c65af075fe846cb90e18836a7d7f9d4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d6574616765326d657461626f2e737667\"\
    \ alt=\"PyPI version\" data-canonical-src=\"https://img.shields.io/pypi/v/metage2metabo.svg\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://github.com/AuReMe/metage2metabo/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/fccd34831109fd6bdad80ef75ccdd11796acfad9808526a620456def8d9d9352/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f417552654d652f6d6574616765326d657461626f2e737667\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/AuReMe/metage2metabo.svg\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://github.com/AuReMe/metage2metabo/actions\"\
    ><img src=\"https://github.com/AuReMe/metage2metabo/workflows/Python%20package/badge.svg\"\
    \ alt=\"Actions Status\" style=\"max-width:100%;\"></a> <a href=\"https://metage2metabo.readthedocs.io/en/latest/?badge=latest\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/88095555c8fdcf3d56ae1cc3261918958b072a5308cc1d5113522bc284afd1b3/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6d6574616765326d657461626f2f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/metage2metabo/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://doi.org/10.7554/eLife.61968\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9e55e07ba04fd05d5e7a35a3dadc73af2472409b8403497e7056fb037f5e7875/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f692d31302e373535342f654c6966652e36313936382d626c756576696f6c65742e737667\"\
    \ alt=\"\" data-canonical-src=\"https://img.shields.io/badge/doi-10.7554/eLife.61968-blueviolet.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-m2m---metage2metabo\"\
    \ class=\"anchor\" href=\"#m2m---metage2metabo\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>M2M - metage2metabo</h1>\n<p>Metage2metabo\
    \ is a Python3 (Python &gt;= 3.6, tested with 3.6 and 3.7) tool to perform graph-based\
    \ metabolic analysis starting from annotated genomes (<strong>reference genomes\
    \ or metagenome-assembled genomes</strong>). It uses <em>Pathway Tools</em> in\
    \ a automatic and parallel way to <strong>reconstruct metabolic networks</strong>\
    \ for a large number of genomes. The obtained metabolic networks are then <strong>analyzed\
    \ individually and collectively</strong> in order to get the <strong>added value\
    \ of metabolic cooperation in microbiota over individual metabolism</strong> and\
    \ to <strong>identify and screen interesting organisms</strong> among all.</p>\n\
    <p>m2m can be used as a whole workflow (<code>m2m workflow</code>, <code>m2m metacom</code>)\
    \ or steps can be performed individually (<code>m2m recon</code> , <code>m2m iscope</code>\
    \ , <code>m2m cscope</code>, <code>m2m addedvalue</code>, <code>m2m mincom</code>,\
    \ <code>m2m seeds</code>).</p>\n<p><strong>If you use M2M, please cite</strong></p>\n\
    <p>Belcour* A, Frioux* C, Aite M, Bretaudeau A, Hildebrand F, Siegel A. Metage2Metabo,\
    \ microbiota-scale metabolic complementarity for the identification of key species.\
    \ eLife 2020;9:e61968 <a href=\"https://doi.org/10.7554/eLife.61968\" rel=\"nofollow\"\
    >https://doi.org/10.7554/eLife.61968</a>.</p>\n<p>For a summary of M2M and its\
    \ applications, you can take a look at these <a href=\"https://hal.inria.fr/hal-03151934/document\"\
    \ rel=\"nofollow\">poster-slides</a>, presented during the <a href=\"https://jobim2020.sciencesconf.org/?forward-action=index&amp;forward-controller=index&amp;lang=en\"\
    \ rel=\"nofollow\">JOBIM 2020 conference</a>.</p>\n<h2>\n<a id=\"user-content-table-of-contents\"\
    \ class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Table of contents</h2>\n<ul>\n\
    <li>\n<a href=\"#m2m---metage2metabo\">M2M - metage2metabo</a>\n<ul>\n<li><a href=\"\
    #table-of-contents\">Table of contents</a></li>\n<li><a href=\"#general-information-about-the-modelling\"\
    >General information about the modelling</a></li>\n<li><a href=\"#license\">License</a></li>\n\
    <li><a href=\"#documentation\">Documentation</a></li>\n<li><a href=\"#technologies\"\
    >Technologies</a></li>\n<li><a href=\"#requirements\">Requirements</a></li>\n\
    <li>\n<a href=\"#installation\">Installation</a>\n<ul>\n<li><a href=\"#installation-with-pip\"\
    >Installation with pip</a></li>\n<li><a href=\"#availability-on-docker-and-singularity\"\
    >Availability on Docker and Singularity</a></li>\n</ul>\n</li>\n<li><a href=\"\
    #m2m-commands\">M2M commands</a></li>\n<li><a href=\"#analysis-of-the-minimal-solutions\"\
    >Analysis of the minimal solutions</a></li>\n<li><a href=\"#release-notes\">Release\
    \ Notes</a></li>\n<li><a href=\"#additional-features\">Additional features</a></li>\n\
    <li><a href=\"#citation\">Citation</a></li>\n<li><a href=\"#article-data\">Article\
    \ data</a></li>\n<li><a href=\"#authors\">Authors</a></li>\n<li><a href=\"#acknowledgement\"\
    >Acknowledgement</a></li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-general-information-about-the-modelling\"\
    \ class=\"anchor\" href=\"#general-information-about-the-modelling\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>General\
    \ information about the modelling</h2>\n<p>M2M has two main dependencies for modelling\
    \ metabolic networks: <a href=\"https://github.com/cfrioux/MeneTools\">MeneTools</a>\
    \ and <a href=\"https://github.com/cfrioux/miscoto\">Miscoto</a>. Accordingly\
    \ metabolic models in M2M follow the producibility in metabolic networks as defined\
    \ by the <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/15712108\" rel=\"nofollow\"\
    >network expansion</a> algorithm.\nMainly, two rules are followed:</p>\n<ul>\n\
    <li>a <em>recursive rule</em>: the products of a reactions are producible if <strong>all</strong>\
    \ reactants of this reaction are themselves producible</li>\n<li>an <em>initiation\
    \ rule</em>: producibility is initiated by the presence of nutrients, called <em>seeds</em>.</li>\n\
    </ul>\n<p>A metabolite that is producible from a set of nutrients is described\
    \ as being \"in the scope of the seeds\".\nThe computation is made using logic\
    \ solvers (Answer Set Programming). The present modelling ignores the stoichiometry\
    \ of reactions (2A + B --&gt; C is considered equivalent to A + B --&gt; C), and\
    \ is therefore suited to non-curated or draft metabolic networks, as the ones\
    \ built using M2M with the PathoLogic software of <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5036846/pdf/bbv079.pdf\"\
    \ rel=\"nofollow\">Pathway Tools</a> handled by <a href=\"https://github.com/AuReMe/mpwt\"\
    >Mpwt</a>. Many works have relied on network expansion to study organisms (<a\
    \ href=\"http://doi.wiley.com/10.1111/tpj.12627\" rel=\"nofollow\">here</a>, <a\
    \ href=\"https://dx.plos.org/10.1371/journal.pcbi.1000049\" rel=\"nofollow\">here</a>\
    \ or <a href=\"http://dx.plos.org/10.1371/journal.pcbi.1005276\" rel=\"nofollow\"\
    >there</a>) and communities (<a href=\"https://academic.oup.com/bioinformatics/article/34/17/i934/5093211\"\
    \ rel=\"nofollow\">here</a>, <a href=\"https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4786-7\"\
    \ rel=\"nofollow\">here</a>, or <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/18546499\"\
    \ rel=\"nofollow\">here</a>). It has been <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/19425125\"\
    \ rel=\"nofollow\">compared</a>, <a href=\"https://www.cambridge.org/core/product/identifier/S1471068418000455/type/journal_article\"\
    \ rel=\"nofollow\">combined</a> to steady-state modelling (Flux Balance Analysis).</p>\n\
    <h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>This project is licensed under the GNU General Public License - see the <a\
    \ href=\"https://github.com/AuReMe/metage2metabo/blob/master/LICENSE\">LICENSE.md</a>\
    \ file for details.</p>\n<h2>\n<a id=\"user-content-documentation\" class=\"anchor\"\
    \ href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Documentation</h2>\n<p>A more detailled documentation\
    \ is available at: <a href=\"https://metage2metabo.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">https://metage2metabo.readthedocs.io</a>.</p>\n<h2>\n<a id=\"\
    user-content-technologies\" class=\"anchor\" href=\"#technologies\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Technologies</h2>\n\
    <p>Python 3 (Python 3.6 is tested). M2M uses a certain number of Python dependencies.\
    \ An example of all these dependencies working for Ubuntu 18.04 is available in\
    \ <a href=\"https://github.com/AuReMe/metage2metabo/blob/master/requirements.txt\"\
    >requirements.txt</a>.\nThey can be installed with:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>pip install -r requirements.txt --no-cache-dir</pre></div>\n\
    <p>In particular, m2m relies on:</p>\n<ul>\n<li>\n<a href=\"https://github.com/AuReMe/mpwt\"\
    >mpwt</a> to automatize metabolic network reconstruction with Pathway Tools</li>\n\
    <li>\n<a href=\"https://github.com/AuReMe/padmet\">padmet</a> to manage metabolic\
    \ networks</li>\n<li>\n<a href=\"https://github.com/cfrioux/MeneTools\">menetools</a>\
    \ to analyze individual metabolic capabilities using logic programming</li>\n\
    <li>\n<a href=\"https://github.com/cfrioux/miscoto\">miscoto</a> to analyze collective\
    \ metabolic capabilities and select communities within microbiota using logic\
    \ programming</li>\n</ul>\n<p>Also, m2m_analysis relies on other packages:</p>\n\
    <ul>\n<li>\n<a href=\"https://github.com/networkx/networkx\">networkx</a> to create\
    \ graph from miscoto results</li>\n<li>\n<a href=\"https://github.com/etetoolkit/ete\"\
    >ete3</a> to add taxonomy information on the graph if you used mpwt taxon file</li>\n\
    <li>\n<a href=\"https://github.com/Aluriak/PowerGrASP\">powergrasp</a> to compress\
    \ networkx graph</li>\n</ul>\n<h2>\n<a id=\"user-content-requirements\" class=\"\
    anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<ul>\n<li>\n<p><a\
    \ href=\"http://bioinformatics.ai.sri.com/ptools/\" rel=\"nofollow\">Pathway Tools</a>\
    \ version 23.0 or higher (free for <a href=\"https://biocyc.org/download-bundle.shtml\"\
    \ rel=\"nofollow\">academic users</a>) is <strong>required for m2m workflow and\
    \ m2m recon</strong></p>\n<ul>\n<li>\n<p>Pathway Tools requirements</p>\n<ul>\n\
    <li>\n<strong>Linux</strong>: Gnome terminal and Libxm4</li>\n</ul>\n<div class=\"\
    highlight highlight-source-shell\"><pre>apt-get update <span class=\"pl-k\">&amp;&amp;</span>\
    \ apt-get install gnome-terminal libxm4</pre></div>\n<ul>\n<li>\n<strong>All OS</strong>:\
    \ <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK279671/\" rel=\"nofollow\">NCBI\
    \ Blast</a> and a ncbirc file in user's home directory\n<ul>\n<li>Install with\
    \ apt-get</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre>apt-get\
    \ update <span class=\"pl-k\">&amp;&amp;</span> apt-get install gnome-terminal\
    \ libxm4 ncbi-blast+ \n<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>[ncbi]\\nData=/usr/bin/data<span class=\"pl-pds\"\
    >\"</span></span> <span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">~</span>/.ncbirc</pre></div>\n\
    <ul>\n<li>Install with a dmg installer on MacOS</li>\n</ul>\n</li>\n</ul>\n</li>\n\
    <li>\n<p>Pathway Tools install</p>\n<ul>\n<li><strong>Linux</strong></li>\n</ul>\n\
    <div class=\"highlight highlight-source-shell\"><pre>chmod +x ./pathway-tools-22.5-linux-64-tier1-install\
    \ \n./pathway-tools-22.5-linux-64-tier1-install </pre></div>\n<p>and follow the\
    \ instructions during the interactive install</p>\n<p><em>For a silent install</em>:\
    \ <code>./pathway-tools-22.5-linux-64-tier1-install --InstallDir your/install/directory/pathway-tools\
    \ --PTOOLS_LOCAL_PATH your/chosen/directory/for/data/ptools --InstallDesktopShortcuts\
    \ 0 --mode unattended</code></p>\n<ul>\n<li><strong>MacOS</strong></li>\n</ul>\n\
    <p>Dmg installer with a graphical interface.</p>\n<ul>\n<li><strong>Warning</strong></li>\n\
    </ul>\n<p>/!\\ For all OS, Pathway Tools must be in <code>$PATH</code>.\nOn Linux\
    \ and MacOS: <code>export PATH=$PATH:your/install/directory/pathway-tools</code>.\n\
    Consider adding Pathway Tools in <code>$PATH</code> permanently by running</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">'</span>export PATH=\"$PATH:your/install/directory/pathway-tools:\"\
    <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">&gt;&gt;</span> <span\
    \ class=\"pl-k\">~</span>/.bashrc</pre></div>\n</li>\n</ul>\n</li>\n<li>\n<p><a\
    \ href=\"http://www.biotec.tu-dresden.de/research/schroeder/powergraphs/download-command-line-tool.html\"\
    \ rel=\"nofollow\">Oog Power Graph Command line tool</a> to create a svg file\
    \ from the compressed graph at the end of m2m_analysis. This tool is a jar file,\
    \ Java is needed to use it.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>Developed\
    \ and tested on Linux (Ubuntu, Fedora, Debian) and MacOs (version 10.14) with\
    \ Python3.6.</p>\n<p>Continuous Integration using GitHub Actions with Python3.6\
    \ and Python3.7 on ubuntu-latest, macos-latest and windows-latest (<a href=\"\
    https://docs.github.com/en/free-pro-team@latest/actions/reference/specifications-for-github-hosted-runners#supported-runners-and-hardware-resources\"\
    >corresponding virtual environment</a>).</p>\n<h3>\n<a id=\"user-content-installation-with-pip\"\
    \ class=\"anchor\" href=\"#installation-with-pip\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation\
    \ with pip</h3>\n<pre><code>pip install Metage2Metabo\n</code></pre>\n<h3>\n<a\
    \ id=\"user-content-availability-on-docker-and-singularity\" class=\"anchor\"\
    \ href=\"#availability-on-docker-and-singularity\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Availability\
    \ on Docker and Singularity</h3>\n<p>Due to Pathway-Tools license, Docker or Singularity\
    \ images are not available publicly.</p>\n<p>But you can create these images by\
    \ using the Dockerfile and Singularity recipes available inside the recipes folder.\n\
    With these files, you can create container with Pathway-Tools and m2m.</p>\n<p>More\
    \ informations in the <a href=\"https://metage2metabo.readthedocs.io/en/latest/install.html#installation-with-docker\"\
    \ rel=\"nofollow\">Docker and Singularity Documentation</a>.</p>\n<h2>\n<a id=\"\
    user-content-m2m-commands\" class=\"anchor\" href=\"#m2m-commands\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>M2M\
    \ commands</h2>\n<p>M2M commands are listed in the <a href=\"https://metage2metabo.readthedocs.io/en/latest/command.html\"\
    \ rel=\"nofollow\">Commands Documentation</a>.</p>\n<pre><code>Copyright (C) Dyliss\
    \ &amp; Pleiade\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\n\
    m2m is free software: you are free to change and redistribute it.\nThere is NO\
    \ WARRANTY, to the extent permitted by law.\n\n\nusage: m2m [-h] [-v]\n      \
    \  {recon,iscope,cscope,addedvalue,mincom,seeds,workflow,metacom,test}\n     \
    \   ...\n\nFrom metabolic network reconstruction with annotated genomes to metabolic\n\
    capabilities screening to identify organisms of interest in a large\nmicrobiota.\
    \ For specific help on each subcommand use: m2m {cmd} --help\n\noptional arguments:\n\
    -h, --help            show this help message and exit\n-v, --version         show\
    \ program's version number and exit\n\nsubcommands:\nvalid subcommands:\n\n{recon,iscope,cscope,addedvalue,mincom,seeds,workflow,metacom,test}\n\
    \    recon               metabolic network reconstruction\n    iscope        \
    \      individual scope computation\n    cscope              community scope computation\n\
    \    addedvalue          added value of microbiota's metabolism over\n       \
    \                 individual's\n    mincom              minimal communtity selection\n\
    \    seeds               creation of seeds SBML file\n    workflow           \
    \ whole workflow\n    metacom             whole metabolism community analysis\n\
    \    test                test on sample data from rumen experiments\n\nRequires:\
    \ Pathway Tools installed and in $PATH, and NCBI Blast\n</code></pre>\n<h2>\n\
    <a id=\"user-content-analysis-of-the-minimal-solutions\" class=\"anchor\" href=\"\
    #analysis-of-the-minimal-solutions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Analysis of the minimal solutions</h2>\n\
    <p>M2M performs a community minimization to find the union and intersection of\
    \ the minimal communities. But it is possible to analyze all the minimal communities.\n\
    M2M has a second command-line, named m2m_analysis that performs this analysis.\
    \ This method is slower than m2m as all sollutions are enumerated.\nThen it creates\
    \ a solutions graph and compresses it in a powergraph. Then it creates visualization\
    \ (html file and optionnaly svg files).</p>\n<p>More information about this command\
    \ in the <a href=\"https://metage2metabo.readthedocs.io/en/latest/m2m_analysis.html\"\
    \ rel=\"nofollow\">m2m_analysis Documentation</a>.</p>\n<pre><code>usage: m2m_analysis\
    \ [-h] [-v] {enum,graph,powergraph,workflow} ...\n\nDetection of key species among\
    \ communities.\n For specific help on each subcommand use: m2m_analysis {cmd}\
    \ --help\n\noptional arguments:\n  -h, --help            show this help message\
    \ and exit\n  -v, --version         show program's version number and exit\n\n\
    subcommands:\n  valid subcommands:\n\n  {enum,graph,powergraph,workflow}\n   \
    \ enum                enumeration using miscoto\n    graph               graph\
    \ creation with enumeration solution\n    powergraph          powergraph creation\
    \ and visualization\n    workflow            whole workflow\n\nOog jar file (http://www.biotec.tu-dresden.de/research/schroeder/powergraphs/download-command-line-tool.html)\
    \ for powergraph svg creation.\n</code></pre>\n<h2>\n<a id=\"user-content-release-notes\"\
    \ class=\"anchor\" href=\"#release-notes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Release Notes</h2>\n<p>Changes\
    \ between version are listed on the <a href=\"https://github.com/AuReMe/metage2metabo/releases\"\
    >release page</a>.</p>\n<h2>\n<a id=\"user-content-additional-features\" class=\"\
    anchor\" href=\"#additional-features\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Additional features</h2>\n<p>M2M\
    \ relies on packages that can also be used independantly with more features:</p>\n\
    <ul>\n<li>\n<a href=\"https://github.com/AuReMe/mpwt\">mpwt</a>: command-line\
    \ and multi-process solutions to run Pathway Tools. Suitable to multiple reconstruction,\
    \ for example genomes of a microbiota</li>\n<li>\n<a href=\"https://github.com/cfrioux/MeneTools\"\
    >menetools</a>: individual metabolic capabilities analysis using graph-based producibility\
    \ criteria</li>\n<li>\n<a href=\"https://github.com/cfrioux/miscoto\">miscoto</a>:\
    \ community selection and metabolic screening in large-scal microbiotas, with\
    \ or without taking a host into account</li>\n</ul>\n<h2>\n<a id=\"user-content-citation\"\
    \ class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Citation</h2>\n<p>Belcour* A,\
    \ Frioux* C, Aite M, Bretaudeau A, Hildebrand F, Siegel A. Metage2Metabo, microbiota-scale\
    \ metabolic complementarity for the identification of key species. eLife 2020;9:e61968\
    \ <a href=\"https://doi.org/10.7554/eLife.61968\" rel=\"nofollow\">https://doi.org/10.7554/eLife.61968</a>.</p>\n\
    <h2>\n<a id=\"user-content-article-data\" class=\"anchor\" href=\"#article-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Article data</h2>\n<p>Data used to create figures and tables are listed\
    \ in the <a href=\"https://github.com/AuReMe/metage2metabo/tree/master/article_data\"\
    >article_data</a> folder, it contains:</p>\n<ul>\n<li>\n<a href=\"https://github.com/AuReMe/metage2metabo/tree/master/article_data/gsmn_characteristics\"\
    >gsmn_characteristics</a>: scripts and tables to show the characteristics of draft\
    \ metabolic networks created by M2M for gut, rumen and diabetes dataset.</li>\n\
    <li>\n<a href=\"https://github.com/AuReMe/metage2metabo/tree/master/article_data/diabetes_study\"\
    >diabetes_study</a>: scripts and tables to create the figures of the diabetes\
    \ analyses in the article.</li>\n</ul>\n<h2>\n<a id=\"user-content-authors\" class=\"\
    anchor\" href=\"#authors\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Authors</h2>\n<p><a href=\"https://cfrioux.github.io/\"\
    \ rel=\"nofollow\">Cl\xE9mence Frioux</a> and <a href=\"https://arnaudbelcour.github.io/blog/\"\
    \ rel=\"nofollow\">Arnaud Belcour</a>, Univ Rennes, Inria, CNRS, IRISA, Rennes,\
    \ France.</p>\n<h2>\n<a id=\"user-content-acknowledgement\" class=\"anchor\" href=\"\
    #acknowledgement\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Acknowledgement</h2>\n<p>People of Pathway Tools (SRI\
    \ International) for their help integrating Pathway Tools with command line and\
    \ multiprocessing in the <a href=\"https://github.com/AuReMe/mpwt\">mpwt</a> package,\
    \ used in M2M.</p>\n"
  stargazers_count: 18
  subscribers_count: 4
  topics:
  - bioinformatics
  - bioinformatics-pipeline
  - metabolic-models
  updated_at: 1621979076.0
AudiovisualMetadataPlatform/gentle-singularity:
  data_format: 2
  description: Builds a singularity container around the forced alignment tool Gentle
  filenames:
  - Singularity.recipe
  full_name: AudiovisualMetadataPlatform/gentle-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-gentle-singularity" class="anchor" href="#gentle-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>gentle-singularity</h1>

    <p>Builds a singularity container around the forced alignment tool Gentle</p>

    '
  stargazers_count: 0
  subscribers_count: 7
  topics: []
  updated_at: 1618627863.0
AudiovisualMetadataPlatform/ina-speech-tools-singularity:
  data_format: 2
  description: INA Speech Tools in a Singularity Container for AMP
  filenames:
  - Singularity.recipe
  full_name: AudiovisualMetadataPlatform/ina-speech-tools-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-ina-speech-tools-singularity" class="anchor" href="#ina-speech-tools-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ina-speech-tools-singularity</h1>

    <p>INA Speech Tools in a Singularity Container for AMP</p>

    <p>To build:</p>

    <pre><code>time singularity build --fakeroot ina-speech-tools-singularity.sif
    Singularity.recipe

    </code></pre>

    <p>To run:</p>

    <pre><code>./ina-speech-tools-singularity.sif &lt;audio file&gt;  &lt;output json&gt;

    </code></pre>

    <p>Both the audio file and the output file should be either in the user''s home
    directory or /tmp.  Other options can be handled by using run-time binding.</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1617321582.0
BIDS-Apps/example:
  data_format: 2
  description: This an example app that can serve as a template.
  filenames:
  - Singularity
  full_name: BIDS-Apps/example
  latest_release: 0.0.7
  readme: "<h2>\n<a id=\"user-content-an-example-bids-app-template-repository\" class=\"\
    anchor\" href=\"#an-example-bids-app-template-repository\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>An example\
    \ BIDS App (template repository)</h2>\n<p>Every BIDS App needs to follow a minimal\
    \ set of command arguments common across\nall of the Apps. This allows users and\
    \ developers to easily use and integrate\nBIDS Apps with their environment.</p>\n\
    <p>This is a minimalist example of a BIDS App consisting of a Dockerfile and a\
    \ simple\nentry point script (written in this case in Python) accepting the standard\
    \ BIDS\nApps command line arguments. This repository can be used as a template\
    \ for new BIDS Apps.</p>\n<p>For more information about the specification of BIDS\
    \ Apps see <a href=\"https://docs.google.com/document/d/1E1Wi5ONvOVVnGhj21S1bmJJ4kyHFT7tkxnV3C23sjIE/\"\
    \ rel=\"nofollow\">here</a>.</p>\n<h3>\n<a id=\"user-content-description\" class=\"\
    anchor\" href=\"#description\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Description</h3>\n<p>This is a placeholder\
    \ for a short description explaining to the user what your App will doing.</p>\n\
    <h3>\n<a id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Documentation</h3>\n<p>Provide a link to the documentation of your\
    \ pipeline.</p>\n<h3>\n<a id=\"user-content-how-to-report-errors\" class=\"anchor\"\
    \ href=\"#how-to-report-errors\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>How to report errors</h3>\n<p>Provide\
    \ instructions for users on how to get help and report errors.</p>\n<h3>\n<a id=\"\
    user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Acknowledgments</h3>\n\
    <p>Describe how would you would like users to acknowledge use of your App in their\
    \ papers (citation, a paragraph that can be copy pasted, etc.)</p>\n<h3>\n<a id=\"\
    user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h3>\n\
    <p>This App has the following command line arguments:</p>\n<pre><code>\tusage:\
    \ run.py [-h]\n\t              [--participant_label PARTICIPANT_LABEL [PARTICIPANT_LABEL\
    \ ...]]\n\t              bids_dir output_dir {participant,group}\n\n\tExample\
    \ BIDS App entry point script.\n\n\tpositional arguments:\n\t  bids_dir      \
    \        The directory with the input dataset formatted\n\t                  \
    \      according to the BIDS standard.\n\t  output_dir            The directory\
    \ where the output files should be stored.\n\t                        If you are\
    \ running a group level analysis, this folder\n\t                        should\
    \ be prepopulated with the results of\n\t                        the participant\
    \ level analysis.\n\t  {participant,group}   Level of the analysis that will be\
    \ performed. Multiple\n\t                        participant level analyses can\
    \ be run independently\n\t                        (in parallel).\n\n\toptional\
    \ arguments:\n\t  -h, --help            show this help message and exit\n\t  --participant_label\
    \ PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]\n\t                        The label(s)\
    \ of the participant(s) that should be\n\t                        analyzed. The\
    \ label corresponds to\n\t                        sub-&lt;participant_label&gt;\
    \ from the BIDS spec (so it does\n\t                        not include \"sub-\"\
    ). If this parameter is not provided\n\t                        all subjects will\
    \ be analyzed. Multiple participants\n\t                        can be specified\
    \ with a space separated list.\n</code></pre>\n<p>To run it in participant level\
    \ mode (for one participant):</p>\n<pre><code>docker run -i --rm \\\n\t-v /Users/filo/data/ds005:/bids_dataset:ro\
    \ \\\n\t-v /Users/filo/outputs:/outputs \\\n\tbids/example \\\n\t/bids_dataset\
    \ /outputs participant --participant_label 01\n</code></pre>\n<p>After doing this\
    \ for all subjects (potentially in parallel), the group level analysis\ncan be\
    \ run:</p>\n<pre><code>docker run -i --rm \\\n\t-v /Users/filo/data/ds005:/bids_dataset:ro\
    \ \\\n\t-v /Users/filo/outputs:/outputs \\\n\tbids/example \\\n\t/bids_dataset\
    \ /outputs group\n</code></pre>\n<h3>\n<a id=\"user-content-special-considerations\"\
    \ class=\"anchor\" href=\"#special-considerations\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Special considerations</h3>\n\
    <p>Describe whether your app has any special requirements. For example:</p>\n\
    <ul>\n<li>Multiple map reduce steps (participant, group, participant2, group2\
    \ etc.)</li>\n<li>Unusual memory requirements</li>\n<li>etc.</li>\n</ul>\n"
  stargazers_count: 13
  subscribers_count: 2
  topics:
  - bids
  - bids-apps
  updated_at: 1621680020.0
BU-ISCIII/PikaVirus:
  data_format: 2
  description: Open source software that implements a new method for metagenomics
    analysis. A new mapping approach is integrated with traditional assembly and blast
    annotation, all integrated in a web-based user friendly visualization platform
    for ease interpretation.
  filenames:
  - Singularity
  full_name: BU-ISCIII/PikaVirus
  latest_release: null
  readme: '<h1>

    <a id="user-content-pikavirus" class="anchor" href="#pikavirus" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PikaVirus</h1>

    <p>Open source software that implements a new method for metagenomics analysis.
    A new mapping approach is integrated with traditional assembly and blast annotation,
    all integrated in a web-based user friendly visualization platform for ease interpretation.</p>

    '
  stargazers_count: 1
  subscribers_count: 5
  topics: []
  updated_at: 1586703492.0
BU-ISCIII/bacterial_assembly-nf:
  data_format: 2
  description: Haploid bacterial assembly and automatic annotation implemented using
    Nextflow
  filenames:
  - Singularity
  full_name: BU-ISCIII/bacterial_assembly-nf
  latest_release: null
  readme: '<h1>

    <a id="user-content-rstudio_aci" class="anchor" href="#rstudio_aci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>rstudio_aci</h1>

    <p>Code and workflow for building <a href="https://www.rocker-project.org/" rel="nofollow">rocker/verse</a>

    in Docker Hub and modifying it with Singularity Hub for use with PSU

    ACI HPC clusters.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p><code>ssh</code> into the PSU ACI HPC with X11 flags.</p>

    <pre><code>ssh USERID@aci-b.aci.ics.psu.edu -X -Y

    </code></pre>

    <p>Start an interactive session using <code>qsub</code>.</p>

    <pre><code>qsub -A open -I -X -l walltime=24:00:00 -l nodes=2:ppn=20 -l pmem=10gb

    </code></pre>

    <p>From ACI, start <code>screen</code> and then execute the following code to

    create an <code>RStudio</code> image running at address <code>127.0.0.1:8787</code>.</p>

    <pre><code>screen


    singularity pull -n rstudio_aci.simg shub://d-bohn/rstudio_aci


    singularity exec rstudio_aci.simg rserver --www-address=127.0.0.1

    </code></pre>

    <p>Next, press <code>CTRL+A+D</code> to detach the screen while allowing the process
    to continue running in the background.</p>

    <p>Finally, start your preferred browser and navigate to <code>127.0.0.1</code>.
    For

    example, firefox:</p>

    <pre><code>singularity pull shub://jpetucci-firefox_icsaci


    singularity exec jpetucci-firefox_icsaci-master-latest.simg /opt/firefox/./firefox

    </code></pre>

    <h2>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <p>1). A <code>shiny</code> server should also start when executing this image,

    the server should be running on port <code>3838</code></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1589387963.0
BergmannLab/MONET:
  data_format: 2
  description: 'MONET : MOdularising NEtwork Toolbox - https://doi.org/10.1093/bioinformatics/btaa236'
  filenames:
  - .containers/M1/singularity/Singularity
  - .containers/K1/singularity/Singularity
  - .containers/R1/singularity/Singularity
  full_name: BergmannLab/MONET
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-monet\" class=\"anchor\" href=\"#monet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>MONET</h1>\n\
    <p>This repository holds the source code for <strong>MONET</strong>, a Linux/MacOS\
    \ command-line toolbox to mine molecular and genetic networks, leveraging the\
    \ top performing methods of the <strong>Disease Module Identification (DMI) DREAM\
    \ Challenge</strong> (see DREAM Challenge paper under section PUBLICATIONS and\
    \ <a href=\"https://www.synapse.org/modulechallenge\" rel=\"nofollow\">https://www.synapse.org/modulechallenge</a>)</p>\n\
    <h2>\n<a id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>PREREQUISITES</h2>\n<p><strong>Operating System</strong>: MONET can\
    \ be run on <strong>either</strong></p>\n<ul>\n<li>Linux (it was tested on <em>Ubuntu\
    \ Linux</em> 20.04, <em>CentOS Linux</em> 7.5)</li>\n<li>MacOS (it was tested\
    \ on <em>macOS Sierra</em> 10.12)</li>\n</ul>\n<p><strong>Software</strong>: MONET\
    \ requires <strong>either</strong>:</p>\n<ul>\n<li>\n<code>Docker</code> (see\
    \ \"Install using the repository\" <a href=\"https://docs.docker.com/engine/install/\"\
    \ rel=\"nofollow\">https://docs.docker.com/engine/install/</a>)</li>\n<li>\n<code>Singularity</code>\
    \ (see <a href=\"http://singularity.lbl.gov\" rel=\"nofollow\">http://singularity.lbl.gov</a>)</li>\n\
    </ul>\n<p><strong>Hardware</strong>: MONET was tested both on server and on commodity\
    \ hardware (i.e., regular desktop). For details, please refer to section COMPUTATIONAL\
    \ RESOURCES below.</p>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>INSTALLATION</h2>\n<p><strong>Just like you\
    \ can <code>ls</code> a folder, after installation will be able to <code>monet</code>\
    \ a network</strong> from any location on your system.</p>\n<p>Simply run:</p>\n\
    <p><code>$ git clone https://github.com/BergmannLab/MONET.git &amp;&amp; cd MONET\
    \ &amp;&amp; ./install.sh</code></p>\n<p>A folder MONET will have been created\
    \ with the source code: you are free to remove it, if you are not interested.\
    \ This will not affect MONET, which has now been installed in your system: the\
    \ command <code>monet</code> can be invoked from any location on the system.</p>\n\
    <h4>\n<a id=\"user-content-if-you-need-some-more-guidance\" class=\"anchor\" href=\"\
    #if-you-need-some-more-guidance\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>IF YOU NEED SOME MORE GUIDANCE</h4>\n\
    <p>You can follow this <a href=\"https://form.jotform.com/tomasonimattia/monet-installation\"\
    \ rel=\"nofollow\">survey-tutorial</a>:</p>\n<ul>\n<li>it will guide you step\
    \ by step (assumes no prior knowledge)</li>\n<li>(optionally) guides you through\
    \ running some examples (feel free to skip those)</li>\n<li>it will help us collect\
    \ information about possible errors on different platforms</li>\n</ul>\n<h4>\n\
    <a id=\"user-content-if-you-are-on-windows\" class=\"anchor\" href=\"#if-you-are-on-windows\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>IF YOU ARE ON WINDOWS</h4>\n<p>Users using Windows are encouraged\
    \ to install a hypervisor (i.e., a software that allows to creates and run virtual\
    \ machines): for example, install VirtualBox <a href=\"https://www.virtualbox.org/wiki/Downloads\"\
    \ rel=\"nofollow\">https://www.virtualbox.org/wiki/Downloads</a> and configure\
    \ it up to run a virtual Ubuntu Linux inside which to install MONET (using the\
    \ instructions above).</p>\n<h4>\n<a id=\"user-content-if-you-are-a-singularity-user-without-sudo-rights\"\
    \ class=\"anchor\" href=\"#if-you-are-a-singularity-user-without-sudo-rights\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>IF YOU ARE A SINGULARITY USER WITHOUT SUDO RIGHTS</h4>\n<p>Sudo rights\
    \ will be required at installation time for Singularity users: Singularity users\
    \ will not need sudo rights while running MONET (i.e., Singularity does not require\
    \ sudo right to run containers), but they will need it at installation time (i.e.,\
    \ at the time the Singularity images are first created).</p>\n<p>Users that don't\
    \ have sudo rights should follow the regular installation procedure explained\
    \ above, then refer to MONET/docs/installation_no_sudo.txt where they will find\
    \ a workaround to complete the installation manually without needing sudo.</p>\n\
    <h2>\n<a id=\"user-content-testing-the-installation\" class=\"anchor\" href=\"\
    #testing-the-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>TESTING THE INSTALLATION</h2>\n<p>At the end\
    \ of the install process, you will be asked whether you want to test MONET. This\
    \ test is completely automatic.</p>\n<h2>\n<a id=\"user-content-monet-help-command\"\
    \ class=\"anchor\" href=\"#monet-help-command\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>MONET HELP COMMAND</h2>\n<p>After\
    \ installing MONET, the help command <code>monet --help</code> will be available\
    \ from any location on your system.</p>\n<h2>\n<a id=\"user-content-running\"\
    \ class=\"anchor\" href=\"#running\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>RUNNING</h2>\n<p>Once installed,\
    \ from any location on your system, you can run the following example command:\
    \ it will run a method called M1 (see section METHODS for details), on a network\
    \ contained in your /tmp folder (see section INPUT for details), using docker\
    \ virtualization (see section PREREQUISITES for details). In the remainder of\
    \ this document, you will find details about what parameters you can use, what\
    \ to expect as an output and resource usage (in the PARAMETERS, OUTPUT and COMPUTATIONAL\
    \ RESOURCES sections respectively).</p>\n<p><code>$ monet --help</code></p>\n\
    <p><code>$ monet --input=/tmp/input_network.txt \u2014-method=M1 --container=docker</code></p>\n\
    <h2>\n<a id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>INPUT</h2>\n\
    <p>The input file is provided to MONET using the <code>--input</code> parameter\
    \ (see section RUNNING and section PARAMETERS).</p>\n<p>The format for the input\
    \ network is the following: a <strong>tab-separated</strong> file containing one\
    \ line for each edge.</p>\n<p>If an edge is connecting two nodes, gene_a and gene_b,\
    \ with a certain weight, the file will contain the line:</p>\n<p><code>gene_a\
    \ \\t gene_b \\t weight \\n</code></p>\n<p>Details:</p>\n<ul>\n<li>gene_a and\
    \ gene_b, the gene ids, can be either <em>string</em> or <em>integer</em>\n</li>\n\
    <li>weight can be of type <em>integer</em> or <em>float</em>\n</li>\n<li>\"\\\
    t\" indicates the tab character and \"\\n\" the newline character</li>\n<li>no\
    \ blank spaces should appear, neither as separators nor as part of the gene ids</li>\n\
    </ul>\n<p>For an example, see MONET/.test/system_test/input/zachary_karate_club.txt.\
    \ The same folder containing the actual inputs to the Disease Module Identification\
    \ (DMI) DREAM Challenge. Beware that some of the inputs will require high amounts\
    \ of computational resources and are not suited to be run on a simple laptop or\
    \ desktop computer; please refer to section COMPUTATIONAL RESOURCES for details.</p>\n\
    <h2>\n<a id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>OUTPUT</h2>\n\
    <p>The output location is provided to MONET using the <code>--output</code> parameter\
    \ (see section OPTIONAL PARAMETERS).</p>\n<p>Two output files will be generated\
    \ in the directory where you run the command. They are marked with a timestamp,\
    \ the name of the selected method and the name of your input network. For example,\
    \ let's assume if you run M1 on 1st January 2020 at midday on a file called input_network.txt:</p>\n\
    <ul>\n<li>a <strong>console-output</strong> file, which will contain the run-time\
    \ outputs generated by the method you have selected, providing details about the\
    \ steps that the M1 algorithm took to generate your output. Any errors would also\
    \ be redirected here. The file would be called: <code>2020-01-01-120000__M1__console-output__input_network.txt</code>\n\
    </li>\n<li>a <strong>result-modules</strong> file, containing the results of your\
    \ analysis and it will not be generated in case of errors. The file would be called:\
    \ <code>2020-01-01-120000__M1__result-modules__input_network.txt</code>. It will\
    \ be in tab-separated format, containing one module per line:\n<ul>\n<li>the first\
    \ value of each line will be a module identifier (in the form of an integer number\
    \ starting from 1)</li>\n<li>the second is a fixed numerical value and can be\
    \ ignored (curerntly set to <code>1.0</code>, it was originally used in the DREAM\
    \ Challenge to provide module-level confidence scores)</li>\n<li>the rest of the\
    \ values on the line will be the gene ids container in the input (like gene_a\
    \ and gene_b, see section INPUT)</li>\n</ul>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-methods\"\
    \ class=\"anchor\" href=\"#methods\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>METHODS</h2>\n<p>Three methods\
    \ are available as part of MONET, which emerged as the top-performing methods\
    \ of the DREAM Challenge.</p>\n<p>In order to run one of the three methods, adapt\
    \ the example command provided in section RUNNING providing the --method option\
    \ with the name of the chosen method (--method=[K1|M1|R1], for details, see section\
    \ PARAMETERS).</p>\n<ul>\n<li>\n<strong>K1</strong>: KERNEL CLUSTERING OPTIMISATION\
    \ algorithm. K1 is based on the \u201CDiffusion State Distance\u201D (DSD), a\
    \ novel graph metric which is built on the premise that paths through low-degree\
    \ nodes are stronger indications of functional similarity than paths that traverse\
    \ high degree nodes by Cao et al. (2014). The DSD metric is used to define a pairwise\
    \ distance matrix between all nodes, on which a spectral clustering algorithm\
    \ is applied. In parallel, dense bipartite sub-graphs are identified using standard\
    \ graph techniques. Finally, results are merged into a single set of non-overlapping\
    \ clusters. For further details, please see: <a href=\"https://www.synapse.org/#!Synapse:syn7349492/wiki/407359\"\
    \ rel=\"nofollow\">https://www.synapse.org/#!Synapse:syn7349492/wiki/407359</a>\n\
    </li>\n<li>\n<strong>M1</strong>: MODULARITY OPTIMIZATION algorithm. M1 employs\
    \ an original technique named Multiresolution introduced by (Arenas et al., 2008)\
    \ to explore all topological scales at which modules may be found. The novelty\
    \ of this approach relies on the introduction of a parameter, called resistance,\
    \ which controls the aversion of nodes to form modules. Modularity (Newman and\
    \ Girvan, 2004; Arenas et al., 2007) is optimized using an ensemble of algorithms:\
    \ Extremal optimization (Duch and Arenas, 2005), Spectral optimization (Newman,\
    \ 2006), Fast algorithm (Newman, 2004), Tabu search (Arenas et al., 2008), and\
    \ fine-tuning by iterative repositioning of individual nodes in adjacent modules.\
    \ For further details, please see: <a href=\"https://www.synapse.org/#!Synapse:syn7352969/wiki/407384\"\
    \ rel=\"nofollow\">https://www.synapse.org/#!Synapse:syn7352969/wiki/407384</a>\n\
    </li>\n<li>\n<strong>R1</strong>: RANDOM-WALK-BASED algorithm. R1 is based on\
    \ a variant of Markov Cluster Algorithm known as balanced Multi-layer Regularized\
    \ Markov Cluster Algorithm(bMLRMCL) (Satuluriet al., 2010) which scales well to\
    \ large graphs and minimizes the number of oversized clusters. First, a pre-processing\
    \ step is applied so that edges with low weights are discarded and all remaining\
    \ edges are scaled to integer values. Then, bMLRMCL is applied iteratively on\
    \ modules of size grater than a user-defined threshold. For further details, please\
    \ see: <a href=\"https://www.synapse.org/#!Synapse:syn7286597/wiki/406659\" rel=\"\
    nofollow\">https://www.synapse.org/#!Synapse:syn7286597/wiki/406659</a>\n</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>PARAMETERS</h2>\n<p>Please, provide values for the following MANDATORY\
    \ parameters:</p>\n<ul>\n<li>\n<strong>--input</strong>: path to the network file\
    \ to be analysed</li>\n<li>\n<strong>--method</strong>: method to be used to analyse\
    \ the input: [K1|M1|R1]</li>\n<li>\n<strong>--container</strong>: virtualisation\
    \ technology available on the system: [docker|singularity]</li>\n</ul>\n<h2>\n\
    <a id=\"user-content-optional-parameters\" class=\"anchor\" href=\"#optional-parameters\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>OPTIONAL PARAMETERS</h2>\n<ul>\n<li>\n<strong>--output</strong>: directory\
    \ in which to output results (default is current directory)</li>\n</ul>\n<p><strong>if\
    \ you select K1</strong> as a method, you may additionally provide the following:</p>\n\
    <ul>\n<li>\n<strong>--nclusters</strong>: initial number of output clusters for\
    \ spectral clustering step; final number may differ (default is 100)</li>\n</ul>\n\
    <p><strong>if you select M1</strong> as a method, you may additionally provide\
    \ the following:</p>\n<ul>\n<li>\n<strong>--smallest</strong>: min size of output\
    \ clusters (default is 3)</li>\n<li>\n<strong>--largest</strong>: max size of\
    \ output clusters (default is 100)</li>\n<li>\n<strong>--linksdir</strong>: directionality\
    \ of links: [undirected|directed] (default is undirected)</li>\n<li>\n<strong>--avgk</strong>:\
    \ desired average degree for nodes in output (default is 25)</li>\n</ul>\n<p><strong>if\
    \ you select R1</strong> as a method, you may additionally provide the following:</p>\n\
    <ul>\n<li>\n<strong>--smallest</strong>: min size of output clusters (default\
    \ is 3)</li>\n<li>\n<strong>--largest</strong>: max size of output clusters (default\
    \ is 100)</li>\n<li>\n<strong>--c</strong>: trade-off parameter for computational\
    \ efficiency; for larger c, the algorithm will run slower, but may provide more\
    \ accurate results (default is 800)</li>\n<li>\n<strong>--i</strong>: inflation\
    \ parameter for standard Markov Clustering algorithm on which R1 is based (default\
    \ is 2)</li>\n<li>\n<strong>--b</strong>: parameter controlling how balanced the\
    \ clustering results should be; for b=0, R1 behaves like standard Regularized\
    \ Markov Cluster (default is 2)</li>\n<li>\n<strong>--threshold</strong>: remove\
    \ edges smaller than threshold from the input (default is 4)</li>\n<li>\n<strong>--post</strong>:\
    \ decide whether to recursively cluster (recluster) or discard too large output\
    \ clusters: [recluster|discard] (default is discard)</li>\n<li>\n<strong>--c2</strong>:\
    \ (only used if --post=recluster) sets --c for reclustering round (default is\
    \ 500)</li>\n<li>\n<strong>--i2</strong>: (only used if --post=recluster) sets\
    \ --i for reclustering round (default is 2)</li>\n<li>\n<strong>--b2</strong>:\
    \ (only used if --post=recluster) sets --b for reclustering round (default is\
    \ 2)</li>\n</ul>\n<h2>\n<a id=\"user-content-computational-resources\" class=\"\
    anchor\" href=\"#computational-resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>COMPUTATIONAL RESOURCES</h2>\n\
    <p>Some of the methods require large amount of resources, depending on your input\
    \ (please, refer to the MONET paper in the PUBLICATIONS section for details about\
    \ how resource needs will scale with the size of the input, for the different\
    \ methods).</p>\n<p>To reproduce the results of the DREAM Challenge, you can run\
    \ MONET/.test/system_test/reproduce_challenge/reproduce_challenge.sh. This might\
    \ fail on commodity hardware (i.e., a regular laptop or desktop) as about 8GB\
    \ or RAM need to be available. In that case, you can allocate a larger SWAP partition\
    \ (on Linux) or run the experiment on more powerful hardware, such as a server.\
    \ Please browser the rest of the contents of MONET/.test/system_test/reproduce_challenge\
    \ to view the exact RAM usage (ram_usage.txt) and the challenge outputs produced\
    \ by MONET (disease_modules_output directory).</p>\n<p>To monitor resource usage\
    \ when running on your own input (and thus determine the amount or RAM / swap\
    \ needed by your particular input network for a particular method), two simple\
    \ scripts have been added to MONET/.test/helper_scripts (for Unix and one for\
    \ MacOS systems): launch them before execution of MONET and redirect their output\
    \ to file for simple inspection (no other task should be running).</p>\n<h2>\n\
    <a id=\"user-content-benchmarking\" class=\"anchor\" href=\"#benchmarking\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>BENCHMARKING</h2>\n\
    <p>For details about the modularization performance of the MONET methods on a\
    \ set of artificial benchmarks (Louvain algorithm is shown as a baseline), please\
    \ refer to the MONET paper in the PUBLICATIONS section; in particular, Fig. 1.\
    \ MONET/.test/benchmarking for a detailed output of the experiments that have\
    \ been carried out.</p>\n<h2>\n<a id=\"user-content-source-code\" class=\"anchor\"\
    \ href=\"#source-code\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>SOURCE CODE</h2>\n<p>The source code is hosted\
    \ at: <a href=\"https://github.com/BergmannLab/MONET.git\">https://github.com/BergmannLab/MONET.git</a></p>\n\
    <h2>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CONTRIBUTING</h2>\n<p>If you are interested in contributing to MONET,\
    \ we encourage you to get in touch! We will be happy to add you to the list of\
    \ our developers <a href=\"https://github.com/BergmannLab/MONET/graphs/contributors\"\
    >https://github.com/BergmannLab/MONET/graphs/contributors</a>. <strong>THANK YOU!</strong></p>\n\
    <p><strong>CONTRIBUTING - CREATING A BRANCH</strong></p>\n<p>First, we will create\
    \ an issue for the specific feature you are willing to contribute; let's say yours\
    \ will happen to be issue 999. You will be then asked to create a new git branch\
    \ where to implement your changes; run the following from the cloned MONET directory:</p>\n\
    <p><code>$ git checkout -b issues_999</code></p>\n<p><code>$ git push origin issues_999</code></p>\n\
    <p>At this point, you are free to make changes to your local code in your laptop.\
    \ Don't worry if you mess things up, it's no problem to add mistakes to a branch.</p>\n\
    <p><strong>CONTRIBUTING - TESTING YOUR CHANGES</strong></p>\n<p>Once you are done\
    \ with your changes, you can test them locally by <strong>reinstalling</strong>\
    \ from the modified MONET directory.</p>\n<p><strong>CONTRIBUTING - PUBLISHING\
    \ YOUR CHANGES</strong></p>\n<p>Once you have tested your changes, run the following\
    \ from the cloned MONET directory:</p>\n<p><code>$ git add .</code></p>\n<p><code>$\
    \ git commit -m \"adding code for feature # issues_999\"</code></p>\n<p><code>$\
    \ git push --set-upstream origin issues_999</code></p>\n<p><code>$ git checkout\
    \ master</code></p>\n<p>One of the MONET developers will test the changes in your\
    \ branch then merge to Master.</p>\n<h2>\n<a id=\"user-content-implementing-local-changes-to-monet\"\
    \ class=\"anchor\" href=\"#implementing-local-changes-to-monet\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>IMPLEMENTING\
    \ LOCAL CHANGES TO MONET</h2>\n<p>If you wish to implement local changes to MONET,\
    \ independently from our github repository, you can simply modify the code in\
    \ your local cloned repository and <strong>reinstall</strong> after having made\
    \ those changes (i.e. run or re-run the <code>install.sh</code> script and confirm\
    \ if you are asked to reinstall). This procedure can be repeated as many times\
    \ as you like.</p>\n<h2>\n<a id=\"user-content-troubleshooting-common-problems\"\
    \ class=\"anchor\" href=\"#troubleshooting-common-problems\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>TROUBLESHOOTING\
    \ COMMON PROBLEMS</h2>\n<p>If a MONET run is suddenly interrupted or if the expected\
    \ outputs has not been generated, here are few common problems that can occur:</p>\n\
    <ul>\n<li>lack of RAM: if the console-output file (see section OUTPUT) contains\
    \ the word \"Killed\", the MONET processed were stopped by the Operating System,\
    \ likely due to a lack of RAM. To confirm this, please read section COMPUTATIONAL\
    \ RESOURCES to learn how to monitor your resource usage while running MONET.</li>\n\
    <li>outdated kernel: Singularity users that work on Linux distributions with old\
    \ kernels (e.g. CentOS 6.1, kernel 2.6) will encounter trouble during the install\
    \ process; they need to contact their system administrator to inquire whether\
    \ a kernel upgrade is possible.</li>\n<li>can't implement local changes: please,\
    \ refer to section IMPLEMENTING LOCAL CHANGES TO MONET.</li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-bug-reports\" class=\"anchor\" href=\"#bug-reports\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>BUG-REPORTS</h2>\n\
    <p>Please, address your questions and bug reports to Mattia Tomasoni, &lt;mattia.tomasoni\
    \ AT unil.ch&gt;. An issue will be opened here to address your problem: <a href=\"\
    https://github.com/BergmannLab/MONET/issues\">https://github.com/BergmannLab/MONET/issues</a></p>\n\
    <h2>\n<a id=\"user-content-publications\" class=\"anchor\" href=\"#publications\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>PUBLICATIONS</h2>\n<ul>\n<li>\n<p><strong>MONET paper</strong>: Mattia\
    \ Tomasoni, Sergio G\xF3mez, Jake Crawford, Weijia Zhang, Sarvenaz Choobdar, Daniel\
    \ Marbach and Sven Bergmann. MONET: a toolbox integrating top-performing methods\
    \ for network modularization. Bioinformatics 36 (12), 3920-3921. doi: <a href=\"\
    https://doi.org/10.1093/bioinformatics/btaa236\" rel=\"nofollow\">https://doi.org/10.1093/bioinformatics/btaa236</a></p>\n\
    </li>\n<li>\n<p><strong>DREAM Challenge paper</strong>: Sarvenaz Choobdar, Mehmet\
    \ Ahsen, Jake Crawford, Mattia Tomasoni, Tao Fang, David Lamparter, Junyuan Lin,\
    \ Benjamin Hescott, Xiaozhe Hu, Johnathan Mercer, Ted Natoli, Rajiv Narayan, The\
    \ DREAM Module Identification Challenge Consortium, Aravind Subramanian, Jitao\
    \ David Zhang, Gustavo Stolovitzky, Zolt\xE1n Kutalik, Kasper Lage, Donna Slonim,\
    \ Julio Saez-Rodriguez, Lenore Cowen, Sven Bergmann, Daniel Marbach. Assessment\
    \ of network module identification across complex diseases. Nature Methods 16\
    \ (2019) 843-852. doi: <a href=\"https://doi.org/10.1038/s41592-019-0509-5\" rel=\"\
    nofollow\">https://doi.org/10.1038/s41592-019-0509-5</a></p>\n</li>\n</ul>\n"
  stargazers_count: 29
  subscribers_count: 5
  topics: []
  updated_at: 1620780040.0
BrainModes/tvb-pipeline-sc:
  data_format: 2
  description: The dwMRI preprocessing leg of the TVB processing pipeline. Initial
    version cloned from BIDS-Apps/MRtrix3_connectome.
  filenames:
  - Singularity
  full_name: BrainModes/tvb-pipeline-sc
  latest_release: null
  readme: '<h1>

    <a id="user-content-tvb-pipeline-sc" class="anchor" href="#tvb-pipeline-sc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>tvb-pipeline-sc</h1>

    <p>The dwMRI preprocessing leg of the TVB processing pipeline. Initial version
    cloned from BIDS-Apps/MRtrix3_connectome.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1591895471.0
BrendelGroup/BWASP:
  data_format: 2
  description: Bisulfite-seq data Workflow Automation Software and Protocols
  filenames:
  - Singularity.v0.9
  - Singularity
  - Singularity.v1.1
  - Singularity.v1.0
  full_name: BrendelGroup/BWASP
  latest_release: null
  readme: '<h1>

    <a id="user-content-bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols"
    class="anchor" href="#bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BWASP
    : Bisulfite-seq data Workflow Automation Software and Protocols</h1>

    <p>The BWASP repository encompasses code and scripts developed in the

    <a href="http://brendelgroup.org/" rel="nofollow">Brendel Group</a> for analyses
    of bisulfite sequencing

    data.

    The entire workflow relies on various other open source software as well as

    <a href="https://www.r-project.org/" rel="nofollow">R</a> scripts from the companion

    <a href="https://github.com/BrendelGroup/BWASPR">BWASPR</a> repository.

    The code conforms to our <a href="https://brendelgroup.github.io/" rel="nofollow">RAMOSE</a>

    philosophy: it generates <strong>reproducible</strong>, <strong>accurate</strong>,
    and <strong>meaningful</strong>

    results; it is <strong>open</strong> (source) and designed to be <strong>scalable</strong>
    and

    <strong>easy</strong> to use.</p>

    <h2>

    <a id="user-content-quick-start-" class="anchor" href="#quick-start-" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start <a href="https://singularity-hub.org/collections/1203"
    rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    </h2>

    <p>Input to the BWASP workflow consists of accession numbers or fastq files of

    bisulfite-sequencing reads as well as the appropriate genome assembly (and, if

    available, genome annotation).

    Output (after read quality control and mapping) are <em>*.mcalls</em> files that
    list

    the sufficiently covered genomic Cs and their methylation percentage in the

    given sample.

    The scripts in the <em>bin</em> directory take care of minor tasks in the overall

    workflow, but configuration and execution is via

    <a href="https://www.gnu.org/software/make/" rel="nofollow">GNU make</a> using
    edited copies of the

    makefiles provided in the <em>makefiles</em> directory.

    All the BWASP dependencies are encapsulated in a

    <a href="https://www.sylabs.io/docs/" rel="nofollow">Singularity</a> container
    available from our

    <a href="http://BrendelGroup.org/SingularityHub/" rel="nofollow">Singularity Hub</a>.

    Thus, once you know what you are doing, execution could be as simple as</p>

    <pre><code>singularity pull http://BrendelGroup.org/SingularityHub/bwasp.sif

    singularity exec bwasp.sif make

    </code></pre>

    <p>(assuming you have prepared a suitable makefile in your working directory).</p>

    <h2>

    <a id="user-content-realistic-start" class="anchor" href="#realistic-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Realistic Start</h2>

    <p>Please find detailed installation instructions and options in the

    <a href="./INSTALL.md">INSTALL</a> document.

    Once all preparatory steps are taken care of, see the <a href="./HOWTO.md">HOWTO</a>

    document for a complete example of how to implement and run a workflow.</p>

    <h2>

    <a id="user-content-reference" class="anchor" href="#reference" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Reference</h2>

    <p>Amy L. Toth, Murat Ozturk, Saranya Sankaranarayanan, and Volker P. Brendel

    (2018) <em>Estimating the size and dynamics of the CpG methylome of social

    insects.</em> To be submitted.</p>

    <h2>

    <a id="user-content-contact" class="anchor" href="#contact" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

    <p>Please direct all comments and suggestions to

    <a href="mailto:vbrendel@indiana.edu">Volker Brendel</a>

    at <a href="http://brendelgroup.org/" rel="nofollow">Indiana University</a>.</p>

    '
  stargazers_count: 2
  subscribers_count: 5
  topics: []
  updated_at: 1621473108.0
BrendelGroup/mRNAmarkup:
  data_format: 2
  description: Evaluation and annotation of assembled transcripts
  filenames:
  - Singularity
  full_name: BrendelGroup/mRNAmarkup
  latest_release: null
  readme: '<h1>

    <a id="user-content-mrnamarkup---a-workflow-for-annotating-transcript-sequences"
    class="anchor" href="#mrnamarkup---a-workflow-for-annotating-transcript-sequences"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>mRNAmarkup
    - a workflow for annotating transcript sequences</h1>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>mRNAmarkup is implemented as a bash script which should work on any Linux or

    UNIX system.  Please see <a href="./INSTALL">INSTALL</a> for requirements of other
    software

    and specific set-up instructions.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>If you want to avoid any installation hussles, the following will do nicely,

    using our mRNAmarkup <a href="https://www.sylabs.io/docs/" rel="nofollow">Singularity</a>
    container that

    encapsulates all scripts, programs, and system packages.</p>

    <pre><code>git clone https://github.com/BrendelGroup/mRNAmarkup

    cd mRNAmarkup/

    singularity pull http://BrendelGroup.org/SingularityHub/mRNAmarkup.sif

    ./xsetup

    cd db

    singularity exec -e -B ${PWD}/.. ../mRNAmarkup.sif bash 0README

    cd ..

    cd data

    singularity exec -e -B ${PWD}/.. ../mRNAmarkup.sif ./xtest

    xdiff

    </code></pre>

    <h2>

    <a id="user-content-reference" class="anchor" href="#reference" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Reference</h2>

    <p>manuscript to be submitted</p>

    <h2>

    <a id="user-content-contact" class="anchor" href="#contact" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

    <p>Please direct all comments and suggestions to

    <a href="mailto:vbrendel@indiana.edu">Volker Brendel</a>

    at <a href="http://brendelgroup.org/" rel="nofollow">Indiana University</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620409883.0
CIA-CCTB/pythrahyper_net:
  data_format: 2
  description: Code and Data for "Biological network growth in complex environments
    - a computational framework"
  filenames:
  - Singularity/Singularity.def
  full_name: CIA-CCTB/pythrahyper_net
  latest_release: null
  readme: '<h1>

    <a id="user-content-pythrahyper_net" class="anchor" href="#pythrahyper_net" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pythrahyper_net</h1>

    <p>Code and data for the paper <em>"Biological network growth in complex environments
    - a computational framework"</em> by T. Paul and P. Kollmannsberger (2020) - <a
    href="https://biorxiv.org/cgi/content/short/2020.06.01.127407v1" rel="nofollow">https://biorxiv.org/cgi/content/short/2020.06.01.127407v1</a></p>

    <p>Please have a look at the notebook <a href="https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Introduction.ipynb">Introduction.ipynb</a>,
    or try it directly here:  <a href="https://colab.research.google.com/github/CIA-CCTB/pythrahyper_net/blob/master/Colab/Introduction_Colab.ipynb"
    rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667"
    alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"
    style="max-width:100%;"></a> (requires Google account)</p>

    <p>The following Jupyter notebooks reproduce the simulations shown in Figure 6
    in the paper:</p>

    <ul>

    <li>

    <a href="https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Multicellular-Network.ipynb">Multicellular-Network.ipynb</a>
    - simulation of network growth between layers of cells</li>

    <li>

    <a href="https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Multi-Simulation-Setup.ipynb">Multi-Simulation-Setup.ipynb</a>
    - generate configuration and batch files for parameter scan</li>

    <li>

    <a href="https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Multi-Simulation-Analysis.ipynb">Multi-Simulation-Analysis.ipynb</a>
    - analyze results and generate plots for parameter scan</li>

    </ul>

    <h1>

    <a id="user-content-instructions" class="anchor" href="#instructions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Instructions</h1>

    <p>The framework is written in python using numpy and the multiprocessing module,
    and has been tested under Linux and MacOS. To run the example notebooks, first
    download or clone this repository, and then follow the instructions below.</p>

    <h2>

    <a id="user-content-1-using-conda" class="anchor" href="#1-using-conda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>1) Using conda</h2>

    <p>The easiest way to install the required python packages is by using conda.
    Creating a new environment with this command will install all dependencies:</p>

    <p><code>conda create --name pythra python=3.7 pyqt=5 scipy tifffile jupyter networkx
    matplotlib</code></p>

    <p>Then change into the new environment using <code>conda activate pythra</code>,
    and start a Jupyter notebook server in the <code>pythrahyper_net</code> directory
    to access the notebooks.</p>

    <h3>

    <a id="user-content-mayavi-visualization-in-the-browser" class="anchor" href="#mayavi-visualization-in-the-browser"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mayavi
    visualization in the browser:</h3>

    <p>To get interactive mayavi visualizations inside the browser, first install
    mayavi and ipyevents:</p>

    <p><code>conda install -c anaconda mayavi</code></p>

    <p><code>conda install -c conda-forge ipyevents</code></p>

    <p>Next, install and activate the required extension:</p>

    <p><code>jupyter nbextension install --py mayavi --user</code></p>

    <p><code>jupyter nbextension enable --py mayavi --user</code></p>

    <p>If you get missing symbol errors upon importing <code>mlab</code>, try this:</p>

    <p><code>conda install -c conda-force "libnetcdf=4.6.2"</code></p>

    <h3>

    <a id="user-content-interactive-matplotlib-plots" class="anchor" href="#interactive-matplotlib-plots"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interactive
    Matplotlib plots:</h3>

    <p>The matplotlib plots can be made interactive using these modules:</p>

    <p><code>conda install -c conda-forge ipympl widgetsnbextension</code></p>

    <h2>

    <a id="user-content-2-using-singularity-container" class="anchor" href="#2-using-singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2)
    Using Singularity container</h2>

    <p>The second possibility is to run the framework inside a Singularity container.
    A container image can be created using the included definition file:</p>

    <p><code>sudo singularity build pythra.simg Singularity.def</code></p>

    <p>After successful build, you can e.g. start a Jupyter notebook server inside
    the container:</p>

    <p><code>singularity exec pythra.simg jupyter notebook</code></p>

    <p>Then copy and paste the server URL into a web browser running outside of the
    container to access the notebooks.</p>

    '
  stargazers_count: 3
  subscribers_count: 3
  topics: []
  updated_at: 1606837605.0
CNRResistanceAntibiotic/CGST:
  data_format: 2
  description: Core-Genome Sequence Typing
  filenames:
  - Singularity
  full_name: CNRResistanceAntibiotic/CGST
  latest_release: null
  readme: "<p>#CGST</p>\n<p>A Core-Genome Sequence Typing tool</p>\n<h2>\n<a id=\"\
    user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Table of contents</h2>\n<ul>\n<li><a href=\"#introduction\">Introduction</a></li>\n\
    <li><a href=\"#requirements\">Requirements</a></li>\n<li><a href=\"#installation\"\
    >Installation</a></li>\n<li><a href=\"#method\">Method</a></li>\n<li><a href=\"\
    #quick-usage\">Quick usage</a></li>\n<li><a href=\"#full-usage\">Full usage</a></li>\n\
    <li><a href=\"#acknowledgments\">Acknowledgments</a></li>\n<li><a href=\"#citation\"\
    >Citation</a></li>\n<li><a href=\"#license\">License</a></li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Introduction</h2>\n\
    <p>CGST (Core-Genome Sequence Typing) has goal to use the species core-genome\
    \ MLST (Multiple Locus Sequence Typing) and the WGS (Whole Genome Sequencing)\
    \ information at a new level of genotyping.</p>\n<p>CGST is a pipeline that have\
    \ 2 main part: <a href=\"#detection\">Detection</a> and <a href=\"#analysis\"\
    >Analysis</a>.</p>\n<h4>\n<a id=\"user-content-detection\" class=\"anchor\" href=\"\
    #detection\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Detection</h4>\n<p>Detection part of CGST consist to detect the allele\
    \ sequence for each locus of a bacterial strain describe in a cgMLST.</p>\n<p>The\
    \ allele detection is done with <a href=\"https://github.com/WGS-TB/MentaLiST\"\
    >MentaLiST</a> tool. The novels allelic sequences found are directly use to update\
    \ the cgMLST database.</p>\n<p>The <a href=\"https://github.com/WGS-TB/MentaLiST\"\
    >MentaLiST</a> output is enhanced by others information like classic MLST schema\
    \ with <a href=\"https://github.com/sanger-pathogens/ariba\">Ariba</a> to produce\
    \ the final CGST output.</p>\n<p>CGST contains for each species cgMLST a list\
    \ of combinations alleles and the combination found in the strain are stored in\
    \ the <code>combination_result.tsv</code> file.</p>\n<p>The detection CGST part\
    \ has a high memory consumption (MentaLiST consumption). More than 16Go with the\
    \ <code>Escherichia coli</code> core-genome.</p>\n<h4>\n<a id=\"user-content-analysis\"\
    \ class=\"anchor\" href=\"#analysis\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Analysis</h4>\n<p>Analysis part\
    \ of CGST consist to classify, summarize and compare the information provides\
    \ by the detection part of CGST in a set of strains base on the same core-genome.</p>\n\
    <ul>\n<li>\n<p>The classification step consist to regroup the strains by their\
    \ alleles difference on a numeric scale.</p>\n</li>\n<li>\n<p>The summarize step\
    \ consist to get the alignment of the allele difference provide by <a href=\"\
    https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\">MAFFT</a> then use\
    \ <a href=\"https://github.com/sanger-pathogens/gubbins\">Gubbins</a> to remove\
    \ the recombination SNP and get a SNP alignment. This alignment is use with <a\
    \ href=\"https://github.com/amkozlov/raxml-ng\">RaXML-ng</a> to build the more\
    \ accurate possible phylotree.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n<p>CGST assumes\
    \ that you have <a href=\"https://github.com/lh3/miniasm\">MentaLiST</a>, <a href=\"\
    https://julialang.org/downloads/\" rel=\"nofollow\">julia</a>(for MentaliST),\
    \ <a href=\"https://github.com/sanger-pathogens/gubbins\">Gubbins</a> - available\
    \ by <code>run_gubbins</code> instead of <code>run_gubbins.py</code> , <a href=\"\
    https://github.com/amkozlov/raxml-ng\">RaXML-ng</a>, <a href=\"https://mafft.cbrc.jp/alignment/software/\"\
    \ rel=\"nofollow\">MAFFT</a> and <a href=\"https://cran.r-project.org/\" rel=\"\
    nofollow\">R</a> installed and available in your PATH. If you can run <code>mentalist\
    \ -v</code>, <code>julia -v</code>, <code>run_gubbins --version</code>, <code>mafft</code>,\
    \ <code>raxml-ng --version</code> and <code>R --version</code> on the command\
    \ line, you should be good to go!</p>\n<p>You'll need Python 3.6 or later to run\
    \ CGST (check with <code>python3 --version</code>). The Python package requirement\
    \ are <a href=\"https://biopython.org/wiki/Download\" rel=\"nofollow\">Biopython</a>\
    \ and <a href=\"https://pypi.org/project/pandas/\" rel=\"nofollow\">pandas</a>.\
    \ If you don't already have this package, it will be installed as part of the\
    \ CGST installation process.</p>\n<p>You'll need R 3.6 or later to run CGST (check\
    \ with <code>R --version</code>). The R package requirement are <a href=\"https://cran.r-project.org/web/packages/optparse/index.html\"\
    \ rel=\"nofollow\">optparse</a>, <a href=\"https://cran.r-project.org/web/packages/questionr/index.html\"\
    \ rel=\"nofollow\">questionr</a>, <a href=\"https://cran.r-project.org/web/packages/cluster/index.html\"\
    \ rel=\"nofollow\">cluster</a> and <a href=\"https://cran.r-project.org/web/packages/fastcluster/index.html\"\
    \ rel=\"nofollow\">fastcluster</a>.</p>\n<h3>\n<a id=\"user-content-help-for-installation-dependencies--or-use-singularity\"\
    \ class=\"anchor\" href=\"#help-for-installation-dependencies--or-use-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Help for installation dependencies / Or use Singularity</h3>\n<p>You\
    \ can follow the installation of the dependencies in the Singularity of CGST available\
    \ on the repository.</p>\n<p>Or you can build your own Singularity image by:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>sudo singularity build CGST.simg\
    \ Singularity\nsingularity <span class=\"pl-c1\">exec</span> CGST.simg cgst</pre></div>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>Install from source</p>\n<p>You can install\
    \ CGST using <a href=\"https://pypi.org/project/pip/\" rel=\"nofollow\">pip</a>,\
    \ either from a local copy:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>git clone https://github.com/CNRResistanceAntibiotic/CGST.git\npip3 install\
    \ ./CGST\ncgst --help</pre></div>\n<p>or directly from GitHub:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>pip3 install git+https://github.com/CNRResistanceAntibiotic/CGST.git\n\
    cgst --help</pre></div>\n<p>If these installation commands aren't working for\
    \ you (e.g. an error message like <code>Command 'pip3' not found</code> or <code>command\
    \ 'gcc' failed with exit status 1</code>) then check out the <a href=\"https://github.com/rrwick/Badread/wiki/Installation-issues\"\
    >installation issues page on the Badread wiki page</a> (a different tool done\
    \ by <a href=\"https://github.com/rrwick\">Ryan Wick</a>).</p>\n<h3>\n<a id=\"\
    user-content-run-without-installation\" class=\"anchor\" href=\"#run-without-installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run without installation</h3>\n<p>CGST can also be run directly from\
    \ its repository by using the <code>CGST.py</code> script, no installation required:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/CNRResistanceAntibiotic/CGST.git\n\
    CGST/cgst/cgst.py -h</pre></div>\n<p>If you run CGST this way, it's up to you\
    \ to make sure that <a href=\"https://biopython.org/wiki/Download\" rel=\"nofollow\"\
    >Biopython</a> and <a href=\"https://pypi.org/project/pandas/\" rel=\"nofollow\"\
    >pandas</a> are installed for your Python environment.</p>\n<h2>\n<a id=\"user-content-method\"\
    \ class=\"anchor\" href=\"#method\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Method</h2>\n<h3>\n<a id=\"user-content-step-1-display-core-genome-available\"\
    \ class=\"anchor\" href=\"#step-1-display-core-genome-available\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 1: Display Core-Genome available</h3>\n<p>CGST mirror the <a href=\"https://github.com/lh3/miniasm\"\
    >MentaLiST</a> own download function that use the core-genome available in <a\
    \ href=\"https://www.cgmlst.org/ncs\" rel=\"nofollow\">cgmlst.org</a> and mirror\
    \ the CNR <a href=\"https://github.com/CNRResistanceAntibiotic/core_genomes\"\
    >core_genomes</a> repo.</p>\n<p>Command : <code>cgst available-species</code></p>\n\
    <h3>\n<a id=\"user-content-step-2-download-data-to-build-the-database\" class=\"\
    anchor\" href=\"#step-2-download-data-to-build-the-database\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 2:\
    \ Download data to build the database</h3>\n<p>CGST download cgMLST in mirror\
    \ of the <a href=\"https://github.com/lh3/miniasm\">MentaLiST</a> build function.</p>\n\
    <p>Example for \"Escherichia coli\"\nCommand :</p>\n<pre><code>cgst build-database\
    \ -db database_CGST_folder_path -sp \"Escherichia coli\" -t 12\n</code></pre>\n\
    <h3>\n<a id=\"user-content-step-3-check-cgst-database\" class=\"anchor\" href=\"\
    #step-3-check-cgst-database\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Step 3: Check CGST database</h3>\n\
    <p>You can check the database of CGST.</p>\n<p>Command : <code>cgst status-genome\
    \ -db database_CGST_folder_path</code></p>\n<h3>\n<a id=\"user-content-step-4-cgst-detection\"\
    \ class=\"anchor\" href=\"#step-4-cgst-detection\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 4: CGST\
    \ detection</h3>\n<p>CGST detection command:</p>\n<pre><code>cgst detection -1\
    \ /illumina_reads/my_ecoli_S30_R1_001.fastq.gz -2 /illumina_reads/my_ecoli_R2_001.fastq.gz\
    \ -db /database_CGST_path -w /detection_folder_output -o my_ecoli -sp \"Escherichia\
    \ coli\" -t 12\n</code></pre>\n<p>The outputs are constitute by :</p>\n<ul>\n\
    <li>\n<code>\xECntermediate_files</code> folder contains the <a href=\"https://github.com/lh3/miniasm\"\
    >MentaLiST</a> output files.</li>\n<li>\n<code>my_ecoli_output_ariba</code> folder\
    \ contains the ARIBA output files.</li>\n<li>\n<code>combination_result.tsv</code>\
    \ file contains the detection combination.</li>\n</ul>\n<table>\n<thead>\n<tr>\n\
    <th>Column header</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n\
    <td>Combination Name</td>\n<td>Name of the locus combination</td>\n</tr>\n<tr>\n\
    <td>Count Reference Locus</td>\n<td>Sum of locus in the combination</td>\n</tr>\n\
    <tr>\n<td>Count Sample Locus</td>\n<td>Sum of locus of the combination retrieves\
    \ in the strain</td>\n</tr>\n<tr>\n<td>Ratio</td>\n<td>Percent ration of the <code>Count\
    \ Sample Locus</code> in the <code>Count Reference Locus</code>\n</td>\n</tr>\n\
    <tr>\n<td>Comment</td>\n<td>Commentary that appreciate the ratio with a scale</td>\n\
    </tr>\n</tbody>\n</table>\n<p>Scale of Comment head columns :</p>\n<pre><code>-\
    \ Ratio == 100  -&gt; `Perfect`\n- 100&gt;Ratio&gt;98  -&gt; `Very Close`\n- 98&gt;Ratio&gt;90\
    \  -&gt; `Close`\n- 90&gt;Ratio&gt;80  -&gt; `Like`\n- 80&gt;Ratio&gt;70  -&gt;\
    \ `Close Like`\n- 70&gt;Ratio&gt;0  -&gt; `No relevant`\n</code></pre>\n<ul>\n\
    <li>\n<code>my_ecoli_output_final</code> file contains CGST final results.</li>\n\
    <li>\n<code>statistics.tsv</code> file contains the CGST statistics.</li>\n</ul>\n\
    <table>\n<thead>\n<tr>\n<th>Column header</th>\n<th>Description</th>\n</tr>\n\
    </thead>\n<tbody>\n<tr>\n<td>Total Locus</td>\n<td>Number of locus in the cgMLST</td>\n\
    </tr>\n<tr>\n<td>Perfect Locus</td>\n<td>Number of perfect locus match in MentaLiST</td>\n\
    </tr>\n<tr>\n<td>Multiple Locus</td>\n<td>Number of multiple locus match (+) in\
    \ MentaLiST</td>\n</tr>\n<tr>\n<td>Low Coverage Locus</td>\n<td>Number of low\
    \ coverage locus match (-) in MentaLiST</td>\n</tr>\n<tr>\n<td>None Locus</td>\n\
    <td>Number of none locus match in MentaLiST</td>\n</tr>\n</tbody>\n</table>\n\
    <h3>\n<a id=\"user-content-step-5-cgst-analysis\" class=\"anchor\" href=\"#step-5-cgst-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 5: CGST analysis</h3>\n<p>Create a folder for analysis and copy\
    \ the output final CGST files of the wanted strains.</p>\n<p>The analysis can\
    \ only done on 5 strains or more.</p>\n<pre><code>mkdir analysis_CGST\ncp /detection_folder_output/detection_*/*_output_final\
    \ analysis_CGST\n</code></pre>\n<p>CGST analysis command:</p>\n<pre><code>cgst\
    \ analysis -dd /analysis_CGST -db /database_CGST_path -w /analysis_CGST/analysis_comput\
    \ -sp \"Escherichia coli\"\n</code></pre>\n<p>The outputs are constitute by 3\
    \ folder and \"all_report.tsv\" file:</p>\n<h5>\n<a id=\"user-content-folder--cluster\"\
    \ class=\"anchor\" href=\"#folder--cluster\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Folder : cluster</h5>\n<p>This\
    \ folder contains:</p>\n<ul>\n<li>The file <code>lvl_report.tsv</code> is the\
    \ strain classification by an increase scale of alleles share by the strains.</li>\n\
    </ul>\n<h5>\n<a id=\"user-content-folder--combination\" class=\"anchor\" href=\"\
    #folder--combination\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Folder : combination</h5>\n<p>This folder contains:</p>\n\
    <ul>\n<li>The file <code>similarity_matrix.tsv</code> is the similarity matrix\
    \ of the relevant locus of strains.</li>\n<li>The file <code>dendrogram_with_class.pdf</code>\
    \ is the dendrogram that represent the 3 more relevant classification class of\
    \ strains.</li>\n</ul>\n  <p align=\"center\"><a href=\"images/dendrogram.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"images/dendrogram.png\"\
    \ alt=\"Dendrogram\" width=\"600\" style=\"max-width:100%;\"></a></p>\n<ul>\n\
    <li>The file <code>groups.tsv</code> is the classification class by strains for\
    \ each of groups of the 3 class.</li>\n<li>The file <code>inertial_graph.pdf</code>\
    \ is the inertial class curve produce by the <code>hclust</code> R package.</li>\n\
    </ul>\n <p align=\"center\"><a href=\"images/inertial.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"images/inertial.png\" alt=\"Inertial\
    \ graph\" width=\"600\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li>The file\
    \ <code>logR.txt</code> is the log file of the R script <code>r_script.R</code>.</li>\n\
    <li>The file <code>groups_alleles.tsv</code> is classification of the strains,\
    \ the alleles share by all strains and the alleles share more than 90% of the\
    \ strains for each groups class and class.</li>\n</ul>\n<h5>\n<a id=\"user-content-folder--phylotree\"\
    \ class=\"anchor\" href=\"#folder--phylotree\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Folder : phylotree</h5>\n<p>This\
    \ folder contains:</p>\n<ul>\n<li>The sub-folder <code>msa</code> contains files\
    \ produces by <a href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\"\
    >MAFFT</a>.</li>\n<li>The file <code>core-genome.aln</code> is the core-genome\
    \ alignment obtains by concatenation of the alignment <a href=\"https://mafft.cbrc.jp/alignment/software/\"\
    \ rel=\"nofollow\">MAFFT</a> files.</li>\n<li>The file <code>resume_core-genome.tsv</code>\
    \ is the resume of each locus position in the <code>core-genome.aln</code> file.</li>\n\
    <li>The sub-folder <code>gubbins</code> contains files produces by <a href=\"\
    https://github.com/sanger-pathogens/gubbins\">Gubbins</a>.</li>\n<li>The sub-folder\
    \ <code>raxml-ng</code> contains files produces by <a href=\"https://github.com/amkozlov/raxml-ng\"\
    >RaXML-ng</a>.</li>\n</ul>\n<p>The <a href=\"https://github.com/amkozlov/raxml-ng\"\
    >RaXML-ng</a> phylotree purpose is to be the more accurate phylogenetic tree between\
    \ the strains based on the SNP.</p>\n<h5>\n<a id=\"user-content-file-all_reporttsv\"\
    \ class=\"anchor\" href=\"#file-all_reporttsv\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>File: all_report.tsv</h5>\n<p>This\
    \ file contains all alleles detected for all strains of the analysis.</p>\n<h3>\n\
    <a id=\"user-content-step-6-add-new-combination\" class=\"anchor\" href=\"#step-6-add-new-combination\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 6: Add new combination</h3>\n<p>In the <code>combination</code>\
    \ folder, the file <code>groups_alleles.tsv</code> contains the information to\
    \ update/create the combination database for CGST detection.</p>\n<p>In the species\
    \ folder of the CGST-cgMLST database, update or create the file <code>combination_list.tsv</code>.\n\
    This file contains this columns:</p>\n<table>\n<thead>\n<tr>\n<th>Column header</th>\n\
    <th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Name</td>\n<td>The name\
    \ of the combination</td>\n</tr>\n<tr>\n<td>Number_Strain</td>\n<td>The number\
    \ of strain regroups by this combination at the declaration of this combination</td>\n\
    </tr>\n<tr>\n<td>Length_Combination</td>\n<td>The number of alleles of the combination</td>\n\
    </tr>\n<tr>\n<td>Combination</td>\n<td>The sequence of the alleles of the combination</td>\n\
    </tr>\n</tbody>\n</table>\n<p>If the new combination that you have identified\
    \ contains new alleles only on your local CGST database and you want to share\
    \ your combination. Please go to the <a href=\"https://github.com/CNRResistanceAntibiotic/CGST/issues\"\
    >issue</a> page.</p>\n<h2>\n<a id=\"user-content-full-usage\" class=\"anchor\"\
    \ href=\"#full-usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Full usage</h2>\n<pre><code>usage: CGST [commands][options]\
    \ \n\nCGST: Core-Genome Sequence Typing - Version 0.0.1\n\npositional arguments:\n\
    \  &lt;commands&gt;                   Description\n    check                 \
    \     check Dependencies\n    available-species          Available Species in\
    \ cgmlst.org\n    build-database             Build Database by species\n    status-genome\
    \              Resume of genomes available\n    detection                  Detection\
    \ Sequence Typing\n    analysis                   Analysis strains that had a\
    \ cgMLST detection\n\noptional arguments:\n  -f [FORCE], --force [FORCE]  Overwrite\
    \ output directory (default: False)\n  -V, --version                Prints version\
    \ number\n  -h, --help                   Show this help message and exit\n</code></pre>\n\
    <h2>\n<a id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Acknowledgments</h2>\n<p>This tool and this GitHub repository is mainly\
    \ inspired by the work of <a href=\"https://github.com/rrwick\">Ryan Wick</a>.\
    \ A big thank to his work and the good practice in computer programming that he\
    \ done.</p>\n<h2>\n<a id=\"user-content-citation\" class=\"anchor\" href=\"#citation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Citation</h2>\n<p>In progress...</p>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p><a href=\"https://www.gnu.org/licenses/gpl-3.0.html\"\
    \ rel=\"nofollow\">GNU General Public License, version 3</a></p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1605284164.0
CNRResistanceAntibiotic/mutAnalysis:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: CNRResistanceAntibiotic/mutAnalysis
  latest_release: null
  readme: '<h1>

    <a id="user-content-mutanalysis" class="anchor" href="#mutanalysis" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>mutAnalysis</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605218495.0
Characterisation-Virtual-Laboratory/CharacterisationVL-Software:
  data_format: 2
  description: null
  filenames:
  - caffe/Singularity.caffe_1.0
  - argos/Singularity.argos_3.0.0-beta53
  - argos/Singularity.argos_3.0.0-beta52
  - meshlab/Singularity.meshlab-2019.03-cuda-9.0
  - ilastik/Singularity.ilastik_1.3.3post3
  - ubuntu-base-image/Singularity.2004-cuda11.0
  - ubuntu-base-image/Singularity.1804-cuda10.1
  - ubuntu-base-image/Singularity.1804
  - ubuntu-base-image/Singularity.1804-cuda9
  - ubuntu-base-image/Singularity.2004
  - bidscoin/Singularity.bidscoin_3
  - cryolo/Singularity.cryolo_v1_0_4
  - cryolo/Singularity.cryolo_v1_5_6
  - cryolo/Singularity.cryolo_v1_6_1
  - cryolo/Singularity.cryolo_v1_0_0
  - colmap/Singularity.colmap_3.6-dev.3
  - colmap/Singularity.colmap_3.5
  - mrtrix3tissue/Singularity.mrtrix3tissue-5.2.8
  - ants/Singularity.ants_2.3.1
  - ants/Singularity.ants_2.3.4
  - eman/Singularity.eman_2.3
  - eman/Singularity.eman_2.91
  - eman/Singularity.eman_2.22
  - eman/Singularity.eman_2.9
  - eman/Singularity.eman_2.3.1
  - quit/Singularity.quit_2.0.2
  - mango/Singularity.mango_4.0.1
  - R/Singularity.R_4.0.5
  - omero-insight/Singularity.1804
  - octave/Singularity.octave-4.2.2
  - globus-cli/Singularity.globus-cli-v2.0.0
  - deeplabcut/Singularity.latest
  - darknet/Singularity.darknet_yolo_v3-cuda-9.0
  - mrtrix/Singularity.mrtrix_3_beta
  - omero.insight/Singularity.omero_5.5.10
  - bids-validator/Singularity.bids-validator-1.2.2
  - bids-validator/Singularity.bids-validator-1.3.1
  - ashs/Singularity.ashs_2.0.0
  - fiji/Singularity.fiji
  - gimp/Singularity.gimp_2.8
  - gimp/Singularity.gimp_2.8.22
  - apex/Singularity.apex_master
  - openmodelica/Singularity.openmodelica_1.14.2-cuda-10.1
  - cytoscape/Singularity.cytoscape_3.8.0
  - volview/Singularity.VolView_3.4-cuda-9.0
  - pyprismatic/Singularity.pyprismatic_1_2_1-cuda-11.0
  - atom/Singularity.atom_1.39.1
  - atom/Singularity.atom_1.45.0
  - amide/Singularity.amide-1.0.5
  - paraview/Singularity.paraview_5.6.0-cuda-9.0
  - mantid/Singularity.mantid_v_3_13_0
  - openrefine/Singularity.openrefine-3.1
  - datalad/Singularity.datalad_0.13.3
  - cloudstor/Singularity.cloudstor-2.4.1
  - imblproc/Singularity.imblproc
  - octopus/Singularity.octopus_8.4
  - octopus/Singularity.octopus_8.4_parallel
  - haystack/Singularity.haystack_bio_v0_5_0
  - anaconda3/Singularity.anaconda3_5.3.0
  - anaconda3/Singularity.anaconda3_5.3.0-cuda-11.0.3
  - caffe-unet/Singularity.caffe-unet_1.0
  - crisprcas/Singularity.crisprcas
  - cvmfs-client/Singularity.cvmfs-client
  - libertem/Singularity.libertem-v0.6.0
  - libertem/Singularity.libertem-v0.2.2
  - libertem/Singularity.libertem-v0.5.1
  - libertem/Singularity.libertem-v0.4.1
  - libertem/Singularity.libertem-v0.4.0
  - libertem/Singularity.libertem-v0.5.0
  - libertem/Singularity.libertem-21-May-2019
  - cistem/Singularity.cisTEM-1.0.0-beta
  - dristhi/Singularity.dristhi_2.6.4
  - imod/Singularity.imod_v4_9_9
  - fsl/Singularity.fsl
  - cellprofiler/Singularity.cellprofiler_3.1.9
  - cellprofiler/Singularity.cellprofiler_3.1.5
  - jupyter-ml/Singularity.jupyter-ml_20210415
  - jupyter-ml/Singularity.jupyter-ml_20201120
  - imagemagick/Singularity.imagemagick-7.0.8-68
  - ariba/Singularity.ariba_2.14.4
  - ariba/Singularity.ariba_2.12.1
  - dragondisk/Singularity.dragondisk_v1_0_5
  - globus-connect-personal/Singularity.globus-connect-personal_latest
  - mydata-python/Singularity.mydata-python_20200603
  - 3dslicer/Singularity.3dslicer_4.10.2
  - 3dslicer/Singularity.3dslicer_4.8.1
  - imagej/Singularity.imagej_1.50e
  - mydata/Singularity.mydata_0.9.2-1
  - git-annex/Singularity.git-annex.6.20180227
  - matlab/Singularity.MATLAB_SAMPLE
  - graphviz/Singularity.graphviz-2.40.1
  - connectome-workbench/Singularity.connectome-workbench_1.4.2
  full_name: Characterisation-Virtual-Laboratory/CharacterisationVL-Software
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterisationvl-software\" class=\"anchor\"\
    \ href=\"#characterisationvl-software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CharacterisationVL-Software</h1>\n\
    <p>The purpose of this repository is for storing definition files to submit to\
    \ <a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity Hub.</a></p>\n\
    <p>If you are new to Singularity containers, please refer to <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.5/user-guide/</a> or a newer version\
    \ of this documentation.</p>\n<p>Each software package is located in its own folder.\
    \ The files are tagged with the software name and version number or date of build.\
    \ Please read below for the naming convention.</p>\n<p>To add software to the\
    \ repository you will need to create a new branch. The new branch is the name\
    \ of the software product. By convention, the new branch will be checked and merged\
    \ into the master branch and then deleted.</p>\n<h2>\n<a id=\"user-content-steps-to-add-a-software-package\"\
    \ class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Steps to\
    \ add a software package</h2>\n<ol>\n<li>Clone this repository</li>\n<li>Create\
    \ a branch</li>\n</ol>\n<pre><code>$ git branch &lt;software name&gt;\n</code></pre>\n\
    <ol start=\"3\">\n<li>Make a subdirectory for the software product.</li>\n</ol>\n\
    <pre><code>$ mkdir &lt;software name&gt;\n</code></pre>\n<ol start=\"4\">\n<li>Add\
    \ all the necessary files.</li>\n</ol>\n<ul>\n<li>Singularity definition file\
    \ or installation script</li>\n<li>Readme file including install and testing notes</li>\n\
    <li>Desktop files for adding to menus with necessary tags</li>\n<li>For full details,\
    \ <a href=\"template/README.md\">please refer to the 'template' folder in this\
    \ repository.</a>\n</li>\n</ul>\n<ol start=\"4\">\n<li>Commit all changes, including\
    \ a helpful message</li>\n</ol>\n<pre><code>$ git commit -m \"&lt;software name&gt;\
    \ added as requested in support ticket\"\n</code></pre>\n<ol start=\"6\">\n<li>Push\
    \ to the remote repository. i.e. this one.</li>\n<li>Submit merge request</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Naming your Singularity definition file, Singularity Hub and Licensing</h2>\n\
    <p>For all Singularity recipes where the software licensing permits redistribution,\
    \ please use this naming convention:</p>\n<pre><code>   Singularity.applicationName_version\n\
    \   Singularity.applicationName_version-cuda-cudaVersion\n\n</code></pre>\n<p>This\
    \ is where Singularity Hub fits into the equation. There is a webhook between\
    \ this repository and <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >Singularity Hub</a>. When a commit is merged into the master branch, Singularity\
    \ Hub will build the container.</p>\n<p>If successfully built, the path to the\
    \ container on Singularity Hub is:</p>\n<pre><code>  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n\
    \  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\
    \n</code></pre>\n<p>For software where licensing does not support redistribution,\
    \ the container recipe can still be defined, but the container should not be built\
    \ on Singularity Hub.</p>\n<p>An example on how to handle this situation is the\
    \ recipe for CCP-EM.\nThe <a href=\"ccp-em/README.md\">README.md</a> contains\
    \ a section on Prerequisites. This section lists the required files to build the\
    \ container. The license must be accepted by the end user to obtain them.</p>\n\
    <p>Prerequisite files should not be committed to this repository.</p>\n<p>To prevent\
    \ Singularity Hub from attempting to build the container, we simply use a different\
    \ recipe naming convention as follows:</p>\n<pre><code>   applicationName_version.def\n\
    \   applicationName_version-cuda-cudaVersion.def\n\n</code></pre>\n<h2>\n<a id=\"\
    user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Ubuntu Base Images</h2>\n<p>The folder 'ubuntu-base-image' contains\
    \ recipes for pre built base containers. These can be used as a starting point\
    \ to aid/speed up the development of your container recipe.</p>\n<p>The current\
    \ versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.</p>\n\
    <p>These are available on Singularity Hub.</p>\n<p>For example: from the Graphviz\
    \ Singularity.graphviz-2.40.1 recipe</p>\n<pre><code>Bootstrap: shub\nFrom:  \
    \    Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n</code></pre>\n\
    <p>These two lines, will tell Singularity to use the 'shub' bootstrap to obtain\
    \ the '1804' ubuntu-base-image container from Singularity Hub.</p>\n<p>From here\
    \ you just need to add the requirements to build a container for your required\
    \ piece of software. Please see <a href=\"graphviz/Singularity.graphviz-2.40.1\"\
    >Singularity.graphviz-2.40.1</a>\nfor the full recipe.</p>\n<p>The current ubuntu-base-images\
    \ include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.</p>\n\
    <h2>\n<a id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"\
    anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ GUI applications on a non-GPU node</h2>\n<p>The applications in the Singularity\
    \ container should run without the need for a dedicated GPU.</p>\n<p>However,\
    \ an X server needs to be running for this to work. On nodes with GPU, X Server\
    \ is started with NVIDIA driver, and on non-GPU nodes, the X Server is started\
    \ with MESA library.</p>\n<p>X Server can be started during boot (for example,\
    \ using <code>systemctl set-default graphical.target</code>).</p>\n<p>Make sure\
    \ that VirtualGL package is installed in the container. The code below will download\
    \ and install VirtualGL.</p>\n<pre><code>wget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\
    \ndpkg -i virtualgl_2.6.2_amd64.deb\n</code></pre>\n<p>The application startup\
    \ script doesn't need to be modified, however, if the application needs to be\
    \ manually started, then <code>vglrun</code> needs to be appended before running\
    \ the application. For example: <code>singularity exec --nv -B /projects:/projects\
    \ -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX</code></p>\n\
    <p><a href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 6
  subscribers_count: 4
  topics: []
  updated_at: 1620048175.0
Clinical-Genomics-Linkoping/Lund_nextflow_wgs:
  data_format: 2
  description: null
  filenames:
  - container/Singularity
  full_name: Clinical-Genomics-Linkoping/Lund_nextflow_wgs
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"\
    #test-starting-kit\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>test-starting-kit</h1>\n<p><g-emoji class=\"\
    g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\
    >\U0001F913</g-emoji></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620156274.0
Clinical-Genomics-Lund/CADD-container:
  data_format: 2
  description: Singularity container recipe for CADD v1.5
  filenames:
  - Singularity
  full_name: Clinical-Genomics-Lund/CADD-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-work-in-progress" class="anchor" href="#work-in-progress"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>work
    in progress</h1>

    <p>attempt to build a base singularity image with spack that can be used as the
    bootstrap for

    other singularity images that would perform the spack install of a particular
    package</p>

    <p>currently having an issue with stage directory for spack attempting to write
    to

    the immutable squashfs</p>

    <p>as expected, the child container will happily install during %post since it
    can write</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1616792704.0
Clinical-Genomics-Lund/SomaticPanelPipeline:
  data_format: 2
  description: null
  filenames:
  - container/Singularity
  - container/Singularity_cnvkit95
  full_name: Clinical-Genomics-Lund/SomaticPanelPipeline
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/5087" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-sing" class="anchor" href="#sing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>sing</h1>

    <p>Singularity Recipe for HPC</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1618430747.0
Clinical-Genomics-Lund/annotsv_container:
  data_format: 2
  description: 'Container for AnnotSV software. '
  filenames:
  - Singularity
  - Singularity_2.2
  full_name: Clinical-Genomics-Lund/annotsv_container
  latest_release: null
  readme: '<h2>

    <a id="user-content-annotsv-container-for-wgs-pipeline" class="anchor" href="#annotsv-container-for-wgs-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>AnnotSV
    container for WGS pipeline</h2>

    <p>This is a singularity recipe for AnnotSV 2.3 and 2.2.</p>

    <p>sudo singularity build annotsv2.3.sif Singularity</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1618855170.0
Clinical-Genomics-Lund/nextflow_microwgs:
  data_format: 2
  description: null
  filenames:
  - container/Singularity
  full_name: Clinical-Genomics-Lund/nextflow_microwgs
  latest_release: null
  readme: '<h1>

    <a id="user-content-nextflow-pipeline-for-typing-and-marker-detection-of-bacteria"
    class="anchor" href="#nextflow-pipeline-for-typing-and-marker-detection-of-bacteria"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nextflow
    pipeline for typing and marker detection of bacteria</h1>

    <h2>

    <a id="user-content-purpose" class="anchor" href="#purpose" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Purpose</h2>

    <p>The pipeline is aimed at producing data useful for epidemiological and surveillance
    purposes.

    In v1 the pipeline is only tested using MRSA, but it should work well with

    any bacteria having a good cgMLST scheme.</p>

    <h2>

    <a id="user-content-components" class="anchor" href="#components" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Components</h2>

    <h3>

    <a id="user-content-qc" class="anchor" href="#qc" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>QC</h3>

    <p>Species detection is performed using <a href="https://ccb.jhu.edu/software/kraken2/"
    rel="nofollow">Kraken2</a> together with <a href="https://ccb.jhu.edu/software/bracken/"
    rel="nofollow">Bracken</a>.

    The database used is a standard Kraken database built with <code>kraken2-build
    --standard --db $DBNAME</code></p>

    <p>Low levels of Intra-species contamination or erronous mapping is removed using
    bwa and filtering away

    the heterozygous mapped bases.</p>

    <p>Genome coverage is estimated by mapping with <a href="https://github.com/lh3/bwa">bwa
    mem</a> and using a bed file containing the cgMLST loci.</p>

    <p>A value on the evenness of coverage is calculated as an <a href="https://en.wikipedia.org/wiki/Interquartile_range"
    rel="nofollow">interquartile range</a>.</p>

    <h3>

    <a id="user-content-epidemiological-typing" class="anchor" href="#epidemiological-typing"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Epidemiological
    typing</h3>

    <p>For de novo asspembly <a href="http://cab.spbu.ru/software/spades/" rel="nofollow">SPAdes</a>
    is used. <a href="http://cab.spbu.ru/software/quast/" rel="nofollow">QUAST</a>

    is used for extraxting QC data from the assembly.</p>

    <p>The cgMLST reference scheme used, is branched off <a href="https://www.cgmlst.org/ncs/schema/141106/"
    rel="nofollow">cgmlst.net</a>

    At the moment this fork is not synced back with new allele numbers. For extracting
    alleles <a href="https://github.com/B-UMMI/chewBBACA/wiki">chewBBACA</a>

    is used. Number of missing loci is calculated and used as a QC parameter.</p>

    <p>Traditional 7-locus MLST is calculated using <a href="https://github.com/tseemann/mlst">mlst</a>.</p>

    <h3>

    <a id="user-content-virulence-and-resistance-markers" class="anchor" href="#virulence-and-resistance-markers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Virulence
    and resistance markers</h3>

    <p><a href="https://github.com/sanger-pathogens/ariba">ARIBA</a> is used as the
    tool to detect genetic markes.

    The database for virulence markes is <a href="http://www.mgc.ac.cn/VFs/" rel="nofollow">VFDB</a>.</p>

    <h2>

    <a id="user-content-report-and-visualisation" class="anchor" href="#report-and-visualisation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Report
    and visualisation</h2>

    <p>The QC data is aggregated in a web service CDM (repo coming) and the cgMLST
    is visualized using a web service

    cgviz that is combined with <a href="https://github.com/achtman-lab/GrapeTree">graptetree</a>
    for manipulating trees (repo coming).</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1620341649.0
Clinical-Genomics-Lund/nextflow_myeloid:
  data_format: 2
  description: Nextflow pipeline for analysis of the twist myeloid panel
  filenames:
  - container/Singularity
  full_name: Clinical-Genomics-Lund/nextflow_myeloid
  latest_release: null
  readme: '<p>README last updated on: 01/24/2018</p>

    <h1>

    <a id="user-content-railrl" class="anchor" href="#railrl" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>railrl</h1>

    <p>Reinforcement learning framework.

    Some implemented algorithms:</p>

    <ul>

    <li><a href="examples/ddpg.py">Deep Deterministic Policy Gradient (DDPG)</a></li>

    <li><a href="examples/sac.py">Soft Actor Critic</a></li>

    <li><a href="examples/dqn_and_double_dqn.py">(Double) Deep Q-Network (DQN)</a></li>

    <li><a href="examples/her.py">Hindsight Experience Replay (HER)</a></li>

    <li><a href="examples/model_based_dagger.py">MPC with Neural Network Model</a></li>

    <li>

    <a href="examples/naf.py">Normalized Advantage Function (NAF)</a>

    <ul>

    <li>WARNING: I haven''t tested this NAF implementation much, so it may not match
    the paper''s performance. I''m pretty confident about the other two implementations
    though.</li>

    </ul>

    </li>

    </ul>

    <p>To get started, checkout the example scripts, linked above.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <h3>

    <a id="user-content-some-dependancies" class="anchor" href="#some-dependancies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Some
    dependancies</h3>

    <ul>

    <li><code>sudo apt-get install swig</code></li>

    </ul>

    <h3>

    <a id="user-content-create-conda-env" class="anchor" href="#create-conda-env"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    Conda Env</h3>

    <p>Install and use the included ananconda environment</p>

    <pre><code>$ conda env create -f docker/railrl/railrl-env.yml

    $ source activate railrl-env

    (railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py

    </code></pre>

    <p>Or if you want you can use the docker image included.</p>

    <h3>

    <a id="user-content-download-simulation-env-code" class="anchor" href="#download-simulation-env-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download
    Simulation Env Code</h3>

    <ul>

    <li>

    <a href="https://github.com/vitchyr/multiworld">multiworld</a> (contains environments):<code>git
    clone https://github.com/vitchyr/multiworld</code>

    </li>

    </ul>

    <h3>

    <a id="user-content-optional-install-doodad" class="anchor" href="#optional-install-doodad"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>(Optional)
    Install doodad</h3>

    <p>I recommend installing <a href="https://github.com/justinjfu/doodad">doodad</a>
    to

    launch jobs. Some of its nice features include:</p>

    <ul>

    <li>Easily switch between running code locally, on a remote compute with

    Docker, on EC2 with Docker</li>

    <li>Easily add your dependencies that can''t be installed via pip (e.g. you

    borrowed someone''s code)</li>

    </ul>

    <p>If you install doodad, also modify <code>CODE_DIRS_TO_MOUNT</code> in <code>config.py</code>
    to

    include:</p>

    <ul>

    <li>Path to rllab directory</li>

    <li>Path to railrl directory</li>

    <li>Path to other code you want to juse</li>

    </ul>

    <p>You''ll probably also need to update the other variables besides the docker

    images/instance stuff.</p>

    <h3>

    <a id="user-content-setup-config-file" class="anchor" href="#setup-config-file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup
    Config File</h3>

    <p>You must setup the config file for launching experiments, providing paths to
    your code and data directories. Inside <code>railrl/config/launcher_config.py</code>,
    fill in the appropriate paths. You can use <code>railrl/config/launcher_config_template.py</code>
    as an example reference.</p>

    <p><code>cp railrl/launchers/config-template.py railrl/launchers/config.py</code></p>

    <h2>

    <a id="user-content-visualizing-a-policy-and-seeing-results" class="anchor" href="#visualizing-a-policy-and-seeing-results"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visualizing
    a policy and seeing results</h2>

    <p>During training, the results will be saved to a file called under</p>

    <pre><code>LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;

    </code></pre>

    <ul>

    <li>

    <code>LOCAL_LOG_DIR</code> is the directory set by <code>railrl.launchers.config.LOCAL_LOG_DIR</code>

    </li>

    <li>

    <code>&lt;exp_prefix&gt;</code> is given either to <code>setup_logger</code>.</li>

    <li>

    <code>&lt;foldername&gt;</code> is auto-generated and based off of <code>exp_prefix</code>.</li>

    <li>inside this folder, you should see a file called <code>params.pkl</code>.
    To visualize a policy, run</li>

    </ul>

    <pre><code>(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;/params.pkl

    </code></pre>

    <p>If you have rllab installed, you can also visualize the results

    using <code>rllab</code>''s viskit, described at

    the bottom of <a href="http://rllab.readthedocs.io/en/latest/user/cluster.html"
    rel="nofollow">this page</a></p>

    <p>tl;dr run</p>

    <div class="highlight highlight-source-shell"><pre>python rllab/viskit/frontend.py
    LOCAL_LOG_DIR/<span class="pl-k">&lt;</span>exp_prefix<span class="pl-k">&gt;</span>/</pre></div>

    <h3>

    <a id="user-content-add-paths" class="anchor" href="#add-paths" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Add paths</h3>

    <pre><code>export PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo

    </code></pre>

    <h2>

    <a id="user-content-credit" class="anchor" href="#credit" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credit</h2>

    <p>A lot of the coding infrastructure is based on <a href="https://github.com/rll/rllab">rllab</a>.

    Also, the serialization and logger code are basically a carbon copy.</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1618430781.0
Clinical-Genomics-Lund/nextflow_rnaseqfus:
  data_format: 2
  description: 'Fusion genes identification pipeline '
  filenames:
  - container/Singularity
  full_name: Clinical-Genomics-Lund/nextflow_rnaseqfus
  latest_release: null
  readme: '<h1>

    <a id="user-content-work-in-progress" class="anchor" href="#work-in-progress"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>work
    in progress</h1>

    <p>attempt to build a base singularity image with spack that can be used as the
    bootstrap for

    other singularity images that would perform the spack install of a particular
    package</p>

    <p>currently having an issue with stage directory for spack attempting to write
    to

    the immutable squashfs</p>

    <p>as expected, the child container will happily install during %post since it
    can write</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1612308500.0
Clinical-Genomics-Lund/nextflow_tumwgs:
  data_format: 2
  description: null
  filenames:
  - container/Singularity
  full_name: Clinical-Genomics-Lund/nextflow_tumwgs
  latest_release: null
  readme: '<h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <ol>

    <li>git clone --recurse-submodules <a href="https://github.com/mgroth0/dnn">https://github.com/mgroth0/dnn</a>

    </li>

    <li>install <a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow">miniconda</a>

    </li>

    <li><code>conda update conda</code></li>

    <li>

    <code>conda create --name dnn --file requirements.txt</code> (requirements.txt
    is currently not working, TODO)</li>

    <li>might need to separately <code>conda install -c mgroth0 mlib-mgroth0</code>--
    When updating, use <code>conda install --file requirements.txt;</code>

    </li>

    <li><code>conda activate dnn</code></li>

    </ol>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ul>

    <li>

    <p>./dnn</p>

    </li>

    <li>

    <p>Generate some images, train/test a model, run analyses, and generate plots.
    Tested on Mac, but not yet on linux/Windows.</p>

    </li>

    <li>

    <p><code>./dnn -cfg=gen_images --INTERACT=0</code></p>

    </li>

    <li>

    <p><code>./dnn -cfg=test_one --INTERACT=0</code></p>

    </li>

    </ul>

    <p>The second command will fail with a Mathematica-related error, but your results
    will be saved in <code>_figs</code>.</p>

    <p>TODO: have to also consider running and developing other executables here:
    human_exp_1 and human_analyze</p>

    <h2>

    <a id="user-content-configuration" class="anchor" href="#configuration" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h2>

    <p>-MODE: (default = FULL) is a string that can contain any combination of the
    following (example: "CLEAN JUSTRUN")</p>

    <ul>

    <li>CLEAN</li>

    <li>JUSTRUN</li>

    <li>GETANDMAKE</li>

    <li>MAKEREPORT</li>

    </ul>

    <p>Edit <a href="">cfg.yml</a> to save configuration options. Feel free to push
    these.</p>

    <p>If there is anything hardcoded that you''d like to be configurable, please
    submit an issue.</p>

    <h2>

    <a id="user-content-testing" class="anchor" href="#testing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Testing</h2>

    <p>todo</p>

    <h2>

    <a id="user-content-development" class="anchor" href="#development" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Development</h2>

    <ul>

    <li>TODO: have separate development and user modes. Developer mode has PYTHONPATH
    link to mlib and instructions for resolving and developing in ide in parallel.
    User mode has mlib as normal dependency. might need to use <code>conda uninstall
    mlib-mgroth0 --force</code>. Also in these public readmes or reqs.txt I have to
    require a specific mlib version</li>

    <li>./dnn build</li>

    </ul>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>Darius, Xavier, Pawan</p>

    <p>heuritech, raghakot, joel</p>

    '
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1619189676.0
Clinical-Genomics-Lund/nextflow_wgs:
  data_format: 2
  description: null
  filenames:
  - container/Singularity
  - container/Singularity_madeline2
  full_name: Clinical-Genomics-Lund/nextflow_wgs
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hd_singularity\" class=\"anchor\" href=\"#hd_singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>hd_singularity</h1>\n<p>Files to support building and maintenance\
    \ of Singularity containers for Hall D.</p>\n<p>Contains scripts and recipes for\
    \ creating Singularity containers from scratch.</p>\n<p>The main script is scripts/create_gluex_container.sh.\
    \ Its usage message is as follows:</p>\n<pre><code>Usage: create_gluex_container.sh\
    \ [-h] -r &lt;recipe-file&gt; -p &lt;prereqs-script&gt; \\\n       [-d DIRECTORY]\
    \ [-t STRING]\n\nNote: must be run as root\n\nOptions:\n  -h print this usage\
    \ message\n  -r Singularity recipe file\n  -p script that installs gluex software\n\
    \  -d output directory for containers (default: current working directory)\n \
    \ -t token to be used to name containers (default = extension in \"Singularity.ext\"\
    )\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1621964896.0
CompArchCam/Cinnamon:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity
  full_name: CompArchCam/Cinnamon
  latest_release: null
  readme: '<h1>

    <a id="user-content-cinnamon" class="anchor" href="#cinnamon" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cinnamon</h1>

    <p>This directory contains the code for the Cinnamon language compiler.  This
    compiler is described in the paper:</p>

    <p>Cinnamon: A Domain-Specific Language for Binary Profiling and Monitoring,

    Mahwish Arif, Ruoyu Zhou, Hsi-Ming Ho and Timothy M. Jones,

    CGO 2021</p>

    <p>Please cite this paper if you produce any work that builds upon this code and
    / or data.</p>

    <h2>

    <a id="user-content-licence" class="anchor" href="#licence" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Licence</h2>

    <p>Cinnamon is released under an Apache licence.</p>

    <h2>

    <a id="user-content-building-cinnamon" class="anchor" href="#building-cinnamon"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Cinnamon</h2>

    <p>Cinnamon can currently target three different binary frameworks; Janus, Pin
    and Dyninst.  To build the compiler:</p>

    <pre lang="shell-session"><code>export CINNAMON_ROOT = /path/to/cinnamon-source

    cd $(CINNAMON_ROOT)

    </code></pre>

    <p>To build the Cinnamon backend for Janus:</p>

    <pre lang="shell-session"><code>make TARGET=janus

    </code></pre>

    <p>To build the Cinnamon backend for Pin:</p>

    <pre lang="shell-session"><code>make TARGET=pin

    </code></pre>

    <p>To build the Cinnamon backend for Dyninst:</p>

    <pre lang="shell-session"><code>make TARGET=dyninst

    </code></pre>

    <h2>

    <a id="user-content-compiling-a-sample-program" class="anchor" href="#compiling-a-sample-program"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Compiling
    a sample program</h2>

    <p>Cinnamon sample programs are available in the  <code>tests</code> directory.  The
    following commands will compile the Cinnamon program <code>ins.dsl</code> and
    integrate the resulting code into one of the target frameworks. You will need
    to set the path to your target framework installation in the respective scripts:</p>

    <pre lang="shell-session"><code>$(CINNAMON_ROOT)/Scripts/compileToJanus.py $CINNAMON_ROOT/tests/ins.dsl

    $(CINNAMON_ROOT)/Scripts/compileToPin.py $CINNAMON_ROOT/tests/ins.dsl

    $(CINNAMON_ROOT)/Scripts/compileToDyn.py $CINNAMON_ROOT/tests/ins.dsl

    </code></pre>

    <p>After this, the final tool can be built and run using the target framework''s
    build instructions.</p>

    <p>If you just want to compile the Cinnamon DSL code and not yet integrate it
    into a target framework, run the following command.  This will generate a number
    of different files containing relevant code for the cinnamon program:</p>

    <pre lang="shell-session"><code>cd $CINNAMON_ROOT

    ./bdc $CINNAMON_ROOT/tests/ins.dsl

    </code></pre>

    <h2>

    <a id="user-content-target-frameworks" class="anchor" href="#target-frameworks"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Target
    frameworks</h2>

    <h3>

    <a id="user-content-janus" class="anchor" href="#janus" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Janus</h3>

    <p>You can get the Janus implementation with placeholders, templates and utility
    libraries for Cinnamon from the main Janus repository at <a href="https://github.com/timothymjones/Janus.git">https://github.com/timothymjones/Janus.git</a>,
    then switch to the <code>cinnamon</code> branch.</p>

    <pre lang="shell-session"><code>git clone https://github.com/timothymjones/Janus.git

    cd Janus

    git checkout -b cinnamon origin/cinnamon

    </code></pre>

    <p>Next set <code>JanusPATH</code> in <code>compileToJanus.py</code> to be the
    location that you have cloned Janus.</p>

    <p>Once the code for Janus has been generated and integrated (after running the
    <code>compileToJanus.py</code> script from above), you can build the final tool
    using the following commands:</p>

    <pre lang="shell-session"><code>(cd build; cmake ..; make -j8)

    </code></pre>

    <p>To run the final tool on the target binary:</p>

    <pre lang="shell-session"><code>./janus/jdsl_run &lt;target_binary&gt;

    </code></pre>

    <h3>

    <a id="user-content-pin" class="anchor" href="#pin" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Pin</h3>

    <p>Everything required for Pin is contained within the <code>targets/Pin</code>
    directory.  Copy the <code>MyDSLTool</code> directory to <code>path-to-your-pin-root-dir/source/tools</code>,
    where <code>path-to-your-pin-root</code> should be self-explanatory.</p>

    <p>Next set <code>PinPATH=your-pin-root-dir/source/tools/MyDSLTool</code> in <code>compileToPin.py</code>.</p>

    <p>Once the code for Pin has been generated and integrated (after running the
    <code>compileToPin.py</code> script from above), you can build the final tool
    using the following commands:</p>

    <pre lang="shell-session"><code>cd your-pin-root-dir/source/tools/MyDSLTool

    make obj-intel64/MyDSLTool.so

    </code></pre>

    <p>To run the final tool on the target binary:</p>

    <pre lang="shell-session"><code>your-pin-root-dir/pin -t obj-intel64/MyDSLTool.so
    -- &lt;target_binary&gt;

    </code></pre>

    <h3>

    <a id="user-content-dyninst" class="anchor" href="#dyninst" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dyninst</h3>

    <p>You can obtain Dyninst version 10.1.0 as follows:</p>

    <pre lang="shell-session"><code>wget https://github.com/dyninst/dyninst/archive/v10.1.0.tar.gz``

    tar xzvf v10.1.0.tar.gz

    </code></pre>

    <p>Once extracted, add <code>c_LoadInsn</code> and <code>c_StoreInsn</code> into
    <code>enum InsnCategory</code> in <code>dyninst-10.1.0/instructionAPI/h/InstructionCategories.h</code>
    and then build by following the Dyninst build instructions.</p>

    <p>Everything else required for Dyninst is contained within the <code>targets/Dyninst</code>
    directory.  Copy the <code>MyDSLTool</code> directory to <code>path-to-your-dyn-root-dir/examples</code>,
    where <code>path-to-your-dyn-root-dir</code> should be self-explanatory.</p>

    <p>Next set <code>DynPATH=path-to-your-dyn-root-dir/examples/MyDSLTool</code>
    in <code>compileToDyn.py</code>.</p>

    <p>Once the code for Dyninst has been generated and integrated (after running
    the <code>compileToDyn.py</code> script from above), you can build the final tool
    using the following commands:</p>

    <pre lang="shell-session"><code>cd path-to-your-dyn-root-dir/examples/MyDSLTool

    make

    </code></pre>

    <p>To run the final tool on the target binary:</p>

    <pre lang="shell-session"><code>path-to-your-dyn-root-dir/examples/MyDSLTool/DSLtool
    -m static -o &lt;output_binary&gt; &lt;input_binary&gt;

    </code></pre>

    '
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1621746520.0
Computational-Plant-Science/3D_model_reconstruction_demo:
  data_format: 2
  description: Implementation of the Visual Structure from Motion algorithm optimized
    for plant branching structures.
  filenames:
  - Singularity_colmap_vsfm
  - Singularity_recipe/Singularity
  - model_preprocess/Singularity
  full_name: Computational-Plant-Science/3D_model_reconstruction_demo
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-3d-root-model-reconstruction\" class=\"anchor\"\
    \ href=\"#3d-root-model-reconstruction\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>3D root model reconstruction</h1>\n\
    <p>The software package was integrated as a module at PlantIT website at : <a\
    \ href=\"https://portnoy.cyverse.org/\" rel=\"nofollow\">https://portnoy.cyverse.org/</a>.\n\
    (Collaborate with Cyverse <a href=\"https://www.cyverse.org/\" rel=\"nofollow\"\
    >https://www.cyverse.org/</a> ) . Users are welcomed to registered as an user\
    \ to try this package via PlantIT website.</p>\n<p>The software package was also\
    \ available at Dockerhub (<a href=\"https://hub.docker.com/r/computationalplantscience/3d-model-reconstruction\"\
    \ rel=\"nofollow\">https://hub.docker.com/r/computationalplantscience/3d-model-reconstruction</a>)\
    \ for advanced users to run locally via singularity at Linux environment:</p>\n\
    <p>This software can be run by docker container, users do not need to install\
    \ many libraries and compile complex source files.</p>\n<p>Following was the demo\
    \ of the comparision of real root and reconstructed 3D model model.</p>\n<p><a\
    \ href=\"../master/media/ProjectDemo.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img src=\"../master/media/ProjectDemo.gif\" alt=\"Optional Text\" style=\"max-width:100%;\"\
    ></a></p>\n<h1>\n<a id=\"user-content-setup-docker-container\" class=\"anchor\"\
    \ href=\"#setup-docker-container\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Setup Docker container</h1>\n<p>OS\
    \ requirements</p>\n<pre><code>To install Docker container (https://docs.docker.com/engine/install/ubuntu/):\
    \ \n\nTo install Docker Engine, you need the 64-bit version of one of these Ubuntu\
    \ versions:\n\nUbuntu Groovy 20.10\nUbuntu Focal 20.04 (LTS)\nUbuntu Bionic 18.04\
    \ (LTS)\nUbuntu Xenial 16.04 (LTS)\n\nDocker Engine is supported on x86_64 (or\
    \ amd64), armhf, and arm64 architectures.\n\nUninstall old versions\n$ sudo apt-get\
    \ remove docker docker-engine docker.io containerd runc\n\nSet up the repository\n\
    \nUpdate the apt package index and install packages to allow apt to use a repository\
    \ over HTTPS:\n\n$ sudo apt-get update\n\n$ sudo apt-get install \\\n    apt-transport-https\
    \ \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\
    \nAdd Docker\u2019s official GPG key:\n\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg\
    \ | sudo apt-key add -\n\nVerify that you now have the key with the fingerprint\
    \ 9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88, by searching for the last\
    \ 8 characters of the fingerprint.\n\n$ sudo apt-key fingerprint 0EBFCD88\n\n\
    pub   rsa4096 2017-02-22 [SCEA]\n      9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C\
    \ 0EBF CD88\nuid           [ unknown] Docker Release (CE deb) &lt;docker@docker.com&gt;\n\
    sub   rsa4096 2017-02-22 [S]\n\n$ sudo add-apt-repository \\\n   \"deb [arch=amd64]\
    \ https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\
    \n\nUpdate the apt package index, and install the latest version of Docker Engine\
    \ and containerd, or go to the next step to install a specific version:\n\n$ sudo\
    \ apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\
    \nVerify that Docker Engine is installed correctly by running the hello-world\
    \ image.\n\n$ sudo docker run hello-world\n</code></pre>\n<h1>\n<a id=\"user-content-run-this-container-by-building-it-locally\"\
    \ class=\"anchor\" href=\"#run-this-container-by-building-it-locally\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run\
    \ this container by building it locally:</h1>\n<pre><code># Clone source code\
    \ to your local path\n$ git clone https://github.com/Computational-Plant-Science/3D_model_reconstruction_demo.git\n\
    \n# Enter into the source code folder named as \"cd 3D_model_reconstruction_demo\"\
    \n$ cd 3D_model_reconstruction_demo/\n\n# Build docker container locally named\
    \ as \"3d_model_reconstruction\" using \"Dockerfile\" in the same folder, note:\
    \ docker repository name must be lowercase.\n$ docker build -t 3d_model_reconstruction\
    \ -f Dockerfile .\n\n# Run the docker container by linking docker container data\
    \ path to user's image data folder local path\n# Note: please replace $path_to_image_folder\
    \ as your local image data folder path, \n# Suggest to check your image folder\
    \ path using \"pwd\" command\n# Example: $ docker run -v /home/suxing/example/root_images:/srv/images\
    \ -it 3d_model_reconstruction\n\n$ docker run -v /$path_to_image_folder:/srv/images\
    \ -it 3d_model_reconstruction\n\n# After launch the docker container, run \"pipeline.sh\"\
    \ or \"pipeline.sh\" insider the container\n$ root@.....:/opt/code# python3 pipeline.py\n\
    \n# Get 3d model result named as \"dense.0.ply\"\n# After the container was executed\
    \ successfully with image data files, user should be able to see output in your\
    \ command window like this:\n'''\nLoading option-0000.ply, 48656 vertices ...\n\
    Save to /srv/images/dense.nvm ... done\nSave /srv/images/dense.0.ply ...done\n\
    ----------------------------------------------------------------\n'''\nThe 3D\
    \ model file was in ply format(https://en.wikipedia.org/wiki/PLY_(file_format)),\
    \ it is located inside your image folder, its name is \"dense.0.ply\".\npath =\
    \ \"/$path_to_image_folder/dense.0.ply\"\n\nTo visualize the 3d model file, suggest\
    \ to install Meshlab(https://www.meshlab.net/) or cloudcompare(https://www.danielgm.net/cc/)\n\
    </code></pre>\n<h1>\n<a id=\"user-content-author\" class=\"anchor\" href=\"#author\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Author</h1>\n<pre><code>suxing liu(suxingliu@gmail.com)\nWesley Paul\
    \ Bonelli(wbonelli@uga.edu)\n\nReference:\nVisualSFM\n[Anders Damsgaard](mailto:adamsgaard@ucsd.edu)\
    \ with contributions by Caleb Adams and Connor P Doherty.\nChangchang Wu ( wucc1130@gmail.com\
    \ )\n+ Structure from Motion\n[1] Changchang Wu, \"Towards Linear-time Incremental\
    \ Structure From Motion\", 3DV 2013\n[2] Changchang Wu, \"VisualSFM: A Visual\
    \ Structure from Motion System\", http://ccwu.me/vsfm/, 2011\n+ Bundle Adjustment\n\
    [3] Changchang Wu, Sameer Agarwal, Brian Curless, and Steven M. Seitz, \"Multicore\
    \ Bundle Adjustment\", CVPR 2011   \n+ Feature Detection\n[4] Changchang Wu, \"\
    SiftGPU: A GPU implementation of Scale Invaraint Feature Transform (SIFT)\", http://cs.unc.edu/~ccwu/siftgpu,\
    \ 2007\n\nCOLMAP\nhttps://colmap.github.io\nAuthor: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n\
    @inproceedings{schoenberger2016sfm,\n    author={Sch\\\"{o}nberger, Johannes Lutz\
    \ and Frahm, Jan-Michael},\n    title={Structure-from-Motion Revisited},\n   \
    \ booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},\n\
    \    year={2016},\n}\n\n@inproceedings{schoenberger2016mvs,\n    author={Sch\\\
    \"{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm,\
    \ Jan-Michael},\n    title={Pixelwise View Selection for Unstructured Multi-View\
    \ Stereo},\n    booktitle={European Conference on Computer Vision (ECCV)},\n \
    \   year={2016},\n}\n</code></pre>\n<p>Docker container was maintained by Wesley\
    \ Paul Bonelli. it was deployed to Plant IT website by Wesley Paul Bonelli (<a\
    \ href=\"mailto:wbonelli@uga.edu\">wbonelli@uga.edu</a>).</p>\n<p>Singularity\
    \ container overlay issues were solved by [Saravanaraj Ayyampalayam] (<a href=\"\
    https://github.com/raj76\">https://github.com/raj76</a>) (mailto:<a href=\"mailto:raj76@uga.edu\"\
    >raj76@uga.edu</a>)</p>\n<p>Special thanks to Chris Cotter building the container\
    \ recipe for testing and debugging.</p>\n<h2>\n<a id=\"user-content-todo\" class=\"\
    anchor\" href=\"#todo\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Todo</h2>\n<ul>\n<li>GPU cuda version container</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h2>\n<p>GNU Public License</p>\n"
  stargazers_count: 4
  subscribers_count: 3
  topics:
  - phenotyping
  - phenotyping-algorithms
  - root
  - plant
  updated_at: 1619688010.0
Computational-Plant-Science/3D_model_traits_demo:
  data_format: 2
  description: Computation of root phenes from 3D point clouds.
  filenames:
  - Singularity
  - model_preprocess/Singularity
  full_name: Computational-Plant-Science/3D_model_traits_demo
  latest_release: null
  readme: '<h1>

    <a id="user-content-3d_model_traits_measurement" class="anchor" href="#3d_model_traits_measurement"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3D_model_traits_measurement</h1>

    <p>Function: Extract gemetrical traits of 3D model</p>

    <p>Author            : Suxing Liu</p>

    <p>Date created      : 04/04/2018</p>

    <p>Date last modified: 04/25/2020</p>

    <p>Python Version    : 3.7</p>

    <p>Example of structure vs. 3D root model</p>

    <p><a href="../master/media/image3.png" target="_blank" rel="noopener noreferrer"><img
    src="../master/media/image3.png" alt="Optional Text" style="max-width:100%;"></a>
    <a href="../master/media/image2.gif" target="_blank" rel="noopener noreferrer"><img
    src="../master/media/image2.gif" alt="Optional Text" style="max-width:100%;"></a></p>

    <p>Example of connecting disconnected root path</p>

    <p><a href="../master/media/image7.png" target="_blank" rel="noopener noreferrer"><img
    src="../master/media/image7.png" alt="Optional Text" style="max-width:100%;"></a>
    <a href="../master/media/image8.png" target="_blank" rel="noopener noreferrer"><img
    src="../master/media/image8.png" alt="Optional Text" style="max-width:100%;"></a></p>

    <p>usage:</p>

    <p>python3 pipeline.py -p /$path_to_your_3D_model/ -m 3D_model_name.ply</p>

    <p>Singularity test:</p>

    <p>sudo singularity build --writable model-scan.img Singularity</p>

    <p>singularity exec model-scan.img python /opt/code/pipeline.py -p /$path_to_your_3D_model/
    -m surface.ply</p>

    <p>singularity exec shub://lsx1980/3D_model_traits_measurement python /opt/code/pipeline.py
    -p /$path_to_your_3D_model/ -m surface.ply</p>

    <ul>

    <li>Pre-requisite:

    <ul>

    <li>Python3.7</li>

    <li>Numpy</li>

    <li>SciPy</li>

    <li>Opencv 3.0 for Python - <a href="http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/"
    rel="nofollow">Installation</a>

    </li>

    </ul>

    </li>

    </ul>

    <p>Visualization requirement:</p>

    <p>pip3 install numba <br>

    imagesize <br>

    progressbar2 <br>

    mayavi <br>

    PyQt5 <br>

    networkx</p>

    <p>python3 graph_compute.py -p /&amp;path/active_component/</p>

    '
  stargazers_count: 3
  subscribers_count: 3
  topics:
  - phenotyping
  - phenotyping-algorithms
  - phenomics
  updated_at: 1620288189.0
Crown421/Singularity.jl:
  data_format: 2
  description: null
  filenames:
  - basebuilds/Singularity.juliabase
  - basebuilds/Singularity.jupyterbase
  full_name: Crown421/Singularity.jl
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity</h1>\n<p>This package was presented at JuliaCon, and the\
    \ presentation is available <a href=\"https://musing-pare-2ba365.netlify.app/#/\"\
    \ rel=\"nofollow\">under this link</a>.</p>\n<p>This package provides a rough\
    \ interface to create <a href=\"https://github.com/sylabs/singularity\">Singularity\
    \ containers</a> from DrWatson Projects.\nIt currently works best on Linux systems,\
    \ as the build command currently not available on Mac.</p>\n<p>Most of the code\
    \ is still very WIP and based on my own processes and needs. If you use Singularity,\
    \ or have a use case that I haven't considered yet, please reach out either by\
    \ email or by opening an issue.</p>\n<h2>\n<a id=\"user-content-install-singularity\"\
    \ class=\"anchor\" href=\"#install-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install singularity</h2>\n<p>The\
    \ <a href=\"https://sylabs.io/guides/3.0/user-guide/installation.html\" rel=\"\
    nofollow\">Sylab documentation</a> contains instructions to install Singularity,\
    \ but appears to be slightly out of date.\nYou can find additional information\
    \ for</p>\n<ul>\n<li>Linux on this <a href=\"https://github.com/hpcng/singularity/blob/master/INSTALL.md\"\
    >Github readme</a>\n</li>\n<li>Mac on the <a href=\"https://sylabs.io/singularity-desktop-macos/\"\
    \ rel=\"nofollow\">download page for the beta release</a>\n</li>\n<li>Windows,\
    \ see this <a href=\"https://github.com/hpcng/singularity/issues/4518\">this issue</a>\
    \ stating that WSL 2 is required (<a href=\"https://docs.microsoft.com/en-us/windows/wsl/install-win10\"\
    \ rel=\"nofollow\">instructions</a>). After installing it, follow the instructions\
    \ for Linux.</li>\n</ul>\n<p>Alternatively, you can use the <a href=\"https://sylabs.io/guides/3.0/user-guide/installation.html#install-on-windows-or-mac\"\
    \ rel=\"nofollow\">instructions for Vagrant</a> on Windows and Mac.</p>\n<h3>\n\
    <a id=\"user-content-basis\" class=\"anchor\" href=\"#basis\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Basis</h3>\n\
    <p>This package uses a minimal debian-based container with Julia installed as\
    \ a base. On the <a href=\"https://cloud.sylabs.io/home\" rel=\"nofollow\">Sylab\
    \ cloud</a> you can find the Juliabase image, and an experimental container also\
    \ including jupyter</p>\n<ul>\n<li>\n<a href=\"https://cloud.sylabs.io/library/_container/5e418a1b2758e9ed1175de24\"\
    \ rel=\"nofollow\">juliabase</a>: 1.4.2, 1.3.1</li>\n<li>\n<a href=\"https://cloud.sylabs.io/library/_container/5f20adbeae86dd3232dec1d1\"\
    \ rel=\"nofollow\">jupyterbase</a>: 1.4.2</li>\n</ul>\n<p>If you would prefer\
    \ to build them yourself, the def files are available in the <code>basebuilds</code>\
    \ folder.</p>\n<h3>\n<a id=\"user-content-assumptions\" class=\"anchor\" href=\"\
    #assumptions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Assumptions:</h3>\n<p>The package assumes that that\
    \ the folder structure contains the following elements</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>\u251C\u2500\u2500 scripts\n\u2502   \u251C\u2500\
    \u2500 run\n\u251C\u2500\u2500 src\n\u2502   \u251C\u2500\u2500 module1\n\u2502\
    \   \u251C\u2500\u2500 module2\n\u251C\u2500\u2500 container\n\u251C\u2500\u2500\
    \ <span class=\"pl-k\">&lt;</span>other folders<span class=\"pl-k\">&gt;</span>\n\
    \u251C\u2500\u2500 Project.toml\n\u251C\u2500\u2500 Manifest.toml</pre></div>\n\
    <p>and everything is under the control of a single git repository. This will be\
    \ automatically the case if the project folder was created by <a href=\"https://github.com/JuliaDynamics/DrWatson.jl\"\
    >DrWatson.jl</a>.</p>\n<p>The src and scripts folder will be copied into the container,\
    \ and it is further assumed that the modules in it are registered as <code>dev</code>ed\
    \ in the project <code>Manifest.toml</code>.</p>\n<h3>\n<a id=\"user-content-warning\"\
    \ class=\"anchor\" href=\"#warning\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Warning:</h3>\n<p>Calling <code>buildsif</code>\
    \ will ask for root privileges, as the underlying <code>singularity build</code>\
    \ commands requires it. This is clearly a potential security risk, so if you are\
    \ unsure, please inspect the <code>Singularity.pack</code> file in the <code>container</code>\
    \ folder.</p>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h2>\n<p>The package provides the following functions. All these\
    \ functions work from any folder as long as the correct project environment is\
    \ loaded. They are also still WIP, so there is very little error checking being\
    \ done.</p>\n<pre><code>    generate_deffile(; excludepkgs = [], commit = \"master\"\
    )\n</code></pre>\n<p>Creates the <code>container</code> folder if it does not\
    \ exist yet, and generates the <code>Singularity.pack</code> def file.</p>\n<ul>\n\
    <li>\n<code>excludepkgs</code> accepts and array of package names. These packages\
    \ will be removed from <code>Project.toml</code> inside the container. This is\
    \ for packages that are needed locally, for example for visualization, but are\
    \ not needed in the container and would only add bloat.</li>\n<li>\n<code>commit</code>\
    \ accept any project commit hash, and will build the container using the <code>src</code>\
    \ and <code>script</code> folder from that commit. Requires the git setting below.</li>\n\
    </ul>\n<pre><code>    buildsif(;verbose = false, force = true)\n</code></pre>\n\
    <p>Builds the container image into the <code>container</code> folder based on\
    \ the existing def file.</p>\n<ul>\n<li>\n<code>verbose</code> sends all the output\
    \ of the build process to the REPL if set to <code>true</code>, otherwise it will\
    \ be written to file.</li>\n<li>\n<code>force</code> set to <code>true</code>\
    \ causes an existing image to be overwritten without asking for confirmation.</li>\n\
    </ul>\n<pre><code>    recreatedata(file::String; dir = [])\n</code></pre>\n<p>Extracts\
    \ the git commit hash and script name from a <code>DrWatson.@tagsave</code>d file,\
    \ and generated a def file. The resulting container, when <code>singularity run</code>,\
    \ should recreate the initial file directly.</p>\n<ul>\n<li>\n<code>dir</code>\
    \ allows the specification of a subdirectory of the <code>DrWatson.datadir()</code>\
    \ directory.</li>\n</ul>\n<pre><code>    servertransfer(host)\n</code></pre>\n\
    <p>Transfers the image to the <code>host</code> into a folder in the home directory\
    \ of the same name as the project folder. This assumed that everything is configured\
    \ such that <code>ssh host</code> just works.</p>\n<h2>\n<a id=\"user-content-further-info\"\
    \ class=\"anchor\" href=\"#further-info\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Further info</h2>\n<p>Currently,\
    \ the commands build a single read-only image. This means, that after any change\
    \ in the project the entire image needs to be rebuilt. This is partly as intended,\
    \ as the result is a tamper-proof complete environment, that can be used at any\
    \ point in the future the return the exact same results.\nHowever for projects\
    \ that are still under more rapid development, I have possible ideas to make that\
    \ initial phase not require frequent lengthy rebuilds.</p>\n<h2>\n<a id=\"user-content-further-work\"\
    \ class=\"anchor\" href=\"#further-work\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Further work:</h2>\n<ul>\n<li>Generate\
    \ different def files</li>\n<li>add interaction with singularity cloud and hub\
    \ (pushing and pulling)</li>\n<li>signing</li>\n<li>add tests</li>\n<li>add various\
    \ error handling and options</li>\n<li>(big) do some remote builder magic to make\
    \ this work on windows/ mac\n<ul>\n<li>Automate image build on repo push, as mentioned\
    \ on <a href=\"https://singularityhub.github.io/singularityhub-docs/docs/builds/automated\"\
    \ rel=\"nofollow\">singularity hub</a>\n</li>\n</ul>\n</li>\n<li>(bigger) add\
    \ singularity binary ?</li>\n</ul>\n"
  stargazers_count: 17
  subscribers_count: 2
  topics: []
  updated_at: 1616523817.0
DavidEWarrenPhD/afnipype:
  data_format: 2
  description: Neurodocker Docker/Singularity container build scripts for AFNI+Python
    3.6+nipype
  filenames:
  - Singularity
  full_name: DavidEWarrenPhD/afnipype
  latest_release: null
  readme: '<h1>

    <a id="user-content-afnipype" class="anchor" href="#afnipype" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>afnipype</h1>

    <p>Neurodocker Docker/Singularity container build scripts for AFNI+Python 3.6+nipype</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621289479.0
DeepLearnPhysics/larcv2-singularity:
  data_format: 2
  description: null
  filenames:
  - arxiv/Singularity.ub16.04-tf1.10.1-torch0.4.1-root6.14.04
  - arxiv/Singularity.ubuntu16.04-larcv_develop
  - arxiv/Singularity.ub16.04-tf1.11.0-torch0.4.1-root6.14.04
  - arxiv/Singularity.ub16.04-tf1.11.0-torch0.4.1
  - arxiv/Singularity.ubuntu16.04-gpu
  - arxiv/Singularity.ub16.04-tf1.10.1-torch0.4.1
  - arxiv/Singularity.ubuntu16.04-gpu-larcv_develop
  - arxiv/Singularity.ubuntu16.04-basic
  - arxiv/Singularity.ubuntu16.04-gpu-py3
  - arxiv/Singularity.ub16.04-tf1.7-torch0.4
  - recipes/Singularity.ub16.04-cuda100-pytorch1.0.0-scn
  - recipes/Singularity.ub16.04-cuda90-pytorch0.4.1
  - recipes/Singularity
  - recipes/Singularity.ub18.04-gpu-ana0-ml-larcv2
  - recipes/Singularity.ub18.04-gpu-ana0-ml
  - recipes/Singularity.ub16.04-cuda90-pytorch1.0.0-scn
  - recipes/Singularity.ub16.04-cuda90-tf1.12.0
  - recipes/Singularity.ub18.04-cuda10.2-extra
  - recipes/Singularity.ub16.04-cuda100-pytorchdev20181215
  - recipes/Singularity.HKMLWorkshop
  - recipes/Singularity.ub18.04-gpu-ana0
  - recipes/Singularity.ub18.04-gpu-ana0-mn
  - recipes/Singularity.ub18.04-cpu-ana0
  - recipes/Singularity.ub16.04-cuda90-py3-pytorch1.0.1-scn
  - recipes/Singularity.ub18.04-cuda10.1-ml-larcv2
  - recipes/Singularity.ub16.04-cuda90-pytorchdev20181015
  - recipes/Singularity.ub18.04-cuda100-py3-pytorch1.1.0-scn-docker
  - recipes/Singularity.ub18.04-cpu
  - recipes/Singularity.ub18.04-gpu
  - recipes/Singularity.ub18.04-cuda100-py3-pytorch1.1.0-scn
  - recipes/Singularity.ub18.04-cpu-ana0-larcv2
  - recipes/Singularity.ub18.04-cuda100-py3-pytorch1.0.1-scn
  full_name: DeepLearnPhysics/larcv2-singularity
  latest_release: null
  readme: '<p><a href="https://raw.githubusercontent.com/DeepLearnPhysics/larcv2-singularity/master/LICENSE"
    rel="nofollow"><img src="https://camo.githubusercontent.com/2ff6a06f2f6e08b17783133ca7ebc23ce1f8ac4415eee8e835647b57048a8f0d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e737667"
    alt="license" data-canonical-src="https://img.shields.io/github/license/mashape/apistatus.svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/459" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-larcv2-singularity" class="anchor" href="#larcv2-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>larcv2-singularity</h1>

    <p>Singularity build scripts for <a href="https://www.singularity-hub.org/collections/459"
    rel="nofollow">singularity hub</a>. You can learn about Singularity in <a href="https://github.com/DeepLearnPhysics/playground-singularity/wiki">our
    wiki</a> or <a href="https://www.sylabs.io/docs/" rel="nofollow">official doc</a>.</p>

    <p>To <code>pull</code> the container, simply try</p>

    <pre><code>TAG=latest

    singularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:$TAG

    </code></pre>

    <p>For more fun things to do, you can read <a href="https://github.com/DeepLearnPhysics/playground-singularity/wiki">our
    wiki</a>.</p>

    <h2>

    <a id="user-content-whats-in-the-build" class="anchor" href="#whats-in-the-build"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What''s
    in the build?</h2>

    <p>All builds are based on <strong>Ubuntu16.04 LTS</strong> with some highlighted
    packages below</p>

    <ul>

    <li>Python packages: <code>pip</code> <code>numpy</code> <code>scipy</code> <code>scikit</code>
    <code>opencv-python</code> <code>h5py</code> <code>tables</code> <code>pandas</code>
    <code>matplotlib</code> <code>ipython</code> <code>jupyter notebook</code> <code>pyyaml</code>
    <code>zmq</code>

    </li>

    <li>Development kit: <code>g++</code>/<code>gcc</code> <code>libqt4-dev</code>
    <code>python-dev</code> <code>cuda-9.0</code> <code>cudnn-7</code> <code>cython</code>

    </li>

    <li>Utility kit    : <code>git</code> <code>wget</code> <code>emacs</code> <code>vim</code>
    <code>asciinema</code>

    </li>

    </ul>

    <p>We build 3 types of images.</p>

    <ul>

    <li>

    <em>Base</em> image

    <ul>

    <li>Latest tag: <strong>ub16.04-tf1.10.1-torch0.4.1</strong>

    </li>

    <li>

    <code>tensorflow-gpu</code> 1.10.1, <code>pytorch</code> 0.4.1</li>

    </ul>

    </li>

    </ul>

    <pre><code>singularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:ub16.04-tf1.10.1-torch0.4.1

    </code></pre>

    <ul>

    <li>

    <em>ROOT</em> image (include <em>Base</em>)

    <ul>

    <li>Latest tag: <strong>ub16.04-tf1.10-torch0.4.1-root6.14.04</strong>

    </li>

    <li>

    <code>ROOT</code> 6.14.04, additional python package <code>root_numpy</code>

    </li>

    </ul>

    </li>

    </ul>

    <pre><code>singularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:ub16.04-tf1.10.1-torch0.4.1-root6.14.04

    </code></pre>

    <ul>

    <li>

    <em>LArCV</em> image (include <em>ROOT</em>)

    <ul>

    <li>Tag: <strong>latest</strong>

    </li>

    <li>Additional python package <code>larcv</code>

    </li>

    </ul>

    </li>

    </ul>

    <pre><code>singularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:latest

    </code></pre>

    <h1>

    <a id="user-content-docker-images" class="anchor" href="#docker-images" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Docker images?</h1>

    <p>Checkout built images on our <a href="https://hub.docker.com/u/deeplearnphysics/dashboard/"
    rel="nofollow">docker hub</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - larcv
  - singularity
  - singularity-hub
  updated_at: 1590751288.0
Dill-PICL/GOMAP-base:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: Dill-PICL/GOMAP-base
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/1184" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-gomap-base" class="anchor" href="#gomap-base" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GOMAP-base</h1>

    <p>This is the base image for the GOMAP-singularity container. This base image
    has all the requirements installed for running GOMAP</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620899212.0
Dill-PICL/GOMAP-img-build:
  data_format: 2
  description: The files to build singularity images for GOMAP pipeline
  filenames:
  - singularity/Singularity
  full_name: Dill-PICL/GOMAP-img-build
  latest_release: null
  readme: '<h1>

    <a id="user-content-mycobacterial-pre-processing-pipeline" class="anchor" href="#mycobacterial-pre-processing-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mycobacterial
    Pre-processing Pipeline</h1>

    <p>Cleans and QCs reads with fastp and FastQC, classifies with Kraken2 &amp; Mykrobe,
    removes non-bacterial content, and - by alignment to any minority genomes - disambiguates
    mixtures of bacterial reads.</p>

    <p>Takes as input one directory containing pairs of fastq(.gz) or bam files.

    Produces as output one directory per sample, containing the relevant reports &amp;
    a pair of cleaned fastqs.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>The workflow is designed to run with either docker <code>-profile docker</code>
    or singularity <code>-profile singularity</code>. Before running the workflow
    using singularity, the singularity images for the workflow will need to be built
    by running <code>singularity/singularity_pull.sh</code></p>

    <p>E.g. to run the workflow:</p>

    <pre><code>nextflow run main.nf -profile singularity --filetype fastq --input_dir
    fq_dir --pattern "*_R{1,2}.fastq.gz" --unmix_myco yes \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj


    nextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco
    no \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj

    </code></pre>

    <h2>

    <a id="user-content-params" class="anchor" href="#params" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Params</h2>

    <p>The following parameters should be set in <code>nextflow.config</code> or specified
    on the command line:</p>

    <ul>

    <li>

    <strong>input_dir</strong><br>

    Directory containing fastq OR bam files</li>

    <li>

    <strong>filetype</strong><br>

    File type in input_dir. Either "fastq" or "bam"</li>

    <li>

    <strong>pattern</strong><br>

    Regex to match fastq files in input_dir, e.g. "*_R{1,2}.fq.gz"</li>

    <li>

    <strong>output_dir</strong><br>

    Output directory</li>

    <li>

    <strong>unmix_myco</strong><br>

    Do you want to disambiguate mixed-mycobacterial samples by read alignment? Either
    "yes" or "no"</li>

    <li>

    <strong>species</strong><br>

    Principal species in each sample, assuming genus Mycobacterium. Default ''null''.
    If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae,
    chimaera, fortuitum, intracellulare, kansasii, tuberculosis</li>

    <li>

    <strong>kraken_db</strong><br>

    Directory containing <code>*.k2d</code> Kraken2 database files (obtain from <a
    href="https://benlangmead.github.io/aws-indexes/k2" rel="nofollow">https://benlangmead.github.io/aws-indexes/k2</a>)</li>

    <li>

    <strong>bowtie2_index</strong><br>

    Directory containing Bowtie2 index (obtain from <a href="ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip"
    rel="nofollow">ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip</a>).
    The specified path should NOT include the index name</li>

    <li>

    <strong>bowtie_index_name</strong><br>

    Name of the bowtie index, e.g. hg19_1kgmaj<br>

    </li>

    </ul>

    <br>

    <p>For more information on the parameters run <code>nextflow run main.nf --help</code></p>

    <h2>

    <a id="user-content-checkpoints" class="anchor" href="#checkpoints" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoints</h2>

    <p>Checkpoints used throughout this workflow to fail a sample/issue warnings:</p>

    <p>processes preprocessing_checkFqValidity or preprocessing_checkBamValidity</p>

    <ol>

    <li>(Fail) If sample does not pass fqtools ''validate'' or samtools ''quickcheck'',
    as appropriate.</li>

    </ol>

    <p>process preprocessing_countReads<br>

    2. (Fail) If sample contains &lt; 100k pairs of raw reads.</p>

    <p>process preprocessing_fastp<br>

    3. (Fail) If sample contains &lt; 100k pairs of cleaned reads, required to all
    be &gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10
    --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size
    1 --cut_tail_mean_quality 20).</p>

    <p>process preprocessing_kraken2<br>

    4. (Fail) If the top family hit is not Mycobacteriaceae<br>

    5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae <br>

    6. (Warn) If the top family classification is mycobacterial, but this is not consistent
    with top genus and species classifications<br>

    7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications
    meet minimum thresholds of &gt; 5000 reads or &gt; 0.5% of the total reads (this
    is not necessarily a concern as not all mycobacteria have a taxonomic classification
    at this rank) <br>

    8. (Warn) If sample is mixed or contaminated - defined as containing reads &gt;
    the 5000/0.5% thresholds from multiple non-human species<br>

    9. (Warn) If sample contains multiple classifications to mycobacterial species
    complexes, each meeting the &gt; 5000/0.5% thresholds<br>

    10. (Warn) If no species classification meets the 5000/0.5% thresholds<br>

    11. (Warn) If no genus classification meets the 5000/0.5% thresholds<br>

    12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant
    given point 5)</p>

    <p>process preprocessing_identifyBacterialContaminants<br>

    13. (Fail) If the sample is not contaminated and the top species hit is not one
    of the 10 supported Mycobacteria:\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis<br>

    14. (Fail) If the sample is not contaminated and the top species hit is contrary
    to the species expected (e.g. "avium" rather than "tuberculosis" - only tested
    if you provide that expectation)<br>

    15. (Warn) If the top species hit is supported by &lt; 75% coverage<br>

    16. (Warn) If the top species hit has a median coverage depth &lt; 10-fold<br>

    17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant
    species, which means we will not be able to locate its genome, and thereby remove
    it as a contaminant<br>

    18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated
    with a contaminant species'' taxon ID<br>

    19. (Warn) If no complete genome could be found for a contaminant species. The
    workflow will proceed with alignment-based contaminant removal, but you''re warned
    that there''s reduced confidence in detecting reads from this species</p>

    <p>process preprocessing_downloadContamGenomes<br>

    20. (Fail) If a contaminant is detected but we are unable to download a representative
    genome, and thereby remove it</p>

    <p>process preprocessing_summarise<br>

    21. (Fail) If after having taken an alignment-based approach to decontamination,
    Kraken still detects a contaminant species<br>

    22. (Fail) If after having taken an alignment-based approach to decontamination,
    the top species hit is not one of the 10 supported Mycobacteria<br>

    23. (Fail) If, after successfully removing contaminants, the top species hit is
    contrary to the species expected (e.g. "avium" rather than "tuberculosis" - only
    tested if you provide that expectation)</p>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1620903521.0
DrVale83/bioinfo:
  data_format: 2
  description: null
  filenames:
  - Git/Singularity.igv
  - Git/Singularity.vcftools
  - Git/Singularity.mcl
  - Git/Singularity.orthofinder
  - Git/Singularity.samtools
  - Scaricati/Singularity.igv
  full_name: DrVale83/bioinfo
  latest_release: null
  readme: '<h1>

    <a id="user-content-pileuppipe" class="anchor" href="#pileuppipe" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PileupPipe</h1>

    <p>Run SNP variant calling using gatk4 haplotypecaller. The pipeline performs
    annotation using vep and produces a vcf and excel file.</p>

    <h1>

    <a id="user-content-run-locally" class="anchor" href="#run-locally" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run Locally</h1>

    <p>Analyse a single sample, using a gene list:</p>

    <pre><code>nextflow pileup_pipeline.nf --bam &lt;input_bam&gt; --working_dir &lt;output_folder&gt;
    --genelist &lt;genelist&gt; -w $TMPDIR -c &lt;config_file_from_setup&gt;

    </code></pre>

    <p>Analyse a single sample, with no gene list:</p>

    <pre><code>nextflow pileup_pipeline.nf --bam &lt;input_bam&gt; --working_dir &lt;output_folder&gt;
    -w $TMPDIR -c &lt;config_file_from_setup&gt;

    </code></pre>

    <h1>

    <a id="user-content-run-on-slurm" class="anchor" href="#run-on-slurm" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run on slurm</h1>

    <p>The slurm scripts are wrappers around the pileup_pipeline.nf script.</p>

    <p>Analyse a single sample, using a gene list:</p>

    <pre><code>sbatch SubmitGeneList.sh &lt;input_bam&gt; &lt;output_folder&gt; &lt;genelist&gt;
    &lt;config_file_from_setup&gt;

    </code></pre>

    <p>Analyse a single sample, with no gene list:</p>

    <pre><code>sbatch SubmitNoGeneList.sh  &lt;input_bam&gt; &lt;output_folder&gt;
    &lt;config_file_from_setup&gt;

    </code></pre>

    <p>Analyse a folder containing bam files using a gene list:</p>

    <pre><code>./snpPipe.sh &lt;input_folder_with_bam&gt; &lt;output_folder&gt; &lt;genelist&gt;
    &lt;config_file_from_setup&gt;

    </code></pre>

    <h1>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h1>

    <p>Download and  install nextflow</p>

    <pre><code>https://www.nextflow.io/

    </code></pre>

    <p>run the setup script</p>

    <pre><code>python setup.py &gt; config.conf

    </code></pre>

    <p>Download the singularity collection (note, the singularity collection needs
    to be placed in the pipeline folder)</p>

    <p>singularity pull --name PileUp.simg shub://J35P312/PileupPipe</p>

    <p>or build it yourself:</p>

    <p>singularity build PileUp.simg Singularity</p>

    <p>open the config file using a text editor and change the path variables:</p>

    <pre><code>nano config.conf

    </code></pre>

    <p>The  reference (line 49) path is required:</p>

    <p>Additionally, you need to put a .vep folder containing the vep database insisde
    the PileupPipe folder.</p>

    <p>If you use the slurm wrappers, you need to edit the slurm account in the files,  SubmitGeneList.sh,
    and SubmitNoGeneList.sh</p>

    <h1>

    <a id="user-content-gene-list" class="anchor" href="#gene-list" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Gene list</h1>

    <p>The gene list is a text file. Each line of the text file contains the HGNC
    symbol of a gene.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1562162180.0
EvanYangAB/Recording-OCR-GSOC-2019:
  data_format: 2
  description: This is a pipeline designed to detect and process text in recordings.
  filenames:
  - Singularity
  - Singularity.cuda
  full_name: EvanYangAB/Recording-OCR-GSOC-2019
  latest_release: null
  readme: '<h1>

    <a id="user-content-recording-ocr-gsoc-2019" class="anchor" href="#recording-ocr-gsoc-2019"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Recording-OCR-GSOC-2019</h1>

    <p>This is a pipeline designed to detect and process text in recordings.</p>

    <p>This pipeline is based on mask rcnn and tesseract OCR. Make sure to install
    tesseract and the supporting languages before running the program.</p>

    <p>brew install tesseract</p>

    <p>or</p>

    <p>apt-get install tesseract</p>

    <p>and also</p>

    <p>brew install tesseract-lang</p>

    <p>or</p>

    <p>apt-get install tesseract-lang.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1564050703.0
Freakey17/CP4TP:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: Freakey17/CP4TP
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularitycontainers" class="anchor" href="#singularitycontainers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularityContainers</h1>

    <p>container files for singularity-hub.org to build into images.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1557429544.0
HALFpipe/HALFpipe:
  data_format: 2
  description: ENIGMA Halfpipe is a user-friendly software that facilitates reproducible
    analysis of fMRI data
  filenames:
  - Singularity.def
  full_name: HALFpipe/HALFpipe
  latest_release: 1.1.1
  readme: '<h1>

    <a id="user-content-welcome-to-enigma-halfpipe" class="anchor" href="#welcome-to-enigma-halfpipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome
    to ENIGMA <code>HALFpipe</code>

    </h1>

    <p><a href="https://singularity-hub.org/collections/4508" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22build%22"><img
    src="https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg" alt="https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22continuous+integration%22"><img
    src="https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg"
    alt="https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg"
    style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/HALFpipe/HALFpipe" rel="nofollow"><img src="https://camo.githubusercontent.com/ef5be2978b13a91a1a602bc0261933d3735a7567176db1ef0c13eb65b3249056/68747470733a2f2f636f6465636f762e696f2f67682f48414c46706970652f48414c46706970652f6272616e63682f6d61737465722f67726170682f62616467652e737667"
    alt="codecov" data-canonical-src="https://codecov.io/gh/HALFpipe/HALFpipe/branch/master/graph/badge.svg"
    style="max-width:100%;"></a></p>

    <p><code>HALFpipe</code> is a user-friendly software that facilitates reproducible
    analysis of

    fMRI data, including preprocessing, single-subject, and group analysis. It

    provides state-of-the-art preprocessing using

    <a href="https://fmriprep.readthedocs.io/" rel="nofollow"><code>fmriprep</code></a>,
    but removes the necessity to

    convert data to the

    <a href="https://bids-specification.readthedocs.io/en/stable/" rel="nofollow"><code>BIDS</code></a>
    format. Common

    resting-state and task-based fMRI features can then be calculated on the fly

    using <a href="http://fsl.fmrib.ox.ac.uk/" rel="nofollow"><code>FSL</code></a>
    and

    <a href="https://nipype.readthedocs.io/" rel="nofollow"><code>nipype</code></a>
    for statistics.</p>

    <blockquote>

    <p>If you encounter issues, please see the <a href="#troubleshooting">troubleshooting</a>

    section of this document.</p>

    </blockquote>

    <h2>

    <a id="user-content-table-of-contents" class="anchor" href="#table-of-contents"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Table
    of Contents</h2>


    <ul>

    <li>

    <a href="#getting-started">Getting started</a>

    <ul>

    <li><a href="#container-platform">Container platform</a></li>

    <li><a href="#download">Download</a></li>

    <li><a href="#running">Running</a></li>

    </ul>

    </li>

    <li>

    <a href="#user-interface">User interface</a>

    <ul>

    <li><a href="#files">Files</a></li>

    <li><a href="#features">Features</a></li>

    <li><a href="#models">Models</a></li>

    </ul>

    </li>

    <li><a href="#running-on-a-high-performance-computing-cluster">Running on a high-performance
    computing cluster</a></li>

    <li><a href="#quality-checks">Quality checks</a></li>

    <li>

    <a href="#outputs">Outputs</a>

    <ul>

    <li><a href="#subject-level-features">Subject-level features</a></li>

    <li><a href="#preprocessed-images">Preprocessed images</a></li>

    <li><a href="#group-level">Group-level</a></li>

    </ul>

    </li>

    <li><a href="#troubleshooting">Troubleshooting</a></li>

    <li>

    <a href="#command-line-flags">Command line flags</a>

    <ul>

    <li><a href="#control-command-line-logging">Control command line logging</a></li>

    <li><a href="#automatically-remove-unneeded-files">Automatically remove unneeded
    files</a></li>

    <li><a href="#adjust-nipype">Adjust nipype</a></li>

    <li><a href="#choose-which-parts-to-run-or-to-skip">Choose which parts to run
    or to skip</a></li>

    <li><a href="#working-directory">Working directory</a></li>

    <li><a href="#data-file-system-root">Data file system root</a></li>

    </ul>

    </li>

    <li><a href="#contact">Contact</a></li>

    </ul>


    <h2>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h2>

    <p><code>HALFpipe</code> is distributed as a container, meaning that all required
    software

    comes bundled in a monolithic file, the container. This allows for easy

    installation on new systems, and makes data analysis more reproducible, because

    software versions are guaranteed to be the same for all users.</p>

    <h3>

    <a id="user-content-container-platform" class="anchor" href="#container-platform"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    platform</h3>

    <p>The first step is to install one of the supported container platforms. If you''re

    using a high-performance computing cluster, more often than not

    <a href="https://sylabs.io" rel="nofollow"><code>Singularity</code></a> will already
    be available.</p>

    <p>If not, we recommend using the latest version

    of<a href="https://sylabs.io" rel="nofollow"><code>Singularity</code></a>. However,
    it can be somewhat cumbersome to

    install, as it needs to be built from source.</p>

    <p>The <a href="https://neuro.debian.net/" rel="nofollow"><code>NeuroDebian</code></a>
    package repository provides an

    older version of <a href="https://sylabs.io/guides/2.6/user-guide/" rel="nofollow"><code>Singularity</code></a>
    for

    <a href="https://neuro.debian.net/pkgs/singularity-container.html" rel="nofollow">some</a>
    Linux

    distributions.</p>

    <p>In contrast to <code>Singularity</code>, <code>Docker</code> always requires
    elevated privileges to

    run containers. In other words, every user running a <code>Docker</code> container

    automatically has administrator privileges on the computer they''re using.

    Therefore, it is inherently a bad choice for multi-user environments, where the

    access of individual users should be limited. <code>Docker</code> is the only
    option that

    is compatible with <code>Mac OS X</code>.</p>

    <table>

    <thead>

    <tr>

    <th>Container platform</th>

    <th>Version</th>

    <th>Installation</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><strong>Singularity</strong></td>

    <td><strong>3.5.3</strong></td>

    <td><strong>See <a href="https://sylabs.io/guides/3.5/user-guide/quick_start.html"
    rel="nofollow">https://sylabs.io/guides/3.5/user-guide/quick_start.html</a></strong></td>

    </tr>

    <tr>

    <td>Singularity</td>

    <td>2.6.1</td>

    <td><code>sudo apt install singularity-container</code></td>

    </tr>

    <tr>

    <td>Docker</td>

    <td></td>

    <td>See <a href="https://docs.docker.com/engine/install/" rel="nofollow">https://docs.docker.com/engine/install/</a>

    </td>

    </tr>

    </tbody>

    </table>

    <h3>

    <a id="user-content-download" class="anchor" href="#download" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Download</h3>

    <p>The second step is to download the <code>HALFpipe</code> to your computer.
    This requires

    approximately 5 gigabytes of storage.</p>

    <table>

    <thead>

    <tr>

    <th>Container platform</th>

    <th>Installation</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Singularity</td>

    <td>

    <a href="https://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g"
    rel="nofollow">https://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g</a>
    or <code>singularity pull docker://halfpipe/halfpipe:1.1.1</code> or <code>singularity
    pull docker://ghcr.io/halfpipe/halfpipe:1.1.1</code>

    </td>

    </tr>

    <tr>

    <td>Docker</td>

    <td><code>docker pull halfpipe/halfpipe:1.1.1</code></td>

    </tr>

    </tbody>

    </table>

    <p><code>Singularity</code> version <code>3.x</code> creates a container image
    file called

    <code>HALFpipe_{version}.sif</code> in the directory where you run the <code>pull</code>
    command. For

    <code>Singularity</code> version <code>2.x</code> the file is named

    <code>halfpipe-halfpipe-master-latest.simg</code>. Whenever you want to use the
    container,

    you need pass <code>Singularity</code> the path to this file.</p>

    <blockquote>

    <p><strong>NOTE:</strong> <code>Singularity</code> may store a copy of the container
    in its cache

    directory. The cache directory is located by default in your home directory at

    <code>~/.singularity</code>. If you need to save disk space in your home directory,
    you

    can safely delete the cache directory after downloading, i.e. by running

    <code>rm -rf ~/.singularity</code>. Alternatively, you could move the cache directory

    somewhere with more free disk space using a symlink. This way, files will

    automatically be stored there in the future. For example, if you have a lot of

    free disk space in <code>/mnt/storage</code>, then you could first run

    <code>mv ~/.singularity /mnt/storage</code> to move the cache directory, and then

    <code>ln -s /mnt/storage/.singularity ~/.singularity</code> to create the symlink.</p>

    </blockquote>

    <p><code>Docker</code> will store the container in its storage base directory,
    so it does not

    matter from which directory you run the <code>pull</code> command.</p>

    <h3>

    <a id="user-content-running" class="anchor" href="#running" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running</h3>

    <p>The third step is to run the downloaded container. You may need to replace

    <code>halfpipe_1.1.1.sif</code> with the actual path and filename where <code>Singularity</code>

    downloaded your container.</p>

    <table>

    <thead>

    <tr>

    <th>Container platform</th>

    <th>Command</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Singularity</td>

    <td><code>singularity run --no-home --cleanenv --bind /:/ext halfpipe_1.1.1.sif</code></td>

    </tr>

    <tr>

    <td>Docker</td>

    <td><code>docker run --interactive --tty --volume /:/ext halfpipe/halfpipe</code></td>

    </tr>

    </tbody>

    </table>

    <p>You should now see the user interface.</p>

    <h4>

    <a id="user-content-background" class="anchor" href="#background" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Background</h4>

    <p>Containers are by default isolated from the host computer. This adds security,

    but also means that the container cannot access the data it needs for analysis.

    <code>HALFpipe</code> expects all inputs (e.g., image files and spreadsheets)
    and outputs

    (the working directory) to be places in the path<code>/ext</code> (see also

    <a href="#data-file-system-root---fs-root"><code>--fs-root</code></a>). Using
    the option

    <code>--bind /:/ext</code>, we instruct <code>Singularity</code> to map all of
    the host file system

    (<code>/</code>) to that path (<code>/ext</code>). You can also run <code>HALFpipe</code>
    and only map only part

    of the host file system, but keep in mind that any directories that are not

    mapped will not be visible later.</p>

    <p><code>Singularity</code> passes the host shell environment to the container
    by default.

    This means that in some cases, the host computer''s configuration can interfere

    with the software. To avoid this, we need to pass the option <code>--cleanenv</code>.

    <code>Docker</code> does not pass the host shell environment by default, so we
    don''t need

    to pass an option.</p>

    <h2>

    <a id="user-content-user-interface" class="anchor" href="#user-interface" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>User interface</h2>

    <p>The user interface asks a series of questions about your data and the analyses

    you want to run. In each question, you can press <code>Control+C</code> to cancel
    the

    current question and go back to the previous one. <code>Control+D</code> exits
    the program

    without saving. Note that these keyboard shortcuts are the same on Mac.</p>

    <h3>

    <a id="user-content-files" class="anchor" href="#files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Files</h3>

    <p>To run preprocessing, at least a T1-weighted structural image and a BOLD image

    file is required. Preprocessing and data analysis proceeds automatically.

    However, to be able to run automatically, data files need to be input in a way

    suitable for automation.</p>

    <p>For this kind of automation, <code>HALFpipe</code> needs to know the relationships
    between

    files, such as which files belong to the same subject. However, even though it

    would be obvious for a human, a program cannot easily assign a file name to a

    subject, and this will be true as long as there are differences in naming

    between different researchers or labs. One researcher may name the same file

    <code>subject_01_rest.nii.gz</code> and another <code>subject_01/scan_rest.nii.gz</code>.</p>

    <p>In <code>HALFpipe</code>, we solve this issue by inputting file names in a
    specific way.

    For example, instead of <code>subject_01/scan_rest.nii.gz</code>, <code>HALFpipe</code>
    expects you to

    input <code>{subject}/scan_rest.nii.gz</code>. <code>HALFpipe</code> can then
    match all files on disk

    that match this naming schema, and extract the subject ID <code>subject_01</code>.
    Using

    the extracted subject ID, other files can now be matched to this image. If all

    input files are available in BIDS format, then this step can be skipped.</p>

    <ol>

    <li>

    <code>Specify working directory</code> All intermediate and outputs of <code>HALFpipe</code>
    will

    be placed in the working directory. Keep in mind to choose a location with

    sufficient free disk space, as intermediates can be multiple gigabytes in

    size for each subject.</li>

    <li>

    <code>Is the data available in BIDS format?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify the path of the BIDS directory</code></li>

    </ol>

    </li>

    <li>

    <code>No</code>

    <ol>

    <li>

    <code>Specify anatomical/structural data</code><br>

    <code>Specify the path of the T1-weighted image files</code>

    </li>

    <li>

    <code>Specify functional data</code><br>

    <code>Specify the path of the BOLD image files</code>

    </li>

    <li>

    <code>Check repetition time values</code> / <code>Specify repetition time in seconds</code>

    </li>

    <li>

    <code>Add more BOLD image files?</code>

    <ul>

    <li>

    <code>Yes</code> Loop back to 2</li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    </ol>

    </li>

    </ul>

    </li>

    <li>

    <code>Do slice timing?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Check slice acquisition direction values</code></li>

    <li><code>Check slice timing values</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Skip this step</li>

    </ul>

    </li>

    <li>

    <code>Specify field maps?</code> If the data was imported from a BIDS directory,
    this

    step will be omitted.

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li>

    <code>Specify the type of the field maps</code>

    <ul>

    <li>EPI (blip-up blip-down)

    <ol>

    <li><code>Specify the path of the blip-up blip-down EPI image files</code></li>

    </ol>

    </li>

    <li>Phase difference and magnitude (used by Siemens scanners)

    <ol>

    <li><code>Specify the path of the magnitude image files</code></li>

    <li><code>Specify the path of the phase/phase difference image files</code></li>

    <li><code>Specify echo time difference in seconds</code></li>

    </ol>

    </li>

    <li>Scanner-computed field map and magnitude (used by GE / Philips

    scanners)

    <ol>

    <li><code>Specify the path of the magnitude image files</code></li>

    <li><code>Specify the path of the field map image files</code></li>

    </ol>

    </li>

    </ul>

    </li>

    <li>

    <code>Add more field maps?</code> Loop back to 1</li>

    <li><code>Specify effective echo spacing for the functional data in seconds</code></li>

    <li><code>Specify phase encoding direction for the functional data</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Skip this step</li>

    </ul>

    </li>

    </ol>

    <h3>

    <a id="user-content-features" class="anchor" href="#features" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Features</h3>

    <p>Features are analyses that are carried out on the preprocessed data, in other

    words, first-level analyses.</p>

    <ol>

    <li>

    <code>Specify first-level features?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li>

    <code>Specify the feature type</code>

    <ul>

    <li>

    <code>Task-based</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li><code>Specify the event file type</code></li>

    </ol>

    <ul>

    <li>

    <code>SPM multiple conditions</code> A MATLAB .mat file containing three

    arrays: <code>names</code> (condition), <code>onsets</code> and <code>durations</code>

    </li>

    <li>

    <code>FSL 3-column</code> One text file for each condition. Each file has its

    corresponding condition in the filename. The first column specifies

    the event onset, the second the duration. The third column of the

    files is ignored, so parametric modulation is not supported</li>

    <li>

    <code>BIDS TSV</code> A tab-separated table with named columns <code>trial_type</code>

    (condition), <code>onset</code> and <code>duration</code>. While BIDS supports
    defining

    additional columns, <code>HALFpipe</code> will currently ignore these</li>

    </ul>

    <ol>

    <li><code>Specify the path of the event files</code></li>

    <li><code>Select conditions to add to the model</code></li>

    <li>

    <code>Specify contrasts</code>

    <ol>

    <li><code>Specify contrast name</code></li>

    <li><code>Specify contrast values</code></li>

    <li>

    <code>Add another contrast?</code>

    <ul>

    <li>

    <code>Yes</code> Loop back to 1</li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    </ol>

    </li>

    <li>

    <code>Apply a temporal filter to the design matrix?</code> A separate temporal

    filter can be specified for the design matrix. In contrast, the

    temporal filtering of the input image and any confound regressors

    added to the design matrix is specified in 10. In general, the two

    settings should match</li>

    <li>

    <code>Apply smoothing?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify smoothing FWHM in mm</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li><code>Grand mean scaling will be applied with a mean of 10000.000000</code></li>

    <li>

    <code>Temporal filtering will be applied using a gaussian-weighted filter</code><br>

    <code>Specify the filter width in seconds</code>

    </li>

    <li><code>Remove confounds?</code></li>

    </ol>

    </li>

    <li>

    <code>Seed-based connectivity</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>

    <code>Specify binary seed mask file(s)</code>

    <ol>

    <li><code>Specify the path of the binary seed mask image files</code></li>

    <li><code>Check space values</code></li>

    <li><code>Add binary seed mask image file</code></li>

    </ol>

    </li>

    </ol>

    </li>

    <li>

    <code>Dual regression</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    <li>

    <code>Atlas-based connectivity matrix</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    <li>

    <code>ReHo</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    <li>

    <code>fALFF</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    </ul>

    </li>

    </ol>

    </li>

    <li>

    <code>No</code> Skip this step</li>

    </ul>

    </li>

    <li>

    <code>Add another first-level feature?</code>

    <ul>

    <li>

    <code>Yes</code> Loop back to 1</li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li>

    <code>Output a preprocessed image?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify setting name</code></li>

    <li><code>Specify images to use</code></li>

    <li>

    <code>Apply smoothing?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify smoothing FWHM in mm</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li>

    <code>Do grand mean scaling?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify grand mean</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li>

    <code>Apply a temporal filter?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li>

    <code>Specify the type of temporal filter</code>

    <ul>

    <li><code>Gaussian-weighted</code></li>

    <li><code>Frequency-based</code></li>

    </ul>

    </li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li><code>Remove confounds?</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    </ol>

    <h3>

    <a id="user-content-models" class="anchor" href="#models" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Models</h3>

    <p>Models are statistical analyses that are carried out on the features.</p>

    <blockquote>

    <p>TODO</p>

    </blockquote>

    <h2>

    <a id="user-content-running-on-a-high-performance-computing-cluster" class="anchor"
    href="#running-on-a-high-performance-computing-cluster" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running on a high-performance
    computing cluster</h2>

    <ol>

    <li>

    <p>Log in to your cluster''s head node</p>

    </li>

    <li>

    <p>Request an interactive job. Refer to your cluster''s documentation for how
    to

    do this</p>

    </li>

    <li>

    <p>In the interactive job, run the <code>HALFpipe</code> user interface, but add
    the flag

    <code>--use-cluster</code> to the end of the command. <br>

    For example, <code>singularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif
    --use-cluster</code></p>

    </li>

    <li>

    <p>As soon as you finish specifying all your data, features and models in the

    user interface, <code>HALFpipe</code> will now generate everything needed to run
    on the

    cluster. For hundreds of subjects, this can take up to a few hours.</p>

    </li>

    <li>

    <p>When <code>HALFpipe</code> exits, edit the generated submit script <code>submit.slurm.sh</code>

    according to your cluster''s documentation and then run it. This submit script

    will calculate everything except group statistics.</p>

    </li>

    <li>

    <p>As soon as all processing has been completed, you can run group statistics.

    This is usually very fast, so you can do this in an interactive session. Run

    <code>singularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif --only-model-chunk</code>

    and then select <code>Run without modification</code> in the user interface.</p>

    </li>

    </ol>

    <blockquote>

    <p>A common issue with remote work via secure shell is that the connection may

    break after a few hours. For batch jobs this is not an issue, but for

    interactive jobs this can be quite frustrating. When the connection is lost,

    the node you were connected to will automatically quit all programs you were

    running. To prevent this, you can run interactive jobs within <code>screen</code>
    or

    <code>tmux</code> (whichever is available). These commands allow you to open sessions
    in

    the terminal that will continue running in the background even when you close

    or disconnect. Here''s a quick overview of how to use the commands (more

    in-depth documentation is available for example at

    [<a href="http://www.dayid.org/comp/tm.html" rel="nofollow">http://www.dayid.org/comp/tm.html</a>]).</p>

    <ol>

    <li>Open a new screen/tmux session on the head node by running either <code>screen</code>

    or <code>tmux</code>

    </li>

    <li>Request an interactive job from within the session, for example with

    <code>srun --pty bash -i</code>

    </li>

    <li>Run the command that you want to run</li>

    <li>Detach from the screen/tmux session, meaning disconnecting with the ability

    to re-connect later <br>

    For screen, this is done by first pressing <code>Control+a</code>, then letting
    go, and

    then pressing <code>d</code> on the keyboard. <br>

    For tmux, it''s <code>Control+b</code> instead of <code>Control+a</code>. <br>

    Note that this is always <code>Control</code>, even if you''re on a mac.</li>

    <li>Close your connection to the head node with <code>Control+d</code>. <code>screen</code>/<code>tmux</code>

    will remain running in the background</li>

    <li>Later, connect again to the head node. Run <code>screen -r</code> or <code>tmux
    attach</code> to

    check back on the interactive job. If everything went well and the command

    you wanted to run finished, close the interactive job with <code>Control+d</code>
    and

    then the <code>screen</code>/<code>tmux</code> session with <code>Control+d</code>
    again. If the command

    hasn''t finished yet, detach as before and come back later</li>

    </ol>

    </blockquote>

    <h2>

    <a id="user-content-quality-checks" class="anchor" href="#quality-checks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quality checks</h2>

    <p>Please see the manual at <a href="https://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8"
    rel="nofollow">https://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8</a></p>

    <h2>

    <a id="user-content-outputs" class="anchor" href="#outputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Outputs</h2>

    <ul>

    <li>

    <p>A visual report page <code>reports/index.html</code></p>

    </li>

    <li>

    <p>A table with image quality metrics <code>reports/reportvals.txt</code></p>

    </li>

    <li>

    <p>A table containing the preprocessing status <code>reports/reportpreproc.txt</code></p>

    </li>

    <li>

    <p>The untouched <code>fmriprep</code> derivatives. Some files have been omitted
    to save

    disk space <code>fmriprep</code> is very strict about only processing data that
    is

    compliant with the BIDS standard. As such, we may need to format subjects

    names for compliance. For example, an input subject named <code>subject_01</code>
    will

    appear as <code>subject01</code> in the <code>fmriprep</code> derivatives. <code>derivatives/fmriprep</code></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-subject-level-features" class="anchor" href="#subject-level-features"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Subject-level
    features</h3>

    <ul>

    <li>

    <p>For task-based, seed-based connectivity and dual regression features,

    <code>HALFpipe</code> outputs the statistical maps for the effect, the variance,
    the

    degrees of freedom of the variance and the z-statistic. In FSL, the effect and

    variance are also called <code>cope</code> and <code>varcope</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-effect_statmap.nii.gz</code>
    <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-variance_statmap.nii.gz</code>
    <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-dof_statmap.nii.gz</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-z_statmap.nii.gz</code> <br>

    The design and contrast matrix used for the final model will be outputted alongside

    the statistical maps <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-design_matrix.tsv</code>

    <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-contrast_matrix.tsv</code></p>

    </li>

    <li>

    <p>ReHo and fALFF are not calculated based on a linear model. As such, only one

    statistical map of the z-scaled values will be output <br>

    <code>derivatives/halfpipe/sub-.../func/..._alff.nii.gz</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._falff.nii.gz</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._reho.nii.gz</code></p>

    </li>

    <li>

    <p>For every feature, a JSON file containing a summary of the preprocessing</p>

    </li>

    <li>

    <p>settings, and a list of the raw data files that were used for the analysis

    (<code>RawSources</code>) <br>

    <code>derivatives/halfpipe/sub-.../func/....json</code></p>

    </li>

    <li>

    <p>For every feature, the corresponding brain mask is output beside the

    statistical maps. Masks do not differ between different features calculated,

    they are only copied out repeatedly for convenience <br>

    <code>derivatives/halfpipe/sub-.../func/...desc-brain_mask.nii.gz</code></p>

    </li>

    <li>

    <p>Atlas-based connectivity outputs the time series and the full covariance and

    correlation matrices as text files <br>

    <code>derivatives/halfpipe/sub-.../func/..._timeseries.txt</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._desc-covariance_matrix.txt</code>
    <br>

    <code>derivatives/halfpipe/sub-.../func/..._desc-correlation_matrix.txt</code></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-preprocessed-images" class="anchor" href="#preprocessed-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Preprocessed
    images</h3>

    <ul>

    <li>

    <p>Masked, preprocessed BOLD image <br>

    <code>derivatives/halfpipe/sub-.../func/..._bold.nii.gz</code></p>

    </li>

    <li>

    <p>Just like for features <br>

    <code>derivatives/halfpipe/sub-.../func/..._bold.json</code></p>

    </li>

    <li>

    <p>Just like for features <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-brain_mask.nii.gz</code></p>

    </li>

    <li>

    <p>Filtered confounds time series, where all filters that are applied to the BOLD

    image are applied to the regressors as well. Note that this means that when

    grand mean scaling is active, confounds time series are also scaled, meaning

    that values such as <code>framewise displacement</code> can not be interpreted
    in terms

    of their original units anymore. <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-confounds_regressors.tsv</code></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-group-level" class="anchor" href="#group-level" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Group-level</h3>

    <ul>

    <li><code>grouplevel/...</code></li>

    </ul>

    <h2>

    <a id="user-content-troubleshooting" class="anchor" href="#troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Troubleshooting</h2>

    <ul>

    <li>If an error occurs, this will be output to the command line and simultaneously

    to the <code>err.txt</code> file in the working directory</li>

    <li>If the error occurs while running, usually a text file detailing the error

    will be placed in the working directory. These are text files and their file

    names start with <code>crash</code>

    <ul>

    <li>Usually, the last line of these text files contains the error message.

    Please read this carefully, as may allow you to understand the error</li>

    <li>For example, consider the following error message:

    <code>ValueError: shape (64, 64, 33) for image 1 not compatible with first image
    shape (64, 64, 34) with axis == None</code>

    This error message may seem cryptic at first. However, looking at the

    message more closely, it suggests that two input images have different,

    incompatible dimensions. In this case, <code>HALFpipe</code> correctly recognized
    this

    issue, and there is no need for concern. The images in question will simply

    be excluded from preprocessing and/or analysis</li>

    <li>In some cases, the cause of the error can be a bug in the <code>HALFpipe</code>
    code.

    Please check that no similar issue has been reported

    <a href="https://github.com/HALFpipe/HALFpipe/issues">here on GitHub</a>. In this
    case,

    please submit an

    <a href="https://github.com/HALFpipe/HALFpipe/issues/new/choose">issue</a>.</li>

    </ul>

    </li>

    </ul>

    <h2>

    <a id="user-content-command-line-flags" class="anchor" href="#command-line-flags"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Command
    line flags</h2>

    <h3>

    <a id="user-content-control-command-line-logging" class="anchor" href="#control-command-line-logging"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Control
    command line logging</h3>

    <div class="highlight highlight-source-shell"><pre>--verbose</pre></div>

    <p>By default, only errors and warnings will be output to the command line. This

    makes it easier to see when something goes wrong, because there is less output.

    However, if you want to be able to inspect what is being run, you can add the

    <code>--verbose</code> flag to the end of the command used to call <code>HALFpipe</code>.</p>

    <p>Verbose logs are always written to the <code>log.txt</code> file in the working
    directory,

    so going back and inspecting this log is always possible, even if the

    <code>--verbose</code> flag was not specified.</p>

    <p>Specifying the flag <code>--debug</code> will print additional, fine-grained
    messages. It

    will also automatically start the

    <a href="https://docs.python.org/3/library/pdb.html" rel="nofollow">Python Debugger</a>
    when an error

    occurs. You should only use <code>--debug</code> if you know what you''re doing.</p>

    <h3>

    <a id="user-content-automatically-remove-unneeded-files" class="anchor" href="#automatically-remove-unneeded-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Automatically
    remove unneeded files</h3>

    <div class="highlight highlight-source-shell"><pre>--keep</pre></div>

    <p><code>HALFpipe</code> saves intermediate files for each pipeline step. This
    speeds up

    re-running with different settings, or resuming after a job after it was

    cancelled. The intermediate file are saved by the

    <a href="https://nipype.readthedocs.io/" rel="nofollow"><code>nipype</code></a>
    workflow engine, which is what

    <code>HALFpipe</code> uses internally. <code>nipype</code> saves the intermediate
    files in the

    <code>nipype</code> folder in the working directory.</p>

    <p>In environments with limited disk capacity, this can be problematic. To limit

    disk usage, <code>HALFpipe</code> can delete intermediate files as soon as they
    are not

    needed anymore. This behavior is controlled with the <code>--keep</code> flag.</p>

    <p>The default option <code>--keep some</code> keeps all intermediate files from
    fMRIPrep and

    MELODIC, which would take the longest to re-run. We believe this is a good

    tradeoff between disk space and computer time. <code>--keep all</code> turns of
    all

    deletion of intermediate files. <code>--keep none</code> deletes as much as possible,

    meaning that the smallest amount possible of disk space will be used.</p>

    <h3>

    <a id="user-content-configure-nipype" class="anchor" href="#configure-nipype"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configure
    nipype</h3>

    <div class="highlight highlight-source-shell"><pre>--nipype-<span class="pl-k">&lt;</span>omp-nthreads<span
    class="pl-k">|</span>memory-gb<span class="pl-k">|</span>n-procs<span class="pl-k">|</span>run-plugin<span
    class="pl-k">&gt;</span></pre></div>

    <p><code>HALFpipe</code> chooses sensible defaults for all of these values.</p>

    <h3>

    <a id="user-content-choose-which-parts-to-run-or-to-skip" class="anchor" href="#choose-which-parts-to-run-or-to-skip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Choose
    which parts to run or to skip</h3>

    <div class="highlight highlight-source-shell"><pre>--<span class="pl-k">&lt;</span>only<span
    class="pl-k">|</span>skip<span class="pl-k">&gt;</span>-<span class="pl-k">&lt;</span>spec-ui<span
    class="pl-k">|</span>workflow<span class="pl-k">|</span>run<span class="pl-k">|</span>model-chunk<span
    class="pl-k">&gt;</span></pre></div>

    <p>A <code>HALFpipe</code> run is divided internally into three stages, spec-ui,
    workflow, and

    run.</p>

    <ol>

    <li>The <code>spec-ui</code> stage is where you specify things in the user interface.
    It

    creates the <code>spec.json</code> file that contains all the information needed
    to run

    <code>HALFpipe</code>. To only run this stage, use the option <code>--only-spec-ui</code>.
    To skip

    this stage, use the option <code>--skip-spec-ui</code>

    </li>

    <li>The <code>workflow</code> stage is where <code>HALFpipe</code> uses the <code>spec.json</code>
    data to search

    for all the files that match what was input in the user interface. It then

    generates a <code>nipype</code> workflow for preprocessing, feature extraction
    and group

    models. <code>nipype</code> then validates the workflow and prepares it for execution.

    This usually takes a couple of minutes and cannot be parallelized. For

    hundreds of subjects, this may even take a few hours. This stage has the

    corresponding option <code>--only-workflow</code> and <code>--skip-workflow</code>.</li>

    </ol>

    <ul>

    <li>This stage saves several intermediate files. These are named

    <code>workflow.{uuid}.pickle.xz</code>, <code>execgraph.{uuid}.pickle.xz</code>
    and

    <code>execgraph.{n_chunks}_chunks.{uuid}.pickle.xz</code>. The <code>uuid</code>
    in the file name is

    a unique identifier generated from the <code>spec.json</code> file and the input
    files.

    It is re-calculated every time we run this stage. The uuid algorithm produces

    a different output if there are any changes (such as when new input files for

    new subjects become available, or the <code>spec.json</code> is changed, for example
    to

    add a new feature or group model). Otherwise, the <code>uuid</code> stays the
    same.

    Therefore, if a workflow file with the calculated <code>uuid</code> already exists,
    then

    we do not need to run this stage. We can simple re-use the workflow from the

    existing file, and save some time.</li>

    <li>In this stage, we can also decide to split the execution into chunks. The
    flag

    <code>--subject-chunks</code> creates one chunk per subject. The flag <code>--use-cluster</code>

    automatically activates <code>--subject-chunks</code>. The flag <code>--n-chunks</code>
    allows the

    user to specify a specific number of chunks. This is useful if the execution

    should be spread over a set number of computers. In addition to these, a model

    chunk is generated.</li>

    </ul>

    <ol>

    <li>The <code>run</code> stage loads the <code>execgraph.{n_chunks}_chunks.{uuid}.pickle.xz</code>
    file

    generated in the previous step and runs it. This file usually contains two

    chunks, one for the subject level preprocessing and feature extraction

    ("subject level chunk"), and one for group statistics ("model chunk"). To run

    a specific chunk, you can use the flags <code>--only-chunk-index ...</code> and

    <code>--only-model-chunk</code>.</li>

    </ol>

    <h3>

    <a id="user-content-working-directory" class="anchor" href="#working-directory"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Working
    directory</h3>

    <div class="highlight highlight-source-shell"><pre>--workdir</pre></div>

    <blockquote>

    <p>TODO</p>

    </blockquote>

    <h3>

    <a id="user-content-data-file-system-root" class="anchor" href="#data-file-system-root"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data
    file system root</h3>

    <div class="highlight highlight-source-shell"><pre>--fs-root</pre></div>

    <p>The <code>HALFpipe</code> container, or really most containers, contain the
    entire base

    system needed to run</p>

    <h2>

    <a id="user-content-contact" class="anchor" href="#contact" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

    <p>For questions or support, please submit an

    <a href="https://github.com/HALFpipe/HALFpipe/issues/new/choose">issue</a> or
    contact us

    via e-mail.</p>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Role</th>

    <th>E-mail address</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Lea Waller</td>

    <td>Developer</td>

    <td><a href="mailto:lea.waller@charite.de">lea.waller@charite.de</a></td>

    </tr>

    <tr>

    <td>Ilya Veer</td>

    <td>Project manager</td>

    <td><a href="mailto:ilya.veer@charite.de">ilya.veer@charite.de</a></td>

    </tr>

    <tr>

    <td>Susanne Erk</td>

    <td>Project manager</td>

    <td><a href="mailto:susanne.erk@charite.de">susanne.erk@charite.de</a></td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 7
  subscribers_count: 1
  topics:
  - neuroimaging
  updated_at: 1621624087.0
HERA-Team/hera-singularity:
  data_format: 2
  description: Singularity recipe for HERA software
  filenames:
  - Singularity.casa6_modular
  - Singularity.hera1
  - Singularity.rtp
  - Singularity.casa_imaging
  full_name: HERA-Team/hera-singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hera-singularity\" class=\"anchor\" href=\"\
    #hera-singularity\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>hera-singularity</h1>\n<p><a href=\"https://singularity-hub.org/collections/4892\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-notice\" class=\"\
    anchor\" href=\"#notice\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Notice</h2>\n<p><strong>April 27, 2021</strong>:\n\
    <a href=\"https://singularityhub.github.io/singularityhub-docs/2021/going-read-only/\"\
    \ rel=\"nofollow\">Singularity Hub remote built service is no longer available.</a>\
    \ We are considering other alternative. The old Singularity Hub builds can still\
    \ be accessed via the badge link above, which now redirects to DataLad. The <code>singularity\
    \ pull</code> can also do the pull from datalad URL. We will manually build and\
    \ upload to Ilifu for the time being.</p>\n<p><strong>April 28, 2021</strong>:\n\
    The containers have been built and are available at <code>/ilifu/astro/projects/hera/containers</code>,\
    \ with a few additional new containers that will be documented soon. We will keep\
    \ rebuilding and replacing these files weekly, to keep the software stack up to\
    \ date with the development, until we have an automated solution.</p>\n<hr>\n\
    <p>This repository contains recipe files for building singularity containers for\
    \ the HERA software suits. The containers are remotely built on Singularity Hub\
    \ when the recipes are pushed to the <code>main</code> branch. Container images\
    \ can be directly download from the the badge link above or by using the <code>singularity\
    \ pull</code> command line (see <a href=\"##-Singularity-Commands\">below</a>).\
    \ Ilifu users, make sure to read <a href=\"###-Specific-Usages-for-Ilifu\">Specific\
    \ Usages for Ilifu</a> section and check the relevant page on the HERA wiki.</p>\n\
    <h2>\n<a id=\"user-content-about-container-and-singularity\" class=\"anchor\"\
    \ href=\"#about-container-and-singularity\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>About Container and Singularity</h2>\n\
    <p>Containers are encapsulated software environments and abstract the software\
    \ and applications from the underlying operating system. This allows users to\
    \ run workflows in customized environments, switch between environments, and to\
    \ share these environments with colleagues and research teams.</p>\n<p>Singularity\
    \ is a free, cross-platform and open-source computer program that performs operating-system-level\
    \ virtualization also known as containerization (another widely used one being\
    \ Docker).</p>\n<p>A singularity container is required for computing on the Ilifu\
    \ cloud-computing cluster, which HERA has access (see the HERA wiki page on this).</p>\n\
    <p>Suggestion for other container recipes and implementations are welcome!</p>\n\
    <h2>\n<a id=\"user-content-container-content\" class=\"anchor\" href=\"#container-content\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Container Content</h2>\n<h3>\n<a id=\"user-content-python-packages\"\
    \ class=\"anchor\" href=\"#python-packages\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Python Packages</h3>\n<p>All\
    \ containers are built with <code>Ubuntu 20.04</code> and <code>miniconda</code>\
    \ with <code>python=3.8</code> unless otherwise specify <a href=\"###-Different-Between-Containers:\"\
    >below</a>, and come standard with the following packages:</p>\n<table>\n<thead>\n\
    <tr>\n<th>Data Analysis</th>\n<th>Astronomical</th>\n<th>HERA</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td><code>dask</code></td>\n<td><code>aipy</code></td>\n<td><code>linsolve</code></td>\n\
    </tr>\n<tr>\n<td><code>jupyterlab</code></td>\n<td><code>astropy</code></td>\n\
    <td><code>uvtools</code></td>\n</tr>\n<tr>\n<td><code>matplotlib</code></td>\n\
    <td><code>astropy-healpix</code></td>\n<td><code>hera_qm</code></td>\n</tr>\n\
    <tr>\n<td><code>numpy</code></td>\n<td><code>astroquery</code></td>\n<td><code>hera_cal</code></td>\n\
    </tr>\n<tr>\n<td><code>pandas</code></td>\n<td><code>cartopy</code></td>\n<td><code>hera_sim</code></td>\n\
    </tr>\n<tr>\n<td><code>scipy</code></td>\n<td><code>healpy</code></td>\n<td><code>hera_psepc</code></td>\n\
    </tr>\n<tr>\n<td><code>scikit-learn</code></td>\n<td>\n<code>pyuvdata</code><sup><a\
    \ href=\"#myfootnote1\">1</a></sup>\n</td>\n<td></td>\n</tr>\n<tr>\n<td><code>xarray</code></td>\n\
    <td>\n<code>pyuvsim</code><sup><a href=\"#myfootnote2\">2</a></sup>\n</td>\n<td></td>\n\
    </tr>\n</tbody>\n</table>\n<p><a name=\"user-content-myfootnote2\">1</a>: with\
    \ CASA measurement sets, HEALPix beam, and CST beam functionalities<br>\n<a name=\"\
    user-content-myfootnote1\">2</a>: with profiling and full simulator</p>\n<h3>\n\
    <a id=\"user-content-different-between-containers\" class=\"anchor\" href=\"#different-between-containers\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Different Between Containers:</h3>\n<ul>\n<li>\n<code>hera1</code>:\n\
    <ul>\n<li>Intended for general-purpose computing with most of the commonly used\
    \ data analysis, astronomical, and HERA software packages</li>\n</ul>\n</li>\n\
    <li>\n<code>rtp</code>:\n<ul>\n<li>For running the RTP pipeline and analysis with\
    \ <code>makeflow</code>.</li>\n<li>Equivalent to <code>hera1</code> with an addition\
    \ of <code>hera_opm</code>, <code>hera_mc</code>, and  <code>hera_notebook_templates</code>.</li>\n\
    <li>\n<code>hera_pipelines</code> is cloned to <code>/usr/local</code>\n</li>\n\
    </ul>\n</li>\n<li>\n<code>casa_imaging</code>:\n<ul>\n<li>Equivalent to <code>hera1</code>\
    \ with a full installation of <code>casa-6</code>\n</li>\n</ul>\n</li>\n<li>\n\
    <code>casa6_modular</code>:\n<ul>\n<li>Equivalent to <code>hera1</code> with a\
    \ pip-wheel installation of <code>casa-6</code>, making <code>casatasks</code>,\
    \ <code>casatools</code>, and <code>casampi</code> packages available (see <a\
    \ href=\"https://casa-pip.nrao.edu/\" rel=\"nofollow\">https://casa-pip.nrao.edu/</a>).</li>\n\
    <li>Based on <code>Python 3.6</code> and <code>Ubuntu 18.04</code> for casa-pip\
    \ compatibility.</li>\n</ul>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-environment-variables\"\
    \ class=\"anchor\" href=\"#environment-variables\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Environment Variables</h3>\n\
    <p>The following environment variables are also exported in all containers:</p>\n\
    <pre><code>CONDA_INSTALL_PATH=\"/usr/local/miniconda3\"\nCONDA_INIT_SCRIPT=\"\
    $CONDA_INSTALL_PATH/etc/profile.d/conda.sh\"\n</code></pre>\n<p>The <code>rtp</code>\
    \ container has an additional environment variable that point to <code>hera_pipelines</code>.</p>\n\
    <pre><code>HERA_PIPELINES_PATH=\"/usr/local/hera_pipelines\"\n</code></pre>\n\
    <h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <h3>\n<a id=\"user-content-singularity-commands\" class=\"anchor\" href=\"#singularity-commands\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity Commands</h3>\n<h4>\n<a id=\"user-content-pull\" class=\"\
    anchor\" href=\"#pull\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><code>pull</code>\n</h4>\n<p>Use <code>singularity\
    \ pull</code> to download the container from Singularity Hub</p>\n<pre><code>$\
    \ singularity pull [name_to_save_the_image_(optional)] shub://HERA-Team/hera-singularity:&lt;recipe&gt;\n\
    </code></pre>\n<p>For example,</p>\n<pre><code>$ singularity pull rtp.sif shub://HERA-Team/hera-singularity:rtp\n\
    INFO:    Downloading shub image\n 1.98 GiB / 1.98 GiB [=======================================================]\
    \ 100.00% 13.12 MiB/s 2m34s\n</code></pre>\n<h4>\n<a id=\"user-content-shell\"\
    \ class=\"anchor\" href=\"#shell\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><code>shell</code>\n</h4>\n<p>The\
    \ <code>singularity shell</code> command allows you to spawn a new shell within\
    \ your container and interact with it as though it were a small virtual machine.</p>\n\
    <p>By default, <code>shell</code> invokes <code>/bin/sh --norc</code>, which means\
    \ that <code>.bashrc</code> will not be executed (more on this <a href=\"https://github.com/hpcng/singularity/issues/643\"\
    >here</a>) and thus <code>conda</code> will not be initialized. To have <code>conda</code>\
    \ working, you can do one of the following:</p>\n<p>a) Run <code>exec $SHELL</code>\
    \ inside the singularity shell. If <code>$SHELL</code> is <code>\\bin\\bash</code>\
    \ (as in our Ubuntu build), <code>.bashrc</code> will be read.</p>\n<pre><code>$\
    \ singularity shell rtp.sif\nSingularity&gt; exec $SHELL\n</code></pre>\n<p>b)\
    \ Manually execute the conda initialization script inside singularity shell. A\
    \ <code>CONDA_INIT_SCRIPT</code> environment variable pointing to the absolute\
    \ path of the script (<code>/usr/local/miniconda3/etc/profile.d/conda.sh</code>),\
    \ is made available for this purpose. Note that <code>.</code> must be used as\
    \ <code>source</code> won't work under <code>sh</code>.</p>\n<pre><code>$ singularity\
    \ shell rtp.sif\nSingularity&gt; . $CONDA_INIT_SCRIPT\n</code></pre>\n<p>b) Specify\
    \ <code>\\bin\\bash</code> as a shell to use when executing the <code>shell</code>\
    \ command, either by using the <code>SINGULARITY_SHELL</code> environment variable,</p>\n\
    <pre><code>$ SINGULARITY_SHELL=/bin/bash singularity shell hera-rtp.sif\n</code></pre>\n\
    <p>or <code>-s</code> option,</p>\n<pre><code>$ singularity shell -s /bin/bash\
    \ hera-rtp.sif\n</code></pre>\n<h4>\n<a id=\"user-content-exec\" class=\"anchor\"\
    \ href=\"#exec\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><code>exec</code>\n</h4>\n<p>The <code>singularity\
    \ exec</code> command allows you to execute a custom command within a container\
    \ by specifying the image file.</p>\n<pre><code>$ singularity exec rtp.sif echo\
    \ \"Hello World!\"\nHello World!\n</code></pre>\n<pre><code>$ cat myscript.sh\n\
    Hello World!\n$ singularity exec rtp.sif bash myscript.sh\nHello World!\n</code></pre>\n\
    <h3>\n<a id=\"user-content-file-permission-and-bind-path\" class=\"anchor\" href=\"\
    #file-permission-and-bind-path\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>File Permission and Bind Path</h3>\n\
    <p>Singularity containers run as the user and share host services. When Singularity\
    \ \u2018switch\u2019 from the host operating system to the containerized operating\
    \ system, the OS-level system files on the host becomes inaccessible. (the root\
    \ user on the host system is also different from the root in the container!)</p>\n\
    <p>By default, the user home directory on the host system will be mapped to the\
    \ user home directory in the container, preserving all file permission. On Ilifu,\
    \ the shared data paths on the host are also mapped.</p>\n<h3>\n<a id=\"user-content-specific-usages-for-ilifu\"\
    \ class=\"anchor\" href=\"#specific-usages-for-ilifu\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Specific Usages\
    \ for Ilifu</h3>\n<h4>\n<a id=\"user-content-container-file-locations\" class=\"\
    anchor\" href=\"#container-file-locations\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Container File Locations</h4>\n\
    <p>Recent builds are available at <code>/ilifu/astro/projects/hera/containers</code></p>\n\
    <h4>\n<a id=\"user-content-using-a-hera-container-as-a-jupyter-kernel\" class=\"\
    anchor\" href=\"#using-a-hera-container-as-a-jupyter-kernel\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using a\
    \ HERA container as a Jupyter kernel</h4>\n<p>See <a href=\"https://docs.ilifu.ac.za/#/tech_docs/software_environments?id=using-a-custom-container-as-a-jupyter-kernel\"\
    \ rel=\"nofollow\">this page</a> on Ilifu documentation. We may try to semi-automate\
    \ this process for users with a shell script in the future.</p>\n"
  stargazers_count: 0
  subscribers_count: 13
  topics: []
  updated_at: 1620417376.0
HanLabUNLV/hanlab_singularity_defs:
  data_format: 2
  description: definition files for containers used in Hanlab
  filenames:
  - singularity.Rconda/R.3.6.3.def
  - singularity.py37.ml.openblas/py37.ml.openblas.def
  - singularity.mkl/mkl.ubuntu.def
  - singularity.mkl/mkl.def
  - singularity.rnaseq/rnaseq.def
  - singularity.SAD/SAD.def
  - singularity.R.3.6.3.Bioc/R.3.6.3.Bioc.def
  - singularity.phylo/phylo.def
  - singularity.R.3.6.3.phylo/R.3.6.3.phylo.def
  - singularity.py37.ml.mkl/py37.ml.mkl.def
  - singularity.R.4.0.2.Bioc/R.4.0.2.Bioc.def
  full_name: HanLabUNLV/hanlab_singularity_defs
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc_mpi_cuda_singu_def_file" class="anchor" href="#hpc_mpi_cuda_singu_def_file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>hpc_mpi_cuda_singu_def_file</h1>

    <p>A collect of definition files to build images for singularity containers, which
    includes hpc benchmarks and mpis with cuda support.</p>

    <p><a href="https://singularity-hub.org/collections/4181" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1617151784.0
HarinarayanKrishnan/fxi_analysis_pipeline:
  data_format: 2
  description: ZMQ-based analysis pipeline for processing Tomography data from FXI-18
    at NSLS-II
  filenames:
  - fxi_zmq/Singularity
  - fxi_analysis/Singularity
  full_name: HarinarayanKrishnan/fxi_analysis_pipeline
  latest_release: null
  readme: '<h1>

    <a id="user-content-wip-azure-singularity-agent" class="anchor" href="#wip-azure-singularity-agent"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>WIP:
    azure-singularity-agent</h1>

    <p>An agent for Azure Pipelines using a Singularity image</p>

    <p>Build with</p>

    <pre><code>sudo singularity build azure-singularity-agent.sif Singularity

    </code></pre>

    <p>Run with</p>

    <pre><code>env AZP_URL=https://dev.azure.com/&lt;organization&gt; AZP_TOKEN=&lt;PAT
    token&gt; AZP_AGENT_NAME=mydockeragent singularity run azure-singularity-agent.sif

    </code></pre>

    <h2>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <ul>

    <li>Seems to not work because the resulting <code>sif</code> is read-only.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1582365848.0
HealthML/Nextflow-pipeline:
  data_format: 2
  description: The pipeline for analyzing the UKBiobank data, with variant annotation
    tool VEP and some in-house tools
  filenames:
  - container/Singularity.vep-96.0
  full_name: HealthML/Nextflow-pipeline
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-nextflow-pipeline\" class=\"anchor\" href=\"\
    #nextflow-pipeline\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Nextflow-pipeline</h1>\n<p>The current pipeline\
    \ analyze the exon-seq data generated from either of Regeneron\u2019s own pipeline\
    \ <code>(SPB)</code> or Functionally\nEquivalent <code>(FE)</code> piplines from\
    \ UKbiobank. Initially, the raw input data (<code>.bed</code>,<code>.fam</code>,\
    \ <code>.bai</code>) are filtered and converted to vcf files using two <code>plink2</code>\
    \ processes. Then, the variants (<code>vcf</code> file) are annotated in the third\
    \ process, using the ensembleVariant Effect Predictor (<code>vep</code>) tool.\
    \ The corresponding result is then processed using an in-house tool called SEAK.\
    \ The pipeline has a total of four processes. The tools used for all the four\
    \ processes are containerized in the <a href=\"https://github.com/HealthML/Nextflow-pipeline/blob/master/container/Dockerfile\"\
    >docker image </a></p>\n<h1>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h1>\n<pre><code>git clone https://github.com/HealthML/Nextflow-pipeline.git\n\
    cd Nextflow-pipeline\ngit checkout dev_nf\n</code></pre>\n<h1>\n<a id=\"user-content-nextflow\"\
    \ class=\"anchor\" href=\"#nextflow\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Nextflow</h1>\n<p><code>make\
    \ install</code></p>\n<h1>\n<a id=\"user-content-docker-image-installion\" class=\"\
    anchor\" href=\"#docker-image-installion\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Docker image installion</h1>\n\
    <p>To install the docker image for all the process tools using Docker, run the\
    \ Makefile command in the container directory.</p>\n<pre><code>cd container\n\
    make docker-build\n</code></pre>\n<h1>\n<a id=\"user-content-how-to-run-the-pipeline\"\
    \ class=\"anchor\" href=\"#how-to-run-the-pipeline\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How to run the\
    \ pipeline</h1>\n<p>In order to run the pipeline for the data generated from Regeneron\u2019\
    s own pipeline <code>(SPB)</code> or Functionally\nEquivalent <code>(FE)</code>\
    \ pipleine from UKbiobank using the VEP's cache references, please use the following\
    \ command. For example, if you wanna run the samples from FE pipeline try the\
    \ follwoing command on the terminal.</p>\n<p><code>./nextflow run main.nf -resume\
    \ --samples ukb_FE_50k_exome_seq</code></p>\n<p>The pipeline downloads automatically\
    \ hg38 fasta file. However, for the current pipeline I am using reference genome\
    \ (<code>.fa</code>), the annoation file (<code>.gtf</code>) and their corresponding\
    \ indexed files (<code>.fai</code> &amp; <code>.tbi</code> files). For runing\
    \ the pipeline using these references, please run the following command on the\
    \ terminal.</p>\n<pre><code>./nextflow run main.nf --ref_fa /home/Alva.Rani/UKbiobank/derived/projects/kernels_VEP/Homo_sapiens.GRCh38.dna.primary_assembly.fa\
    \ --gtf /home/Alva.Rani/data/reference/Homo_sapiens.GRCh38.97.gtf.gz --gtf_tbi\
    \ /home/Alva.Rani/data/reference/Homo_sapiens.GRCh38.97.gtf.gz.tbi\n</code></pre>\n\
    <p>If you can access the VM server and the above mentioned folder, there is index\
    \ for the reference genome.</p>\n<p>Otherwise, you can also run the whole pipeline\
    \ by using the following one liner,</p>\n<p><code>./nextflow run main.nf</code></p>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1613765139.0
HephyAnalysisSW/MLContainer:
  data_format: 2
  description: Container for ML based analysis @ HEPHY
  filenames:
  - machine-learning-hats/Singularity.cpu
  - machine-learning-hats/Singularity.gpu
  full_name: HephyAnalysisSW/MLContainer
  latest_release: null
  readme: '<h1>

    <a id="user-content-mlcontainer" class="anchor" href="#mlcontainer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MLContainer</h1>

    <p>Singularity container and conda environments for ML based analysis @ HEPHY</p>

    <h2>

    <a id="user-content-machine-learning-hats" class="anchor" href="#machine-learning-hats"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>machine-learning-hats</h2>

    <p><a href="https://github.com/FNALLPC/machine-learning-hats">https://github.com/FNALLPC/machine-learning-hats</a></p>

    <p>On HEPGPU01 its easier to use the conda environment</p>

    <pre><code>conda env create --file environment-gpu.yml

    </code></pre>

    <p>On CLIP its better to use the container. It will

    be installed after the shutdown.</p>

    <pre><code>build_ml-hats.sh

    </code></pre>

    <p>Run the shell container</p>

    <pre><code>run_ml-hats.sh

    </code></pre>

    <p>Run a script</p>

    <pre><code>run_ml-hats.sh &lt;scripts&gt;

    </code></pre>

    <h2>

    <a id="user-content-deepjetcore" class="anchor" href="#deepjetcore" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DeepJetCore</h2>

    <p><a href="https://github.com/DL4Jets/DeepJetCore">https://github.com/DL4Jets/DeepJetCore</a></p>

    <h1>

    <a id="user-content-container-for-deepjetcore" class="anchor" href="#container-for-deepjetcore"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    for DeepJetCore</h1>

    <p>Build the container (only on  HEPGPU01)</p>

    <pre><code>build_deepjet3.sh

    </code></pre>

    <p>Run the container</p>

    <pre><code>run_deepjet3.sh

    </code></pre>

    <h2>

    <a id="user-content-todo" class="anchor" href="#todo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TODO</h2>

    <p>Try container on CLIP</p>

    <h2>

    <a id="user-content-open-points" class="anchor" href="#open-points" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Open Points</h2>

    <h3>

    <a id="user-content-conda-environments-on-clip" class="anchor" href="#conda-environments-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conda
    Environments on CLIP</h3>

    <p>CLIP provides already installations of Conda</p>

    <pre><code>ml add Anaconda3/19.10

    </code></pre>

    <p>Nevertheless I could not build reliable environments due to limited

    quota. Also trying to move the correspondig directories to /scratch-cbe/users

    was not successfull.</p>

    <p>At this point it seems better to use the conda environment inside a container.</p>

    <h3>

    <a id="user-content-building-container-on-clip" class="anchor" href="#building-container-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on CLIP</h3>

    <p>Building containers on CLIP has two problems</p>

    <ul>

    <li>root rights required to build container (possible solution fakeroot)</li>

    <li>docker container could be transformed in singularity container, but

    the filesystems (BeeGEFS) do not fullfill the singularity requirements</li>

    </ul>

    <p>The best way seems to use CI with Jenkis ans the local singularity hub. This

    has to be understood in the future.</p>

    <h3>

    <a id="user-content-building-container-on-hepgpu01" class="anchor" href="#building-container-on-hepgpu01"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on HEPGPU01</h3>

    <p>Root rights are required in case container are build from scratch. Fakeroot

    would be a possible workaround. Has to be tried.</p>

    <p>Another possibility is to use "sudo".</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1603481294.0
Himani2000/GSOC_2020:
  data_format: 2
  description: null
  filenames:
  - Singularity.clustering
  full_name: Himani2000/GSOC_2020
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-google-summer-of-code-2020\" class=\"anchor\"\
    \ href=\"#google-summer-of-code-2020\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Google Summer of code 2020</h1>\n\
    <p>This is my google summer of code project 2020. I worked with the Redhenlab\
    \ organization. My project is based on the Image and Audio clustering and to deploy\
    \ in Rapid Annotator.\nFor more details about the code and weekly progress during\
    \ Google Summer of Code visit my GSoC blog:\n<a href=\"https://himani2000.github.io/gsoc/index.html\"\
    \ rel=\"nofollow\">https://himani2000.github.io/gsoc/index.html</a></p>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p>To install the dependencies use singualrity container .</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>\n1.Pull the singularity image using \n\
    \nsingularity pull --name clustering-image_and_audio.img shub://Himani2000/GSOC_2020:clustering\n\
    \n2. Run the image using \n\nsingularity <span class=\"pl-c1\">exec</span> -B\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">`</span>pwd<span class=\"pl-pds\"\
    >`</span></span> [singularity_image_name ].img python3 [python file]</pre></div>\n\
    <h2>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contributing</h2>\n<pre><code>Pull requests are welcome. For major\
    \ changes, please open an issue first to discuss what you would like to change.\n\
    \n</code></pre>\n<h2>\n<a id=\"user-content-acknowledgments\" class=\"anchor\"\
    \ href=\"#acknowledgments\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Acknowledgments</h2>\n<pre><code>1. Google summer\
    \ of code 2020\n2. Redhenlab \n</code></pre>\n"
  stargazers_count: 3
  subscribers_count: 1
  topics: []
  updated_at: 1607662384.0
HippocampusGirl/HALFpipe:
  data_format: 2
  description: null
  filenames:
  - Singularity.def
  full_name: HippocampusGirl/HALFpipe
  latest_release: null
  readme: '<h1>

    <a id="user-content-welcome-to-enigma-halfpipe" class="anchor" href="#welcome-to-enigma-halfpipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome
    to ENIGMA <code>HALFpipe</code>

    </h1>

    <p><a href="https://singularity-hub.org/collections/4508" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22build%22"><img
    src="https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg" alt="https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22continuous+integration%22"><img
    src="https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg"
    alt="https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg"
    style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/HALFpipe/HALFpipe" rel="nofollow"><img src="https://camo.githubusercontent.com/ef5be2978b13a91a1a602bc0261933d3735a7567176db1ef0c13eb65b3249056/68747470733a2f2f636f6465636f762e696f2f67682f48414c46706970652f48414c46706970652f6272616e63682f6d61737465722f67726170682f62616467652e737667"
    alt="codecov" data-canonical-src="https://codecov.io/gh/HALFpipe/HALFpipe/branch/master/graph/badge.svg"
    style="max-width:100%;"></a></p>

    <p><code>HALFpipe</code> is a user-friendly software that facilitates reproducible
    analysis of

    fMRI data, including preprocessing, single-subject, and group analysis. It

    provides state-of-the-art preprocessing using

    <a href="https://fmriprep.readthedocs.io/" rel="nofollow"><code>fmriprep</code></a>,
    but removes the necessity to

    convert data to the

    <a href="https://bids-specification.readthedocs.io/en/stable/" rel="nofollow"><code>BIDS</code></a>
    format. Common

    resting-state and task-based fMRI features can then be calculated on the fly

    using <a href="http://fsl.fmrib.ox.ac.uk/" rel="nofollow"><code>FSL</code></a>
    and

    <a href="https://nipype.readthedocs.io/" rel="nofollow"><code>nipype</code></a>
    for statistics.</p>

    <blockquote>

    <p>If you encounter issues, please see the <a href="#troubleshooting">troubleshooting</a>

    section of this document.</p>

    </blockquote>

    <h2>

    <a id="user-content-table-of-contents" class="anchor" href="#table-of-contents"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Table
    of Contents</h2>


    <ul>

    <li>

    <a href="#getting-started">Getting started</a>

    <ul>

    <li><a href="#container-platform">Container platform</a></li>

    <li><a href="#download">Download</a></li>

    <li><a href="#running">Running</a></li>

    </ul>

    </li>

    <li>

    <a href="#user-interface">User interface</a>

    <ul>

    <li><a href="#files">Files</a></li>

    <li><a href="#features">Features</a></li>

    <li><a href="#models">Models</a></li>

    </ul>

    </li>

    <li><a href="#running-on-a-high-performance-computing-cluster">Running on a high-performance
    computing cluster</a></li>

    <li><a href="#quality-checks">Quality checks</a></li>

    <li>

    <a href="#outputs">Outputs</a>

    <ul>

    <li><a href="#subject-level-features">Subject-level features</a></li>

    <li><a href="#preprocessed-images">Preprocessed images</a></li>

    <li><a href="#group-level">Group-level</a></li>

    </ul>

    </li>

    <li><a href="#troubleshooting">Troubleshooting</a></li>

    <li>

    <a href="#command-line-flags">Command line flags</a>

    <ul>

    <li><a href="#control-command-line-logging">Control command line logging</a></li>

    <li><a href="#automatically-remove-unneeded-files">Automatically remove unneeded
    files</a></li>

    <li><a href="#adjust-nipype">Adjust nipype</a></li>

    <li><a href="#choose-which-parts-to-run-or-to-skip">Choose which parts to run
    or to skip</a></li>

    <li><a href="#working-directory">Working directory</a></li>

    <li><a href="#data-file-system-root">Data file system root</a></li>

    </ul>

    </li>

    <li><a href="#contact">Contact</a></li>

    </ul>


    <h2>

    <a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Getting started</h2>

    <p><code>HALFpipe</code> is distributed as a container, meaning that all required
    software

    comes bundled in a monolithic file, the container. This allows for easy

    installation on new systems, and makes data analysis more reproducible, because

    software versions are guaranteed to be the same for all users.</p>

    <h3>

    <a id="user-content-container-platform" class="anchor" href="#container-platform"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    platform</h3>

    <p>The first step is to install one of the supported container platforms. If you''re

    using a high-performance computing cluster, more often than not

    <a href="https://sylabs.io" rel="nofollow"><code>Singularity</code></a> will already
    be available.</p>

    <p>If not, we recommend using the latest version

    of<a href="https://sylabs.io" rel="nofollow"><code>Singularity</code></a>. However,
    it can be somewhat cumbersome to

    install, as it needs to be built from source.</p>

    <p>The <a href="https://neuro.debian.net/" rel="nofollow"><code>NeuroDebian</code></a>
    package repository provides an

    older version of <a href="https://sylabs.io/guides/2.6/user-guide/" rel="nofollow"><code>Singularity</code></a>
    for

    <a href="https://neuro.debian.net/pkgs/singularity-container.html" rel="nofollow">some</a>
    Linux

    distributions.</p>

    <p>In contrast to <code>Singularity</code>, <code>Docker</code> always requires
    elevated privileges to

    run containers. In other words, every user running a <code>Docker</code> container

    automatically has administrator privileges on the computer they''re using.

    Therefore, it is inherently a bad choice for multi-user environments, where the

    access of individual users should be limited. <code>Docker</code> is the only
    option that

    is compatible with <code>Mac OS X</code>.</p>

    <table>

    <thead>

    <tr>

    <th>Container platform</th>

    <th>Version</th>

    <th>Installation</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><strong>Singularity</strong></td>

    <td><strong>3.5.3</strong></td>

    <td><strong>See <a href="https://sylabs.io/guides/3.5/user-guide/quick_start.html"
    rel="nofollow">https://sylabs.io/guides/3.5/user-guide/quick_start.html</a></strong></td>

    </tr>

    <tr>

    <td>Singularity</td>

    <td>2.6.1</td>

    <td><code>sudo apt install singularity-container</code></td>

    </tr>

    <tr>

    <td>Docker</td>

    <td></td>

    <td>See <a href="https://docs.docker.com/engine/install/" rel="nofollow">https://docs.docker.com/engine/install/</a>

    </td>

    </tr>

    </tbody>

    </table>

    <h3>

    <a id="user-content-download" class="anchor" href="#download" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Download</h3>

    <p>The second step is to download the <code>HALFpipe</code> to your computer.
    This requires

    approximately 5 gigabytes of storage.</p>

    <table>

    <thead>

    <tr>

    <th>Container platform</th>

    <th>Installation</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Singularity</td>

    <td>

    <a href="https://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g"
    rel="nofollow">https://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g</a>
    or <code>singularity pull docker://halfpipe/halfpipe:1.1.0</code> or <code>singularity
    pull docker://ghcr.io/halfpipe/halfpipe:1.1.0</code>

    </td>

    </tr>

    <tr>

    <td>Docker</td>

    <td><code>docker pull halfpipe/halfpipe:1.1.0</code></td>

    </tr>

    </tbody>

    </table>

    <p><code>Singularity</code> version <code>3.x</code> creates a container image
    file called

    <code>HALFpipe_{version}.sif</code> in the directory where you run the <code>pull</code>
    command. For

    <code>Singularity</code> version <code>2.x</code> the file is named

    <code>halfpipe-halfpipe-master-latest.simg</code>. Whenever you want to use the
    container,

    you need pass <code>Singularity</code> the path to this file.</p>

    <blockquote>

    <p><strong>NOTE:</strong> <code>Singularity</code> may store a copy of the container
    in its cache

    directory. The cache directory is located by default in your home directory at

    <code>~/.singularity</code>. If you need to save disk space in your home directory,
    you

    can safely delete the cache directory after downloading, i.e. by running

    <code>rm -rf ~/.singularity</code>. Alternatively, you could move the cache directory

    somewhere with more free disk space using a symlink. This way, files will

    automatically be stored there in the future. For example, if you have a lot of

    free disk space in <code>/mnt/storage</code>, then you could first run

    <code>mv ~/.singularity /mnt/storage</code> to move the cache directory, and then

    <code>ln -s /mnt/storage/.singularity ~/.singularity</code> to create the symlink.</p>

    </blockquote>

    <p><code>Docker</code> will store the container in its storage base directory,
    so it does not

    matter from which directory you run the <code>pull</code> command.</p>

    <h3>

    <a id="user-content-running" class="anchor" href="#running" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running</h3>

    <p>The third step is to run the downloaded container. You may need to replace

    <code>halfpipe_1.0.1.sif</code> with the actual path and filename where <code>Singularity</code>

    downloaded your container.</p>

    <table>

    <thead>

    <tr>

    <th>Container platform</th>

    <th>Command</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Singularity</td>

    <td><code>singularity run --no-home --cleanenv --bind /:/ext halfpipe_1.1.0.sif</code></td>

    </tr>

    <tr>

    <td>Docker</td>

    <td><code>docker run --interactive --tty --volume /:/ext halfpipe/halfpipe</code></td>

    </tr>

    </tbody>

    </table>

    <p>You should now see the user interface.</p>

    <h4>

    <a id="user-content-background" class="anchor" href="#background" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Background</h4>

    <p>Containers are by default isolated from the host computer. This adds security,

    but also means that the container cannot access the data it needs for analysis.

    <code>HALFpipe</code> expects all inputs (e.g., image files and spreadsheets)
    and outputs

    (the working directory) to be places in the path<code>/ext</code> (see also

    <a href="#data-file-system-root---fs-root"><code>--fs-root</code></a>). Using
    the option

    <code>--bind /:/ext</code>, we instruct <code>Singularity</code> to map all of
    the host file system

    (<code>/</code>) to that path (<code>/ext</code>). You can also run <code>HALFpipe</code>
    and only map only part

    of the host file system, but keep in mind that any directories that are not

    mapped will not be visible later.</p>

    <p><code>Singularity</code> passes the host shell environment to the container
    by default.

    This means that in some cases, the host computer''s configuration can interfere

    with the software. To avoid this, we need to pass the option <code>--cleanenv</code>.

    <code>Docker</code> does not pass the host shell environment by default, so we
    don''t need

    to pass an option.</p>

    <h2>

    <a id="user-content-user-interface" class="anchor" href="#user-interface" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>User interface</h2>

    <p>The user interface asks a series of questions about your data and the analyses

    you want to run. In each question, you can press <code>Control+C</code> to cancel
    the

    current question and go back to the previous one. <code>Control+D</code> exits
    the program

    without saving. Note that these keyboard shortcuts are the same on Mac.</p>

    <h3>

    <a id="user-content-files" class="anchor" href="#files" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Files</h3>

    <p>To run preprocessing, at least a T1-weighted structural image and a BOLD image

    file is required. Preprocessing and data analysis proceeds automatically.

    However, to be able to run automatically, data files need to be input in a way

    suitable for automation.</p>

    <p>For this kind of automation, <code>HALFpipe</code> needs to know the relationships
    between

    files, such as which files belong to the same subject. However, even though it

    would be obvious for a human, a program cannot easily assign a file name to a

    subject, and this will be true as long as there are differences in naming

    between different researchers or labs. One researcher may name the same file

    <code>subject_01_rest.nii.gz</code> and another <code>subject_01/scan_rest.nii.gz</code>.</p>

    <p>In <code>HALFpipe</code>, we solve this issue by inputting file names in a
    specific way.

    For example, instead of <code>subject_01/scan_rest.nii.gz</code>, <code>HALFpipe</code>
    expects you to

    input <code>{subject}/scan_rest.nii.gz</code>. <code>HALFpipe</code> can then
    match all files on disk

    that match this naming schema, and extract the subject ID <code>subject_01</code>.
    Using

    the extracted subject ID, other files can now be matched to this image. If all

    input files are available in BIDS format, then this step can be skipped.</p>

    <ol>

    <li>

    <code>Specify working directory</code> All intermediate and outputs of <code>HALFpipe</code>
    will

    be placed in the working directory. Keep in mind to choose a location with

    sufficient free disk space, as intermediates can be multiple gigabytes in

    size for each subject.</li>

    <li>

    <code>Is the data available in BIDS format?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify the path of the BIDS directory</code></li>

    </ol>

    </li>

    <li>

    <code>No</code>

    <ol>

    <li>

    <code>Specify anatomical/structural data</code><br>

    <code>Specify the path of the T1-weighted image files</code>

    </li>

    <li>

    <code>Specify functional data</code><br>

    <code>Specify the path of the BOLD image files</code>

    </li>

    <li>

    <code>Check repetition time values</code> / <code>Specify repetition time in seconds</code>

    </li>

    <li>

    <code>Add more BOLD image files?</code>

    <ul>

    <li>

    <code>Yes</code> Loop back to 2</li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    </ol>

    </li>

    </ul>

    </li>

    <li>

    <code>Do slice timing?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Check slice acquisition direction values</code></li>

    <li><code>Check slice timing values</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Skip this step</li>

    </ul>

    </li>

    <li>

    <code>Specify field maps?</code> If the data was imported from a BIDS directory,
    this

    step will be omitted.

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li>

    <code>Specify the type of the field maps</code>

    <ul>

    <li>EPI (blip-up blip-down)

    <ol>

    <li><code>Specify the path of the blip-up blip-down EPI image files</code></li>

    </ol>

    </li>

    <li>Phase difference and magnitude (used by Siemens scanners)

    <ol>

    <li><code>Specify the path of the magnitude image files</code></li>

    <li><code>Specify the path of the phase/phase difference image files</code></li>

    <li><code>Specify echo time difference in seconds</code></li>

    </ol>

    </li>

    <li>Scanner-computed field map and magnitude (used by GE / Philips

    scanners)

    <ol>

    <li><code>Specify the path of the magnitude image files</code></li>

    <li><code>Specify the path of the field map image files</code></li>

    </ol>

    </li>

    </ul>

    </li>

    <li>

    <code>Add more field maps?</code> Loop back to 1</li>

    <li><code>Specify effective echo spacing for the functional data in seconds</code></li>

    <li><code>Specify phase encoding direction for the functional data</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Skip this step</li>

    </ul>

    </li>

    </ol>

    <h3>

    <a id="user-content-features" class="anchor" href="#features" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Features</h3>

    <p>Features are analyses that are carried out on the preprocessed data, in other

    words, first-level analyses.</p>

    <ol>

    <li>

    <code>Specify first-level features?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li>

    <code>Specify the feature type</code>

    <ul>

    <li>

    <code>Task-based</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li><code>Specify the event file type</code></li>

    </ol>

    <ul>

    <li>

    <code>SPM multiple conditions</code> A MATLAB .mat file containing three

    arrays: <code>names</code> (condition), <code>onsets</code> and <code>durations</code>

    </li>

    <li>

    <code>FSL 3-column</code> One text file for each condition. Each file has its

    corresponding condition in the filename. The first column specifies

    the event onset, the second the duration. The third column of the

    files is ignored, so parametric modulation is not supported</li>

    <li>

    <code>BIDS TSV</code> A tab-separated table with named columns <code>trial_type</code>

    (condition), <code>onset</code> and <code>duration</code>. While BIDS supports
    defining

    additional columns, <code>HALFpipe</code> will currently ignore these</li>

    </ul>

    <ol>

    <li><code>Specify the path of the event files</code></li>

    <li><code>Select conditions to add to the model</code></li>

    <li>

    <code>Specify contrasts</code>

    <ol>

    <li><code>Specify contrast name</code></li>

    <li><code>Specify contrast values</code></li>

    <li>

    <code>Add another contrast?</code>

    <ul>

    <li>

    <code>Yes</code> Loop back to 1</li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    </ol>

    </li>

    <li>

    <code>Apply a temporal filter to the design matrix?</code> A separate temporal

    filter can be specified for the design matrix. In contrast, the

    temporal filtering of the input image and any confound regressors

    added to the design matrix is specified in 10. In general, the two

    settings should match</li>

    <li>

    <code>Apply smoothing?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify smoothing FWHM in mm</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li><code>Grand mean scaling will be applied with a mean of 10000.000000</code></li>

    <li>

    <code>Temporal filtering will be applied using a gaussian-weighted filter</code><br>

    <code>Specify the filter width in seconds</code>

    </li>

    <li><code>Remove confounds?</code></li>

    </ol>

    </li>

    <li>

    <code>Seed-based connectivity</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>

    <code>Specify binary seed mask file(s)</code>

    <ol>

    <li><code>Specify the path of the binary seed mask image files</code></li>

    <li><code>Check space values</code></li>

    <li><code>Add binary seed mask image file</code></li>

    </ol>

    </li>

    </ol>

    </li>

    <li>

    <code>Dual regression</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    <li>

    <code>Atlas-based connectivity matrix</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    <li>

    <code>ReHo</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    <li>

    <code>fALFF</code>

    <ol>

    <li><code>Specify feature name</code></li>

    <li><code>Specify images to use</code></li>

    <li>TODO</li>

    </ol>

    </li>

    </ul>

    </li>

    </ol>

    </li>

    <li>

    <code>No</code> Skip this step</li>

    </ul>

    </li>

    <li>

    <code>Add another first-level feature?</code>

    <ul>

    <li>

    <code>Yes</code> Loop back to 1</li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li>

    <code>Output a preprocessed image?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify setting name</code></li>

    <li><code>Specify images to use</code></li>

    <li>

    <code>Apply smoothing?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify smoothing FWHM in mm</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li>

    <code>Do grand mean scaling?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li><code>Specify grand mean</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li>

    <code>Apply a temporal filter?</code>

    <ul>

    <li>

    <code>Yes</code>

    <ol>

    <li>

    <code>Specify the type of temporal filter</code>

    <ul>

    <li><code>Gaussian-weighted</code></li>

    <li><code>Frequency-based</code></li>

    </ul>

    </li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    <li><code>Remove confounds?</code></li>

    </ol>

    </li>

    <li>

    <code>No</code> Continue</li>

    </ul>

    </li>

    </ol>

    <h3>

    <a id="user-content-models" class="anchor" href="#models" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Models</h3>

    <p>Models are statistical analyses that are carried out on the features.</p>

    <blockquote>

    <p>TODO</p>

    </blockquote>

    <h2>

    <a id="user-content-running-on-a-high-performance-computing-cluster" class="anchor"
    href="#running-on-a-high-performance-computing-cluster" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running on a high-performance
    computing cluster</h2>

    <ol>

    <li>

    <p>Log in to your cluster''s head node</p>

    </li>

    <li>

    <p>Request an interactive job. Refer to your cluster''s documentation for how
    to

    do this</p>

    </li>

    <li>

    <p>In the interactive job, run the <code>HALFpipe</code> user interface, but add
    the flag

    <code>--use-cluster</code> to the end of the command. <br>

    For example, <code>singularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif
    --use-cluster</code></p>

    </li>

    <li>

    <p>As soon as you finish specifying all your data, features and models in the

    user interface, <code>HALFpipe</code> will now generate everything needed to run
    on the

    cluster. For hundreds of subjects, this can take up to a few hours.</p>

    </li>

    <li>

    <p>When <code>HALFpipe</code> exits, edit the generated submit script <code>submit.slurm.sh</code>

    according to your cluster''s documentation and then run it. This submit script

    will calculate everything except group statistics.</p>

    </li>

    <li>

    <p>As soon as all processing has been completed, you can run group statistics.

    This is usually very fast, so you can do this in an interactive session. Run

    <code>singularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif --only-model-chunk</code>

    and then select <code>Run without modification</code> in the user interface.</p>

    </li>

    </ol>

    <blockquote>

    <p>A common issue with remote work via secure shell is that the connection may

    break after a few hours. For batch jobs this is not an issue, but for

    interactive jobs this can be quite frustrating. When the connection is lost,

    the node you were connected to will automatically quit all programs you were

    running. To prevent this, you can run interactive jobs within <code>screen</code>
    or

    <code>tmux</code> (whichever is available). These commands allow you to open sessions
    in

    the terminal that will continue running in the background even when you close

    or disconnect. Here''s a quick overview of how to use the commands (more

    in-depth documentation is available for example at

    [<a href="http://www.dayid.org/comp/tm.html" rel="nofollow">http://www.dayid.org/comp/tm.html</a>]).</p>

    <ol>

    <li>Open a new screen/tmux session on the head node by running either <code>screen</code>

    or <code>tmux</code>

    </li>

    <li>Request an interactive job from within the session, for example with

    <code>srun --pty bash -i</code>

    </li>

    <li>Run the command that you want to run</li>

    <li>Detach from the screen/tmux session, meaning disconnecting with the ability

    to re-connect later <br>

    For screen, this is done by first pressing <code>Control+a</code>, then letting
    go, and

    then pressing <code>d</code> on the keyboard. <br>

    For tmux, it''s <code>Control+b</code> instead of <code>Control+a</code>. <br>

    Note that this is always <code>Control</code>, even if you''re on a mac.</li>

    <li>Close your connection to the head node with <code>Control+d</code>. <code>screen</code>/<code>tmux</code>

    will remain running in the background</li>

    <li>Later, connect again to the head node. Run <code>screen -r</code> or <code>tmux
    attach</code> to

    check back on the interactive job. If everything went well and the command

    you wanted to run finished, close the interactive job with <code>Control+d</code>
    and

    then the <code>screen</code>/<code>tmux</code> session with <code>Control+d</code>
    again. If the command

    hasn''t finished yet, detach as before and come back later</li>

    </ol>

    </blockquote>

    <h2>

    <a id="user-content-quality-checks" class="anchor" href="#quality-checks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quality checks</h2>

    <p>Please see the manual at <a href="https://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8"
    rel="nofollow">https://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8</a></p>

    <h2>

    <a id="user-content-outputs" class="anchor" href="#outputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Outputs</h2>

    <ul>

    <li>

    <p>A visual report page <code>reports/index.html</code></p>

    </li>

    <li>

    <p>A table with image quality metrics <code>reports/reportvals.txt</code></p>

    </li>

    <li>

    <p>A table containing the preprocessing status <code>reports/reportpreproc.txt</code></p>

    </li>

    <li>

    <p>The untouched <code>fmriprep</code> derivatives. Some files have been omitted
    to save

    disk space <code>fmriprep</code> is very strict about only processing data that
    is

    compliant with the BIDS standard. As such, we may need to format subjects

    names for compliance. For example, an input subject named <code>subject_01</code>
    will

    appear as <code>subject01</code> in the <code>fmriprep</code> derivatives. <code>derivatives/fmriprep</code></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-subject-level-features" class="anchor" href="#subject-level-features"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Subject-level
    features</h3>

    <ul>

    <li>

    <p>For task-based, seed-based connectivity and dual regression features,

    <code>HALFpipe</code> outputs the statistical maps for the effect, the variance,
    the

    degrees of freedom of the variance and the z-statistic. In FSL, the effect and

    variance are also called <code>cope</code> and <code>varcope</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-effect_statmap.nii.gz</code>
    <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-variance_statmap.nii.gz</code>
    <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-dof_statmap.nii.gz</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._stat-z_statmap.nii.gz</code> <br>

    The design and contrast matrix used for the final model will be outputted alongside

    the statistical maps <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-design_matrix.tsv</code>

    <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-contrast_matrix.tsv</code></p>

    </li>

    <li>

    <p>ReHo and fALFF are not calculated based on a linear model. As such, only one

    statistical map of the z-scaled values will be output <br>

    <code>derivatives/halfpipe/sub-.../func/..._alff.nii.gz</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._falff.nii.gz</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._reho.nii.gz</code></p>

    </li>

    <li>

    <p>For every feature, a JSON file containing a summary of the preprocessing</p>

    </li>

    <li>

    <p>settings, and a list of the raw data files that were used for the analysis

    (<code>RawSources</code>) <br>

    <code>derivatives/halfpipe/sub-.../func/....json</code></p>

    </li>

    <li>

    <p>For every feature, the corresponding brain mask is output beside the

    statistical maps. Masks do not differ between different features calculated,

    they are only copied out repeatedly for convenience <br>

    <code>derivatives/halfpipe/sub-.../func/...desc-brain_mask.nii.gz</code></p>

    </li>

    <li>

    <p>Atlas-based connectivity outputs the time series and the full covariance and

    correlation matrices as text files <br>

    <code>derivatives/halfpipe/sub-.../func/..._timeseries.txt</code> <br>

    <code>derivatives/halfpipe/sub-.../func/..._desc-covariance_matrix.txt</code>
    <br>

    <code>derivatives/halfpipe/sub-.../func/..._desc-correlation_matrix.txt</code></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-preprocessed-images" class="anchor" href="#preprocessed-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Preprocessed
    images</h3>

    <ul>

    <li>

    <p>Masked, preprocessed BOLD image <br>

    <code>derivatives/halfpipe/sub-.../func/..._bold.nii.gz</code></p>

    </li>

    <li>

    <p>Just like for features <br>

    <code>derivatives/halfpipe/sub-.../func/..._bold.json</code></p>

    </li>

    <li>

    <p>Just like for features <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-brain_mask.nii.gz</code></p>

    </li>

    <li>

    <p>Filtered confounds time series, where all filters that are applied to the BOLD

    image are applied to the regressors as well. Note that this means that when

    grand mean scaling is active, confounds time series are also scaled, meaning

    that values such as <code>framewise displacement</code> can not be interpreted
    in terms

    of their original units anymore. <br>

    <code>derivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-confounds_regressors.tsv</code></p>

    </li>

    </ul>

    <h3>

    <a id="user-content-group-level" class="anchor" href="#group-level" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Group-level</h3>

    <ul>

    <li><code>grouplevel/...</code></li>

    </ul>

    <h2>

    <a id="user-content-troubleshooting" class="anchor" href="#troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Troubleshooting</h2>

    <ul>

    <li>If an error occurs, this will be output to the command line and simultaneously

    to the <code>err.txt</code> file in the working directory</li>

    <li>If the error occurs while running, usually a text file detailing the error

    will be placed in the working directory. These are text files and their file

    names start with <code>crash</code>

    <ul>

    <li>Usually, the last line of these text files contains the error message.

    Please read this carefully, as may allow you to understand the error</li>

    <li>For example, consider the following error message:

    <code>ValueError: shape (64, 64, 33) for image 1 not compatible with first image
    shape (64, 64, 34) with axis == None</code>

    This error message may seem cryptic at first. However, looking at the

    message more closely, it suggests that two input images have different,

    incompatible dimensions. In this case, <code>HALFpipe</code> correctly recognized
    this

    issue, and there is no need for concern. The images in question will simply

    be excluded from preprocessing and/or analysis</li>

    <li>In some cases, the cause of the error can be a bug in the <code>HALFpipe</code>
    code.

    Please check that no similar issue has been reported

    <a href="https://github.com/HALFpipe/HALFpipe/issues">here on GitHub</a>. In this
    case,

    please submit an

    <a href="https://github.com/HALFpipe/HALFpipe/issues/new/choose">issue</a>.</li>

    </ul>

    </li>

    </ul>

    <h2>

    <a id="user-content-command-line-flags" class="anchor" href="#command-line-flags"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Command
    line flags</h2>

    <h3>

    <a id="user-content-control-command-line-logging" class="anchor" href="#control-command-line-logging"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Control
    command line logging</h3>

    <div class="highlight highlight-source-shell"><pre>--verbose</pre></div>

    <p>By default, only errors and warnings will be output to the command line. This

    makes it easier to see when something goes wrong, because there is less output.

    However, if you want to be able to inspect what is being run, you can add the

    <code>--verbose</code> flag to the end of the command used to call <code>HALFpipe</code>.</p>

    <p>Verbose logs are always written to the <code>log.txt</code> file in the working
    directory,

    so going back and inspecting this log is always possible, even if the

    <code>--verbose</code> flag was not specified.</p>

    <p>Specifying the flag <code>--debug</code> will print additional, fine-grained
    messages. It

    will also automatically start the

    <a href="https://docs.python.org/3/library/pdb.html" rel="nofollow">Python Debugger</a>
    when an error

    occurs. You should only use <code>--debug</code> if you know what you''re doing.</p>

    <h3>

    <a id="user-content-automatically-remove-unneeded-files" class="anchor" href="#automatically-remove-unneeded-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Automatically
    remove unneeded files</h3>

    <div class="highlight highlight-source-shell"><pre>--keep</pre></div>

    <p><code>HALFpipe</code> saves intermediate files for each pipeline step. This
    speeds up

    re-running with different settings, or resuming after a job after it was

    cancelled. The intermediate file are saved by the

    <a href="https://nipype.readthedocs.io/" rel="nofollow"><code>nipype</code></a>
    workflow engine, which is what

    <code>HALFpipe</code> uses internally. <code>nipype</code> saves the intermediate
    files in the

    <code>nipype</code> folder in the working directory.</p>

    <p>In environments with limited disk capacity, this can be problematic. To limit

    disk usage, <code>HALFpipe</code> can delete intermediate files as soon as they
    are not

    needed anymore. This behavior is controlled with the <code>--keep</code> flag.</p>

    <p>The default option <code>--keep some</code> keeps all intermediate files from
    fMRIPrep and

    MELODIC, which would take the longest to re-run. We believe this is a good

    tradeoff between disk space and computer time. <code>--keep all</code> turns of
    all

    deletion of intermediate files. <code>--keep none</code> deletes as much as possible,

    meaning that the smallest amount possible of disk space will be used.</p>

    <h3>

    <a id="user-content-configure-nipype" class="anchor" href="#configure-nipype"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configure
    nipype</h3>

    <div class="highlight highlight-source-shell"><pre>--nipype-<span class="pl-k">&lt;</span>omp-nthreads<span
    class="pl-k">|</span>memory-gb<span class="pl-k">|</span>n-procs<span class="pl-k">|</span>run-plugin<span
    class="pl-k">&gt;</span></pre></div>

    <p><code>HALFpipe</code> chooses sensible defaults for all of these values.</p>

    <h3>

    <a id="user-content-choose-which-parts-to-run-or-to-skip" class="anchor" href="#choose-which-parts-to-run-or-to-skip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Choose
    which parts to run or to skip</h3>

    <div class="highlight highlight-source-shell"><pre>--<span class="pl-k">&lt;</span>only<span
    class="pl-k">|</span>skip<span class="pl-k">&gt;</span>-<span class="pl-k">&lt;</span>spec-ui<span
    class="pl-k">|</span>workflow<span class="pl-k">|</span>run<span class="pl-k">|</span>model-chunk<span
    class="pl-k">&gt;</span></pre></div>

    <p>A <code>HALFpipe</code> run is divided internally into three stages, spec-ui,
    workflow, and

    run.</p>

    <ol>

    <li>The <code>spec-ui</code> stage is where you specify things in the user interface.
    It

    creates the <code>spec.json</code> file that contains all the information needed
    to run

    <code>HALFpipe</code>. To only run this stage, use the option <code>--only-spec-ui</code>.
    To skip

    this stage, use the option <code>--skip-spec-ui</code>

    </li>

    <li>The <code>workflow</code> stage is where <code>HALFpipe</code> uses the <code>spec.json</code>
    data to search

    for all the files that match what was input in the user interface. It then

    generates a <code>nipype</code> workflow for preprocessing, feature extraction
    and group

    models. <code>nipype</code> then validates the workflow and prepares it for execution.

    This usually takes a couple of minutes and cannot be parallelized. For

    hundreds of subjects, this may even take a few hours. This stage has the

    corresponding option <code>--only-workflow</code> and <code>--skip-workflow</code>.</li>

    </ol>

    <ul>

    <li>This stage saves several intermediate files. These are named

    <code>workflow.{uuid}.pickle.xz</code>, <code>execgraph.{uuid}.pickle.xz</code>
    and

    <code>execgraph.{n_chunks}_chunks.{uuid}.pickle.xz</code>. The <code>uuid</code>
    in the file name is

    a unique identifier generated from the <code>spec.json</code> file and the input
    files.

    It is re-calculated every time we run this stage. The uuid algorithm produces

    a different output if there are any changes (such as when new input files for

    new subjects become available, or the <code>spec.json</code> is changed, for example
    to

    add a new feature or group model). Otherwise, the <code>uuid</code> stays the
    same.

    Therefore, if a workflow file with the calculated <code>uuid</code> already exists,
    then

    we do not need to run this stage. We can simple re-use the workflow from the

    existing file, and save some time.</li>

    <li>In this stage, we can also decide to split the execution into chunks. The
    flag

    <code>--subject-chunks</code> creates one chunk per subject. The flag <code>--use-cluster</code>

    automatically activates <code>--subject-chunks</code>. The flag <code>--n-chunks</code>
    allows the

    user to specify a specific number of chunks. This is useful if the execution

    should be spread over a set number of computers. In addition to these, a model

    chunk is generated.</li>

    </ul>

    <ol>

    <li>The <code>run</code> stage loads the <code>execgraph.{n_chunks}_chunks.{uuid}.pickle.xz</code>
    file

    generated in the previous step and runs it. This file usually contains two

    chunks, one for the subject level preprocessing and feature extraction

    ("subject level chunk"), and one for group statistics ("model chunk"). To run

    a specific chunk, you can use the flags <code>--only-chunk-index ...</code> and

    <code>--only-model-chunk</code>.</li>

    </ol>

    <h3>

    <a id="user-content-working-directory" class="anchor" href="#working-directory"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Working
    directory</h3>

    <div class="highlight highlight-source-shell"><pre>--workdir</pre></div>

    <blockquote>

    <p>TODO</p>

    </blockquote>

    <h3>

    <a id="user-content-data-file-system-root" class="anchor" href="#data-file-system-root"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data
    file system root</h3>

    <div class="highlight highlight-source-shell"><pre>--fs-root</pre></div>

    <p>The <code>HALFpipe</code> container, or really most containers, contain the
    entire base

    system needed to run</p>

    <h2>

    <a id="user-content-contact" class="anchor" href="#contact" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

    <p>For questions or support, please submit an

    <a href="https://github.com/HALFpipe/HALFpipe/issues/new/choose">issue</a> or
    contact us

    via e-mail.</p>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Role</th>

    <th>E-mail address</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Lea Waller</td>

    <td>Developer</td>

    <td><a href="mailto:lea.waller@charite.de">lea.waller@charite.de</a></td>

    </tr>

    <tr>

    <td>Ilya Veer</td>

    <td>Project manager</td>

    <td><a href="mailto:ilya.veer@charite.de">ilya.veer@charite.de</a></td>

    </tr>

    <tr>

    <td>Susanne Erk</td>

    <td>Project manager</td>

    <td><a href="mailto:susanne.erk@charite.de">susanne.erk@charite.de</a></td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 4
  subscribers_count: 1
  topics: []
  updated_at: 1620930214.0
Hyedryn/elikopy:
  data_format: 2
  description: ElikoPy is Python library aiming at easing the processing of diffusion
    imaging for microstructural analysis.
  filenames:
  - Singularity_elikopy
  full_name: Hyedryn/elikopy
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-elikopy\" class=\"anchor\" href=\"#elikopy\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ElikoPy</h1>\n<p>ElikoPy is Python library aiming at easing the processing\
    \ of diffusion imaging for microstructural analysis.\nThis Python library is based\
    \ on</p>\n<ul>\n<li>DIPY, a python library for the analysis of MR diffusion imaging.</li>\n\
    <li>Microstructure fingerprinting, a python library doing estimation of white\
    \ matter microstructural properties from a dictionary of Monte Carlo diffusion\
    \ MRI fingerprints.</li>\n<li>FSL, a comprehensive library of analysis tools for\
    \ FMRI, MRI and DTI brain imaging data.</li>\n<li>DIAMOND, a c software that is\
    \ characterizing brain tissue by assessment of the distribution of anisotropic\
    \ microstructural environments in diffusion\u2010compartment imaging.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h3>\n<p>ElikoPy requires <a href=\"https://www.python.org/\"\
    \ rel=\"nofollow\">Python</a> v3.7+ to run.</p>\n<p>After cloning the repo, you\
    \ can either firstly install all the python dependencies including optionnal dependency\
    \ used to speed up the code:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ pip install -r requirements.txt --user</pre></div>\n<p>Or you can install\
    \ directly the library with only the mandatory dependencies (if you performed\
    \ the previous step, you still need to perform this step):</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>$ python3 setup.py install --user</pre></div>\n\
    <p>Microstructure Fingerprinting is currently not avaible in the standard python\
    \ repo, you can clone and install this library manually.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ git clone git@github.com:rensonnetg/microstructure_fingerprinting.git\n\
    $ <span class=\"pl-c1\">cd</span> microstructure_fingerprinting\n$ python setup.py\
    \ install</pre></div>\n<p>FSL also needs to be installed and availabe in our path\
    \ if you want to perform mouvement correction or tbss.</p>\n<p>Unfortunatly, the\
    \ DIAMOND code is not publically available. If you do not have it in your possesion,\
    \ you will not be able to use this algorithm. If you have it, simply add the executable\
    \ to your path.</p>\n<h3>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"\
    #usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h3>\n<p>Todo</p>\n<h3>\n<a id=\"user-content-development\"\
    \ class=\"anchor\" href=\"#development\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development</h3>\n<p>Want to\
    \ contribute? Great!</p>\n<p>Do not hesitate to open issue or pull request!</p>\n\
    <h3>\n<a id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Todos</h3>\n\
    <ul>\n<li>Release a complete and accurate documentation for the library</li>\n\
    </ul>\n<p><strong>Free Software, Hell Yeah!</strong></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - microstructure-fingerprinting
  - fsl
  - tbss
  - python-library
  - diffusion-imaging
  - preprocessing
  - dmri
  - diamond
  - noddi
  - dti
  updated_at: 1621866034.0
IARCbioinfo/ITH_pipeline:
  data_format: 2
  description: Pipeline to study intra tumor heterogeneity using HATCHet, DeCiFer
    and ClonEvol
  filenames:
  - Singularity
  full_name: IARCbioinfo/ITH_pipeline
  latest_release: null
  readme: '<h1>

    <a id="user-content-ith_pipeline" class="anchor" href="#ith_pipeline" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ITH_pipeline</h1>

    <h2>

    <a id="user-content-nextflow-pipeline-to-study-intra-tumoral-heterogeneity" class="anchor"
    href="#nextflow-pipeline-to-study-intra-tumoral-heterogeneity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Nextflow pipeline to
    study intra-tumoral heterogeneity</h2>

    <h2>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h2>

    <p>Nextflow pipeline to study intra-tumoral heterogeneity through subclonality
    reconstruction, using HATCHet, DeCiFer and clonEvol.

    PIPELINE IN DEVELOPMENT.</p>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <ol>

    <li>This pipeline is based on <a href="https://www.nextflow.io" rel="nofollow">nextflow</a>.
    As we have several nextflow pipelines, we have centralized the common information
    in the <a href="https://github.com/IARCbioinfo/IARC-nf">IARC-nf</a> repository.
    Please read it carefully as it contains essential information for the installation,
    basic usage and configuration of nextflow and our pipelines.</li>

    <li>External software:</li>

    </ol>

    <ul>

    <li><a href="https://github.com/raphael-group/hatchet">HATCHet</a></li>

    <li><a href="https://github.com/raphael-group/decifer">DeCiFer</a></li>

    <li><a href="https://github.com/hdng/clonevol">ClonEvol</a></li>

    </ul>

    <p>You can avoid installing all the external software by only installing Docker.
    See the <a href="https://github.com/IARCbioinfo/IARC-nf">IARC-nf</a> repository
    for more information.</p>

    <h2>

    <a id="user-content-input" class="anchor" href="#input" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Input</h2>

    <table>

    <thead>

    <tr>

    <th>Type</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--bam_folder</td>

    <td>Folder containing BAM files to pass to HATCHet</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-parameters" class="anchor" href="#parameters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Parameters</h2>

    <ul>

    <li>

    <h4>

    <a id="user-content-mandatory" class="anchor" href="#mandatory" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Mandatory</h4>

    </li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Example value</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--ref</td>

    <td>/whole/path/to/genome.fa</td>

    <td>Reference genome in fasta format</td>

    </tr>

    <tr>

    <td>--correspondance</td>

    <td>TN_pairs.txt</td>

    <td>Tabulated file containing two columns: tumor and normal bam names</td>

    </tr>

    </tbody>

    </table>

    <ul>

    <li>

    <h4>

    <a id="user-content-optional" class="anchor" href="#optional" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Optional</h4>

    </li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Default value</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--cpu</td>

    <td>1</td>

    <td>Number of cpu to use</td>

    </tr>

    <tr>

    <td>--config</td>

    <td>null</td>

    <td>Use custom configuration file</td>

    </tr>

    <tr>

    <td>--mem</td>

    <td>4</td>

    <td>Size of memory used in GB</td>

    </tr>

    <tr>

    <td>--output_folder</td>

    <td>.</td>

    <td>Path to output folder</td>

    </tr>

    <tr>

    <td>--samtools_folder</td>

    <td>/usr/bin/</td>

    <td>samtools installation dir</td>

    </tr>

    <tr>

    <td>--bcftools_folder</td>

    <td>/usr/bin/</td>

    <td>bcftools installation dir</td>

    </tr>

    <tr>

    <td>--hatchet_folder</td>

    <td>/usr/bin/</td>

    <td>hatchet installation dir</td>

    </tr>

    <tr>

    <td>--bnpy_folder</td>

    <td>/usr/bin/</td>

    <td>bnpy installation dir</td>

    </tr>

    </tbody>

    </table>

    <ul>

    <li>

    <h4>

    <a id="user-content-flags" class="anchor" href="#flags" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Flags</h4>

    </li>

    </ul>

    <p>Flags are special parameters without value.</p>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--help</td>

    <td>Display help</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <pre><code>nextflow run iarcbioinfo/ITH_pipeline --bam_folder bam_folder/ --ref
    genome.fa --correspondance pairs.txt --cpu 24 --samtools_folder ~/bin/samtools-1.7
    --bcftools_folder ~/bin/bcftools-1.7 --hatchet_folder ~/bin/hatchet --bnpy_folder
    ~/bin/bnpy-dev

    </code></pre>

    <h2>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h2>

    <table>

    <thead>

    <tr>

    <th>Type</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>......</td>

    <td>......</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-detailed-description" class="anchor" href="#detailed-description"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Detailed
    description</h2>

    <p>...</p>

    <h2>

    <a id="user-content-directed-acyclic-graph" class="anchor" href="#directed-acyclic-graph"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Directed
    Acyclic Graph</h2>

    <h2>

    <a id="user-content-contributions" class="anchor" href="#contributions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributions</h2>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Email</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Tiffany Delhomme</td>

    <td><a href="mailto:delhommet@students.iarc">delhommet@students.iarc</a></td>

    <td>Developer to contact for support</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-references-optional" class="anchor" href="#references-optional"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References
    (optional)</h2>

    <h2>

    <a id="user-content-faq-optional" class="anchor" href="#faq-optional" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FAQ (optional)</h2>

    '
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1621628184.0
IARCbioinfo/Imputation-nf:
  data_format: 2
  description: Imputation pipeline
  filenames:
  - Singularity/Singularity.v1.0
  full_name: IARCbioinfo/Imputation-nf
  latest_release: v1.0
  readme: "<h1>\n<a id=\"user-content-genotyping-imputation---pipeline-v10\" class=\"\
    anchor\" href=\"#genotyping-imputation---pipeline-v10\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Genotyping imputation\
    \  : Pipeline V1.0</h1>\n<h2>\n<a id=\"user-content-a-nextflow-pipeline-to-realise-a-datasets-genotyping-imputation\"\
    \ class=\"anchor\" href=\"#a-nextflow-pipeline-to-realise-a-datasets-genotyping-imputation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>A nextflow pipeline to realise a dataset's genotyping imputation</h2>\n\
    <p><a href=\"https://circleci.com/gh/IARCbioinfo/Imputation-nf\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/e3a0e94b24410397271294a485f985c85941b292ba2c7cbf3fefb26bf1e0c76b/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f74656d706c6174652d6e662e7376673f7374796c653d737667\"\
    \ alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/IARCbioinfo/template-nf.svg?style=svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/iarcbioinfo/imputation-nf/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5c5d5383ee3d6248c7d31b5fcaa21fc7d689b53ecb330dbfe628cd1fae38c853/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d72656164792d626c75652e737667\"\
    \ alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/badge/docker-ready-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/4533\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://zenodo.org/badge/latestdoi/94193130\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/5decad826d7116b1c950b6b48c06052496f141e803525b088ce3cdcaaa4d7b88/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f39343139333133302e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/94193130.svg\" style=\"\
    max-width:100%;\"></a></p>\n<p><a href=\"template-nf.png\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img src=\"template-nf.png\" alt=\"Workflow representation\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-description\"\
    \ class=\"anchor\" href=\"#description\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Description</h2>\n<p>The pipeline\
    \ used to perform the imputation of several targets datasets processed with standard\
    \ input.</p>\n<p>Here is a summary of the method :</p>\n<ul>\n<li>Preprocessing\
    \ of data : by using the nextflow script Preparation.nf with create a directory\
    \ \"file/\" with all the dependencies.</li>\n<li>First step : Origin estimation\
    \ of sample from the target dataset by using admixture tools and the hapmap dataset\
    \ as reference.</li>\n<li>Second step : Series of SNPs filters and quality checking\
    \ from the target dataset before the imputation step.</li>\n<li>Third step : VCF\
    \ production</li>\n<li>Last step : Phasing and imputation</li>\n</ul>\n<p>See\
    \ the Usage section to test the full pipeline with your target dataset.</p>\n\
    <h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<p>The pipeline works under Linux distributions.</p>\n\
    <ol>\n<li>\n<p>This pipeline is based on <a href=\"https://www.nextflow.io\" rel=\"\
    nofollow\">nextflow</a>. As we have several nextflow pipelines, we have centralized\
    \ the common information in the <a href=\"https://github.com/IARCbioinfo/IARC-nf\"\
    >IARC-nf</a> repository. Please read it carefully as it contains essential information\
    \ for the installation, basic usage and configuration of nextflow and our pipelines.</p>\n\
    </li>\n<li>\n<p>External software:</p>\n</li>\n</ol>\n<ul>\n<li>LiftOver : conda\
    \ install ucsc-liftover</li>\n<li>Plink (PLINK v1.90b6.12 64-bit (28 Oct 2019))\
    \ : conda install plink</li>\n<li>Admixture (ADMIXTURE Version 1.3.0) : conda\
    \ install admixture</li>\n<li>Perl : conda install perl</li>\n<li>Term::ReadKey\
    \ module : conda install perl-termreadkey</li>\n<li>BcfTools : conda install bcftools</li>\n\
    <li>eagle 2.4.1 : <a href=\"https://data.broadinstitute.org/alkesgroup/Eagle/#x1-50002.2\"\
    \ rel=\"nofollow\">See instructions</a>\n</li>\n<li>minimac4 : conda install cmake\
    \ ; pip install cget ; git clone <a href=\"https://github.com/statgen/Minimac4.git\"\
    >https://github.com/statgen/Minimac4.git</a> ; cd Minimac4 ; bash install.sh</li>\n\
    <li>Samtools : conda install samtools</li>\n</ul>\n<ol start=\"3\">\n<li>File\
    \ to download :</li>\n</ol>\n<ul>\n<li>\n<a href=\"zzz.bwh.harvard.edu/plink/dist/hapmap_r23a.zip\"\
    >Hapmap Dataset</a> : as reference's dataset for admixture</li>\n<li>\n<a href=\"\
    http://www.hagsc.org/hgdp/data/hgdp.zip\" rel=\"nofollow\">HGDP Dataset</a> :\
    \ for the dataset's test, you have to use the toMap.py &amp; toPed.py in the 'converstion'\
    \ directory to convert files in the .map/.ped plink format. Next you have to convert\
    \ this last output in the .bed/.bam/.fam plink format by using plink line command\
    \ and run the imputation's pipeline.</li>\n<li>Perl tool : <a href=\"https://www.well.ox.ac.uk/~wrayner/tools/\"\
    \ rel=\"nofollow\">HRC-1000G-check-bim-NoReadKey.pl</a> &amp; <a href=\"https://www.well.ox.ac.uk/~wrayner/tools/1000GP_Phase3_combined.legend.gz\"\
    \ rel=\"nofollow\">1000GP_Phase3_combined.legend</a>\n</li>\n<li>LiftOver tool\
    \ : <a href=\"http://hgdownload.cse.ucsc.edu/goldenpath/hg19/liftOver/hg19ToHg38.over.chain.gz\"\
    \ rel=\"nofollow\">hg19ToHg38.over.chain</a> &amp; <a href=\"http://hgdownload.cse.ucsc.edu/goldenpath/hg18/liftOver/hg18ToHg38.over.chain.gz\"\
    \ rel=\"nofollow\">hg18ToHg38.over.chain</a>\n</li>\n<li>Peparation dataset tool\
    \ : <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2432498/bin/pone.0002551.s003.xls\"\
    \ rel=\"nofollow\">pone.0002551.s003.xls</a> (Convert it in .csv format)</li>\n\
    <li>Admixture tool : relationships_w_pops_121708.txt</li>\n<li>\n<a href=\"https://github.com/zhanxw/checkVCF/raw/master/checkVCF.py\"\
    >CheckVCF</a>, <a href=\"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz\"\
    \ rel=\"nofollow\">Fasta file in V37</a> &amp; <a href=\"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/\"\
    \ rel=\"nofollow\">Fasta file in V38</a>\n</li>\n<li>\n<a href=\"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/\"\
    \ rel=\"nofollow\">1000G Reference in Hg38</a> with the <a href=\"https://data.broadinstitute.org/alkesgroup/Eagle/#x1-320005.3.2\"\
    \ rel=\"nofollow\">doc</a>\n</li>\n<li>Create <a href=\"https://imputationserver.readthedocs.io/en/latest/create-reference-panels/#create-legend-files\"\
    \ rel=\"nofollow\">legend</a>, <a href=\"https://data.broadinstitute.org/alkesgroup/Eagle/#x1-320005.3.2\"\
    \ rel=\"nofollow\">bcf</a> &amp; <a href=\"https://imputationserver.readthedocs.io/en/latest/create-reference-panels/#create-m3vcf-files\"\
    \ rel=\"nofollow\">m3vcf</a> files for the reference</li>\n</ul>\n<ol start=\"\
    4\">\n<li>Other to know :</li>\n</ol>\n<ul>\n<li>See the Usage part to create\
    \ the environment to run the pipeline. All the necessary dependencies are download\
    \ with the using of the script Preparation.nf. To run it, you'll need to install\
    \ the next software : in2csv(1.0.5), liftOver, plink, Minimac3(2.0.1) &amp; bcftools</li>\n\
    </ul>\n<p>You can avoid installing all the external software of the main scritp\
    \ by only installing Docker. See the <a href=\"https://github.com/IARCbioinfo/IARC-nf\"\
    >IARC-nf</a> repository for more information.</p>\n<h2>\n<a id=\"user-content-input\"\
    \ class=\"anchor\" href=\"#input\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Input</h2>\n<table>\n<thead>\n<tr>\n\
    <th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Plink\
    \ datasets</td>\n<td>Corresponds to the target dataset to be analysed. Composed\
    \ by the following files : bed, bim &amp; fam</td>\n</tr>\n<tr>\n<td>Input environment</td>\n\
    <td>Path to your input directory</td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"\
    user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Parameters</h2>\n\
    <ul>\n<li>\n<h4>\n<a id=\"user-content-mandatory\" class=\"anchor\" href=\"#mandatory\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Mandatory</h4>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n\
    <th>Example value</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n\
    <td>--target</td>\n<td>my_target</td>\n<td>Pattern of the target dataset which\
    \ do the link with the file .bed/.bim./fam for plink</td>\n</tr>\n<tr>\n<td>--input</td>\n\
    <td>user/main_data/</td>\n<td>The path of the main directory where we can find\
    \ 2 directory : my_target/ + files/</td>\n</tr>\n<tr>\n<td>--output</td>\n<td>user/my_result/</td>\n\
    <td>The path of the main directory where you want to place your results</td>\n\
    </tr>\n</tbody>\n</table>\n<ul>\n<li>\n<h4>\n<a id=\"user-content-optional\" class=\"\
    anchor\" href=\"#optional\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Optional</h4>\n</li>\n</ul>\n<table>\n<thead>\n\
    <tr>\n<th>Name</th>\n<th>Default value</th>\n<th>Description</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td>--script</td>\n<td>my/directory/script/bin</td>\n<td>The path\
    \ of the bin script directory, to be able to run the annexe programme grom the\
    \ pipeline</td>\n</tr>\n<tr>\n<td>--geno1</td>\n<td>0.03</td>\n<td>First genotyping\
    \ call rate plink threshold, apply in the full target dataset</td>\n</tr>\n<tr>\n\
    <td>--geno2</td>\n<td>0.03</td>\n<td>Second genotyping call rate plink threshold,\
    \ apply in the target dataset divide by population</td>\n</tr>\n<tr>\n<td>--maf</td>\n\
    <td>0.01</td>\n<td>Minor allele frequencies plink threshold, apply in the full\
    \ target dataset</td>\n</tr>\n<tr>\n<td>--pihat</td>\n<td>0.185</td>\n<td>Minimum\
    \ pi_hat value use for the relatedness test, 0.185 is halfway between the expected\
    \ IBD for third- and second-degree relatives</td>\n</tr>\n<tr>\n<td>--hwe</td>\n\
    <td>1e-8</td>\n<td>Hardy-Weinberg Equilibrium plink p-value threshold</td>\n</tr>\n\
    <tr>\n<td>--legend</td>\n<td>ALL.chr_GRCh38.genotypes.20170504.legend</td>\n<td>File\
    \ to use as .legend</td>\n</tr>\n<tr>\n<td>--fasta</td>\n<td>GRCh38_full_analysis_set_plus_decoy_hla.fa</td>\n\
    <td>File to use as fasta reference</td>\n</tr>\n<tr>\n<td>--chain</td>\n<td>hg18ToHg38.over.chain</td>\n\
    <td>File to use as liftover conversion</td>\n</tr>\n<tr>\n<td>--VCFref</td>\n\
    <td>my/directory/ref/vcf/</td>\n<td>Directory to use as VCF reference</td>\n</tr>\n\
    <tr>\n<td>--BCFref</td>\n<td>my/directory/ref/bcf/</td>\n<td>Directory to use\
    \ as BCF reference</td>\n</tr>\n<tr>\n<td>--M3VCFref</td>\n<td>my/directory/ref/m3vcf/</td>\n\
    <td>Directory to use as M3VCF reference</td>\n</tr>\n<tr>\n<td>--conversion</td>\n\
    <td>hg38/hg18/hg19</td>\n<td>Option to convert data from hg18 to HG38 version\
    \ of the genome. Standard value is hg38</td>\n</tr>\n<tr>\n<td>--cloud</td>\n\
    <td>hg38/hg18/hg19</td>\n<td>Option to convert data from hg18 to HG38 version\
    \ of the genome. Standard value is hg38</td>\n</tr>\n<tr>\n<td>--token_Michighan</td>\n\
    <td>path/to/my_token.txt</td>\n<td>Option to convert data from hg18 to HG38 version\
    \ of the genome. Standard value is hg38</td>\n</tr>\n<tr>\n<td>--token_TOPMed</td>\n\
    <td>path/to/my_token.txt</td>\n<td>Option to convert data from hg18 to HG38 version\
    \ of the genome. Standard value is hg38</td>\n</tr>\n<tr>\n<td>--QC_cloud</td>\n\
    <td>my/directory/donwload_imputation_server</td>\n<td>Option to convert data from\
    \ hg18 to HG38 version of the genome. Standard value is hg38</td>\n</tr>\n</tbody>\n\
    </table>\n<ul>\n<li>\n<h4>\n<a id=\"user-content-flags\" class=\"anchor\" href=\"\
    #flags\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Flags</h4>\n</li>\n</ul>\n<p>Flags are special parameters without\
    \ value.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Description</th>\n</tr>\n\
    </thead>\n<tbody>\n<tr>\n<td>--help</td>\n<td>Display help</td>\n</tr>\n</tbody>\n\
    </table>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h2>\n<ol>\n<li>Prepare the environment to run the imputation\
    \ pipeline.</li>\n</ol>\n<pre><code>mkdir data\ncd data\nnextflow run IARCbioinfo/Imputation-nf/bin/Preparation.nf\
    \ --out /data/\n</code></pre>\n<ol start=\"2\">\n<li>Paste the bim/bed/fam plink\
    \ target files in a directory, and the directory in your \"data/\" directory.\
    \ You have to call the plink files and your directory with the same pattern, as\
    \ the following exemple : data/target/target{.bed,.bim,.fam}. So now you have\
    \ 2 directories in your \"data/\" repertory :</li>\n</ol>\n<p>_ data/my_target/\
    \ : with the plink target files (my_target.bed, my_target.bim, my_target.fam).</p>\n\
    <p>_ data/files/ : with all the dependencies.</p>\n<ol start=\"3\">\n<li>Run the\
    \ imputation pipeline.</li>\n</ol>\n<pre><code>nextflow run IARCbioinfo/Imputation.nf\
    \ --target my_target --input /data/ --output /results/ -r v1.0 -profile singularity\
    \ \n</code></pre>\n<ol start=\"4\">\n<li>If you want to run the imputation in\
    \ one of the server (Michigan and/or TOPMed Imputation), you need you write your\
    \ token acces in a file and to give it in argument. For example :</li>\n</ol>\n\
    <pre><code>nextflow run IARCbioinfo/Imputation.nf --target my_target --input /data/\
    \ --output /results/ --cloud on --token_Michighan /folder/my_token_Michighan.txt\
    \ --token_TOPMed /folder/my_token_TOPMed.txt -r v1.0 -profile singularity \n</code></pre>\n\
    <p>Once your imputation data is downloaded, you can run the end of the QC analysis\
    \ :</p>\n<pre><code>nextflow run IARCbioinfo/Imputation.nf --target my_target\
    \ --input /data/ --output /results/ --QC_cloud /downloaded_imputation_server_file/\
    \ -r v1.0 -profile singularity \n</code></pre>\n<h2>\n<a id=\"user-content-output\"\
    \ class=\"anchor\" href=\"#output\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Output</h2>\n<table>\n<thead>\n\
    <tr>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>output1</td>\n\
    <td>......</td>\n</tr>\n<tr>\n<td>output2</td>\n<td>......</td>\n</tr>\n</tbody>\n\
    </table>\n<h2>\n<a id=\"user-content-detailed-description-optional-section\" class=\"\
    anchor\" href=\"#detailed-description-optional-section\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Detailed\
    \ description (optional section)</h2>\n<p>...</p>\n<h2>\n<a id=\"user-content-directed-acyclic-graph\"\
    \ class=\"anchor\" href=\"#directed-acyclic-graph\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directed Acyclic\
    \ Graph</h2>\n<p><a href=\"http://htmlpreview.github.io/?https://github.com/IARCbioinfo/Imputation-nf/blob/master/dag.html\"\
    \ rel=\"nofollow\"><img src=\"dag.png\" alt=\"DAG\" style=\"max-width:100%;\"\
    ></a></p>\n<h2>\n<a id=\"user-content-contributions\" class=\"anchor\" href=\"\
    #contributions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Contributions</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n\
    <th>Email</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Gabriel\
    \ Aur\xE9lie</td>\n<td><a href=\"mailto:gabriela@students.iarc.fr\">gabriela@students.iarc.fr</a></td>\n\
    <td>Developer to contact for support</td>\n</tr>\n<tr>\n<td>Lipinski Boris</td>\n\
    <td>\n<a href=\"mailto:LipinskiB@students.iarc.fr\">LipinskiB@students.iarc.fr</a>\
    \ / <a href=\"mailto:boris.lipinski@etu.univ-lyon1.fr\">boris.lipinski@etu.univ-lyon1.fr</a>\n\
    </td>\n<td>Developer to contact for support</td>\n</tr>\n</tbody>\n</table>\n\
    <h2>\n<a id=\"user-content-references-optional\" class=\"anchor\" href=\"#references-optional\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>References (optional)</h2>\n<h2>\n<a id=\"user-content-faq-optional\"\
    \ class=\"anchor\" href=\"#faq-optional\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>FAQ (optional)</h2>\n<h1>\n<a\
    \ id=\"user-content-test-pipeline\" class=\"anchor\" href=\"#test-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>test-pipeline</h1>\n"
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1614573597.0
IARCbioinfo/platypus-nf:
  data_format: 2
  description: Platypus germline variant calling with nextflow
  filenames:
  - Singularity
  full_name: IARCbioinfo/platypus-nf
  latest_release: v1.0
  readme: "<h1>\n<a id=\"user-content-platypus-nf\" class=\"anchor\" href=\"#platypus-nf\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Platypus-nf</h1>\n<h2>\n<a id=\"user-content-germline-variant-calling-with-platypus\"\
    \ class=\"anchor\" href=\"#germline-variant-calling-with-platypus\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Germline\
    \ variant calling with platypus</h2>\n<p><a href=\"platypus-nf.png?raw=true\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"platypus-nf.png?raw=true\"\
    \ alt=\"Workflow representation\" title=\"Scheme of platypus germline variant\
    \ calling Workflow\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-description\"\
    \ class=\"anchor\" href=\"#description\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Description</h2>\n<p>Perform\
    \ germline variant calling with platypus, with optional use of optimized parameters\
    \ based on performance analysis on <a href=\"https://www.illumina.com/platinumgenomes.html\"\
    \ rel=\"nofollow\">Illumina Platinium Genome</a> (both whole exome/genome sequencing).</p>\n\
    <p>The platypus nextflow pipeline can also add a step to the variant calling:</p>\n\
    <ul>\n<li>compress (<code>bgzip</code>) and index (<code>tabix</code>). These\
    \ two tools are part of the <a href=\"http://www.htslib.org/doc/\" rel=\"nofollow\"\
    >samtools/htslib</a> C library, see documentation for the installation.</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Dependencies</h2>\n<ol>\n<li>\n<p>This pipeline is based on <a href=\"\
    https://www.nextflow.io\" rel=\"nofollow\">nextflow</a>. As we have several nextflow\
    \ pipelines, we have centralized the common information in the <a href=\"https://github.com/IARCbioinfo/IARC-nf\"\
    >IARC-nf</a> repository. Please read it carefully as it contains essential information\
    \ for the installation, basic usage and configuration of nextflow and our pipelines.</p>\n\
    </li>\n<li>\n<p>Platypus: see official installation <a href=\"https://github.com/andyrimmer/Platypus\"\
    >here</a>. You can avoid installing all the external software by only installing\
    \ Docker. See the <a href=\"https://github.com/IARCbioinfo/IARC-nf\">IARC-nf</a>\
    \ repository for more information.</p>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-input\"\
    \ class=\"anchor\" href=\"#input\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Input</h2>\n<table>\n<thead>\n<tr>\n\
    <th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>--input_folder</td>\n\
    <td>Folder containing BAM files</td>\n</tr>\n<tr>\n<td>--ref</td>\n<td>Path fo\
    \ fasta reference</td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-parameters\"\
    \ class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Parameters</h2>\n<ul>\n<li>\n\
    <h4>\n<a id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Optional</h4>\n\
    </li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Example value</th>\n\
    <th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>--platypus_bin</td>\n\
    <td>/usr/bin/Platypus.py</td>\n<td>path to platypus executable</td>\n</tr>\n<tr>\n\
    <td>--region</td>\n<td>chr1;chr1:0-1000; mybed.bed</td>\n<td>region to call</td>\n\
    </tr>\n<tr>\n<td>--cpu</td>\n<td>12</td>\n<td>number of cpu used by platypus</td>\n\
    </tr>\n<tr>\n<td>--mem</td>\n<td>4</td>\n<td>memory in GB used by platypus</td>\n\
    </tr>\n<tr>\n<td>--output_folder</td>\n<td>.</td>\n<td>folder to store output\
    \ vcfs</td>\n</tr>\n<tr>\n<td>--options</td>\n<td>\" --scThreshold=0.9 --qdThreshold=10\
    \ \"</td>\n<td>options to pass to platypus</td>\n</tr>\n</tbody>\n</table>\n<ul>\n\
    <li>\n<h4>\n<a id=\"user-content-flags\" class=\"anchor\" href=\"#flags\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Flags</h4>\n\
    </li>\n</ul>\n<p>Flags are special parameters without value.</p>\n<table>\n<thead>\n\
    <tr>\n<th>Name</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>--help</td>\n\
    <td>Display help</td>\n</tr>\n<tr>\n<td>--compression</td>\n<td>compress and index\
    \ the VCF file</td>\n</tr>\n<tr>\n<td>--filter</td>\n<td>output only PASS variants</td>\n\
    </tr>\n<tr>\n<td>--optimized</td>\n<td>use optimized parameters: --badReadsThreshold=0\
    \ --qdThreshold=0 --rmsmqThreshold=20 --hapScoreThreshold=10 --scThreshold=0.99</td>\n\
    </tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-download-test-data-set\"\
    \ class=\"anchor\" href=\"#download-test-data-set\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Download test\
    \ data set</h2>\n<pre><code>git clone https://github.com/iarcbioinfo/data_test\n\
    </code></pre>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h2>\n<pre><code>With Docker:\nnextflow run iarcbioinfo/platypus-nf\
    \ -profile docker --input_folder data_test/BAM/ --ref data_test/REF/17.fasta\n\
    \ \nWith Singularity\nnextflow run iarcbioinfo/platypus-nf -profile singularity\
    \ --input_folder data_test/BAM/ --ref data_test/REF/17.fasta\n\nWith Conda\nnextflow\
    \ run iarcbioinfo/platypus-nf -profile conda --input_folder data_test/BAM/ --ref\
    \ data_test/REF/17.fasta\n</code></pre>\n<h2>\n<a id=\"user-content-output\" class=\"\
    anchor\" href=\"#output\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Output</h2>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n\
    <th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>VCFs</td>\n<td>one VCF\
    \ by input BAM</td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-directed-acyclic-graph\"\
    \ class=\"anchor\" href=\"#directed-acyclic-graph\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Directed Acyclic\
    \ Graph</h2>\n<p><a href=\"http://htmlpreview.github.io/?https://github.com/IARCbioinfo/platypus-nf/blob/dev/dag.html\"\
    \ rel=\"nofollow\"><img src=\"dag.png\" alt=\"DAG\" style=\"max-width:100%;\"\
    ></a></p>\n<h2>\n<a id=\"user-content-contributions\" class=\"anchor\" href=\"\
    #contributions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Contributions</h2>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n\
    <th>Email</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Tiffany\
    \ Delhomme*</td>\n<td><a href=\"mailto:delhommet@students.iarc.fr\">delhommet@students.iarc.fr</a></td>\n\
    <td>Developer to contact for support (link to specific gitter chatroom)</td>\n\
    </tr>\n</tbody>\n</table>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1557253380.0
IARCbioinfo/quantiseq-nf:
  data_format: 2
  description: Pipeline to run software quanTIseq in parallel to quantify immune cell
    content from RNA-seq data
  filenames:
  - Singularity/Singularity.v1.1
  - Singularity/Singularity.v1.0
  full_name: IARCbioinfo/quantiseq-nf
  latest_release: v1.1
  readme: '<h1>

    <a id="user-content-quantiseq-nf" class="anchor" href="#quantiseq-nf" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>quantiseq-nf</h1>

    <h2>

    <a id="user-content-nextflow-pipeline-to-run-software-quantiseq-in-parallel-to-quantify-immune-cell-content-from-rna-seq-data"
    class="anchor" href="#nextflow-pipeline-to-run-software-quantiseq-in-parallel-to-quantify-immune-cell-content-from-rna-seq-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nextflow
    pipeline to run software quanTIseq in parallel to quantify immune cell content
    from RNA-seq data</h2>

    <p><a href="https://circleci.com/gh/IARCbioinfo/quantiseq-nf/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/096b73c4eff8e2ab2e45b7dbb39af65ff4b7a4ccda58d7199d16b3eb2f39a79f/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f7175616e74697365712d6e662f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/IARCbioinfo/quantiseq-nf/tree/master.svg?style=svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3065" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p><a href="quantiseq-nf.png?raw=true" target="_blank" rel="noopener noreferrer"><img
    src="quantiseq-nf.png?raw=true" alt="workflow" title="Scheme of alignment/realignment
    Workflow" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h2>

    <p>This Nextflow pipeline uses the singularity image of quanTIseq to launch the
    quanTIseq pipeline that performs immune cell quantification of 10 cell types from
    RNA-seq data. See <a href="https://icbi.i-med.ac.at/software/quantiseq/doc/" rel="nofollow">https://icbi.i-med.ac.at/software/quantiseq/doc/</a>
    for general information about quanTIseq and the companion article Finotello, et
    al. Molecular and pharmacological modulators of the tumor immune contexture revealed
    by deconvolution of RNA-seq data. Genome Med. 2019;11:34. <a href="https://doi.org/10.1186/s13073-019-0638-6"
    rel="nofollow">https://doi.org/10.1186/s13073-019-0638-6</a></p>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <ol>

    <li>Nextflow : for common installation procedures see the <a href="https://github.com/IARCbioinfo/IARC-nf">IARC-nf</a>
    repository.</li>

    <li><a href="https://singularity.lbl.gov/all-releases" rel="nofollow"><em>singularity</em></a></li>

    </ol>

    <h2>

    <a id="user-content-input" class="anchor" href="#input" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Input</h2>

    <table>

    <thead>

    <tr>

    <th>Type</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--input_folder</td>

    <td>a folder with RNA-seq fastq files</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-parameters" class="anchor" href="#parameters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Parameters</h2>

    <ul>

    <li>

    <h4>

    <a id="user-content-optional" class="anchor" href="#optional" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Optional</h4>

    </li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Default value</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--output_folder</td>

    <td>.</td>

    <td>Output folder for results</td>

    </tr>

    <tr>

    <td>--cpu</td>

    <td>1</td>

    <td>number of CPUs</td>

    </tr>

    <tr>

    <td>--mem</td>

    <td>2</td>

    <td>memory</td>

    </tr>

    <tr>

    <td>--fastq_ext</td>

    <td>fq.gz</td>

    <td>extension of fastq files</td>

    </tr>

    <tr>

    <td>--suffix1</td>

    <td>_1</td>

    <td>suffix for second element of read files pair</td>

    </tr>

    <tr>

    <td>--suffix2</td>

    <td>_2</td>

    <td>suffix for second element of read files pair</td>

    </tr>

    <tr>

    <td>--image</td>

    <td>null</td>

    <td>singularity image</td>

    </tr>

    </tbody>

    </table>

    <p>Note that if no singularity image is provided, the image is pulled from singularity
    hub. If the pipeline is reused frequently, it might be more efficient to pull
    the image manually with the command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull quantiseq2.img
    IARCbioinfo/quantiseq-nf:v1.1</pre></div>

    <p>and then to provide the path to quantiseq2.img as a parameter.</p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <div class="highlight highlight-source-shell"><pre>nextflow run iarcbioinfo/quantiseq-nf
    -r v1.1 --input_folder input --output_folder output --image quantiseq2.img</pre></div>

    <h2>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h2>

    <table>

    <thead>

    <tr>

    <th>Type</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>quanTIseq_cell_fractions_matrix.txt</td>

    <td>a matrix with cell fractions (columns) for each sample (row)</td>

    </tr>

    <tr>

    <td>quanTIseq_gene_tpm_matrix.txt</td>

    <td>a matrix with gene counts (rows) for each sample (column)</td>

    </tr>

    <tr>

    <td>intermediate_results/quantiseqResults_sample</td>

    <td>a folder with quanTIseq results</td>

    </tr>

    </tbody>

    </table>

    <p>For each sample, a folder is created in folder intermediate_results with the
    two quanTIseq output files (see <a href="https://icbi.i-med.ac.at/software/quantiseq/doc/"
    rel="nofollow">https://icbi.i-med.ac.at/software/quantiseq/doc/</a> for details):</p>

    <ul>

    <li>quanTIseq_gene_tpm_sample.txt, the expression quantification table (in Transcripts
    Per Million or TPM) with a row for each of the 19424 annotated genes</li>

    <li>quanTIseq_cell_fractions_sample.txt, the table with the proportion of cells
    from each cell type (columns)</li>

    </ul>

    <h2>

    <a id="user-content-directed-acyclic-graph" class="anchor" href="#directed-acyclic-graph"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Directed
    Acyclic Graph</h2>

    <h3>

    <a id="user-content-with-default-options" class="anchor" href="#with-default-options"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>With
    default options</h3>

    <p><a href="http://htmlpreview.github.io/?https://github.com/IARCbioinfo/quantiseq-nf/blob/master/dag.html"
    rel="nofollow"><img src="dag.png" alt="DAG" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-contributions" class="anchor" href="#contributions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributions</h2>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Email</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Tiffany Delhomme</td>

    <td><a href="mailto:Delhommet@student.iarc.fr">Delhommet@student.iarc.fr</a></td>

    <td>Developer</td>

    </tr>

    <tr>

    <td>Nicolas Alcala*</td>

    <td><a href="mailto:AlcalaN@iarc.fr">AlcalaN@iarc.fr</a></td>

    <td>Developer to contact for support</td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1596489786.0
IARCbioinfo/svaba-nf:
  data_format: 2
  description: null
  filenames:
  - Singularity/Singularity.v1.0
  full_name: IARCbioinfo/svaba-nf
  latest_release: v1.0
  readme: '<h1>

    <a id="user-content-svaba-nf" class="anchor" href="#svaba-nf" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>svaba-nf</h1>

    <h2>

    <a id="user-content-structural-variant-calling" class="anchor" href="#structural-variant-calling"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Structural
    variant calling</h2>

    <p><a href="https://circleci.com/gh/IARCbioinfo/svaba-nf" rel="nofollow"><img
    src="https://camo.githubusercontent.com/f5e9b3e470d2c780fc3cc37f9f44d338736561a859f1c8677933503b83c46108/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f73766162612d6e662e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/IARCbioinfo/svaba-nf.svg?style=svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/iarcbioinfo/svaba-nf/" rel="nofollow"><img src="https://camo.githubusercontent.com/5c5d5383ee3d6248c7d31b5fcaa21fc7d689b53ecb330dbfe628cd1fae38c853/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d72656164792d626c75652e737667"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/badge/docker-ready-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/4720" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p><a href="https://github.com/IARCbioinfo/svaba-nf/blob/master/svaba.png" target="_blank"
    rel="noopener noreferrer"><img src="https://github.com/IARCbioinfo/svaba-nf/raw/master/svaba.png"
    alt="Image SvABA" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h2>

    <p>Perform structural variant calling with SvABA.</p>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <ol>

    <li>This pipeline is based on <a href="https://www.nextflow.io" rel="nofollow">nextflow</a>.
    As we have several nextflow pipelines, we have centralized the common information
    in the <a href="https://github.com/IARCbioinfo/IARC-nf">IARC-nf</a> repository.
    Please read it carefully as it contains essential information for the installation,
    basic usage and configuration of nextflow and our pipelines.</li>

    <li>SvABA: see official installation <a href="https://github.com/walaj/svaba">here</a>.</li>

    </ol>

    <h2>

    <a id="user-content-input" class="anchor" href="#input" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Input</h2>

    <table>

    <thead>

    <tr>

    <th>Type</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--input_folder</td>

    <td>Folder containing normal (.normal.bam) and tumor (.tumor.bam) BAM files</td>

    </tr>

    <tr>

    <td>--correspondance</td>

    <td>A correspondance file, with columns <code>ID</code>, <code>tumor</code>, and
    <code>normal</code> specifying the name of the sample and the tumor/normal file
    names in the input folder</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-parameters" class="anchor" href="#parameters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Parameters</h2>

    <ul>

    <li>

    <h4>

    <a id="user-content-mandatory" class="anchor" href="#mandatory" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Mandatory</h4>

    </li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Example value</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--ref</td>

    <td>ref.fa</td>

    <td>Path to reference fasta file. It should be indexed</td>

    </tr>

    </tbody>

    </table>

    <ul>

    <li>

    <h4>

    <a id="user-content-optional" class="anchor" href="#optional" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Optional</h4>

    </li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Default value</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--output_folder</td>

    <td>"."</td>

    <td>Path to output folder</td>

    </tr>

    <tr>

    <td>--dbsnp_file</td>

    <td>dbsnp_indel.vcf</td>

    <td>DbSNP file, e.g. available <a href="https://data.broadinstitute.org/snowman/dbsnp_indel.vcf"
    rel="nofollow">here</a>

    </td>

    </tr>

    <tr>

    <td>--cpu</td>

    <td>1</td>

    <td>Number of cpu to use</td>

    </tr>

    <tr>

    <td>--mem</td>

    <td>4</td>

    <td>Size of memory used in GB</td>

    </tr>

    <tr>

    <td>--targets</td>

    <td>NULL</td>

    <td>bed file with target positions</td>

    </tr>

    <tr>

    <td>--options</td>

    <td>NULL</td>

    <td>List of options to pass to svaba</td>

    </tr>

    </tbody>

    </table>

    <table>

    <thead>

    <tr>

    <th><strong>Name</strong></th>

    <th><strong>Description</strong></th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>--help</td>

    <td>Display help</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-download-test-data-set" class="anchor" href="#download-test-data-set"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download
    test data set</h2>

    <p><code>git clone https://github.com/iarcbioinfo/data_test</code></p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p><code>nextflow run IARCbioinfo/svaba-nf -r v1.0 -profile singularity--input_folder  path/to/input/
    --svaba path/to/svaba/ --ref_file path/to/ref/ --dbsnp_file path/to/dbsnp_indel.vcf
    --output_folder /path/to/output</code></p>

    <p>To run the pipeline without singularity just remove "-profile singularity".
    Alternatively, one can run the pipeline using a docker container (-profile docker).</p>

    <h3>

    <a id="user-content-tumor-only-mode" class="anchor" href="#tumor-only-mode" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Tumor-only mode</h3>

    <p>To trigger the Tumor-only mode in some samples, put "None" (with capital N)
    in the normal column of the corresponding sample.</p>

    <h2>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h2>

    <table>

    <thead>

    <tr>

    <th><strong>Name</strong></th>

    <th><strong>Description</strong></th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>txts (.bps.txt.gz)</td>

    <td>Raw, unfiltered variants</td>

    </tr>

    <tr>

    <td>BAMs (.contigs.bam)</td>

    <td>Unsorted assembly contigs as aligned to the reference with BWA-MEM</td>

    </tr>

    <tr>

    <td>Logs (.log)</td>

    <td>Run-time information</td>

    </tr>

    <tr>

    <td>txts (.discordants.txt.gz)</td>

    <td>Discordant reads identified with 2+ reads</td>

    </tr>

    <tr>

    <td>VCFs (.vcf )</td>

    <td>VCF of rearrangements and indels</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-directed-acyclic-graph" class="anchor" href="#directed-acyclic-graph"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Directed
    Acyclic Graph</h2>

    <p><a href="http://htmlpreview.github.io/?https://github.com/IARCbioinfo/svaba-nf/blob/master/dag.html"
    rel="nofollow"><img src="dag.png" alt="DAG" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-contributions" class="anchor" href="#contributions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributions</h2>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Email</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Nicolas Alcala*</td>

    <td><a href="mailto:AlcalaN@iarc.fr">AlcalaN@iarc.fr</a></td>

    <td>Developer to contact for support</td>

    </tr>

    <tr>

    <td>Tiffany Delhomme</td>

    <td><a href="mailto:DelhommeT@students.iarc.fr">DelhommeT@students.iarc.fr</a></td>

    <td>Developer</td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 0
  subscribers_count: 11
  topics: []
  updated_at: 1598988848.0
IARCbioinfo/vcf_normalization-nf:
  data_format: 2
  description: VCF normalization
  filenames:
  - Singularity/Singularity.v1.1
  - Singularity/Singularity.v1.0
  full_name: IARCbioinfo/vcf_normalization-nf
  latest_release: v1.1
  readme: '<h1>

    <a id="user-content-vcf_normalization-nf" class="anchor" href="#vcf_normalization-nf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>vcf_normalization-nf</h1>

    <h2>

    <a id="user-content-nextflow-pipeline-for-vcf-normalization" class="anchor" href="#nextflow-pipeline-for-vcf-normalization"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nextflow
    pipeline for vcf normalization</h2>

    <p><a href="https://circleci.com/gh/IARCbioinfo/vcf_normalization-nf/tree/master"
    rel="nofollow"><img src="https://camo.githubusercontent.com/972f6bfd365386090c6b3e3cd6e549a88d55259c394799f2be26101fa1495f52/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f7663665f6e6f726d616c697a6174696f6e2d6e662f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/IARCbioinfo/vcf_normalization-nf/tree/master.svg?style=svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/iarcbioinfo/vcf_normalization-nf/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/5c5d5383ee3d6248c7d31b5fcaa21fc7d689b53ecb330dbfe628cd1fae38c853/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d72656164792d626c75652e737667"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/badge/docker-ready-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/4381" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p><a href="vcf_normalization-nf.png" target="_blank" rel="noopener noreferrer"><img
    src="vcf_normalization-nf.png" alt="Workflow representation" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h2>

    <p>Apply <a href="http://samtools.github.io/bcftools/bcftools.html" rel="nofollow">bcftools
    norm</a> to decompose and normalize variants from a set of VCF (compressed with
    gzip/bgzip).</p>

    <p>This scripts takes a set of a folder containing <a href="https://samtools.github.io/hts-specs/VCFv4.2.pdf"
    rel="nofollow">compressed VCF files</a> (<code>*.vcf.gz</code>) as an input.

    It consists at four piped steps:</p>

    <ul>

    <li>(optional) filtering of variants (<code>bcftoolvs view -f</code>)</li>

    <li>split multiallelic sites into biallelic records (<code>bcftools norm -m -</code>)
    and left-alignment and normalization (<code>-f ref</code>)</li>

    <li>sorting (<code>bcftools sort </code>)</li>

    <li>duplicate removal (<code>bcftools norm -d exact</code>) and compression (<code>-Oz</code>)</li>

    </ul>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <ol>

    <li>

    <p>This pipeline is based on <a href="https://www.nextflow.io" rel="nofollow">nextflow</a>.
    As we have several nextflow pipelines, we have centralized the common information
    in the <a href="https://github.com/IARCbioinfo/IARC-nf">IARC-nf</a> repository.
    Please read it carefully as it contains essential information for the installation,
    basic usage and configuration of nextflow and our pipelines.</p>

    </li>

    <li>

    <p>External software:</p>

    </li>

    </ol>

    <ul>

    <li><a href="http://samtools.github.io/bcftools/bcftools.html" rel="nofollow">bcftools</a></li>

    </ul>

    <p><strong>Caution</strong>: <code>bcftools</code> has to be in your $PATH. Try
    each of the commands <code>bcftools</code> and <code>bgzip</code>, if it returns
    the options this is ok.</p>

    <h2>

    <a id="user-content-input" class="anchor" href="#input" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Input</h2>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><code>--vcf_folder</code></td>

    <td>Folder containing tumor zipped VCF files</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-parameters" class="anchor" href="#parameters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Parameters</h2>

    <ul>

    <li>

    <h4>

    <a id="user-content-mandatory" class="anchor" href="#mandatory" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Mandatory</h4>

    </li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Example value</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><code>--ref</code></td>

    <td><code>/path/to/ref.fasta</code></td>

    <td>Reference fasta file indexed</td>

    </tr>

    </tbody>

    </table>

    <ul>

    <li>

    <h4>

    <a id="user-content-optional" class="anchor" href="#optional" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Optional</h4>

    </li>

    </ul>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Default value</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><code>--output_folder</code></td>

    <td><code>normalized_VCF/</code></td>

    <td>Folder to output resulting compressed vcf</td>

    </tr>

    <tr>

    <td><code>--filter_opt</code></td>

    <td><code>-f PASS</code></td>

    <td>Options for bcftools view</td>

    </tr>

    <tr>

    <td><code>--cpu</code></td>

    <td>2</td>

    <td>Number of cpus to use</td>

    </tr>

    <tr>

    <td><code>--mem</code></td>

    <td>8</td>

    <td>Size of memory used for mapping (in GB)</td>

    </tr>

    </tbody>

    </table>

    <p>Note that the default is to filter variants with the PASS flag. To deactivate,
    use <code>--filter_opt " "</code>.</p>

    <ul>

    <li>

    <h4>

    <a id="user-content-flags" class="anchor" href="#flags" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Flags</h4>

    </li>

    </ul>

    <p>Flags are special parameters without value.</p>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><code>--help</code></td>

    <td>Display help</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>Simple use case example:</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run iarcbioinfo/vcf_normalization-nf
    -r v1.1 -profile singularity --vcf_folder VCF/ --ref ref.fasta</pre></div>

    <p>To run the pipeline without singularity just remove "-profile singularity".
    Alternatively, one can run the pipeline using a docker container (-profile docker)
    the conda receipe containing all required dependencies (-profile conda).</p>

    <h2>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h2>

    <table>

    <thead>

    <tr>

    <th>Type</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>VCF.gz, VCF.gz.tbi</td>

    <td>Compressed normalized VCF files with indexes</td>

    </tr>

    </tbody>

    </table>

    <h2>

    <a id="user-content-contributions" class="anchor" href="#contributions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributions</h2>

    <table>

    <thead>

    <tr>

    <th>Name</th>

    <th>Email</th>

    <th>Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td>Nicolas Alcala*</td>

    <td><a href="mailto:alcalan@iarc.fr">alcalan@iarc.fr</a></td>

    <td>Developer to contact for support</td>

    </tr>

    <tr>

    <td>Tiffany Delhomme</td>

    <td><a href="mailto:delhommet@students.iarc.fr">delhommet@students.iarc.fr</a></td>

    <td>Developer</td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1590435797.0
IMIMF-UNILJSI/spikeScreenContainer:
  data_format: 2
  description: Container used to run IMI spikeScreen
  filenames:
  - Singularity
  full_name: IMIMF-UNILJSI/spikeScreenContainer
  latest_release: null
  readme: '<h1>

    <a id="user-content-spikescreencontainer" class="anchor" href="#spikescreencontainer"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>spikeScreenContainer</h1>

    <p>Container used to run IMI spikeScreen

    This repo is meant to increase portability through automatic automatic container
    builds on shub.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1614360734.0
ISU-HPC/funannotate:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ISU-HPC/funannotate
  latest_release: null
  readme: '<h1>

    <a id="user-content-funannotate" class="anchor" href="#funannotate" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>funannotate</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607481566.0
InigoMoreno/deep_ga:
  data_format: 2
  description: Global Alignment using Deep Learning
  filenames:
  - Singularity
  full_name: InigoMoreno/deep_ga
  latest_release: null
  readme: '<h1>

    <a id="user-content-deep-ga" class="anchor" href="#deep-ga" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>deep-ga</h1>

    <p>Global Alignment using Deep Learning</p>

    <p>For the moment this is just a python package with some helper functions</p>

    <h1>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <p>This will try to install tensorflow, for this to work, you need python 3.6-3.8
    (see <a href="https://www.tensorflow.org/install" rel="nofollow">https://www.tensorflow.org/install</a>)</p>

    <pre><code>python -m pip install --upgrade pip

    python -m pip install git+https://github.com/InigoMoreno/deep_ga

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621892077.0
J35P312/PileupPipe:
  data_format: 2
  description: a pipeline performing  snp variant calling using samtools mpileup
  filenames:
  - Singularity
  full_name: J35P312/PileupPipe
  latest_release: null
  readme: '<h1>

    <a id="user-content-pileuppipe" class="anchor" href="#pileuppipe" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PileupPipe</h1>

    <p>Run SNP variant calling using gatk4 haplotypecaller. The pipeline performs
    annotation using vep and produces a vcf and excel file.</p>

    <h1>

    <a id="user-content-run-locally" class="anchor" href="#run-locally" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run Locally</h1>

    <p>Analyse a single sample, using a gene list:</p>

    <pre><code>nextflow pileup_pipeline.nf --bam &lt;input_bam&gt; --working_dir &lt;output_folder&gt;
    --genelist &lt;genelist&gt; -w $TMPDIR -c &lt;config_file_from_setup&gt;

    </code></pre>

    <p>Analyse a single sample, with no gene list:</p>

    <pre><code>nextflow pileup_pipeline.nf --bam &lt;input_bam&gt; --working_dir &lt;output_folder&gt;
    -w $TMPDIR -c &lt;config_file_from_setup&gt;

    </code></pre>

    <h1>

    <a id="user-content-run-on-slurm" class="anchor" href="#run-on-slurm" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run on slurm</h1>

    <p>The slurm scripts are wrappers around the pileup_pipeline.nf script.</p>

    <p>Analyse a single sample, using a gene list:</p>

    <pre><code>sbatch SubmitGeneList.sh &lt;input_bam&gt; &lt;output_folder&gt; &lt;genelist&gt;
    &lt;config_file_from_setup&gt;

    </code></pre>

    <p>Analyse a single sample, with no gene list:</p>

    <pre><code>sbatch SubmitNoGeneList.sh  &lt;input_bam&gt; &lt;output_folder&gt;
    &lt;config_file_from_setup&gt;

    </code></pre>

    <p>Analyse a folder containing bam files using a gene list:</p>

    <pre><code>./snpPipe.sh &lt;input_folder_with_bam&gt; &lt;output_folder&gt; &lt;genelist&gt;
    &lt;config_file_from_setup&gt;

    </code></pre>

    <h1>

    <a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install</h1>

    <p>Download and  install nextflow</p>

    <pre><code>https://www.nextflow.io/

    </code></pre>

    <p>run the setup script</p>

    <pre><code>python setup.py &gt; config.conf

    </code></pre>

    <p>Download the singularity collection (note, the singularity collection needs
    to be placed in the pipeline folder)</p>

    <p>singularity pull --name PileUp.simg shub://J35P312/PileupPipe</p>

    <p>or build it yourself:</p>

    <p>singularity build PileUp.simg Singularity</p>

    <p>open the config file using a text editor and change the path variables:</p>

    <pre><code>nano config.conf

    </code></pre>

    <p>The  reference (line 49) path is required:</p>

    <p>Additionally, you need to put a .vep folder containing the vep database insisde
    the PileupPipe folder.</p>

    <p>If you use the slurm wrappers, you need to edit the slurm account in the files,  SubmitGeneList.sh,
    and SubmitNoGeneList.sh</p>

    <h1>

    <a id="user-content-gene-list" class="anchor" href="#gene-list" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Gene list</h1>

    <p>The gene list is a text file. Each line of the text file contains the HGNC
    symbol of a gene.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1552853604.0
J35P312/fluffy:
  data_format: 2
  description: FetaL AneUploidy and FetalFraction analYsis Pipeline
  filenames:
  - Singularity
  full_name: J35P312/fluffy
  latest_release: 0.6.0
  readme: '<p><a href="https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg"
    alt="Build" style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/Clinical-Genomics/fluffy" rel="nofollow"><img src="https://camo.githubusercontent.com/5a8950551f5fd61495950779e32145a28a2346b9809af6e347b18f58dce06213/68747470733a2f2f636f6465636f762e696f2f67682f436c696e6963616c2d47656e6f6d6963732f666c756666792f6272616e63682f6d61737465722f67726170682f62616467652e737667"
    alt="codecov" data-canonical-src="https://codecov.io/gh/Clinical-Genomics/fluffy/branch/master/graph/badge.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-fluffypipe" class="anchor" href="#fluffypipe" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FluFFyPipe</h1>

    <p>NIPT analysis pipeline, using WisecondorX for detecting aneuplodies and large
    CNVs, AMYCNE for FFY and PREFACE for FF prediction (optional). FluFFYPipe produces
    a variety of output files, as well as a per batch csv summary.</p>

    <p align="center">

    <a href="https://github.com/J35P312/FluFFyPipe/blob/master/logo/IMG_20200320_132001.jpg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/J35P312/FluFFyPipe/raw/master/logo/IMG_20200320_132001.jpg"
    width="400" height="400" style="max-width:100%;"></a>

    </p>

    <h1>

    <a id="user-content-run-fluffypipe" class="anchor" href="#run-fluffypipe" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run FluFFyPipe</h1>

    <p>Run NIPT analysis, using a previously comnputed reference:</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; analyse

    </code></pre>

    <p>Run NIPT analysis, using an internally computed reference (i.e the reference
    is built using all samples listed in samplesheet):</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; analyse --batch-ref

    </code></pre>

    <p>optionally, skip preface:</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; --skip_preface analyse

    </code></pre>

    <p>All output will be written to the output folder, this output includes:</p>

    <pre><code>bam files

    wisecondorX output

    tiddit coverage summary

    Fetal fraction estimation

    </code></pre>

    <p>as well as a summary csv and multiqc html (per batch)</p>

    <p>the input folder is a project folder containing one folder per sample, each
    of these subfolders contain the fastq file(s).

    The samplesheet contains at least a "sampleID" column, the sampleID should match
    the subfolders in the input folder. The samplesheet may contain other columns,
    such as flowcell and index folder: such columns will be printed to the summary
    csv.

    If the samplesheet contains a SampleName column, fluffy will name the output according
    to SampleName</p>

    <p>Create a WisecondorX reference</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; reference

    </code></pre>

    <p>samplesheet should contain atleast a "sampleID" column. All samples in the
    samplesheet will be used to construct the reference, visit the WisecondorX manual
    for more information.</p>

    <h1>

    <a id="user-content-troubleshooting-and-rerun" class="anchor" href="#troubleshooting-and-rerun"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Troubleshooting
    and rerun</h1>

    <p>There are three statuses of the fluffy pipeline:

    running, complete, and failed</p>

    <p>The status of a fluffy run is found in the</p>

    <pre><code>&lt;output_folder&gt;/analysis_status.json

    </code></pre>

    <p>The status of all jobs are listed in</p>

    <pre><code>&lt;output_folder&gt;/sacct/fluffy_&lt;date&gt;.log.status

    </code></pre>

    <p>Where  is the timepoint when the jobs were submitted

    Use grep to find the failed jobs:</p>

    <pre><code>grep -v COMPLETE &lt;output_folder&gt;/sacct/fluffy_&lt;date&gt;.log.status

    </code></pre>

    <p>The output logs are stored in:</p>

    <pre><code> &lt;output_folder&gt;/logs

    </code></pre>

    <p>Before continuing, you may want to generate the summary csv for all completed
    cases:</p>

    <pre><code>bash &lt;output_folder&gt;/scripts/summarizebatch-&lt;hash&gt;

    </code></pre>

    <p>where  is a randomly generated string.</p>

    <p>use the rerun module to rerun failed fluffy analyses:</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; --skip_preface rerun

    </code></pre>

    <h1>

    <a id="user-content-install-fluffypipe" class="anchor" href="#install-fluffypipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    FluFFyPipe</h1>

    <p>FluFFyPipe requires python 3, slurm, slurmpy, and singularity, python-coloredlogs.</p>

    <p>fluffy may be installed using pip:</p>

    <pre><code>pip install fluffy-cg

    </code></pre>

    <p>alternatively, fluffy is cloned and installed from github:

    git clone <a href="https://github.com/Clinical-Genomics/fluffy">https://github.com/Clinical-Genomics/fluffy</a>

    cd fluffy

    pip install -e .</p>

    <p>Next download the FluFFyPipe singularity container</p>

    <pre><code> singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe

    </code></pre>

    <p>copy the example config (found in example_config), and edit the variables.

    You will need to download/create the following files:</p>

    <pre><code>Reference fasta (indexed using bwa)


    WisecondorX reference files (created using the reference mode)


    PREFACE model file (optional)


    blacklist bed file (used by wisecondorX)


    FluFFyPipe singularity collection (singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe)

    </code></pre>

    '
  stargazers_count: 2
  subscribers_count: 5
  topics: []
  updated_at: 1620326897.0
J35P312/test_singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: J35P312/test_singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-test_singularity" class="anchor" href="#test_singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>test_singularity</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611094520.0
James-S-Santangelo/singularity-recipes:
  data_format: 2
  description: Collection of singularity recipes
  filenames:
  - PCAngsd/Singularity.PCAngsd_v0.99
  - clumpak/Singularity.clumpak_v1.1
  - angsd/Singularity.angsd_v0.933
  - ngsLD/Singularity.ngsLD_v1.1.1
  - ngsRelate/Singularity.ngsRelate_v2.0
  full_name: James-S-Santangelo/singularity-recipes
  latest_release: null
  readme: '<p>This repository contains Singularity recipes for genomics tools that
    I have not found available through other means (e.g., Conda, Docker).</p>

    <p>Singularity images are available on <a href="https://cloud.sylabs.io/library/james-s-santangelo"
    rel="nofollow">Sylab''s Cloud Library</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621059542.0
JeffersonLab/hd_singularity:
  data_format: 2
  description: Files to support building and maintenance of Singularity containers
    for Hall D
  filenames:
  - recipes/Singularity.ubuntu.bionic-20210222
  - recipes/Singularity.ubuntu.focal-20200925
  - recipes/Singularity.centos-6.10
  - recipes/Singularity.centos-8.2.2004
  - recipes/Singularity.markito3.gluex_docker_devel
  - recipes/Singularity.fedora-33
  - recipes/Singularity.centos-7.7.1908
  - recipes/Singularity.ubuntu.xenial-20210114
  - recipes/Singularity.markito3-gluex_docker_prod
  - recipes/Singularity.fedora-32
  full_name: JeffersonLab/hd_singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-hd_singularity\" class=\"anchor\" href=\"#hd_singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>hd_singularity</h1>\n<p>Files to support building and maintenance\
    \ of Singularity containers for Hall D.</p>\n<p>Contains scripts and recipes for\
    \ creating Singularity containers from scratch.</p>\n<p>The main script is scripts/create_gluex_container.sh.\
    \ Its usage message is as follows:</p>\n<pre><code>Usage: create_gluex_container.sh\
    \ [-h] -r &lt;recipe-file&gt; -p &lt;prereqs-script&gt; \\\n       [-d DIRECTORY]\
    \ [-t STRING]\n\nNote: must be run as root\n\nOptions:\n  -h print this usage\
    \ message\n  -r Singularity recipe file\n  -p script that installs gluex software\n\
    \  -d output directory for containers (default: current working directory)\n \
    \ -t token to be used to name containers (default = extension in \"Singularity.ext\"\
    )\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 8
  topics: []
  updated_at: 1621622436.0
JessyD/adolescent-brain-parcellation:
  data_format: 2
  description: null
  filenames:
  - code/container/Singularity
  full_name: JessyD/adolescent-brain-parcellation
  latest_release: null
  readme: '<h1>

    <a id="user-content-adolescent-brain-parcellation" class="anchor" href="#adolescent-brain-parcellation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>adolescent-brain-parcellation</h1>

    <p>This project aims to create a brain parcellation for pre-adolescents subjects
    using the ABCD dataset. When performing a neuroscience study, it is common to
    extract regions of interest using a parcellation scheme. Although many parcellation
    schemes exist, these parcellations have been derived from adult data and might
    lead to biases when applied to a different age group. This repository contains
    code to create pre-adolescent brain parcellation following the methods described
    by <a href="https://journals.physiology.org/doi/full/10.1152/jn.00338.2011" rel="nofollow">Yeo
    et al. (2011)</a> using the ABCD dataset.</p>

    <p>The code inside ''cbig_simplified'' contains a modified version of the code
    provided by the <a href="https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/brain_parcellation/Yeo2011_fcMRI_clustering">Computational
    Brain Imaging Group (CBIG)</a>. This modified version make sure that the code
    can be run using a singularity container.</p>

    <p>The code was developed during the <a href="https://www.abcd-repronim.org/about.html"
    rel="nofollow">ABCD-ReproNim course''s project week</a>.</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics:
  - parcellations
  - neuroimaging
  updated_at: 1617076593.0
M30819-2020/setapDocker:
  data_format: 2
  description: null
  filenames:
  - Singularity.def
  full_name: M30819-2020/setapDocker
  latest_release: latest
  readme: '<h1>

    <a id="user-content-setapdocker" class="anchor" href="#setapdocker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>setapDocker</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621993189.0
MASILab/PreQual:
  data_format: 2
  description: An automated pipeline for integrated preprocessing and quality assurance
    of diffusion weighted MRI images
  filenames:
  - Singularity
  full_name: MASILab/PreQual
  latest_release: v1.0.5
  readme: "<h1>\n<a id=\"user-content-prequal-dtiqa-v7-multi-user-guide\" class=\"\
    anchor\" href=\"#prequal-dtiqa-v7-multi-user-guide\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>PreQual (dtiQA\
    \ v7 Multi) User Guide</h1>\n<h2>\n<a id=\"user-content-contents\" class=\"anchor\"\
    \ href=\"#contents\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Contents</h2>\n<ul>\n<li><a href=\"#overview\"\
    >Overview</a></li>\n<li><a href=\"#authors-and-reference\">Authors and Reference</a></li>\n\
    <li><a href=\"#getting-started\">Getting Started</a></li>\n<li><a href=\"#containerization-of-source-code\"\
    >Containerization of Source Code</a></li>\n<li><a href=\"#command\">Command</a></li>\n\
    <li><a href=\"#arguments-and-io\">Arguments and I/O</a></li>\n<li><a href=\"#configuration-file\"\
    >Configuration File</a></li>\n<li><a href=\"#examples\">Examples</a></li>\n<li><a\
    \ href=\"#running-bids-data\">Running BIDS Data</a></li>\n<li><a href=\"#options\"\
    >Options</a></li>\n<li><a href=\"#pipeline-assumptions\">Pipeline Assumptions</a></li>\n\
    <li><a href=\"#pipeline-processing-steps\">Pipeline Processing Steps</a></li>\n\
    <li><a href=\"#pipeline-quality-assurance-steps\">Pipeline Quality Assurance Steps</a></li>\n\
    <li><a href=\"#outputs\">Outputs</a></li>\n<li><a href=\"#note-on-versioning-for-vuiis-xnat-users\"\
    >Note on Versioning for VUIIS XNAT Users</a></li>\n</ul>\n<h2>\n<a id=\"user-content-overview\"\
    \ class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Overview</h2>\n<p><a href=\"\
    https://github.com/MASILab/PreQual/blob/master/overview.png?raw=true\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/MASILab/PreQual/raw/master/overview.png?raw=true\"\
    \ alt=\"Pipeline Overview\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li>\n<p><strong>Summary:</strong>\
    \ Perform integrated preprocessing and quality assurance of diffusion MRI data</p>\n\
    </li>\n<li>\n<p><strong>Preprocessing Steps:</strong></p>\n<ol>\n<li>MP-PCA denoising\
    \ (default on)</li>\n<li>Gibbs de-ringing (default off)</li>\n<li>Rician correction\
    \ (default off)</li>\n<li>Inter-scan normalization (default on)</li>\n<li>Susceptibility-induced\
    \ distortion correction, with or without reverse gradient images or field maps</li>\n\
    <li>Eddy current-induced distortion correction</li>\n<li>Inter-volume motion correction</li>\n\
    <li>Slice-wise signal dropout imputation</li>\n<li>N4 B1 bias field correction\
    \ (default off)</li>\n</ol>\n</li>\n<li>\n<p><strong>Quality Assurance Steps:</strong></p>\n\
    <ol>\n<li>Verification of phase encoding schemes</li>\n<li>Analysis of gradient\
    \ directions</li>\n<li>Shell-wise analysis of signal-to-noise and contrast-to-noise\
    \ ratios</li>\n<li>Visualization of Gibbs de-ringing changes (if applicable)</li>\n\
    <li>Visualization of within brain intensity distributions before and after Rician\
    \ correction (if applicable)</li>\n<li>Correction (if applicable) or visualization\
    \ of inter-scan intensity relationships</li>\n<li>Shell-wise analysis of distortion\
    \ corrections</li>\n<li>Analysis of inter-volume motion and slice-wise signal\
    \ dropout</li>\n<li>Analysis of B1 bias fields (if applicable)</li>\n<li>Verification\
    \ of intra-pipeline masking</li>\n<li>Analysis of tensor goodness-of-fit</li>\n\
    <li>Voxel-wise and region-wise quantification of FA</li>\n<li>Voxel-wise quantification\
    \ of MD</li>\n</ol>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-authors-and-reference\"\
    \ class=\"anchor\" href=\"#authors-and-reference\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Authors and Reference</h2>\n\
    <p><a href=\"mailto:leon.y.cai@vanderbilt.edu\">Leon Y. Cai</a>, Qi Yang, Colin\
    \ B. Hansen, Vishwesh Nath, Karthik Ramadass, Graham W. Johnson, Benjamin N. Conrad,\
    \ Brian D. Boyd, John P. Begnoche, Lori L. Beason-Held, Andrea T. Shafer, Susan\
    \ M. Resnick, Warren D. Taylor, Gavin R. Price, Victoria L. Morgan, Baxter P.\
    \ Rogers, Kurt G. Schilling, Bennett A. Landman. <em>PreQual: An automated pipeline\
    \ for integrated preprocessing and quality assurance of diffusion weighted MRI\
    \ images</em>. <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.28678\"\
    \ rel=\"nofollow\">Magnetic Resonance in Medicine</a>, 2021.</p>\n<p><a href=\"\
    https://my.vanderbilt.edu/masi\" rel=\"nofollow\">Medical-image Analysis and Statistical\
    \ Interpretation (MASI) Lab</a>, Vanderbilt University, Nashville, TN, USA</p>\n\
    <h2>\n<a id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Getting Started</h2>\n<p>The PreQual software is designed to run inside\
    \ a <a href=\"#containerization-of-source-code\">Singularity container</a>. The\
    \ container requires an \"<a href=\"#arguments-and-io\">inputs</a>\" folder that\
    \ holds all required input diffusion image files (i.e., .nii.gz, .bval, and .bvec\
    \ files) and a <a href=\"#configuration-file\">configuration file</a>. For those\
    \ running Synb0-DisCo to correct susceptibility distortions without reverse phase-encoded\
    \ images, this folder will also contain the <a href=\"#arguments-and-io\">structural\
    \ T1 image</a>. The container also requires an \"<a href=\"#arguments-and-io\"\
    >outputs</a>\" folder that will hold all the outputs after the pipeline runs.\
    \ We also need to know the image <em><a href=\"#arguments-and-io\">axis</a></em>\
    \ on which phase encoding was performed for all inputs (i.e., \"i\" for the first\
    \ dimension, \"j\" for the second). To build the configuration file, we need to\
    \ know the <em><a href=\"#configuration-file\">direction</a></em> along said axis\
    \ in which each image was phase encoded (i.e., \"+\" for positive direction and\
    \ \"-\" for the negative direction) and the <a href=\"#configuration-file\">readout\
    \ time</a> for each input image. Once we have this information, we bind the inputs\
    \ and outputs directories into the container to <a href=\"#command\">run the pipeline</a>.</p>\n\
    <p>Note: The phase encoding axis, direction, and readout time must be known ahead\
    \ of time, as this information is not stored in NIFTI headers. Depending on the\
    \ scanner used, they may be available in JSON sidecars when NIFTIs are converted\
    \ from DICOMs with <a href=\"#pipeline-assumptions\">dcm2niix</a>.</p>\n<h2>\n\
    <a id=\"user-content-containerization-of-source-code\" class=\"anchor\" href=\"\
    #containerization-of-source-code\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Containerization of Source Code</h2>\n\
    <pre><code>git clone https://github.com/MASILab/PreQual.git\ncd /path/to/repo/PreQual\n\
    git checkout v1.0.5\nsudo singularity build /path/to/prequal.simg Singularity\n\
    </code></pre>\n<p>We use Singularity version 3.4 with root permissions.</p>\n\
    <h2>\n<a id=\"user-content-command\" class=\"anchor\" href=\"#command\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Command</h2>\n\
    <pre><code>singularity run \n-e \n--contain\n-B /path/to/inputs/directory/:/INPUTS\n\
    -B /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n\
    --nv\n/path/to/prequal.simg\npe_axis\n[options]\n</code></pre>\n<ul>\n<li>Binding\
    \ the freesurfer license is optional and only needed for Synb0-DisCo</li>\n<li>Binding\
    \ the tmp directory is necessary when running the image with <code>--contain</code>.</li>\n\
    <li>\n<code>--nv</code> is optional. See options <code>--eddy_cuda</code> and\
    \ <code>--eddy_extra_args</code>. <strong>GPU support is currently experimental.</strong>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-arguments-and-io\" class=\"anchor\" href=\"\
    #arguments-and-io\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Arguments and I/O</h2>\n<ul>\n<li>\n<p><strong>Input\
    \ Directory:</strong> The dtiQA_config.csv configuration file and at least one\
    \ diffusion weighted image must be provided.</p>\n<ul>\n<li>\n<p>dtiQA_config.csv\
    \ (see <a href=\"#configuration-file\">below</a> for format, must be named exactly)</p>\n\
    </li>\n<li>\n<p>&lt;image1&gt;.nii.gz (diffusion weighted image)</p>\n</li>\n\
    <li>\n<p>&lt;image1&gt;.bval (units of s/mm<sup>2</sup>, in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n</li>\n<li>\n<p>&lt;image1&gt;.bvec (normalized\
    \ unit vectors in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n<p>:</p>\n</li>\n<li>\n<p>&lt;imageN&gt;.nii.gz\
    \ (diffusion weighted image)</p>\n</li>\n<li>\n<p>&lt;imageN&gt;.bval (units of\
    \ s/mm<sup>2</sup>, in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n</li>\n<li>\n<p>&lt;imageN&gt;.bvec (normalized\
    \ unit vectors in the <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\"\
    \ rel=\"nofollow\">FSL format</a>)</p>\n</li>\n<li>\n<p>t1.nii.gz (Optional, used\
    \ for Synb0-DisCo, must be named exactly)</p>\n</li>\n<li>\n<p>Other files as\
    \ needed (see <code>--extra_eddy_args</code> for more information)</p>\n</li>\n\
    </ul>\n</li>\n<li>\n<p><strong>Output Directory:</strong> Full outputs listed\
    \ at the <a href=\"#outputs\">end</a> of this document</p>\n<ul>\n<li>\n<p>The\
    \ output preprocessed images are available in the PREPROCESSED subfolder in the\
    \ output directory:</p>\n<ul>\n<li>PREPROCESSED/dwmri.nii.gz</li>\n<li>PREPROCESSED/dwmri.bval</li>\n\
    <li>PREPROCESSED/dwmri.bvec</li>\n</ul>\n</li>\n<li>\n<p>The QA document is available\
    \ in the PDF subfolder in the output directory:</p>\n<ul>\n<li>PDF/dtiQA.pdf</li>\n\
    </ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>pe_axis:</strong> Phase encoding\
    \ axis of all the input images. We do NOT support different phase encoding axes\
    \ between different input images at this time. The options are i and j and correspond\
    \ to the first and second dimension of the input images, respectively. Note that\
    \ FSL does not currently support phase encoding in the third dimension (i.e. k,\
    \ the dimension in which the image slices were acquired, commonly axial for RAS\
    \ and LAS oriented images). <strong>This parameter is direction AGNOSTIC</strong>.\
    \ The phase encoding directions of the input images along this axis are specified\
    \ in the dtiQA_config.csv file. See <a href=\"#configuration-file\">Configuration\
    \ File</a> and <a href=\"#examples\">Examples</a> for more information.</p>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-configuration-file\" class=\"anchor\"\
    \ href=\"#configuration-file\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Configuration File</h2>\n<p>The format\
    \ for the lines of the configuration CSV file, dtiQA_config.csv (must be named\
    \ exactly), are as follows:</p>\n<pre><code>&lt;image1&gt;,pe_dir,readout_time\n\
    :\n&lt;imageN&gt;,pe_dir,readout_time\n</code></pre>\n<ul>\n<li>\n<p><strong>&lt;image&gt;</strong>\
    \ is the shared file PREFIX between the corresponding NIFTI, BVAL, and BVEC files\
    \ for that particular image in the input directory (i.e., my_dwi.nii.gz/.bval/.bvec\
    \ -&gt; my_dwi). Do NOT include the path to the input directory.</p>\n</li>\n\
    <li>\n<p><strong>pe_dir</strong> is either + or -, corresponding to the direction\
    \ along the phase encoding axis (as defined by the parameter <code>pe_axis</code>)\
    \ on which the image is phase encoded.</p>\n<ul>\n<li>Note that a combination\
    \ of phase encoding axis and direction map to specific anatomical (i.e. APA, APP,\
    \ etc.) directions based on the orientation of the image. So, for instance in\
    \ a RAS image, an axis of j and direction of + map to APP. We infer the orientation\
    \ of the image from the header of the NIFTI using nibabel tools and output the\
    \ best anatomical phase encoding direction interpretation of the input direction\
    \ in the PDF for QA.</li>\n</ul>\n</li>\n<li>\n<p><strong>readout_time</strong>\
    \ is a non-negative number, the readout_time parameter required by FSL\u2019s\
    \ eddy. The absolute value of this parameter is used to scale the estimated b0\
    \ field. Note a value of 0 indicates that the images are infinite bandwidth (i.e.\
    \ no susceptibility distortion). See <a href=\"#examples\">Examples</a> for more\
    \ information.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-examples\" class=\"\
    anchor\" href=\"#examples\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Examples</h2>\n<p>Here are some different example\
    \ combinations of pe_axis, pe_dir, and readout_time parameters and the corresponding\
    \ FSL acquisition parameters lines:</p>\n<table>\n<thead>\n<tr>\n<th>pe_axis</th>\n\
    <th>pe_dir</th>\n<th>readout_time</th>\n<th>acqparams line</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td>i</td>\n<td>+</td>\n<td>0.05</td>\n<td>1, 0, 0, 0.05</td>\n\
    </tr>\n<tr>\n<td>j</td>\n<td>-</td>\n<td>0.1</td>\n<td>0, -1, 0, 0.1</td>\n</tr>\n\
    </tbody>\n</table>\n<p>These are examples of common use cases. They also all share\
    \ the same command, as detailed above. The PREPROCESSED output folder will contain\
    \ the final outputs and the PDF folder will contain the QA report.</p>\n<table>\n\
    <thead>\n<tr>\n<th>Phase Encoding<br>Axis</th>\n<th>Reverse Phase<br>Encoded (RPE)\
    \ Image</th>\n<th>T1<br>Image</th>\n<th>Contents of<br>Input Directory</th>\n\
    <th>Contents of<br>dtiQA_config.csv</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>j</td>\n\
    <td>Yes</td>\n<td>N/A</td>\n<td>dti1.nii.gz<br>dti1.bval<br>dti1.bvec<br>dti2.nii.gz<br>dti2.bval<br>dti2.bvec<br>rpe.nii.gz<br>rpe.bval<br>rpe.bvec<br>dtiQA_config.csv</td>\n\
    <td>dti1,+,0.05<br>dti2,+,0.05<br>rpe,-,0.05</td>\n</tr>\n<tr>\n<td>j</td>\n<td>No</td>\n\
    <td>Yes</td>\n<td>dti1.nii.gz<br>dti1.bval<br>dti1.bvec<br>dti2.nii.gz<br>dti2.bval<br>dti2.bvec<br>t1.nii.gz<br>dtiQA_config.csv</td>\n\
    <td>dti1,+,0.05<br>dti2,+,0.05</td>\n</tr>\n<tr>\n<td>j</td>\n<td>No</td>\n<td>No</td>\n\
    <td>dti1.nii.gz<br>dti1.bval<br>dti1.bvec<br>dti2.nii.gz<br>dti2.bval<br>dti2.bvec<br>dtiQA_config.csv</td>\n\
    <td>dti1,+,0.05<br>dti2,+,0.05</td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"\
    user-content-running-bids-data\" class=\"anchor\" href=\"#running-bids-data\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running BIDS Data</h2>\n<p>While not a BIDS pipeline, data in BIDS\
    \ format can be run with PreQual without moving or copying data. The key is that\
    \ the input directory structure must be as described relative to <em>inside the\
    \ container</em>. By creatively binding files/folders into the container, we can\
    \ achieve the same effect:</p>\n<pre><code>-B /path/to/sub-X/ses-X/dwi/:/INPUTS\n\
    -B /path/to/sub-X/ses-X/anat/sub-X_ses-X_T1w.nii.gz:/INPUTS/t1.nii.gz (optional,\
    \ Synb0-DisCo only)\n-B /path/to/config/file.csv:/INPUTS/dtiQA_config.csv\n-B\
    \ /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n\
    </code></pre>\n<p>The outputs directory and configuration file can be created\
    \ wherever makes the most sense for the user. The contents of the configuration\
    \ file will look something like this:</p>\n<pre><code>sub-X_ses-X_acq-1_dwi,pe_dir,readout_time\n\
    :\nsub-X_ses-X_acq-N_dwi,pe_dir,readout_time\n</code></pre>\n<h2>\n<a id=\"user-content-options\"\
    \ class=\"anchor\" href=\"#options\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Options</h2>\n<p><strong>--bval_threshold\
    \ N</strong></p>\n<p>A non-negative integer threshold under which to consider\
    \ a b-value to be zero. Useful when some MRI machines do not allow for more than\
    \ one b0 volume to be acquired so some users acquire scans with extremely low\
    \ b-values to be treated like b0 volumes. Setting this value to 0 results in no\
    \ thresholding. Units = s/mm<sup>2</sup>.</p>\n<p>Default = 50</p>\n<p><strong>--nonzero_shells\
    \ s1,s2,...,sn/auto</strong></p>\n<p>A comma separated list of positive integers\
    \ (s/mm<sup>2</sup>) indicating nonzero shells for SNR/CNR analysis when there\
    \ are more unique b-values than shells determined by eddy or automatically determine\
    \ shells by rounding to nearest 100. Useful when b-values are modulated around\
    \ a shell value instead of set exactly at that value. Only used when determining\
    \ shells for SNR/CNR analysis. Original b-values used elsewhere in pipeline.</p>\n\
    <p>Default = auto</p>\n<p><strong>--denoise on/off</strong></p>\n<p>Denoise images\
    \ prior to preprocessing using Marchenko-Pastur PCA <a href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwidenoise.html\"\
    \ rel=\"nofollow\">implemented in MRTrix3</a>. The SNR of the b0s of the final\
    \ preprocessed images are reported in the PDF output regardless of whether this\
    \ option is on or off.</p>\n<p>Default = on</p>\n<p><strong>--degibbs on/off</strong></p>\n\
    <p>Remove Gibbs ringing artifacts using the local subvoxel-shifts method as <a\
    \ href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/mrdegibbs.html\"\
    \ rel=\"nofollow\">implemented in MRTrix3</a>. We caution against using this feature\
    \ as it not designed for the partial Fourier schemes with which most echo planar\
    \ diffusion images are acquired. It is also difficult to quality check, but we\
    \ include a visualization of averaged residuals across all b = 0 s/mm<sup>2</sup>\
    \ volumes, looking for larger signals near high contrast (i.e. parenchyma-CSF)\
    \ interfaces.</p>\n<p>Default = off</p>\n<p><strong>--rician on/off</strong></p>\n\
    <p>Perform Rician correction using the method of moments. We normally do not perform\
    \ this step as we empirically do not find it to affect results drastically. It\
    \ is also difficult to quality check, but we include a plot of the shell-wise\
    \ within brain intensity distributions for each input before and after correction,\
    \ looking for a slight drop in intensity with correction.</p>\n<p>Default = off</p>\n\
    <p><strong>--prenormalize on/off</strong></p>\n<p>Intensity normalize images prior\
    \ to preprocessing by maximizing the intra-mask intensity-histogram intersections\
    \ between the averaged b0s of the scans. If this option is on, these histograms\
    \ before and after prenormalization will be reported in the output PDF. This is\
    \ done to avoid gain differences between different diffusion scans. If this option\
    \ is off, we assume that the various input images all have the same gain. That\
    \ being said, we still estimate and report the gain factors and intensity histograms\
    \ in a gain QA page and report warnings if estimated gains greater than 5% are\
    \ found.</p>\n<p>Default = on</p>\n<p><strong>--synb0 on/off</strong></p>\n<p>Run\
    \ <code>topup</code> with a synthetic b0 generated with the Synb0-DisCo deep-learning\
    \ framework if no reverse phase encoded images are supplied and a T1 image is\
    \ supplied. Synb0-DisCo requires at least 24GB of RAM.</p>\n<p>Default = on</p>\n\
    <p><strong>--topup_first_b0s_only</strong></p>\n<p>Run <code>topup</code> with\
    \ only the first b0 from each input image. At the time of writing, <strong>FSL's\
    \ topup cannot be parallelized</strong>, and the runtime of topup can increase\
    \ dramatically as more b0 volumes are included. This flag allows for faster processing\
    \ at the expense of information lost from any interleaved b0s.</p>\n<p>Default\
    \ = use ALL b0s</p>\n<p><strong>--extra_topup_args=\"string\u201D</strong></p>\n\
    <p>Extra arguments to pass to FSL\u2019s <code>topup</code>. <code>Topup</code>\
    \ will run with the following by default (as listed in the <code>/SUPPLEMENTAL/topup.cnf</code>\
    \ configuration file) but will be overwritten by arguments passed to <code>--extra_topup_args</code>:</p>\n\
    <pre><code># Resolution (knot-spacing) of warps in mm\n--warpres=20,16,14,12,10,6,4,4,4\n\
    # Subsampling level (a value of 2 indicates that a 2x2x2 neighbourhood is collapsed\
    \ to 1 voxel)\n--subsamp=1,1,1,1,1,1,1,1,1\n# FWHM of gaussian smoothing\n--fwhm=8,6,4,3,3,2,1,0,0\n\
    # Maximum number of iterations\n--miter=10,10,10,10,10,20,20,30,30\n# Relative\
    \ weight of regularisation\n--lambda=0.00033,0.000067,0.0000067,0.000001,0.00000033,0.000000033,0.0000000033,0.000000000033,0.00000000000067\n\
    # If set to 1 lambda is multiplied by the current average squared difference\n\
    --ssqlambda=1\n# Regularisation model\n--regmod=bending_energy\n# If set to 1\
    \ movements are estimated along with the field\n--estmov=1,1,1,1,1,0,0,0,0\n#\
    \ 0=Levenberg-Marquardt, 1=Scaled Conjugate Gradient\n--minmet=0,0,0,0,0,1,1,1,1\n\
    # Quadratic or cubic splines\n--splineorder=3\n# Precision for calculation and\
    \ storage of Hessian\n--numprec=double\n# Linear or spline interpolation\n--interp=spline\n\
    # If set to 1 the images are individually scaled to a common mean intensity \n\
    --scale=0\n</code></pre>\n<p>For <code>topup</code> options that require additional\
    \ inputs, place the file in the inputs directory and use the following syntax:\
    \ <code>--&lt;myinputoption&gt; /INPUTS/&lt;file.ext&gt;</code>. For <code>topup</code>\
    \ options that produce additional outputs, the file will save in the output directory\
    \ under the \u201CTOPUP\u201D folder by using the following syntax: <code>--&lt;myoutputoption&gt;\
    \ /OUTPUTS/TOPUP/&lt;file.ext&gt;</code>. Note that in this case <code>/INPUTS</code>\
    \ and <code>/OUTPUTS</code> should be named exactly as is and are NOT the path\
    \ to the input and output directory on your file system.</p>\n<p>Default = none</p>\n\
    <p><strong>--eddy_cuda 8.0/9.1/off</strong></p>\n<p>Run FSL\u2019s <code>eddy</code>\
    \ with NVIDIA GPU acceleration. If this parameter is 8.0 or 9.1, either CUDA 8.0\
    \ or 9.1 must be installed and properly configured on your system, respectively,\
    \ and the <code>--nv</code> flag must be run in the singularity command. If this\
    \ parameter is off, <code>eddy</code> is run with OPENMP CPU multithreading. See\
    \ <code>--num_threads</code> for more information. CUDA is required to run <code>eddy</code>\
    \ with <code>--mporder</code> (intra-volume slice-wise motion correction). See\
    \ <code>--extra_eddy_args</code> for more information.</p>\n<p>Default = off</p>\n\
    <p><strong>--eddy_mask on/off</strong></p>\n<p>Run <code>eddy</code> with or without\
    \ a brain mask. If on, FSL\u2019s brain extraction tool (<code>bet</code>) is\
    \ used with a low threshold to create a rough brain mask for <code>eddy</code>.\
    \ This can sometimes produce poor results. If off, no mask is used and produces\
    \ empirically minor differences in results than when a mask is used. If this option\
    \ is on, the contour of this mask is drawn in the PDF.</p>\n<p>Default = on</p>\n\
    <p><strong>--eddy_bval_scale N/off</strong></p>\n<p>Run <code>eddy</code> with\
    \ b-values scaled by the positive number N. All other steps of the pipeline use\
    \ the original b-values. This can help <code>eddy</code> finish distortion correction\
    \ when extremely low b-values (&lt;200) are involved. If off, no scaling of b-values\
    \ is used.</p>\n<p>Default = off</p>\n<p><strong>--extra_eddy_args=\"string\u201D\
    </strong></p>\n<p>Extra arguments to pass to FSL\u2019s <code>eddy</code>. <code>Eddy</code>\
    \ will always run with the following:</p>\n<pre><code>--repol\n</code></pre>\n\
    <p>Note that if <code>--mporder</code> is passed here, <code>--eddy_cuda</code>\
    \ must be 8.0 or 9.1 and the singularity option <code>--nv</code> must be passed\
    \ into the container, as intra-volume slice-wise motion correction requires GPU\
    \ acceleration.</p>\n<p>For <code>eddy</code> options that require additional\
    \ inputs, place the file in the inputs directory and use the following syntax:\
    \ <code>--&lt;myinputoption&gt; /INPUTS/&lt;file.ext&gt;</code>. For <code>eddy</code>\
    \ options that produce additional outputs, the file will save in the output directory\
    \ under the \u201CEDDY\u201D folder by using the following syntax: <code>--&lt;myoutputoption&gt;\
    \ /OUTPUTS/EDDY/&lt;file.ext&gt;</code>. Note that in this case <code>/INPUTS</code>\
    \ and <code>/OUTPUTS</code> should be named exactly as is and are NOT the path\
    \ to the input and output directory on your file system.</p>\n<p>Default = none</p>\n\
    <p><strong>--postnormalize on/off</strong></p>\n<p>Intensity normalize images\
    \ after preprocessing by maximizing the intra-mask intensity-histogram intersections\
    \ between the averaged b0s of the scans. If this option is on, these histograms\
    \ before and after postnormalization will be reported in the output PDF.</p>\n\
    <p>Note: This option was intended for testing and is left for posterity. It is\
    \ not recommended at this time and will be deprecated.</p>\n<p>Default = off</p>\n\
    <p><strong>--correct_bias on/off</strong></p>\n<p>Perform <a href=\"https://manpages.debian.org/testing/ants/N4BiasFieldCorrection.1.en.html\"\
    \ rel=\"nofollow\">ANTs N4 bias field correction</a> as <a href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwibiascorrect.html\"\
    \ rel=\"nofollow\">called in MRTrix3</a>. If this option is on, the calculated\
    \ bias field will be visualized in the output PDF.</p>\n<p>Default = off</p>\n\
    <p><strong>--glyph_type tensor/vector</strong></p>\n<p>Visualize either tensors\
    \ or principal eigenvectors in the QA document.</p>\n<p>Default = tensor</p>\n\
    <p><strong>--atlas_reg_type FA/b0</strong></p>\n<p>Perform JHU white matter atlas\
    \ registration to the subject by either deformably registering the subject's FA\
    \ map or average b0 to the MNI FA or T2 template, respectively. Empirically, the\
    \ FA approach tends to be more accurate for white matter whereas the b0 approach\
    \ tends to be more accurate globally. The b0 approach is more robust for acquisitions\
    \ with low shells (i.e., b &lt; 500 s/mm<sup>2</sup>) or poor masking that result\
    \ in the inclusion of a lot of facial structure.</p>\n<p>Default = FA</p>\n<p><strong>--split_outputs</strong></p>\n\
    <p>Split the fully preprocessed output (a concatenation of the input images) back\
    \ into their component parts and do NOT keep the concatenated preprocessed output.</p>\n\
    <p>Default = Do NOT split and return only the concatenated output</p>\n<p><strong>--keep_intermediates</strong></p>\n\
    <p>Keep intermediate copies of diffusion data (i.e. denoised, prenormalized, bias-corrected,\
    \ etc.) used to generate final preprocessed data. Using this flag may result in\
    \ a large consumption of hard disk space.</p>\n<p>Note: Due to space concerns,\
    \ special permission needed to use this option on XNAT.</p>\n<p>Default = do NOT\
    \ keep intermediates</p>\n<p><strong>--num_threads N</strong></p>\n<p>A positive\
    \ integer indicating the number of threads to use when running portions of the\
    \ pipeline that can be multithreaded (i.e. MRTrix3, ANTs, and FSL\u2019s eddy\
    \ without GPU acceleration). Please note that at the time of writing, <strong>FSL's\
    \ topup cannot be parallelized</strong>, and that the runtime of topup can increase\
    \ dramatically as more b0 volumes are included. See <code>--topup_first_b0s_only</code>\
    \ for more information.</p>\n<p>Note: Due to resource concerns, special permission\
    \ needed to multi-thread on XNAT.</p>\n<p>Default = 1 (do NOT multithread)</p>\n\
    <p><strong>--project string</strong></p>\n<p>String describing project in which\
    \ the input data belong to label PDF output</p>\n<p>Default = proj</p>\n<p><strong>--subject\
    \ string</strong></p>\n<p>String describing subject from which the input data\
    \ were acquired to label PDF output</p>\n<p>Default = subj</p>\n<p><strong>--session\
    \ string</strong></p>\n<p>String describing session in which the input data were\
    \ acquired to label PDF output</p>\n<p>Default = sess</p>\n<p><strong>--help,\
    \ -h</strong></p>\n<h2>\n<a id=\"user-content-pipeline-assumptions\" class=\"\
    anchor\" href=\"#pipeline-assumptions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Assumptions</h2>\n<ul>\n\
    <li>\n<p>All NIFTI images are consistent with a conversion from a DICOM using\
    \ <code>dcm2niix</code> (<a href=\"https://github.com/rordenlab/dcm2niix/releases/tag/v1.0.20180622\"\
    >at least v1.0.20180622</a>) by Chris Rorden and are raw NIFTIs without distortion\
    \ correction. We require this as dcm2niix exports b-value/b-vector files in FSL\
    \ format and removes ADC or trace images auto-generated in some Philips DICOMs.\
    \ In addition <code>dcm2niix</code> correctly moves the gradients from scanner\
    \ to subject space and does not re-order volumes, both of which can cause spurious\
    \ results or pipeline failure.</p>\n<ul>\n<li>\n<p><strong>We expect raw volumes\
    \ only, no ADC or trace volumes.</strong> ADC volumes are sometimes encoded as\
    \ having a b-value greater than 0 with a corresponding b-vector of (0,0,0) and\
    \ trace volumes are sometimes encoded as having a b-value of 0 with a corresponding\
    \ non-unit normalized b-vector, as in the case of some Philips PARREC converters.\
    \ We check for these cases, remove the affected volumes, and report a warning\
    \ in the console and in the PDF.</p>\n</li>\n<li>\n<p>We cannot, unfortunately,\
    \ account for failure of reorientation of gradients into subject space. Visualization\
    \ of tensor glyphs or principal eigenvectors can be helpful in distinguishing\
    \ this. However, this error can be subtle so we suggest proper DICOM to NIFTI\
    \ conversion with the above release of <code>dcm2niix</code>.</p>\n</li>\n</ul>\n\
    </li>\n<li>\n<p>Images will be processed in the order they are listed in dtiQA_config.csv.</p>\n\
    </li>\n<li>\n<p>The size of all the volumes across all images must all be the\
    \ same.</p>\n</li>\n<li>\n<p>The location of b0 images inside the input images\
    \ do not matter.</p>\n</li>\n<li>\n<p>As per the FSL format, we do not support\
    \ non-unit normalized gradients. We also do not support gradient directions of\
    \ 0,0,0 when the corresponding b-value is non-zero. Gradients with the latter\
    \ configurations may cause pipeline failure. We report warnings in the output\
    \ PDF if we identify these.</p>\n</li>\n<li>\n<p>The phase encoding axis of all\
    \ volumes across all images is the same.</p>\n</li>\n<li>\n<p>The phase encoding\
    \ direction along the axis is the same for all volumes inside an image and is\
    \ specified in the dtiQA_config.csv file.</p>\n</li>\n<li>\n<p>Unless <code>--prenormalize</code>\
    \ is on, we assume all input images have the same gain.</p>\n</li>\n<li>\n<p>We\
    \ will preferentially preprocess images with FSL\u2019s topup using available\
    \ images with complementary phase encoding directions (i.e. + and -, \"reverse\
    \ phase encodings\"). If none are available and a T1 is available, we will synthesize\
    \ a susceptibility-corrected b0 from the first image listed in dtiQA_config.csv\
    \ with Synb0-DisCo for use with topup, unless the user turns the <code>--synb0</code>\
    \ parameter off. The readout time of this synthetic b0 will be zero and the phase\
    \ encoding direction will be equal to that of the first image in dtiQA_config.csv.\
    \ Otherwise, we will preprocess without topup and move straight to FSL\u2019s\
    \ eddy.</p>\n</li>\n<li>\n<p>We use topup and eddy for preprocessing, both of\
    \ which at the present moment do NOT officially support DSI acquisitions but only\
    \ single- and multi-shell. We will force topup and eddy to run on DSI data, but\
    \ may not produce quality results. Please carefully check the PDF output as we\
    \ report a warning if eddy detected non-shelled data and thus required the use\
    \ of the force flag.</p>\n<ul>\n<li>Note that eddy may erroneously detect data\
    \ as non-shelled if there are fewer directions in one of the shells than others.\
    \ Because we merge the images for preprocessing, a notable example of this is\
    \ when a reverse-phase encoded image uses a different shell than the forward images\
    \ and has significantly fewer directions.</li>\n</ul>\n</li>\n<li>\n<p>For preprocessing,\
    \ eddy will motion correct to the first b0 of each image.</p>\n</li>\n<li>\n<p>MRTrix3\
    \ by default preferentially uses the qform for understanding NIFTI orientations.\
    \ Nibabel uses the sform. We set MRTrix3 to use the sform in our pipeline, and\
    \ thus we preferentially use the sform when the two don\u2019t match.</p>\n</li>\n\
    <li>\n<p>No b0 drift correction is performed.</p>\n</li>\n<li>\n<p>We use the\
    \ fit tensor model primarily for QA. If b-values less than 500 s/mm<sup>2</sup>\
    \ or greater than 1500 s/mm<sup>2</sup> are present, we suggest careful review\
    \ of the fit prior to use for non-QA purposes.</p>\n</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-pipeline-processing-steps\" class=\"anchor\" href=\"#pipeline-processing-steps\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pipeline Processing Steps</h2>\n<ol>\n<li>\n<p>Threshold all b-values\
    \ such that values less than the <code>--bval_threshold</code> parameter are 0.</p>\n\
    </li>\n<li>\n<p>Check that all b-vectors are unit normalized and all b-values\
    \ greater than zero have associated non-zero b-vectors. For any volumes where\
    \ this is not the case, we remove them, flag a warning for the output PDF, and\
    \ continue the pipeline.</p>\n</li>\n<li>\n<p>If applicable, denoise all diffusion\
    \ scans with <code>dwidenoise</code> (Marchenko-Pastur PCA) from MrTrix3 and save\
    \ the noise profiles (needed for Rician correction later).</p>\n</li>\n<li>\n\
    <p>If applicable, perform Gibbs de-ringing on all diffusion scans with <code>mrdegibbs</code>\
    \ from MRTrix3.</p>\n</li>\n<li>\n<p>If applicable, perform Rician correction\
    \ on all diffusion scans with the method of moments.</p>\n</li>\n<li>\n<p>If applicable,\
    \ prenormalize all diffusion scans. To accomplish this, extract all b0 images\
    \ from each diffusion scan and average them. Then find a rough brain-mask with\
    \ FSL\u2019s bet and calculate an intensity scale factor such that the histogram\
    \ intersection between the intra-mask histogram of the different scans\u2019 averaged\
    \ b0s to that of the first scan is maximized. Apply this scale factor to the entire\
    \ diffusion weighted scan. This is done to avoid gain differences between different\
    \ diffusion scans.</p>\n<ol>\n<li>If prenormalization is not indicated, we still\
    \ run the prenormalization algorithms to calculate rough gain differences and\
    \ report the gain factors and intensity histograms in a gain QA page. The outputs\
    \ of the algorithms, however, are NOT propagated through to the rest of the pipeline.</li>\n\
    </ol>\n</li>\n<li>\n<p>Prepare data for and run preprocessing with topup and eddy</p>\n\
    <ol>\n<li>\n<p>Topup:</p>\n<ol>\n<li>\n<p>Extract all b0s from all scans, maintaining\
    \ their relative order.</p>\n</li>\n<li>\n<p>(Optional) If a T1 is supplied and\
    \ no complementary (i.e. reverse) phase encoded images are provided, use Synb0-DisCo\
    \ to convert the first b0 of the first scan to a susceptibility-corrected b0.</p>\n\
    </li>\n<li>\n<p>Build the acquisition parameters file required by both topup and\
    \ eddy</p>\n<ol>\n<li>\n<p>For the number of b0s from each image, add the same\
    \ phase encoding and readout time line to the acquisition parameters file, as\
    \ outlined in \"Example Phase Encoding Schemes\".</p>\n<ol>\n<li>Example: In the\
    \ case where we have a phase encoding axis of j and two images, one with 7 b0s,\
    \ + direction, and 0.05 readout time and one with 3 b0s, - direction, and 0.02\
    \ readout time, this file will have 10 lines. The first 7 lines are identical\
    \ and equal to [0, 1, 0, 0.05]. The last three lines are also identical and equal\
    \ to [0, -1, 0, 0.02].</li>\n</ol>\n</li>\n<li>\n<p>(Optional) If Synb0-DisCo\
    \ is run because no complementary phase encoding directions are supplied and --synb0\
    \ is not off, we add an additional line to the end of the file. This line is the\
    \ same as the first line of the file except that the readout time is 0 instead.</p>\n\
    <ol>\n<li>Example: In the case where we have a phase encoding axis of j and two\
    \ images, one with 7 b0s, + direction, and 0.05 readout time and one with 3 b0s,\
    \ + direction, and 0.02 readout time, this file will have 11 lines. The first\
    \ 7 lines are identical and equal to [0, 1, 0, 0.05]. The next three lines are\
    \ also identical and equal to [0, 1, 0, 0.02]. Finally, the last line is equal\
    \ to [0, 1, 0, 0].</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>\n<p>We then concatenate\
    \ all the b0s maintaining their order and run topup with the acquisition parameters\
    \ file if images with complementary phase encoding directions are supplied or\
    \ if a T1 was supplied. Otherwise, we move on to the next step, eddy.</p>\n</li>\n\
    </ol>\n</li>\n<li>\n<p>Eddy</p>\n<ol>\n<li>\n<p>Using the acquisition parameters\
    \ file from the topup step, regardless of whether topup was performed, we build\
    \ the eddy index file such that each volume in each image corresponds to the line\
    \ in the acquisition parameters file associated with the first b0 of each scan.\
    \ This is done to tell eddy that each volume in a given scan has the same underlying\
    \ phase encoding scheme as the first b0 of that scan.</p>\n<ol>\n<li>Example:\
    \ In the case where we have two images, one with 7 b0s and 100 total volumes and\
    \ one with 3 b0s and 10 total volumes, the eddy index file has 100 1\u2019s followed\
    \ by 10 8\u2019s.</li>\n</ol>\n</li>\n<li>\n<p>Eddy is then run with either a\
    \ mask generated with bet and the -f 0.25 and -R options or without a mask (aka\
    \ with a mask of all 1\u2019s), depending on user input (see the --eddy_mask option)\
    \ and with the output of topup if topup was run. Eddy also runs with the --repol\
    \ option for outlier slice replacement. We also first run eddy with a check looking\
    \ for shelled data. If the check fails, eddy is then run with the --data_is_shelled\
    \ flag to force eddy to run on all scans, DSI included. Note that DSI data is\
    \ not officially supported by FSL\u2026 yet?</p>\n<ol>\n<li>\n<p>If eddy detects\
    \ data is not shelled, we report this as a warning</p>\n</li>\n<li>\n<p>As noted\
    \ in the assumptions section above, eddy may erroneously detect data as non-shelled\
    \ if there are fewer directions in one of the shells than others. Because we merge\
    \ the images for preprocessing, a notable example of this is when a reverse-phase\
    \ encoded image uses a different shell than the forward images and has significantly\
    \ fewer directions.</p>\n</li>\n</ol>\n</li>\n<li>\n<p>Eddy also performs bvec\
    \ rotation correction and calculates the voxel-wise signal-to-noise ratios of\
    \ the b0 images and the voxel-wise contrast-to-noise ratios for the diffusion\
    \ weighted images. SNR is defined as the mean value divided by the standard deviation.\
    \ CNR is defined as the standard deviation of the Gaussian Process predictions\
    \ (GP) divided by the standard deviation of the residuals between the measured\
    \ data and the GP predictions.</p>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>\n\
    <p>If the user chooses to, we then perform post-normalization in the same fashion\
    \ as pre-normalization.</p>\n</li>\n<li>\n<p>If the user chooses to, we then wrap\
    \ up preprocessing with an N4 bias field correction as implemented in ANTs via\
    \ MRTrix3\u2019s dwibiascorrect.</p>\n</li>\n<li>\n<p>We generate a brain mask\
    \ using FSL\u2019s bet2 with the following options:</p>\n<p><code>-f 0.25 -R</code></p>\n\
    </li>\n<li>\n<p>We then apply the mask to the preprocessed images while we calculate\
    \ tensors using MRTrix3\u2019s dwi2tensor function. For visualization we discard\
    \ tensors that have diagonal elements greater than 3 times the apparent diffusion\
    \ coefficient of water at 37\xB0C (~0.01).</p>\n<ol>\n<li>We also reconstruct\
    \ the preprocessed image from the tensor fit for further analysis later. dwi2tensor\
    \ does this for us.</li>\n</ol>\n</li>\n<li>\n<p>We then convert the tensor to\
    \ FA and MD images (and visualize them later too) as well as AD, RD, and V1 eigenvector\
    \ images for the user. The latter 3 are not visualized.</p>\n</li>\n</ol>\n<h2>\n\
    <a id=\"user-content-pipeline-quality-assurance-steps\" class=\"anchor\" href=\"\
    #pipeline-quality-assurance-steps\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Pipeline Quality Assurance Steps</h2>\n\
    <ol>\n<li>\n<p>We start with the brain mask generated above to generate a mask\
    \ used for the following quantification of tensor fit using a chi-squared statistic.</p>\n\
    <ol>\n<li>\n<p>First, we calculate the mean image for each unique b-value (0 not\
    \ included). Then we run FSL\u2019s FAST to isolate the CSF on each meaned image.\
    \ We then take the average probability of a voxel being CSF across all unique\
    \ b-values and assign &gt;15% probability to be a positive CSF voxel.</p>\n</li>\n\
    <li>\n<p>Then we call the final chi-squared mask to be the intersection of the\
    \ inverted CSF mask and a 1-pixel eroded version of the brain mask.</p>\n</li>\n\
    </ol>\n</li>\n<li>\n<p>On the voxels inside the chi-squared mask, we perform the\
    \ following quality assurance:</p>\n<ol>\n<li>\n<p>We perform a chi-squared analysis\
    \ for each slice for each volume in the main image by calculating the ratio between\
    \ the sum-squared error of the fit and the sum-squared intensities of the slice.</p>\n\
    </li>\n<li>\n<p>We extract the average FA for a number of white matter ROIs defined\
    \ by the Hopkins atlas. We do this by non-rigidly registering the atlas to our\
    \ FA output and extracting the FA values contained in each ROI.</p>\n</li>\n<li>\n\
    <p>We check the gradients output by eddy (i.e. the preprocessed gradients) with\
    \ <a href=\"https://mrtrix.readthedocs.io/en/3.0.0/reference/commands/dwigradcheck.html\"\
    \ rel=\"nofollow\">dwigradcheck from MRTrix3</a>. This performs tractography and\
    \ finds the optimal sign and order permutation of the b-vectors such that the\
    \ average tract length in the brain is most physiological.</p>\n<ol>\n<li>\n<p>These\
    \ optimized gradients are saved in the OPTIMIZED_BVECS output folder, and the\
    \ gradients output by eddy in the PREPROCESSED folder are NOT overwritten.</p>\n\
    </li>\n<li>\n<p>The original, preprocessed, and preprocessed + optimized gradients\
    \ are visualized as outlined below.</p>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n<li>\n\
    <p>We then visualize the entire pipeline.</p>\n<ol>\n<li>\n<p>On the first page\
    \ we describe the methods used for that run of the pipeline (what inputs were\
    \ provided, what sort of preprocessing happened, etc.).</p>\n</li>\n<li>\n<p>We\
    \ then visualize the raw images with the interpreted phase encoding schemes.</p>\n\
    </li>\n<li>\n<p>If Gibbs de-ringing was run, we visualize central slices of the\
    \ averaged residuals across b0 volumes before and after Gibbs de-ringing, looking\
    \ for large residuals near high contrast interfaces (i.e. parenchyma against CSF)</p>\n\
    </li>\n<li>\n<p>If Rician correction was performed, we visualize the within brain\
    \ intensity distributions of each shell of each image before and after correction,\
    \ looking for downward shifts after correction.</p>\n</li>\n<li>\n<p>If Synb0-DisCo\
    \ was run, we then visualize the distorted b0 (first b0 of first scan) and T1\
    \ used as inputs as well as the output susceptibility corrected b0 in their native\
    \ space.</p>\n</li>\n<li>\n<p>If pre- or post-normalization was performed, we\
    \ then visualize the intra-mask histograms before and after these steps as well\
    \ as the calculated scaling factors. If pre-normalization is not performed, we\
    \ visualize the histograms that would have been generated with pre-normalization\
    \ ONLY as a check for gain differences.</p>\n</li>\n<li>\n<p>We then visualize\
    \ the first b0 of the images before and after preprocessing with the contours\
    \ of the brain and stats masks overlaid as well as the contours of the eddy mask\
    \ overlaid if it is used.</p>\n</li>\n<li>\n<p>We plot the motion and angle correction\
    \ done by eddy as well as the RMS displacement and median intensity for each volume\
    \ and the volume\u2019s associated b-value. These values are read in from an eddy\
    \ output text file and we also compute and save the average of these values. In\
    \ addition, we plot the outlier slices removed and then imputed by eddy as well\
    \ as the chi-squared fit, with maximal bounds 0 to 0.2. The median chi-squared\
    \ values are shown across volumes and slices.</p>\n</li>\n<li>\n<p>We then plot\
    \ the original raw b-vectors scaled by their b-values, the preprocessed ones output\
    \ by eddy, and the optimized ones determined by <code>dwigradcheck</code> applied\
    \ to the preprocessed ones.</p>\n</li>\n<li>\n<p>If bias field correction was\
    \ performed, we then visualize the calculated fields.</p>\n</li>\n<li>\n<p>We\
    \ then visualize some central slices of the average volumes for all unique b-values,\
    \ including b = 0 and report the median intra-mask SNR or CNR calculated by eddy\
    \ as appropriate. If there are more unique b-values than shells deteremined by\
    \ eddy, we round the b-values to the nearest 100 by default to assign volumes\
    \ to shells or we choose the nearest shell indicated by the user (see <code>--nonzero_shells</code>).</p>\n\
    </li>\n<li>\n<p>We visualize the tensors (or principal eigenvectors depending\
    \ on <code>--glyph_type</code>) using MRTrix3\u2019s mrview, omitting the tensors\
    \ with negative eigenvalues or eigenvalues greater than 3 times the ADC of water\
    \ at 37\xB0C.</p>\n</li>\n<li>\n<p>We then visualize some central slices of the\
    \ FA map clipped from 0 to 1 as well as the average FA for the Hopkins ROIs and\
    \ the quality of the atlas registration.</p>\n</li>\n<li>\n<p>Lastly, we visualize\
    \ some central slices of the MD map clipped from 0 to 0.003 (ADC of water at 37\xB0\
    C).</p>\n</li>\n</ol>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-outputs\" class=\"\
    anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Outputs</h2>\n<p>&lt;imageN_%&gt; denotes the\
    \ original prefix of imageN with the preceding preprocessing step descriptors\
    \ tacked on the end. For example, in the case of the PRENORMALIZED directory,\
    \ the prefix for imageJ may or may not include \"_denoised\" depending on whether\
    \ the denoising step was run.</p>\n<p>Folders and files in <strong>bold</strong>\
    \ are always included.</p>\n<p>Folders and files in <em>italics</em> are removed\
    \ if <code>--keep_intermediates</code> is NOT indicated</p>\n<ol>\n<li>\n<p><strong>THRESHOLDED_BVALS</strong></p>\n\
    <ul>\n<li>\n<p><strong>&lt;image1&gt;.bval</strong></p>\n<p>:</p>\n</li>\n<li>\n\
    <p><strong>&lt;imageN&gt;.bval</strong></p>\n</li>\n</ul>\n</li>\n<li>\n<p><em>CHECKED</em>\
    \ (these contain the volumes that have passed the bval/bvec checks)</p>\n<ul>\n\
    <li>\n<p><em>&lt;image1&gt;_checked.nii.gz</em></p>\n</li>\n<li>\n<p><em>&lt;image1&gt;_checked.bval</em></p>\n\
    </li>\n<li>\n<p><em>&lt;image1&gt;_checked.bvec</em></p>\n<p>:</p>\n</li>\n<li>\n\
    <p><em>&lt;imageN&gt;_checked.nii.gz</em></p>\n</li>\n<li>\n<p><em>&lt;imageN&gt;_checked.bval</em></p>\n\
    </li>\n<li>\n<p><em>&lt;imageN&gt;_checked.bvec</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>DENOISED</em> (these files are only created if <code>--denoise</code>\
    \ is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_denoised.nii.gz</em></p>\n</li>\n\
    <li>\n<p><em>&lt;image1_%&gt;_noise.nii.gz</em> (needed for Rician correction)</p>\n\
    <p>:</p>\n</li>\n<li>\n<p><em>&lt;imageN_%&gt;_denoised.nii.gz</em></p>\n</li>\n\
    <li>\n<p><em>&lt;imageN_%&gt;_noise.nii.gz</em></p>\n</li>\n</ul>\n</li>\n<li>\n\
    <p><em>DEGIBBS</em> (these files are only created if <code>--degibbs</code> is\
    \ on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_degibbs.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_degibbs.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>RICIAN</em> (these files are only created if <code>--rician</code>\
    \ is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_rician.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_rician.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>PRENORMALIZED</em> (these files are only created if <code>--prenormalize</code>\
    \ is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_norm.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_norm.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><em>GAIN_CHECK</em> (these files are only created if <code>--prenormalize</code>\
    \ is off)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_norm.nii.gz</em></p>\n<p>:</p>\n\
    </li>\n<li>\n<p><em>&lt;imageN_%&gt;_norm.nii.gz</em></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><strong>TOPUP</strong> (these files are only created if <code>topup</code>\
    \ was run)</p>\n<ul>\n<li>\n<p>acqparams.txt (same as OUTPUTS/EDDY/acqparams.txt)</p>\n\
    </li>\n<li>\n<p><em>preproc_input_b0_first.nii.gz</em> (only if Synb0-DisCo is\
    \ run)</p>\n</li>\n<li>\n<p>b0_syn.nii.gz (only if Synb0-DisCo is run)</p>\n</li>\n\
    <li>\n<p><em>preproc_input_b0_all.nii.gz</em> or <em>preproc_input_b0_all_smooth_with_b0_syn.nii.gz</em></p>\n\
    </li>\n<li>\n<p><em>preproc_input_b0_all_topped_up.nii.gz</em> or <em>preproc_input_b0_all_smooth_with_b0_syn_topped_up.nii.gz</em></p>\n\
    </li>\n<li>\n<p>preproc_input_b0_all.topup_log or preproc_input_b0_all_smooth_with_b0_syn.topup_log</p>\n\
    </li>\n<li>\n<p>topup_field.nii.gz</p>\n</li>\n<li>\n<p>topup_results_fieldcoef.nii.gz</p>\n\
    </li>\n<li>\n<p>topup_results_movpar.txt</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>EDDY</strong></p>\n\
    <ul>\n<li>\n<p><strong>acqparams.txt</strong> (same as OUTPUTS/TOPUP/acqparams.txt)</p>\n\
    </li>\n<li>\n<p><strong>index.txt</strong></p>\n</li>\n<li>\n<p><em>preproc_input.nii.gz</em></p>\n\
    </li>\n<li>\n<p><em>preproc_input.bval</em></p>\n</li>\n<li>\n<p><em>preproc_input.bvec</em></p>\n\
    </li>\n<li>\n<p><em>preproc_input_eddyed.nii.gz</em> (renamed from \"eddy_results.nii.gz\"\
    )</p>\n</li>\n<li>\n<p><em>preproc_input_eddyed.bval</em></p>\n</li>\n<li>\n<p><em>preproc_input_eddyed.bvec</em></p>\n\
    </li>\n<li>\n<p>eddy_mask.nii.gz (only included if <code>--eddy_mask</code> is\
    \ on)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_command_txt</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_movement_rms</strong> (describes volume-wise\
    \ RMS displacement)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_outlier_free_data.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_outlier_map</strong> (describes which\
    \ slices were deemed outliers)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_outlier_n_sqr_stdev_map</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_outlier_n_stdev_map</strong></p>\n</li>\n\
    <li>\n<p><strong>eddy_results.eddy_outlier_report</strong></p>\n</li>\n<li>\n\
    <p><strong>eddy_results.eddy_parameters</strong> (describes volume-wise rotation\
    \ and translation)</p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_post_eddy_shell_alignment_parameters</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_post_eddy_shell_PE_translation_parameters</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_restricted_movement_rms</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_rotated_bvecs (describes properly rotated\
    \ b-vectors)</strong></p>\n</li>\n<li>\n<p><strong>eddy_results.eddy_values_of_all_input_parameters</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_results.eddy_cnr_maps.nii.gz</strong></p>\n</li>\n\
    </ul>\n</li>\n<li>\n<p><em>POSTNORMALIZED</em> (these files are only created if\
    \ <code>--postnormalize</code> is on)</p>\n<ul>\n<li>\n<p><em>&lt;image1_%&gt;_topup_eddy_norm.nii.gz</em>\
    \ (\"_topup\" only applies if topup was run)</p>\n<p>:</p>\n</li>\n<li>\n<p><em>&lt;imageN_%&gt;_topup_eddy_norm.nii.gz</em></p>\n\
    </li>\n</ul>\n</li>\n<li>\n<p><em>UNBIASED</em> (these files are only created\
    \ if <code>--correct_bias</code> is on; this folder is removed if <code>--correct_bias</code>\
    \ is off)</p>\n<ul>\n<li>\n<p><em>normed_unbiased.nii.gz</em> (if postnormalization\
    \ is run) or <em>preproc_input_eddyed_unbiased.nii.gz</em> (if postnormalization\
    \ is not run)</p>\n</li>\n<li>\n<p>bias_field.nii.gz</p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><strong>PREPROCESSED</strong> (these represent the final output of the\
    \ pipeline)</p>\n<ul>\n<li>\n<p><em>dwmri.nii.gz</em> (dwmri* files deleted only\
    \ if <code>--split_outputs</code> is also set)</p>\n</li>\n<li>\n<p><em>dwmri.bval</em></p>\n\
    </li>\n<li>\n<p><em>dwmri.bvec</em></p>\n</li>\n<li>\n<p>&lt;image1&gt;_preproc.nii.gz\
    \ (*_preproc files exist only if <code>--split_outputs</code> is set)</p>\n</li>\n\
    <li>\n<p>&lt;image1&gt;_preproc.bval</p>\n</li>\n<li>\n<p>&lt;image1&gt;_preproc.bvec</p>\n\
    <p>:</p>\n</li>\n<li>\n<p>&lt;imageN&gt;_preproc.nii.gz</p>\n</li>\n<li>\n<p>&lt;imageN&gt;_preproc.bval</p>\n\
    </li>\n<li>\n<p>&lt;imageN&gt;_preproc.bvec</p>\n</li>\n<li>\n<p><strong>mask.nii.gz</strong></p>\n\
    </li>\n</ul>\n</li>\n<li>\n<p><strong>TENSOR</strong></p>\n<ul>\n<li>\n<p><strong>dwmri_tensor.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><em>dwmri_recon.nii.gz</em></p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>SCALARS</strong></p>\n\
    <ul>\n<li>\n<p><strong>dwmri_tensor_fa.nii.gz</strong></p>\n</li>\n<li>\n<p><strong>dwmri_tensor_md.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>dwmri_tensor_ad.nii.gz</strong></p>\n</li>\n<li>\n<p><strong>dwmri_tensor_rd.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>dwmri_tensor_v1.nii.gz</strong></p>\n</li>\n</ul>\n</li>\n\
    <li>\n<p><strong>STATS</strong></p>\n<ul>\n<li>\n<p><strong>atlas2subj.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>b02template_0GenericAffine.mat</strong> or <strong>fa2template_0GenericAffine.mat</strong>\
    \ depending on <code>--atlas_reg_type</code></p>\n</li>\n<li>\n<p><strong>b02template_1Warp.nii.gz</strong>\
    \ or <strong>fa2template_1Warp.nii.gz</strong> depending on <code>--atlas_reg_type</code></p>\n\
    </li>\n<li>\n<p><strong>b02template_1InverseWarp.nii.gz</strong> or <strong>fa2template_1InverseWarp.nii.gz</strong>\
    \ depending on <code>--atlas_reg_type</code></p>\n</li>\n<li>\n<p><strong>chisq_mask.nii.gz</strong></p>\n\
    </li>\n<li>\n<p><strong>chisq_matrix.txt</strong></p>\n</li>\n<li>\n<p><strong>eddy_avg_abs_displacement.txt</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_median_cnr.txt</strong></p>\n</li>\n<li>\n<p><strong>eddy_avg_rel_displacement.txt</strong></p>\n\
    </li>\n<li>\n<p><strong>eddy_avg_rotations.txt</strong></p>\n</li>\n<li>\n<p><strong>eddy_avg_translations.txt</strong></p>\n\
    </li>\n<li>\n<p><strong>roi_avg_fa.txt</strong></p>\n</li>\n<li>\n<p><strong>stats.csv</strong>\
    \ (contains summary of all motion, SNR/CNR, and average FA stats)</p>\n</li>\n\
    </ul>\n</li>\n<li>\n<p><strong>OPTIMIZED_BVECS</strong> (these are sign/axis permuted\
    \ per <code>dwigradcheck</code> and are only used for QA purposes)</p>\n<ul>\n\
    <li>\n<p><strong>dwmri.bval</strong></p>\n</li>\n<li>\n<p><strong>dwmri.bvec</strong></p>\n\
    </li>\n</ul>\n</li>\n<li>\n<p><strong>PDF</strong></p>\n<ul>\n<li>\n<strong>dtiQA.pdf</strong>\
    \ (final QA document)</li>\n</ul>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-note-on-versioning-for-vuiis-xnat-users\"\
    \ class=\"anchor\" href=\"#note-on-versioning-for-vuiis-xnat-users\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Note\
    \ on Versioning for VUIIS XNAT Users</h2>\n<p>PreQual was developed at Vanderbilt\
    \ under the project name \"dtiQA v7 Multi\". PreQual v1.0.0 represents dtiQA v7.2.0.\
    \ Thus, on XNAT, dtiQA v7.2.x refers to PreQual v1.0.x.</p>\n"
  stargazers_count: 9
  subscribers_count: 0
  topics:
  - diffusion
  - mri
  - preprocessing
  - quality
  - assurance
  updated_at: 1621986378.0
MASILab/Synb0-DISCO:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: MASILab/Synb0-DISCO
  latest_release: v2.0
  readme: "<h1>\n<a id=\"user-content-synb0-disco\" class=\"anchor\" href=\"#synb0-disco\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Synb0-DISCO</h1>\n<p>This repository implements the paper \"Synthesized\
    \ b0 for diffusion distortion correction\". For deployment we provide a docker\
    \ container which uses the trained model to predict the undistorted b0 to be used\
    \ in susceptability distortion correction for diffusion weighted MRI. Please use\
    \ the following citation to refer to this work:</p>\n<p>Schilling KG, Blaber J,\
    \ Huo Y, Newton A, Hansen C, Nath V, Shafer AT, Williams O, Resnick SM, Rogers\
    \ B, Anderson AW, Landman BA. Synthesized b0 for diffusion distortion correction\
    \ (Synb0-DisCo). Magn Reson Imaging. 2019 Dec;64:62-70. doi: 10.1016/j.mri.2019.05.008.\
    \ Epub 2019 May 7. PMID: 31075422; PMCID: PMC6834894.</p>\n<h1>\n<a id=\"user-content-synb0_25iso_app\"\
    \ class=\"anchor\" href=\"#synb0_25iso_app\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>synb0_25iso_app</h1>\n<p><a href=\"\
    https://hub.docker.com/repository/docker/hansencb/synb0\" rel=\"nofollow\">Docker\
    \ Hub</a></p>\n<h1>\n<a id=\"user-content-run-instructions\" class=\"anchor\"\
    \ href=\"#run-instructions\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Run Instructions:</h1>\n<p>For docker:</p>\n\
    <pre><code>sudo docker run --rm \\\n-v $(pwd)/INPUTS/:/INPUTS/ \\\n-v $(pwd)/OUTPUTS:/OUTPUTS/\
    \ \\\n-v &lt;path to license.txt&gt;:/extra/freesurfer/license.txt \\\n--user\
    \ $(id -u):$(id -g) \\\nhansencb/synb0\n\nFlags:\n--notopup Skips the application\
    \ of FSL's topup susceptibility correction \n* as a default, we run topup for\
    \ you, although you may want to run this on\n your own (for example with your\
    \ own config file, or if you would like to \n utilize multiple b0's)\n\nSee INPUTS/OUTPUTS\
    \ sections below.\nIn short, if within your current directory you have your INPUTS\
    \ \nand OUTPUTS folder, you can run this command copy/paste with the \nonly change\
    \ being &lt;path to license.txt&gt; should point to \nfreesurfer licesnse.txt\
    \ file on your system.\n\nIf INPUTS and OUTPUTS are not within your current directory,\
    \ you\nwill need to change $(pwd)/INPUTS/ to the full path to your \ninput directory,\
    \ and similarly for OUTPUTS.\n\n*** For Mac users, Docker defaults allows only\
    \ 2gb of RAM \nand 2 cores - we suggest giving Docker access to &gt;8Gb \nof RAM\n\
    *** Additionally on MAC, if permissions issues prevent binding the\npath to the\
    \ license.txt file, we suggest moving the freesurfer\nlicense.txt file to the\
    \ current path and replacing the path line to\n\" $(pwd)/license.txt:/extra/freesurfer/license.txt\
    \ \"\n</code></pre>\n<p>For singularity:</p>\n<ul>\n<li>Build synb0.simg in the\
    \ current directory:</li>\n</ul>\n<pre><code>singularity pull docker://hansencb/synb0\n\
    </code></pre>\n<ul>\n<li>Run the synb0.simg container:</li>\n</ul>\n<pre><code>singularity\
    \ run -e \\\n-B INPUTS/:/INPUTS \\\n-B OUTPUTS/:/OUTPUTS \\\n-B &lt;path to license.txt&gt;:/extra/freesurfer/license.txt\
    \ \\\n&lt;path to synb0.simg&gt;\n\n&lt;path to license.txt&gt; should point to\
    \ freesurfer licesnse.txt file\n&lt;path to synb0.simg&gt; should point to the\
    \ singularity container \n\nFlags:\n--notopup Skips the application of FSL's topup\
    \ susceptibility correction \n* as a default, we run topup for you, although you\
    \ may want to run this on\n your own (for example with your own config file, or\
    \ if you would like to \n utilize multiple b0's)\n</code></pre>\n<p>INPUTS:</p>\n\
    <pre><code>INPUTS directory must contain the following:\nb0.nii.gz, T1.nii.gz,\
    \ and acqparams.txt\n\nb0.nii.gz includes the non-diffusion weighted image(s).\
    \ \nT1.nii.gz is the T1-weighted image.\nacqparams.txt describes the acqusition\
    \ parameters, and is described in detail \non the FslWiki for topup (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/topup).Briefly,\n\
    it describes the direction of distortion and tells TOPUP that the synthesized\
    \ image\nhas an effective echo spacing of 0 (infinite bandwidth). An example acqparams.txt\
    \ is\ndisplayed below, in which distortion is in the second dimension, note that\
    \ the second\nrow corresponds to the synthesized, undistorted, b0:\n$ cat acqparams.txt\
    \ \n0 1 0 0.062\n0 1 0 0.000\n\nT1_mask.nii.gz is an optional user provided mask\
    \ which excludes the skull in the T1 image. \n               If not provided a\
    \ mask will be estimated using FSL's BET.\n</code></pre>\n<p>Without singularity\
    \ or Docker:</p>\n<pre><code>If you choose to run this  in bash, the script that\
    \ is containerized is located in\nsrc/pipeline.sh. The paths in pipeline.sh are\
    \ specific to the docker/singularity file\nsystem, but the processing can be replicated\
    \ using the scripts in src.\n\nThese utilize freesurfer, FSL, ANTS, and a python\
    \ environment with pytorch.\n</code></pre>\n<p>OUTPUTS:</p>\n<pre><code>After\
    \ running, the outputs directory contains the following:\nT1_mask.nii.gz: brain\
    \ extracted (skullstripped) T1   \nT1_norm.nii.gz: normalized T1\nepi_reg_d.mat:\
    \ epi_reg b0 to T1 in FSL format\nepi_reg_d_ANTS.txt: epi_reg to T1 in ANTS format\n\
    \nAnts registration of T1_norm to/from MNI space:\nANTS0GenericAffine.mat\nANTS1InverseWarp.nii.gz\
    \  \nANTS1Warp.nii.gz\n   \nT1_norm_lin_atlas_2_5.nii.gz: linear transform T1\
    \ to MNI   \nb0_d_lin_atlas_2_5.nii.gz  : linear transform distorted b0 in MNI\
    \ space   \nT1_norm_nonlin_atlas_2_5.nii.gz: nonlinear transform T1 to MNI   \n\
    b0_d_nonlin_atlas_2_5.nii.gz  : nonlinear transform distorted b0 in MNI space\
    \  \n\nInferences (predictions) for each of five folds:\nT1 input path: /OUTPUTS/T1_norm_lin_atlas_2_5.nii.gz\n\
    b0 input path: /OUTPUTS/b0_d_lin_atlas_2_5.nii.gz\nb0_u_lin_atlas_2_5_FOLD_1.nii.gz\
    \  \nb0_u_lin_atlas_2_5_FOLD_2.nii.gz  \nb0_u_lin_atlas_2_5_FOLD_3.nii.gz  \n\
    b0_u_lin_atlas_2_5_FOLD_4.nii.gz  \nb0_u_lin_atlas_2_5_FOLD_5.nii.gz  \n\nEnsemble\
    \ average of inferences:\nb0_u_lin_atlas_2_5_merged.nii.gz  \nb0_u_lin_atlas_2_5.nii.gz\
    \         \n\nb0_u.nii.gz: Syntehtic b0 native space                      \n\n\
    b0_d_smooth.nii.gz: smoothed b0\n\nb0_all.nii.gz: stack of distorted and synthetized\
    \ image as input to topup        \n\ntopup outputs to be used for eddy:\ntopup_movpar.txt\n\
    b0_all_topup.nii.gz\nb0_all.topup_log         \ntopup_fieldcoef.nii.gz\n</code></pre>\n\
    <p>AFTER RUNNING:</p>\n<pre><code>After running, we envision using the topup outputs\
    \ directly with FSL's \neddy command, exactly as would be done if a full set of\
    \ reverse PE \nscans was acquired. For example:\n\neddy --imain=path/to/diffusiondata.nii.gz\
    \ --mask=path/to/brainmask.nii.gz \\\n--acqp=path/to/acqparams.txt --index=path/to/index.txt\
    \ \\\n--bvecs=path/to/bvecs.txt --bvals=path/to/bvals.txt \n--topup=path/to/OUTPUTS/topup\
    \ --out=eddy_unwarped_images\n\nwhere imain is the original diffusion data, mask\
    \ is a brain mask, acqparams\nis from before, index is the traditional eddy index\
    \ file which contains an \nindex (most likely a 1) for every volume in the diffusion\
    \ dataset, topup points \nto the output of the singularity/docker pipeline, and\
    \ out is the eddy-corrected\nimages utilizing the field coefficients from the\
    \ previous step.\n\nAlternatively, if you choose to run --notopup flag, the file\
    \ you are interested in\nis b0_all. This is a concatenation of the real b0 and\
    \ the synthesized undistorted\nb0. We run topup with this file, although you may\
    \ chose to do so utilizing your \ntopup version or config file. \n\n</code></pre>\n"
  stargazers_count: 11
  subscribers_count: 10
  topics: []
  updated_at: 1619075973.0
MIPT-Oulu/DeepWrist:
  data_format: 2
  description: Deep Learning Pipeline for Wrist Fracture Detection
  filenames:
  - Singularity
  full_name: MIPT-Oulu/DeepWrist
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-paper-link\" class=\"anchor\" href=\"#paper-link\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Paper Link</h1>\n<p>Title: Deep Learning for Wrist Fracture Detection:\
    \ Are We There Yet? <br>\n<a href=\"https://arxiv.org/abs/2012.02577\" rel=\"\
    nofollow\">arXiv link</a></p>\n<h1>\n<a id=\"user-content-deepwrist-pipeline\"\
    \ class=\"anchor\" href=\"#deepwrist-pipeline\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>DeepWrist Pipeline</h1>\n<p>A\
    \ transfer learning pipeline to detect wrist fracture from DICOM files. It has\
    \ two blocks: Landmark Localization Block\nand Fracture Detection Block.\n<a href=\"\
    ./figures/DeepWrist_pipeline.png\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img src=\"./figures/DeepWrist_pipeline.png\" alt=\"DeepWrist\" style=\"max-width:100%;\"\
    ></a></p>\n<p>Both of the blocks are yml configuration based. We used OmegaConf\
    \ for this purpose. Each executable python file can\neither run standalone or\
    \ requires a yml file as <code>experiment</code> argument to be passed down at\
    \ command line.</p>\n<h2>\n<a id=\"user-content-landmark-localization-block\"\
    \ class=\"anchor\" href=\"#landmark-localization-block\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Landmark\
    \ Localization Block</h2>\n<p>Landmark Localization Block is adapted from <a href=\"\
    https://arxiv.org/pdf/1907.12237\" rel=\"nofollow\">KNEEL</a>. However, we developed\
    \ some data\naugmentation methods suited to our task.  The <code>localizer</code>\
    \ folder contains the source code for Landmark Localizer and\nstructured as <br></p>\n\
    <pre><code>localizer \n|---config \n|   |---experiment\n|---kneel_before_wrist\
    \ \n|   |---data \n|   |---model \n|---scripts \n</code></pre>\n<p>The <code>config</code>\
    \ folder contains the initial default configuration and a configuration processor.\
    \ There is a folder named\n<code>experiment</code> inside <code>config</code>\
    \ folder which holds confgiguration for different experiments.</p>\n<p><code>kneel_before_wrist</code>\
    \ hosts the body of the localizer part of our pipeline. It has two sub-directory:\
    \ <code>data</code> and <code>model</code>.\nThe <code>data</code> folder contains\
    \ utilities necessary to process and augment data for training and evaluation.\
    \ <code>model</code>\nsub-directory contains the pytorch lightning version of\
    \ HourGlass network which we will use for training the localizer.</p>\n<p>The\
    \ <code>scripts</code> directory hosts all the experiment scripts for which the\
    \ yaml configurations are created.</p>\n<h2>\n<a id=\"user-content-how-to-train-localizer-with-your-own-data\"\
    \ class=\"anchor\" href=\"#how-to-train-localizer-with-your-own-data\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How\
    \ to Train Localizer with your own data</h2>\n<ol>\n<li>\n<p>First step for training\
    \ your own localizer is to collect data. For our research we used private hospital\
    \ data,\nthrefore it cannot be shared. To make things simple, we used csv file\
    \ to store meta data about the dataset. This way,\nyou don't have to load the\
    \ full dataset to the memory rather fecth the file location from csv file and\
    \ read it just-in-time.\nSo, we are dealing with wrist fracture images. We will\
    \ consider posterioanterio (PA) and lateral (LAT) view of the wrist x-ray. To\n\
    make your own dataset, you have to create a csv metadata file containing  at least\
    \ <code>Fname, Points,  Side</code> columns. <code>Fname</code>\nis the absolute\
    \ path to the wrist image, <code>Points</code> column will contain the landmark\
    \ coordinates of top of distal ulna,\ntop of distal radius and assumed center\
    \ of the wrist for PA view and two distinguishalbe points on top part of distal\n\
    radio-ulna bone and the assumed center of wrist for LAT view. As the name suggest,\
    \ the <code>Side</code> column contains the side\ninformation of corresponding\
    \ wrist x-ray. Put 0 for PA and 1 for LAT. Once the metadata is ready, we can\
    \ move forward.</p>\n</li>\n<li>\n<p>Second step is to clone <code>wrist_landmark.yaml</code>\
    \ configuration file and modify the clone. Inside the yaml file modify\nfollowing</p>\n\
    </li>\n</ol>\n<pre><code>  data_home: # root folder that contains the data folder\
    \ \n  data_folder: # your data folder name\n  meta: the csv meta file you have\
    \ created. should be inside data folder \n</code></pre>\n<ol start=\"3\">\n<li>Once\
    \ you are done with step 2, run the <code>train_ptl.py --experiment=YourClonedYAMLFile</code>.\
    \ This file is located inside\n<code>scripts</code> folder. It will start the\
    \ training.</li>\n</ol>\n<h2>\n<a id=\"user-content-fracture-detection-block\"\
    \ class=\"anchor\" href=\"#fracture-detection-block\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Fracture Detection\
    \ Block</h2>\n<p>The <code>classifier</code> folder hosts the Fracture Detection\
    \ Block. It has a similar structure like localizer.</p>\n<pre><code>classifier\
    \ \n|---config \n|---fracture_detector\n|   |---callback \n|   |---data \n|  \
    \ |---model \n|---script \n\n</code></pre>\n<p>Like before <code>config</code>\
    \ folder hosts the script configurations. <code>fracture_detector</code> folder\
    \ hosts necessary folders and\nfiles for model, data and training related stuffs.\
    \ Inside this folder, there are three folders: 1) <code>callback</code> (hosts\n\
    callback function definitions), 2) <code>data</code> (hosts data related utilities)\
    \ and 3) <code>model</code> (hosts model definition and\ntraining methods)</p>\n\
    <h2>\n<a id=\"user-content-how-to-train-your-fracture-detector\" class=\"anchor\"\
    \ href=\"#how-to-train-your-fracture-detector\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How to train your Fracture Detector</h2>\n\
    <p>Step 1. First step to train your custom fracture detector is to collect data\
    \ using the <code>localizer</code> model trained previously.\nSave the generated\
    \ ROI with the corresponding <code>ID</code> as filename. Create a csv metadata\
    \ file with <code>ID</code>, <code>Side</code>, <code>Fname</code>(optional)\n\
    and <code>Fracture</code> columns. Say, the meta file name is <code>your_meta.csv</code>Create\
    \ a <code>root</code> folder which we will use as data home where the generated\
    \ ROI images and the csv\nmeta file are saved. There shoudl be <code>PA</code>\
    \ and <code>LAT</code> folder in the <code>root</code> folder to host PA ROI and\
    \ LAT ROI respectively.</p>\n<p>Step 2. Clone the existing training conf <code>fracture_detector_seresnet.yaml</code>\
    \ to <code>your_config_file.yaml</code>.\nOpen <code>your_config_file.yaml</code>\
    \ and update the following field</p>\n<pre><code>data_home: root\nmeta: your_meta.csv\n\
    </code></pre>\n<p>Step 3. Once you are done with the config file go inside the\
    \ <code>scripts</code> folder and run <code>python train_ptl.py experiment=your_config_file</code>.\n\
    this will start the training.</p>\n<h2>\n<a id=\"user-content-inference-on-your-data\"\
    \ class=\"anchor\" href=\"#inference-on-your-data\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Inference on\
    \ your Data</h2>\n<p>Step 1. Create a csv meta file of for the data you want to\
    \ predict. Use <code>ID</code>, <code>Side</code>, <code>Fname</code>, and <code>Fracture</code>\
    \ columns.</p>\n<p>Step2. Clone <code>fracture_deteciton_testset_1.yaml</code>\
    \ to <code>your_testset.yaml</code></p>\n<p>Step 3. Find and update the following\
    \ field in <code>your_testset.yaml</code></p>\n<pre><code>dataset:\n  train_data_home:\
    \ root\n  test_data_home: /location/of/test/data\n  meta: /absolute/location/to/your_testset.csv\n\
    save_path: /absolute/location/to/save/prediction.csv\nsnapshot_folder: /folder/location/where/fracture/detector/models/are/saved\n\
    save_image: true or false\nsave_image_dir: /folder/location/if/you/want/to/save/output/images\n\
    \nlocalizer:\n  snapshot_folder: /folder/location/where/roi/localizer/models/are/saved\n\
    \  dataset:\n    train_data_home: /folder/location/where/localization/data/are/stored\n\
    </code></pre>\n<p>keep only <code>Fracture</code> in <code>gt</code>. If you want\
    \ to save gradcam set <code>save_gradcam: true</code> and define <code>gradcam_dir</code></p>\n\
    <p>Step 4. Now in the <code>scripts</code> folder run, <code>python test.py experiment=your_testset</code>\n\
    This will do inference on your data, the predicitons will be saved in the csv\
    \ file you defined.</p>\n<h2></h2>\n<h1>\n<a id=\"user-content-trained-models\"\
    \ class=\"anchor\" href=\"#trained-models\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Trained Models</h1>\n<p>Use the\
    \ following commands to get the trained models.</p>\n<pre><code>wget http://mipt-ml.oulu.fi/models/DeepWrist/Fracture_Detection_Block.tar.gz\n\
    wget http://mipt-ml.oulu.fi/models/DeepWrist/ROI_Localization_Block.tar.gz\n</code></pre>\n\
    <h1>\n<a id=\"user-content-how-to-cite\" class=\"anchor\" href=\"#how-to-cite\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to Cite</h1>\n<p>For citation, please use the following bibtex</p>\n\
    <pre><code>@misc{raisuddin2020deep,\n      title={Deep Learning for Wrist Fracture\
    \ Detection: Are We There Yet?}, \n      author={Abu Mohammed Raisuddin and Elias\
    \ Vaattovaara and Mika Nevalainen and Marko Nikki and Elina J\xE4rvenp\xE4\xE4\
    \ and Kaisa Makkonen and Pekka Pinola and Tuula Palsio and Arttu Niemensivu and\
    \ Osmo Tervonen and Aleksei Tiulpin},\n      year={2020},\n      eprint={2012.02577},\n\
    \      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>\n"
  stargazers_count: 1
  subscribers_count: 4
  topics: []
  updated_at: 1614800295.0
MRtrix3/mrtrix3:
  data_format: 2
  description: MRtrix3 provides a set of tools to perform various advanced diffusion
    MRI analyses, including constrained spherical deconvolution (CSD), probabilistic
    tractography, track-density imaging, and apparent fibre density
  filenames:
  - Singularity
  full_name: MRtrix3/mrtrix3
  latest_release: 3.0.2
  readme: "<h1>\n<a id=\"user-content-mrtrix\" class=\"anchor\" href=\"#mrtrix\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>MRtrix</h1>\n\
    <p><a href=\"https://github.com/MRtrix3/mrtrix3/actions\"><img src=\"https://github.com/MRtrix3/mrtrix3/workflows/checks/badge.svg\"\
    \ alt=\"Build Status\" style=\"max-width:100%;\"></a></p>\n<p><em>MRtrix3</em>\
    \ can be installed / run through multiple avenues:</p>\n<ul>\n<li>\n<a href=\"\
    https://www.mrtrix.org/download/\" rel=\"nofollow\">Direct download</a> through\
    \ mechanisms tailored for different OS platforms;</li>\n<li>Compiled from the\
    \ source code in this repository, for which <a href=\"https://mrtrix.readthedocs.io/en/latest/installation/build_from_source.html\"\
    \ rel=\"nofollow\">comprehensive instructions</a> are provided in the <a href=\"\
    https://mrtrix.readthedocs.io/en/\" rel=\"nofollow\">online documentation</a>;</li>\n\
    <li>Via containerisation technology using Docker or Singularity; see <a href=\"\
    https://mrtrix.readthedocs.org/en/latest/installation/using_containers.html\"\
    \ rel=\"nofollow\">online documentation page</a> for details.</li>\n</ul>\n<h2>\n\
    <a id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Getting\
    \ help</h2>\n<p>Instructions on software setup and use are provided in the <a\
    \ href=\"https://mrtrix.readthedocs.org\" rel=\"nofollow\">online documentation</a>.\n\
    Support and general discussion is hosted on the <a href=\"http://community.mrtrix.org/\"\
    \ rel=\"nofollow\"><em>MRtrix3</em> Community Forum</a>.\nPlease also look through\
    \ the Frequently Asked Questions on the <a href=\"http://community.mrtrix.org/c/wiki\"\
    \ rel=\"nofollow\">wiki section of the forum</a>.\nYou can address all <em>MRtrix3</em>-related\
    \ queries there, using your GitHub or Google login to post questions.</p>\n<h2>\n\
    <a id=\"user-content-quick-install\" class=\"anchor\" href=\"#quick-install\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quick install</h2>\n<ol>\n<li>\n<p>Install dependencies by whichever\
    \ means your system uses.\nThese include: Python (&gt;=2.6), a C++ compiler with\
    \ full C++11 support (<code>g++</code> 4.9 or later, <code>clang++</code>),\n\
    Eigen (&gt;=3.2.8), zlib, OpenGL (&gt;=3.3), and Qt (&gt;=4.8, or at least 5.1\
    \ on MacOSX).</p>\n</li>\n<li>\n<p>Clone Git repository and compile:</p>\n<pre><code>\
    \ $ git clone https://github.com/MRtrix3/mrtrix3.git\n $ cd mrtrix3/\n $ ./configure\n\
    \ $ ./build\n</code></pre>\n</li>\n<li>\n<p>Set the <code>PATH</code>:</p>\n<ul>\n\
    <li>\n<p>Bash shell:</p>\n<p>run the <code>set_path</code> script provided:</p>\n\
    <pre><code>  $ ./set_path\n</code></pre>\n<p>or edit the startup <code>~/.bashrc</code>\
    \ or <code>/etc/bash.bashrc</code> file manually by adding this line:</p>\n<pre><code>\
    \  $ export PATH=/&lt;edit as appropriate&gt;/mrtrix3/bin:$PATH\n</code></pre>\n\
    </li>\n<li>\n<p>C shell:</p>\n<p>edit the startup <code>~/.cshrc</code> or <code>/etc/csh.cshrc</code>\
    \ file manually by adding this line:</p>\n<pre><code>  $ setenv PATH /&lt;edit\
    \ as appropriate&gt;/mrtrix3/bin:$PATH\n</code></pre>\n</li>\n</ul>\n</li>\n<li>\n\
    <p>Test installation:</p>\n<p>Command-line:</p>\n<pre><code> $ mrconvert\n</code></pre>\n\
    <p>GUI:</p>\n<pre><code> $ mrview\n</code></pre>\n</li>\n</ol>\n<h2>\n<a id=\"\
    user-content-keeping-mrtrix3-up-to-date\" class=\"anchor\" href=\"#keeping-mrtrix3-up-to-date\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Keeping MRtrix3 up to date</h2>\n<ol>\n<li>\n<p>You can update your\
    \ installation at any time by opening a terminal in the mrtrix3 folder, and typing:</p>\n\
    <pre><code> git pull\n ./build\n</code></pre>\n</li>\n<li>\n<p>If this doesn't\
    \ work immediately, it may be that you need to re-run the configure script:</p>\n\
    <pre><code> ./configure\n</code></pre>\n<p>and re-run step 1 again.</p>\n</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-building-a-specific-release-of-mrtrix3\" class=\"\
    anchor\" href=\"#building-a-specific-release-of-mrtrix3\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ a specific release of MRtrix3</h2>\n<p>You can build a particular release of\
    \ MRtrix3 by checking out the corresponding <em>tag</em>, and using the same procedure\
    \ as above to build it:</p>\n<pre><code>git checkout 3.0_RC3\n./configure\n./build\n\
    </code></pre>\n<h2>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"\
    #contributing\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Contributing</h2>\n<p>Thank you for your interest\
    \ in contributing to <em>MRtrix3</em>! Please read on <a href=\"CONTRIBUTING.md\"\
    >here</a> to find out how to report issues, request features and make direct contributions.</p>\n"
  stargazers_count: 181
  subscribers_count: 35
  topics: []
  updated_at: 1621885646.0
MarioProjects/MnMsCardiac:
  data_format: 2
  description: 4th place solution of Multi-Centre, Multi-Vendor, Multi-Disease Cardiac
    Image Segmentation Challenge
  filenames:
  - attempts/attempt3/Singularity_v3
  - attempts/attempt1/Singularity_v1
  - attempts/attempt7/Singularity_v7
  - attempts/attempt5/Singularity_v5
  - attempts/attempt6/Singularity_v6
  - attempts/attempt4/Singularity_v4
  - attempts/attempt2/Singularity_v2
  full_name: MarioProjects/MnMsCardiac
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-mms-challenge-2020\" class=\"anchor\" href=\"\
    #mms-challenge-2020\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>M&amp;Ms Challenge 2020</h1>\n<p>The CMR images\
    \ have been segmented by experienced clinicians from the respective institutions,\
    \ including contours\nfor the left (LV) and right ventricle (RV) blood pools,\
    \ as well as for the left ventricular myocardium (MYO).\nLabels are: 1 (LV), 2\
    \ (MYO) and 3 (RV)</p>\n<h2>\n<a id=\"user-content-motivation\" class=\"anchor\"\
    \ href=\"#motivation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Motivation</h2>\n<p>In the recent years, many\
    \ machine/deep learning models have been proposed to accurately segment cardiac\
    \ structures\nin magnetic resonance imaging. However, when these models are tested\
    \ on unseen datasets acquired from distinct\nMRI scanners or clinical centres,\
    \ the segmentation accuracy can be greatly reduced.</p>\n<p>The M&amp;Ms challenge\
    \ aims to contribute to the effort of building generalisable models that can be\
    \ applied consistently\nacross clinical centres. Furthermore, M&amp;Ms will provide\
    \ a reference dataset for the community to build and assess\nfuture generalisable\
    \ models in CMR segmentation.</p>\n<h2>\n<a id=\"user-content-environment-setup\"\
    \ class=\"anchor\" href=\"#environment-setup\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Environment Setup</h2>\n<p>To\
    \ use the code, the user needs to set te environment variable to access the data.\
    \ At your ~/.bashrc add:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-k\">export</span> MMsCardiac_DATA_PATH=<span class=\"pl-s\"\
    ><span class=\"pl-pds\">'</span>/path/to/data/M&amp;MsData/<span class=\"pl-pds\"\
    >'</span></span></pre></div>\n<p>Also, the user needs to to pre-install a few\
    \ packages:</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ pip install\
    \ wheel setuptools\n$ pip install -r requirements.txt\n$ pip install torch==1.5.0+cu101\
    \ torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n\
    $ pip install torchcontrib~=0.0.2</pre></div>\n<h3>\n<a id=\"user-content-data-preparation\"\
    \ class=\"anchor\" href=\"#data-preparation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Data preparation</h3>\n<h4>\n\
    <a id=\"user-content-train-csv\" class=\"anchor\" href=\"#train-csv\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Train\
    \ csv</h4>\n<p>You can generate train csv for dataloaders using <code>python3\
    \ preprocess/generate_train_df.py</code>.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>usage: generate_train_df.py [-h] [--meta_graphs]\n\nM<span class=\"pl-k\"\
    >&amp;</span>Ms 2020 Challenge - Training info generation\n\noptional arguments:\n\
    \  -h, --help     show this <span class=\"pl-c1\">help</span> message and <span\
    \ class=\"pl-c1\">exit</span>\n  --meta_graphs  Generate train meta information\
    \ graphs</pre></div>\n<h4>\n<a id=\"user-content-data-refactor\" class=\"anchor\"\
    \ href=\"#data-refactor\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Data Refactor</h4>\n<p>Load each volume to extract\
    \ only 1 slice is time consuming. To solve this, save each slice in numpy arrays:\n\
    <code>python3 preprocess/dataloader_refactor.py</code></p>\n<h4>\n<a id=\"user-content-global-training-mean-and-std\"\
    \ class=\"anchor\" href=\"#global-training-mean-and-std\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Global Training\
    \ Mean and STD</h4>\n<p>You can easily get global mean and std from labeled training\
    \ samples using <code>python3 preprocess/get_mean_std.py</code>.</p>\n<h2>\n<a\
    \ id=\"user-content-data-description\" class=\"anchor\" href=\"#data-description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Data Description</h2>\n<p>The challenge cohort is composed of 350\
    \ patients with hypertrophic and dilated cardiomyopathies\nas well as healthy\
    \ subjects. All subjects were scanned in clinical centres in three different\n\
    countries (Spain, Germany and Canada) using four different magnetic resonance\n\
    scanner vendors (Siemens, General Electric, Philips and Canon).</p>\n<table>\n\
    <thead>\n<tr>\n<th align=\"center\">Hospital</th>\n<th align=\"center\">Num. studies</th>\n\
    <th align=\"center\">Country</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"\
    center\">Clinica Sagrada Familia</td>\n<td align=\"center\">50</td>\n<td align=\"\
    center\">Spain</td>\n</tr>\n<tr>\n<td align=\"center\">Hospital de la Santa Creu\
    \ i Sant Pau</td>\n<td align=\"center\">50</td>\n<td align=\"center\">Spain</td>\n\
    </tr>\n<tr>\n<td align=\"center\">Hospital Universitari Dexeus</td>\n<td align=\"\
    center\">50</td>\n<td align=\"center\">Spain</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >Hospital Vall d'Hebron</td>\n<td align=\"center\">100</td>\n<td align=\"center\"\
    >Spain</td>\n</tr>\n<tr>\n<td align=\"center\">McGill University Health Centre</td>\n\
    <td align=\"center\">50</td>\n<td align=\"center\">Canada</td>\n</tr>\n<tr>\n\
    <td align=\"center\">Universit\xE4tsklinikum Hamburg-Eppendorf</td>\n<td align=\"\
    center\">50</td>\n<td align=\"center\">Germany</td>\n</tr>\n</tbody>\n</table>\n\
    <h3>\n<a id=\"user-content-training-set-15025-studies\" class=\"anchor\" href=\"\
    #training-set-15025-studies\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Training set (150+25 studies)</h3>\n\
    <p>The training set will contain 150 annotated images from two different MRI vendors\
    \ (75 each) and 25 unannotated\nimages from a third vendor. The CMR images have\
    \ been segmented by experienced clinicians from the respective\ninstitutions,\
    \ including contours for the left (LV) and right ventricle (RV) blood pools, as\
    \ well as for the\nleft ventricular myocardium (MYO). Labels are: 1 (LV), 2 (MYO)\
    \ and 3 (RV).</p>\n<h3>\n<a id=\"user-content-testing-set-200-studies\" class=\"\
    anchor\" href=\"#testing-set-200-studies\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Testing set (200 studies)</h3>\n\
    <p>The 200 test cases correspond to 50 new studies from each of the vendors provided\
    \ in the training set and\n50 additional studies from a fourth unseen vendor,\
    \ that will be tested for model generalizability.\n20% of these datasets will\
    \ be used for validation and the rest will be reserved for testing and ranking\
    \ participants.</p>\n<h3>\n<a id=\"user-content-standard-operating-procedure-sop-for-data-annotation\"\
    \ class=\"anchor\" href=\"#standard-operating-procedure-sop-for-data-annotation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Standard Operating Procedure (SOP) for data annotation</h3>\n<p>In\
    \ order to build a useful dataset for the community we have decided to build on\
    \ top of\n<a href=\"https://ieeexplore.ieee.org/document/8360453\" rel=\"nofollow\"\
    >ACDC MICCAI 2017</a> challenge SOP and correct our contours accordingly.</p>\n\
    <p>In particular, clinical contours have been corrected by two in-house annotators\
    \ that had to agree on the final result.\nThese annotators followed these rules:</p>\n\
    <ul>\n<li>LV and RV cavities must be completely covered, with papillary muscles\
    \ included.</li>\n<li>No interpolation of the LV myocardium must be performed\
    \ at the base.</li>\n<li>RV must have a larger surface in end-diastole compared\
    \ to end-systole and avoid the pulmonary artery.</li>\n</ul>\n<p>The main difficulty\
    \ and source of disagreement is the exact RV form in basal slices.</p>\n<h2>\n\
    <a id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Results</h2>\n\
    <p>Using ACDC checkpoint:</p>\n<p>Average -&gt; 0.7397 -&gt; 0.9933 (background),\
    \ 0.6931 (LV), 0.5624 (MYO), 0.71(RV)</p>\n<p>Calculated using resnet34_unet_imagenet_encoder,\
    \ Adam and constant learning rate. Fold metrics are calculated\nusing mean of\
    \ averaged iou and dice values. Only mnms data.</p>\n<table>\n<thead>\n<tr>\n\
    <th align=\"center\">Method</th>\n<th>Normalization</th>\n<th align=\"center\"\
    >Fold 0</th>\n<th align=\"center\">Fold 1</th>\n<th>Fold 2</th>\n<th>Fold 3</th>\n\
    <th>Fold 4</th>\n<th>Mean</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.01</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.7958</td>\n<td align=\"center\">0.8272</td>\n<td>0.8064</td>\n\
    <td>0.8107</td>\n<td>0.8220</td>\n<td>0.8124</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.001</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.8163</td>\n<td align=\"center\">0.8384</td>\n<td>0.8382</td>\n\
    <td>0.8336</td>\n<td>0.8498</td>\n<td>0.8352</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.0001</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.8066</td>\n<td align=\"center\">0.8359</td>\n<td>0.8235</td>\n\
    <td>0.8281</td>\n<td>0.8310</td>\n<td>0.8250</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.01</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.7711</td>\n<td align=\"center\">0.7745</td>\n<td>0.7993</td>\n\
    <td>0.8248</td>\n<td>0.7791</td>\n<td>0.7897</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.001</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.8058</td>\n<td align=\"center\">0.8324</td>\n<td>0.8322</td>\n\
    <td>0.8138</td>\n<td>0.8433</td>\n<td>0.8254</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.0001</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.7970</td>\n<td align=\"center\">0.8382</td>\n<td>0.8212</td>\n\
    <td>0.8313</td>\n<td>0.8344</td>\n<td>0.8244</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.01</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.7977</td>\n<td align=\"center\">0.8150</td>\n<td>0.8053</td>\n\
    <td>0.8188</td>\n<td>0.8212</td>\n<td>0.8116</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.001</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.8184</td>\n<td align=\"center\">0.8400</td>\n<td>0.8339</td>\n\
    <td>0.8408</td>\n<td>0.8469</td>\n<td>0.8360</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.0001</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.8096</td>\n<td align=\"center\">0.8377</td>\n<td>0.8230</td>\n\
    <td>0.8286</td>\n<td>0.8316</td>\n<td>0.8261</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.01</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.7842</td>\n<td align=\"center\">0.8373</td>\n<td>0.8254</td>\n\
    <td>0.8333</td>\n<td>0.8318</td>\n<td>0.8224</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.001</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.8235</td>\n<td align=\"center\">0.8556</td>\n<td>0.7736</td>\n\
    <td>0.8477</td>\n<td>0.8598</td>\n<td>0.8320</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.0001</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.8221</td>\n<td align=\"center\">0.8494</td>\n<td>0.8349</td>\n\
    <td>0.8453</td>\n<td>0.8503</td>\n<td>0.8404</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.01</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.7783</td>\n<td align=\"center\">0.8101</td>\n<td>0.8041</td>\n\
    <td>0.8021</td>\n<td>0.8331</td>\n<td>0.8055</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.001</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.8162</td>\n<td align=\"center\">0.8378</td>\n<td>0.8330</td>\n\
    <td>0.8322</td>\n<td>0.8456</td>\n<td>0.8329</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.0001</td>\n<td>Reescale</td>\n\
    <td align=\"center\">0.7971</td>\n<td align=\"center\">0.8328</td>\n<td>0.8065</td>\n\
    <td>0.8251</td>\n<td>0.8291</td>\n<td>0.8181</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.01</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.7893</td>\n<td align=\"center\">0.7775</td>\n<td>0.7257</td>\n\
    <td>0.8152</td>\n<td>0.8162</td>\n<td>0.7847</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.001</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.8091</td>\n<td align=\"center\">0.8367</td>\n<td>0.8204</td>\n\
    <td>0.8215</td>\n<td>0.8436</td>\n<td>0.8262</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_border_ce -&gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.0001</td>\n<td>Standardize</td>\n\
    <td align=\"center\">0.7320</td>\n<td align=\"center\">0.8234</td>\n<td>0.7945</td>\n\
    <td>0.8245</td>\n<td>0.8173</td>\n<td>0.7983</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_ce -&gt; 0.5,0.3,0.2,0.65 - lr 0.001</td>\n<td>Standardize</td>\n<td\
    \ align=\"center\">0.7962</td>\n<td align=\"center\">0.8384</td>\n<td>0.8157</td>\n\
    <td>0.8053</td>\n<td>0.8181</td>\n<td>0.8147</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >bce_dice_ce -&gt; 0.5,0.3,0.2,0.65 - lr 0.0001</td>\n<td>Standardize</td>\n<td\
    \ align=\"center\">0.7915</td>\n<td align=\"center\">0.8398</td>\n<td>0.8148</td>\n\
    <td>0.8291</td>\n<td>0.8244</td>\n<td>0.8199</td>\n</tr>\n</tbody>\n</table>\n\
    <p>Principal conclusions: bce_dice_border_ce with 0.5,0.2,0.2,0.2,0.5 - lr 0.001/0.0001\
    \ - standardize.</p>\n<p>Now, using lr 0.001, standardize and bce_dice_border_ce\
    \ with 0.5,0.2,0.2,0.2,0.5, explore data augmentation.\nWithout data augmentation\
    \ score 0.8360.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Data Augmentation</th>\n\
    <th align=\"center\">Fold 0</th>\n<th align=\"center\">Fold 1</th>\n<th>Fold 2</th>\n\
    <th>Fold 3</th>\n<th>Fold 4</th>\n<th>Mean</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n\
    <td align=\"center\">Vertical flip</td>\n<td align=\"center\">0.8004</td>\n<td\
    \ align=\"center\">0.8273</td>\n<td>0.8176</td>\n<td>0.8074</td>\n<td>0.8386</td>\n\
    <td>0.8182</td>\n</tr>\n<tr>\n<td align=\"center\">Horizontal flip</td>\n<td align=\"\
    center\">0.8032</td>\n<td align=\"center\">0.8225</td>\n<td>0.8226</td>\n<td>0.8244</td>\n\
    <td>0.8318</td>\n<td>0.8209</td>\n</tr>\n<tr>\n<td align=\"center\">Random Crops</td>\n\
    <td align=\"center\">0.8137</td>\n<td align=\"center\">0.8376</td>\n<td>0.8208</td>\n\
    <td>0.8283</td>\n<td>0.7876</td>\n<td>0.8181</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >Shift</td>\n<td align=\"center\">0.8117</td>\n<td align=\"center\">0.8240</td>\n\
    <td>0.8222</td>\n<td>0.8330</td>\n<td>0.8307</td>\n<td>0.8243</td>\n</tr>\n<tr>\n\
    <td align=\"center\">Downscale</td>\n<td align=\"center\">0.7949</td>\n<td align=\"\
    center\">0.8192</td>\n<td>0.8166</td>\n<td>0.8219</td>\n<td>0.8384</td>\n<td>0.8181</td>\n\
    </tr>\n<tr>\n<td align=\"center\">Elastic Transform</td>\n<td align=\"center\"\
    >0.7991</td>\n<td align=\"center\">0.8425</td>\n<td>0.8274</td>\n<td>0.8213</td>\n\
    <td>0.8408</td>\n<td>0.8262</td>\n</tr>\n<tr>\n<td align=\"center\">Rotations</td>\n\
    <td align=\"center\">0.8158</td>\n<td align=\"center\">0.8426</td>\n<td>0.8255</td>\n\
    <td>0.8290</td>\n<td>0.8524</td>\n<td>0.8330</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >Grid Distortion</td>\n<td align=\"center\">0.8028</td>\n<td align=\"center\"\
    >0.8361</td>\n<td>0.7864</td>\n<td>0.8275</td>\n<td>0.8231</td>\n<td>0.8151</td>\n\
    </tr>\n<tr>\n<td align=\"center\">Optical Distortion</td>\n<td align=\"center\"\
    >0.7705</td>\n<td align=\"center\">0.8418</td>\n<td>0.8255</td>\n<td>0.7996</td>\n\
    <td>0.8354</td>\n<td>0.8145</td>\n</tr>\n</tbody>\n</table>\n<h3>\n<a id=\"user-content-competition-models\"\
    \ class=\"anchor\" href=\"#competition-models\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Competition Models</h3>\n<h4>\n\
    <a id=\"user-content-bala-1\" class=\"anchor\" href=\"#bala-1\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><em>Bala\
    \ 1</em>\n</h4>\n<p>Using standardization, data augmentation combination old and\
    \ bce_dice_border_ce with 0.5,0.2,0.2,0.2,0.5.\nResnet34 Unet with lr 0.001 and\
    \ adam optimizer.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Method</th>\n\
    <th align=\"center\">Fold 0</th>\n<th align=\"center\">Fold 1</th>\n<th>Fold 2</th>\n\
    <th>Fold 3</th>\n<th>Mean</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"\
    >weakly -&gt; labeled</td>\n<td align=\"center\">0.8286</td>\n<td align=\"center\"\
    >0.8596</td>\n<td>0.8505</td>\n<td>0.8540</td>\n<td>0.8482</td>\n</tr>\n<tr>\n\
    <td align=\"center\">combined -&gt; labeled</td>\n<td align=\"center\">0.8271</td>\n\
    <td align=\"center\">0.8473</td>\n<td>0.8424</td>\n<td>0.8573</td>\n<td>0.8435</td>\n\
    </tr>\n</tbody>\n</table>\n<h4>\n<a id=\"user-content-bala-2\" class=\"anchor\"\
    \ href=\"#bala-2\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a><em>Bala 2</em>\n</h4>\n<p>Using standardization,\
    \ data augmentation combination old and bce_dice_border_ce with 0.5,0.2,0.2,0.2,0.5</p>\n\
    <table>\n<thead>\n<tr>\n<th align=\"center\">Method</th>\n<th align=\"center\"\
    >Fold 0</th>\n<th align=\"center\">Fold 1</th>\n<th>Fold 2</th>\n<th>Fold 3</th>\n\
    <th>Fold 4</th>\n<th>Mean</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"\
    >Resnet34 Unet lr 0.001</td>\n<td align=\"center\">0.8092</td>\n<td align=\"center\"\
    >0.8257</td>\n<td>0.8115</td>\n<td>0.8293</td>\n<td>0.8276</td>\n<td>0.8207</td>\n\
    </tr>\n</tbody>\n</table>\n<h3>\n<a id=\"user-content-not-pretrained-model\" class=\"\
    anchor\" href=\"#not-pretrained-model\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Not Pretrained Model</h3>\n<p>Folding\
    \ by patient.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Method</th>\n\
    <th>Normalization</th>\n<th align=\"center\">Fold 0</th>\n<th align=\"center\"\
    >Fold 1</th>\n<th>Fold 2</th>\n<th>Fold 3</th>\n<th>Fold 4</th>\n<th>Mean</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">bce_dice_border_ce -&gt;\
    \ 0.5,0.2,0.2,0.2,0.65 - lr 0.01</td>\n<td>Standardize</td>\n<td align=\"center\"\
    >0.7873</td>\n<td align=\"center\">0.8263</td>\n<td>0.8004</td>\n<td>0.8195</td>\n\
    <td>0.7616</td>\n<td>0.7990</td>\n</tr>\n<tr>\n<td align=\"center\">bce_dice_border_ce\
    \ -&gt; 0.5,0.2,0.2,0.2,0.65 - lr 0.001</td>\n<td>Standardize</td>\n<td align=\"\
    center\">0.7741</td>\n<td align=\"center\">0.7879</td>\n<td>0.7743</td>\n<td>0.7883</td>\n\
    <td>0.8071</td>\n<td>0.7863</td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-update-11062020-meeting\"\
    \ class=\"anchor\" href=\"#update-11062020-meeting\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Update: 11/06/2020\
    \ Meeting</h2>\n<p>Changes and ideas:</p>\n<ul>\n<li>[x] Use 2 folds grouping\
    \ by vendor (A vs. B), instead of <em>n</em> grouping by patient. Then error analysis\
    \ by vendor</li>\n<li>[x] Since is not permited the use of pre-trained models,\
    \ try smaller architectures</li>\n<li>[ ] Create convolutional network that learns\
    \ to distinguish if an image comes from vendor A or vendor B. \xBFWorks?\n<ul>\n\
    <li>If works then we can create a DCGAN trying to apply a initial transformation\
    \ to fool the discriminator and\ndo something like normalize the input images!\
    \ <strong>Note</strong>: Do not add vendor C in CNN classification step since\n\
    we will use it for validate our GAN later.</li>\n</ul>\n</li>\n<li>[ ] Self-Supervised\
    \ Learning for unseen vendor C</li>\n</ul>\n<h2>\n<a id=\"user-content-folding-by-vendor-resuts\"\
    \ class=\"anchor\" href=\"#folding-by-vendor-resuts\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Folding by Vendor\
    \ Resuts</h2>\n<h4>\n<a id=\"user-content-wrong-folding-no-train-subpartitionpatients-to-compare\"\
    \ class=\"anchor\" href=\"#wrong-folding-no-train-subpartitionpatients-to-compare\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>(Wrong folding, no train subpartition/patients to compare)</h4>\n\
    <p>Normalization by reescale. Criterion bce_dice_border_ce -&gt; 0.5,0.2,0.2,0.2,0.5.</p>\n\
    <table>\n<thead>\n<tr>\n<th align=\"center\">Method</th>\n<th align=\"center\"\
    >DA</th>\n<th align=\"center\">A -&gt; B</th>\n<th align=\"center\">B -&gt; A</th>\n\
    <th>Mean</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">resnet18_pspnet_unet\
    \ - lr 0.001</td>\n<td align=\"center\">None</td>\n<td align=\"center\">0.7573</td>\n\
    <td align=\"center\">0.7121</td>\n<td>0.7346</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >resnet18_pspnet_unet - lr 0.0001</td>\n<td align=\"center\">None</td>\n<td align=\"\
    center\">0.6838</td>\n<td align=\"center\">0.5532</td>\n<td>0.6185</td>\n</tr>\n\
    <tr>\n<td align=\"center\">resnet18_pspnet_unet - lr 0.001</td>\n<td align=\"\
    center\">Combination</td>\n<td align=\"center\">0.7612</td>\n<td align=\"center\"\
    >0.6793</td>\n<td>0.7202</td>\n</tr>\n<tr>\n<td align=\"center\">resnet18_pspnet_unet\
    \ - lr 0.0001</td>\n<td align=\"center\">Combination</td>\n<td align=\"center\"\
    >0.6982</td>\n<td align=\"center\">0.5580</td>\n<td>0.6281</td>\n</tr>\n<tr>\n\
    <td align=\"center\">resnet18_unet_scratch - lr 0.001</td>\n<td align=\"center\"\
    >None</td>\n<td align=\"center\">0.7498</td>\n<td align=\"center\">0.6835</td>\n\
    <td>0.7166</td>\n</tr>\n<tr>\n<td align=\"center\">resnet18_unet_scratch - lr\
    \ 0.0001</td>\n<td align=\"center\">None</td>\n<td align=\"center\">0.6779</td>\n\
    <td align=\"center\">0.4997</td>\n<td>0.5888</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >resnet18_unet_scratch - lr 0.001</td>\n<td align=\"center\">Combination</td>\n\
    <td align=\"center\">0.7421</td>\n<td align=\"center\">0.6627</td>\n<td>0.7023</td>\n\
    </tr>\n<tr>\n<td align=\"center\">resnet18_unet_scratch - lr 0.0001</td>\n<td\
    \ align=\"center\">Combination</td>\n<td align=\"center\">0.7588</td>\n<td align=\"\
    center\">0.6281</td>\n<td>0.6934</td>\n</tr>\n<tr>\n<td align=\"center\">resnet34_unet_scratch\
    \ - lr 0.001</td>\n<td align=\"center\">None</td>\n<td align=\"center\">0.7649</td>\n\
    <td align=\"center\">0.6313</td>\n<td>0.6980</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >resnet34_unet_scratch - lr 0.0001</td>\n<td align=\"center\">None</td>\n<td align=\"\
    center\">0.7189</td>\n<td align=\"center\">0.6273</td>\n<td>0.6731</td>\n</tr>\n\
    <tr>\n<td align=\"center\">resnet34_unet_scratch - lr 0.001</td>\n<td align=\"\
    center\">Combination</td>\n<td align=\"center\">0.7673</td>\n<td align=\"center\"\
    >0.6530</td>\n<td>0.7101</td>\n</tr>\n<tr>\n<td align=\"center\">resnet34_unet_scratch\
    \ - lr 0.0001</td>\n<td align=\"center\">Combination</td>\n<td align=\"center\"\
    >0.7707</td>\n<td align=\"center\">0.6128</td>\n<td>0.6917</td>\n</tr>\n<tr>\n\
    <td align=\"center\">nano_unet - lr 0.001</td>\n<td align=\"center\">None</td>\n\
    <td align=\"center\">0.5035</td>\n<td align=\"center\">0.4284</td>\n<td>0.4659</td>\n\
    </tr>\n<tr>\n<td align=\"center\">nano_unet - lr 0.0001</td>\n<td align=\"center\"\
    >None</td>\n<td align=\"center\">0.4432</td>\n<td align=\"center\">0.2821</td>\n\
    <td>0.3626</td>\n</tr>\n<tr>\n<td align=\"center\">nano_unet - lr 0.001</td>\n\
    <td align=\"center\">Combination</td>\n<td align=\"center\">0.4871</td>\n<td align=\"\
    center\">0.4771</td>\n<td>0.4821</td>\n</tr>\n<tr>\n<td align=\"center\">nano_unet\
    \ - lr 0.0001</td>\n<td align=\"center\">Combination</td>\n<td align=\"center\"\
    >0.4310</td>\n<td align=\"center\">0.2187</td>\n<td>0.3248</td>\n</tr>\n</tbody>\n\
    </table>\n<p>General conclusions:</p>\n<ul>\n<li>Models can extract more information\
    \ and thus make better predictions when training with Vendor 'A'\nand then testing\
    \ on 'B'. GAN should approximate images to Vendor A?</li>\n<li>lr 0.001 works\
    \ better than lower ones.</li>\n<li>Not clear difference using data augmentation\
    \ and without apply it...</li>\n<li>Intermediate models size, resnet18_pspnet_unet,\
    \ performs better than bigger ones and smaller ones.</li>\n</ul>\n<h4>\n<a id=\"\
    user-content-11-random-patients-to-compare\" class=\"anchor\" href=\"#11-random-patients-to-compare\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>11 random patients to compare</h4>\n<p>Criterion bce_dice_border_ce\
    \ -&gt; 0.5,0.2,0.2,0.2,0.5. Using resnet18_pspnet_unet.</p>\n<table>\n<thead>\n\
    <tr>\n<th align=\"center\">Normalization</th>\n<th align=\"center\">Data Augmentation</th>\n\
    <th align=\"center\">Learning Rate</th>\n<th align=\"center\">A -&gt; B</th>\n\
    <th align=\"center\">B -&gt; A</th>\n<th>Mean</th>\n</tr>\n</thead>\n<tbody>\n\
    <tr>\n<td align=\"center\">Reescale</td>\n<td align=\"center\">Combination (Old)</td>\n\
    <td align=\"center\">0.001</td>\n<td align=\"center\">0.7328</td>\n<td align=\"\
    center\">0.6915</td>\n<td>0.7121</td>\n</tr>\n<tr>\n<td align=\"center\">Standardize</td>\n\
    <td align=\"center\">Combination (Old)</td>\n<td align=\"center\">0.001</td>\n\
    <td align=\"center\">0.7601</td>\n<td align=\"center\">0.6704</td>\n<td>0.7152</td>\n\
    </tr>\n<tr>\n<td align=\"center\">Reescale</td>\n<td align=\"center\">Combination\
    \ (Old)</td>\n<td align=\"center\">0.005</td>\n<td align=\"center\">0.6593</td>\n\
    <td align=\"center\">0.4914</td>\n<td>0.5753</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >Standardize</td>\n<td align=\"center\">Combination (Old)</td>\n<td align=\"center\"\
    >0.005</td>\n<td align=\"center\">0.7499</td>\n<td align=\"center\">0.6342</td>\n\
    <td>0.6920</td>\n</tr>\n<tr>\n<td align=\"center\">Reescale</td>\n<td align=\"\
    center\">Combination</td>\n<td align=\"center\">0.001</td>\n<td align=\"center\"\
    >0.7502</td>\n<td align=\"center\">0.7014</td>\n<td>0.7258</td>\n</tr>\n<tr>\n\
    <td align=\"center\">Standardize</td>\n<td align=\"center\">Combination</td>\n\
    <td align=\"center\">0.001</td>\n<td align=\"center\">0.7561</td>\n<td align=\"\
    center\">0.6723</td>\n<td>0.7142</td>\n</tr>\n<tr>\n<td align=\"center\">Reescale</td>\n\
    <td align=\"center\">Combination</td>\n<td align=\"center\">0.005</td>\n<td align=\"\
    center\">0.7370</td>\n<td align=\"center\">0.5143</td>\n<td>0.6257</td>\n</tr>\n\
    <tr>\n<td align=\"center\">Standardize</td>\n<td align=\"center\">Combination</td>\n\
    <td align=\"center\">0.005</td>\n<td align=\"center\">0.7123</td>\n<td align=\"\
    center\">0.6826</td>\n<td>0.6975</td>\n</tr>\n<tr>\n<td align=\"center\">Reescale</td>\n\
    <td align=\"center\">None</td>\n<td align=\"center\">0.001</td>\n<td align=\"\
    center\">0.7462</td>\n<td align=\"center\">0.7283</td>\n<td>0.7372</td>\n</tr>\n\
    <tr>\n<td align=\"center\">Standardize</td>\n<td align=\"center\">None</td>\n\
    <td align=\"center\">0.001</td>\n<td align=\"center\">0.7668</td>\n<td align=\"\
    center\">0.6312</td>\n<td>0.6990</td>\n</tr>\n<tr>\n<td align=\"center\">Reescale</td>\n\
    <td align=\"center\">None</td>\n<td align=\"center\">0.005</td>\n<td align=\"\
    center\">0.7098</td>\n<td align=\"center\">0.6280</td>\n<td>0.6689</td>\n</tr>\n\
    <tr>\n<td align=\"center\">Standardize</td>\n<td align=\"center\">None</td>\n\
    <td align=\"center\">0.005</td>\n<td align=\"center\">0.7606</td>\n<td align=\"\
    center\">0.6604</td>\n<td>0.7105</td>\n</tr>\n</tbody>\n</table>\n<p>General conclusions:</p>\n\
    <ul>\n<li>When using Vendor A as training set, generalizes better to Vendor B\
    \ cases.</li>\n</ul>\n<h2>\n<a id=\"user-content-classification-vendor-a---b-discriminator\"\
    \ class=\"anchor\" href=\"#classification-vendor-a---b-discriminator\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Classification:\
    \ Vendor 'A' - 'B' Discriminator</h2>\n<p>Using resnet18_pspnet_classification\
    \ model. Adam with bce. 60 epochs and *0.1 steps as 25 and 50.\nImg size 224x224.\
    \ fold_system=\"patient\" &amp; label_type=\"vendor_label\". Normalization standardize.\
    \ Learning rate 0.001.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Data\
    \ Augmentation</th>\n<th align=\"center\">Fold 0</th>\n<th align=\"center\">Fold\
    \ 1</th>\n<th>Fold 2</th>\n<th>Fold 3</th>\n<th>Fold 4</th>\n<th>Mean</th>\n</tr>\n\
    </thead>\n<tbody>\n<tr>\n<td align=\"center\">None</td>\n<td align=\"center\"\
    >0.9954</td>\n<td align=\"center\">0.9726</td>\n<td>1.0000</td>\n<td>0.9878</td>\n\
    <td>0.9970</td>\n<td>0.9906</td>\n</tr>\n<tr>\n<td align=\"center\">Combination</td>\n\
    <td align=\"center\">0.9954</td>\n<td align=\"center\">0.9771</td>\n<td>0.9985</td>\n\
    <td>1.0000</td>\n<td>0.9939</td>\n<td>0.9930</td>\n</tr>\n</tbody>\n</table>\n\
    <h2>\n<a id=\"user-content-classification-vendor-a---b---c-discriminator\" class=\"\
    anchor\" href=\"#classification-vendor-a---b---c-discriminator\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Classification:\
    \ Vendor 'A' - 'B' - 'C' Discriminator</h2>\n<p>Adam with bce. 80 epochs and *0.1\
    \ steps as 25 and 60.\nImg size 224x224. fold_system=\"patient\" &amp; label_type=\"\
    vendor_label\".  Normalization standardize. Learning rate 0.001.\nData Augmentation\
    \ combination (old).</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Model</th>\n\
    <th align=\"center\">Fold 0</th>\n<th align=\"center\">Fold 1</th>\n<th>Fold 2</th>\n\
    <th>Fold 3</th>\n<th>Fold 4</th>\n<th>Mean</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n\
    <td align=\"center\">resnet34_pspnet</td>\n<td align=\"center\">0.9954</td>\n\
    <td align=\"center\">0.9726</td>\n<td>1.0000</td>\n<td>0.9878</td>\n<td>0.9970</td>\n\
    <td>0.9906</td>\n</tr>\n<tr>\n<td align=\"center\">resnet34_pspnet</td>\n<td align=\"\
    center\">0.9954</td>\n<td align=\"center\">0.9771</td>\n<td>0.9985</td>\n<td>1.0000</td>\n\
    <td>0.9939</td>\n<td>0.9930</td>\n</tr>\n<tr>\n<td align=\"center\">resnet34_unet</td>\n\
    <td align=\"center\">0.9910</td>\n<td align=\"center\">0.9871</td>\n<td>1.0000</td>\n\
    <td>0.9740</td>\n<td>0.9805</td>\n<td>0.9865</td>\n</tr>\n</tbody>\n</table>\n\
    <h2>\n<a id=\"user-content-discriminator-entropy-backwards-a---b---c\" class=\"\
    anchor\" href=\"#discriminator-entropy-backwards-a---b---c\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Discriminator\
    \ Entropy backwards 'A' - 'B' - 'C'</h2>\n<p>Using gradient gamma 0.99, max iterations\
    \ 250, standardize normalization. Segmentator Training with 'A'.\nBaseline: 0.7799\
    \ IOU on B.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Out threshold</th>\n\
    <th align=\"center\">Target</th>\n<th align=\"center\">More</th>\n<th align=\"\
    center\">B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">0.01</td>\n\
    <td align=\"center\">A</td>\n<td align=\"center\">----</td>\n<td align=\"center\"\
    >0.7827</td>\n</tr>\n<tr>\n<td align=\"center\">0.01</td>\n<td align=\"center\"\
    >A</td>\n<td align=\"center\">L1 2.0</td>\n<td align=\"center\">0.7825</td>\n\
    </tr>\n<tr>\n<td align=\"center\">0.01</td>\n<td align=\"center\">A</td>\n<td\
    \ align=\"center\">L1 5.0</td>\n<td align=\"center\">0.7827</td>\n</tr>\n<tr>\n\
    <td align=\"center\">0.01</td>\n<td align=\"center\">A</td>\n<td align=\"center\"\
    >L1 10.0</td>\n<td align=\"center\">0.7829</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >0.01</td>\n<td align=\"center\">Equal</td>\n<td align=\"center\">----</td>\n\
    <td align=\"center\">0.7713</td>\n</tr>\n<tr>\n<td align=\"center\">0.01</td>\n\
    <td align=\"center\">Equal</td>\n<td align=\"center\">L1 2.0</td>\n<td align=\"\
    center\">0.7723</td>\n</tr>\n<tr>\n<td align=\"center\">0.01</td>\n<td align=\"\
    center\">Equal</td>\n<td align=\"center\">L1 5.0</td>\n<td align=\"center\">0.7725</td>\n\
    </tr>\n<tr>\n<td align=\"center\">0.01</td>\n<td align=\"center\">Equal</td>\n\
    <td align=\"center\">L1 10.0</td>\n<td align=\"center\">0.7744</td>\n</tr>\n<tr>\n\
    <td align=\"center\">0.001</td>\n<td align=\"center\">A</td>\n<td align=\"center\"\
    >----</td>\n<td align=\"center\">0.7827</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >0.001</td>\n<td align=\"center\">A</td>\n<td align=\"center\">L1 2.0</td>\n<td\
    \ align=\"center\">0.7826</td>\n</tr>\n<tr>\n<td align=\"center\">0.001</td>\n\
    <td align=\"center\">A</td>\n<td align=\"center\">L1 5.0</td>\n<td align=\"center\"\
    >0.7827</td>\n</tr>\n<tr>\n<td align=\"center\">0.001</td>\n<td align=\"center\"\
    >A</td>\n<td align=\"center\">L1 10.0</td>\n<td align=\"center\">0.7828</td>\n\
    </tr>\n<tr>\n<td align=\"center\">0.001</td>\n<td align=\"center\">Equal</td>\n\
    <td align=\"center\">----</td>\n<td align=\"center\">0.7713</td>\n</tr>\n<tr>\n\
    <td align=\"center\">0.001</td>\n<td align=\"center\">Equal</td>\n<td align=\"\
    center\">L1 2.0</td>\n<td align=\"center\">0.7723</td>\n</tr>\n<tr>\n<td align=\"\
    center\">0.001</td>\n<td align=\"center\">Equal</td>\n<td align=\"center\">L1\
    \ 5.0</td>\n<td align=\"center\">0.7725</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >0.001</td>\n<td align=\"center\">Equal</td>\n<td align=\"center\">L1 10.0</td>\n\
    <td align=\"center\">0.7744</td>\n</tr>\n<tr>\n<td align=\"center\">0.0001</td>\n\
    <td align=\"center\">A</td>\n<td align=\"center\">----</td>\n<td align=\"center\"\
    >0.7827</td>\n</tr>\n<tr>\n<td align=\"center\">0.0001</td>\n<td align=\"center\"\
    >A</td>\n<td align=\"center\">L1 2.0</td>\n<td align=\"center\">0.7826</td>\n\
    </tr>\n<tr>\n<td align=\"center\">0.0001</td>\n<td align=\"center\">A</td>\n<td\
    \ align=\"center\">L1 5.0</td>\n<td align=\"center\">0.7828</td>\n</tr>\n<tr>\n\
    <td align=\"center\">0.0001</td>\n<td align=\"center\">A</td>\n<td align=\"center\"\
    >L1 10.0</td>\n<td align=\"center\">0.7828</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >0.0001</td>\n<td align=\"center\">Equal</td>\n<td align=\"center\">----</td>\n\
    <td align=\"center\">0.7713</td>\n</tr>\n<tr>\n<td align=\"center\">0.0001</td>\n\
    <td align=\"center\">Equal</td>\n<td align=\"center\">L1 2.0</td>\n<td align=\"\
    center\">0.7723</td>\n</tr>\n<tr>\n<td align=\"center\">0.0001</td>\n<td align=\"\
    center\">Equal</td>\n<td align=\"center\">L1 5.0</td>\n<td align=\"center\">0.7725</td>\n\
    </tr>\n<tr>\n<td align=\"center\">0.0001</td>\n<td align=\"center\">Equal</td>\n\
    <td align=\"center\">L1 10.0</td>\n<td align=\"center\">0.7744</td>\n</tr>\n</tbody>\n\
    </table>\n<ul>\n<li>Problem with low out thresholds... Waste all iterations and\
    \ stops.</li>\n</ul>\n<h2>\n<a id=\"user-content-discriminator-entropy-backwards-a---b---c--with-blur-unblur-and-gamma\"\
    \ class=\"anchor\" href=\"#discriminator-entropy-backwards-a---b---c--with-blur-unblur-and-gamma\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Discriminator Entropy backwards 'A' - 'B' - 'C' / With blur, unblur\
    \ and gamma</h2>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Out threshold</th>\n\
    <th align=\"center\">Entropy</th>\n<th align=\"center\">Blur</th>\n<th align=\"\
    center\">Unblur</th>\n<th align=\"center\">Gamma</th>\n<th align=\"center\">Target</th>\n\
    <th align=\"center\">Iters</th>\n<th align=\"center\">B</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td align=\"center\">0.5</td>\n<td align=\"center\">0.0</td>\n\
    <td align=\"center\">0.01</td>\n<td align=\"center\">0.01</td>\n<td align=\"center\"\
    >0.01</td>\n<td align=\"center\">A</td>\n<td align=\"center\">100</td>\n<td align=\"\
    center\">0.7770</td>\n</tr>\n<tr>\n<td align=\"center\">0.5</td>\n<td align=\"\
    center\">0.0</td>\n<td align=\"center\">0.0001</td>\n<td align=\"center\">0.0001</td>\n\
    <td align=\"center\">0.0001</td>\n<td align=\"center\">A</td>\n<td align=\"center\"\
    >100</td>\n<td align=\"center\">0.7786</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >0.5</td>\n<td align=\"center\">0.0</td>\n<td align=\"center\">0.000001</td>\n\
    <td align=\"center\">0.000001</td>\n<td align=\"center\">0.000001</td>\n<td align=\"\
    center\">A</td>\n<td align=\"center\">100</td>\n<td align=\"center\">0.7779</td>\n\
    </tr>\n</tbody>\n</table>\n<h1>\n<a id=\"user-content-7-july\" class=\"anchor\"\
    \ href=\"#7-july\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>7 July</h1>\n<h3>\n<a id=\"user-content-hausdorff-loss-tests\"\
    \ class=\"anchor\" href=\"#hausdorff-loss-tests\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Hausdorff loss tests</h3>\n<p>Mean\
    \ average values for 5 folds. Data combination old. Lr 0.001 with resnet_unet_scratch.</p>\n\
    <table>\n<thead>\n<tr>\n<th align=\"center\">Hausdorff Weight</th>\n<th align=\"\
    center\">IOU A</th>\n<th align=\"center\">IOU B</th>\n<th>DICE A</th>\n<th>DICE\
    \ B</th>\n<th>HAUSSDORF A</th>\n<th>HAUSSDORF B</th>\n<th>ASSD A</th>\n<th>ASSD\
    \ B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">0.0</td>\n<td align=\"\
    center\">0.7333</td>\n<td align=\"center\">0.7835</td>\n<td>0.8087</td>\n<td>0.8561</td>\n\
    <td>4.4773</td>\n<td>3.4890</td>\n<td>1.2458</td>\n<td>0.9624</td>\n</tr>\n<tr>\n\
    <td align=\"center\">0.05</td>\n<td align=\"center\">0.7417</td>\n<td align=\"\
    center\">0.7867</td>\n<td>0.8158</td>\n<td>0.8589</td>\n<td>4.0958</td>\n<td>3.4073</td>\n\
    <td>1.1618</td>\n<td>0.9646</td>\n</tr>\n<tr>\n<td align=\"center\">0.1</td>\n\
    <td align=\"center\">0.7399</td>\n<td align=\"center\">0.7827</td>\n<td>0.8153</td>\n\
    <td>0.8550</td>\n<td>4.1999</td>\n<td>3.4355</td>\n<td>1.1925</td>\n<td>0.9735</td>\n\
    </tr>\n<tr>\n<td align=\"center\">0.2</td>\n<td align=\"center\">0.7421</td>\n\
    <td align=\"center\">0.7806</td>\n<td>0.8193</td>\n<td>0.8522</td>\n<td>4.2831</td>\n\
    <td>3.4414</td>\n<td>1.1953</td>\n<td>0.9831</td>\n</tr>\n<tr>\n<td align=\"center\"\
    >0.3</td>\n<td align=\"center\">0.7370</td>\n<td align=\"center\">0.7790</td>\n\
    <td>0.8134</td>\n<td>0.8534</td>\n<td>4.3634</td>\n<td>3.4972</td>\n<td>1.2264</td>\n\
    <td>0.9886</td>\n</tr>\n</tbody>\n</table>\n<h2>\n<a id=\"user-content-other\"\
    \ class=\"anchor\" href=\"#other\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Other</h2>\n<ul>\n<li>Development\
    \ environment -&gt; CUDA 10.1 and cudnn 7603. Python 3.8.2 - GCC 9.3.0</li>\n\
    <li>Challenge homepage <a href=\"https://www.ub.edu/mnms/\" rel=\"nofollow\">here</a>.</li>\n\
    <li>ACDC nomenclature: 0, 1, 2 and 3 represent voxels located in the background,\
    \ in the right ventricular cavity,\nin the myocardium, and in the left ventricular\
    \ cavity, respectively.</li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - ventricle
  - semantinc
  - segmentation
  - lv
  - rv
  - myo
  - computervision
  updated_at: 1602614455.0
MariusCausemann/brain-inversion:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: MariusCausemann/brain-inversion
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4791" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-using-container-based-solutions-on-c3se-clusters" class="anchor"
    href="#using-container-based-solutions-on-c3se-clusters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using container-based
    solutions on C3SE clusters</h1>

    <p>Container technology has a number of advantages over the traditional workflow
    of using scientific software. The singularity flavour, in particular, targets
    reproducibility, performance and security with respect to running software in
    an HPC environment. Here, we provide our containerized solutions at C3SE. For
    each of the provided containers, read the specific instructions in the corresponding
    folder to easily get started with using them in your workflow. The actual container
    images are hosted on Singularity Hub. Click on the badge above to quickly get
    access to them!</p>

    <h2>

    <a id="user-content-missing-containers-updates-and-troubleshooting" class="anchor"
    href="#missing-containers-updates-and-troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Missing containers,
    updates, and troubleshooting</h2>

    <p>We continuously add more packages to this repository. If you can''t find a
    relevant container for your needs, or in the case of encountering any errors or
    deprecated features in the material, feel free to contact us: <a href="mailto:support@c3se.chalmers.se">support@c3se.chalmers.se</a>
    or open a pull-request.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1609864119.0
MathOnco/ploidyEvolution:
  data_format: 2
  description: A general N-compartment model of ploidy states is developed and analyzed.
  filenames:
  - Containers/hatchetContainers/Singularity
  full_name: MathOnco/ploidyEvolution
  latest_release: null
  readme: '<h1>

    <a id="user-content-cytotoxic-therapy-induced-shifts-in-the-cost-to-benefit-ratio-of-high-ploidy"
    class="anchor" href="#cytotoxic-therapy-induced-shifts-in-the-cost-to-benefit-ratio-of-high-ploidy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cytotoxic
    therapy induced shifts in the cost-to-benefit ratio of high ploidy</h1>

    <p>Energetic costs of DNA content levels required for &gt;75% SCNA load do not,
    in the absence of cytotoxic therapy, justify the masking benefits they bring.
    We integrate single cell sequencing with imaging and mathematical modeling of
    heterogeneous populations evolving through chromosome missegregations to explain
    observed SCNA landscapes and missegregation tolerances, and to predict effective
    cytotoxic therapy doses. We evaluate Oxygen, Phosphate and Glucose as rate-limiting
    substrates of dNTP synthesis of co-evolving subpopulations in stomach and brain
    tissue environments.</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1621982601.0
McCoyGroup/PyHPCDMC:
  data_format: 2
  description: A high-performance computing implementation of DMC
  filenames:
  - RynLib/setup/build/Singularity/Singularity.def
  - RynLib/setup/build/Singularity/SingularityUpdate.def
  - RynLib/setup/build/Singularity/Singularity.ubuntu
  - RynLib/setup/build/Singularity/SingularityCore.def
  full_name: McCoyGroup/PyHPCDMC
  latest_release: v1.0.0
  readme: '<h1>

    <a id="user-content-pyhpdcdmc" class="anchor" href="#pyhpdcdmc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PyHPDCDMC</h1>

    <p>This started out as a quick layer between python and entos for running DMC</p>

    <p>It''s grown a bit...</p>

    <p>You can find some documentation <a href="https//:mccoygroup.github.io/Documentation/RynLib">here</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618010480.0
McCoyGroup/RynLib:
  data_format: 2
  description: Containerized DMC application for HPCs
  filenames:
  - setup/build/Singularity/Singularity.def
  - setup/build/Singularity/SingularityUpdate.def
  - setup/build/Singularity/Singularity.ubuntu
  - setup/build/Singularity/SingularityCore.def
  full_name: McCoyGroup/RynLib
  latest_release: null
  readme: '<h1>

    <a id="user-content-rynlib" class="anchor" href="#rynlib" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>RynLib</h1>

    <p>This started out as a quick layer between python and entos for running DMC</p>

    <p>It''s grown a bit...</p>

    <p>You can find some documentation <a href="https//:mccoygroup.github.io/Documentation/RynLib">here</a></p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1613598399.0
MetOffice/LFRic-Containers:
  data_format: 2
  description: null
  filenames:
  - Singularity/Singularity-GCC-VisTools-MINT
  full_name: MetOffice/LFRic-Containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-containerisation-of-lfric" class="anchor" href="#containerisation-of-lfric"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Containerisation
    of LFRic</h1>

    <p>This repository hosts LFRic container recipes and links to similar external

    repositories.</p>

    <p>More detailed information about <code>LFRic</code> and further references can
    be found in

    <a href="https://github.com/MetOffice/LFRic-Containers/blob/master/LFRicIntro.md"><em>Introduction
    to LFRic</em></a>

    section.</p>

    <p>Instructions on building and runing <code>LFRic</code> in two container platforms,

    <a href="https://docs.docker.com/install/" rel="nofollow">Docker CE</a> and

    <a href="https://sylabs.io/docs/" rel="nofollow">Singularity</a>, are stored in
    two subdirectories:</p>

    <ul>

    <li>

    <a href="https://github.com/MetOffice/LFRic-Containers/blob/master/Docker/README.md">Docker</a>;</li>

    <li>

    <a href="https://github.com/MetOffice/LFRic-Containers/blob/master/Singularity/README.md">Singularity</a>.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1620312607.0
MontrealSergiy/deformation:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: MontrealSergiy/deformation
  latest_release: null
  readme: '<h1>

    <a id="user-content-deformation-field" class="anchor" href="#deformation-field"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deformation
    field</h1>

    <p>This PERL script is a wrapper that is calling sequence of commands for generating
    deformation fields scrips

    <a href="https://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields"
    rel="nofollow">https://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields</a>

    Source code for deformation pipeline and dependencies (MINC):

    <a href="https://github.com/Mouse-Imaging-Centre/generate_deformation_fields">https://github.com/Mouse-Imaging-Centre/generate_deformation_fields</a></p>

    <p>Usage</p>

    <p>./deformation.pl -input ICBM_00100_t1_final.mnc &lt;&lt;this could be any anatomical
    minc file, for a collection of minc files&gt;&gt; -output dummy_hoho -deformation_ratio
    0.6 -coordinate 70 100 70 10 10 10 -tolerance_space 4 &lt;&gt; -blur_determinant
    0.25 &lt;&gt; -error 0.00001 &lt;&gt; -iteration 100</p>

    <p>The output of running this command looks like this:

    ICBM_00100_t1_final_deformed_by_0.4atROIx70-y100-z70dimx10.dimy10.dimz10.mnc.
    </p>

    <p>We will also have a directory dummy_hoho/TMP that will contain the in-between-files.</p>

    <p>$:/dummy_hoho/TMP$ ls</p>

    <p>block.mnc</p>

    <p>blurred0.25determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc</p>

    <p>DDDDdilated.mnc</p>

    <p>DDDDring.mnc</p>

    <p>determinant_r_0.4_grid.mnc</p>

    <p>determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc</p>

    <p>determinant_r_0.4.xfm</p>

    <p>mask.mnc</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621971777.0
MostafaRizk/TS-Platform:
  data_format: 2
  description: null
  filenames:
  - src/archive/Singularity.gym
  full_name: MostafaRizk/TS-Platform
  latest_release: null
  readme: '<h1>

    <a id="user-content-work-in-progress" class="anchor" href="#work-in-progress"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>work
    in progress</h1>

    <p>attempt to build a base singularity image with spack that can be used as the
    bootstrap for

    other singularity images that would perform the spack install of a particular
    package</p>

    <p>currently having an issue with stage directory for spack attempting to write
    to

    the immutable squashfs</p>

    <p>as expected, the child container will happily install during %post since it
    can write</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619701199.0
NIH-HPC/singularity-def-files:
  data_format: 2
  description: definition files and wrapper scripts used by NIH HPC staff to install
    user-facing apps on the Biowulf cluster
  filenames:
  - sequence-analysis/saige/0.44.1/saige.def
  - sequence-analysis/cicero/1.4.0/cicero.def
  - sequence-analysis/sonicparanoid/1.3.2/sonicparanoid.def
  - sequence-analysis/sonicparanoid/1.3.5/sonicparanoid.def
  - sequence-analysis/orffinder/0.4.3-sing-install/orffinder.def
  - sequence-analysis/smoove/0.2.5/smoove.def
  - sequence-analysis/smoove/0.2.1/smoove.def
  - sequence-analysis/htgtsrep/9fe74ff/htgtsrep.def
  - sequence-analysis/xhla/2018-04-04/xhla.def
  - sequence-analysis/accurity/20180724/accurity.def
  - sequence-analysis/accurity/20210209/accurity.def
  - sequence-analysis/cactus/1.2.3/cactus.def
  - sequence-analysis/anvio/7/anvio.def
  - sequence-analysis/glnexus/1.1.11/glnexus.def
  - sequence-analysis/glnexus/1.2.7/glnexus.def
  - sequence-analysis/vcf-kit/0.1.6/vcf-kit.def
  - sequence-analysis/mmarge/1.0/mmarge.def
  - sequence-analysis/roary/3.12.0/roary.def
  - sequence-analysis/roary/3.13.0/roary.def
  - sequence-analysis/wisexome/20180814/wisexome.def
  - sequence-analysis/glu/1.0b3/glu.def
  - sequence-analysis/arriba/1.2.0/arriba.def
  - sequence-analysis/arriba/2.0.0/arriba.def
  - sequence-analysis/intarna/3.2.0/intarna.def
  - sequence-analysis/netoglyc/3.1d/netoglyc.def
  - sequence-analysis/prokka/1.13/prokka.def
  - sequence-analysis/prokka/1.14.6/prokka.def
  - sequence-analysis/annogesic/1.0.2/annogesic.def
  - sequence-analysis/acfs/20180316/acfs.def
  - sequence-analysis/augustus/3.3.3/augustus.def
  - sequence-analysis/phaser/1.1.1/phaser.def
  - sequence-analysis/bamgineer/2-20200624/bamgineer.def
  - sequence-analysis/focus/0.6.10/focus.def
  - sequence-analysis/m-tools/20210208/m-tools.def
  - sequence-analysis/eukrep/20180308/eukrep.def
  - sequence-analysis/braker/2/braker.def
  - sequence-analysis/ldsc/3d0c4464/ldsc.def
  - sequence-analysis/deepsea/0.94c/deepsea.def
  - sequence-analysis/asgal/1.0/asgal.def
  - sequence-analysis/qtltools/1.3.1/qtltools.def
  - sequence-analysis/chipseq_pipeline/1.2.0/chipseq_pipeline.def
  - sequence-analysis/svtools/0.5.1/svtools.def
  - molecular-modeling-graphics/blender/2.82/blender.def
  - molecular-modeling-graphics/starseqr/0.6.7/starseqr.def
  - molecular-modeling-graphics/chimerax/0.93/chimerax.def
  - molecular-modeling-graphics/chimerax/1.1/chimerax.def
  - image-analysis/fitlins/0.8.0/fitlins.def
  - image-analysis/fitlins/0.7.0/fitlins.def
  - image-analysis/qsiprep/0.8.0/qsiprep.def
  - image-analysis/xcpengine/1.2.3/xcpengine.def
  - image-analysis/xcpengine/1.0/xcpengine.def
  - image-analysis/xcpengine/1.2.1/xcpengine.def
  - image-analysis/deepmedic/0.8.0/deepmedic.def
  - image-analysis/deepmedic/0.8.2/deepmedic.def
  - image-analysis/tesseract/4.1.1/tesseract.def
  - image-analysis/mrtrix/3.0.0/mrtrix.def
  - image-analysis/mrtrix/3.0.2-cuda9.1/mrtrix.def
  - image-analysis/mrtrix/3.0.1/mrtrix.def
  - image-analysis/topaz/0.2.5/topaz.def
  - image-analysis/minc-toolkit/1.9.16/minc-toolkit.def
  - image-analysis/minc-toolkit/1.9.18/minc-toolkit.def
  - image-analysis/resmap/1.95/resmap.def
  - image-analysis/broccoli/1.0.1/broccoli.def
  - image-analysis/civet/2.1.1/civet.def
  - image-analysis/fmriprep/20.2.0/fmriprep.def
  - image-analysis/fmriprep/20.1.3/fmriprep.def
  - image-analysis/fmriprep/20.0.5/fmriprep.def
  - image-analysis/fmriprep/20.2.1/fmriprep.def
  - image-analysis/fmriprep/20.1.1/fmriprep.def
  - image-analysis/baracus/1.1.4/baracus.def
  - image-analysis/terastitcher/1.11.10/terastitcher.def
  - image-analysis/terastitcher/1.10.8/terastitcher.def
  - image-analysis/mriqc/0.16.1/mriqc.def
  - image-analysis/mriqc/0.15.2-0be03bf/mriqc.def
  - image-analysis/mriqc/0.15.1/mriqc.def
  - image-analysis/mriqc/0.15.2/mriqc.def
  - mathematical-statistics/omeclust/1.1.4/omeclust.def
  - mathematical-statistics/omeclust/1.1.6/omeclust.def
  - mathematical-statistics/m2clust/0.0.7/m2clust.def
  - mathematical-statistics/m2clust/0.0.8/m2clust.def
  - mathematical-statistics/m2clust/1.1.3/m2clust.def
  - computational-chemistry/ampl/f35623d4/ampl.def
  - linkage-phylogenetics/gubbins/2.3.4/gubbins.def
  - linkage-phylogenetics/bali-phy/3.5/bali-phy.def
  - mass-spectrometry/maxquant/1.6.17.0/maxquant.def
  - mass-spectrometry/maxquant/1.6.3.3/maxquant.def
  - mass-spectrometry/maxquant/1.6.7.0/maxquant.def
  - systems-biology/cellphonedb/2.1.2/cellphonedb.def
  - systems-biology/cellphonedb/2.1.7/cellphonedb.def
  - utilities/visidata/2.2/visidata.def
  - utilities/snp-sites/2.4.1/snp-sites.def
  - utilities/pyega3/3.3.0/pyega3.def
  - utilities/gdc-client/1.5.0/gdc-client.def
  - utilities/pdf2svg/0.2.3/pdf2svg.def
  - utilities/atom/1.13.1/atom.def
  - utilities/vcf2db/2020.09.14/vcf2db.def
  - utilities/xvfb/1.19.6/xvfb.def
  - utilities/datalad/0.13.0rc2/datalad.def
  - utilities/sysbench/1.0.11/sysbench.def
  - utilities/sysbench/1.0.20/sysbench.def
  - utilities/uropa/3.5.0/uropa.def
  - utilities/longshot/0.3.5/longshot.def
  - utilities/ariba/2.14.4/ariba.def
  - utilities/whatshap/0.18/whatshap.def
  - high-throughput-sequencing/tvc/5.10.1/tvc.def
  - high-throughput-sequencing/tpmcalculator/0.0.3/tpmcalculator.def
  - high-throughput-sequencing/tpmcalculator/0.0.4/tpmcalculator.def
  - high-throughput-sequencing/rsd/1.1.7/rsd.def
  - high-throughput-sequencing/xengsort/28762aac/xengsort.def
  - high-throughput-sequencing/pvactools/2.0.1/pvactools.def
  - high-throughput-sequencing/pvactools/1.5.5/pvactools.def
  - high-throughput-sequencing/parliament/0.1.7/parliament.def
  - high-throughput-sequencing/flye/2.7/flye.def
  - high-throughput-sequencing/flye/2.8-1/flye.def
  - high-throughput-sequencing/hicpro/2.11.4/hicpro.def
  - high-throughput-sequencing/dropest/0.8.6/dropest.def
  - high-throughput-sequencing/bamreadcount/cram-v0.0.1/bamreadcount.def
  - high-throughput-sequencing/cicero/0.3.0/cicero.def
  - high-throughput-sequencing/lefse/1.0.8/lefse.def
  - high-throughput-sequencing/lefse/1.0.7/lefse.def
  - high-throughput-sequencing/deeptools/3.5.0/deeptools.def
  - high-throughput-sequencing/deeptools/3.4.2/deeptools.def
  - high-throughput-sequencing/epic2/0.0.41/epic2.def
  - high-throughput-sequencing/crispresso/2.0.40/crispresso.def
  - high-throughput-sequencing/crispresso/2.0.45/crispresso.def
  - high-throughput-sequencing/transvar/2.5.9/transvar.def
  - high-throughput-sequencing/ricopili/2019_Jun_25.001/ricopili.def
  - high-throughput-sequencing/hicexplorer/3.5.1/hicexplorer.def
  - high-throughput-sequencing/cgpbattenberg/3.5.3/cgpbattenberg.def
  - high-throughput-sequencing/gridss/2.9.4/gridss.def
  - high-throughput-sequencing/salmon/1.4.0/salmon.def
  - high-throughput-sequencing/bamliquidator/1.3.8/bamliquidator.def
  - high-throughput-sequencing/pepr/1.1.24/pepr.def
  - high-throughput-sequencing/vireosnp/0.3.2/vireosnp.def
  - high-throughput-sequencing/vireosnp/0.5.1/vireosnp.def
  - high-throughput-sequencing/scramble/1.0.1-32893ef/scramble.def
  - high-throughput-sequencing/scramble/0.0.20190211.82c78b9/scramble.def
  - high-throughput-sequencing/ascatngs/4.3.3/ascatngs.def
  - high-throughput-sequencing/ascatngs/4.5.0/ascatngs.def
  - high-throughput-sequencing/ascatngs/4.3.4/ascatngs.def
  - high-throughput-sequencing/htseq/0.11.4/htseq.def
  - high-throughput-sequencing/pychopper/2.4.0/pychopper.def
  - high-throughput-sequencing/brass/6.3.4/brass.def
  - high-throughput-sequencing/brass/6.1.2/brass.def
  - high-throughput-sequencing/busco/4.1.3/busco.def
  - high-throughput-sequencing/busco/5.0.0/busco.def
  - high-throughput-sequencing/surpi/1.0.67/surpi.def
  - high-throughput-sequencing/delly/0.8.7/delly.def
  - high-throughput-sequencing/multiqc/1.10/multiqc.def
  - high-throughput-sequencing/multiqc/1.9/multiqc.def
  - high-throughput-sequencing/rilseq/0.75/rilseq.def
  - high-throughput-sequencing/seqlinkage/1.0/seqlinkage.def
  - high-throughput-sequencing/guppy/4.0.15/guppy.def
  - high-throughput-sequencing/guppy/4.2.2/guppy.def
  - high-throughput-sequencing/guppy/3.4.5/guppy.def
  - high-throughput-sequencing/metaphlan/3.0.6/metaphlan.def
  - high-throughput-sequencing/metaphlan/3.0/metaphlan.def
  - high-throughput-sequencing/mitosuite/1.0.9b/mitosuite.def
  - high-throughput-sequencing/abruijn/1.0/abruijn.def
  - high-throughput-sequencing/bison/0.4.0/bison.def
  - high-throughput-sequencing/hail/0.2.3/hail.def
  - high-throughput-sequencing/hail/0.2.61/hail.def
  - high-throughput-sequencing/hail/0.2.56/hail.def
  - high-throughput-sequencing/biom-format/2.1.10/biom-format.def
  - high-throughput-sequencing/csvkit/1.0.5/csvkit.def
  - high-throughput-sequencing/fusioninspector/2.5.0/fusioninspector.def
  - high-throughput-sequencing/fusioninspector/2.3.0/fusioninspector.def
  - high-throughput-sequencing/rnapeg/current/rnapeg.def
  - high-throughput-sequencing/cellsnp/0.3.2/cellsnp.def
  - high-throughput-sequencing/cellsnp/0.1.7/cellsnp.def
  - high-throughput-sequencing/sicer/2-1.0.2/sicer.def
  - high-throughput-sequencing/freebayes/1.3.5/freebayes.def
  - high-throughput-sequencing/svtk/0.1/svtk.def
  - high-throughput-sequencing/cnvnator/0.4.1/cnvnator.def
  - high-throughput-sequencing/rseqc/4.0.0/rseqc.def
  - high-throughput-sequencing/tandemtools/current/tandemtools.def
  - high-throughput-sequencing/humann/3.0.0-alpha.3/humann.def
  - high-throughput-sequencing/bamutil/1.0.15/bamutil.def
  - high-throughput-sequencing/rmats/4.0.2/rmats.def
  - high-throughput-sequencing/stream/20180816/stream.def
  - high-throughput-sequencing/idep/0.81/idep.def
  - high-throughput-sequencing/sve/0.1.0/sve.def
  - high-throughput-sequencing/humann2/2.8.1/humann2.def
  - high-throughput-sequencing/neusomatic/0.2.1/neusomatic.def
  - high-throughput-sequencing/flappie/1.0.0/flappie.def
  - high-throughput-sequencing/flappie/2.1.3/flappie.def
  - high-throughput-sequencing/canvas/1.40/canvas.def
  - high-throughput-sequencing/maggie/0.3.4/maggie.def
  - high-throughput-sequencing/macs/2.2.7.1/macs.def
  - high-throughput-sequencing/cancerit-wgs/2.1.0/cancerit-wgs.def
  - high-throughput-sequencing/mageck-vispr/0.5.4/mageck-vispr.def
  - high-throughput-sequencing/mtoolbox/1.1/mtoolbox.def
  - high-throughput-sequencing/slamdunk/0.4.3/slamdunk.def
  - high-throughput-sequencing/bamsurgeon/1111e5d/bamsurgeon.def
  - high-throughput-sequencing/crossmap/0.5.2/crossmap.def
  - high-throughput-sequencing/bigscale2/20191119/bigscale2.def
  - high-throughput-sequencing/metabat/2.13/metabat.def
  - high-throughput-sequencing/hap.py/0.3.9/hap.py.def
  - high-throughput-sequencing/megalodon/2.2.9/megalodon.def
  - high-throughput-sequencing/taiji/1.1.0/taiji.def
  - high-throughput-sequencing/taiji/1.2.0/taiji.def
  - high-throughput-sequencing/cutadapt/1.18/cutadapt.def
  - high-throughput-sequencing/cutadapt/2.10/cutadapt.def
  - high-throughput-sequencing/cutadapt/3.0/cutadapt.def
  - high-throughput-sequencing/vep/101/vep.def
  - high-throughput-sequencing/vep/97/vep.def
  - high-throughput-sequencing/vep/103/vep.def
  - high-throughput-sequencing/atropos/1.1.18/atropos.def
  - high-throughput-sequencing/pcap-core/4.3.5/pcap-core.def
  - high-throughput-sequencing/umitools/1.1.1/umitools.def
  - high-throughput-sequencing/deepsignal/0.1.8/deepsignal.def
  - high-throughput-sequencing/tetoolkit/2.2.1/tetoolkit.def
  - high-throughput-sequencing/tetoolkit/2.1.4/tetoolkit.def
  - high-throughput-sequencing/cnvkit/0.9.8/cnvkit.def
  - high-throughput-sequencing/cnvkit/0.9.6/cnvkit.def
  - high-throughput-sequencing/gossamer/ac492a8/gossamer.def
  - high-throughput-sequencing/svtyper/0.7.1/svtyper.def
  - high-throughput-sequencing/raremetal/4.15.1/raremetal.def
  - high-throughput-sequencing/vagrent/3.3.4/vagrent.def
  - high-throughput-sequencing/eager/1.92/eager.def
  - high-throughput-sequencing/repeatmodeler/2.0.1/repeatmodeler.def
  - high-throughput-sequencing/atac_dnase_pipelines/0.3.4-19-gcbd2a00/atac_dnase_pipelines.def
  - high-throughput-sequencing/deepvariant/1.1.0/deepvariant.def
  - high-throughput-sequencing/deepvariant/0.9.0/deepvariant.def
  - high-throughput-sequencing/deepvariant/0.10.0/deepvariant.def
  - high-throughput-sequencing/medaka/1.0.3/medaka.def
  - high-throughput-sequencing/medaka/1.2.0/medaka.def
  - high-throughput-sequencing/medaka/0.12.1/medaka.def
  - structural-biology/parsnip/20180507/parsnip.def
  - structural-biology/pymol/2.4.0/pymol.def
  - structural-biology/pymol/2.3.0/pymol_2.3.0.def
  - structural-biology/rdock/2013.1/rdock.def
  - deep-learning/unet/20180704/unet.def
  - deep-learning/deeplab/20180816/deeplab.def
  - deep-learning/basset/0.1.0/basset.def
  - deep-learning/dextr-pytorch/20180710/dextr-pytorch.def
  - deep-learning/digits/6.0/digits.def
  - deep-learning/clairvoyante/1.0/clairvoyante.def
  - deep-learning/caffe2/0.8.1/caffe2.def
  - deep-learning/polyrnnpp/20180718/polyrnnpp.def
  - deep-learning/few-shot-ssl/20180723/few-shot-ssl.def
  - deep-learning/tensorrt/18.09/tensorrt.def
  full_name: NIH-HPC/singularity-def-files
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-nih-hpc-singularity-definition-files\" class=\"\
    anchor\" href=\"#nih-hpc-singularity-definition-files\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>NIH HPC Singularity\
    \ Definition Files</h1>\n<p>These definition files and wrapper scripts are used\
    \ by the <a href=\"https://hpc.nih.gov/\" rel=\"nofollow\">NIH HPC (Biowulf)</a>\
    \ staff to install containerized applications using <a href=\"https://github.com/sylabs/singularity\"\
    >Singularity</a>. Each app is installed in a self-contained directory and access\
    \ to the app is controlled through a module system (<a href=\"https://github.com/TACC/Lmod\"\
    >Lmod</a>). This strategy allows users to transparently access apps that are installed\
    \ within containers as though they were installed directly on the host system.\
    \ More details can be found <a href=\"https://hpc.nih.gov/apps/singularity.html#bind-stationary\"\
    \ rel=\"nofollow\">here</a>.</p>\n<p>Typically, apps are installed under in a\
    \ directory structure like so:</p>\n<pre><code>$ tree appname/ver\nappname/ver\n\
    |-- bin\n|   |-- cmd1 -&gt; ../libexec/wrapper.sh\n|   |-- cmd2 -&gt; ../libexec/wrapper.sh\n\
    |   `-- cmd3 -&gt; ../libexec/wrapper.sh\n`-- libexec\n    |-- app.sif\n    `--\
    \ wrapper.sh\n</code></pre>\n<p>Because <code>wrapper.sh</code> is written to\
    \ be introspective, any command symlinked to it will be carried through and executed\
    \ within the associated container. The wrapper script is also sufficiently generic\
    \ that it can be reused across apps with little or no modification.</p>\n<p>Each\
    \ app has its own <code>README.md</code> that contains:</p>\n<ul>\n<li>a link\
    \ to the NIH HPC app page or developer's documentation</li>\n<li>a list of symlinks\
    \ that should be created to the wrapper script to expose executables within the\
    \ container</li>\n<li>any app specific installation notes</li>\n</ul>\n<p>Finally,\
    \ please note that these definition files <strong>are not guaranteed to reproduce\
    \ the same container, or even to produce any container at all</strong>. The internet,\
    \ upon which these definition files are based, is subject to change without notice.\
    \ These definition files are therefore intended to be treated as (potentially)\
    \ helpful suggestions.</p>\n<h2>\n<a id=\"user-content-computational-chemistry\"\
    \ class=\"anchor\" href=\"#computational-chemistry\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"/computational-chemistry\"\
    >Computational Chemistry</a>\n</h2>\n<ul>\n<li><a href=\"/computational-chemistry/ampl\"\
    >ampl</a></li>\n</ul>\n<h2>\n<a id=\"user-content-deep-learning\" class=\"anchor\"\
    \ href=\"#deep-learning\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"/deep-learning\">Deep Learning</a>\n\
    </h2>\n<ul>\n<li><a href=\"/deep-learning/caffe2\">Caffe2</a></li>\n<li><a href=\"\
    /deep-learning/dextr-pytorch\">DEXTR-PyTorch</a></li>\n<li><a href=\"/deep-learning/polyrnnpp\"\
    >PolyRNNpp</a></li>\n<li><a href=\"/deep-learning/basset\">basset</a></li>\n<li><a\
    \ href=\"/deep-learning/clairvoyante\">clairvoyante</a></li>\n<li><a href=\"/deep-learning/deeplab\"\
    >deeplab</a></li>\n<li><a href=\"/deep-learning/digits\">digits</a></li>\n<li><a\
    \ href=\"/deep-learning/few-shot-ssl\">few-shot-ssl</a></li>\n<li><a href=\"/deep-learning/tensorrt\"\
    >tensorrt</a></li>\n<li><a href=\"/deep-learning/unet\">unet</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-high-throughput-sequencing\" class=\"anchor\" href=\"\
    #high-throughput-sequencing\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a><a href=\"/high-throughput-sequencing\"\
    >High Throughput Sequencing</a>\n</h2>\n<ul>\n<li><a href=\"/high-throughput-sequencing/atac_dnase_pipelines\"\
    >ATAC-Seq / DNase-Seq Pipeline</a></li>\n<li><a href=\"/high-throughput-sequencing/ascatngs\"\
    >AscatNGS</a></li>\n<li><a href=\"/high-throughput-sequencing/atropos\">Atropos</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/bamsurgeon\">BAMSurgeon</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/brass\">BRASS</a></li>\n<li><a href=\"/high-throughput-sequencing/canvas\"\
    >Canvas</a></li>\n<li><a href=\"/high-throughput-sequencing/maggie\">MAGGIE</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/pcap-core\">PCAP-core</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/pepr\">PePr</a></li>\n<li><a href=\"/high-throughput-sequencing/rsd\"\
    >RSD</a></li>\n<li><a href=\"/high-throughput-sequencing/surpi\">SURPI</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/tpmcalculator\">TPMCalculator</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/tvc\">TVC</a></li>\n<li><a href=\"/high-throughput-sequencing/vagrent\"\
    >VAGrENT</a></li>\n<li><a href=\"/high-throughput-sequencing/vep\">VEP</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/abruijn\">abruijn</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamliquidator\">bamliquidator</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamreadcount\">bamreadcount</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/bamutil\">bamutil</a></li>\n<li><a href=\"/high-throughput-sequencing/bigscale2\"\
    >bigscale2</a></li>\n<li><a href=\"/high-throughput-sequencing/biom-format\">biom-format</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/bison\">bison</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/busco\">busco</a></li>\n<li><a href=\"/high-throughput-sequencing/cancerit-wgs\"\
    >cancerit-wgs</a></li>\n<li><a href=\"/high-throughput-sequencing/cellsnp\">cellsnp</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/cgpbattenberg\">cgpBattenberg</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/cicero\">cicero</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/cnvkit\">cnvkit</a></li>\n<li><a href=\"/high-throughput-sequencing/cnvnator\"\
    >cnvnator</a></li>\n<li><a href=\"/high-throughput-sequencing/crispresso\">crispresso</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/crossmap\">crossmap</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/csvkit\">csvkit</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/cutadapt\">cutadapt</a></li>\n<li><a href=\"/high-throughput-sequencing/deepsignal\"\
    >deepsignal</a></li>\n<li><a href=\"/high-throughput-sequencing/deeptools\">deeptools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/deepvariant\">deepvariant</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/delly\">delly</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/dropest\">dropest</a></li>\n<li><a href=\"/high-throughput-sequencing/eager\"\
    >eager</a></li>\n<li><a href=\"/high-throughput-sequencing/epic2\">epic2</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/flappie\">flappie</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/flye\">flye</a></li>\n<li><a href=\"/high-throughput-sequencing/freebayes\"\
    >freebayes</a></li>\n<li><a href=\"/high-throughput-sequencing/fusioninspector\"\
    >fusioninspector</a></li>\n<li><a href=\"/high-throughput-sequencing/gossamer\"\
    >gossamer</a></li>\n<li><a href=\"/high-throughput-sequencing/gridss\">gridss</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/guppy\">guppy</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/hail\">hail</a></li>\n<li><a href=\"/high-throughput-sequencing/hap.py\"\
    >hap.py</a></li>\n<li><a href=\"/high-throughput-sequencing/hicexplorer\">hicexplorer</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/hicpro\">hicpro</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/htseq\">htseq</a></li>\n<li><a href=\"/high-throughput-sequencing/humann2\"\
    >humann2</a></li>\n<li><a href=\"/high-throughput-sequencing/idep\">idep</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/lefse\">lefse</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/macs\">macs</a></li>\n<li><a href=\"/high-throughput-sequencing/mageck-vispr\"\
    >mageck-vispr</a></li>\n<li><a href=\"/high-throughput-sequencing/medaka\">medaka</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/megalodon\">megalodon</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/metabat\">metabat</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/metaphlan\">metaphlan</a></li>\n<li><a href=\"/high-throughput-sequencing/mitosuite\"\
    >mitosuite</a></li>\n<li><a href=\"/high-throughput-sequencing/mtoolbox\">mtoolbox</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/multiqc\">multiqc</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/neusomatic\">neusomatic</a></li>\n<li><a href=\"/high-throughput-sequencing/parliament\"\
    >parliament</a></li>\n<li><a href=\"/high-throughput-sequencing/pvactools\">pvactools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/pychopper\">pychopper</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/raremetal\">raremetal</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/repeatmodeler\">repeatmodeler</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/ricopili\">ricopili</a></li>\n<li><a href=\"/high-throughput-sequencing/rilseq\"\
    >rilseq</a></li>\n<li><a href=\"/high-throughput-sequencing/rmats\">rmats</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/rnapeg\">rnapeg</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/rseqc\">rseqc</a></li>\n<li><a href=\"/high-throughput-sequencing/salmon\"\
    >salmon</a></li>\n<li><a href=\"/high-throughput-sequencing/scramble\">scramble</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/seqlinkage\">seqlinkage</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/sicer\">sicer</a></li>\n<li><a href=\"/high-throughput-sequencing/slamdunk\"\
    >slamdunk</a></li>\n<li><a href=\"/high-throughput-sequencing/stream\">stream</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/sve\">sve</a></li>\n<li><a href=\"/high-throughput-sequencing/svtk\"\
    >svtk</a></li>\n<li><a href=\"/high-throughput-sequencing/svtyper\">svtyper</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/taiji\">taiji</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/tandemtools\">tandemtools</a></li>\n<li><a href=\"\
    /high-throughput-sequencing/tetoolkit\">tetoolkit</a></li>\n<li><a href=\"/high-throughput-sequencing/transvar\"\
    >transvar</a></li>\n<li><a href=\"/high-throughput-sequencing/umitools\">umitools</a></li>\n\
    <li><a href=\"/high-throughput-sequencing/vireosnp\">vireosnp</a></li>\n<li><a\
    \ href=\"/high-throughput-sequencing/xengsort\">xengsort</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-image-analysis\" class=\"anchor\" href=\"#image-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/image-analysis\">Image Analysis</a>\n</h2>\n<ul>\n<li><a\
    \ href=\"/image-analysis/resmap\">ResMap</a></li>\n<li><a href=\"/image-analysis/terastitcher\"\
    >TeraStitcher</a></li>\n<li><a href=\"/image-analysis/baracus\">baracus</a></li>\n\
    <li><a href=\"/image-analysis/broccoli\">broccoli</a></li>\n<li><a href=\"/image-analysis/civet\"\
    >civet</a></li>\n<li><a href=\"/image-analysis/ctf\">ctf</a></li>\n<li><a href=\"\
    /image-analysis/deepmedic\">deepmedic</a></li>\n<li><a href=\"/image-analysis/fitlins\"\
    >fitlins</a></li>\n<li><a href=\"/image-analysis/fmriprep\">fmriprep</a></li>\n\
    <li><a href=\"/image-analysis/minc-toolkit\">minc-toolkit</a></li>\n<li><a href=\"\
    /image-analysis/mriqc\">mriqc</a></li>\n<li><a href=\"/image-analysis/mrtrix\"\
    >mrtrix</a></li>\n<li><a href=\"/image-analysis/qsiprep\">qsiprep</a></li>\n<li><a\
    \ href=\"/image-analysis/tesseract\">tesseract</a></li>\n<li><a href=\"/image-analysis/topaz\"\
    >topaz</a></li>\n<li><a href=\"/image-analysis/xcpengine\">xcpengine</a></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-linkage-phylogenetics\" class=\"anchor\" href=\"\
    #linkage-phylogenetics\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"/linkage-phylogenetics\">Linkage Phylogenetics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/linkage-phylogenetics/bali-phy\">bali-phy</a></li>\n\
    <li><a href=\"/linkage-phylogenetics/gubbins\">gubbins</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-mass-spectrometry\" class=\"anchor\" href=\"#mass-spectrometry\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/mass-spectrometry\">Mass Spectrometry</a>\n</h2>\n<ul>\n\
    <li><a href=\"/mass-spectrometry/maxquant\">maxquant</a></li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-mathematicalstatistics\" class=\"anchor\" href=\"#mathematicalstatistics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/mathematical-statistics\">Mathematical/Statistics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/mathematical-statistics/m2clust\">m2clust</a></li>\n\
    <li><a href=\"/mathematical-statistics/omeclust\">omeClust</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-molecular-modeling-graphics\" class=\"anchor\" href=\"#molecular-modeling-graphics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/molecular-modeling-graphics\">Molecular Modeling Graphics</a>\n\
    </h2>\n<ul>\n<li><a href=\"/molecular-modeling-graphics/chimerax\">ChimeraX</a></li>\n\
    <li><a href=\"/molecular-modeling-graphics/blender\">blender</a></li>\n<li><a\
    \ href=\"/molecular-modeling-graphics/starseqr\">starseqr</a></li>\n</ul>\n<h2>\n\
    <a id=\"user-content-sequence-analysis\" class=\"anchor\" href=\"#sequence-analysis\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/sequence-analysis\">Sequence Analysis</a>\n</h2>\n<ul>\n\
    <li><a href=\"/sequence-analysis/acfs\">ACFS</a></li>\n<li><a href=\"/sequence-analysis/annogesic\"\
    >ANNOgesic</a></li>\n<li><a href=\"/sequence-analysis/asgal\">ASGAL</a></li>\n\
    <li><a href=\"/sequence-analysis/accurity\">Accurity</a></li>\n<li><a href=\"\
    /sequence-analysis/eukrep\">EukRep</a></li>\n<li><a href=\"/sequence-analysis/glu\"\
    >GLU</a></li>\n<li><a href=\"/sequence-analysis/htgtsrep\">HTGTSrep</a></li>\n\
    <li><a href=\"/sequence-analysis/orffinder\">ORFfinder</a></li>\n<li><a href=\"\
    /sequence-analysis/saige\">SAIGE</a></li>\n<li><a href=\"/sequence-analysis/vcf-kit\"\
    >VCF-kit</a></li>\n<li><a href=\"/sequence-analysis/anvio\">anvio</a></li>\n<li><a\
    \ href=\"/sequence-analysis/arriba\">arriba</a></li>\n<li><a href=\"/sequence-analysis/augustus\"\
    >augustus</a></li>\n<li><a href=\"/sequence-analysis/bamgineer\">bamgineer</a></li>\n\
    <li><a href=\"/sequence-analysis/braker\">braker</a></li>\n<li><a href=\"/sequence-analysis/cactus\"\
    >cactus</a></li>\n<li><a href=\"/sequence-analysis/chipseq_pipeline\">chipseq_pipeline</a></li>\n\
    <li><a href=\"/sequence-analysis/cicero\">cicero</a></li>\n<li><a href=\"/sequence-analysis/deepsea\"\
    >deepsea</a></li>\n<li><a href=\"/sequence-analysis/focus\">focus</a></li>\n<li><a\
    \ href=\"/sequence-analysis/glnexus\">glnexus</a></li>\n<li><a href=\"/sequence-analysis/intarna\"\
    >intarna</a></li>\n<li><a href=\"/sequence-analysis/ldsc\">ldsc</a></li>\n<li><a\
    \ href=\"/sequence-analysis/m-tools\">m-tools</a></li>\n<li><a href=\"/sequence-analysis/mmarge\"\
    >mmarge</a></li>\n<li><a href=\"/sequence-analysis/netoglyc\">netOglyc</a></li>\n\
    <li><a href=\"/sequence-analysis/phaser\">phaser</a></li>\n<li><a href=\"/sequence-analysis/prokka\"\
    >prokka</a></li>\n<li><a href=\"/sequence-analysis/qtltools\">qtltools</a></li>\n\
    <li><a href=\"/sequence-analysis/roary\">roary</a></li>\n<li><a href=\"/sequence-analysis/smoove\"\
    >smoove</a></li>\n<li><a href=\"/sequence-analysis/sonicparanoid\">sonicparanoid</a></li>\n\
    <li><a href=\"/sequence-analysis/svtools\">svtools</a></li>\n<li><a href=\"/sequence-analysis/wisexome\"\
    >wisexome</a></li>\n<li><a href=\"/sequence-analysis/xhla\">xHLA</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-structural-biology\" class=\"anchor\" href=\"#structural-biology\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/structural-biology\">Structural Biology</a>\n</h2>\n<ul>\n\
    <li><a href=\"/structural-biology/parsnip\">parsnip</a></li>\n<li><a href=\"/structural-biology/pymol\"\
    >pymol</a></li>\n<li><a href=\"/structural-biology/rdock\">rDock</a></li>\n</ul>\n\
    <h2>\n<a id=\"user-content-systems-biology\" class=\"anchor\" href=\"#systems-biology\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a><a href=\"/systems-biology\">Systems Biology</a>\n</h2>\n<ul>\n<li><a\
    \ href=\"/systems-biology/cellphonedb\">cellphonedb</a></li>\n</ul>\n<h2>\n<a\
    \ id=\"user-content-utilities\" class=\"anchor\" href=\"#utilities\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a\
    \ href=\"/utilities\">Utilities</a>\n</h2>\n<ul>\n<li><a href=\"/utilities/xvfb\"\
    >Xvfb</a></li>\n<li><a href=\"/utilities/ariba\">ariba</a></li>\n<li><a href=\"\
    /utilities/atom\">atom</a></li>\n<li><a href=\"/utilities/datalad\">datalad</a></li>\n\
    <li><a href=\"/utilities/gdc-client\">gdc-client</a></li>\n<li><a href=\"/utilities/longshot\"\
    >longshot</a></li>\n<li><a href=\"/utilities/pdf2svg\">pdf2svg</a></li>\n<li><a\
    \ href=\"/utilities/pyega3\">pyega3</a></li>\n<li><a href=\"/utilities/snp-sites\"\
    >snp-sites</a></li>\n<li><a href=\"/utilities/sysbench\">sysbench</a></li>\n<li><a\
    \ href=\"/utilities/uropa\">uropa</a></li>\n<li><a href=\"/utilities/vcf2db\"\
    >vcf2db</a></li>\n<li><a href=\"/utilities/visidata\">visidata</a></li>\n<li><a\
    \ href=\"/utilities/whatshap\">whatshap</a></li>\n</ul>\n"
  stargazers_count: 6
  subscribers_count: 6
  topics: []
  updated_at: 1621576784.0
NVIDIA/hpc-container-maker:
  data_format: 2
  description: HPC Container Maker
  filenames:
  - recipes/hpccm/Singularity.def
  full_name: NVIDIA/hpc-container-maker
  latest_release: null
  readme: '<p><a href="https://github.com/NVIDIA/hpc-container-maker/actions?query=workflow%3A%22Python+3%22"><img
    src="https://github.com/NVIDIA/hpc-container-maker/workflows/Python%203/badge.svg"
    alt="Python 3" style="max-width:100%;"></a>

    <a href="https://github.com/NVIDIA/hpc-container-maker/actions?query=workflow%3A%22Python+2%22"><img
    src="https://github.com/NVIDIA/hpc-container-maker/workflows/Python%202/badge.svg"
    alt="Python 2" style="max-width:100%;"></a>

    <a href="https://anaconda.org/conda-forge/hpccm" rel="nofollow"><img src="https://camo.githubusercontent.com/90b084021ad87cd5d22b3c0a5eb48d35f96c19aed786c703e9c4b6f735e59479/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f636f6e64612d666f7267652f687063636d3f6c6162656c3d436f6e6461253230646f776e6c6f616473"
    alt="Conda" data-canonical-src="https://img.shields.io/conda/dn/conda-forge/hpccm?label=Conda%20downloads"
    style="max-width:100%;"></a>

    <a href="https://pypi.org/project/hpccm/" rel="nofollow"><img src="https://camo.githubusercontent.com/fc8ae25e1ec1ff10f43999db82f62d053fd8f7d7fe735335cd87eae369f0640c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f687063636d3f6c6162656c3d50795049253230646f776e6c6f616473"
    alt="PyPI - Downloads" data-canonical-src="https://img.shields.io/pypi/dm/hpccm?label=PyPI%20downloads"
    style="max-width:100%;"></a>

    <a href="https://lgtm.com/projects/g/NVIDIA/hpc-container-maker/context:python"
    rel="nofollow"><img src="https://camo.githubusercontent.com/c5cb9f60bc18eec63ef91be12f6390a9eccaa2ebcccd607a526aa17076d011cb/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f4e56494449412f6870632d636f6e7461696e65722d6d616b65722e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138"
    alt="Language grade: Python" data-canonical-src="https://img.shields.io/lgtm/grade/python/g/NVIDIA/hpc-container-maker.svg?logo=lgtm&amp;logoWidth=18"
    style="max-width:100%;"></a>

    <a href="https://github.com/NVIDIA/hpc-container-maker/blob/master/LICENSE"><img
    src="https://camo.githubusercontent.com/c1dbf7021ca84175a5ff16f8b927fa120abad46a944de3ef991aeb54e1c8cba6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4e56494449412f6870632d636f6e7461696e65722d6d616b6572"
    alt="License" data-canonical-src="https://img.shields.io/github/license/NVIDIA/hpc-container-maker"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-hpc-container-maker" class="anchor" href="#hpc-container-maker"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>HPC
    Container Maker</h1>

    <p>HPC Container Maker (HPCCM - pronounced H-P-see-M) is an open source

    tool to make it easier to generate container specification files.</p>

    <ul>

    <li>

    <a href="/docs">Documentation</a>

    <ul>

    <li><a href="/docs/getting_started.md">Getting Started</a></li>

    <li><a href="/docs/tutorial.md">Tutorial</a></li>

    <li><a href="/docs/recipes.md">Recipes</a></li>

    <li><a href="/docs/workflows.md">Workflows</a></li>

    <li><a href="/docs/building_blocks.md">API: Building Blocks</a></li>

    <li><a href="/docs/primitives.md">API: Primitives</a></li>

    <li><a href="/docs/misc_api.md">API: Miscellaneous</a></li>

    </ul>

    </li>

    <li><a href="/recipes/">Examples</a></li>

    <li><a href="/docs/citation.md">Citation</a></li>

    <li><a href="/LICENSE">License</a></li>

    </ul>

    <h2>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

    <p>HPC Container Maker generates Dockerfiles or Singularity definition

    files from a high level Python recipe.  HPCCM recipes have some

    distinct advantages over "native" container specification formats.</p>

    <ol>

    <li>

    <p>A library of HPC <a href="/docs/building_blocks.md">building blocks</a> that

    separate the choice of what to include in a container image from

    the details of how it''s done.  The building blocks transparently

    provide the latest component and container best practices.</p>

    </li>

    <li>

    <p>Python provides increased flexibility over static container

    specification formats.  Python-based recipes can branch, validate

    user input, etc. - the same recipe can generate multiple container

    specifications.</p>

    </li>

    <li>

    <p>Generate either Dockerfiles or Singularity definition files from

    the same recipe.</p>

    </li>

    </ol>

    <h2>

    <a id="user-content-additional-resources" class="anchor" href="#additional-resources"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Additional
    Resources</h2>

    <ul>

    <li>

    <a href="https://github.com/HPCSYSPROS/Workshop18/blob/master/Making_Containers_Easier_with_HPC_Container_Maker/ws_hpcsysp103.pdf">Making
    Containers Easier With HPC Container Maker (paper)</a>, presented at the <a href="/docs/citation.md">HPC
    Systems Professionals Workshop at SC18</a>

    </li>

    <li><a href="http://on-demand.gputechconf.com/supercomputing/2018/video/sc1843-making-containers-easier-hpc-container-maker.html"
    rel="nofollow">Overview presentation at SC18 (video)</a></li>

    <li><a href="https://www.nvidia.com/content/webinar-portal/src/webinar-portal.html?D2C=1802760&amp;isSocialSharing=Y&amp;partnerref=emailShareFromGateway"
    rel="nofollow">Making Containers Easier with HPC Container Maker (webinar)</a></li>

    <li>

    <a href="http://www.admin-magazine.com/HPC/Articles/HPC-Container-Maker" rel="nofollow">ADMIN
    Magazine article</a> by Jeff Layton</li>

    <li>

    <a href="https://devblogs.nvidia.com/making-containers-easier-with-hpc-container-maker/"
    rel="nofollow">NVIDIA Developer Blog</a> by Scott McMillan</li>

    </ul>

    '
  stargazers_count: 278
  subscribers_count: 22
  topics:
  - containers
  - hpc
  - docker
  - singularity
  updated_at: 1621304635.0
Nahuel-Mk2/def-space:
  data_format: 2
  description: Def File of Singularity
  filenames:
  - def/edge-connect.def
  - def/contextual-attention.def
  - def/sc-fegan.def
  - def/singan.def
  - def/vae-mnist.def
  - def/wav2pix.def
  - def/lafin.def
  - def/stargan.def
  full_name: Nahuel-Mk2/def-space
  latest_release: null
  readme: '<h1>

    <a id="user-content-def-space" class="anchor" href="#def-space" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>def-space</h1>

    <p>This repository is def-space for Singularity</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606215100.0
OxfordSKA/OSKAR:
  data_format: 2
  description: SKA Radio Telescope Simulator
  filenames:
  - singularity/Singularity.base-dep
  - singularity/Singularity.base
  - singularity/Singularity.python3
  full_name: OxfordSKA/OSKAR
  latest_release: 2.7.6
  readme: "<p><a href=\"https://github.com/OxfordSKA/OSKAR/releases\"><img src=\"\
    https://camo.githubusercontent.com/2b31f28bf12daa344a4541064866fa171f3f3989a53604cec781f6c15206daab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f4f78666f7264534b412f4f534b41522e7376673f7374796c653d666c61742d737175617265\"\
    \ alt=\"GitHub release\" data-canonical-src=\"https://img.shields.io/github/release/OxfordSKA/OSKAR.svg?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://doi.org/10.5281/zenodo.3758491\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/db055c7e33d6db9ce4f5f69f83628cd0827c9a0e13aa2aa78a369d938a4ef137/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333735383439312e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3758491.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-oskar-a-gpu-accelerated-simulator-for-the-square-kilometre-array\"\
    \ class=\"anchor\" href=\"#oskar-a-gpu-accelerated-simulator-for-the-square-kilometre-array\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>OSKAR: A GPU-accelerated simulator for the Square Kilometre Array</h1>\n\
    <p>OSKAR has been designed to produce simulated visibility data from radio\ntelescopes\
    \ containing aperture arrays, such as those envisaged for the\nSquare Kilometre\
    \ Array.</p>\n<p>A source code archive, and pre-built binary packages for Linux\
    \ (using\nSingularity), macOS and Windows platforms are available to download\
    \ from</p>\n<ul>\n<li><a href=\"https://github.com/OxfordSKA/OSKAR/releases\"\
    >https://github.com/OxfordSKA/OSKAR/releases</a></li>\n</ul>\n<p>OSKAR is licensed\
    \ under the terms of the 3-clause BSD License.\nPlease see the <a href=\"LICENSE\"\
    >LICENSE</a> file for details.</p>\n<h3>\n<a id=\"user-content-singularity-image\"\
    \ class=\"anchor\" href=\"#singularity-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity image</h3>\n<p>A\
    \ pre-built <a href=\"https://sylabs.io/singularity/\" rel=\"nofollow\">Singularity</a>\
    \ SIF container image\nis available for Linux which can be used to run OSKAR command\
    \ line\napplications or Python scripts directly, without needing to compile or\
    \ install\nanything. For Singularity 3.0 or later, an application or script can\
    \ be run\nusing the downloaded <a href=\"https://github.com/OxfordSKA/OSKAR/releases\"\
    >container</a>\nwith the <code>singularity exec</code> command, which takes the\
    \ form:</p>\n<pre><code>$ singularity exec [flags] &lt;container_path&gt; &lt;app_name&gt;\
    \ &lt;arguments&gt;...\n</code></pre>\n<p>Use the <code>--nv</code> flag to enable\
    \ NVIDIA GPU support in Singularity, if\napplicable.</p>\n<p>Note also that Singularity\
    \ will mount the home directory into the container by\ndefault, unless configured\
    \ otherwise. If you have packages installed in your\nhome area that should be\
    \ kept isolated from those in the container (for\nexample, because of conflicting\
    \ packages or Python versions, or if you see\nother errors caused by trying to\
    \ load wrong versions of shared libraries when\nstarting the container) then it\
    \ may be necessary to disable this either by\nusing the <code>--no-home</code>\
    \ flag, or re-bind the home directory in the container\nto somewhere other than\
    \ your actual $HOME using the <code>-H</code> flag.</p>\n<p>As an example, to\
    \ run the application <code>oskar_sim_interferometer</code>\nwith a parameter\
    \ file <code>settings.ini</code> and a container image file\n<code>OSKAR-Python3.sif</code>\
    \ (both in the current directory) on a GPU use:</p>\n<pre><code>$ singularity\
    \ exec --no-home --nv ./OSKAR-Python3.sif oskar_sim_interferometer settings.ini\n\
    </code></pre>\n<p>Similarly, to run a Python script <code>sim_script.py</code>\
    \ that uses OSKAR:</p>\n<pre><code>$ singularity exec --no-home --nv ./OSKAR-Python3.sif\
    \ python3 sim_script.py\n</code></pre>\n<h3>\n<a id=\"user-content-dependencies\"\
    \ class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Dependencies</h3>\n<p>If hardware\
    \ acceleration is required, be sure to install appropriate GPU\ndrivers which\
    \ are supported by the hardware manufacturer. Third-party graphics\ndrivers are\
    \ unlikely to work.</p>\n<p>When building from source, the only required dependency\
    \ is\n<a href=\"https://cmake.org\" rel=\"nofollow\">CMake &gt;= 3.1</a>.\nAll\
    \ other dependencies are optional, but functionality will be\nlimited if these\
    \ are not found by CMake.\n<em>Note that these dependencies are required only\
    \ if building from source</em>, not\nif using a <a href=\"https://github.com/OxfordSKA/OSKAR/releases\"\
    >pre-built package</a>.</p>\n<ul>\n<li><a href=\"https://cmake.org\" rel=\"nofollow\"\
    >CMake &gt;= 3.1</a></li>\n<li>(Optional) <a href=\"https://developer.nvidia.com/cuda-downloads\"\
    \ rel=\"nofollow\">CUDA &gt;= 7.0</a>\nor OpenCL, required for GPU acceleration\
    \ on supported hardware.</li>\n<li>(Optional) <a href=\"https://qt.io\" rel=\"\
    nofollow\">Qt 5</a>,\nrequired to build the graphical user interface.</li>\n<li>(Optional)\
    \ <a href=\"https://github.com/casacore/casacore\">casacore &gt;= 2.0</a>,\nrequired\
    \ to use CASA Measurement Sets.</li>\n</ul>\n<p>Packages for these dependencies\
    \ are available in the package repositories\nof many recent Linux distributions,\
    \ including Debian and Ubuntu.</p>\n<h3>\n<a id=\"user-content-build-commands\"\
    \ class=\"anchor\" href=\"#build-commands\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build commands</h3>\n<p>To build\
    \ from source, either clone the repository using\n<code>git clone https://github.com/OxfordSKA/OSKAR.git</code>\
    \ (for the current master\nbranch) or download and unpack the source archive,\
    \ then:</p>\n<pre><code>$ mkdir build\n$ cd build\n$ cmake [OPTIONS] ../path/to/top/level/source/folder\n\
    $ make -j4\n$ make install\n</code></pre>\n<p>When running the 'cmake' command\
    \ a number of options can be specified:</p>\n<pre><code>* -DCUDA_ARCH=\"&lt;arch&gt;\"\
    \ (default: all)\n    Sets the target architecture for the compilation of CUDA\
    \ device code.\n    &lt;arch&gt; must be one of either: 2.0, 2.1, 3.0, 3.2, 3.5,\
    \ 3.7,\n                                  5.0, 5.2, 6.0, 6.1, 6.2, 7.0, 7.5,\n\
    \                                  8.0, 8.6 or ALL.\n    ALL is for all currently\
    \ supported architectures.\n    Separate multiple architectures using semi-colons,\
    \ if required.\n\n* -DCMAKE_INSTALL_PREFIX=&lt;path&gt; (default: /usr/local/)\n\
    \    Path prefix used to install OSKAR (with make install).\n</code></pre>\n<h4>\n\
    <a id=\"user-content-advanced-build-options\" class=\"anchor\" href=\"#advanced-build-options\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Advanced build options</h4>\n<pre><code>* -DCASACORE_LIB_DIR=&lt;path&gt;\
    \ (default: searches the system library paths)\n    Specifies a location to search\
    \ for the casacore libraries\n    (libcasa_ms.so and others) if they are not in\
    \ the system library path.\n\n* -DCASACORE_INC_DIR=&lt;path&gt; (default: searches\
    \ the system include paths)\n    Specifies a location to search for the casacore\
    \ library headers if they\n    are not in the system include path.\n    This is\
    \ the path to the top level casacore include folder.\n\n* -DCMAKE_PREFIX_PATH=&lt;path&gt;\
    \ (default: None)\n    Specifies a location to search for Qt 5 if it is not in\
    \ a standard\n    system path. For example, if using Homebrew on macOS, this may\
    \ need\n    to be set to /usr/local/opt/qt5/\n\n* -DFIND_CUDA=ON|OFF (default:\
    \ ON)\n    Can be used not to find or link against CUDA.\n\n* -DFIND_OPENCL=ON|OFF\
    \ (default: OFF)\n    Can be used not to find or link against OpenCL.\n    OpenCL\
    \ support in OSKAR is currently experimental.\n\n* -DNVCC_COMPILER_BINDIR=&lt;path&gt;\
    \ (default: None)\n    Specifies a nvcc compiler binary directory override. See\
    \ nvcc help.\n    This is likely to be needed only on macOS when the version of\
    \ the\n    compiler picked up by nvcc (which is related to the version of XCode\n\
    \    being used) is incompatible with the current version of CUDA.\n    Set this\
    \ to 'clang' on macOS if using GCC to build the rest of OSKAR.\n\n* -DFORCE_LIBSTDC++=ON|OFF\
    \ (default: OFF)\n    If ON forces the use of libstdc++ with the Clang compiler.\n\
    \    Used for controlling linking behaviour when using clang\n    or clang-omp\
    \ compilers with dependencies which may have been compiled\n    against libstdc++\n\
    \n* -DCMAKE_BUILD_TYPE=&lt;release or debug&gt; (default: release)\n    Build\
    \ in release or debug mode.\n\n* -DBUILD_INFO=ON|OFF (default: OFF)\n    If ON\
    \ enables the display of diagnostic build information when\n    running CMake.\n\
    </code></pre>\n<h3>\n<a id=\"user-content-unit-tests\" class=\"anchor\" href=\"\
    #unit-tests\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Unit tests</h3>\n<p>From the build directory, the\
    \ unit tests can be run by typing:</p>\n<pre><code>$ ctest [--verbose]\n</code></pre>\n\
    <h3>\n<a id=\"user-content-python-interface\" class=\"anchor\" href=\"#python-interface\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Python interface</h3>\n<p>After installing OSKAR, the Python interface\
    \ to it can be installed to\nmake it easier to use from Python scripts.\nStraightforward\
    \ instructions for installation with <code>pip</code> can be\n<a href=\"python/README.md\"\
    >found in the python subdirectory</a>.</p>\n<h3>\n<a id=\"user-content-example-simulation\"\
    \ class=\"anchor\" href=\"#example-simulation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Example simulation</h3>\n<p>The\
    \ example simulation described in the\n<a href=\"https://github.com/OxfordSKA/OSKAR/releases\"\
    >documentation</a>\ncan be run to check that a simple simulation behaves as expected.</p>\n"
  stargazers_count: 31
  subscribers_count: 8
  topics:
  - astronomy
  - radio-telescopes
  - simulator
  updated_at: 1621880107.0
PREDICT-DPACC/dpdash-devel:
  data_format: 2
  description: Dashboard for project tracking, inspired by https://github.com/harvard-nrg/dpdash
  filenames:
  - singularity/Singularity
  full_name: PREDICT-DPACC/dpdash-devel
  latest_release: null
  readme: '<p>Dashboard for project tracking, adapted from <a href="https://github.com/harvard-nrg/dpdash">https://github.com/harvard-nrg/dpdash</a></p>

    <p>Maintained by Tashrif Billah, PNL, BWH (HMS)</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1611883890.0
ParkerLab/containers:
  data_format: 2
  description: Singularity definition files for various bioinformatics tasks
  filenames:
  - general/Singularity
  - snATAC/Singularity
  - snRNA/Singularity
  full_name: ParkerLab/containers
  latest_release: null
  readme: '<h2>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h2>

    <p>Repository containers Singularity container defintion files and necessary commands
    to

    build those containers.</p>

    <p>Ideally, you''d want to keep these definition files updated so that lab members
    can

    re-build containers as needed.</p>

    <h2>

    <a id="user-content-locate-singularity-images" class="anchor" href="#locate-singularity-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Locate
    Singularity images</h2>

    <p>Pre-built Singularity images are located at <code>/lab/data/sw/containers/</code>.</p>

    <p>Thanks to Peter Orchard (@porchard).</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1580658783.0
Pathogen-Genomics-Cymru/tb-pipeline:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity.ppFqtools
  - singularity/Singularity.ppPerljson
  - singularity/Singularity.ppFastqc
  - singularity/Singularity.ppMykrobe
  - singularity/Singularity.ppBowtie2
  - singularity/Singularity.ppBedtools
  - singularity/Singularity.ppBwa
  - singularity/Singularity.ppFastp
  - singularity/Singularity.ppKraken2
  full_name: Pathogen-Genomics-Cymru/tb-pipeline
  latest_release: null
  readme: '<h1>

    <a id="user-content-mycobacterial-pre-processing-pipeline" class="anchor" href="#mycobacterial-pre-processing-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mycobacterial
    Pre-processing Pipeline</h1>

    <p>Cleans and QCs reads with fastp and FastQC, classifies with Kraken2 &amp; Mykrobe,
    removes non-bacterial content, and - by alignment to any minority genomes - disambiguates
    mixtures of bacterial reads.</p>

    <p>Takes as input one directory containing pairs of fastq(.gz) or bam files.

    Produces as output one directory per sample, containing the relevant reports &amp;
    a pair of cleaned fastqs.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>The workflow is designed to run with either docker <code>-profile docker</code>
    or singularity <code>-profile singularity</code>. Before running the workflow
    using singularity, the singularity images for the workflow will need to be built
    by running <code>singularity/singularity_pull.sh</code></p>

    <p>E.g. to run the workflow:</p>

    <pre><code>nextflow run main.nf -profile singularity --filetype fastq --input_dir
    fq_dir --pattern "*_R{1,2}.fastq.gz" --unmix_myco yes \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj


    nextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco
    no \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj

    </code></pre>

    <h2>

    <a id="user-content-params" class="anchor" href="#params" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Params</h2>

    <p>The following parameters should be set in <code>nextflow.config</code> or specified
    on the command line:</p>

    <ul>

    <li>

    <strong>input_dir</strong><br>

    Directory containing fastq OR bam files</li>

    <li>

    <strong>filetype</strong><br>

    File type in input_dir. Either "fastq" or "bam"</li>

    <li>

    <strong>pattern</strong><br>

    Regex to match fastq files in input_dir, e.g. "*_R{1,2}.fq.gz"</li>

    <li>

    <strong>output_dir</strong><br>

    Output directory</li>

    <li>

    <strong>unmix_myco</strong><br>

    Do you want to disambiguate mixed-mycobacterial samples by read alignment? Either
    "yes" or "no"</li>

    <li>

    <strong>species</strong><br>

    Principal species in each sample, assuming genus Mycobacterium. Default ''null''.
    If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae,
    chimaera, fortuitum, intracellulare, kansasii, tuberculosis</li>

    <li>

    <strong>kraken_db</strong><br>

    Directory containing <code>*.k2d</code> Kraken2 database files (obtain from <a
    href="https://benlangmead.github.io/aws-indexes/k2" rel="nofollow">https://benlangmead.github.io/aws-indexes/k2</a>)</li>

    <li>

    <strong>bowtie2_index</strong><br>

    Directory containing Bowtie2 index (obtain from <a href="ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip"
    rel="nofollow">ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip</a>).
    The specified path should NOT include the index name</li>

    <li>

    <strong>bowtie_index_name</strong><br>

    Name of the bowtie index, e.g. hg19_1kgmaj<br>

    </li>

    </ul>

    <br>

    <p>For more information on the parameters run <code>nextflow run main.nf --help</code></p>

    <h2>

    <a id="user-content-checkpoints" class="anchor" href="#checkpoints" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoints</h2>

    <p>Checkpoints used throughout this workflow to fail a sample/issue warnings:</p>

    <p>processes preprocessing_checkFqValidity or preprocessing_checkBamValidity</p>

    <ol>

    <li>(Fail) If sample does not pass fqtools ''validate'' or samtools ''quickcheck'',
    as appropriate.</li>

    </ol>

    <p>process preprocessing_countReads<br>

    2. (Fail) If sample contains &lt; 100k pairs of raw reads.</p>

    <p>process preprocessing_fastp<br>

    3. (Fail) If sample contains &lt; 100k pairs of cleaned reads, required to all
    be &gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10
    --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size
    1 --cut_tail_mean_quality 20).</p>

    <p>process preprocessing_kraken2<br>

    4. (Fail) If the top family hit is not Mycobacteriaceae<br>

    5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae <br>

    6. (Warn) If the top family classification is mycobacterial, but this is not consistent
    with top genus and species classifications<br>

    7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications
    meet minimum thresholds of &gt; 5000 reads or &gt; 0.5% of the total reads (this
    is not necessarily a concern as not all mycobacteria have a taxonomic classification
    at this rank) <br>

    8. (Warn) If sample is mixed or contaminated - defined as containing reads &gt;
    the 5000/0.5% thresholds from multiple non-human species<br>

    9. (Warn) If sample contains multiple classifications to mycobacterial species
    complexes, each meeting the &gt; 5000/0.5% thresholds<br>

    10. (Warn) If no species classification meets the 5000/0.5% thresholds<br>

    11. (Warn) If no genus classification meets the 5000/0.5% thresholds<br>

    12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant
    given point 5)</p>

    <p>process preprocessing_identifyBacterialContaminants<br>

    13. (Fail) If the sample is not contaminated and the top species hit is not one
    of the 10 supported Mycobacteria:\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis<br>

    14. (Fail) If the sample is not contaminated and the top species hit is contrary
    to the species expected (e.g. "avium" rather than "tuberculosis" - only tested
    if you provide that expectation)<br>

    15. (Warn) If the top species hit is supported by &lt; 75% coverage<br>

    16. (Warn) If the top species hit has a median coverage depth &lt; 10-fold<br>

    17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant
    species, which means we will not be able to locate its genome, and thereby remove
    it as a contaminant<br>

    18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated
    with a contaminant species'' taxon ID<br>

    19. (Warn) If no complete genome could be found for a contaminant species. The
    workflow will proceed with alignment-based contaminant removal, but you''re warned
    that there''s reduced confidence in detecting reads from this species</p>

    <p>process preprocessing_downloadContamGenomes<br>

    20. (Fail) If a contaminant is detected but we are unable to download a representative
    genome, and thereby remove it</p>

    <p>process preprocessing_summarise<br>

    21. (Fail) If after having taken an alignment-based approach to decontamination,
    Kraken still detects a contaminant species<br>

    22. (Fail) If after having taken an alignment-based approach to decontamination,
    the top species hit is not one of the 10 supported Mycobacteria<br>

    23. (Fail) If, after successfully removing contaminants, the top species hit is
    contrary to the species expected (e.g. "avium" rather than "tuberculosis" - only
    tested if you provide that expectation)</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1620947690.0
PawseySC/containers-openfoam-workshop-scripts:
  data_format: 2
  description: UNDER CONSTRUCTION - Scripts for the workshop on OpenFOAM containers
  filenames:
  - 04_buildingAnOpenFOAMContainer/openfoam-2.4.x/02_PortingToSingularity/Singularity.def
  full_name: PawseySC/containers-openfoam-workshop-scripts
  latest_release: null
  readme: '<h1>

    <a id="user-content-workshop-on-the-usage-of-openfoam-containers-at-pawsey" class="anchor"
    href="#workshop-on-the-usage-of-openfoam-containers-at-pawsey" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Workshop on the usage
    of OpenFOAM containers at Pawsey</h1>

    <p><strong>Organisers</strong>: Alexis Espinosa (PawseySC) and Marco De La Pierre
    (PawseySC)</p>

    <p>The use of containers has become an attractive framework for several areas
    of research supported by Pawsey (including bioinformatics and machine learning,
    among others).</p>

    <p><strong>Now, Pawsey supports the usage of OpenFOAM containers.</strong> For
    the most recent versions of OpenFOAM (and some others), Pawsey have prebuilt and
    tested Singularity containers.</p>

    <p>This repository contains material for the exercises of the workshop.</p>

    <p><strong>Step-by-step guide</strong>: <a href="https://pawseysc.github.io/containers-openfoam-workshop"
    rel="nofollow">https://pawseysc.github.io/containers-openfoam-workshop</a></p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1619177799.0
PawseySC/pawsey-containers:
  data_format: 2
  description: A collection of Dockerfiles and Singularity deffiles for Pawsey-supported
    images
  filenames:
  - OpenFOAM/installationsWithAdditionalTools/openfoam-5.x_CFDEM/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-7/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-v1712/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-v2006/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-v1812/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-v1912/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-5.x/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-v2012/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-2.2.0/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-2.4.x/02_PortingToSingularity/Singularity.def
  - OpenFOAM/basicInstallations/openfoam-8/02_PortingToSingularity/Singularity.def
  full_name: PawseySC/pawsey-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-pawsey-containers" class="anchor" href="#pawsey-containers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>pawsey-containers</h1>

    <p>This is a repo to store and track Dockerfiles and Singularity deffiles for
    images built and maintained by Pawsey Supercomputing Centre.</p>

    <p>Docker repo: <a href="https://hub.docker.com/u/pawsey" rel="nofollow">https://hub.docker.com/u/pawsey</a></p>

    '
  stargazers_count: 4
  subscribers_count: 10
  topics: []
  updated_at: 1617165302.0
Poulami-Sarkar/Bengali-Hindi-OCR:
  data_format: 2
  description: null
  filenames:
  - Singularity.BenOCR
  full_name: Poulami-Sarkar/Bengali-Hindi-OCR
  latest_release: null
  readme: '<h1>

    <a id="user-content-bengali-ocr" class="anchor" href="#bengali-ocr" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Bengali-OCR</h1>

    <h2>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

    <p>This project implements OCR for television news from Bengali and Hindi news
    channels. I am using OpenCV along with a pre-trained tensorflow model called EAST(An
    Efficient and Accurte Scene Test detector) for detecting ROI (Regions of interest)
    from the news videos.

    Then the detected ROIs are extracted and OCR, implemented using tesseract 4.0
    is used to exract the text.</p>

    <h2>

    <a id="user-content-prequisites" class="anchor" href="#prequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prequisites</h2>

    <p><code>apt update</code>

    <code>apt install -y python3-pip build-essential libssl-dev</code> <code>libffi-dev
    python3-dev</code>

    <code>apt install  -y tesseract-ocr</code>

    <code>apt install -y libtesseract-dev libsm-dev</code>

    <code>pip3 install pytesseract opencv-python numpy</code></p>

    <h2>

    <a id="user-content-working" class="anchor" href="#working" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Working</h2>

    <h2>

    <a id="user-content-running-the-code" class="anchor" href="#running-the-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    the code</h2>

    <p><code>python3 textdetection.py &lt;filename&gt;</code>

    Specify additional argument ''O'' for running OCR.

    Output is saved to the file output/output.txt</p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1566655098.0
RationalTangle/hcc:
  data_format: 2
  description: null
  filenames:
  - Singularity.snippy
  - Singularity.py3-matt
  - Singularity.py2
  - Singularity.py3-pytorch
  - Singularity.py3-21
  full_name: RationalTangle/hcc
  latest_release: null
  readme: '<h1>

    <a id="user-content-hcc" class="anchor" href="#hcc" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>hcc</h1>

    <p>A collection of useful scripts and containers for use with the HCC cluster.</p>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1619580780.0
SKA-INAF/caesar-container:
  data_format: 2
  description: Singularity container for CAESAR software tool
  filenames:
  - caesar/Singularity-fromdocker.xenial
  - caesar/Singularity.xenial
  - caesar-base/Singularity.xenial
  full_name: SKA-INAF/caesar-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-caesar-container" class="anchor" href="#caesar-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>caesar-container</h1>

    <p>Singularity container for CAESAR software tool</p>

    <h2>

    <a id="user-content-about" class="anchor" href="#about" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><strong>About</strong>

    </h2>

    <p>This repository provides the following Singularity recipe files:</p>

    <ul>

    <li>

    <p><strong>caesar-base</strong>: recipe to build a Singularity base container
    for CAESAR tool (maintained at <a href="https://github.com/SKA-INAF/caesar.git">https://github.com/SKA-INAF/caesar.git</a>)
    from a docker Ubuntu image (Ubuntu 16 xenial). The built container will provide
    all the dependencies needed to build CAESAR:</p>

    <ul>

    <li>ROOT</li>

    <li>OpenCV</li>

    <li>BOOST</li>

    <li>Log4Cxx</li>

    <li>Jsoncpp</li>

    <li>cfitsio</li>

    <li>protobuf</li>

    <li>MPICH</li>

    <li>OpenMPI</li>

    <li>python 2.7 + modules (numpy, astropy, pyfits)</li>

    </ul>

    </li>

    </ul>

    <p>The container is used mainly to build CAESAR upon it.</p>

    <ul>

    <li>

    <strong>caesar</strong>: recipe to build a CAESAR singularity container from latest
    release (<a href="https://github.com/SKA-INAF/caesar.git">https://github.com/SKA-INAF/caesar.git</a>))
    using a local caesar base image (built using previous recipe).</li>

    </ul>

    <h2>

    <a id="user-content-how-to-build-caesar-images" class="anchor" href="#how-to-build-caesar-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>How
    to build CAESAR images?</strong>

    </h2>

    <h3>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><strong>Prerequisites</strong>

    </h3>

    <p>To build images on your system you must have a recent version of Singularity
    (<a href="https://github.com/singularityware/singularity.git">https://github.com/singularityware/singularity.git</a>)
    installed. CAESAR makes use of singularity apps so a singularity version &gt;2.3
    is required. A system-wide installation is required, e.g. you need root permissions
    on the build system.</p>

    <h3>

    <a id="user-content-build-images" class="anchor" href="#build-images" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><strong>Build images</strong>

    </h3>

    <p>To build images do the following:</p>

    <p><code>singularity build [IMAGE_NAME] [RECIPE_FILE]</code></p>

    <p>where:</p>

    <ul>

    <li>

    <code>IMAGE_NAME</code>: Name of image file you want to be created (e.g. caesar-base.simg
    for base container or caesar.simg for caesar container).</li>

    <li>

    <code>RECIPE_FILE</code>: Name of the recipe file provided in this repository
    (e.g. Singularity.xenial)</li>

    </ul>

    <h2>

    <a id="user-content-how-to-use-caesar-images" class="anchor" href="#how-to-use-caesar-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>How
    to use CAESAR images?</strong>

    </h2>

    <p>To see the list of apps installed in your CAESAR container (say named <code>caesar.simg</code>):</p>

    <ul>

    <li><code> [...]$ singularity apps caesar.simg</code></li>

    </ul>

    <p>To run one app (say the <code>sfinder</code> app):</p>

    <ul>

    <li><code>[...]$ singularity run --app sfinder caesar.simg --config=myconfig.cfg</code></li>

    </ul>

    <pre><code>

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602170137.0
Samanwaya1301/ET_CE_bilby_pipe:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity.dev
  - containers/Singularity.0.0.3
  - containers/Singularity.0.0.4
  - containers/Singularity.0.0.2
  - containers/Singularity.0.0.1
  full_name: Samanwaya1301/ET_CE_bilby_pipe
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpcplayground" class="anchor" href="#hpcplayground" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPCPlayground</h1>

    <p>This repository should contain all the works related to HPC</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610562729.0
Samanwaya1301/bilby-BHNS:
  data_format: 2
  description: null
  filenames:
  - containers/Singularity.0.3.6
  - containers/Singularity.0.4.1
  - containers/Singularity.0.3.5
  - containers/Singularity.0.3.3
  - containers/Singularity.0.4.0
  full_name: Samanwaya1301/bilby-BHNS
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"\
    #universal-data-tool\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Universal Data Tool</h1>\n<p><a href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ alt=\"Master Branch\" style=\"max-width:100%;\"></a>\n<a href=\"https://badge.fury.io/js/universal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    ><img src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\"\
    \ alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\"\
    \ alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://twitter.com/UniversalDataTl\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\"\
    \ alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Try it out at <a href=\"https://udt.dev\"\
    \ rel=\"nofollow\">udt.dev</a>, <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >download the desktop app</a> or <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">run on-premise</a>.</p>\n<p align=\"center\">\n  <a href=\"\
    https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p align=\"center\">\n  <b>\n  <a href=\"\
    https://docs.universaldatatool.com\" rel=\"nofollow\">Docs</a> \u2022 <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">Website</a> \u2022 <a href=\"\
    https://udt.dev\" rel=\"nofollow\">Playground</a> \u2022 <a href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\"\
    \ rel=\"nofollow\">Library Usage</a> \u2022 <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">On-Premise</a>\n  </b>\n</p>\n<p>The Universal Data Tool is\
    \ a web/desktop app for editing and annotating images, text, audio, documents\
    \ and to view and edit any data defined in the extensible <a href=\"https://github.com/UniversalDataTool/udt-format\"\
    >.udt.json and .udt.csv standard</a>.</p>\n<h2>\n<a id=\"user-content-supported-data\"\
    \ class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Supported Data</h2>\n<p align=\"\
    center\">\n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\"\
    \ rel=\"nofollow\">Image Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\"\
    \ rel=\"nofollow\">Image Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\"\
    \ rel=\"nofollow\">Text Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\"\
    \ rel=\"nofollow\">Named Entity Recognition</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\"\
    \ rel=\"nofollow\">Named Entity Relations / Part of Speech Tagging</a> \u2022\
    \ \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\"\
    \ rel=\"nofollow\">Audio Transcription</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\"\
    \ rel=\"nofollow\">Data Entry</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\"\
    \ rel=\"nofollow\">Video Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\"\
    \ rel=\"nofollow\">Landmark / Pose Annotation</a>\n</p>\n<h2>\n<a id=\"user-content-recent-updates\"\
    \ class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recent Updates</h2>\n<p><a href=\"\
    https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\">Follow\
    \ our development on Youtube!</a></p>\n\n<ul>\n<li><a href=\"https://youtu.be/q20WrCRcG4k\"\
    \ rel=\"nofollow\">Community Update Video 9</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\"\
    \ rel=\"nofollow\">Community Update Video 8</a></li>\n<li>\n<a href=\"https://youtu.be/glPPFgXibdw\"\
    \ rel=\"nofollow\">Community Update Video 7</a> <a href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\"\
    \ rel=\"nofollow\">(blog version)</a>\n\n</li>\n</ul>\n<h2>\n<a id=\"user-content-features\"\
    \ class=\"anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Features</h2>\n<ul>\n<li><strong>Collaborate\
    \ with others in real time, no sign up!</strong></li>\n<li>Usable on <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">web</a> or as <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\
    >Windows,Mac or Linux desktop application</a>\n</li>\n<li>Configure your project\
    \ with an easy-to-use GUI</li>\n<li><a href=\"https://universaldatatool.com/courses\"\
    \ rel=\"nofollow\">Easily create courses to train your labelers</a></li>\n<li>Download/upload\
    \ as easy-to-use CSV (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\
    >sample.udt.csv</a>) or JSON (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\
    >sample.udt.json</a>)</li>\n<li>Support for Images, Videos, PDFs, Text, Audio\
    \ Transcription and many other formats</li>\n<li>Can be <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\
    >easily integrated into a React application</a>\n</li>\n<li>Annotate images or\
    \ videos with classifications, tags, bounding boxes, polygons and points</li>\n\
    <li>Fast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly</li>\n\
    <li>Import data from Google Drive, Youtube, CSV, Clipboard and more</li>\n<li>Annotate\
    \ NLP datasets with Named Entity Recognition (NER), classification and Part of\
    \ Speech (PoS) tagging.</li>\n<li>Easily <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\
    >load into pandas</a> or <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\
    >use with fast.ai</a>\n</li>\n<li>Runs <a href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\"\
    \ rel=\"nofollow\">with docker</a> <code>docker run -p 3000:3000 universaldatatool/universaldatatool</code>\n\
    </li>\n<li>Runs <a href=\"https://singularity-hub.org/collections/4792\" rel=\"\
    nofollow\">with singularity</a> <code>singularity run universaldatatool/universaldatatool</code>\n\
    </li>\n</ul>\n<p align=\"center\"><kbd><a href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<h2>\n<a id=\"user-content-sponsors\"\
    \ class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sponsors</h2>\n<p><a href=\"\
    https://wao.ai\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\"\
    \ alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"></a>\n<a href=\"\
    https://momentum-tech.ca/\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\"\
    \ alt=\"momentum image\" style=\"max-width:100%;\"></a>\n<a href=\"https://www.enabledintelligence.net/\"\
    \ rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\"\
    \ alt=\"enabled intelligence image\" style=\"max-width:100%;\"></a></p>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <h3>\n<a id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Web\
    \ App</h3>\n<p>Just visit <a href=\"https://universaldatatool.com\" rel=\"nofollow\"\
    >universaldatatool.com</a>!</p>\n<p><em>Trying to run the web app locally? Run\
    \ <code>npm install</code> then <code>npm run start</code> after cloning this\
    \ repository to start the web server.</em></p>\n<h3>\n<a id=\"user-content-desktop-application\"\
    \ class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Desktop Application</h3>\n<p>Download\
    \ the latest release from the <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >releases page</a> and run the executable you downloaded.</p>\n<h2>\n<a id=\"\
    user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n\
    <ul>\n<li>(Optional) Say hi in the <a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\">Slack channel</a>!</li>\n<li>Read <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\
    >this guide to get started with development</a>.</li>\n</ul>\n<h2>\n<a id=\"user-content-contributors-\"\
    \ class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors <g-emoji class=\"\
    g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\
    >\u2728</g-emoji>\n</h2>\n<p>Thanks goes to these wonderful people (<a href=\"\
    https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\">emoji key</a>):</p>\n\
    \n\n\n<table>\n  <tr>\n    <td align=\"center\">\n<a href=\"https://twitter.com/seveibar\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Severin Ibarluzea</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://puskuruk.github.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Puskuruk</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/CedricJean\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>CedricJean</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://berupon.hatenablog.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>beru</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Ownmarc\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Marc</b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Wafaa-arbash\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Wafaa-arbash</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/pgrimaud\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Pierre Grimaud</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/sreevardhanreddi\"><img src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>sreevardhanreddi</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/mrdadah\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Mohammed Eldadah</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://x8795278.blogspot.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>x213212</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hysios\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>hysios </b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://congdv.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Cong Dao</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Renato Junior</b></sub></a><br><a\
    \ href=\"#translation-MrJunato\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://gitlab.com/rickstaa\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Rick</b></sub></a><br><a\
    \ href=\"#translation-rickstaa\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/anaplian\"><img src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>anaplian</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.behance.net/MiguelCarvalho13\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Miguel Carvalho</b></sub></a><br><a\
    \ href=\"#translation-miguelcarvalho13\" title=\"Translation\"><g-emoji class=\"\
    g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://kyleo.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Kyle OBrien</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hakkiyagiz\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Hakk\u0131 Ya\u011F\u0131z ERD\u0130\
    N\xC7</b></sub></a><br><a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/jvdavim\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Jo\xE3o Victor Davim</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n</table>\n\n\n\n<p>This project follows the <a\
    \ href=\"https://github.com/all-contributors/all-contributors\">all-contributors</a>\
    \ specification. Contributions of any kind welcome!</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1615492416.0
Samanwaya1301/tidal-heating-bilby-pipe:
  data_format: 2
  description: This is the repo from Khun Sang's gitlab
  filenames:
  - containers/Singularity.dev
  - containers/Singularity.0.0.3
  - containers/Singularity.0.0.4
  - containers/Singularity.0.0.2
  - containers/Singularity.0.0.1
  full_name: Samanwaya1301/tidal-heating-bilby-pipe
  latest_release: null
  readme: '<h1>

    <a id="user-content-mlcontainer" class="anchor" href="#mlcontainer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MLContainer</h1>

    <p>Singularity container and conda environments for ML based analysis @ HEPHY</p>

    <h2>

    <a id="user-content-machine-learning-hats" class="anchor" href="#machine-learning-hats"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>machine-learning-hats</h2>

    <p><a href="https://github.com/FNALLPC/machine-learning-hats">https://github.com/FNALLPC/machine-learning-hats</a></p>

    <p>On HEPGPU01 its easier to use the conda environment</p>

    <pre><code>conda env create --file environment-gpu.yml

    </code></pre>

    <p>On CLIP its better to use the container. It will

    be installed after the shutdown.</p>

    <pre><code>build_ml-hats.sh

    </code></pre>

    <p>Run the shell container</p>

    <pre><code>run_ml-hats.sh

    </code></pre>

    <p>Run a script</p>

    <pre><code>run_ml-hats.sh &lt;scripts&gt;

    </code></pre>

    <h2>

    <a id="user-content-deepjetcore" class="anchor" href="#deepjetcore" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DeepJetCore</h2>

    <p><a href="https://github.com/DL4Jets/DeepJetCore">https://github.com/DL4Jets/DeepJetCore</a></p>

    <h1>

    <a id="user-content-container-for-deepjetcore" class="anchor" href="#container-for-deepjetcore"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    for DeepJetCore</h1>

    <p>Build the container (only on  HEPGPU01)</p>

    <pre><code>build_deepjet3.sh

    </code></pre>

    <p>Run the container</p>

    <pre><code>run_deepjet3.sh

    </code></pre>

    <h2>

    <a id="user-content-todo" class="anchor" href="#todo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TODO</h2>

    <p>Try container on CLIP</p>

    <h2>

    <a id="user-content-open-points" class="anchor" href="#open-points" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Open Points</h2>

    <h3>

    <a id="user-content-conda-environments-on-clip" class="anchor" href="#conda-environments-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conda
    Environments on CLIP</h3>

    <p>CLIP provides already installations of Conda</p>

    <pre><code>ml add Anaconda3/19.10

    </code></pre>

    <p>Nevertheless I could not build reliable environments due to limited

    quota. Also trying to move the correspondig directories to /scratch-cbe/users

    was not successfull.</p>

    <p>At this point it seems better to use the conda environment inside a container.</p>

    <h3>

    <a id="user-content-building-container-on-clip" class="anchor" href="#building-container-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on CLIP</h3>

    <p>Building containers on CLIP has two problems</p>

    <ul>

    <li>root rights required to build container (possible solution fakeroot)</li>

    <li>docker container could be transformed in singularity container, but

    the filesystems (BeeGEFS) do not fullfill the singularity requirements</li>

    </ul>

    <p>The best way seems to use CI with Jenkis ans the local singularity hub. This

    has to be understood in the future.</p>

    <h3>

    <a id="user-content-building-container-on-hepgpu01" class="anchor" href="#building-container-on-hepgpu01"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on HEPGPU01</h3>

    <p>Root rights are required in case container are build from scratch. Fakeroot

    would be a possible workaround. Has to be tried.</p>

    <p>Another possibility is to use "sudo".</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605113430.0
ScarfZapdos/conan-bge-questgen:
  data_format: 2
  description: null
  filenames:
  - src/downward/misc/releases/latest/Singularity
  - src/downward/misc/releases/20.06/Singularity.20.06
  - src/downward/misc/releases/19.06/Singularity.19.06
  - src/downward/misc/releases/19.12/Singularity.19.12
  full_name: ScarfZapdos/conan-bge-questgen
  latest_release: null
  readme: '<h1>

    <a id="user-content-a-quest-generator-based-on-conan-code" class="anchor" href="#a-quest-generator-based-on-conan-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A
    Quest generator based on CONAN code</h1>

    <p>Reference : Let CONAN tell you a story: Procedural quest generation, or multi-agent
    planning for interactive emergent narrative systems</p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>cd src/downward<br>

    ./build.py<br>

    cd ../<br>

    python3 adventuresawaitbge.py</p>

    <p>It will generate a quest for each implemented characters of BGE.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1621449910.0
SemWES/docs-and-training:
  data_format: 2
  description: Documentation, code examples, and tutorials for the SemWES platform
  filenames:
  - code_examples/Singularity/abortable_demo_job/Singularity
  - code_examples/Singularity/hellocuda/Singularity
  - code_examples/Singularity/waiter/Singularity
  - code_examples/Singularity/abortable_waiter/Singularity
  full_name: SemWES/docs-and-training
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-service-and-workflow-development-for-the-semwes-platform-stack-in-cloudifacturing\"\
    \ class=\"anchor\" href=\"#service-and-workflow-development-for-the-semwes-platform-stack-in-cloudifacturing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Service and workflow development for the SemWES platform stack in\
    \ CloudiFacturing</h1>\n<p>Welcome to the SemWES/CloudiFacturing service-development\
    \ resources!</p>\n<p>This repository is meant to be a center for documentation\
    \ relevant for everyone\nwho develops services and workflows on the SemWES platform\
    \ stack in\nthe CloudiFacturing project. Here, you will find everything from high-level\n\
    descriptions of the concepts behind the platform to step-by-step\ntutorials for\
    \ advanced topics in service development.</p>\n<h2>\n<a id=\"user-content-contents-\"\
    \ class=\"anchor\" href=\"#contents-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contents </h2>\n<ul>\n<li><a\
    \ href=\"#documentation-version\">Documentation version</a></li>\n<li><a href=\"\
    #contributing\">Contributing</a></li>\n<li><a href=\"#platform-overview\">Platform\
    \ overview</a></li>\n<li>\n<a href=\"#service-implementation-concepts-examples-tutorials\"\
    >Service implementation: concepts, examples, tutorials</a>\n<ul>\n<li><a href=\"\
    #general-concepts\">General concepts</a></li>\n<li><a href=\"#semwes-synchronous-services\"\
    >SemWES Synchronous services</a></li>\n<li><a href=\"#semwes-aynchronous-services\"\
    >SemWES Aynchronous services</a></li>\n<li><a href=\"#semwes-applications\">SemWES\
    \ Applications</a></li>\n<li><a href=\"#service-deployment\">Service deployment</a></li>\n\
    <li><a href=\"#soap-services\">SOAP services</a></li>\n<li><a href=\"#file-access\"\
    >File access</a></li>\n<li><a href=\"#using-hpc-resources\">Using HPC resources</a></li>\n\
    <li><a href=\"#advanced-topics\">Advanced topics</a></li>\n</ul>\n</li>\n<li>\n\
    <a href=\"#workflow-creation-execution-and-monitoring\">Workflow creation, execution,\
    \ and monitoring</a>\n<ul>\n<li><a href=\"#basic-workflow-editing\">Basic workflow\
    \ editing</a></li>\n<li><a href=\"#using-the-hpc-service\">Using the HPC service</a></li>\n\
    <li><a href=\"#available-utility-services\">Available utility services</a></li>\n\
    <li><a href=\"#workflow-debugging\">Workflow debugging</a></li>\n<li><a href=\"\
    #advanced-topics\">Advanced topics</a></li>\n</ul>\n</li>\n<li><a href=\"#stream-data-integration\"\
    >Stream Data Integration</a></li>\n<li><a href=\"#emgora-integration\">emGORA\
    \ Integration</a></li>\n<li><a href=\"#reference-documentation-of-platform-services\"\
    >Reference documentation of platform services</a></li>\n</ul>\n<h2>\n<a id=\"\
    user-content-documentation-version\" class=\"anchor\" href=\"#documentation-version\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Documentation version</h2>\n<p>Current documentation version: <code>3.20.0</code></p>\n\
    <p>See the <a href=\"CHANGELOG.md\">Changelog</a> for versioning details.</p>\n\
    <h2>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contributing</h2>\n<p>Everyone is <em>welcome</em> to contribute to\
    \ this documentation center. Typos,\nclarifications, interesting code examples,\
    \ or also just questions \u2013 anything is\nvaluable. To contribute, choose one\
    \ of the following options:</p>\n<ul>\n<li>Fork the repository, add your changes\
    \ and create a pull request.</li>\n<li>Create an issue in the GitHub issue tracker.</li>\n\
    <li>Write an email with your ideas to <a href=\"mailto:robert.schittny@sintef.no\"\
    >Robert</a>\n(discouraged, rather use one of the options above).</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-platform-overview\" class=\"anchor\" href=\"#platform-overview\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Platform overview</h2>\n<p>New to the SemWES platform? Read all about\
    \ its concepts and background here.</p>\n<ul>\n<li>\n<p><a href=\"infrastructure_overview/getting_access.md\"\
    >Getting access</a>: Need access to\nthe SemWES platform in CloudiFacturing? Look\
    \ here.</p>\n</li>\n<li>\n<p><a href=\"infrastructure_overview/demos.md\">Demos</a>:\
    \ If you want to get your hands\ndirty immediately and try out some demo workflows\
    \ on the platform, look here.</p>\n</li>\n<li>\n<p><a href=\"./infrastructure_overview/authentication.md\"\
    >Users, projects, and authentication</a>:\nGives a short overview of how user\
    \ authentication and data-access restriction\nworks on the SemWES platform.</p>\n\
    </li>\n<li>\n<p><a href=\"infrastructure_overview/workflows_and_services.md\"\
    >Workflows and services in the SemWES cloud \u2013 an overview</a>:\nGives a compact\
    \ overview over the concepts behind SemWES and what services\nand workflows are\
    \ and how they are executed. Also explains the nomenclature\nused in this repository.</p>\n\
    </li>\n<li>\n<p><a href=\"infrastructure_overview/service_types.md\">The SemWES\
    \ service types</a>:\nDescription and requirements of synchronous services, asynchronous\
    \ services,\nand applications. Read this if you're wondering what kind of service\
    \ you\nneed to develop for a certain use case.</p>\n</li>\n<li>\n<p><a href=\"\
    infrastructure_overview/storage.md\">Accessing cloud storage: Generic Storage\
    \ Services (GSS)</a>:\nOn the SemWES platform, different cloud storages can be\
    \ accessed in a\nsimple, unified way. Learn about the basic concepts of the SemWES\n\
    Generic Storage Services and the available storage solutions in\nCloudiFacturing.</p>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-service-implementation-concepts-examples-tutorials\"\
    \ class=\"anchor\" href=\"#service-implementation-concepts-examples-tutorials\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Service implementation: concepts, examples, tutorials</h2>\n<p>All\
    \ workflows in the SemWES platform are basically a series of calls to\nindividual\
    \ web services. This section provides information on how to develop\nsuch web\
    \ services in a way that they can be registered in the SemWES\nplatform.</p>\n\
    <p>All documentation here deals with things done \"in code\" (as opposed to via\n\
    the graphical tools provided on the portal).</p>\n<h3>\n<a id=\"user-content-general-concepts\"\
    \ class=\"anchor\" href=\"#general-concepts\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>General concepts</h3>\n<ul>\n\
    <li>\n<a href=\"./service_implementation/available_parameters.md\">Available parameters</a>:\n\
    The workflow manager offers a set of \"global\" parameters which are available\n\
    to every service. Here, we take a closer look at these parameters and their\n\
    main use cases.</li>\n</ul>\n<h3>\n<a id=\"user-content-semwes-synchronous-services\"\
    \ class=\"anchor\" href=\"#semwes-synchronous-services\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>SemWES Synchronous\
    \ services</h3>\n<ul>\n<li>Check out the <a href=\"infrastructure_overview/service_types.md\"\
    >SemWES service types</a>\nfor a high-level description of synchronous services\
    \ and their required\ninterface.</li>\n<li>\n<a href=\"code_examples/Python/sync_calculator\"\
    >Code example (Python): synchronous service</a>:\nA very simple synchronous calculator\
    \ service implemented in Python</li>\n<li>\n<a href=\"code_examples/Java/skeleton_syncservice\"\
    >Code example (Java): synchronous-service skeleton</a>:\nBare-bone skeleton of\
    \ a synchronous service</li>\n<li>\n<a href=\"tutorials/services/python_sync_calculator.md\"\
    >Tutorial: Deploy and modify synchronous calculator webservice</a>:\nThis tutorial\
    \ teaches you how to deploy and modify one of the code examples,\nnamely the synchronous\
    \ Calculator webservice implemented in Python. Good as a\nstarting point in the\
    \ service development.</li>\n</ul>\n<h3>\n<a id=\"user-content-semwes-aynchronous-services\"\
    \ class=\"anchor\" href=\"#semwes-aynchronous-services\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>SemWES Aynchronous\
    \ services</h3>\n<ul>\n<li>Check out the <a href=\"infrastructure_overview/service_types.md\"\
    >SemWES service types</a>\nfor a high-level description of asynchronous services\
    \ and their required\ninterface.</li>\n<li>\n<a href=\"code_examples/Python/async_waiter\"\
    >Code example (Python): Waiter</a>:\nAn asynchronous service which does nothing\
    \ but waiting</li>\n<li>\n<a href=\"tutorials/services/python_async_waiter.md\"\
    >Tutorial: Create a simple asynchronous service</a>:\nThis tutorial guides you\
    \ through the deployment steps of a simple\nasynchronous service. Starting from\
    \ a simply Python script representing a\nlong-running computation, we will wrap\
    \ an asynchronous web service around it\nwhich can be deployed in the SemWES infrastructure\
    \ stack.</li>\n</ul>\n<h3>\n<a id=\"user-content-semwes-applications\" class=\"\
    anchor\" href=\"#semwes-applications\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>SemWES Applications</h3>\n<ul>\n\
    <li>Check out the <a href=\"infrastructure_overview/service_types.md\">SemWES\
    \ service types</a>\nfor a high-level description of applications and their required\
    \ interface.</li>\n<li>\n<a href=\"code_examples/Python/app_simple\">Code example\
    \ (Python): Dialog</a>:\nSimplest possible example application containing a one-button\
    \ dialog.</li>\n<li>\n<a href=\"code_examples/Python/app_debugger\">Code example\
    \ (Python): Debugger</a>:\nDebug application for workflows; pauses a workflow\
    \ and displays parameter\ncontents.</li>\n</ul>\n<h3>\n<a id=\"user-content-service-deployment\"\
    \ class=\"anchor\" href=\"#service-deployment\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Service deployment</h3>\n<ul>\n\
    <li>\n<p><a href=\"service_implementation/deployment_strategy.md\">Service-deployment\
    \ concept</a>:\nLearn about the SemWES deployment strategy here.</p>\n</li>\n\
    <li>\n<p><a href=\"service_implementation/deployment_automated.md\">Service-deployment\
    \ manual</a>:\nDescribes how services can be deployed in SemWES</p>\n</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-soap-services\" class=\"anchor\" href=\"#soap-services\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>SOAP services</h3>\n<ul>\n<li>\n<a href=\"service_implementation/basics_testing.md\"\
    >Testing SOAP services</a>:\nExplains how deployed SOAP services can be tested\
    \ without having to go\nthrough the workflow manager.</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-file-access\" class=\"anchor\" href=\"#file-access\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>File\
    \ access</h3>\n<ul>\n<li>If you haven't done so already, it is recommended to\
    \ read the general\nprinciples of <a href=\"infrastructure_overview/storage.md\"\
    >accessing cloud storage</a>\non the SemWES platform.</li>\n<li>\n<a href=\"service_implementation/basics_gss_libraries.md\"\
    >High-level file access using GSS libraries</a>:\nWhile it is perfectly possible\
    \ to directly interact with the GSS API, it is\nhighly recommended to use the\
    \ provided GSS client libraries for file access\nin SemWES. This article gives\
    \ an overview over existing libraries and\ntheir usage.</li>\n</ul>\n<h3>\n<a\
    \ id=\"user-content-using-hpc-resources\" class=\"anchor\" href=\"#using-hpc-resources\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Using HPC resources</h3>\n<ul>\n<li>\n<p><a href=\"service_implementation/basics_hpc.md\"\
    >HPC access through the SemWES platform</a>:\nThe SemWES platform abstracts access\
    \ of specific HPC resources with a\ngeneric API, making it possible to run computations\
    \ on different HPC\nresources without any change to the computation code. This\
    \ article explains\nthe concepts and technical background of this solution.</p>\n\
    </li>\n<li>\n<p><a href=\"service_implementation/basics_singularity.md\">Packaging\
    \ software in Singularity images</a>:\nAll software that should be run on the\
    \ HPC resources accessible through the\nSemWES platform must be wrapped into Singularity\
    \ images which are then\nexecuted as isolated containers on an HPC cluster. Learn\
    \ how to create,\nupload, and register such images in this article.</p>\n</li>\n\
    <li>\n<p><a href=\"service_implementation/advanced_hpc_notifications.md\">Communicating\
    \ with a running HPC job</a>:\nSometimes, communication with a running HPC job\
    \ is important, for example to\nbe able to control or abort a simulation if required.\
    \ This article explains\nhow to set up a Singularity image for this kind of communication.</p>\n\
    </li>\n<li>\n<p><a href=\"service_implementation/basics_hpc_logs.md\">Debugging\
    \ HPC applications</a>:\nDebugging HPC applications in SemWES can be difficult\
    \ due to many\nlayers of abstraction between the running application and the user.\
    \ This\narticle gives some hints on debugging and loggin.</p>\n</li>\n<li>\n<p><a\
    \ href=\"service_implementation/advanced_hpc_mpi.md\">Singularity and MPI applications</a>:\n\
    Learn how to prepare your Singularity image for parallel\nexecution using MPI.</p>\n\
    </li>\n<li>\n<p><a href=\"service_implementation/advanced_hpc_gpu.md\">GPU support\
    \ for Singularity images</a>:\nLearn how to prepare your Singularity image for\
    \ access of underlying GPU cores.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-advanced-topics\"\
    \ class=\"anchor\" href=\"#advanced-topics\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Advanced topics</h3>\n<ul>\n\
    <li>\n<p><a href=\"service_implementation/advanced_error_handling.md\">Error handling\
    \ in SOAP services</a>:\nExplains how to gracefully handle errors inside SOAP\
    \ services such that the\noutside world can process them.</p>\n</li>\n<li>\n<p><a\
    \ href=\"service_implementation/advanced_authentication.md\">Using authentication\
    \ services inside a service</a>:\nWhen a service is executed inside a workflow,\
    \ the portal takes care that\nproper user authentication. If necessary, however,\
    \ the authentication\nmanager can also be used inside services.</p>\n</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-workflow-creation-execution-and-monitoring\" class=\"\
    anchor\" href=\"#workflow-creation-execution-and-monitoring\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Workflow\
    \ creation, execution, and monitoring</h2>\n<p>Once all required web services\
    \ are developed, they need to be registered and\nhooked up to make a workflow.\
    \ This section deals with all things done\n\"graphically\" via the tools provided\
    \ on the portal.</p>\n<h3>\n<a id=\"user-content-basic-workflow-editing\" class=\"\
    anchor\" href=\"#basic-workflow-editing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Basic workflow editing</h3>\n\
    <ul>\n<li>\n<p>Don't forget to have a look at the <a href=\"./infrastructure_overview/demos.md\"\
    >demo\nworkflows</a>\nto learn from some pre-defined workflow examples.</p>\n\
    </li>\n<li>\n<p><a href=\"tutorials/workflows/basics_portal_overview.md\">Tutorial:\
    \ Overview over the portal GUI</a>:\nIn this tutorial you will get to know the\
    \ portal GUI, start a workflow and\ninspect its results.</p>\n</li>\n<li>\n<p><a\
    \ href=\"tutorials/workflows/basics_editing.md\">Tutorial: Basic workflow editing</a>:\n\
    In this tutorial you will load, modify, and save an existing workflow using\n\
    the graphical workflow editor.</p>\n</li>\n<li>\n<p><a href=\"tutorials/workflows/basics_service_registration.md\"\
    >Tutorial: Registration of new services</a>:\nAny newly created service needs\
    \ to be registered properly to be usable in the\nSemWES platform. Learn about\
    \ all details and caveats of service\nregistration here.</p>\n</li>\n<li>\n<p><a\
    \ href=\"workflow_creation/service_upgrades.md\">Upgrading services</a>:\nSometimes,\
    \ an already deployed and registered service needs to be upgraded,\npossibly with\
    \ changes to the input and output parameters. Read here what to\nkeep in mind\
    \ when performing such upgrades.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-using-the-hpc-service\"\
    \ class=\"anchor\" href=\"#using-the-hpc-service\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using the HPC\
    \ service</h3>\n<ul>\n<li>\n<p><a href=\"workflow_creation/HPC_service.md\">Overview\
    \ over the generic HPC service</a>:\nIntroduces the generic HPC service and showcases\
    \ how it can be used to\nexecute Singularity images on the available HPC resources.</p>\n\
    </li>\n<li>\n<p><a href=\"workflow_creation/HPC_gss_conversion.md\">Converting\
    \ from GSS URIs to file paths and back</a>:\nFiles and folders are handled using\
    \ GSS URIs within the SemWES\nplatform, but on an HPC cluster, absolute paths\
    \ are required. Learn how\nto convert between the two here.</p>\n</li>\n<li>\n\
    <p><a href=\"workflow_creation/HPC_prepost.md\">Pre- and post-processor services</a>:\n\
    Explains how one can interface with the generic HPC service by writing pre-\n\
    and post-processor services.</p>\n</li>\n<li>\n<p><a href=\"workflow_creation/HPC_background.md\"\
    >Launching HPC jobs in the background</a>:\nExplains how HPC jobs can be launched\
    \ in the background, such that other\nparts of the workflow can run in parallel.</p>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-available-utility-services\" class=\"\
    anchor\" href=\"#available-utility-services\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Available utility services</h3>\n\
    <ul>\n<li>\n<p><a href=\"workflow_creation/utilities_filechooser.md\">File selection\
    \ using the FileChooser service</a>:\nThe FileChooser service is one of two ways\
    \ to easily let the user upload and\nselect files for a workflow. Learn how to\
    \ integrate the file chooser into\nyour workflows.</p>\n</li>\n<li>\n<p><a href=\"\
    workflow_creation/utilities_auto_gui.md\">Automatic creation of graphical interfaces\
    \ for user input</a>:\nWant an HTML form for user input at the start of a workflow?\
    \ Then use this\ntool which automatically creates one for you with minimal input\
    \ on your side.</p>\n</li>\n<li>\n<p><a href=\"workflow_creation/utilities_dfki.md\"\
    >DFKI\u2019s utility services</a>:\nWant to show some HTML during a workflow?\
    \ Need a user decision somewhere inside\nthe workflow? DFKI's utility suite offers\
    \ ready-made services just for that.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-workflow-debugging\"\
    \ class=\"anchor\" href=\"#workflow-debugging\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Workflow debugging</h3>\n<ul>\n\
    <li>\n<a href=\"code_examples/Python/app_debugger/README.md\">Parameter debugger</a>:\n\
    This simple application offers the option to pause a workflow and display any\n\
    parameters that are currently in use. Great for debugging failing workflows\n\
    or services. Offered as a code example with complete source code.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-advanced-topics-1\" class=\"anchor\" href=\"#advanced-topics-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Advanced topics</h3>\n<ul>\n<li>\n<p><a href=\"workflow_creation/advanced_workflow_nesting.md\"\
    >Using workflows inside workflows</a>:\nOne of the great strengths of the SemWES\
    \ workflow concepts is that one can\nuse complete workflows as services inside\
    \ another workflow. Learn about this\nworkflow nesting here.</p>\n</li>\n<li>\n\
    <p><a href=\"workflow_creation/advanced_branching_looping.md\">Branching and looping</a>:\n\
    Read how you can implement basic branches and loops using only the workflow\n\
    editor.</p>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-stream-data-integration\"\
    \ class=\"anchor\" href=\"#stream-data-integration\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Stream data integration</h2>\n\
    <ul>\n<li>\n<a href=\"infrastructure_overview/stream_data_integration.md\">Stream\
    \ data handling</a>: Due to its flexible webservice-based workflow structure,\
    \ SemWES does natively support the integration of stream data into SemWES workflows.\n\
    This section features a python based prototype and explanations, and links to\
    \ demo services and workflows.</li>\n</ul>\n<h2>\n<a id=\"user-content-emgora-integration\"\
    \ class=\"anchor\" href=\"#emgora-integration\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>emGORA integration</h2>\n<ul>\n\
    <li>\n<a href=\"infrastructure_overview/emgora_integration.md\">emGORA integration\
    \ manual</a>:\nThis section will address how to integrate SemWES workflows with\
    \ the emgora marketplace.</li>\n</ul>\n<h2>\n<a id=\"user-content-reference-documentation-of-platform-services\"\
    \ class=\"anchor\" href=\"#reference-documentation-of-platform-services\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Reference\
    \ documentation of platform services</h2>\n<p>If you need information on a specific\
    \ method of one of the platform services,\nhave a look at our API references:</p>\n\
    <ul>\n<li><a href=\"service_APIs/api_wfm.md\">Workflow manager</a></li>\n<li><a\
    \ href=\"service_APIs/api_wfe.md\">Workflow editor</a></li>\n<li><a href=\"service_APIs/api_authentication.md\"\
    >Authentication manager</a></li>\n<li><a href=\"service_APIs/api_gss.md\">GSS</a></li>\n\
    <li><a href=\"service_APIs/api_refissh.md\">refissh</a></li>\n<li><a href=\"service_APIs/api_servicectl.md\"\
    >servicectl</a></li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605725180.0
SofianeB/singularity-recipes:
  data_format: 2
  description: Recipes for singularity images supported on Mistral.
  filenames:
  - jupyterhub/Singularity.jupyterhub
  - jupyter/Singularity.jupyter
  - ml/Singularity.ml
  full_name: SofianeB/singularity-recipes
  latest_release: null
  readme: "<p><a href=\"https://singularity-hub.org/collections/4819\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-how-to-use\"\
    \ class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How to use?</h2>\n<pre><code>module\
    \ load singularity \n\nsingularity pull shub://statiksof/singularity-recipes:TAG\n\
    </code></pre>\n<p>Then, you can <code>shell</code> or <code>run</code> the container.</p>\n\
    <p>see <a href=\"https://singularity-hub.org/collections/4819\" rel=\"nofollow\"\
    >here</a> for all available tags.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity-image
  - hpc
  - containers
  updated_at: 1619185671.0
SouthGreenPlatform/CulebrONT_pipeline:
  data_format: 2
  description: A snakemake pipeline to assembly, polishing, correction and quality
    check from Oxford nanopore reads.
  filenames:
  - Containers/Singularity.report.def
  - Containers/Singularity.blobtools-1.1.1.def
  - Containers/Singularity.kat-latest.def
  - Containers/Singularity.conda.def
  - Containers/Singularity.shasta-0.7.0.def
  - Containers/Singularity.pilon-1.24.def
  - Containers/Singularity.busco-4.1.4.def
  - Containers/Singularity.assemblytics-1.2.def
  full_name: SouthGreenPlatform/CulebrONT_pipeline
  latest_release: '1.4'
  readme: "<p><a href=\"./docs/source/SupplementaryFiles/culebront_logo.png\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"./docs/source/SupplementaryFiles/culebront_logo.png\"\
    \ alt=\"Culebront Logo\" style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://www.python.org/downloads\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e4779c52a0f8acf7c62517ff771deebcf8ab8913544dd508ccdd6cec2f2b400a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e372532422d626c7565\"\
    \ alt=\"PythonVersions\" data-canonical-src=\"https://img.shields.io/badge/python-3.7%2B-blue\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://snakemake.readthedocs.io\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/35030e6ddc253302ffcdf599ce8a8e387c27d88eb3de9cfe4e103b3ec6161f96/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d254532253839254135352e31302e302d627269676874677265656e2e7376673f7374796c653d666c6174\"\
    \ alt=\"SnakemakeVersions\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%E2%89%A55.10.0-brightgreen.svg?style=flat\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a324f41bf4495d7dc95ac4693962834b38ff77e1a6ed7f5c4dca9c3e3f92a6d3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d254532253839254135332e332e302d3745344337342e737667\"\
    \ alt=\"Singularity\" data-canonical-src=\"https://img.shields.io/badge/singularity-%E2%89%A53.3.0-7E4C74.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://docs.conda.io/projects/conda/en/latest/index.html\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/cacdb0b19bd30d76ae4faaee3355a6d65ecc448b587bac638adbd5eb04339c20/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612d342e382e352532302d677265656e\"\
    \ alt=\"Conda\" data-canonical-src=\"https://img.shields.io/badge/conda-4.8.5%20-green\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Using data from long reads obtained by\
    \ Oxford Nanopore Technologies sequencing makes genome assembly easier, in particular\
    \ to solve repeats and structural variants, in prokaryotic as well as in eukaryotic\
    \ genomes, resulting in increased contiguity and accuracy.</p>\n<p>Bunch of softwares\
    \ and tools are released or updated every week, and a lot of species see their\
    \ genome assembled using those.</p>\n<p>That\u2019s right.</p>\n<p>\"<em>But which\
    \ assembly tool could give the best results for my favorite organism?</em>\"</p>\n\
    <p><strong>CulebrONT can help you!</strong> CulebrONT is an open-source, scalable,\
    \ modulable and traceable snakemake pipeline, able to launch multiple assembly\
    \ tools in parallel and providing help for choosing the best possible assembly\
    \ between all possibilities.</p>\n<p><strong>Homepage: <a href=\"https://culebront-pipeline.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">https://culebront-pipeline.readthedocs.io/en/latest/</a></strong></p>\n\
    <p><a name=\"user-content-citation\"></a></p>\n<h2>\n<a id=\"user-content-citation\"\
    \ class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Citation</h2>\n<p>@Authors:</p>\n\
    <p>Julie Orjuela (IRD), Aurore Comte(IRD), S\xE9bastien Ravel(CIRAD), Florian\
    \ Charriat(INRAE), Tram Vi(IRD, AGI), Francois Sabot(IRD) and S\xE9bastien Cunnac(IRD).</p>\n\
    <p><a name=\"user-content-notes\"></a></p>\n<h2>\n<a id=\"user-content-useful-notes\"\
    \ class=\"anchor\" href=\"#useful-notes\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Useful notes</h2>\n<p>Before\
    \ launching CulebrONT, you could base-calling of arbitrarily multiplexed libraries\
    \ across several Minion runs with sequencing quality control and gather the output\
    \ files by genome for subsequent steps. For that use <a href=\"https://github.com/vibaotram/baseDmux\"\
    >https://github.com/vibaotram/baseDmux</a>.</p>\n<h4>\n<a id=\"user-content-thanks\"\
    \ class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Thanks</h4>\n<p>Thanks to Ndomassi\
    \ Tando (i-Trop IRD) by administration support.</p>\n<p>The authors acknowledge\
    \ the IRD i-Trop HPC (South Green Platform) at IRD Montpellier for providing HPC\
    \ resources that have contributed to this work. <a href=\"https://bioinfo.ird.fr/\"\
    \ rel=\"nofollow\">https://bioinfo.ird.fr/</a> - <a href=\"http://www.southgreen.fr\"\
    \ rel=\"nofollow\">http://www.southgreen.fr</a></p>\n<p>Thanks to Yann Delorme\
    \ for this beautiful logo <a href=\"https://nimarell.github.io/resume\" rel=\"\
    nofollow\">https://nimarell.github.io/resume</a></p>\n<p><a name=\"user-content-licence\"\
    ></a></p>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h2>\n<p>Licencied under CeCill-C (<a href=\"http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html\"\
    \ rel=\"nofollow\">http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html</a>)\
    \ and GPLv3\nIntellectual property belongs to IRD and authors.</p>\n"
  stargazers_count: 17
  subscribers_count: 16
  topics: []
  updated_at: 1621527295.0
SouthernMethodistUniversity/think-play-hack:
  data_format: 2
  description: 'Think-Play-Hack: World Views'
  filenames:
  - containers/python/Singularity
  - containers/r/Singularity
  full_name: SouthernMethodistUniversity/think-play-hack
  latest_release: null
  readme: '<h1>

    <a id="user-content-think-play-hack-world-views" class="anchor" href="#think-play-hack-world-views"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Think-Play-Hack:
    World Views</h1>

    <h2>

    <a id="user-content-preparatory-readings" class="anchor" href="#preparatory-readings"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Preparatory
    Readings</h2>

    <p>Dr. Guldi has compiled a list of readings to get your creative juices flowing.
    You can find them <a href="https://www.dropbox.com/sh/ru4dxh6rr6uqvfl/AADlPVWVEZ1BE4OcxPnZ0dpDa?dl=0"
    rel="nofollow">here</a>.</p>

    <h2>

    <a id="user-content-conference-activities" class="anchor" href="#conference-activities"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conference
    activities</h2>

    <ul>

    <li><a href="https://docs.google.com/document/d/1PdxiKuEQFj0KIQbHnvfthqG7gtBYNlN3QZavc5g1T9M/edit?usp=sharing"
    rel="nofollow">Wednesday hike</a></li>

    <li><a href="https://docs.google.com/document/d/1xDC_5mEJOZvfcWAvARDaLFl68Y44cDRh7jZJjpfUVKM/edit?usp=sharing"
    rel="nofollow">Thursday rafting trip</a></li>

    </ul>

    <p>These two activities are here because they happen to be more technically complex.
    There are other opportunities that are being informally discussed.</p>

    <h2>

    <a id="user-content-slack-instructions" class="anchor" href="#slack-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="https://get.slack.help/hc/en-us/articles/212675257" rel="nofollow">Slack
    Instructions</a>

    </h2>

    <p>You likely recieved an invitation to our Slack channel. This will be a good
    way to communicate with people at the conference and ask the Data Team questions
    if you need to. If you did not get an invite, just ask and we can send you one.</p>

    <h2>

    <a id="user-content-github-instructions" class="anchor" href="#github-instructions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="https://guides.github.com/activities/hello-world/">GitHub Instructions</a>

    </h2>

    <p>Though not required to use this repository, having an account on GitHub is
    a good idea for any programmer. It provides you with a portfolio of projects you
    have worked on as well as a way to collaborate with other coders.</p>

    <p>It will also allow you to clone this repository as well as add issues and suggest
    changes for the data team.</p>

    <h2>

    <a id="user-content-software" class="anchor" href="#software" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Software</h2>

    <p>We have ready-to-go software stacks for Python with Jupyter and R with RStudio.</p>

    <h3>

    <a id="user-content-docker-setup-for-personal-machines" class="anchor" href="#docker-setup-for-personal-machines"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="docs/docker.md">Docker Setup for Personal Machines</a>

    </h3>

    <p>Docker is a tool that allows software to run on your computer without actually
    needing to install the full software. These instructions will guide you through
    setting up Docker and getting our image running on your personal machine.</p>

    <p>We have provided two images: one that runs R and RStudio and one that runs
    Python, Jupyter Notebooks and JupyterLab.</p>

    <h3>

    <a id="user-content-using-maneframe-ii-m2" class="anchor" href="#using-maneframe-ii-m2"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a
    href="docs/m2.md">Using ManeFrame II (M2)</a>

    </h3>

    <p><a href="http://faculty.smu.edu/csc/documentation/about.html" rel="nofollow">ManeFrame
    II (M2)</a> is SMU''s high performance computing (HPC) cluster. M2 features 11,000
    cores, 60 NVIDIA V100 and P100 GPU accelerators, and 256 GB, 768 GB, and 1.5 TB
    memory configurations per node. Guest accounts on the cluster can be requested
    <a href="https://smu.az1.qualtrics.com/jfe/form/SV_2i6o7BztWg52rK5" rel="nofollow">here</a>.</p>

    <h2>

    <a id="user-content-data" class="anchor" href="#data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

    <h3>

    <a id="user-content-data-on-box" class="anchor" href="#data-on-box" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://smu.box.com/s/lk8mqfbgjproqda5jmlbynnbjclvybar"
    rel="nofollow">Data on Box</a>

    </h3>

    <p>We have provided access to all the data for the event on Box. Given the size,
    consider what you might want to work on prior to downloading it. Should you have
    trouble, we have flash drives and hard drives with the data stored locally as
    well.</p>

    <h3>

    <a id="user-content-reddit" class="anchor" href="#reddit" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="docs/reddit.md">Reddit</a>

    </h3>

    <p>We have over 1 TB of reddit data available in a database. You can <a href="docs/reddit.md">get
    subsets of this data</a> for analysis.</p>

    <h2>

    <a id="user-content-think-prompts" class="anchor" href="#think-prompts" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Think Prompts</h2>

    <p>In case you are having trouble getting started:</p>

    <ol>

    <li>

    <p>Can one imagine developing a method to trace narrative elements across genres?
    How would you formalize "narrative element"?</p>

    </li>

    <li>

    <p>How do we align narrative elements with aspects of cultural ideology (norms,
    beliefs, values)?</p>

    </li>

    <li>

    <p>What aspects of storytelling can we map? To what end? (i.e. can you imagine
    a new historic-geographic methodology?)</p>

    </li>

    <li>

    <p>What impact do popular films (e.g. Snow White and the 7 Dwarves) have on traditional
    tales (and vice versa)?</p>

    </li>

    <li>

    <p>Can we discover/trace the impact of traditional stories on literary works such
    as The Hobbit? On films?</p>

    </li>

    <li>

    <p>Where did you find that lovely frosted mug filled with such an alluringly amber-hewed
    frothy brew? Were there nuts as well?</p>

    </li>

    </ol>

    '
  stargazers_count: 2
  subscribers_count: 6
  topics: []
  updated_at: 1566358077.0
SupercomputingWales/singularity_hub:
  data_format: 2
  description: Repository to store Singularity recipts
  filenames:
  - system/hello/Singularity.hello-world
  - system/base/Singularity.base_image
  - physics/nextnano/Singularity.nextnano
  - physics/mpb/Singularity.mpb_bionic
  - physics/mpb/Singularity.mpb_xenial
  - libraries/opencv/Singularity.opencv_cuda
  - libraries/opencv/Singularity.opencv
  - biology/cell-profiler/Singularity.cell-profiler
  - biology/deeplabcut/Singularity.deeplabcut
  - biology/deeplabcut/Singularity.deeplabcut_cpu
  - biology/chimerax/Singularity.chimerax
  - biology/alphafold_casp13/Singularity.alphafold_casp13
  - languages/rstudio-server/Singularity.rstudio-server
  - medical/mrtrix/Singularity.mrtrix3
  - machine-learning/vico/Singularity.vico_cuda90
  - machine-learning/vico/Singularity.vico
  - materials/openfoam/Singularity.openfoam7_rheotool
  - materials/openfoam/Singularity.openfoam7
  - materials/fluidity/Singularity.fluidity_openmpi
  - materials/fluidity/Singularity.fluidity_mpich
  - chemistry/fpocket/Singularity.fpocket
  - chemistry/molden/Singularity.molden
  - chemistry/gpaw/Singularity.gpaw
  - environment/metview/Singularity.metview
  - environment/metview/Singularity.metview_suse
  - environment/underworld2/Singularity.underworld2
  - genomics/prsice/Singularity.prsice
  - genomics/macs2/Singularity.macs2
  - genomics/gubbins/Singularity.gubbins
  - genomics/covid-19-signal/Singularity.covid-19-signal
  - genomics/irap/Singularity.irap
  - genomics/indelible/Singularity.indelible
  - genomics/MEA/Singularity.MEA
  - creative/k3d/Singularity.k3d
  - creative/ipyparaview/Singularity.ipyparaview
  - creative/tesseract/Singularity.tesseract_opencv
  - creative/tesseract/Singularity.tesseract
  - creative/tesseract/Singularity.tesseract_opencv_cuda
  - creative/open3d/Singularity.open3d
  - creative/open3d/Singularity.open3d_user
  full_name: SupercomputingWales/singularity_hub
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_hub" class="anchor" href="#singularity_hub" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>singularity_hub</h1>

    <p>Repository to store Singularity recipts</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1620243135.0
TheJacksonLaboratory/mousegwas:
  data_format: 2
  description: An R package for easy execution of mouse GWAS
  filenames:
  - singularity/Singularity
  full_name: TheJacksonLaboratory/mousegwas
  latest_release: GRCm38
  readme: "<h1>\n<a id=\"user-content-mousegwas\" class=\"anchor\" href=\"#mousegwas\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>MouseGWAS</h1>\n<h2>\n<a id=\"user-content-introduction\" class=\"\
    anchor\" href=\"#introduction\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Introduction</h2>\n<p>This package\
    \ was built to manage the GWAS analysis of mouse phenotypes. The mice in the study\
    \ were genotypes using either MDA or UCLA chips and deposited in the mouse phenome\
    \ database (<a href=\"https://phenome.jax.org/genotypes\" rel=\"nofollow\">https://phenome.jax.org/genotypes</a>).</p>\n\
    <h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>Rscript -e <span class=\"pl-s\"><span class=\"pl-pds\">'</span>library(devtools);\
    \ install_github(\"TheJacksonLaboratory/mousegwas\")<span class=\"pl-pds\">'</span></span></pre></div>\n\
    <h2>\n<a id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Input</h2>\n\
    <p>The input for the script is the genotype csv files downloaded from the MPD\
    \ website, the measured phenotypes as a csv file and a yaml file describing the\
    \ input file.\nThe input csv file should contain a column for strain, a column\
    \ for sex and columns for phenotype measurements. The names of the columns should\
    \ be defined in the yaml file using the keywords <code>strain</code> and <code>sex</code>\
    \ and the phenotypes should be a list under the <code>phenotypes</code> keyword.\n\
    Another data that should reside in the yaml file is translation of strains to\
    \ the strain names in the genotypes files, it is a dictionary under the <code>translate</code>\
    \ keyword, and <code>F1</code> keyword which is a dictionary translating the F1\
    \ names to their parent names, make sure the female parent is always first, it\
    \ will be used to determine the X chromosome of make F1s. Confounding SNPs could\
    \ be given using the <code>confSNPs</code>, this might be useful to control for\
    \ obvious markers like coat color alleles. For sanity check you can supply coat\
    \ color under <code>coat</code> as a dictionary from strain name to coat color\
    \ and execute a GWAS of coat color with <code>--coat_phenotype</code>, it can\
    \ also be used as a covariate with <code>--coat_covar</code>.</p>\n<h2>\n<a id=\"\
    user-content-execution\" class=\"anchor\" href=\"#execution\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Execution</h2>\n\
    <p>The script <code>run_GWAS.py</code> will read the input files and will prepapre\
    \ the input for either GEMMA or pyLMM. In the case of GEMMA it will download version\
    \ 0.98 to the working directory if it can't find the GEMMA executable, if you\
    \ wish to use pyLMM it should be installed and available in the path. A common\
    \ process would be creating a virtual environment in python, activating it and\
    \ installing pyLMM using <code>pip</code>, see <a href=\"https://github.com/nickFurlotte/pylmm\"\
    >https://github.com/nickFurlotte/pylmm</a> for details.\nThe mousegwas will also\
    \ download METASOFT and run it on the output if there is more than one phenotype.</p>\n\
    <p>As part of the data processing, mousegwas can select a subset of the individuals,\
    \ restricting the number of mice in each strain x sex group or use the average\
    \ phenotype of all the individuals in such a group. This is controlled by the\
    \ <code>-d</code> option with 0 for average or any other integer for number restriction.</p>\n\
    <p>By default LOCO will be used, use the <code>--noloco</code> argument to disable\
    \ it.</p>\n<p>A quantile-quantile normalizatin of each phenotype meausrement could\
    \ be done using the <code>--qqnorm</code> argument.\nOther parameters will control\
    \ the final Manhattan plot, it is a bit unnecessary since the <code>postprocess_GWAS.R</code>\
    \ script will generate more and publication ready figures.</p>\n<h2>\n<a id=\"\
    user-content-nextflow-pipeline\" class=\"anchor\" href=\"#nextflow-pipeline\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Nextflow pipeline</h2>\n<p>To execute the scripts in an easy way we\
    \ included a nextflow pipeline that runs the initial GWAS, the shuffled executions,\n\
    determine a p-value and run the postprocess.\nTo run coat color phenotype GWAS\
    \ you can simply install <code>nextflow</code>, make sure that <code>singularity</code>\
    \ is installed and run:</p>\n<div class=\"highlight highlight-source-shell\"><pre>nextflow\
    \ run TheJacksonLaboratory/mousegwas \\\n  --yaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/coat_color_MDA.yaml\
    \ \\\n  --shufyaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/coat_color_MDA.yaml\
    \ \\\n  --addgwas=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--coat_phenotype<span\
    \ class=\"pl-pds\">\"</span></span> --addpostp=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>--coat_phenotype --colorgroup<span class=\"pl-pds\">\"</span></span>\
    \ --pvalue 0.1 --clusters 1 --outdir coatout -profile singularity,slurm</pre></div>\n\
    <p><code>slurm</code> can be changed to <code>pbs</code> or ignored for local\
    \ execution.</p>\n<p>To regenerate the results in the paper: <a href=\"https://www.biorxiv.org/content/10.1101/2020.10.08.331017v1\"\
    \ rel=\"nofollow\">https://www.biorxiv.org/content/10.1101/2020.10.08.331017v1</a>\
    \ :</p>\n<div class=\"highlight highlight-source-shell\"><pre>nextflow run TheJacksonLaboratory/mousegwas\
    \ \\\n  --yaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/grooming_nowild.yaml\
    \ \\\n  --shufyaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/grooming_shuffle.yaml\
    \ \\\n  --input https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/grooming_paper_strain_survey_2019_11_21.csv\
    \ <span class=\"pl-cce\">\\ </span>\\\n  --outdir grooming_output --addpostp=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>--loddrop 0<span class=\"pl-pds\"\
    >\"</span></span> -profile slurm,singularity</pre></div>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1620076690.0
TomHarrop/assemblers:
  data_format: 2
  description: null
  filenames:
  - Singularity.spades_3.14.1
  - Singularity.flye_2.8
  - Singularity.masurca_4.0.3
  - Singularity.trinity_2.11.0
  - Singularity.canu_1.9
  - Singularity.meraculous_2.2.6
  - Singularity.miniasm_0.3r179
  full_name: TomHarrop/assemblers
  latest_release: 0.0.1
  readme: "<h1>\n<a id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"\
    #test-starting-kit\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>test-starting-kit</h1>\n<p><g-emoji class=\"\
    g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\
    >\U0001F913</g-emoji></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1620120967.0
UCLBrain/MSLS:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: UCLBrain/MSLS
  latest_release: null
  readme: "<p><a href=\"https://github.com/UCLBrain/MSLS/issues\"><img src=\"https://camo.githubusercontent.com/f9cab98cc8f1052f0c0096b8b462cf9b2280a706b1adc0895d8a4859f3743314/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f55434c427261696e2f4d534c53\"\
    \ alt=\"GitHub issues\" data-canonical-src=\"https://img.shields.io/github/issues/UCLBrain/MSLS\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UCLBrain/MSLS/network\"\
    ><img src=\"https://camo.githubusercontent.com/b6c7a087c43d2a65a6ee22ec8ce2e004a29212c4b9619b117f4b2bbabf98a6c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f55434c427261696e2f4d534c53\"\
    \ alt=\"GitHub forks\" data-canonical-src=\"https://img.shields.io/github/forks/UCLBrain/MSLS\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UCLBrain/MSLS/stargazers\"\
    ><img src=\"https://camo.githubusercontent.com/10fab859502fd8c9b24dde937e50fecba7449e94ea057774713238ab3ec754fc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f55434c427261696e2f4d534c53\"\
    \ alt=\"GitHub stars\" data-canonical-src=\"https://img.shields.io/github/stars/UCLBrain/MSLS\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UCLBrain/MSLS/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/0d525d837b03e3b7a24b6322fc2197a134fa12e6a96188e5f18d6cd92236aa47/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f55434c427261696e2f4d534c53\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UCLBrain/MSLS\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-multi-label-multisingle-class-image-segmentation\"\
    \ class=\"anchor\" href=\"#multi-label-multisingle-class-image-segmentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Multi-Label Multi/Single-Class Image Segmentation</h1>\n<br>\n <p><a\
    \ href=\"images/diag.png\" target=\"_blank\" rel=\"noopener noreferrer\"><img\
    \ height=\"510\" src=\"images/diag.png\" style=\"max-width:100%;\"></a></p>\n\n\
    <h1>\n<a id=\"user-content-publication\" class=\"anchor\" href=\"#publication\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Publication</h1>\n<p>Le Zhang, Ryutaro Tanno, Kevin Bronik, Chen Jin,\
    \ Parashkev Nachev, Frederik Barkhof, Olga Ciccarelli, and Daniel C. Alexander,\
    \ Learning to Segment When Experts Disagree, International Conference on Medical\
    \ image computing and Computer-Assisted Intervention (MICCAI). Springer, Cham,\
    \ 2020.</p>\n<br>\n <p><a href=\"images/Miccai_2020_abs.jpg\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img align=\"center\" height=\"1000\" src=\"images/Miccai_2020_abs.jpg\"\
    \ style=\"max-width:100%;\"></a></p>\n\n\n    \n        <p>Click here to see full\
    \ pdf file: <a href=\"https://github.com/UCLBrain/MSLS/blob/master/MICCAI_2020.pdf\"\
    >Link to PDF</a></p>\n    \n\n<h1>\n<a id=\"user-content-running-the-gui-program\"\
    \ class=\"anchor\" href=\"#running-the-gui-program\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running the GUI\
    \ Program!</h1>\n<p>First, user needs to install Anaconda <a href=\"https://www.anaconda.com/\"\
    \ rel=\"nofollow\">https://www.anaconda.com/</a></p>\n<p>Then</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>  - conda env create -f conda_environment_Training_Inference.yml\
    \  </pre></div>\n<p>and</p>\n<div class=\"highlight highlight-source-shell\"><pre>\
    \  - conda activate traintestenv  </pre></div>\n<p>finally</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>  - python  Training_Inference_GUI.py\
    \ </pre></div>\n<p>After lunching the graphical user interface, user will need\
    \ to provide necessary information to start training/testing as follows:</p>\n\
    <br>\n <p><a href=\"images/GUI.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img height=\"510\" src=\"images/GUI.jpg\" style=\"max-width:100%;\"></a></p>\n\
    \n<h1>\n<a id=\"user-content-running-the-program-from-the-command-line\" class=\"\
    anchor\" href=\"#running-the-program-from-the-command-line\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ the Program from the command line!</h1>\n<p>First</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>  - conda activate traintestenv  </pre></div>\n\
    <p>then for training</p>\n<div class=\"highlight highlight-source-shell\"><pre>\
    \  - python  segmentation_network_Training_without_GUI.py  [or annotation_network_Training_without_GUI.py]</pre></div>\n\
    <p>for testing</p>\n<div class=\"highlight highlight-source-shell\"><pre>  - python\
    \  segmentation_network_Inference_without_GUI.py  [or annotation_network_Inference_without_GUI.py]</pre></div>\n\
    <h1>\n<a id=\"user-content-testing-the-program\" class=\"anchor\" href=\"#testing-the-program\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Testing the Program!</h1>\n<p>Examples of Training and Testing subjects\
    \ can be found in: <a href=\"https://github.com/UCLBrain/MSLS/tree/master/examples\"\
    >https://github.com/UCLBrain/MSLS/tree/master/examples</a> (which will allow users\
    \ to quickly and easily train and test the program)</p>\n<br>\n <p><a href=\"\
    images/bin_seg_ex.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"><img height=\"\
    510\" src=\"images/bin_seg_ex.jpg\" style=\"max-width:100%;\"></a></p>\n\n<p>..................................................................................................................................................................</p>\n\
    <br>\n <p><a href=\"images/multi_seg_ex.jpg\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img height=\"510\" src=\"images/multi_seg_ex.jpg\" style=\"max-width:100%;\"\
    ></a></p>\n\n"
  stargazers_count: 5
  subscribers_count: 1
  topics: []
  updated_at: 1621237070.0
UPPMAX/install-methods:
  data_format: 2
  description: Install methods for UPPMAX modules plus some helper scripts
  filenames:
  - singularity_info/metaWRAP_1.3.2/Singularity.metaWRAP
  - singularity_info/gapseq-RT-227932/Singularity.gapseq
  full_name: UPPMAX/install-methods
  latest_release: null
  readme: '<h1>

    <a id="user-content-module-installation-methods" class="anchor" href="#module-installation-methods"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Module
    Installation Methods</h1>

    <p>This is a collection of READMEs generated during installation of software

    applications on Uppmax clusters.  It is incomplete in terms of modules

    available on Uppmax, and the individual READMEs may also be incomplete in terms

    of what was actually done to install the modules.  We are publicising these in

    the hopes that they can be helpful.</p>

    <h2>

    <a id="user-content-example-workflow-of-a-basic-installation" class="anchor" href="#example-workflow-of-a-basic-installation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example
    workflow of a basic installation</h2>

    <ol>

    <li>Clone the install methods git repo (<code>git clone https://github.com/UPPMAX/install-methods.git</code>)</li>

    <li>Add the repo to your <code>$PATH</code> and source the <code>uppmax_functions.sh</code>
    file to get access to the functions.</li>

    <li>Run <code>run_makeroom</code> with at least <code>-t</code> and <code>-v</code>,
    to generate a <code>.sh</code> (<code>makeroom_toolname_version.sh</code>) file
    that will create the directory structure needed in <code>/sw</code>

    </li>

    <li>Run the <code>.sh</code> file created in the directory you are standing to
    create the directory structure (<code>/sw/category/toolname/</code> and <code>/sw/mf/common/category</code>)
    and template files.</li>

    <li>Put the source code for the program in <code>/sw/category/toolname/version/src</code>

    </li>

    <li>Compile and/or install the tool in <code>/sw/category/toolname/version/cluster/bin</code>
    etc.</li>

    <li>Edit the readme file, explaining how you did the installation, in <code>/sw/category/toolname/toolname-version_install-README.md</code>

    </li>

    <li>Edit the template module file <code>/sw/category/toolname/mf/version</code>
    to do what you want when the module loads.</li>

    <li>Copy the module file to the live location, <code>/sw/mf/common/category/[section]/toolname</code>

    </li>

    <li>Run <code>all_mflink toolname version</code> to create links for all clusters
    to the module file in <code>/sw/mf/common/category/[section]/toolname</code>

    </li>

    <li>Run <code>fixup /sw/category/toolname/version /sw/mf/common/category/[section]/toolname</code>
    to make sure the ownership and permissions are ok.</li>

    </ol>

    <h2>

    <a id="user-content-scripts" class="anchor" href="#scripts" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Scripts</h2>

    <p><code>gather-READMEs.sh</code> - bash script to scan installation directories,
    looking for

    README files having a particular filename format that we create during

    installation of tools</p>

    <p><code>fixup</code> - bash script fixing up permissions and group membership
    within

    installation trees; our local installation group is <code>sw</code>. With the
    <code>-g</code> option,

    this script will <code>chmod g+s</code> directories in the tree, too.</p>

    <p><code>uppmax_functions.sh</code> - bash helper functions for SLURM job viewing
    and various

    module-related tasks, mostly to do with setting up mf files for loading

    modules; the latter require appexpert privileges.  Source these from <code>.bashrc</code>.</p>

    <h2>

    <a id="user-content-installation-directories" class="anchor" href="#installation-directories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation
    directories</h2>

    <p>The directories contain software installations in major subject areas.</p>

    <h3>

    <a id="user-content-apps" class="anchor" href="#apps" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>apps/</h3>

    <p>General applications.</p>

    <h3>

    <a id="user-content-appsbioinfo" class="anchor" href="#appsbioinfo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>apps/bioinfo/</h3>

    <p>Bioinformatics applications.</p>

    <h3>

    <a id="user-content-libs" class="anchor" href="#libs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>libs/</h3>

    <p>Libraries.</p>

    <h3>

    <a id="user-content-comp" class="anchor" href="#comp" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>comp/</h3>

    <p>Compilers, interpreters, build tools.</p>

    <h2>

    <a id="user-content-database-directories" class="anchor" href="#database-directories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Database
    directories</h2>

    <p>These directories cover installations of databases updated either manually,
    or via update scripts.</p>

    <h3>

    <a id="user-content-data_uppnex" class="anchor" href="#data_uppnex" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>data_uppnex/</h3>

    <p>Installation instructions for databases under <code>/sw/data/uppnex/</code>.  Database

    directories containing <code>*-install-README.md</code> files are updated manually.

    Database directories containing <code>*-db-README.md</code> files and scripts
    (currently,

    <code>Kraken</code>, <code>diamond_databases</code> and <code>RTG</code>) are
    updated monthly via crontab entries.</p>

    <p>Blast database updates are included here, and involve multiple scripts, crontab

    entries and a test directory.  These are updated monthly via crontab entries.</p>

    <h3>

    <a id="user-content-data_other" class="anchor" href="#data_other" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>data_other/</h3>

    <p>Installation instructions for databases under other locations, currently just

    <code>BUSCO</code> lineage sets, which are kept in the module installation directory.

    These are updated monthly via crontab entries.</p>

    '
  stargazers_count: 3
  subscribers_count: 5
  topics: []
  updated_at: 1621891662.0
UPPMAX/offline-uppmax-env:
  data_format: 2
  description: A container that looks like uppmax but can be run completely offline.
  filenames:
  - Singularity.default
  - Singularity.ngsintro
  full_name: UPPMAX/offline-uppmax-env
  latest_release: null
  readme: '<h1>

    <a id="user-content-offline-uppmax-env" class="anchor" href="#offline-uppmax-env"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>offline-uppmax-env</h1>

    <p>A container that has the same operating system, same packages installed, and
    a copy of the module system (not the actual software though) at UPPMAX. The script
    <code>software_packer.sh</code> can be run at UPPMAX to create a tarball of the
    software you wish to include in container at build time. If any data needs to
    be accessed from inside the container it can be mounted at runtime.</p>

    <h1>

    <a id="user-content-tldr" class="anchor" href="#tldr" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TLDR</h1>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>#
    ON UPPMAX</span>

    <span class="pl-c"><span class="pl-c">#</span> package the software you want to
    have in your image</span>

    git clone https://github.com/UPPMAX/offline-uppmax-env.git

    <span class="pl-c1">cd</span> offline-uppmax-env

    bash software_packer.sh bwa star samtools


    <span class="pl-c"><span class="pl-c">#</span># ON LOCAL COMPUTER</span>

    git clone https://github.com/UPPMAX/offline-uppmax-env.git

    <span class="pl-c1">cd</span> offline-uppmax-env


    <span class="pl-c"><span class="pl-c">#</span> download the created software.package.tar.gz
    to the package/ folder</span>


    <span class="pl-c"><span class="pl-c">#</span> using Docker</span>

    docker build <span class="pl-c1">.</span>

    docker run \

    -v offline-uppmax-env-proj:/proj \

    -v /any/host/data/you/want/access/to:/path/inside/container \

    -it \

    uppmax/offline-uppmax-env:latest


    <span class="pl-c"><span class="pl-c">#</span> using singularity</span>

    singularity build offline-uppmax-env.sif Singularity

    singularity shell \

    -b /host/path/to/persistent/projfolder:/proj \

    -b /any/host/data/you/want/access/to:/path/inside/container \

    offline-uppmax-env.sif</pre></div>

    <p><strong>What you get</strong></p>

    <ul>

    <li>CentOS 7</li>

    <li>All yum packages installed at UPPMAX</li>

    <li>A copy of the module files at UPPMAX (not the programs themselves)</li>

    <li>The option to include any of the installed programs at UPPMAX, requires you
    to rebuild the image.</li>

    </ul>

    <p><strong>What you don''t get</strong></p>

    <ul>

    <li>Shared libraries, these would bloat the image quite a bit. These are solvable
    on a case by case basis, more on that further down.</li>

    </ul>

    <h2>

    <a id="user-content-use-case" class="anchor" href="#use-case" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Use case</h2>

    <p>This repo was created to make a offline replacement for UPPMAX for courses,
    in case there is some kind of problem making UPPMAX unusable at the time the course
    is given. If UPPMAX suddenly disappears we can just tell the students to start
    up a container and all data and software needed would be included, making it possible
    to continue the course. This will require us to build our own version of this
    image where we include the software we want to be installed and to provide any
    data we want to be accessible to the students.</p>

    <h2>

    <a id="user-content-how-to-create-a-course-specific-image" class="anchor" href="#how-to-create-a-course-specific-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How
    to create a course specific image</h2>

    <h3>

    <a id="user-content-building-off-the-base-image-in-dockerhub" class="anchor" href="#building-off-the-base-image-in-dockerhub"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    off the base image in Dockerhub</h3>

    <p>The base image will just have the OS and packages of UPPMAX, and the <code>uppmax</code>
    and <code>bioinfo-tools</code> module. To include the software you want to have
    access to you will have to login to UPPMAX and run <code>software_packer.sh</code>.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    run on uppmax</span>

    bash software_packer.sh bwa star R GATK</pre></div>

    <p>This will package everything needed to load these modules into a file called
    <code>software.package.tar.gz</code>.  Download this file to your computer, put
    it in the <code>packages</code> folder and build the Dockerfile in that folder
    (replace <code>repo/name:version</code> with whatever you want to name it on Dockerhub,
    or remove it to have it untagged). The dockerfile will copy all files in <code>packages/</code>
    and unzip all files named <code>*.package.tar.gz</code>, so feel free to put additional
    files there following this naming pattern.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    run locally</span>

    docker build -t repo/name:version <span class="pl-c1">.</span></pre></div>

    <h3>

    <a id="user-content-building-your-own-base-image" class="anchor" href="#building-your-own-base-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    your own base image</h3>

    <p>If the base image on Dockerhub is too old for your liking you can rebuild it
    yourself. Follow the same steps as above, but put the <code>software.package.tar.gz</code>
    you created on UPPMAX in the <code>base/packages</code> folder instead. The dockerfile
    will copy all files in <code>packages/</code> and unzip all files named <code>*.package.tar.gz</code>,
    so feel free to put additional files there following this naming pattern.</p>

    <p>To update the list of packages installed by <code>yum</code>, run the following
    line on UPPMAX:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    list all installed packages and print the all on a single line</span>

    yum list installed <span class="pl-k">|</span> cut -f 1 -d <span class="pl-s"><span
    class="pl-pds">"</span> <span class="pl-pds">"</span></span> <span class="pl-k">|</span>
    cut -f 1 -d <span class="pl-s"><span class="pl-pds">"</span>.<span class="pl-pds">"</span></span>
    <span class="pl-k">|</span> sort <span class="pl-k">|</span> awk -vORS=<span class="pl-s"><span
    class="pl-pds">''</span> <span class="pl-pds">''</span></span> <span class="pl-s"><span
    class="pl-pds">''</span>{ print $1 }<span class="pl-pds">''</span></span></pre></div>

    <p>and replace the list of packages in the <code>Dockerfile</code>.</p>

    <p>Then build the Dockerfile in the <code>base</code> folder.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    base

    docker build -t repo/name:version <span class="pl-c1">.</span></pre></div>

    <p>This build will download and install all the yum packages from scratch so the
    image will be completely up-to-date, but it will take about an hour to build it.</p>

    <h2>

    <a id="user-content-how-to-run-the-image-once-it-is-built" class="anchor" href="#how-to-run-the-image-once-it-is-built"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How
    to run the image once it is built</h2>

    <p>This will create a named volume called <code>offline-uppmax-env-proj</code>
    which will be mounted to <code>/proj</code> inside the container. All data put
    in there will persist between restarts of the container, i.e. this is where the
    students should put their lab work. The data used in the labs are usually so big
    (10+gb) that it does not make sens to put it inside the image. It''s better to
    download it separately and mount it when starting the container.</p>

    <p><strong>Docker</strong></p>

    <div class="highlight highlight-source-shell"><pre>docker run \

    -v offline-uppmax-env-proj:/proj \

    -v /host/path/to/data:/container/path/to/data \

    -it \

    repo/name:version


    <span class="pl-c"><span class="pl-c">#</span> example</span>

    docker run \

    -v offline-uppmax-env-proj:/proj \

    -v /home/user/ngsintro_data:/sw/courses/ngsintro \

    -it \

    uppmax/offline-uppmax-env:latest

    </pre></div>

    <p>After the container is running it should be just like working on uppmax. <code>module
    load</code> should behave the same way and all modules you packed with <code>software_packer.sh</code>
    should be available.</p>

    <p><strong>Singularity</strong>

    To get the module system to work in Singularity you have to build the Singularity
    file as sudo and everything should work. Package the software you need on UPPMAX
    like in the Docker approach, put the downloaded tarball in the <code>packages/</code>
    folder just like with Docker, and then build it with Singularity.</p>

    <p>Just building from Dockerhub (uppmax/offline-uppmax-env:latest) will give you
    a container with only the <code>uppmax</code> and <code>bioinfo-tools</code> in
    it, and the <code>module</code> command will not work since it is a function that
    is not inherited properly when being converted by Singularity. You can get around
    this by manually typing <code>source /etc/bashrc.module_env</code> every time
    the container starts.</p>

    <p>If you build your own Docker image with the software your want, push it to
    Dockerhub, and convert it to Singularity, you will still have the problem of the
    <code>module</code> command not working. The solution is the same, manually type
    <code>source /etc/bashrc.module_env</code> when the container starts and it should
    start working. Building the Singularity file instead will not have this problem.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    </span>

    </pre></div>

    <h1>

    <a id="user-content-troubleshooting" class="anchor" href="#troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Troubleshooting</h1>

    <h3>

    <a id="user-content-missing-shared-libraries" class="anchor" href="#missing-shared-libraries"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Missing
    shared libraries</h3>

    <p>Unfortunately I could not find an easy way to automatically pull all the shared
    libraries needed by programs. I had a problem with STAR, that it needed a newer
    version of GCC. I could get around it by running <code>ldd $(which star)</code>
    on uppmax and see that the file uses was <code>/sw/comp/gcc/8.3.0_rackham/lib64/libstdc++.so.6</code>.
    I put this file in a tar file,</p>

    <div class="highlight highlight-source-shell"><pre>tar -chzvf libs.package.tar.gz
    /sw/comp/gcc/8.3.0_rackham/lib64/libstdc++.so.6  <span class="pl-c"><span class="pl-c">#</span>
    note the -h option, will dereference symbolic links</span></pre></div>

    <p>and put the <code>libs.package.tar.gz</code> file in the <code>packages</code>
    folder, build the image, and it worked after that.</p>

    <h1>

    <a id="user-content-todos" class="anchor" href="#todos" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Todos</h1>

    <ul>

    <li>Test if it works.</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1604891556.0
UPPMAX/uppmax_in_a_can:
  data_format: 2
  description: Using singularity to create a near-identical uppmax environment (experimental)
  filenames:
  - Singularity
  - tags/200311/Singularity.200311
  full_name: UPPMAX/uppmax_in_a_can
  latest_release: null
  readme: '<h1>

    <a id="user-content-uppmax-in-a-can" class="anchor" href="#uppmax-in-a-can" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>UPPMAX in a can</h1>

    <p>This Singularity container will let you run a near-identical UPPMAX environment
    on your own computer. You will have access to all of the installed software at
    UPPMAX, all your files and reference data on UPPMAX, but it will be your own computer
    that does the calculations. You can even use it to analyse data that you only
    have on your own computer, but using the software and reference data on UPPMAX.</p>

    <h1>

    <a id="user-content-typical-use-cases" class="anchor" href="#typical-use-cases"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Typical
    use cases</h1>

    <ul>

    <li>You have sensitive data that is not allowed to leave your computers, but you
    want to use the programs and references at UPPMAX to do the analysis.</li>

    <li>You have your own server but want to avoid installing all the software yourself.</li>

    <li>The queues at UPPMAX are too long or you have run out of core hours but you
    want the analysis done yesterday.</li>

    <li>You have your data and compute hours at another SNIC centre, but want to use
    the software installed at UPPMAX.</li>

    </ul>

    <h1>

    <a id="user-content-what-you-get" class="anchor" href="#what-you-get" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>What you get</h1>

    <ul>

    <li>Access to your home folder and all project folders.</li>

    <li>Access to all the installed programs at UPPMAX.</li>

    <li>Access to all reference data at UPPMAX.</li>

    </ul>

    <h1>

    <a id="user-content-what-you-dont-get" class="anchor" href="#what-you-dont-get"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What
    you don''t get</h1>

    <ul>

    <li>UPPMAX high-performace computers. You will be limited by the computer you
    are running the container on.</li>

    <li>No slurm access. Everything runs on your computer.</li>

    </ul>

    <h1>

    <a id="user-content-important-notes" class="anchor" href="#important-notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Important notes</h1>

    <ul>

    <li>Since all data you read/write to the UPPMAX file system will have to travel
    over the internet, disk intensive programs will be much slower, and transfer rate
    is limited to your internet connection. Try working on your local harddrive as
    much as possible.</li>

    </ul>

    <h1>

    <a id="user-content-issues" class="anchor" href="#issues" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Issues</h1>

    <ul>

    <li>Some of the tools at uppmax, like projinfo and uquota are using your user''s
    group membership to determin what information to show. Since your local user won''t
    have the same groups as you have on uppmax they misbehave and will not show you
    the same information as when you run the program on uppmax.</li>

    <li>If your user has not connected to uppmax before it will need to accept the
    login nodes fingerprint before connecting (a security feature). Sshfs will just
    hang if this happens, so you will have to connect normally to uppmax and manually
    accept the fingerprint. To make things more complicated there are 3 different
    login nodes, each with its own fingerprint, and you are randomly assigned to one.
    There is a possibility to turn this off in the sshfs mount command by adding <code>-o
    StrictHostKeyChecking=no</code>, but it might not be a good idea to disable it
    due to security. Easiest way around it is just to run <code>ssh user@rackham.uppmax.uu.se</code>
    a couple of times until you have seen all 3 fingerprints.</li>

    </ul>

    <h1>

    <a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h1>

    <p>This tool has been developed on Ubuntu 18.04 with Singularity v.3.5. You should
    only need 2 things for this to work,</p>

    <ul>

    <li><a href="https://sylabs.io/guides/3.5/user-guide/quick_start.html" rel="nofollow">Singularity
    (only tested with v3.5)</a></li>

    <li>

    <a href="https://github.com/libfuse/sshfs">SSHFS</a><br>

    (Make sure that the option <code>user_allow_other</code> is uncommented in <code>/etc/fuse.conf</code>)</li>

    </ul>

    <p>For installation instructions for these, see respective projects homepage.</p>

    <h1>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h1>

    <h3>

    <a id="user-content-using-a-pre-built-image" class="anchor" href="#using-a-pre-built-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    a pre-built image</h3>

    <div class="highlight highlight-source-shell"><pre>singularity pull shub://UPPMAX/uppmax_in_a_can:latest</pre></div>

    <h3>

    <a id="user-content-building-your-own-image-from-github" class="anchor" href="#building-your-own-image-from-github"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    your own image from github.</h3>

    <p>Clone the github repo and build the image:</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/UPPMAX/uppmax_in_a_can.git

    <span class="pl-c1">cd</span> uppmax_in_a_can

    singularity build uppmax_in_a_can_latest.sif Singularity</pre></div>

    <p>The build takes 10-20 minutes on a modern laptop with gigabit Ethernet.</p>

    <h1>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

    <h2>

    <a id="user-content-first-time-usage" class="anchor" href="#first-time-usage"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>First
    time usage</h2>

    <p>Once the build is done, run the initialization script in the container,</p>

    <div class="highlight highlight-source-shell"><pre>./uppmax_in_a_can_latest.sif
    uppmax_init


    <span class="pl-c"><span class="pl-c">#</span> to see more options, run</span>

    ./uppmax_in_a_can_latest.sif</pre></div>

    <h2>

    <a id="user-content-subsequent-usage" class="anchor" href="#subsequent-usage"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Subsequent
    usage</h2>

    <p>Start the virtual node:</p>

    <div class="highlight highlight-source-shell"><pre>./uiac_node.sh -i uppmax_in_a_can_latest.sif


    <span class="pl-c"><span class="pl-c">#</span> to see more options, run</span>

    ./uiac_node.sh -h</pre></div>

    <p>You will now be on the command-line inside the container and you can run commands
    as if you were logged in on UPPMAX. You will see all project folders in <code>/proj</code>,
    all software in <code>/sw</code>, your UPPMAX home folder in <code>/home/&lt;UPPMAX_username&gt;</code></p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    ex</span>

    module load bioinf-tools samtools

    samtools view file.bam</pre></div>

    <p>To close down and return to your own computers command-line, just type <code>exit</code>.</p>

    <h1>

    <a id="user-content-advanced-usage" class="anchor" href="#advanced-usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Advanced usage</h1>

    <p>You can also specify any additional singularity options to the <code>uiac_node.sh</code>
    script. If you want to make your computers file system visible inside the container
    so that you can analyse files residing on your computer, just add a bind argument:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Entire hard drive </span>

    ./uiac_node.sh -e <span class="pl-s"><span class="pl-pds">"</span>--bind /:/hostfs<span
    class="pl-pds">"</span></span> -i uppmax_in_a_can_latest.sif


    <span class="pl-c"><span class="pl-c">#</span> Only a specific directory</span>

    ./uiac_node.sh -e <span class="pl-s"><span class="pl-pds">"</span>--bind /home/user/data:/hostfs<span
    class="pl-pds">"</span></span> -i uppmax_in_a_can_latest.sif</pre></div>

    <p>This command will make your computers file system available under <code>/hostfs</code>
    (or wherever you would like it).</p>

    <h1>

    <a id="user-content-developer-notes" class="anchor" href="#developer-notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Developer notes</h1>

    <p>To get the list of installed software on UPPMAX, run this command on UPPMAX:

    <code>yum list installed | cut -f 1 -d " " | cut -f 1 -d "." | sort &gt; yum_installed.txt</code></p>

    <p>This list will contain <strong>all</strong> installed packages, even core packages
    and nvidia packages that are not available in the default repos. It will just
    give a couple of warning messages when you send the list to <code>yum install</code>,
    but it will not break the installation process.</p>

    <p>To get the the list of environment variables in <code>env/99-uppmaxvars.sh</code>,
    type <code>env | egrep -i ''module|lmod'' | sort</code> when logged in on UPPMAX
    and pick out everything that has to do with the module system.</p>

    <p>The PS1 is made to be different from the default shell the user has to make
    it obvious they are inside the container.</p>

    '
  stargazers_count: 2
  subscribers_count: 7
  topics: []
  updated_at: 1620056516.0
VEuPathDB/singularity-craig:
  data_format: 2
  description: Singularity recipe for CraiG
  filenames:
  - Singularity
  full_name: VEuPathDB/singularity-craig
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-recipe-for-craig\" class=\"anchor\"\
    \ href=\"#singularity-recipe-for-craig\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity Recipe for CraiG</h1>\n\
    <h2>\n<a id=\"user-content-craig-source\" class=\"anchor\" href=\"#craig-source\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CraiG Source</h2>\n<p><a href=\"https://github.com/axl-bernal/CraiG\"\
    >https://github.com/axl-bernal/CraiG</a></p>\n<h2>\n<a id=\"user-content-build-image\"\
    \ class=\"anchor\" href=\"#build-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build image</h2>\n<p>The <a href=\"\
    https://github.com/EuPathDB/vagrant-rpmbuild\"><code>vagrant-rpmbuild</code></a>\n\
    virtual machine provisions Singularity suitable for building.</p>\n<p>Typically\
    \ you should remove any existing image so you get clean build.</p>\n<pre><code>$\
    \ rm -f craig.simg\n$ sudo singularity build craig.simg Singularity \n</code></pre>\n\
    <h2></h2>\n<p>$ singularity pull --name craig.simg shub://mheiges/singularity-craig</p>\n\
    <h2>\n<a id=\"user-content-exec\" class=\"anchor\" href=\"#exec\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Exec</h2>\n\
    <pre><code>$ singularity exec --bind /eupath craig.simg ls  /eupath\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1528493265.0
XSEDE/singularity-nix-base:
  data_format: 2
  description: Singularity base container with Nix to be used in XSEDE compute environment
    (currently in development)
  filenames:
  - Singularity
  full_name: XSEDE/singularity-nix-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-nix-base" class="anchor" href="#singularity-nix-base"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-nix-base</h1>

    <p><a href="https://singularity-hub.org/collections/4462" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity base container with Nix to be used in XSEDE compute environment
    (currently in development)</p>

    '
  stargazers_count: 0
  subscribers_count: 13
  topics:
  - nix
  - singularity
  - singularity-nix
  updated_at: 1621060868.0
achennings/neurodocker:
  data_format: 2
  description: Custom implementation of neurodocker (https://github.com/ReproNim/neurodocker)
  filenames:
  - Singularity
  full_name: achennings/neurodocker
  latest_release: null
  readme: '<h1>

    <a id="user-content-neurodocker" class="anchor" href="#neurodocker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>neurodocker</h1>

    <p>Custom implementation of neurodocker (<a href="https://github.com/ReproNim/neurodocker">https://github.com/ReproNim/neurodocker</a>)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621991816.0
adaptive-intelligent-robotics/Deep-Grid_MAP-Elites:
  data_format: 2
  description: Repository hosting the code associated with "Fast and stable MAP-Elites
    in noisy domains using deep grids"
  filenames:
  - Singularity
  full_name: adaptive-intelligent-robotics/Deep-Grid_MAP-Elites
  latest_release: null
  readme: '<h1>

    <a id="user-content-fast-and-stable-map-elites-in-noisy-domains-using-deep-grids"
    class="anchor" href="#fast-and-stable-map-elites-in-noisy-domains-using-deep-grids"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Fast
    and stable MAP-Elites in noisy domains using deep grids</h1>

    <p><a href="https://singularity-hub.org/collections/4459" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>This repository contains the code associated with <a href="https://direct.mit.edu/isal/proceedings/isal2020/32/273/98397"
    rel="nofollow">Fast and stable MAP-Elites in noisy domains using deep grids</a>.</p>

    <p>This code proposes an implementation of several MAP-Elites variants to handle
    uncertainty: the Explicit-averaging approach,  the two Adaptive-sampling approaches
    proposed in <a href="https://dl.acm.org/doi/abs/10.1145/3319619.3321904" rel="nofollow">MAP-Elites
    for noisy domains by  adaptive sampling</a> and the DG-MAP-Elites approach. It
    allows to compare these approaches on the three tasks described in the paper.</p>

    <h1>

    <a id="user-content-libraries-and-dependencies" class="anchor" href="#libraries-and-dependencies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Libraries
    and dependencies</h1>

    <p>The implementation of all tasks and algorithms is based on the qd-branch of
    the C++ <a href="https://github.com/sferes2/sferes2">Sferes2</a>  library presented
    in <a href="https://ieeexplore.ieee.org/abstract/document/5586158/?casa_token=EhBJLkircvMAAAAA:ls8I90Y5H2vsJk5RxCYs8X1T9yZHDhDEz5S6g5gatOzETle1LK_ib8zwodx6t5J_-Uwq_YP9"
    rel="nofollow">Sferesv2: Evolvin'' in the multi-core world</a>, and the hexapod  control
    task uses the Dart simulator introduced in <a href="https://joss.theoj.org/papers/10.21105/joss.00500.pdf"
    rel="nofollow">Dart: Dynamic animation and robotics toolkit</a>.

    Furthermore, the analysis of the results is based on <a href="https://pandas.pydata.org/"
    rel="nofollow">Panda</a>, <a href="https://matplotlib.org/" rel="nofollow">Matplotlib</a>
    and <a href="https://seaborn.pydata.org/index.html" rel="nofollow">Seaborn</a>
    libraries.</p>

    <h1>

    <a id="user-content-structure" class="anchor" href="#structure" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Structure</h1>

    <p>The main part of the code contains the following files and folders:</p>

    <ul>

    <li>

    <code>deep_grid</code> contains the main structure of all algorithms based on
    the structure of QD algorithms introduced in <a href="https://ieeexplore.ieee.org/abstract/document/7959075/"
    rel="nofollow">Quality and diversity optimization: A  unifying modular  framework</a>.
    The approaches differ by the nature of their selector, the add-function of their
    container and, in the specific case of Adaptive sampling, the descriptor they
    are using for the individuals, which is implemented in the structure of the cells
    of the grid.</li>

    <li>

    <code>task</code> contains a re-definition of the qd fitness from the Sferes2
    library which allow to handle re-sampling of the solutions. It also contains an
    implementation of each of the three tasks used in the paper, build on top of the
    original deterministic tasks used in previous works.</li>

    <li>

    <code>modifier</code> defines the procedure to evaluate the "true" value of fitness
    and behaviour descriptor of each cell as described in the paper.</li>

    <li>

    <code>stat</code> implements all the stats used to compare the algorithms.</li>

    <li>

    <code>run_utils</code> contains utils function used to read the options of each
    run and run all algorithms.</li>

    </ul>

    <p>In addition, the <code>analysis</code> folder is used for the results analysis,
    the <code>waf_tools</code> and <code>wscript</code> files for compilation, and
    the <code>ressource</code> folder and <code>Singularity</code> file to compile
    the Singularity container for this experiment.</p>

    <h1>

    <a id="user-content-execution" class="anchor" href="#execution" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Execution</h1>

    <p>The results of the paper can be reproduced by running the Singularity container
    image of the experiment. Instructions to install Singularity can be found in <a
    href="https://sylabs.io/guides/3.5/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">Singularity documentation</a>.

    To pull the image from Singularity Hub, use the command: <code>singularity pull
    shub://adaptive-intelligent-robotics/Deep-Grid_MAP-Elites</code>.</p>

    <p>This container contains an <code>app</code> for each approach-task combination,
    defined in the <code>Singularity</code> file:</p>

    <ul>

    <li>

    <code>Truth_arm_var</code>, <code>Truth_rastrigin</code>, <code>Truth_hexa</code>:
    Noise-free Baseline for each of the three tasks.</li>

    <li>

    <code>Naive_1_arm_var</code>, <code>Naive_1_rastrigin</code>, <code>Naive_1_hexa</code>:
    Explicit-averaging approach with 1 re-sampling.</li>

    <li>

    <code>Naive_50_arm_var</code>, <code>Naive_50_rastrigin</code>, <code>Naive_50_hexa</code>:
    Explicit-averaging approach with 50 re-sampling.</li>

    <li>

    <code>Adapt_arm_var</code>, <code>Adapt_rastrigin</code>, <code>Adapt_hexa</code>:
    Adaptive sampling approach without drifting elites.</li>

    <li>

    <code>Adapt_BD_10_arm_var</code>, <code>Adapt_BD_10_rastrigin</code>, <code>Adapt_BD_10_hexa</code>:
    Adaptive sampling approach with drifting elites.</li>

    <li>

    <code>Deep_50_arm_var</code>, <code>Deep_50_rastrigin</code>, <code>Deep_50_hexa</code>:
    DG-MAP-Elites approach with depth 50.</li>

    <li>

    <code>Analysis</code>: apps to generate the graphs and container plots of all
    variants and tasks which results are stored in the folder given as input.</li>

    </ul>

    <p>A given app can be run with the following command: <code>singularity run --app
    *app_name* Deep-Grid_MAP-Elites_latest.sif</code>. All run-parameters are defined
    inside the apps, and the results of the execution are solved in a <code>results</code>
    folder, outside of the image, at the same location.</p>

    <p>This container can be recompile using the <code>Singularity</code> file.</p>

    '
  stargazers_count: 3
  subscribers_count: 1
  topics: []
  updated_at: 1618442518.0
adigenova/nf-gene-fusions:
  data_format: 2
  description: A nextflow pipeline to call somatic gene fusions
  filenames:
  - Singularity/Singularity.v1.0
  full_name: adigenova/nf-gene-fusions
  latest_release: v1.0
  readme: '<h1>

    <a id="user-content-nf-rna-fusions" class="anchor" href="#nf-rna-fusions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-rna-fusions</h1>

    <p>A nextflow pipeline to call somatic rna fusions</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1603289117.0
ahalfpen727/Docker-Resources:
  data_format: 2
  description: null
  filenames:
  - bioconductor_full/Singularity
  - bioconductor_full/Singularity.RELEASE_3_8
  - bioconductor_full/Singularity.RELEASE_3_10
  - bioconductor_full/Singularity.RELEASE_3_9
  - bioconductor_docker/Singularity
  full_name: ahalfpen727/Docker-Resources
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-bioconductor-devel-docker-image\" class=\"anchor\"\
    \ href=\"#bioconductor-devel-docker-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Bioconductor Devel Docker image</h1>\n\
    <p>Bioconductor Docker image with full set of system dependencies so that\nall\
    \ Bioconductor packages can be installed.</p>\n<p>The Docker images have R and\
    \ Bioconductor with different versions\nunder each \"branch\" in git.</p>\n<p><strong>NOTE</strong>:\
    \ Docker image for bioconductor_full:devel is in the <code>master</code>\nbranch,\
    \ and all the release branches will be under the branch\n<code>RELEASE_X_Y</code>.</p>\n\
    <p>Important Links:</p>\n<p><a href=\"https://hub.docker.com/r/bioconductor/bioconductor_full\"\
    \ rel=\"nofollow\">Docker hub link for bioconductor_full</a></p>\n<p><a href=\"\
    https://github.com/Bioconductor/bioconductor_full\">Github development link for\
    \ bioconductor_full</a></p>\n<h2>\n<a id=\"user-content-advantages-of-the-bioconductor_full-docker-image\"\
    \ class=\"anchor\" href=\"#advantages-of-the-bioconductor_full-docker-image\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Advantages of the <code>bioconductor_full</code> docker image</h2>\n\
    <ol>\n<li>\n<p>The bioconductor_full docker images can be used instead of\ninstalling\
    \ complex dependencies needed for Bioconductor\npackages. The image comes with\
    \ most of the dependencies installed.</p>\n</li>\n<li>\n<p>Quick start up to start\
    \ your analysis with all the Bioconductor\npackages if needed.</p>\n</li>\n<li>\n\
    <p>The image will be regularly updated to reflect the build system on\nBioconductor.\
    \ This is a very useful resource for maintainers who\nare actively developing\
    \ their package to see if it works in tandem\nwith the bioconductor ecosystem.\
    \ It provides a local testing outlet\nfor maintainers and developers.</p>\n</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-installation-and-quick-start\" class=\"anchor\"\
    \ href=\"#installation-and-quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation and quick start</h2>\n\
    <p>This document assumes you have <a href=\"https://www.docker.com/\" rel=\"nofollow\"\
    >docker</a> installed. Please check\n<a href=\"https://www.docker.com/products/docker-desktop\"\
    \ rel=\"nofollow\">installation</a> if you have more questions regarding this.</p>\n\
    <h3>\n<a id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quick start</h3>\n<ul>\n<li>\n<p>Start docker on your machine.</p>\n\
    </li>\n<li>\n<p>On the command line, \"pull\" the bioconductor_full docker image\
    \ with\nthe correct tag. These images are hosted on docker hub under the\n<a href=\"\
    https://cloud.docker.com/u/bioconductor/repository/registry-1.docker.io/bioconductor/bioconductor_full\"\
    \ rel=\"nofollow\">Bioconductor organization</a> page.</p>\n<pre><code>  docker\
    \ pull bioconductor/bioconductor_full:devel\n</code></pre>\n<p>or</p>\n<pre><code>\
    \  docker pull bioconductor/bioconductor_full:RELEASE_X_Y\n</code></pre>\n</li>\n\
    <li>\n<p>Once the image is available on your local machine, you can check to\n\
    see if they are available.</p>\n<pre><code>  docker images\n</code></pre>\n</li>\n\
    <li>\n<p>To start using these images with RStudio, this will start the image\n\
    under the 'rstudio' user</p>\n<pre><code>  docker run                        \
    \              \\\n      -e PASSWORD=your_password                   \\\n    \
    \  -p 8787:8787                                \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library\
    \ \\\n      bioconductor/bioconductor_full:devel\n</code></pre>\n</li>\n<li>\n\
    <p>To start the image interactively using the <code>bioc</code> user</p>\n<pre><code>\
    \  docker run                                     \\\n      -it              \
    \                          \\\n      --user bioc                             \
    \   \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library \\\n \
    \     bioconductor/bioconductor_full:devel       \\\n      R\n</code></pre>\n\
    <p>NOTE: The path <code>/usr/local/lib/R/host-site-library</code> is mapped to\n\
    <code>.libPaths()</code> in R. So, when R is started, all the libraries in\nthe\
    \ host directory, <code>host-site-library</code> are available to R. It is\nstored\
    \ on your machine mounted from the volume you fill in place\nof <code>host-site-library</code>.</p>\n\
    <p>These libraries will only work if they are pre-compiled with the\nsame version\
    \ of R that is in the docker image. To explain further,\nyou would need the packages\
    \ built with Bioconductor version '3.9'\nto work with R-3.6. Similarly, you'd\
    \ need Bioconductor version\n'3.9' to work with R-3.6.z.</p>\n</li>\n<li>\n<p>To\
    \ start the docker image in deamon mode, i.e, have the container\nrunning in the\
    \ background use the <code>-d</code> option.</p>\n<pre><code>  sudo docker run\
    \ -it                            \\\n      -d                                \
    \         \\\n      -v host-site-library:/usr/local/lib/R/host-site-library \\\
    \n      --entrypoint /bin/bash                     \\\n      bioconductor/bioconductor_full:devel\n\
    </code></pre>\n<p>This will start the container in the background and keep it\n\
    running. You may check the running processes using <code>docker ps</code>,\nand\
    \ copy the container id.</p>\n<pre><code>  docker ps\n</code></pre>\n<p>To attach\
    \ to a container which is running in the background</p>\n<pre><code>  docker exec\
    \ -it &lt;container_id&gt; bash\n</code></pre>\n<p>NOTE: You can replace <code>bash</code>\
    \ with R to start R directly in the\ncontainer.</p>\n<pre><code>  docker exec\
    \ -it &lt;container_id&gt; R\n</code></pre>\n</li>\n<li>\n<p>To run multiple RStudio\
    \ instances, use a different external port\nmapping (the first port in <code>-p\
    \ XXXX:YYYY</code>) for each instance.\nUse standard shell commands (e.g., adding\
    \ a <code>&amp;</code> at the end of the\nfirst docker command) to place docker\
    \ processes in the\nbackground. The 'devel' instance will be available at\n<a\
    \ href=\"http://localhost:8787\" rel=\"nofollow\">http://localhost:8787</a>, and\
    \ the release image at\n<a href=\"http://localhost:8788\" rel=\"nofollow\">http://localhost:8788</a></p>\n\
    <pre><code>  docker run                                      \\\n      -e PASSWORD=your_password\
    \                   \\\n      -p 8787:8787                                \\\n\
    \      bioconductor/bioconductor_full:devel\n\n  docker run                  \
    \                    \\\n      -e PASSWORD=your_password                   \\\n\
    \      -p 8788:8787                                \\\n      bioconductor/bioconductor_full:RELEASE_3_10\n\
    </code></pre>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-using-latex-inside-the-container\"\
    \ class=\"anchor\" href=\"#using-latex-inside-the-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>using LaTeX\
    \ inside the container?</h3>\n<p>Install the <code>tinytex</code> package (<a\
    \ href=\"https://yihui.name/tinytex/\" rel=\"nofollow\">https://yihui.name/tinytex/</a>)\
    \ which has helpers for installing LaTeX functionality.</p>\n<div class=\"highlight\
    \ highlight-source-r\"><pre>install.packages(<span class=\"pl-s\"><span class=\"\
    pl-pds\">'</span>tinytex<span class=\"pl-pds\">'</span></span>)\n<span class=\"\
    pl-e\">tinytex</span><span class=\"pl-k\">::</span>install_tinytex()</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621559866.0
akhanf/mdt-singularity:
  data_format: 2
  description: Singularity recipe for Microstructure Diffusion Toolbox (http://mdt-toolbox.readthedocs.io/en/latest/index.html)
  filenames:
  - Singularity
  full_name: akhanf/mdt-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-mdt-singularity" class="anchor" href="#mdt-singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>mdt-singularity</h1>

    <p>Singularity recipe for Microstructure Diffusion Toolbox (MDT), a fast and flexible
    python toolbox for microstructural modelling (including NODDI etc..). <em>Note:</em>  I
    am not a developer/contributor of MDT, for more info on MDT see here:</p>

    <ul>

    <li>Code: <a href="https://github.com/cbclab/MDT">https://github.com/cbclab/MDT</a>

    </li>

    <li>Docs: <a href="http://mdt-toolbox.readthedocs.io/en/latest/index.html" rel="nofollow">http://mdt-toolbox.readthedocs.io/en/latest/index.html</a>

    </li>

    </ul>

    <p>I made this Singularity container for utilizing the CPUs for OpenCL on Linux
    systems that either have an incompatible GPU (Quadro), or no GPU (HPC systems).
    Installs Intel OpenCL drivers on Ubuntu 14.04 (newer versions not supported),
    and then installs MDT (pip3) and all dependencies (since PPA not supported on
    older Ubuntu version that Intel OpenCL supports). Big props to the MDT developers
    for creating this great toolbox -- this should hopefully make it easier for anyone
    to run it on an Intel CPU-based compute cluster.</p>

    <p><em>Note:</em> This requires Singularity to be installed (<a href="http://singularity.lbl.gov/"
    rel="nofollow">http://singularity.lbl.gov/</a>), requires sudo privilege to build
    the container (local machine), but can be used without sudo privilege by copying
    the container to another system.</p>

    <p>Build:</p>

    <pre><code>git clone http://github.com/akhanf/mdt-singularity

    cd mdt-singularity

    sudo singularity build mdt.simg Singularity

    </code></pre>

    <p>Test:</p>

    <pre><code>singularity exec mdt.simg mdt-list-devices

    </code></pre>

    <p>Preliminary testing: CPU - Intel(R) Xeon(R) CPU E5-2687W v3 @ 3.10GHz (Intel(R)
    OpenCL), with 20 cores takes ~2.5 hours to run NODDI (Cascade) on a HCP dataset
    (very slow compared to what you could do with a proper GPU, but amazingly fast
    compared to NODDI toolbox!! (would take several days..)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1522544846.0
aksa2832/Simvascular-on-cluster:
  data_format: 2
  description: Files and resources to build Simvascular's svpre, svpost, svsolver
    on a supercomputing cluster.
  filenames:
  - Singularity
  full_name: aksa2832/Simvascular-on-cluster
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-simvascular-on-cluster\" class=\"anchor\" href=\"\
    #simvascular-on-cluster\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Simvascular-on-cluster</h1>\n<p>Files and resources\
    \ to :</p>\n<ol>\n<li>Build Simvascular's svpre, svpost, svsolver on a supercomputing\
    \ cluster.</li>\n<li>Submit a simulation job using svsolver.</li>\n<li>Check the\
    \ progress of your simulation</li>\n<li>Download the generated files after the\
    \ simulation is completed</li>\n</ol>\n<blockquote>\n<p>Note : the following instructions\
    \ have been written for use on University of Colorado Boulder's research computing\
    \ account.\nPlease consult your supercomputing facility if you are affiliated\
    \ with a different cluster, as some steps might be unique for your cluster account\
    \ system.</p>\n</blockquote>\n<h2>\n<a id=\"user-content-1-to-build-a-container-for-use-on-the-cluster\"\
    \ class=\"anchor\" href=\"#1-to-build-a-container-for-use-on-the-cluster\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.\
    \ To build a container for use on the cluster</h2>\n<p>You can directly pull this\
    \ <a href=\"https://singularity-hub.org/collections/4568\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a> ready built container onto your supercomputing\
    \ cluster account's project directory (the directory intended to store software\
    \ builds and smaller data sets) by:</p>\n<ul>\n<li>Login to your cluster account</li>\n\
    <li>go to projects directory</li>\n<li>ssh into compile node</li>\n<li>load singularity\
    \ module</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre>  $\
    \ <span class=\"pl-c1\">cd</span> projects/<span class=\"pl-smi\">$USER</span>/\n\
    \  $ ssh scompile\n  $ module load singularity/\n  $ mkdir svsolver-container-build\n\
    \  $ <span class=\"pl-c1\">cd</span> svsolver-container-build\n  $ singularity\
    \ pull simvascular-svsolver.sif shub://aksa2832/Simvascular-on-cluster</pre></div>\n\
    <p><strong>OR</strong></p>\n<ul>\n<li>Create a public github repository and copy\
    \ the 3 build here files to your github repo.\nBuild files for CU Boulder's supercomputing\
    \ cluster: quick-build-linux.sh, quick-build-centos7-open-mpi.sh, Singularity\
    \ (text file containing recipe for the build)</li>\n<li>Then, login to <a href=\"\
    https://singularity-hub.org\" rel=\"nofollow\">https://singularity-hub.org</a>\
    \ with your github credentials.</li>\n<li>Go to <strong>My collections</strong>\
    \ -&gt; <strong>Add collection</strong> -&gt; choose your new github repository\
    \ with the build files and enter the recipe file name as <strong>Singularity</strong>.\n\
    This will automatically build the container for you and place it on singularity\
    \ hub. Here is how it should be after build: <a href=\"https://singularity-hub.org/collections/4568\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>Download the container to your supercomputing\
    \ cluster account's project directory (the directory intended to store software\
    \ builds and smaller data sets) by:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ <span class=\"pl-c1\">cd</span> projects/<span class=\"pl-smi\">$USER</span>/\n\
    $ ssh scompile\n$ module load singularity/\n$ mkdir svsolver-container-build\n\
    $ <span class=\"pl-c1\">cd</span> svsolver-container-build\n$ singularity pull\
    \ simvascular-svsolver.sif shub://<span class=\"pl-k\">&lt;</span>yourgithubusername<span\
    \ class=\"pl-k\">&gt;</span>/<span class=\"pl-k\">&lt;</span>svsolver_github_reponame<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<hr>\n<h2>\n<a id=\"user-content-2-to-submit-a-job-prepare-data-folder-and-job-script\"\
    \ class=\"anchor\" href=\"#2-to-submit-a-job-prepare-data-folder-and-job-script\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2. To submit a job: Prepare data-folder and job-script</h2>\n<p>Step\
    \ 1: Create the data files for the job with <strong>Simvascular application</strong>\
    \ on your local computer.</p>\n<ul>\n<li>Open SimVascular</li>\n<li>Right click\
    \ the job node (eg. \"test-simvascular-project\") under <strong>Simulations</strong>\
    \ in Data Manager</li>\n<li>Click \"Export Data Files\"</li>\n<li>Select a folder\
    \ for exporting(eg. \"test-simvascular-project-data\").</li>\n</ul>\n<p>Step 2:\
    \ Write and load a solver script.</p>\n<ul>\n<li>Write the script: Follow instructions\
    \ given in the template file \"sample_jobscript\" and modify the parameters according\
    \ to your requirements.</li>\n<li>Save the script as your-job-script.sh to the\
    \ folder containing the exported data files from SimVascular after step 1(\"test-simvascular-project-data\"\
    ) .\nThis folder should then contain all the Data Files and the job script required\
    \ for the simulation.</li>\n</ul>\n<p>Step 3: Upload this data-folder on your\
    \ supercomputing cluster account's working directory(usually /scratch/$USER).</p>\n\
    <ul>\n<li>Open terminal(or equivalent command prompt).</li>\n<li>Secure copy the\
    \ data-folder(\"test-simvascular-project-data\") onto the cluster's working directory.\
    \ Here @login.rc.colorado.edu is the login id you use for cluster account.</li>\n\
    </ul>\n<div class=\"highlight highlight-source-shell\"><pre>$ scp -rv <span class=\"\
    pl-k\">&lt;</span>/path/to/test-simvascular-project-data/on/your/computer<span\
    \ class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>username<span class=\"\
    pl-k\">&gt;</span>@login.rc.colorado.edu:<span class=\"pl-k\">&lt;</span>/target/path/to/working/directory/on/cluster/<span\
    \ class=\"pl-k\">&gt;</span></pre></div>\n<h2>\n<a id=\"user-content-2-submit-the-job\"\
    \ class=\"anchor\" href=\"#2-submit-the-job\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2. Submit the job:</h2>\n<p>Step\
    \ 4: Submit the simulation job.</p>\n<ul>\n<li>Login to your cluster account</li>\n\
    <li>navigate to the working directory(eg. /scratch/$USER/test-simvascular-project-data)\
    \ from where the simulation will be launched.</li>\n<li>Enter the following commands\
    \ to submit the job:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ module load slurm/summit\n$ sbatch your-job-script.sh</pre></div>\n<hr>\n\
    <h2>\n<a id=\"user-content-3-check-the-progress-of-your-simulation\" class=\"\
    anchor\" href=\"#3-check-the-progress-of-your-simulation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3. Check\
    \ the progress of your simulation</h2>\n<ul>\n<li>If you have enabled email notification\
    \ in you job script, you will be notified as soon as the simulation starts and\
    \ the moment it ends via. email.</li>\n<li>Find information on your job\u2019\
    s start time using the squeue command:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ squeue --user=your_rc-username --start</pre></div>\n<ul>\n<li>To check\
    \ the status/progress of the submitted job</li>\n</ul>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ sacct --starttime=YYYY-MM-DD --format=User,JobName,JobId,partition,state,time,start,end,elapsed,MaxRss,MaxVMSize,nnodes,ncpus,nodelist\n\
    </pre></div>\n<ul>\n<li>Finally check the output file(.out file) generated in\
    \ &lt;/scratch/$USER/test-simvascular-project-data&gt; for simulation performance\
    \ details.</li>\n</ul>\n<hr>\n<h2>\n<a id=\"user-content-4-download-generated-files\"\
    \ class=\"anchor\" href=\"#4-download-generated-files\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>4. Download generated\
    \ files</h2>\n<ul>\n<li>Compress the folder containing generated files(will be\
    \ named n-procs_case for n cores used):</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ tar -cvzf <span class=\"pl-s\"><span class=\"pl-k\">&lt;&lt;</span><span\
    \ class=\"pl-k\">n-cores&gt;-procs_case&gt;.tar.gz</span> &lt;/scratch/summit/$USER/test-simvascular-project-data/&lt;n-cores&gt;-procs_case&gt;</span></pre></div>\n\
    <ul>\n<li>Use globus file transfer or secure copy the zipped folder on your local\
    \ computer:</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"><pre>scp\
    \ -rv <span class=\"pl-k\">&lt;</span>username<span class=\"pl-k\">&gt;</span>@login.rc.colorado.edu:<span\
    \ class=\"pl-k\">&lt;</span>/scratch/summit/<span class=\"pl-smi\">$USER</span>/test-simvascular-project-data/<span\
    \ class=\"pl-k\">&lt;</span>n-cores<span class=\"pl-k\">&gt;</span>-procs_case.tar.gz<span\
    \ class=\"pl-k\">&gt;</span> <span class=\"pl-k\">&lt;</span>target/storage/path/on/your/computer<span\
    \ class=\"pl-k\">&gt;</span> </pre></div>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1606621737.0
alexisespinosa-uptake/condaContainers:
  data_format: 2
  description: null
  filenames:
  - basicInstallations/polar-bear/02_PortingToSingularity/Singularity.def
  - basicInstallations/pangolin/02_PortingToSingularity/Singularity.def
  full_name: alexisespinosa-uptake/condaContainers
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4791" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-using-container-based-solutions-on-c3se-clusters" class="anchor"
    href="#using-container-based-solutions-on-c3se-clusters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using container-based
    solutions on C3SE clusters</h1>

    <p>Container technology has a number of advantages over the traditional workflow
    of using scientific software. The singularity flavour, in particular, targets
    reproducibility, performance and security with respect to running software in
    an HPC environment. Here, we provide our containerized solutions at C3SE. For
    each of the provided containers, read the specific instructions in the corresponding
    folder to easily get started with using them in your workflow. The actual container
    images are hosted on Singularity Hub. Click on the badge above to quickly get
    access to them!</p>

    <h2>

    <a id="user-content-missing-containers-updates-and-troubleshooting" class="anchor"
    href="#missing-containers-updates-and-troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Missing containers,
    updates, and troubleshooting</h2>

    <p>We continuously add more packages to this repository. If you can''t find a
    relevant container for your needs, or in the case of encountering any errors or
    deprecated features in the material, feel free to contact us: <a href="mailto:support@c3se.chalmers.se">support@c3se.chalmers.se</a>
    or open a pull-request.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1592319832.0
alexiswl/quay_containers:
  data_format: 2
  description: Installation of BioContainers through quay. Generate a module, bash
    wrapper recipe and singularity image ready for take-off on a HPC cluster
  filenames:
  - templates/Singularity._quay_.File
  - templates/Singularity._bioconda_.File
  full_name: alexiswl/quay_containers
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-quay-containers\" class=\"anchor\" href=\"#quay-containers\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quay Containers</h1>\n<p>Installation of BioContainers through quay.\
    \ Generate a module, bash wrapper recipe and singularity image ready for take-off\
    \ on a HPC cluster</p>\n<p><a href=\"https://alexiswl.github.io/presentations/HPC_and_Singularity/HPC_and_Singularity.html\"\
    \ rel=\"nofollow\">Overall Tutorial</a><br>\n<a href=\"https://alexiswl.github.io/presentations/HPC_and_Singularity/HPC_Singularity_Presentation.html\"\
    \ rel=\"nofollow\">Presentation</a></p>\n<h2>\n<a id=\"user-content-guide-to-getting-the-right-values-in-the-yaml\"\
    \ class=\"anchor\" href=\"#guide-to-getting-the-right-values-in-the-yaml\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Guide\
    \ to getting the right values in the yaml.</h2>\n<ol>\n<li>Search for the software\
    \ through <a href=\"https://bioconda.github.io/\" rel=\"nofollow\">bioconda</a>\n\
    </li>\n<li>Ensure that the container is 'container ready' as denoted by a light\
    \ green symbol under the package name.</li>\n<li>Click on the 'tags' link that\
    \ will direct you to the <a href=\"https://quay.io/repository/\" rel=\"nofollow\"\
    >quay.io</a>\n</li>\n<li>Use the name of the quay repository for the software_quay\
    \ item.</li>\n<li>Use the full name of the tag for the version_quay item</li>\n\
    </ol>\n<p><strong>Example:</strong><br>\nI intend to install the star package.<br>\n\
    Searching star in bioconda leads me to <a href=\"https://bioconda.github.io/recipes/star/README.html\"\
    \ rel=\"nofollow\">here</a><br>\nI select <a href=\"https://quay.io/repository/biocontainers/star?tab=tags\"\
    \ rel=\"nofollow\">star/tags</a> to see the quay repo.<br>\nI decide to install\
    \ star version 2.7.0,<br>\nso I specify <code>software_quay</code> as 'star' and\
    \ <code>version_quay</code> as '2.7.0d--0'<br>\nI then run the following</p>\n\
    <pre><code>create_container_from_quay \\\n--yaml yamls/star.yaml \\\n--output-dir\
    \ containers \\\n--module-template templates/module \\\n--bash-template templates/bash_wrapper.sh\
    \ \\\n--singularity-template templates/Singularity._quay_.File \n</code></pre>\n\
    <h2>\n<a id=\"user-content-git-help\" class=\"anchor\" href=\"#git-help\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Git\
    \ Help</h2>\n<h3>\n<a id=\"user-content-adding-all-your-files-to-the-git-repo\"\
    \ class=\"anchor\" href=\"#adding-all-your-files-to-the-git-repo\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Adding\
    \ all your files to the git repo</h3>\n<pre><code>for file in `find -L containers\
    \ -type f -not -name '*.simg'`; do \ngit add $file;\ndone\n</code></pre>\n<h3>\n\
    <a id=\"user-content-adding-just-one-of-your-software-packages-to-the-git-repo\"\
    \ class=\"anchor\" href=\"#adding-just-one-of-your-software-packages-to-the-git-repo\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Adding just one of your software packages to the git repo</h3>\n<pre><code>package_name='my_package'\n\
    for file in `find -L containers/${package_name} -type f -not -name '*.simg'`;\
    \ do \ngit add $file;\ndone\n</code></pre>\n<h3>\n<a id=\"user-content-check-what-has-been-staged-added-to-git-but-not-committed\"\
    \ class=\"anchor\" href=\"#check-what-has-been-staged-added-to-git-but-not-committed\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Check what has been staged (added to git but not committed)</h3>\n\
    <pre><code>git diff --name-only --cached\n</code></pre>\n<h3>\n<a id=\"user-content-check-what-has-not-been-staged-not-added-to-the-git-repo\"\
    \ class=\"anchor\" href=\"#check-what-has-not-been-staged-not-added-to-the-git-repo\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Check what has <strong>not</strong> been staged (not added to the\
    \ git repo)</h3>\n<pre><code>git diff --name-only\n</code></pre>\n<h2>\n<a id=\"\
    user-content-some-extra-goodies\" class=\"anchor\" href=\"#some-extra-goodies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Some extra goodies</h2>\n<h3>\n<a id=\"user-content-building-all-recipes-concurrently\"\
    \ class=\"anchor\" href=\"#building-all-recipes-concurrently\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ all recipes concurrently</h3>\n<pre><code>for recipe in `find containers -name\
    \ '*.recipe'`; do\n# Get image\nimage=${recipe%.recipe}.simg\n# Build image\n\
    sudo singularity build $image $recipe\ndone\n</code></pre>\n<h3>\n<a id=\"user-content-ive-built-all-my-containers\"\
    \ class=\"anchor\" href=\"#ive-built-all-my-containers\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>I've built\
    \ all my containers</h3>\n<h3>\n<a id=\"user-content-im-ready-to-move-them-to-the-specified-container-repo-on-my-hpc\"\
    \ class=\"anchor\" href=\"#im-ready-to-move-them-to-the-specified-container-repo-on-my-hpc\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>I'm ready to move them to the specified container repo on my HPC</h3>\n\
    <p>Chances are, your module files want to go to your  <code>MODULEPATH</code>\n\
    so we'll leave them behind.</p>\n<pre><code>HPC_CONTAINER_PATH=/path/to/containers\n\
    rsync --links --archive --prune-empty-dirs \\\n      --include='*/' --exclude='module'\
    \ \\\n      --verbose \\\n      containers/ ${HPC_CONTAINER_PATH}/\n</code></pre>\n\
    <h3>\n<a id=\"user-content-hard-code-the-container_dir-environment-variable-for-all-my-modules\"\
    \ class=\"anchor\" href=\"#hard-code-the-container_dir-environment-variable-for-all-my-modules\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Hard-code the CONTAINER_DIR environment variable for all my modules</h3>\n\
    <p>In each module template, the <strong>CONTAINER_DIR</strong> variable component\
    \ remains unset.<br>\nThis way, modules are transferrable between HPC clusters\
    \ and organisations.<br>\nTo customise this to your HPC the following code should\
    \ suffice.<br>\nYou may wish to also fork this repo and change the module template.</p>\n\
    <pre><code>HPC_CONTAINER_PATH=/path/to/containers\nfor module in `find containers\
    \ -type f -name module`; do\nsed -i \"s%__CONTAINER_DIR__%${HPC_CONTAINER_PATH}%g\"\
    \ ${module}\ndone\n</code></pre>\n<h3>\n<a id=\"user-content-copy-across-all-my-modules\"\
    \ class=\"anchor\" href=\"#copy-across-all-my-modules\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Copy across all\
    \ my modules</h3>\n<pre><code>HPC_MODULEPATH=/path/to/modules\nfor module in `find\
    \ containers -type f -name module`; do\n# Get software \nsoftware=$(basename $(dirname\
    \ $(dirname $(dirname ${module}))))\n# Get version\nversion=$(basename $(dirname\
    \ $(dirname ${module})))\n# Create software directory if it doesn't exist\nmkdir\
    \ -p ${HPC_MODULEPATH}/$software\n# Copy module over to software \nrsync --checksum\
    \ $module ${HPC_MODULEPATH}/$software/$version\ndone\n</code></pre>\n<h2>\n<a\
    \ id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Troubleshooting</h2>\n<h3>\n<a id=\"user-content-i-get-a-warning-complaining-that-vartmp-is-already-mounted\"\
    \ class=\"anchor\" href=\"#i-get-a-warning-complaining-that-vartmp-is-already-mounted\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>I get a warning complaining that /var/tmp is already mounted</h3>\n\
    <p>This is likely due to /var/tmp existing during the installation of the job.\n\
    <code>/var/tmp</code> links to <code>/tmp</code> at run-time so likely your <code>/var/tmp</code>\
    \ during installation is also just a link to <code>/tmp</code></p>\n<h4>\n<a id=\"\
    user-content-solution\" class=\"anchor\" href=\"#solution\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Solution</h4>\n\
    <p>Append <code>rm /var/tmp</code> to the end of your %post script.</p>\n<h4>\n\
    <a id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Notes</h4>\n\
    <p><strong>DO NOT</strong> delete /tmp during post, do not use <code>rm -rf /var/tmp</code>\
    \ as this will likely delete the contents inside <code>/tmp</code>.\nSince <code>/tmp</code>\
    \ is mounted to <code>/tmp</code> to build time\nThis is the entire host's /tmp\
    \ directory.</p>\n<h4>\n<a id=\"user-content-example-case\" class=\"anchor\" href=\"\
    #example-case\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Example Case</h4>\n<p>fgbio/0.8.0</p>\n<h3>\n<a id=\"\
    user-content-i-get-an-error-about-locales-not-being-able-to-be-set\" class=\"\
    anchor\" href=\"#i-get-an-error-about-locales-not-being-able-to-be-set\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>I\
    \ get an error about locales not being able to be set.</h3>\n<p>If your container\
    \ has been created with busybox, you're out of luck.</p>\n<h4>\n<a id=\"user-content-solution-1\"\
    \ class=\"anchor\" href=\"#solution-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Solution</h4>\n<p>Your best bet\
    \ is to use the bioconda singularity template.\nYou'll need to rename the version_quay\
    \ item in the yaml to the version in bioconda\nand use the Singularity.<em>bioconda</em>.File\
    \ template rather than the quay template.</p>\n<h4>\n<a id=\"user-content-example-case-1\"\
    \ class=\"anchor\" href=\"#example-case-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Example Case</h4>\n<p>See the\
    \ picard 2.18.27 recipe for as an example.</p>\n<h3>\n<a id=\"user-content-can-i-use-a-dry-run-mode-to-see-whats-going-on-underneath\"\
    \ class=\"anchor\" href=\"#can-i-use-a-dry-run-mode-to-see-whats-going-on-underneath\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Can I use a dry-run mode to see what's going on underneath.</h3>\n\
    <p>Check out the help component of the module file.<br>\nYou should see by specifying\
    \ DRY_RUN_&lt;SOFTWARE&gt; to '1', that you can enter dry-run mode for a given\
    \ software.<br>\nYou can also see the environment that the software would be running\
    \ in by setting the dry-run environment variable to 2.</p>\n<h3>\n<a id=\"user-content-i-get-this-java-error-complaining-about-an-unknownhostexception\"\
    \ class=\"anchor\" href=\"#i-get-this-java-error-complaining-about-an-unknownhostexception\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>I get this Java error complaining about an UnknownHostException</h3>\n\
    <h4>\n<a id=\"user-content-solution-2\" class=\"anchor\" href=\"#solution-2\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Solution</h4>\n<p>The quay container has a rogue link to it's /etc/resolv.conf.\
    \ By default Singularity will mount this at runtime from the host (so it doesn't\
    \ need to exist). In this case you will need to remove the link during the %post\
    \ script.</p>\n<h4>\n<a id=\"user-content-example-case-2\" class=\"anchor\" href=\"\
    #example-case-2\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Example case</h4>\n<p>See the fgbio 0.8.0 recipe as\
    \ an example.</p>\n<pre><code># I want to see what STAR would run (but not actually\
    \ run it)\nmodule load star\nexport DRY_RUN_STAR=1\nSTAR --help\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1555076056.0
annacprice/containerCI-test:
  data_format: 2
  description: Testing automated Docker Hub and Singularity Hub builds
  filenames:
  - kraken2/latest/Singularity.kraken2-latest
  - kraken2/2.0.8/Singularity.kraken2-2.0.8
  - abricate/latest/Singularity.abricate-latest
  - abricate/0.9.9/Singularity.abricate-0.9.9
  - quast/latest/Singularity.quast-latest
  - quast/5.0.2/Singularity.quast-5.0.2
  - mykrobe/latest/Singularity.mykrobe-latest
  - mykrobe/0.8.1/Singularity.mykrobe-0.8.1
  - minimap2/latest/Singularity.minimap2-latest
  - minimap2/2.17/Singularity.minimap2-2.17
  - multiqc/latest/Singularity.multiqc-latest
  - multiqc/1.9/Singularity.multiqc-1.9
  - centrifuge/latest/Singularity.centrifuge-latest
  - centrifuge/1.0.4/Singularity.centrifuge-1.0.4
  - trimgalore/latest/Singularity.trimgalore-latest
  - trimgalore/0.6.5/Singularity.trimgalore-0.6.5
  - krona/2.7.1/Singularity.krona-2.7.1
  - krona/latest/Singularity.krona-latest
  - prokka/latest/Singularity.prokka-latest
  - prokka/1.14.5/Singularity.prokka-1.14.5
  - shovill/1.1.0/Singularity.shovill-1.1.0
  - shovill/latest/Singularity.shovill-latest
  - shovill/1.0.0/Singularity.shovill-1.0.0
  full_name: annacprice/containerCI-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-containerci-test" class="anchor" href="#containerci-test"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>containerCI-test</h1>

    <p>Automated Docker Hub and Singularity Hub builds</p>

    <h2>

    <a id="user-content-current-workflow" class="anchor" href="#current-workflow"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Current
    Workflow</h2>

    <ol>

    <li>Create/edit Dockerfile</li>

    <li>Use spython to translate Dockerfile to Singularity recipe</li>

    <li>Set tag and push changes to GitHub, which triggers Docker Hub and Singularity
    Hub builds</li>

    </ol>

    <h2>

    <a id="user-content-downloading-images" class="anchor" href="#downloading-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Downloading
    Images</h2>

    <p>To download all the latest docker images associated with this repo:</p>

    <pre><code>wget https://raw.githubusercontent.com/annacprice/containerCI-test/master/scripts/docker_download.sh

    ./docker_download.sh

    </code></pre>

    <p>To download all the latest singularity images associated with this repo:</p>

    <pre><code>wget https://raw.githubusercontent.com/annacprice/containerCI-test/master/scripts/singularity_download.sh

    ./singularity_download.sh

    </code></pre>

    <h2>

    <a id="user-content-image-repositories" class="anchor" href="#image-repositories"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Image
    Repositories</h2>

    <p><a href="https://hub.docker.com/u/annacprice" rel="nofollow">https://hub.docker.com/u/annacprice</a>
    <br>

    <a href="https://singularity-hub.org/collections/4576" rel="nofollow">https://singularity-hub.org/collections/4576</a></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1603930984.0
anwarMZ/nf-ncov-voc:
  data_format: 2
  description: nf-ncov-voc
  filenames:
  - environments/Singularity
  full_name: anwarMZ/nf-ncov-voc
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-ncov-voc" class="anchor" href="#nf-ncov-voc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-ncov-voc</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620791202.0
anwarMZ/nf-upcoast-v:
  data_format: 2
  description: Nextflow based WGS workflow for Vp (Will be updated later)
  filenames:
  - environments/Singularity
  full_name: anwarMZ/nf-upcoast-v
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-upcoast-v" class="anchor" href="#nf-upcoast-v" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-upcoast-v</h1>

    <p>Nextflow based WGS workflow for Vp (Will be updated later)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619136571.0
anwarMZ/nf-upcoastv-singularity:
  data_format: 2
  description: Singularity container for nf-upcoast-v (https://github.com/anwarMZ/nf-upcoast-v)
  filenames:
  - Singularity
  full_name: anwarMZ/nf-upcoastv-singularity
  latest_release: 1.0.1
  readme: "<h1>\n<a id=\"user-content-singularity-deploy\" class=\"anchor\" href=\"\
    #singularity-deploy\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Singularity Deploy</h1>\n<p><a href=\"img/shpc.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"img/shpc.png\" alt=\"\
    img/shpc.png\" style=\"max-width:100%;\"></a></p>\n<p>Wouldn't it be nice to build\
    \ Singularity images without a registry proper,\nand just keep them alongside\
    \ the GitHub codebase? This is now possible!\nThis small repository provides an\
    \ example to get you started. It will\nbuild one or more images (whatever Singularity.*\
    \ files that are present at\nthe root) and then release them as assets to your\
    \ GitHub repository so\nthat they can be programatically obtained. It is associated\
    \ with\n<a href=\"https://github.com/singularityhub/singularity-hpc\">singularity-hpc</a>\
    \ to allow\nyou to then define LMOD modules for these same containers.</p>\n<blockquote>\n\
    <p>Can I upload the largest of chonkers?</p>\n</blockquote>\n<p>Yes and no. Note\
    \ that assets are limited to 2 GB in size, which is still fairly good. You can\
    \ use\nit as a template for your own recipes as is, or modify it for your custom\n\
    use case. Instructions are below!</p>\n<h2>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h2>\n<h3>\n\
    <a id=\"user-content-1-template-or-fork\" class=\"anchor\" href=\"#1-template-or-fork\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1. Template or Fork</h3>\n<p>If you haven't already, template or fork\
    \ this repository. You can then clone\nyour fork:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ git clone git@github.com:<span class=\"pl-k\"\
    >&lt;</span>username<span class=\"pl-k\">&gt;</span>/singularity-deploy</pre></div>\n\
    <p>You likely want to name the repository by the container. For example, if I\
    \ would\nhave created a container on Docker Hub or similar with the name <code>vsoch/salad</code>,\n\
    here I'd call the repository <code>salad</code>. You obviously are limited to\
    \ your username\nor an organizational namespace.</p>\n<h3>\n<a id=\"user-content-1-write-your-singularity-recipes\"\
    \ class=\"anchor\" href=\"#1-write-your-singularity-recipes\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1. Write\
    \ your Singularity Recipe(s)</h3>\n<p>First, you should write your container recipe(s)\
    \ in the present working directory.\nFor good practice, when you are updating\
    \ recipes you should checkout a new branch\nand open a pull request, as the repository\
    \ comes with a workflow to trigger on a PR\nto <a href=\".github/workflows/test.yml\"\
    >test your container build</a>. You can add any additional\ntests that that you\
    \ might need. By default, any Singularity.* file will be automatically detected.\n\
    If there is no extension (the name Singularity), the name used will be \"latest.\"\
    \nYou can use these tags across multiple releases of your containers. For example,\n\
    these files would generate packages with sifs named as follows:</p>\n<ul>\n<li>\n\
    <a href=\"Singularity\">Singularity</a> maps to <a href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\
    >https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif</a>\n\
    </li>\n<li>\n<a href=\"Singularity.pokemon\">Singularity.pokemon</a> maps to <a\
    \ href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif\"\
    >https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif</a>\n\
    </li>\n<li>\n<a href=\"Singularity.salad\">Singularity.salad</a> maps to <a href=\"\
    https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\
    >https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif</a>\n\
    </li>\n</ul>\n<p>For each name, you can see the direct download URL contains the\
    \ repository (singularityhub/singularity-deploy),\nYou should not use any <code>:</code>\
    \ characters in either your container tag (the GitHub extension) or\nthe GitHub\
    \ tags (the release tags) as this might interfere with parsing.\nThe GitHub release\
    \ tag (0.0.1 in the example above) is discussed next.</p>\n<h3>\n<a id=\"user-content-2-update-the-version-file\"\
    \ class=\"anchor\" href=\"#2-update-the-version-file\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Update the\
    \ VERSION file</h3>\n<p>Any time that you prepare new container recipes, you should\
    \ update the <a href=\"VERSION\">VERSION</a>\nfile. The way that this repository\
    \ works is to generate a release based on the\nstring in <code>VERSION</code>.\
    \ A version is just a tag, so it could be something like\n<code>0.0.1</code> or\
    \ <code>0.0.1-slim</code>. Keep in mind that GitHub releases cannot have duplicated\n\
    names, so you should not repeat the same tag. Do not use <code>:</code> in your\
    \ tag names.\nIf you do need to re-release a tag (not recommended if a user might\
    \ be using it and then it's changed) you can manually delete\nthe release and\
    \ the tag in the GitHub interface. This is a nice structure because it\nmeans\
    \ you can have containers with different names under the same tag. In the example\n\
    above, we have each of \"deploy,\" \"latest,\" and \"salad\" released under tag\
    \ 0.0.1.\nThis is how it looks on GitHub:</p>\n<p><a href=\"img/releases.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"img/releases.png\"\
    \ alt=\"img/releases.png\" style=\"max-width:100%;\"></a></p>\n<h3>\n<a id=\"\
    user-content-3-how-to-develop\" class=\"anchor\" href=\"#3-how-to-develop\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3.\
    \ How to Develop</h3>\n<p>As we mentioned previously, the container builds will\
    \ be tested on a pull request,\nand the release will trigger on merge into your\
    \ main branch (main). See the <a href=\".github/workflows/builder.yml\">.github/workflows/builder.yml</a>)\n\
    to edit this. The idea is that you can:</p>\n<ol>\n<li>Develop your container\
    \ via a development branch</li>\n<li>Open a pull request to test the container\
    \ (the <a href=\".github/workflows/test.yml\">.github/workflows/test.yml</a>)</li>\n\
    <li>On merge, your container will be released!</li>\n</ol>\n<h3>\n<a id=\"user-content-4-how-to-pull\"\
    \ class=\"anchor\" href=\"#4-how-to-pull\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>4. How to pull</h3>\n<p>Technically,\
    \ Singularity can pull just knowing the URL. For example:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ singularity pull https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif</pre></div>\n\
    <p>However, the <a href=\"singularity-hpc\">singularity-hpc</a> tool (will be)\
    \ designed to be able to parse and handle\nthese container uris automatically.\
    \ For the containers here, you could do:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:latest\n$ shpc\
    \ pull gh://singularityhub/singularity-deploy/0.0.1:salad\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:pokemon</pre></div>\n\
    <p>or write the container URI into a registry entry:</p>\n<pre><code>gh: singularityhub/singularity-deploy\n\
    latest:\n  latest: \"0.0.1\"\ntags:\n  \"latest\": \"0.0.1\"\n  \"salad\": \"\
    0.0.1\"\n  \"pokemon\": \"0.0.1\"\nmaintainer: \"@vsoch\"\nurl: https://github.com/singularityhub/singularity-deploy\n\
    </code></pre>\n<p>(This part is still under development!)</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - container-recipes
  - singularity-hpc
  updated_at: 1619679157.0
ar90n/lab:
  data_format: 2
  description: my laboratory
  filenames:
  - singularity_recipes/vnc/Singularity
  - singularity_recipes/common/Singularity
  - singularity_recipes/dotnet/Singularity
  - singularity_recipes/deno/Singularity.tmp
  - singularity_recipes/deno/Singularity.template
  - singularity_recipes/python/Singularity.tmp
  - singularity_recipes/python/Singularity.template
  - singularity_recipes/node/Singularity.tmp
  - singularity_recipes/node/Singularity.template
  - singularity_recipes/procon/Singularity
  - singularity_recipes/cxx/Singularity
  - singularity_recipes/rust/Singularity
  full_name: ar90n/lab
  latest_release: null
  readme: '<h1>

    <a id="user-content-h3abioneth3arefgraph" class="anchor" href="#h3abioneth3arefgraph"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>h3abionet/h3arefgraph</h1>

    <p><strong>RefGraph Workflows Hackathon</strong></p>

    <p><a href="https://travis-ci.org/h3abionet/h3arefgraph" rel="nofollow"><img src="https://camo.githubusercontent.com/1ac1f732e5b8f802a198a533b88e24608b4b10e38d8744373f7bcb5284832ca8/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f68336172656667726170682e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/h3abionet/h3arefgraph.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/h3arefgraph" rel="nofollow"><img src="https://camo.githubusercontent.com/910d36cd9eef9bfef42722b54a531cf2c72b7ed04f37e85d72b93d50b7a3e0c1/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f68336172656667726170682e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/h3arefgraph.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>This pipeline is for the use and testing of graph based methods for variant
    calling.</p>

    <p>The aim is to allow the user to choose the reference graph construction method
    and the alignment / variant calling methods separately.</p>

    <p>We also provide tools for reporting the results of the variant calling, that
    take advantage of the additional contextual information that using reference graphs
    provides.</p>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h3>

    <p>The aim of this project is to separate the different parts of the variant calling
    process to allow the development of

    task specific tools. This is more in line with traditional variant calling where
    specific alignment tools may preform

    better for different organisms, but should not require a different downstream
    analysis for each output.</p>

    <p><a href="assets/images/Overview_slide.jpeg" target="_blank" rel="noopener noreferrer"><img
    src="assets/images/Overview_slide.jpeg" alt="Overview slide" style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The h3abionet/h3arefgraph pipeline comes with documentation about the pipeline,
    found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    <li><a href="docs/configuration/reference_genomes.md">Reference genomes</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>


    <h3>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>

    <p>h3abionet/h3arefgraph was originally written by the H3ABioNet RefGraph Team.</p>

    '
  stargazers_count: 3
  subscribers_count: 1
  topics: []
  updated_at: 1617389118.0
ariclenesGBSN/data-labeling-tool-reactjs:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ariclenesGBSN/data-labeling-tool-reactjs
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"\
    #universal-data-tool\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Universal Data Tool</h1>\n<p><a href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\"\
    \ alt=\"Master Branch\" style=\"max-width:100%;\"></a>\n<a href=\"https://badge.fury.io/js/universal-data-tool\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\"\
    \ alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    ><img src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\"\
    \ alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\"\
    \ alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://twitter.com/UniversalDataTl\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\"\
    \ alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Try it out at <a href=\"https://udt.dev\"\
    \ rel=\"nofollow\">udt.dev</a>, <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >download the desktop app</a> or <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">run on-premise</a>.</p>\n<p align=\"center\">\n  <a href=\"\
    https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\"\
    \ style=\"max-width:100%;\"></a>\n</p>\n<p align=\"center\">\n  <b>\n  <a href=\"\
    https://docs.universaldatatool.com\" rel=\"nofollow\">Docs</a> \u2022 <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">Website</a> \u2022 <a href=\"\
    https://udt.dev\" rel=\"nofollow\">Playground</a> \u2022 <a href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\"\
    \ rel=\"nofollow\">Library Usage</a> \u2022 <a href=\"https://docs.universaldatatool.com/running-on-premise\"\
    \ rel=\"nofollow\">On-Premise</a>\n  </b>\n</p>\n<p>The Universal Data Tool is\
    \ a web/desktop app for editing and annotating images, text, audio, documents\
    \ and to view and edit any data defined in the extensible <a href=\"https://github.com/UniversalDataTool/udt-format\"\
    >.udt.json and .udt.csv standard</a>.</p>\n<h2>\n<a id=\"user-content-supported-data\"\
    \ class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Supported Data</h2>\n<p align=\"\
    center\">\n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\"\
    \ rel=\"nofollow\">Image Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\"\
    \ rel=\"nofollow\">Image Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\"\
    \ rel=\"nofollow\">Text Classification</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\"\
    \ rel=\"nofollow\">Named Entity Recognition</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\"\
    \ rel=\"nofollow\">Named Entity Relations / Part of Speech Tagging</a> \u2022\
    \ \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\"\
    \ rel=\"nofollow\">Audio Transcription</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\"\
    \ rel=\"nofollow\">Data Entry</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\"\
    \ rel=\"nofollow\">Video Segmentation</a> \u2022 \n    <a href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\"\
    \ rel=\"nofollow\">Landmark / Pose Annotation</a>\n</p>\n<h2>\n<a id=\"user-content-recent-updates\"\
    \ class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Recent Updates</h2>\n<p><a href=\"\
    https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\">Follow\
    \ our development on Youtube!</a></p>\n\n<ul>\n<li><a href=\"https://youtu.be/q20WrCRcG4k\"\
    \ rel=\"nofollow\">Community Update Video 9</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\"\
    \ rel=\"nofollow\">Community Update Video 8</a></li>\n<li>\n<a href=\"https://youtu.be/glPPFgXibdw\"\
    \ rel=\"nofollow\">Community Update Video 7</a> <a href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\"\
    \ rel=\"nofollow\">(blog version)</a>\n\n</li>\n</ul>\n<h2>\n<a id=\"user-content-features\"\
    \ class=\"anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Features</h2>\n<ul>\n<li><strong>Collaborate\
    \ with others in real time, no sign up!</strong></li>\n<li>Usable on <a href=\"\
    https://universaldatatool.com\" rel=\"nofollow\">web</a> or as <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\
    >Windows,Mac or Linux desktop application</a>\n</li>\n<li>Configure your project\
    \ with an easy-to-use GUI</li>\n<li><a href=\"https://universaldatatool.com/courses\"\
    \ rel=\"nofollow\">Easily create courses to train your labelers</a></li>\n<li>Download/upload\
    \ as easy-to-use CSV (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\
    >sample.udt.csv</a>) or JSON (<a href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\
    >sample.udt.json</a>)</li>\n<li>Support for Images, Videos, PDFs, Text, Audio\
    \ Transcription and many other formats</li>\n<li>Can be <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\
    >easily integrated into a React application</a>\n</li>\n<li>Annotate images or\
    \ videos with classifications, tags, bounding boxes, polygons and points</li>\n\
    <li>Fast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly</li>\n\
    <li>Import data from Google Drive, Youtube, CSV, Clipboard and more</li>\n<li>Annotate\
    \ NLP datasets with Named Entity Recognition (NER), classification and Part of\
    \ Speech (PoS) tagging.</li>\n<li>Easily <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\
    >load into pandas</a> or <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\
    >use with fast.ai</a>\n</li>\n<li>Runs <a href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\"\
    \ rel=\"nofollow\">with docker</a> <code>docker run -p 3000:3000 universaldatatool/universaldatatool</code>\n\
    </li>\n<li>Runs <a href=\"https://singularity-hub.org/collections/4792\" rel=\"\
    nofollow\">with singularity</a> <code>singularity run universaldatatool/universaldatatool</code>\n\
    </li>\n</ul>\n<p align=\"center\"><kbd><a href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<p align=\"center\"><kbd><a href=\"\
    https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\"\
    \ style=\"max-width:100%;\"></a></kbd></p>\n<h2>\n<a id=\"user-content-sponsors\"\
    \ class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Sponsors</h2>\n<p><a href=\"\
    https://wao.ai\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\"\
    \ alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"></a>\n<a href=\"\
    https://momentum-tech.ca/\" rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\"\
    \ alt=\"momentum image\" style=\"max-width:100%;\"></a>\n<a href=\"https://www.enabledintelligence.net/\"\
    \ rel=\"nofollow\"><img src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\"\
    \ alt=\"enabled intelligence image\" style=\"max-width:100%;\"></a></p>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <h3>\n<a id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Web\
    \ App</h3>\n<p>Just visit <a href=\"https://universaldatatool.com\" rel=\"nofollow\"\
    >universaldatatool.com</a>!</p>\n<p><em>Trying to run the web app locally? Run\
    \ <code>npm install</code> then <code>npm run start</code> after cloning this\
    \ repository to start the web server.</em></p>\n<h3>\n<a id=\"user-content-desktop-application\"\
    \ class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Desktop Application</h3>\n<p>Download\
    \ the latest release from the <a href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\
    >releases page</a> and run the executable you downloaded.</p>\n<h2>\n<a id=\"\
    user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n\
    <ul>\n<li>(Optional) Say hi in the <a href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\"\
    \ rel=\"nofollow\">Slack channel</a>!</li>\n<li>Read <a href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\
    >this guide to get started with development</a>.</li>\n</ul>\n<h2>\n<a id=\"user-content-contributors-\"\
    \ class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributors <g-emoji class=\"\
    g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\
    >\u2728</g-emoji>\n</h2>\n<p>Thanks goes to these wonderful people (<a href=\"\
    https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\">emoji key</a>):</p>\n\
    \n\n\n<table>\n  <tr>\n    <td align=\"center\">\n<a href=\"https://twitter.com/seveibar\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Severin Ibarluzea</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://puskuruk.github.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Puskuruk</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\"\
    \ title=\"Reviewed Pull Requests\"><g-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\">\U0001F440\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/CedricJean\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>CedricJean</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"http://berupon.hatenablog.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>beru</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Ownmarc\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Marc</b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/Wafaa-arbash\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Wafaa-arbash</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/pgrimaud\"\
    ><img src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Pierre Grimaud</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\"\
    \ title=\"Documentation\"><g-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\">\U0001F4D6\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/sreevardhanreddi\"><img src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>sreevardhanreddi</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/mrdadah\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Mohammed Eldadah</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://x8795278.blogspot.com/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>x213212</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hysios\"\
    ><img src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>hysios </b></sub></a><br><a href=\"\
    https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://congdv.github.io/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Cong Dao</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Renato Junior</b></sub></a><br><a\
    \ href=\"#translation-MrJunato\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://gitlab.com/rickstaa\"\
    \ rel=\"nofollow\"><img src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Rick</b></sub></a><br><a\
    \ href=\"#translation-rickstaa\" title=\"Translation\"><g-emoji class=\"g-emoji\"\
    \ alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a> <a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n  <tr>\n    <td align=\"center\">\n<a href=\"\
    https://github.com/anaplian\"><img src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>anaplian</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://www.behance.net/MiguelCarvalho13\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Miguel Carvalho</b></sub></a><br><a\
    \ href=\"#translation-miguelcarvalho13\" title=\"Translation\"><g-emoji class=\"\
    g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\
    >\U0001F30D</g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://kyleo.io\"\
    \ rel=\"nofollow\"><img src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\"\
    \ width=\"100px;\" alt=\"\" style=\"max-width:100%;\"><br><sub><b>Kyle OBrien</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/hakkiyagiz\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Hakk\u0131 Ya\u011F\u0131z ERD\u0130\
    N\xC7</b></sub></a><br><a href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n    <td align=\"center\">\n<a href=\"https://github.com/jvdavim\"\
    ><img src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\"\
    \ alt=\"\" style=\"max-width:100%;\"><br><sub><b>Jo\xE3o Victor Davim</b></sub></a><br><a\
    \ href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\"\
    \ title=\"Code\"><g-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\">\U0001F4BB\
    </g-emoji></a>\n</td>\n  </tr>\n</table>\n\n\n\n<p>This project follows the <a\
    \ href=\"https://github.com/all-contributors/all-contributors\">all-contributors</a>\
    \ specification. Contributions of any kind welcome!</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1615266457.0
automl/HPOBench:
  data_format: 2
  description: Collection of hyperparameter optimization benchmark problems
  filenames:
  - hpobench/container/recipes/Singularity.template
  - hpobench/container/recipes/surrogates/Singularity.ParamnetBenchmark
  - hpobench/container/recipes/nas/Singularity.nasbench_1shot1
  - hpobench/container/recipes/nas/Singularity.nasbench_101
  - hpobench/container/recipes/nas/Singularity.TabularBenchmarks
  - hpobench/container/recipes/nas/Singularity.nasbench_201
  - hpobench/container/recipes/rl/Singularity.learnaBenchmark
  - hpobench/container/recipes/rl/Singularity.Cartpole
  - hpobench/container/recipes/ml/Singularity.PyBNN
  - hpobench/container/recipes/ml/Singularity.XGBoostBenchmark
  - hpobench/container/recipes/ml/Singularity.SupportVectorMachine
  full_name: automl/HPOBench
  latest_release: v0.0.5
  readme: "<h1>\n<a id=\"user-content-hpobench\" class=\"anchor\" href=\"#hpobench\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>HPOBench</h1>\n<p>HPOBench is a library for hyperparameter optimization\
    \ and black-box optimization benchmark with a focus on reproducibility.</p>\n\
    <p><strong>Note:</strong> HPOBench is under active construction. Stay tuned for\
    \ more benchmarks. Information on how to contribute a new benchmark will follow\
    \ shortly.</p>\n<p><strong>Note:</strong> If you are looking for a different or\
    \ older version of our benchmarking library, you might be looking for\n<a href=\"\
    https://github.com/automl/HPOlib1.5\">HPOlib1.5</a></p>\n<h2>\n<a id=\"user-content-in-4-lines-of-code\"\
    \ class=\"anchor\" href=\"#in-4-lines-of-code\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>In 4 lines of code</h2>\n<p>Run\
    \ a random configuration within a singularity container</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">hpobench</span>.<span class=\"pl-s1\">container</span>.<span class=\"\
    pl-s1\">benchmarks</span>.<span class=\"pl-s1\">ml</span>.<span class=\"pl-s1\"\
    >xgboost_benchmark</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\"\
    >XGBoostBenchmark</span>\n<span class=\"pl-s1\">b</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-v\">XGBoostBenchmark</span>(<span class=\"pl-s1\">task_id</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">167149</span>, <span class=\"\
    pl-s1\">container_source</span><span class=\"pl-c1\">=</span><span class=\"pl-s\"\
    >'library://phmueller/automl'</span>, <span class=\"pl-s1\">rng</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-s1\">config</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"\
    pl-en\">get_configuration_space</span>(<span class=\"pl-s1\">seed</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>).<span class=\"pl-en\"\
    >sample_configuration</span>()\n<span class=\"pl-s1\">result_dict</span> <span\
    \ class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"pl-en\"\
    >objective_function</span>(<span class=\"pl-s1\">configuration</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">config</span>, <span class=\"pl-s1\">fidelity</span><span\
    \ class=\"pl-c1\">=</span>{<span class=\"pl-s\">\"n_estimators\"</span>: <span\
    \ class=\"pl-c1\">128</span>, <span class=\"pl-s\">\"dataset_fraction\"</span>:\
    \ <span class=\"pl-c1\">0.5</span>}, <span class=\"pl-s1\">rng</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">1</span>)</pre></div>\n<p>All benchmarks\
    \ can also be queried with fewer or no fidelities:</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">hpobench</span>.<span class=\"pl-s1\">container</span>.<span class=\"\
    pl-s1\">benchmarks</span>.<span class=\"pl-s1\">ml</span>.<span class=\"pl-s1\"\
    >xgboost_benchmark</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\"\
    >XGBoostBenchmark</span>\n<span class=\"pl-s1\">b</span> <span class=\"pl-c1\"\
    >=</span> <span class=\"pl-v\">XGBoostBenchmark</span>(<span class=\"pl-s1\">task_id</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">167149</span>, <span class=\"\
    pl-s1\">container_source</span><span class=\"pl-c1\">=</span><span class=\"pl-s\"\
    >'library://phmueller/automl'</span>, <span class=\"pl-s1\">rng</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-s1\">config</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"\
    pl-en\">get_configuration_space</span>(<span class=\"pl-s1\">seed</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>).<span class=\"pl-en\"\
    >sample_configuration</span>()\n<span class=\"pl-s1\">result_dict</span> <span\
    \ class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"pl-en\"\
    >objective_function</span>(<span class=\"pl-s1\">configuration</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">config</span>, <span class=\"pl-s1\">fidelity</span><span\
    \ class=\"pl-c1\">=</span>{<span class=\"pl-s\">\"n_estimators\"</span>: <span\
    \ class=\"pl-c1\">128</span>,}, <span class=\"pl-s1\">rng</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-s1\">result_dict</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"\
    pl-en\">objective_function</span>(<span class=\"pl-s1\">configuration</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s1\">config</span>, <span class=\"\
    pl-s1\">rng</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>)</pre></div>\n\
    <p>Containerized benchmarks do not rely on external dependencies and thus do not\
    \ change. To do so, we rely on <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">Singularity (version 3.5)</a>.</p>\n<p>Further requirements\
    \ are: <a href=\"https://github.com/automl/ConfigSpace\">ConfigSpace</a>, <em>scipy</em>\
    \ and <em>numpy</em></p>\n<p><strong>Note:</strong> Each benchmark can also be\
    \ run locally, but the dependencies must be installed manually and might conflict\
    \ with other benchmarks.\nThis can be arbitrarily complex and further information\
    \ can be found in the docstring of the benchmark.</p>\n<p>A simple example is\
    \ the XGBoost benchmark which can be installed with <code>pip install .[xgboost]</code></p>\n\
    <div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">hpobench</span>.<span class=\"pl-s1\">benchmarks</span>.<span\
    \ class=\"pl-s1\">ml</span>.<span class=\"pl-s1\">xgboost_benchmark</span> <span\
    \ class=\"pl-k\">import</span> <span class=\"pl-v\">XGBoostBenchmark</span>\n\
    <span class=\"pl-s1\">b</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\"\
    >XGBoostBenchmark</span>(<span class=\"pl-s1\">task_id</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-c1\">167149</span>)\n<span class=\"pl-s1\">config</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"\
    pl-en\">get_configuration_space</span>(<span class=\"pl-s1\">seed</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>).<span class=\"pl-en\"\
    >sample_configuration</span>()\n<span class=\"pl-s1\">result_dict</span> <span\
    \ class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"pl-en\"\
    >objective_function</span>(<span class=\"pl-s1\">configuration</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">config</span>, <span class=\"pl-s1\">fidelity</span><span\
    \ class=\"pl-c1\">=</span>{<span class=\"pl-s\">\"n_estimators\"</span>: <span\
    \ class=\"pl-c1\">128</span>, <span class=\"pl-s\">\"dataset_fraction\"</span>:\
    \ <span class=\"pl-c1\">0.5</span>}, <span class=\"pl-s1\">rng</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-c1\">1</span>)</pre></div>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>Before\
    \ we start, we recommend using a virtual environment. To run any benchmark using\
    \ its singularity container,\nrun the following:</p>\n<pre><code>git clone https://github.com/automl/HPOBench.git\n\
    cd HPOBench \npip install .\n</code></pre>\n<p><strong>Note:</strong> This does\
    \ not install <em>singularity (version 3.5)</em>. Please follow the steps described\
    \ here: <a href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html#quick-installation-steps\"\
    \ rel=\"nofollow\">user-guide</a>.</p>\n<h2>\n<a id=\"user-content-available-containerized-benchmarks\"\
    \ class=\"anchor\" href=\"#available-containerized-benchmarks\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Available\
    \ Containerized Benchmarks</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Benchmark\
    \ Name</th>\n<th>Container Name</th>\n<th>Additional Info</th>\n</tr>\n</thead>\n\
    <tbody>\n<tr>\n<td align=\"left\">BNNOn*</td>\n<td>pybnn</td>\n<td>There are 4\
    \ benchmark in total (ToyFunction, BostonHousing, ProteinStructure, YearPrediction)</td>\n\
    </tr>\n<tr>\n<td align=\"left\">CartpoleFull</td>\n<td>cartpole</td>\n<td>Not\
    \ deterministic.</td>\n</tr>\n<tr>\n<td align=\"left\">CartpoleReduced</td>\n\
    <td>cartpole</td>\n<td>Not deterministic.</td>\n</tr>\n<tr>\n<td align=\"left\"\
    >SliceLocalizationBenchmark</td>\n<td>tabular_benchmarks</td>\n<td>Loading may\
    \ take several minutes.</td>\n</tr>\n<tr>\n<td align=\"left\">ProteinStructureBenchmark</td>\n\
    <td>tabular_benchmarks</td>\n<td>Loading may take several minutes.</td>\n</tr>\n\
    <tr>\n<td align=\"left\">NavalPropulsionBenchmark</td>\n<td>tabular_benchmarks</td>\n\
    <td>Loading may take several minutes.</td>\n</tr>\n<tr>\n<td align=\"left\">ParkinsonsTelemonitoringBenchmark</td>\n\
    <td>tabular_benchmarks</td>\n<td>Loading may take several minutes.</td>\n</tr>\n\
    <tr>\n<td align=\"left\">NASCifar10*Benchmark</td>\n<td>nasbench_101</td>\n<td>Loading\
    \ may take several minutes. There are 3 benchmark in total (A, B, C)</td>\n</tr>\n\
    <tr>\n<td align=\"left\">*NasBench201Benchmark</td>\n<td>nasbench_201</td>\n<td>Loading\
    \ may take several minutes. There are 3 benchmarks in total (Cifar10Valid, Cifar100,\
    \ ImageNet)</td>\n</tr>\n<tr>\n<td align=\"left\">NASBench1shot1SearchSpace*Benchmark</td>\n\
    <td>nasbench_1shot1</td>\n<td>Loading may take several minutes. There are 3 benchmarks\
    \ in total (1,2,3)</td>\n</tr>\n<tr>\n<td align=\"left\">ParamNet*OnStepsBenchmark</td>\n\
    <td>paramnet</td>\n<td>There are 6 benchmarks in total (Adult, Higgs, Letter,\
    \ Mnist, Optdigits, Poker)</td>\n</tr>\n<tr>\n<td align=\"left\">ParamNet*OnTimeBenchmark</td>\n\
    <td>paramnet</td>\n<td>There are 6 benchmarks in total (Adult, Higgs, Letter,\
    \ Mnist, Optdigits, Poker)</td>\n</tr>\n<tr>\n<td align=\"left\">Learna\u207A\
    </td>\n<td>learna_benchmark</td>\n<td>Not deterministic.</td>\n</tr>\n<tr>\n<td\
    \ align=\"left\">MetaLearna\u207A</td>\n<td>learna_benchmark</td>\n<td>Not deterministic.</td>\n\
    </tr>\n<tr>\n<td align=\"left\">XGBoostBenchmark\u207A</td>\n<td>xgboost_benchmark</td>\n\
    <td>Works with OpenML task ids.</td>\n</tr>\n<tr>\n<td align=\"left\">XGBoostExtendedBenchmark\u207A\
    </td>\n<td>xgboost_benchmark</td>\n<td>Works with OpenML task ids + Contains Additional\
    \ Parameter `Booster</td>\n</tr>\n<tr>\n<td align=\"left\">SupportVectorMachine\u207A\
    </td>\n<td>svm_benchmark</td>\n<td>Works with OpenML task ids.</td>\n</tr>\n</tbody>\n\
    </table>\n<p>\u207A these benchmarks are not yet final and might change</p>\n\
    <p><strong>Note:</strong> All containers are uploaded <a href=\"https://gitlab.tf.uni-freiburg.de/muelleph/hpobench-registry/container_registry\"\
    \ rel=\"nofollow\">here</a></p>\n<h2>\n<a id=\"user-content-further-notes\" class=\"\
    anchor\" href=\"#further-notes\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Further Notes</h2>\n<h3>\n<a id=\"\
    user-content-configure-the-hpobench\" class=\"anchor\" href=\"#configure-the-hpobench\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Configure the HPOBench</h3>\n<p>All of HPOBench's settings are stored\
    \ in a file, the <code>hpobenchrc</code>-file.\nIt is a yaml file, which is automatically\
    \ generated at the first use of HPOBench.\nBy default, it is placed in <code>$XDG_CONFIG_HOME</code>.\
    \ If <code>$XDG_CONFIG_HOME</code> is not set, then the\n<code>hpobenchrc</code>-file\
    \ is saved to <code>'~/.config/hpobench'</code>.\nMake sure to have write permissions\
    \ in this directory.</p>\n<p>In the <code>hpobenchrc</code>, you can specify for\
    \ example the directory, in that the benchmark-containers are\ndownloaded. We\
    \ encourage you to take a look into the <code>hpobenchrc</code>, to find out more\
    \ about all\npossible settings.</p>\n<h3>\n<a id=\"user-content-how-to-build-a-container-locally\"\
    \ class=\"anchor\" href=\"#how-to-build-a-container-locally\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How to build\
    \ a container locally</h3>\n<p>With singularity installed run the following to\
    \ built the xgboost container</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> hpobench/container/recipes/ml\nsudo singularity\
    \ build xgboost_benchmark Singularity.XGBoostBenchmark</pre></div>\n<p>You can\
    \ use this local image with:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">hpobench</span>.<span\
    \ class=\"pl-s1\">container</span>.<span class=\"pl-s1\">benchmarks</span>.<span\
    \ class=\"pl-s1\">ml</span>.<span class=\"pl-s1\">xgboost_benchmark</span> <span\
    \ class=\"pl-k\">import</span> <span class=\"pl-v\">XGBoostBenchmark</span>\n\
    <span class=\"pl-s1\">b</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\"\
    >XGBoostBenchmark</span>(<span class=\"pl-s1\">task_id</span><span class=\"pl-c1\"\
    >=</span><span class=\"pl-c1\">167149</span>, <span class=\"pl-s1\">container_name</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s\">\"xgboost_benchmark\"</span>, \n\
    \                     <span class=\"pl-s1\">container_source</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s\">'./'</span>) <span class=\"pl-c\"># path\
    \ to hpobench/container/recipes/ml</span>\n<span class=\"pl-s1\">config</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"\
    pl-en\">get_configuration_space</span>(<span class=\"pl-s1\">seed</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>).<span class=\"pl-en\"\
    >sample_configuration</span>()\n<span class=\"pl-s1\">result_dict</span> <span\
    \ class=\"pl-c1\">=</span> <span class=\"pl-s1\">b</span>.<span class=\"pl-en\"\
    >objective_function</span>(<span class=\"pl-s1\">config</span>, <span class=\"\
    pl-s1\">fidelity</span><span class=\"pl-c1\">=</span>{<span class=\"pl-s\">\"\
    n_estimators\"</span>: <span class=\"pl-c1\">128</span>, <span class=\"pl-s\"\
    >\"dataset_fraction\"</span>: <span class=\"pl-c1\">0.5</span>})</pre></div>\n\
    <h3>\n<a id=\"user-content-remove-all-caches\" class=\"anchor\" href=\"#remove-all-caches\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Remove all caches</h3>\n<h4>\n<a id=\"user-content-hpobench-data\"\
    \ class=\"anchor\" href=\"#hpobench-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>HPOBench data</h4>\n<p>HPOBench\
    \ stores downloaded containers and datasets at the following locations:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-smi\">$XDG_CONFIG_HOME</span>\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> ~/.config/hpobench</span>\n\
    <span class=\"pl-smi\">$XDG_CACHE_HOME</span> <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> ~/.cache/hpobench</span>\n<span class=\"pl-smi\">$XDG_DATA_HOME</span>\
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span> ~/.local/share/hpobench</span></pre></div>\n\
    <p>For crashes or when not properly shutting down containers, there might be socket\
    \ files left under <code>/tmp/</code>.</p>\n<h4>\n<a id=\"user-content-openml-data\"\
    \ class=\"anchor\" href=\"#openml-data\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>OpenML data</h4>\n<p>OpenML data\
    \ additionally maintains it's own cache which is located at <code>~/.openml/</code></p>\n\
    <h4>\n<a id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity container</h4>\n<p>Singularity additionally maintains\
    \ it's own cache which can be removed with <code>singularity cache clean</code></p>\n\
    <h3>\n<a id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Troubleshooting</h3>\n<ul>\n<li>\n<p><strong>Singularity throws an\
    \ 'Invalid Image format' exception</strong>\nUse a singularity version &gt; 3.\
    \ For users of the Meta-Cluster in Freiburg, you have to set the following path:\n\
    <code>export PATH=/usr/local/kislurm/singularity-3.5/bin/:$PATH</code></p>\n</li>\n\
    <li>\n<p><strong>A Benchmark fails with <code>SystemError: Could not start a instance\
    \ of the benchmark. Retried 5 times</code> but the container\ncan be started locally\
    \ with <code>singularity instance start &lt;pathtocontainer&gt; test</code></strong>\n\
    See whether in <code>~/.singularity/instances/sing/$HOSTNAME/*/</code> there is\
    \ a file that does not end with '}'. If yes delete this file and retry.</p>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-status\" class=\"anchor\" href=\"#status\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Status</h2>\n<p>Status for Master Branch:\n<a href=\"https://https://github.com/automl/HPOBench/actions\"\
    \ rel=\"nofollow\"><img src=\"https://github.com/automl/HPOBench/workflows/Test%20Pull%20Requests/badge.svg?branch=master\"\
    \ alt=\"Build Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/gh/automl/HPOBench\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a034097751613923b89642227271f5932b385694eb079b833b66cc6bce2b924a/68747470733a2f2f636f6465636f762e696f2f67682f6175746f6d6c2f48504f42656e63682f6272616e63682f6d61737465722f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/automl/HPOBench/branch/master/graph/badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Status for Development Branch:\n<a href=\"\
    https://https://github.com/automl/HPOBench/actions\" rel=\"nofollow\"><img src=\"\
    https://github.com/automl/HPOBench/workflows/Test%20Pull%20Requests/badge.svg?branch=development\"\
    \ alt=\"Build Status\" style=\"max-width:100%;\"></a>\n<a href=\"https://codecov.io/gh/automl/HPOBench\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/86b984292042d233294c09940c1b7dade7c877c1220bc419f2e2227eb0f775f0/68747470733a2f2f636f6465636f762e696f2f67682f6175746f6d6c2f48504f42656e63682f6272616e63682f646576656c6f706d656e742f67726170682f62616467652e737667\"\
    \ alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/automl/HPOBench/branch/development/graph/badge.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 31
  subscribers_count: 7
  topics:
  - hyperparameter-optimization
  - benchmarking
  - bayesian-optimization
  - benchmark
  - containerized-benchmarks
  - hpolib
  updated_at: 1621464102.0
auze7347/Singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: auze7347/Singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4791" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-using-container-based-solutions-on-c3se-clusters" class="anchor"
    href="#using-container-based-solutions-on-c3se-clusters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using container-based
    solutions on C3SE clusters</h1>

    <p>Container technology has a number of advantages over the traditional workflow
    of using scientific software. The singularity flavour, in particular, targets
    reproducibility, performance and security with respect to running software in
    an HPC environment. Here, we provide our containerized solutions at C3SE. For
    each of the provided containers, read the specific instructions in the corresponding
    folder to easily get started with using them in your workflow. The actual container
    images are hosted on Singularity Hub. Click on the badge above to quickly get
    access to them!</p>

    <h2>

    <a id="user-content-missing-containers-updates-and-troubleshooting" class="anchor"
    href="#missing-containers-updates-and-troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Missing containers,
    updates, and troubleshooting</h2>

    <p>We continuously add more packages to this repository. If you can''t find a
    relevant container for your needs, or in the case of encountering any errors or
    deprecated features in the material, feel free to contact us: <a href="mailto:support@c3se.chalmers.se">support@c3se.chalmers.se</a>
    or open a pull-request.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604388527.0
baberlevi/spack-singularity:
  data_format: 2
  description: Singularity container with Spack
  filenames:
  - Singularity.spack-root
  - Singularity.spack-rhel
  - Singularity.spackbase
  - Singularity.spack-lmod
  - Singularity.spack-fastqvalidator
  - Singularity.spack-bowtie
  - Singularity.spack
  full_name: baberlevi/spack-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-work-in-progress" class="anchor" href="#work-in-progress"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>work
    in progress</h1>

    <p>attempt to build a base singularity image with spack that can be used as the
    bootstrap for

    other singularity images that would perform the spack install of a particular
    package</p>

    <p>currently having an issue with stage directory for spack attempting to write
    to

    the immutable squashfs</p>

    <p>as expected, the child container will happily install during %post since it
    can write</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1521605340.0
barbagroup/geoclaw-landspill:
  data_format: 2
  description: An oil land-spill and overland flow simulator for pipeline rupture
    events
  filenames:
  - Singularityfiles/Singularity.v1.0.dev1
  - Singularityfiles/Singularity.v1.0.dev3
  - Singularityfiles/Singularity.v0.1.bionic
  - Singularityfiles/Singularity.v1.0.dev4
  - Singularityfiles/Singularity.v1.0.dev2
  - Singularityfiles/Singularity.v0.1.trusty
  full_name: barbagroup/geoclaw-landspill
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-geoclaw-landspill\" class=\"anchor\" href=\"\
    #geoclaw-landspill\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>geoclaw-landspill</h1>\n<p><a href=\"https://github.com/barbagroup/geoclaw-landspill/raw/master/LICENSE\"\
    ><img src=\"https://camo.githubusercontent.com/8ccf186e7288af6d88a1f6a930c0fcc4e7a8a9936b34e07629d815d1eab4d977/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.com/barbagroup/geoclaw-landspill\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7f22540fd3a0f3b9a8d8a57ead3744961fd8b2d9edd257d02e4a8e0ae1d6d7a6/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f636f6d2f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6d61737465723f6c6162656c3d5472617669732532304349\"\
    \ alt=\"Travis CI\" data-canonical-src=\"https://img.shields.io/travis/com/barbagroup/geoclaw-landspill/master?label=Travis%20CI\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/barbagroup/geoclaw-landspill/actions?query=workflow%3ACI\"\
    ><img src=\"https://camo.githubusercontent.com/91894ad74ed7c9cc23b3f2fb08cc1ea432c8ed6830abd92e489e7f59ac619ab3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f43492f6d61737465723f6c6162656c3d476974487562253230416374696f6e2532304349\"\
    \ alt=\"GitHub Action CI\" data-canonical-src=\"https://img.shields.io/github/workflow/status/barbagroup/geoclaw-landspill/CI/master?label=GitHub%20Action%20CI\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/036ff156f41dafdb919e29a22cd5aa00a7f0ded742b75831240ea25fe720350e/68747470733a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f66623762303132373939613730633962346335356562346262306633366639372f7374617475732e737667\"\
    \ alt=\"status\" data-canonical-src=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97/status.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://anaconda.org/barbagroup/geoclaw-landspill\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9e96d790dd4b2cdbdea7b49eff75628a95ae5cbd3c8f5b7bc902b7f9b603b149/68747470733a2f2f616e61636f6e64612e6f72672f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6261646765732f696e7374616c6c65722f636f6e64612e737667\"\
    \ alt=\"Conda\" data-canonical-src=\"https://anaconda.org/barbagroup/geoclaw-landspill/badges/installer/conda.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><em><strong>Note: if looking for content\
    \ of <code>geoclaw-landspill-cases</code>, please checkout tag\n<code>v0.1</code>.\
    \ This repository has been converted to a fully working solver package.</strong></em></p>\n\
    <p><em>geoclaw-landspill</em> is a package for running oil overland flow simulations\
    \ for\napplications in pipeline risk management. It includes a numerical solver\
    \ and\nsome pre-/post-processing utilities.</p>\n<p><a href=\"./doc/sample.gif\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"./doc/sample.gif\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>The numerical solver is a modified version\
    \ of\n<a href=\"http://www.clawpack.org/geoclaw.html\" rel=\"nofollow\">GeoClaw</a>.\n\
    GeoClaw solves full shallow-water equations. We added several new features and\n\
    utilities to it and make it usable to simulate the overland flow from pipeline\n\
    ruptures. These features include:</p>\n<ul>\n<li>adding point sources to mimic\
    \ the rupture points</li>\n<li>adding evaporation models</li>\n<li>adding Darcy-Weisbach\
    \ bottom friction models with land roughness</li>\n<li>adding temperature-dependent\
    \ viscosity</li>\n<li>recording detail locations and time of oil flowing into\
    \ in-land waterbodies</li>\n<li>downloading topography and hydrology data automatically\
    \ (the US only)</li>\n<li>generating CF-1.7 compliant NetCDF files</li>\n</ul>\n\
    <h2>\n<a id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Documentation</h2>\n<ol>\n<li><a href=\"doc/deps_install_tests.md\"\
    >Dependencies, installation, and tests</a></li>\n<li><a href=\"doc/usage.md\"\
    >Usage</a></li>\n<li><a href=\"doc/configuration.md\">Configuration file: <code>setrun.py</code></a></li>\n\
    <li><a href=\"cases/README.md\">Example cases</a></li>\n<li><a href=\"doc/container.md\"\
    >Containers: Docker and Singularity</a></li>\n</ol>\n<hr>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick start</h2>\n<p>We only\
    \ maintain compatibility with Linux. Though using <code>pip</code> or building\
    \ from\nsource may still work in Mac OS or Windows (e.g., through WSL), we are\
    \ not able\nto help with the installation issues on these two systems.</p>\n<p>Beyond\
    \ this quick start, to see more details, please refer to the\n<a href=\"#documentation\"\
    >documentation</a> section.</p>\n<h3>\n<a id=\"user-content-1-installation\" class=\"\
    anchor\" href=\"#1-installation\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>1. Installation</h3>\n<p>The fast\
    \ way to install <em>geoclaw-landspill</em> is through\n<a href=\"https://www.anaconda.com/\"\
    \ rel=\"nofollow\">Anaconda</a>'s <code>conda</code> command. The following command\n\
    creates a conda environment (called <code>landspill</code>) and installs the package\
    \ and\ndependencies:</p>\n<pre><code>$ conda create \\\n    -n landspill -c barbagroup\
    \ -c conda-forge \\\n    python=3.8 geoclaw-landspill\n</code></pre>\n<p>Then\
    \ use <code>conda activate landspill</code> or\n<code>source &lt;conda installation\
    \ prefix&gt;/bin/activate landspill</code> to activate the\nenvironment. Type\
    \ <code>geoclaw-landspill --help</code> in the terminal to see if\n<em>geoclaw-landspill</em>\
    \ is correctly installed.</p>\n<h3>\n<a id=\"user-content-2-running-an-example-case\"\
    \ class=\"anchor\" href=\"#2-running-an-example-case\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Running an\
    \ example case</h3>\n<p>To run an example case under the folder <code>cases</code>,\
    \ users have to clone this\nrepository. We currently don't maintain another repository\
    \ for cases. After\ncloning this repository, run</p>\n<pre><code>$ geoclaw-landspill\
    \ run &lt;path to an example case folder&gt;\n</code></pre>\n<p>For example, to\
    \ run <code>utal-flat-maya</code>:</p>\n<pre><code>$ geoclaw-landspill run ./cases/utah-flat-maya\n\
    </code></pre>\n<p>Users can use environment variable <code>OMP_NUM_THREADS</code>\
    \ to control how many CPU\nthreads the simulation should use for OpenMP parallelization.</p>\n\
    <h3>\n<a id=\"user-content-3-creating-a-cf-compliant-netcdf-raster-file\" class=\"\
    anchor\" href=\"#3-creating-a-cf-compliant-netcdf-raster-file\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3.\
    \ Creating a CF-compliant NetCDF raster file</h3>\n<p>After a simulation is done,\
    \ users can convert flow depth in raw simulation data\ninto a CF-compliant NetCDF\
    \ raster file. For example,</p>\n<pre><code>$ geoclaw-landspill createnc ./case/utah-flat-maya\n\
    </code></pre>\n<p>Replace <code>./cases/utah-flat-maya</code> with the path to\
    \ another desired case.</p>\n<p>QGIS and ArcGIS should be able to read the resulting\
    \ NetCDF raster file.</p>\n<hr>\n<h2>\n<a id=\"user-content-third-party-codes-and-licenses\"\
    \ class=\"anchor\" href=\"#third-party-codes-and-licenses\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Third-party\
    \ codes and licenses</h2>\n<ul>\n<li>amrclaw: <a href=\"https://github.com/clawpack/amrclaw\"\
    >https://github.com/clawpack/amrclaw</a>\n(<a href=\"https://github.com/clawpack/amrclaw/blob/ee85c1fe178ec319a8403503e779d3f8faf22840/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>geoclaw: <a href=\"https://github.com/clawpack/geoclaw\"\
    >https://github.com/clawpack/geoclaw</a>\n(<a href=\"https://github.com/clawpack/geoclaw/blob/3593cb1b418fd52739c186a8845a288037c8f575/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>pyclaw: <a href=\"https://github.com/clawpack/pyclaw\"\
    >https://github.com/clawpack/pyclaw</a>\n(<a href=\"https://github.com/clawpack/pyclaw/blob/a85a01a5f20be1a18dde70b7bb37dc1cdcbd0b26/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>clawutil: <a href=\"https://github.com/clawpack/clawutil\"\
    >https://github.com/clawpack/clawutil</a>\n(<a href=\"https://github.com/clawpack/clawutil/blob/116ffb792e889fbf0854d7ac599657039d7b1f3e/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n<li>riemann: <a href=\"https://github.com/clawpack/riemann\"\
    >https://github.com/clawpack/riemann</a>\n(<a href=\"https://github.com/clawpack/riemann/blob/597824c051d56fa0c8818e00d740867283329b24/LICENSE\"\
    >BSD 3-Clause License</a>)</li>\n</ul>\n<hr>\n<h2>\n<a id=\"user-content-contributing\"\
    \ class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n<p>See <a\
    \ href=\"CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p>\n<hr>\n<h2>\n<a id=\"user-content-contact\"\
    \ class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contact</h2>\n<p>Pi-Yueh Chuang:\
    \ <a href=\"mailto:pychuang@gwu.edu\">pychuang@gwu.edu</a></p>\n"
  stargazers_count: 4
  subscribers_count: 5
  topics:
  - geoclaw
  - overland-flow
  - pipeline
  - shallow-water-equations
  - pipeline-ruptures
  - land-spill
  updated_at: 1621675162.0
basnijholt/azure-singularity-agent:
  data_format: 2
  description: An agent for Azure Pipelines using a Singularity image
  filenames:
  - Singularity
  full_name: basnijholt/azure-singularity-agent
  latest_release: null
  readme: '<h1>

    <a id="user-content-wip-azure-singularity-agent" class="anchor" href="#wip-azure-singularity-agent"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>WIP:
    azure-singularity-agent</h1>

    <p>An agent for Azure Pipelines using a Singularity image</p>

    <p>Build with</p>

    <pre><code>sudo singularity build azure-singularity-agent.sif Singularity

    </code></pre>

    <p>Run with</p>

    <pre><code>env AZP_URL=https://dev.azure.com/&lt;organization&gt; AZP_TOKEN=&lt;PAT
    token&gt; AZP_AGENT_NAME=mydockeragent singularity run azure-singularity-agent.sif

    </code></pre>

    <h2>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <ul>

    <li>Seems to not work because the resulting <code>sif</code> is read-only.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1583436019.0
bast/singularity-latex:
  data_format: 2
  description: Singularity recipe for LaTeX.
  filenames:
  - Singularity.pdflatex
  full_name: bast/singularity-latex
  latest_release: 0.1.0
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-latex" class="anchor" href="#singularity-recipe-for-latex"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe for LaTeX</h1>

    <p>How to fetch and use the image:</p>

    <pre><code>$ singularity pull https://github.com/bast/singularity-latex/releases/download/0.1.0/pdflatex.sif

    $ ./pdflatex.sif example.tex

    </code></pre>

    <p>I have used this wonderful guide as starting point and inspiration:

    <a href="https://github.com/singularityhub/singularity-deploy">https://github.com/singularityhub/singularity-deploy</a></p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics:
  - latex
  - pdflatex
  - singularity
  updated_at: 1620408929.0
bast/singularity-pandoc:
  data_format: 2
  description: Singularity recipe for Pandoc.
  filenames:
  - Singularity.pandoc
  full_name: bast/singularity-pandoc
  latest_release: 0.1.0
  readme: '<h1>

    <a id="user-content-singularity-recipe-for-pandoc" class="anchor" href="#singularity-recipe-for-pandoc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe for <a href="https://pandoc.org/" rel="nofollow">Pandoc</a>

    </h1>

    <p>How to fetch and use the image:</p>

    <pre><code>$ singularity pull https://github.com/bast/singularity-pandoc/releases/download/0.1.0/pandoc.sif

    $ ./pandoc.sif --from=markdown --to=rst --output=README.rst README.md

    </code></pre>

    <p>I have used this wonderful guide as starting point and inspiration:

    <a href="https://github.com/singularityhub/singularity-deploy">https://github.com/singularityhub/singularity-deploy</a></p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1620408929.0
baxpr/bedpost-singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: baxpr/bedpost-singularity
  latest_release: null
  readme: '<p>Runs FSL''s bedpostx on the input DWI data set, and creates a PDF report
    of the results.

    Quite simple - see /opt/src/pipeline.sh for the main script.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621826090.0
baxpr/petreg:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: baxpr/petreg
  latest_release: v1.0.0
  readme: "<h1>\n<a id=\"user-content-pet-to-mr-registration-via-ct\" class=\"anchor\"\
    \ href=\"#pet-to-mr-registration-via-ct\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>PET to MR registration via CT</h1>\n\
    <p>Preprocessing and extraction of data from regions in a PET image, using FSL.\
    \ Regions are derived\nfrom a MultiAtlas segmentation (<a href=\"https://github.com/VUIIS/Multi-Atlas-v3.0.0\"\
    >https://github.com/VUIIS/Multi-Atlas-v3.0.0</a>,\n<a href=\"https://github.com/MASILab/SLANTbrainSeg\"\
    >https://github.com/MASILab/SLANTbrainSeg</a>).</p>\n<ol>\n<li>Motion correction\
    \ applied to PET time series (<code>mcflirt</code>, 6 dof)</li>\n<li>Mean PET\
    \ registered to CT (<code>flirt</code>, 6 dof)</li>\n<li>CT registered to MR (<code>flirt</code>,\
    \ default 6 dof)</li>\n<li>SUVR calculated from summed PET image, referenced to\
    \ cerebellum</li>\n<li>PET regional values extracted with nilearn</li>\n</ol>\n\
    <h2>\n<a id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Inputs</h2>\n\
    <p>The command line is</p>\n<pre><code>petreg.sh &lt;arguments&gt;\n</code></pre>\n\
    <p>And <code>&lt;arguments&gt;</code> are below. The first four are required:</p>\n\
    <pre><code>--pet_niigz       PET time series\n--ct_niigz        CT\n--mr_niigz\
    \        MR (typically T1W)\n--seg_niigz       Segmentation of MR from multi-atlas\
    \ / slant\n\n--ctmr_dof        DOF for CT/MR registration (default 6)\n\n--project\
    \         Information from XNAT, if used (optional). These\n--subject        \
    \   are only used to annotate the PDF QA report.\n--session\n--scan\n\n--out_dir\
    \         Where outputs will be stored (default /OUTPUTS)\n\n--labels_csv    \
    \  Labels corresponding to seg_niigz. Optional argument\n                    only\
    \ needed in special circumstances.\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<p>All output images\
    \ have been resampled to the field of view, voxel size,\nand geometry of the MR.</p>\n\
    <pre><code>CT_REG                   CT registered to MR\nCT_REG_MAT          \
    \     Transform from CT to MR (FSL format)\nMEANPET_REG              Mean PET\
    \ registered to MR\nMOT_PARAMS               PET time series motion parameters\
    \ (FSL format)\nPDF                      QA report\nPET_REG                  PET\
    \ time series registered to MR\nPET_REG_MAT              Transform from PET to\
    \ MR (FSL format)\nPET_ROI_MEANS            ROI means of PET time series\nPET_ROI_SDEVS\
    \            ROI std devs of PET time series\nROIS                     ROI image\n\
    ROI_LABELS               ROI label names\nSUM_REG                  Sum of PET\
    \ images over time\nSUM_ROI_MEANS            ROI means of summed image\nSUM_ROI_SDEVS\
    \            ROI std devs of summed image\nSUVR_REG                 SUVR image\
    \ calculated from summed image\nSUVR_ROI_MEANS           ROI means of SUVR image\n\
    SUVR_ROI_SDEVS           ROI std devs of SUVR image\n</code></pre>\n<h2>\n<a id=\"\
    user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>References</h2>\n\
    <ul>\n<li>FSL: <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki\" rel=\"nofollow\"\
    >https://fsl.fmrib.ox.ac.uk/fsl/fslwiki</a>\n</li>\n<li>MCFLIRT: <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT\"\
    \ rel=\"nofollow\">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT</a>\n</li>\n\
    <li>FLIRT: <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT#References\"\
    \ rel=\"nofollow\">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT#References</a>\n\
    </li>\n<li>nilearn: <a href=\"https://nilearn.github.io/authors.html#citing\"\
    \ rel=\"nofollow\">https://nilearn.github.io/authors.html#citing</a>\n</li>\n\
    <li>MultiAtlas: <a href=\"https://pubmed.ncbi.nlm.nih.gov/23265798/\" rel=\"nofollow\"\
    >https://pubmed.ncbi.nlm.nih.gov/23265798/</a>\n</li>\n<li>SLANT: <a href=\"https://pubmed.ncbi.nlm.nih.gov/30910724/\"\
    \ rel=\"nofollow\">https://pubmed.ncbi.nlm.nih.gov/30910724/</a>\n</li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621746836.0
bglick13/dotaservice:
  data_format: 2
  description: null
  filenames:
  - dockerfiles/Singularity-dota.simg
  - dockerfiles/Singularity-dotaservice.simg
  full_name: bglick13/dotaservice
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-dotaservice\" class=\"anchor\" href=\"#dotaservice\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>DotaService</h1>\n<p><a href=\"dotaservice-icon.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"dotaservice-icon.png\" alt=\"dotaservice\
    \ icon\" width=\"128\" style=\"max-width:100%;\"></a></p>\n<hr>\n<p>NOTE: The\
    \ project that uses the dotaservice in a k8s environment is the <a href=\"https://github.com/TimZaman/dotaclient\"\
    >DotaClient</a> repo.</p>\n<p>DotaService is a service to play Dota 2 through\
    \ gRPC. There are first class python bindings\nand examples, so you can play dota\
    \ as you would use the OpenAI gym API.</p>\n<p>It's fully functional and super\
    \ lightweight. Starting Dota <code>obs = env.reset()</code> takes 5 seconds,\n\
    and each <code>obs = env.step(action)</code> in the environment takes between\
    \ 10 and 30 ms.</p>\n<p>You can even set the config of <code>render=True</code>\
    \ and you can watch the game play live. Each game will\nhave a uuid and folder\
    \ associated where there's a Dota demo (replay) and console logs.</p>\n<p><a href=\"\
    demo.gif\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"demo.gif\"\
    \ alt=\"demo\" width=\"600\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"\
    user-content-run-dotaservice-locally\" class=\"anchor\" href=\"#run-dotaservice-locally\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Run DotaService Locally</h2>\n<p>Run the DotaService so you can connect\
    \ your client to it later. Only one client per server\nis supported, and only\
    \ one DotaService per VM (eg local or one per docker container).</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>python3 -m dotaservice\n&gt;&gt;&gt; Serving\
    \ on 127.0.0.1:13337</pre></div>\n<h2>\n<a id=\"user-content-run-dotaservice-distributed\"\
    \ class=\"anchor\" href=\"#run-dotaservice-distributed\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run DotaService\
    \ Distributed</h2>\n<p>See <a href=\"docker/README.md\">docker/README.md</a>.</p>\n\
    <p>To run two dockerservice instances, one on port <code>13337</code> and one\
    \ on <code>13338</code>, f.e. run:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>docker run -dp 13337:13337 ds\ndocker run -dp 13338:13337 ds</pre></div>\n\
    <p>You can run as many as you want, until you run out of ports or ip addresses.\
    \ If you are wearing\nyour fancy pants, use Kubernetes to deploy gazillions.</p>\n\
    <h2>\n<a id=\"user-content-client-code\" class=\"anchor\" href=\"#client-code\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Client Code</h2>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">grpclib</span>.<span\
    \ class=\"pl-s1\">client</span> <span class=\"pl-k\">import</span> <span class=\"\
    pl-v\">Channel</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\"\
    >protobuf</span>.<span class=\"pl-v\">DotaService_grpc</span> <span class=\"pl-k\"\
    >import</span> <span class=\"pl-v\">DotaServiceStub</span>\n<span class=\"pl-k\"\
    >from</span> <span class=\"pl-s1\">protobuf</span>.<span class=\"pl-v\">DotaService_pb2</span>\
    \ <span class=\"pl-k\">import</span> <span class=\"pl-v\">Action</span>\n<span\
    \ class=\"pl-k\">from</span> <span class=\"pl-s1\">protobuf</span>.<span class=\"\
    pl-v\">DotaService_pb2</span> <span class=\"pl-k\">import</span> <span class=\"\
    pl-v\">Config</span>\n\n<span class=\"pl-c\"># Connect to the DotaService.</span>\n\
    <span class=\"pl-s1\">env</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-v\">DotaServiceStub</span>(<span class=\"pl-v\">Channel</span>(<span class=\"\
    pl-s\">'127.0.0.1'</span>, <span class=\"pl-c1\">13337</span>))\n\n<span class=\"\
    pl-c\"># Get the initial observation.</span>\n<span class=\"pl-s1\">observation</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-k\">await</span> <span class=\"\
    pl-s1\">env</span>.<span class=\"pl-en\">reset</span>(<span class=\"pl-v\">Config</span>())\n\
    <span class=\"pl-k\">for</span> <span class=\"pl-s1\">i</span> <span class=\"\
    pl-c1\">in</span> <span class=\"pl-en\">range</span>(<span class=\"pl-c1\">8</span>):\n\
    \    <span class=\"pl-c\"># Sample an action from the action protobuf</span>\n\
    \    <span class=\"pl-s1\">action</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-v\">Action</span>.<span class=\"pl-v\">MoveToLocation</span>(<span\
    \ class=\"pl-s1\">x</span><span class=\"pl-c1\">=</span>.., <span class=\"pl-s1\"\
    >y</span><span class=\"pl-c1\">=</span>.., <span class=\"pl-s1\">z</span><span\
    \ class=\"pl-c1\">=</span>..)\n    <span class=\"pl-c\"># Take an action, returning\
    \ the resulting observation.</span>\n    <span class=\"pl-s1\">observation</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-k\">await</span> <span class=\"\
    pl-s1\">env</span>.<span class=\"pl-en\">step</span>(<span class=\"pl-s1\">action</span>)</pre></div>\n\
    <p>This is very useful to provide an environment for reinforcement learning, and\
    \ service aspect of it makes it\nespecially useful for distributed training. I\
    \ am planning to provide a client python\nmodule for this (<code>PyDota</code>)\
    \ that mimics typical OpenAI gym APIs. Maybe I won't even make PyDota\nand the\
    \ gRPC client is enough.</p>\n<div>\n<a href=\"dotaservice.png\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"dotaservice.png\" alt=\"dotaservice connections\"\
    \ width=\"680\" style=\"max-width:100%;\"></a>\n</div>\n<h3>\n<a id=\"user-content-requirements\"\
    \ class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Requirements</h3>\n<ul>\n<li>Python\
    \ 3.7</li>\n<li>Unix: MacOS, Ubuntu. A dockerfile is also provided see: <a href=\"\
    docker/README.md\">docker/README.md</a>.</li>\n</ul>\n<h3>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h3>\n<p>Installing\
    \ from pypi:</p>\n<div class=\"highlight highlight-source-shell\"><pre>pip3 install\
    \ dotaservice</pre></div>\n<p>For development; installing from source:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>pip3 install -e <span class=\"\
    pl-c1\">.</span></pre></div>\n<p>(Optional) Compile the protos for Python (run\
    \ from repository root):</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>python3 -m grpc_tools.protoc -I. --python_out=. --python_grpc_out=. --grpc_python_out=.\
    \ dotaservice/protos/<span class=\"pl-k\">*</span>.proto</pre></div>\n<h1>\n<a\
    \ id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Notes</h1>\n\
    <p>My dev notes: <a href=\"NOTES.md\">NOTES.md</a>.</p>\n<hr>\n<h1>\n<a id=\"\
    user-content-acknowledgements\" class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Acknowledgements</h1>\n\
    <ul>\n<li>OpenAI Dota crew</li>\n<li><a href=\"http://karpathy.github.io/2016/05/31/rl/\"\
    \ rel=\"nofollow\">Karpathy</a></li>\n<li>Jan Ivanecky</li>\n<li><a href=\"https://github.com/Nostrademous\"\
    >Nostrademous</a></li>\n</ul>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1585945278.0
bii-dpi/sing:
  data_format: 2
  description: null
  filenames:
  - Singularity.d
  full_name: bii-dpi/sing
  latest_release: null
  readme: '<h1>

    <a id="user-content-genmon-sidef" class="anchor" href="#genmon-sidef" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>genmon-sidef</h1>

    <p>Singularity definition and tools for GenMon. GenMon is tool for monitoring
    genetic resources of animal populations.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618901689.0
bioexcel/biobb_cmip:
  data_format: 2
  description: null
  filenames:
  - Singularity.latest
  full_name: bioexcel/biobb_cmip
  latest_release: null
  readme: '<p><a href="https://biobb-cmip.readthedocs.io/en/latest/?badge=latest"
    rel="nofollow"><img src="https://camo.githubusercontent.com/8bd09664a4dca78a8f246d76f3af7fc6da719393b3f9c6cbc6a8b291b19f3d80/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f62696f62622d636d69702f62616467652f3f76657273696f6e3d6c6174657374"
    alt="" data-canonical-src="https://readthedocs.org/projects/biobb-cmip/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="https://anaconda.org/bioconda/biobb_cmip" rel="nofollow"><img src="https://camo.githubusercontent.com/a477fdb1fd9bc9eb7ffa6cae6a019c6d4c3902fd468b3126f1b78e56c7dcff83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e7376673f7374796c653d666c6174"
    alt="" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat"
    style="max-width:100%;"></a>

    <a href="https://quay.io/repository/biocontainers/biobb_cmip" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ca418e4db0b3de91a09a5df4a59446da015b6164598a8bc255918e911484f84f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d517561792e696f2d626c7565"
    alt="" data-canonical-src="https://img.shields.io/badge/docker-Quay.io-blue" style="max-width:100%;"></a></p>

    <p><a href="https://www.singularity-hub.org/collections/2735/usage" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667"
    alt="" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-biobb_cmip" class="anchor" href="#biobb_cmip" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>biobb_cmip</h1>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>Biobb_cmip is the Biobb module collection to compute classical molecular interaction
    potentials.

    Biobb (BioExcel building blocks) packages are Python building blocks that

    create new layer of compatibility and interoperability over popular

    bioinformatics tools.

    The latest documentation of this package can be found in our readthedocs site:

    <a href="http://biobb-cmip.readthedocs.io/en/latest/" rel="nofollow">latest API
    documentation</a>.</p>

    <h3>

    <a id="user-content-version" class="anchor" href="#version" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version</h3>

    <p>v3.7.0 2021.2</p>

    <h3>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h3>

    <p>Using PIP:</p>

    <blockquote>

    <p><strong>Important:</strong> PIP only installs the package. All the dependencies
    must be installed separately. To perform a complete installation, please use ANACONDA,
    DOCKER or SINGULARITY.</p>

    </blockquote>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  pip install "biobb_cmip&gt;=3.7.0"

    </code></pre>

    </li>

    <li>

    <p>Usage: <a href="https://biobb-cmip.readthedocs.io/en/latest/modules.html" rel="nofollow">Python
    API documentation</a></p>

    </li>

    </ul>

    <p>Using ANACONDA:</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  conda install -c bioconda "biobb_cmip&gt;=3.7.0"

    </code></pre>

    </li>

    <li>

    <p>Usage: With conda installation BioBBs can be used with the <a href="https://biobb-cmip.readthedocs.io/en/latest/modules.html"
    rel="nofollow">Python API documentation</a> and the <a href="https://biobb-cmip.readthedocs.io/en/latest/command_line.html"
    rel="nofollow">Command Line documentation</a></p>

    </li>

    </ul>

    <p>Using DOCKER:</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  docker pull quay.io/biocontainers/biobb_cmip:3.7.0--py_0

    </code></pre>

    </li>

    <li>

    <p>Usage:</p>

    <pre><code>  docker run quay.io/biocontainers/biobb_cmip:3.7.0--py_0 &lt;command&gt;

    </code></pre>

    </li>

    </ul>

    <p>Using SINGULARITY:</p>

    <p><strong>MacOS users</strong>: it''s strongly recommended to avoid Singularity
    and use <strong>Docker</strong> as containerization system.</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  singularity pull --name biobb_cmip.sif shub://bioexcel/biobb_cmip

    </code></pre>

    </li>

    <li>

    <p>Usage:</p>

    <pre><code>  singularity exec biobb_cmip.sif &lt;command&gt;

    </code></pre>

    </li>

    </ul>

    <p>The command list and specification can be found at the <a href="https://biobb-cmip.readthedocs.io/en/latest/command_line.html"
    rel="nofollow">Command Line documentation</a>.</p>

    <h3>

    <a id="user-content-copyright--licensing" class="anchor" href="#copyright--licensing"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Copyright
    &amp; Licensing</h3>

    <p>This software has been developed in the <a href="http://mmb.irbbarcelona.org"
    rel="nofollow">MMB group</a> at the <a href="http://www.bsc.es/" rel="nofollow">BSC</a>
    &amp; <a href="https://www.irbbarcelona.org/" rel="nofollow">IRB</a> for the <a
    href="http://bioexcel.eu/" rel="nofollow">European BioExcel</a>, funded by the
    European Commission (EU H2020 <a href="http://cordis.europa.eu/projects/823830"
    rel="nofollow">823830</a>, EU H2020 <a href="http://cordis.europa.eu/projects/675728"
    rel="nofollow">675728</a>).</p>

    <ul>

    <li>(c) 2015-2021 <a href="https://www.bsc.es/" rel="nofollow">Barcelona Supercomputing
    Center</a>

    </li>

    <li>(c) 2015-2021 <a href="https://www.irbbarcelona.org/" rel="nofollow">Institute
    for Research in Biomedicine</a>

    </li>

    </ul>

    <p>Licensed under the

    <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache License
    2.0</a>, see the file LICENSE for details.</p>

    <p><a href="https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67"
    alt="" title="Bioexcel" data-canonical-src="https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 10
  topics: []
  updated_at: 1621526007.0
bioexcel/biobb_md:
  data_format: 2
  description: Biobb_md is the Biobb module collection to perform molecular dynamics
    simulations.
  filenames:
  - Singularity.latest
  full_name: bioexcel/biobb_md
  latest_release: null
  readme: '<p><a href="https://biobb-md.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img
    src="https://camo.githubusercontent.com/fb21acbebea7f37e1dbd5234470daa21656b5d221993e4eaafaf9c6452f316d6/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f62696f62622d6d642f62616467652f3f76657273696f6e3d6c6174657374"
    alt="" data-canonical-src="https://readthedocs.org/projects/biobb-md/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="https://anaconda.org/bioconda/biobb_md" rel="nofollow"><img src="https://camo.githubusercontent.com/a477fdb1fd9bc9eb7ffa6cae6a019c6d4c3902fd468b3126f1b78e56c7dcff83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e7376673f7374796c653d666c6174"
    alt="" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat"
    style="max-width:100%;"></a>

    <a href="https://quay.io/repository/biocontainers/biobb_md" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ca418e4db0b3de91a09a5df4a59446da015b6164598a8bc255918e911484f84f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d517561792e696f2d626c7565"
    alt="" data-canonical-src="https://img.shields.io/badge/docker-Quay.io-blue" style="max-width:100%;"></a>

    <a href="https://www.singularity-hub.org/collections/2735/usage" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667"
    alt="" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-biobb_md" class="anchor" href="#biobb_md" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>biobb_md</h1>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>Biobb_md is the Biobb module collection to perform molecular dynamics simulations.

    Biobb (BioExcel building blocks) packages are Python building blocks that

    create new layer of compatibility and interoperability over popular

    bioinformatics tools.

    The latest documentation of this package can be found in our readthedocs site:

    <a href="http://biobb-md.readthedocs.io/en/latest/" rel="nofollow">latest API
    documentation</a>.</p>

    <h3>

    <a id="user-content-version" class="anchor" href="#version" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version</h3>

    <p>v3.5.1 2020.4</p>

    <h3>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h3>

    <p>Using PIP:</p>

    <blockquote>

    <p><strong>Important:</strong> PIP only installs the package. All the dependencies
    must be installed separately. To perform a complete installation, please use ANACONDA,
    DOCKER or SINGULARITY.</p>

    </blockquote>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  pip install "biobb_md&gt;=3.5.1"

    </code></pre>

    </li>

    <li>

    <p>Usage: <a href="https://biobb-md.readthedocs.io/en/latest/modules.html" rel="nofollow">Python
    API documentation</a></p>

    </li>

    </ul>

    <p>Using ANACONDA:</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  conda install -c bioconda "biobb_md&gt;=3.5.1"

    </code></pre>

    </li>

    <li>

    <p>Usage: With conda installation BioBBs can be used with the <a href="https://biobb-md.readthedocs.io/en/latest/modules.html"
    rel="nofollow">Python API documentation</a> and the <a href="https://biobb-md.readthedocs.io/en/latest/command_line.html"
    rel="nofollow">Command Line documentation</a></p>

    </li>

    </ul>

    <p>Using DOCKER:</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  docker pull quay.io/biocontainers/biobb_md:3.5.1--py_0

    </code></pre>

    </li>

    <li>

    <p>Usage:</p>

    <pre><code>  docker run quay.io/biocontainers/biobb_md:3.5.1--py_0 &lt;command&gt;

    </code></pre>

    </li>

    </ul>

    <p>Using SINGULARITY:</p>

    <p><strong>MacOS users</strong>: it''s strongly recommended to avoid Singularity
    and use <strong>Docker</strong> as containerization system.</p>

    <ul>

    <li>

    <p>Installation:</p>

    <pre><code>  singularity pull --name biobb_md.sif shub://bioexcel/biobb_md

    </code></pre>

    </li>

    <li>

    <p>Usage:</p>

    <pre><code>  singularity exec biobb_md.sif &lt;command&gt;

    </code></pre>

    </li>

    </ul>

    <p>The command list and specification can be found at the <a href="https://biobb-md.readthedocs.io/en/latest/command_line.html"
    rel="nofollow">Command Line documentation</a>.</p>

    <h3>

    <a id="user-content-copyright--licensing" class="anchor" href="#copyright--licensing"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Copyright
    &amp; Licensing</h3>

    <p>This software has been developed in the <a href="http://mmb.irbbarcelona.org"
    rel="nofollow">MMB group</a> at the <a href="http://www.bsc.es/" rel="nofollow">BSC</a>
    &amp; <a href="https://www.irbbarcelona.org/" rel="nofollow">IRB</a> for the <a
    href="http://bioexcel.eu/" rel="nofollow">European BioExcel</a>, funded by the
    European Commission (EU H2020 <a href="http://cordis.europa.eu/projects/823830"
    rel="nofollow">823830</a>, EU H2020 <a href="http://cordis.europa.eu/projects/675728"
    rel="nofollow">675728</a>).</p>

    <ul>

    <li>(c) 2015-2021 <a href="https://www.bsc.es/" rel="nofollow">Barcelona Supercomputing
    Center</a>

    </li>

    <li>(c) 2015-2021 <a href="https://www.irbbarcelona.org/" rel="nofollow">Institute
    for Research in Biomedicine</a>

    </li>

    </ul>

    <p>Licensed under the

    <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache License
    2.0</a>, see the file LICENSE for details.</p>

    <p><a href="https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67"
    alt="" title="Bioexcel" data-canonical-src="https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 2
  subscribers_count: 5
  topics: []
  updated_at: 1620768602.0
bjhall/sarscov2-nf:
  data_format: 2
  description: null
  filenames:
  - container/Singularity
  full_name: bjhall/sarscov2-nf
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-elikopy\" class=\"anchor\" href=\"#elikopy\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>ElikoPy</h1>\n<p>ElikoPy is Python library aiming at easing the processing\
    \ of diffusion imaging for microstructural analysis.\nThis Python library is based\
    \ on</p>\n<ul>\n<li>DIPY, a python library for the analysis of MR diffusion imaging.</li>\n\
    <li>Microstructure fingerprinting, a python library doing estimation of white\
    \ matter microstructural properties from a dictionary of Monte Carlo diffusion\
    \ MRI fingerprints.</li>\n<li>FSL, a comprehensive library of analysis tools for\
    \ FMRI, MRI and DTI brain imaging data.</li>\n<li>DIAMOND, a c software that is\
    \ characterizing brain tissue by assessment of the distribution of anisotropic\
    \ microstructural environments in diffusion\u2010compartment imaging.</li>\n</ul>\n\
    <h3>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h3>\n<p>ElikoPy requires <a href=\"https://www.python.org/\"\
    \ rel=\"nofollow\">Python</a> v3.7+ to run.</p>\n<p>After cloning the repo, you\
    \ can either firstly install all the python dependencies including optionnal dependency\
    \ used to speed up the code:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ pip install -r requirements.txt --user</pre></div>\n<p>Or you can install\
    \ directly the library with only the mandatory dependencies (if you performed\
    \ the previous step, you still need to perform this step):</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>$ python3 setup.py install --user</pre></div>\n\
    <p>Microstructure Fingerprinting is currently not avaible in the standard python\
    \ repo, you can clone and install this library manually.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ git clone git@github.com:rensonnetg/microstructure_fingerprinting.git\n\
    $ <span class=\"pl-c1\">cd</span> microstructure_fingerprinting\n$ python setup.py\
    \ install</pre></div>\n<p>FSL also needs to be installed and availabe in our path\
    \ if you want to perform mouvement correction or tbss.</p>\n<p>Unfortunatly, the\
    \ DIAMOND code is not publically available. If you do not have it in your possesion,\
    \ you will not be able to use this algorithm. If you have it, simply add the executable\
    \ to your path.</p>\n<h3>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"\
    #usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h3>\n<p>Todo</p>\n<h3>\n<a id=\"user-content-development\"\
    \ class=\"anchor\" href=\"#development\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development</h3>\n<p>Want to\
    \ contribute? Great!</p>\n<p>Do not hesitate to open issue or pull request!</p>\n\
    <h3>\n<a id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Todos</h3>\n\
    <ul>\n<li>Release a complete and accurate documentation for the library</li>\n\
    </ul>\n<p><strong>Free Software, Hell Yeah!</strong></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619701441.0
bragalab/boldqc:
  data_format: 2
  description: null
  filenames:
  - boldqc2-master/Singularity
  full_name: bragalab/boldqc
  latest_release: null
  readme: "<p>After forking this repository, replace <code>orion_username</code> in\
    \ the file <code>2d_unet_CT_W_PET.json</code> with your actual orion username.</p>\n\
    <h1>\n<a id=\"user-content-setup-in-orion-cluster\" class=\"anchor\" href=\"#setup-in-orion-cluster\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Setup in Orion cluster</h1>\n<p>The <code>$HOME</code> directory of\
    \ your login machine (<code>[username@login ~]</code>) should have the following\
    \ structure</p>\n<pre><code>$HOME\n   \u251C\u2500\u2500 cnn_template (the forked\
    \ repository)\n   \u2502   \u251C\u2500\u2500 config\n   \u2502   |   \u251C\u2500\
    \u2500 2d_unet.json\n   \u2502   |   \u2514\u2500\u2500 (other configurations)\n\
    \   |   \u251C\u2500\u2500 outputs\n   \u2502   |   \u2514\u2500\u2500 README.md\n\
    \   \u2502   \u251C\u2500\u2500 customize_obj.py\n   \u2502   \u251C\u2500\u2500\
    \ experiment.py\n   \u2502   \u251C\u2500\u2500 slurm.sh\n   \u2502   \u251C\u2500\
    \u2500 setup.sh\n   \u2502   \u2514\u2500\u2500 (other files)\n   \u251C\u2500\
    \u2500 datasets (Put your datasets in here)\n   \u2502   \u2514\u2500\u2500 headneck\n\
    \   \u2502       \u251C\u2500\u2500 full_dataset_singleclass.h5\n   \u2502   \
    \    \u2514\u2500\u2500 (other datasets)\n   \u251C\u2500\u2500 hnperf (log files\
    \ will be saved in here)\n   \u2502\n</code></pre>\n<p>Start by running <code>setup.sh</code>\
    \ to download the singularity container</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> cnn-template\n./setup.sh</pre></div>\n<p>Alternative\
    \ you can directly download the image file</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> cnn-template\nsingularity pull --name deoxys.sif\
    \ shub://huynhngoc/head-neck-analysis</pre></div>\n<h1>\n<a id=\"user-content-run-experiments-on-orion\"\
    \ class=\"anchor\" href=\"#run-experiments-on-orion\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run experiments\
    \ on Orion</h1>\n<h2>\n<a id=\"user-content-submit-jobs\" class=\"anchor\" href=\"\
    #submit-jobs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Submit jobs</h2>\n<p>Submit slurm jobs like this:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>sbatch slurm.sh config/2d_unet.json\
    \ 2d_unet 200</pre></div>\n<p>Which will load the setup from the <code>config/2d_unet.json</code>\
    \ file, train for 200 epochs\nand store the results in the folder <code>$HOME/hnperf/2d_unet/</code>.</p>\n\
    <p>To customize model and prediction checkpoints, add the <code>model_checkpoint_period</code>\
    \ and <code>prediction_checkpoint_period</code> as arguments</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm.sh config/2d_unet_CT_W_PET.json\
    \ 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period\
    \ 5\n</pre></div>\n<p>Which will save the trained model every 5 epochs and predict\
    \ the validation set every 5 epoch</p>\n<h2>\n<a id=\"user-content-continue-experiments-and-run-test\"\
    \ class=\"anchor\" href=\"#continue-experiments-and-run-test\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Continue\
    \ experiments and run test</h2>\n<p>To continue an experiment</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_cont.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period\
    \ 5</pre></div>\n<p>Which will load the saved model and continue training 100\
    \ more epochs</p>\n<p>In the case the job ended unexpectedly before plotting the\
    \ performance:</p>\n<div class=\"highlight highlight-source-shell\"><pre>sbatch\
    \ slurm_vis.sh 2d_unet_CT_W_PET</pre></div>\n<p>To run test</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_test.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET</pre></div>\n<p>To run external validation</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_external.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET maastro.json</pre></div>\n<h1>\n<a id=\"user-content-misc\"\
    \ class=\"anchor\" href=\"#misc\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Misc</h1>\n<p>Manually build the singularity\
    \ image file</p>\n<pre><code>singularity build --fakeroot Singularity deoxys.sif\n\
    </code></pre>\n<p>Login to a gpu session to use the gpu</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>qlogin --partition=gpu --gres=gpu:1\nsingularity\
    \ <span class=\"pl-c1\">exec</span> --nv deoxys.sif ipython</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1620161860.0
bstriner/cuda-10.1-cudnn7-devel-ubuntu16.04:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: bstriner/cuda-10.1-cudnn7-devel-ubuntu16.04
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-101-cudnn7-devel-ubuntu1604" class="anchor" href="#cuda-101-cudnn7-devel-ubuntu1604"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cuda-10.1-cudnn7-devel-ubuntu16.04</h1>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1599516090.0
bunop/nextflow-training:
  data_format: 2
  description: nextflow training course at https://seqera.io/training/
  filenames:
  - singularity/Singularity
  full_name: bunop/nextflow-training
  latest_release: null
  readme: '<h1>

    <a id="user-content-nextflow-training" class="anchor" href="#nextflow-training"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>nextflow-training</h1>

    <h2>

    <a id="user-content-run-some-examples" class="anchor" href="#run-some-examples"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    some examples</h2>

    <pre><code>$ nextflow run combineInput.nf -resume -process.echo

    $ nextflow run inputRepeaters.nf -resume -process.echo

    </code></pre>

    <h2>

    <a id="user-content-how-to-use-containers-with-nextflow" class="anchor" href="#how-to-use-containers-with-nextflow"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How
    to use containers with nextflow</h2>

    <p>Create a file <code>nextflow.config</code> (mind <code>''''</code> for delimiting
    the paramters):</p>

    <pre><code>process.container=''nextflow/rnaseq-nf''

    docker.runOptions=''-u $(id -u):$(id -g)''

    </code></pre>

    <p>Pull a docker image:</p>

    <pre><code>$ docker pull nextflow/rnaseq-nf

    </code></pre>

    <p>Then call nextflow with <code>with-docker</code> parameter:</p>

    <pre><code>$ nextflow run script2.nf -with-docker

    </code></pre>

    <p>Adding:</p>

    <pre><code>docker.enabled=true

    </code></pre>

    <p>to <code>nextflow.config</code> let to avoid specifing <code>-with-docker</code>
    parameter when calling nextflow</p>

    <h2>

    <a id="user-content-override-parameters-using-cmd" class="anchor" href="#override-parameters-using-cmd"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Override
    parameters using cmd</h2>

    <p>Override the default <code>reads</code> parameter:</p>

    <pre><code>$ nextflow run script3.nf --reads ''data/ggal/*_{1,2}.fq'' -resume

    </code></pre>

    <p>Calling:</p>

    <pre><code>$ nextflow run script4.nf

    $ nextflow run script4.nf -resume --reads ''data/ggal/*_{1,2}.fq''

    </code></pre>

    <p>will execute only the new calculations and will re-use the old computed results</p>

    <h2>

    <a id="user-content-calling-a-nextflow-project-using-git" class="anchor" href="#calling-a-nextflow-project-using-git"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Calling
    a nextflow project using git</h2>

    <p>calling nextflow on this git repository: <a href="https://github.com/nextflow-io/rnaseq-nf.git">https://github.com/nextflow-io/rnaseq-nf.git</a></p>

    <pre><code>$ nextflow run nextflow-io/rnaseq-nf

    </code></pre>

    <p>I can call a specific revison of a nextflow git pipeline. Or by calling:</p>

    <pre><code>$ nextflow run rnaseq-nf -with-docker -with-report -with-trace -with-timeline
    -with-dag dag.png -resume

    </code></pre>

    <p>I can generate some useful reports</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606151572.0
buschlab/Egyptref:
  data_format: 2
  description: Scripts and pipelines used in the generation of "An Egyptian Genome
    Reference"
  filenames:
  - EGP_vep/Singularityfiles/Singularity.99-GRCh38-merged
  - EGP_vep/Singularityfiles/Singularity.99-GRCh37-merged
  full_name: buschlab/Egyptref
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-egyptref\" class=\"anchor\" href=\"#egyptref\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Egyptref</h1>\n<p>Scripts and pipelines used in the generation of\
    \ \"An Egyptian Genome Reference\"</p>\n<p>Forked archives and scripts of collaborators\
    \ on Egypt reference genome project.</p>\n<ol>\n<li>EGP_admixture  - contribution\
    \ from Michael Olbrich (<a href=\"https://github.com/ScienceOlbrich\">https://github.com/ScienceOlbrich</a>)</li>\n\
    <li>EGP_delly      - contribution from Axel K\xFCnstner (<a href=\"https://github.com/kunstner\"\
    >https://github.com/kunstner</a>)</li>\n<li>EGP_gwas       - contribution from\
    \ Matthias Munz (<a href=\"https://github.com/matmu\">https://github.com/matmu</a>)</li>\n\
    <li>EGP_small_variant_calling  - contribution from Matthias Munz</li>\n<li>EGP_sv_merging\
    \  - contribution from Matthias Munz</li>\n<li>EGP_unique_insertions  - contribution\
    \ from Matthias Munz</li>\n<li>EGP_vep        - contribution from Matthias Munz\
    \ (<a href=\"https://github.com/matmu\">https://github.com/matmu</a>)</li>\n</ol>\n"
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1617920957.0
c3se/containers:
  data_format: 2
  description: 'The main platform for sharing all the information about AI/HPC-related
    containers at C3SE. See also the included tutorial to get started. '
  filenames:
  - Julia/Singularity.Julia-v1.4.0
  - ambertools/Singularity.ambertools-v20
  - FEniCS/Singularity.FEniCS
  - PyTorch/Singularity.PyTorch-v1.5.0-py3
  - PyTorch/Singularity.PyTorch-v1.6.0-py3
  - PyTorch/Singularity.PyTorch-v1.7.0-py3
  - TensorFlow/Singularity.TensorFlow-v2.2.0-tf2-py3-NGC-R20.08
  - TensorFlow/Singularity.TensorFlow-v2.3.1-tf2-py3-GPU-Jupyter
  - TensorFlow/Singularity.TensorFlow-v2.1.0-tf2-py3-NGC-R20.03
  full_name: c3se/containers
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4791" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-using-container-based-solutions-on-c3se-clusters" class="anchor"
    href="#using-container-based-solutions-on-c3se-clusters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using container-based
    solutions on C3SE clusters</h1>

    <p>Container technology has a number of advantages over the traditional workflow
    of using scientific software. The singularity flavour, in particular, targets
    reproducibility, performance and security with respect to running software in
    an HPC environment. Here, we provide our containerized solutions at C3SE. For
    each of the provided containers, read the specific instructions in the corresponding
    folder to easily get started with using them in your workflow. The actual container
    images are hosted on Singularity Hub. Click on the badge above to quickly get
    access to them!</p>

    <h2>

    <a id="user-content-missing-containers-updates-and-troubleshooting" class="anchor"
    href="#missing-containers-updates-and-troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Missing containers,
    updates, and troubleshooting</h2>

    <p>We continuously add more packages to this repository. If you can''t find a
    relevant container for your needs, or in the case of encountering any errors or
    deprecated features in the material, feel free to contact us: <a href="mailto:support@c3se.chalmers.se">support@c3se.chalmers.se</a>
    or open a pull-request.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - singularity-containers
  - docker
  - hpc
  updated_at: 1605120969.0
canlab/cantainer:
  data_format: 2
  description: Containerizing the Canlab code
  filenames:
  - Singularity
  full_name: canlab/cantainer
  latest_release: null
  readme: '<h1>

    <a id="user-content-h3abioneth3arefgraph" class="anchor" href="#h3abioneth3arefgraph"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>h3abionet/h3arefgraph</h1>

    <p><strong>RefGraph Workflows Hackathon</strong></p>

    <p><a href="https://travis-ci.org/h3abionet/h3arefgraph" rel="nofollow"><img src="https://camo.githubusercontent.com/1ac1f732e5b8f802a198a533b88e24608b4b10e38d8744373f7bcb5284832ca8/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f68336172656667726170682e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/h3abionet/h3arefgraph.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/h3arefgraph" rel="nofollow"><img src="https://camo.githubusercontent.com/910d36cd9eef9bfef42722b54a531cf2c72b7ed04f37e85d72b93d50b7a3e0c1/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f68336172656667726170682e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/h3arefgraph.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>This pipeline is for the use and testing of graph based methods for variant
    calling.</p>

    <p>The aim is to allow the user to choose the reference graph construction method
    and the alignment / variant calling methods separately.</p>

    <p>We also provide tools for reporting the results of the variant calling, that
    take advantage of the additional contextual information that using reference graphs
    provides.</p>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h3>

    <p>The aim of this project is to separate the different parts of the variant calling
    process to allow the development of

    task specific tools. This is more in line with traditional variant calling where
    specific alignment tools may preform

    better for different organisms, but should not require a different downstream
    analysis for each output.</p>

    <p><a href="assets/images/Overview_slide.jpeg" target="_blank" rel="noopener noreferrer"><img
    src="assets/images/Overview_slide.jpeg" alt="Overview slide" style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The h3abionet/h3arefgraph pipeline comes with documentation about the pipeline,
    found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    <li><a href="docs/configuration/reference_genomes.md">Reference genomes</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>


    <h3>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>

    <p>h3abionet/h3arefgraph was originally written by the H3ABioNet RefGraph Team.</p>

    '
  stargazers_count: 3
  subscribers_count: 10
  topics: []
  updated_at: 1525841449.0
ccmvumc/FS6:
  data_format: 2
  description: FreeSurfer 6 Pipeline
  filenames:
  - Singularity
  - Singularity.v1.2.1
  - Singularity.v1.1.0
  - Singularity.v1.2.3
  - Singularity.v1.0.0
  - Singularity.v1.2.0
  full_name: ccmvumc/FS6
  latest_release: v1.2.3
  readme: '<h1>

    <a id="user-content-fs6" class="anchor" href="#fs6" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>FS6</h1>

    <p>FreeSurfer 6 Pipeline</p>

    <p><a href="https://singularity-hub.org/collections/1120" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621039187.0
ccmvumc/TRACULA:
  data_format: 2
  description: TRACULA Pipeline
  filenames:
  - Singularity
  - Singularity.v2.1.1
  - Singularity.v2.0.0
  full_name: ccmvumc/TRACULA
  latest_release: v2.1.1
  readme: '<h1>

    <a id="user-content-tracula" class="anchor" href="#tracula" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TRACULA</h1>

    <p>TRACULA Pipeline</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621037592.0
cfe-lab/proviral:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: cfe-lab/proviral
  latest_release: v2.3.4
  readme: '<h2>

    <a id="user-content-readme" class="anchor" href="#readme" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Readme</h2>

    <h3>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h3>

    <ol>

    <li>minimap2 (<a href="https://github.com/lh3/minimap2">https://github.com/lh3/minimap2</a>)
    (must be available via commandline)</li>

    <li>blast tools (<a href="ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/"
    rel="nofollow">ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/</a>)</li>

    <li>R and RSCRIPT (<a href="https://www.r-project.org/" rel="nofollow">https://www.r-project.org/</a>)</li>

    </ol>

    <h3>

    <a id="user-content-singularity-builds" class="anchor" href="#singularity-builds"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    builds</h3>

    <ul>

    <li>Build all singularity images inside of the <code>simages</code> folder</li>

    </ul>

    <h3>

    <a id="user-content-filtering" class="anchor" href="#filtering" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Filtering</h3>

    <ul>

    <li>At the core of the proviral pipeline, data is read from <code>contigs.csv</code>
    and <code>conseqs.csv</code> files produced by MiCall</li>

    <li>First the pipeline reads through all of the contigs, then the contigs</li>

    <li>When it does this (see the <code>find_primers()</code> function) it applies
    the following logic in this order for filtering/tagging:

    <ol>

    <li>If a sample is not proviral, skip it. Do not attempt to find primers or anything,
    just log a message saying <code>sample X was skipped because it was non-proviral</code>

    </li>

    <li>If a sample has 0 in the remap column of the <code>cascade.csv</code> file,
    tag that sequence with an error: <code>No contig/conseq constructed</code>, do
    not analyze it or try to find primers, and write it to the <code>*primer_analysis.csv</code>
    file (which records all failures)</li>

    <li>If the <code>consensus-percent-cutoff</code> is NOT <code>MAX</code>, tag
    it with an error: <code>contig not MAX</code> and skip the sequence (do not try
    to find primers)</li>

    <li>If the reference of the sample is <code>HIV1-CON-XX-Consensus-seed</code>
    tag that sequence with an error: <code>is V3 sequence</code>, skip the sequence
    (do not try to find primers), and write it to the <code>*primer_analysis.csv</code>
    file</li>

    <li>If there is an <code>X</code> in the middle of the sequence, tag that sequence
    with an error: <code>low internal read coverage</code>, skip the sequence (do
    not try to find primers), and write it to the <code>*primer_analysis.csv</code>
    file</li>

    <li>If there are ANY non-TCGA characters in the sequence, tag that sequence with
    an error: <code>contig sequence contained non-TCGA/gap</code>, skip the sequence
    (do not try to find primers), and write it to the <code>*primer_analysis.csv</code>
    file</li>

    <li>For each end (5'' (fwd), 3'' (rev)) of the sequence:

    <ol>

    <li>If there are <code>X</code> characters found, try to remove them (if they
    are clustered) and if not possible to remove tag the fwd/rev end with a fwd/rev
    primer error: <code>low read coverage in primer region</code>, skip the fwd/rev
    end (do not try to find primers)</li>

    <li>If fwd/rev end has zero nucleotides found for primer, tag the fwd/rev end
    with a fwd/rev primer error: <code>primer was not found</code>, skip to the next
    end if any</li>

    <li>If the fwd/rev primer is deemed not valid, tag the fwd/rev end with a fwd/rev
    primer error: <code>primer failed secondary validation</code>, skip to the next
    end if any</li>

    </ol>

    </li>

    <li>Write the sequence to the <code>*primer_analysis.csv</code> file regardless
    of tagged errors in any error column</li>

    <li>Load the <code>*primer_analysis.csv</code> files for both contigs and conseqs
    and for both of them apply the following filters in order:

    <ol>

    <li>Remove all rows where either the <code>error</code>, <code>fwd_error</code>,
    or <code>rev_error</code> is tagged</li>

    <li>Remove the primers from the sequences (for hivseqinr)</li>

    <li>Remove rows where sample name appears twice (duplicates)</li>

    <li>Remove rows where the reference contains <code>unknown</code> or <code>reverse</code>

    </li>

    </ol>

    </li>

    <li>Finally merge the filtered contigs and conseqs and write the final <code>*filtered.csv</code>
    file with conseqs taking precedence over contigs</li>

    </ol>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1621296936.0
cfusting/conditional-growth:
  data_format: 2
  description: Grow virtual creatures in static and physics simulated environments.
  filenames:
  - cluster/Singularity
  full_name: cfusting/conditional-growth
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-growing-virtual-creatures\" class=\"anchor\"\
    \ href=\"#growing-virtual-creatures\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Growing Virtual Creatures</h1>\n\
    <p><a href=\"https://travis-ci.com/cfusting/conditional-growth\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/5d1f5f4e97ec14d197ffc19e3f24ec51393310f3052d9db011fade1eb7a0a581/68747470733a2f2f7472617669732d63692e636f6d2f6366757374696e672f636f6e646974696f6e616c2d67726f7774682e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/cfusting/conditional-growth.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-about\" class=\"\
    anchor\" href=\"#about\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>About</h2>\n<p>This package provides the necessary\
    \ tooling to grow virtual creatures made from three-dimensional blocks called\
    \ voxels (3d pixels). Starting with one or more voxels new voxels are iteratively\
    \ added based on the composition of nearby voxels and the current position. Environments\
    \ exist for gridworld and <a href=\"https://github.com/voxcraft/voxcraft-sim\"\
    >voxcraft-sim</a>. Gridworld has no physics engine and is thus extremely fast\
    \ to run.</p>\n<p><br><br></p>\n<h2>\n<a id=\"user-content-building-with-docker\"\
    \ class=\"anchor\" href=\"#building-with-docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building with Docker</h2>\n<h3>\n\
    <a id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h3>\n\
    <p>If you would like to use a GPU make sure to install <a href=\"https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime\"\
    \ rel=\"nofollow\">nvidia-container-runtime</a>. Other than that the Dockerfile\
    \ will handle all the dependencies.</p>\n<h3>\n<a id=\"user-content-installing-nvidia-container-runtime\"\
    \ class=\"anchor\" href=\"#installing-nvidia-container-runtime\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ Nvidia container runtime</h3>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>distribution=<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>. /etc/os-release<span\
    \ class=\"pl-k\">;</span><span class=\"pl-c1\">echo</span> <span class=\"pl-smi\"\
    >$ID$VERSION_ID</span><span class=\"pl-pds\">)</span></span> \\\n   <span class=\"\
    pl-k\">&amp;&amp;</span> curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey\
    \ <span class=\"pl-k\">|</span> sudo apt-key add - \\\n   <span class=\"pl-k\"\
    >&amp;&amp;</span> curl -s -L https://nvidia.github.io/nvidia-docker/<span class=\"\
    pl-smi\">$distribution</span>/nvidia-docker.list <span class=\"pl-k\">|</span>\
    \ sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n   \nsudo apt-get update\
    \ <span class=\"pl-k\">&amp;&amp;</span> sudo apt-get install -y nvidia-docker2\n\
    sudo systemctl restart docker</pre></div>\n<h3>\n<a id=\"user-content-build\"\
    \ class=\"anchor\" href=\"#build\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Build</h3>\n<p>Clone this repository\
    \ and navigate into the root folder. Build the Dockerfile and tag it \"grow\"\
    .</p>\n<div class=\"highlight highlight-source-shell\"><pre>docker build -t grow\
    \ <span class=\"pl-c1\">.</span></pre></div>\n<p><br><br></p>\n<h2>\n<a id=\"\
    user-content-example-optimizing-a-creature-in-gridworld\" class=\"anchor\" href=\"\
    #example-optimizing-a-creature-in-gridworld\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Example: Optimizing a Creature\
    \ in gridworld</h2>\n<p>In this example we will build a creature for which surface\
    \ area is maximized and volume is minimized.</p>\n<h3>\n<a id=\"user-content-run\"\
    \ class=\"anchor\" href=\"#run\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Run</h3>\n<p>Run the optimization\
    \ script, storing the results in the host environment's /tmp directory.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>docker run --rm --gpus all\
    \ -v /tmp:/home/ray/ray_results --shm-size 2G grow python examples/optimize_grid.py</pre></div>\n\
    <h3>\n<a id=\"user-content-metrics\" class=\"anchor\" href=\"#metrics\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Metrics</h3>\n\
    <p>Metrics are captured by the <a href=\"https://docs.ray.io/en/master/\" rel=\"\
    nofollow\">ray</a> framework in /tmp/expname where expname is specified in the\
    \ optimize_grid.py script. The easiest way to view the metrics is to use tensorboard.\
    \ For example:</p>\n<div class=\"highlight highlight-source-shell\"><pre>docker\
    \ run -p 6006:6006 --rm -v /tmp:/tmp tensorflow/tensorflow tensorboard --logdir\
    \ /tmp/max_surface_area --host 0.0.0.0 --port 6000</pre></div>\n<p><a href=\"\
    ./docs/tensorboard.png\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"\
    ./docs/tensorboard.png\" alt=\"tensorboard\" style=\"max-width:100%;\"></a></p>\n\
    <h3>\n<a id=\"user-content-viewing-the-creatures\" class=\"anchor\" href=\"#viewing-the-creatures\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Viewing the Creatures</h3>\n<p>Uncommenting <code>monitor=True</code>\
    \ in optimize_grid.py will enable the recording of a creature being built. The\
    \ resulting movies can be found in /tmp/ray_results/expname/trialname (as can\
    \ all the other logs). Refer to <a href=\"https://docs.ray.io/en/master/rllib.html\"\
    \ rel=\"nofollow\">RLlib</a> for more details.</p>\n<p>Due to a memory leak in\
    \ vtk (which is the graphics library used to create the movies), enabling monitoring\
    \ will eventually cause the trial to crash. To avoid this run your experiment\
    \ until convergence and turn on monitoring after loading a checkpoint to capture\
    \ a few movies at that point in training.</p>\n<p>Below are some videos of this\
    \ example mid-way through training and at convergence. At convergence the growth\
    \ function builds a pillar. This is the optimal creature given the space is unconstrained\
    \ and voxels must connect.</p>\n<p><a href=\"./docs/midway.gif\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"./docs/midway.gif\" alt=\"midway\" style=\"\
    max-width:100%;\"></a>\n<a href=\"./docs/column.gif\" target=\"_blank\" rel=\"\
    noopener noreferrer\"><img src=\"./docs/column.gif\" alt=\"column\" style=\"max-width:100%;\"\
    ></a></p>\n<p><br><br></p>\n<h2>\n<a id=\"user-content-theory\" class=\"anchor\"\
    \ href=\"#theory\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Theory</h2>\n<p><a href=\"./docs/theory1.jpg\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"./docs/theory1.jpg\" alt=\"theory1\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"./docs/theory2.jpg\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"./docs/theory2.jpg\" alt=\"theory2\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 8
  subscribers_count: 1
  topics:
  - virtual-creatures
  - reinforcement-learning
  - reinforcement-learning-environments
  updated_at: 1621465175.0
challenge-engine/test-starting-kit:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: challenge-engine/test-starting-kit
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"\
    #test-starting-kit\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>test-starting-kit</h1>\n<p><g-emoji class=\"\
    g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\
    >\U0001F913</g-emoji></p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1620159134.0
chidiugonna/DTI-hippo:
  data_format: 2
  description: Tractography-inspired targetting for Hippocampal TMS
  filenames:
  - singularity/mrtrix3-connectome-custom/Singularity
  - singularity/mrtrix3-connectome-custom/src/Singularity
  full_name: chidiugonna/DTI-hippo
  latest_release: null
  readme: '<h1>

    <a id="user-content-dti-hippo" class="anchor" href="#dti-hippo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DTI-hippo</h1>

    <p>Tractography-inspired targetting for hippocampal TMS</p>

    <h2>

    <a id="user-content-references" class="anchor" href="#references" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

    <p>Smith, R. E.; Tournier, J.-D.; Calamante, F. &amp; Connelly, A. <a href="https://pubmed.ncbi.nlm.nih.gov/22705374/"
    rel="nofollow">Anatomically-constrained tractography: Improved diffusion MRI streamlines
    tractography through effective use of anatomical information.</a> NeuroImage,
    2012, 62, 1924-1938</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1617884735.0
churchill-lab/scRATE:
  data_format: 2
  description: Bayesian model selection to detect zero-inflated genes
  filenames:
  - Singularity
  full_name: churchill-lab/scRATE
  latest_release: null
  readme: "<p><a href=\"https://singularity-hub.org/collections/4398\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/kbchoi/scrate\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/803db9bd957916e2b4e1dc773894c7890a271e4dd3dec7137ec4b9ac01351461/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f636c6f75642f6175746f6d617465642f6b6263686f692f736372617465\"\
    \ alt=\"Docker Cloud Automated build\" data-canonical-src=\"https://img.shields.io/docker/cloud/automated/kbchoi/scrate\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-scrate\" class=\"\
    anchor\" href=\"#scrate\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>scRATE</h1>\n<p>scRATE is a model selection-based\
    \ scRNA-seq quantitation algorithm that controls overfitting and unwarranted imputation\
    \ of technical zeros. We first fit UMI counts with Poisson (P), Negative-Binomial\
    \ (NB), Zero-inflated Poisson (ZIP), and Zero-inflated Negative Binomial models\
    \ (ZINB). These models are applicable to UMI counts directly and therefore no\
    \ need of arbitrary preprocessing of counts, e.g., normalization (by scaling to,\
    \ for example, CPM) or log-transformation (with pseudocounts). Our model comparison\
    \ enables us to compute denoised rates of gene expression using the best model\
    \ which each gene data is conforming to.</p>\n<ul>\n<li>Free software: GPLv3 license</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>Installation of scRATE is simple, although it\
    \ may take a while as it has to compile rstan, the related packages, and all their\
    \ dependencies.</p>\n<div class=\"highlight highlight-source-r\"><pre><span class=\"\
    pl-k\">&gt;</span> <span class=\"pl-e\">devtools</span><span class=\"pl-k\">::</span>install_github(<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span>churchill-lab/scRATE<span class=\"\
    pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;</span> library(<span class=\"\
    pl-smi\">scRATE</span>)\n<span class=\"pl-k\">&gt;</span> version()</pre></div>\n\
    <pre><code>____ ____ ____ ____ ___ ____\n[__  |    |__/ |__|  |  |___\n___] |___\
    \ |  \\ |  |  |  |___\n                   Ver:0.1.2\n</code></pre>\n<p>This might\
    \ hick up because of hdf5r or loomR. Then, try the following.</p>\n<div class=\"\
    highlight highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span> install.packages(<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span>hdf5r<span class=\"pl-pds\">'</span></span>)\n\
    <span class=\"pl-k\">&gt;</span> <span class=\"pl-e\">devtools</span><span class=\"\
    pl-k\">::</span>install_github(<span class=\"pl-v\">repo</span> <span class=\"\
    pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mojaveazure/loomR<span\
    \ class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">ref</span> <span class=\"\
    pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>develop<span\
    \ class=\"pl-pds\">\"</span></span>)</pre></div>\n<h2>\n<a id=\"user-content-quick-start\"\
    \ class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Quick Start</h2>\n<p>Here is\
    \ a quick example on how to use it. We first load <code>scRATE</code> and <code>loomR</code>\
    \ package. We use loom format files to store single-cell gene expression data\
    \ as it is supported in both <code>R</code> and <code>python</code>. In <code>scRATE</code>,\
    \ we also provide handy features that facilitate the handling of model selection\
    \ results with loom format file.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-k\">&gt;</span> library(<span class=\"pl-smi\">scRATE</span>)\n\
    <span class=\"pl-k\">&gt;</span> library(<span class=\"pl-smi\">loomR</span>)</pre></div>\n\
    <p>A data file in the example is available at <a href=\"ftp://churchill-lab.jax.org/analysis/scRATE/DC-like_cells.loom\"\
    \ rel=\"nofollow\">ftp://churchill-lab.jax.org/analysis/scRATE/DC-like_cells.loom</a>.</p>\n\
    <div class=\"highlight highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span>\
    \ <span class=\"pl-smi\">ds</span> <span class=\"pl-k\">&lt;-</span> connect(<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">'</span>DC-like_cells.loom<span class=\"\
    pl-pds\">'</span></span>)\n<span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\"\
    >cntmat</span> <span class=\"pl-k\">&lt;-</span> t(<span class=\"pl-smi\">ds</span><span\
    \ class=\"pl-k\">$</span><span class=\"pl-smi\">matrix</span>[,])\n<span class=\"\
    pl-k\">&gt;</span> <span class=\"pl-smi\">gsymb</span> <span class=\"pl-k\">&lt;-</span>\
    \ <span class=\"pl-smi\">ds</span><span class=\"pl-k\">$</span><span class=\"\
    pl-smi\">row.attrs</span><span class=\"pl-k\">$</span><span class=\"pl-smi\">GeneID</span>[]\n\
    <span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">ds</span><span class=\"\
    pl-k\">$</span>close_all()\n<span class=\"pl-k\">&gt;</span> head(<span class=\"\
    pl-smi\">gsymb</span>)</pre></div>\n<pre><code>[1] \"Xkr4\"    \"Gm1992\"  \"\
    Gm37381\" \"Rp1\"     \"Rp1.1\"   \"Sox17\"\n</code></pre>\n<p>We recommend using\
    \ offset (or exposure) in order to reflect the difference in depth of coverage\
    \ across cells while fitting the count models. The models we compare use log link\
    \ function, and therefore, the offsets should be log transformed too.</p>\n<div\
    \ class=\"highlight highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span>\
    \ <span class=\"pl-smi\">exposure</span> <span class=\"pl-k\">&lt;-</span> log(colSums(<span\
    \ class=\"pl-smi\">cntmat</span>))\n<span class=\"pl-k\">&gt;</span> head(<span\
    \ class=\"pl-smi\">exposure</span>)</pre></div>\n<pre><code>  [1] 8.815518 8.996157\
    \ 9.025816 9.037771 9.062420 9.397732\n</code></pre>\n<p>We will pick a gene,\
    \ <em>Cybb</em> (one that we know that it fits to ZINB model significantly better\
    \ than the other models), and load its UMI counts into a data frame.</p>\n<div\
    \ class=\"highlight highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span>\
    \ <span class=\"pl-smi\">gg</span> <span class=\"pl-k\">&lt;-</span> <span class=\"\
    pl-c1\">4153</span>\n<span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">gsymb</span>[<span\
    \ class=\"pl-smi\">gg</span>]</pre></div>\n<pre><code>[1] \"Cybb\"\n</code></pre>\n\
    <div class=\"highlight highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span>\
    \ <span class=\"pl-smi\">y</span> <span class=\"pl-k\">&lt;-</span> <span class=\"\
    pl-smi\">cntmat</span>[<span class=\"pl-smi\">gg</span>,]\n<span class=\"pl-k\"\
    >&gt;</span> <span class=\"pl-smi\">y</span></pre></div>\n<pre><code>  [1]  1\
    \  0  8  7  3  6  7  1  4  0  0  2  2 10  0  3  1  1  2  4  6  3  9  3  1\n [26]\
    \  4  1  2  3  8  0  0  1  6  2  6  3  0  0  1  0  4  1  3  0  2 17  1  2  5\n\
    \ [51]  0  1  1  0  6  3  4  4  3  1  3  1  7  8  0  2  4  6  2  7  0  4  0  1\
    \  0\n [76]  2  0  8  2  0  1  4  9  3  0  2  2  3  0  3  0  0  4  1  4  3  2\
    \  3  2  2\n[101]  8  0  5  8  2  3  0  3  0  3  6  0  4  2 11  5  1  2 12  2\
    \  4  2  6  8  6\n[126]  3  3  1  0  2  1  1 10  2  2  0  2  2  1  0  1  0  3\
    \ 11  0  7  2  2  2  3\n[151]  7  0  1 10  4  1  4  5  1  0  1  3  3  0  5  4\
    \  9  4  2  0  4  1  2  4  1\n[176]  2  1  4  2  1  0  1  0  1  4  6  8  9  4\
    \  3\n</code></pre>\n<div class=\"highlight highlight-source-r\"><pre><span class=\"\
    pl-k\">&gt;</span> <span class=\"pl-smi\">gexpr</span> <span class=\"pl-k\">&lt;-</span>\
    \ <span class=\"pl-k\">data.frame</span>(<span class=\"pl-v\">y</span><span class=\"\
    pl-k\">=</span><span class=\"pl-smi\">y</span>, <span class=\"pl-v\">exposure</span><span\
    \ class=\"pl-k\">=</span><span class=\"pl-smi\">exposure</span>)</pre></div>\n\
    <p>We must have <code>y</code> and <code>exposure</code> as variables. We can\
    \ also append covariates to the data frame.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">gexpr</span> <span\
    \ class=\"pl-k\">&lt;-</span> <span class=\"pl-k\">data.frame</span>(<span class=\"\
    pl-v\">y</span><span class=\"pl-k\">=</span><span class=\"pl-smi\">y</span>, <span\
    \ class=\"pl-v\">exposure</span><span class=\"pl-k\">=</span><span class=\"pl-smi\"\
    >exposure</span>, <span class=\"pl-smi\">celltype</span>, <span class=\"pl-smi\"\
    >sex</span>)</pre></div>\n<p>We are now ready to fit the models.</p>\n<div class=\"\
    highlight highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span> <span class=\"\
    pl-smi\">model_fit</span> <span class=\"pl-k\">&lt;-</span> fit_count_models(<span\
    \ class=\"pl-smi\">gexpr</span>)</pre></div>\n<p>Or we can specify our own model\
    \ formula (See brms documentation) if there are covariates. Note we should not\
    \ use <code>offset()</code> function in the formula.</p>\n<div class=\"highlight\
    \ highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\"\
    >model_fit</span> <span class=\"pl-k\">&lt;-</span> fit_count_models(<span class=\"\
    pl-smi\">gexpr</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>y ~\
    \ 1 + (1|sex) + (1|celltype)<span class=\"pl-pds\">'</span></span>)</pre></div>\n\
    <pre><code>Fitting models for Cybb\nFitting data with Poisson model...\nFitting\
    \ data with Negative Binomial model...\nFitting data with Zero-Inflated Poisson\
    \ model...\nCompiling the C++ model\n\nStart sampling\nSAMPLING FOR MODEL 'zip'\
    \ NOW (CHAIN 1).\n\nChain 1:\nChain 1: Gradient evaluation took 0.000177 seconds\n\
    Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.77\
    \ seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1:\nChain 1:\n\
    ...\n\nFitting data with Zero-Inflated Negative Binomial model...\nCompiling the\
    \ C++ model\nStart sampling\n\nSAMPLING FOR MODEL 'zinb' NOW (CHAIN 1).\nChain\
    \ 1:\nChain 1: Gradient evaluation took 0.000329 seconds\nChain 1: 1000 transitions\
    \ using 10 leapfrog steps per transition would take 3.29 seconds.\nChain 1: Adjust\
    \ your expectations accordingly!\nChain 1:\nChain 1:\n...\n</code></pre>\n<p>Then\
    \ we compare the models with the leave-one-out cross validation test, and select\
    \ the model that best fits the data.</p>\n<div class=\"highlight highlight-source-r\"\
    ><pre><span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">elpd_loo</span>\
    \ <span class=\"pl-k\">&lt;-</span> compare_count_models(<span class=\"pl-smi\"\
    >model_fit</span>)\n<span class=\"pl-k\">&gt;</span> <span class=\"pl-smi\">elpd_loo</span></pre></div>\n\
    <pre><code>     elpd_diff se_diff\nZINB    0.0       0.0\nNB     -3.0       2.7\n\
    ZIP   -35.6       9.5\nP    -103.9      19.6\n</code></pre>\n<div class=\"highlight\
    \ highlight-source-r\"><pre><span class=\"pl-k\">&gt;</span> select_model(<span\
    \ class=\"pl-smi\">elpd_loo</span>, <span class=\"pl-v\">margin</span><span class=\"\
    pl-k\">=</span><span class=\"pl-c1\">1</span>)\n[<span class=\"pl-c1\">1</span>]\
    \ <span class=\"pl-c1\">4</span></pre></div>\n<h2>\n<a id=\"user-content-how-to-cite\"\
    \ class=\"anchor\" href=\"#how-to-cite\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>How to cite</h2>\n<p>K. Choi,\
    \ Y. Chen, D.A. Skelly, G.A. Churchill. \u201CBayesian model selection reveals\
    \ biological origins of zero inflation in single-cell transcriptomics.\u201D Genome\
    \ Biology, 21, 183 (2020). <a href=\"https://doi.org/10.1186/s13059-020-02103-2\"\
    \ rel=\"nofollow\">https://doi.org/10.1186/s13059-020-02103-2</a></p>\n"
  stargazers_count: 8
  subscribers_count: 4
  topics: []
  updated_at: 1614372806.0
clemsonciti/singularity-images:
  data_format: 2
  description: Scripts for building Singularity images
  filenames:
  - tensorflow/ubuntu.def
  - caffe/ubuntu.def
  - mxnet/ubuntu.def
  - caffe2/ubuntu.def
  - circuitscape/ubuntu.def
  - dl/ubuntu.def
  full_name: clemsonciti/singularity-images
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-image-scripts" class="anchor" href="#singularity-image-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    image scripts</h1>

    <p>Scripts to generate singularity images

    for running different software on Palmetto cluster.</p>

    '
  stargazers_count: 10
  subscribers_count: 5
  topics: []
  updated_at: 1597407988.0
cokelaer/damona:
  data_format: 2
  description: 'singularity environment manager for NGS pipelines '
  filenames:
  - damona/recipes/fitter/Singularity.fitter_1.3.0
  - damona/recipes/phantompeakqualtools/Singularity.phantompeakqualtools_1.2.2
  - damona/recipes/salmon/Singularity.salmon_1.3.0
  - damona/recipes/minimap2/Singularity.minimap2_2.17.0
  - damona/recipes/R/Singularity.R_4.0.2
  - damona/recipes/R/Singularity.R_3.6.3
  - damona/recipes/sequana_perl_tools/Singularity.sequana_perl_tools_0.1.0
  - damona/recipes/rnaseqc/Singularity.rnaseqc_2.35.0
  - damona/recipes/ucsc/Singularity.ucsc_0.1.0
  - damona/recipes/damona/Singularity.damona_0.4.2
  - damona/recipes/damona/Singularity.damona_0.3.0
  - damona/recipes/rtools/Singularity.Rtools_1.1.0
  - damona/recipes/rtools/Singularity.Rtools_1.0.0
  - damona/recipes/kraken/Singularity.kraken_1.1
  - damona/recipes/kraken/Singularity.kraken_2.0.9
  - damona/recipes/gffread/Singularity.gffread_0.12.1
  - damona/recipes/canu/Singularity.canu_1.8.0
  - damona/recipes/canu/Singularity.canu_1.6.0
  - damona/recipes/fastqc/Singularity.fastqc_0.11.8
  - damona/recipes/fastqc/Singularity.fastqc_0.11.9
  - damona/recipes/rnadiff/Singularity.rnadiff_1.7.1
  - damona/recipes/prokka/Singularity.prokka_1.14.5
  - damona/recipes/art/Singularity.art_3.11.14
  - damona/recipes/trf/Singularity.trf_4.09
  - damona/recipes/trf/Singularity.trf_4.10.0
  - damona/recipes/falco/Singularity.falco_0.2.1
  - damona/recipes/conda/Singularity.conda_4.7.12
  - damona/recipes/conda/Singularity.conda_4.9.2
  - damona/recipes/bcl2fastq/Singularity.bcl2fastq_2.20.0
  - damona/recipes/graphviz/Singularity.graphviz_2.43.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.9.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.11.0
  - damona/recipes/sequana_tools/Singularity.sequana_tools_0.10.0
  full_name: cokelaer/damona
  latest_release: v0.4.3
  readme: "<h1>\n<a id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"\
    #test-starting-kit\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>test-starting-kit</h1>\n<p><g-emoji class=\"\
    g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\
    >\U0001F913</g-emoji></p>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1620073798.0
csorianot/snATACseq-NextFlow:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: csorianot/snATACseq-NextFlow
  latest_release: null
  readme: '<h1>

    <a id="user-content-nextflow-pipeline-for-10x-snatac-seq-data" class="anchor"
    href="#nextflow-pipeline-for-10x-snatac-seq-data" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>NextFlow pipeline for 10X snATAC-seq data</h1>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <p>If you have Singularity installed, you can use the config provided here (''Singularity'')
    to build a container with all the dependencies.</p>

    <p>Otherwise, you''ll need to have the following installed:</p>

    <ol>

    <li>biopython</li>

    <li>bwa</li>

    <li>picardtools</li>

    <li>fastqc</li>

    <li>samtools</li>

    <li>pysam</li>

    <li>ataqv</li>

    <li>cta (the forked version on the porchard GitHub)</li>

    </ol>

    <p>I''ve used this pipeline with NextFlow v. 19.04.1</p>

    <h2>

    <a id="user-content-configuration" class="anchor" href="#configuration" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h2>

    <p>Paths to various generic files (e.g., bwa indices) must be included in the
    nextflow.config file -- check that file and change paths accordingly. These include:</p>

    <ol>

    <li>Blacklist bed files for each genome</li>

    <li>Chrom size files for each genome</li>

    <li>BWA indices</li>

    <li>TSS files (BED6 files denoting TSS positions)</li>

    <li>Gene bed files (BED4 files; included because we get per-gene read counts for
    use with LIGER in downstream processing). You probably want these to represent
    gene bodies + promoters if you plan to use these with LIGER.</li>

    <li>Path to the barcode whitelist (the 10X whitelist is included in this repo)</li>

    </ol>

    <p>You''ll also need to set the params.results variable -- either in the nextflow.config
    file itself, or on the command line when you run the pipeline (''--results /path/to/results'').</p>

    <p>To reduce memory usage of ataqv, we filter out nuclei with low read counts
    before running ataqv. The minimum read threshold is set in the nextflow.config
    file.</p>

    <p>Lastly, you''ll need to include information about each ATAC-seq library, including
    the genome(s) for the species that each library includes, and the paths to the
    fastq files for each readgroup. Organize this information in a JSON file, as in
    library-config.json. Note that for each readgroup, three fastq files are required
    -- the first and second insert reads (''1'' and ''2''), and the read with the
    nuclear barcode (''index'')</p>

    <h2>

    <a id="user-content-running" class="anchor" href="#running" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running</h2>

    <p>Once you have all of the above information, you can run the pipeline as follows
    (in this case, indicating the path to the results on the command line):</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run -with-singularity
    /path/to/Singularity.simg -params-file library-config.json --results /path/to/results
    /path/to/main.nf</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619189918.0
cta-observatory/CTADIRAC:
  data_format: 2
  description: CTA-customized version of the DIRAC middleware
  filenames:
  - Singularity
  full_name: cta-observatory/CTADIRAC
  latest_release: v1r62test
  readme: '<p>CTADIRAC project:</p>

    <ul>

    <li>moved to CTAO Observatory gitlab @ <a href="https://gitlab.cta-observatory.org/cta-computing/dpps/CTADIRAC"
    rel="nofollow">https://gitlab.cta-observatory.org/cta-computing/dpps/CTADIRAC</a>
    on Decembe 2020</li>

    <li>moved to git on Septembre 5th 2017</li>

    <li>add things, need add a licence GPLv3 ?</li>

    </ul>

    <p>Authors from svn:<br>

    Adrian Casajus <a href="mailto:adria@ecm.ub.es">adria@ecm.ub.es</a> <br>

    Luisa Arrabito <a href="mailto:arrabito@in2p3.fr">arrabito@in2p3.fr</a> <br>

    Johan Bregeon &lt;<a href="mailto:bregeon@.in2p3.fr">bregeon@.in2p3.fr</a>&gt;
    <br>

    Johann Cohen Tanugi <a href="mailto:johann.cohen-tanugi@umontpellier.fr">johann.cohen-tanugi@umontpellier.fr</a>
    <br>

    ? Han Bcn <a href="mailto:nhan.bcn@gmail.com">nhan.bcn@gmail.com</a> <br>

    Ricardo Graciani <a href="mailto:graciani@ecm.ub.edu">graciani@ecm.ub.edu</a>
    <br></p>

    '
  stargazers_count: 2
  subscribers_count: 7
  topics: []
  updated_at: 1609949938.0
ctpelok77/fdss:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ctpelok77/fdss
  latest_release: null
  readme: '<p>Fast Downward is a domain-independent planning system.</p>

    <p>For documentation and contact information see <a href="http://www.fast-downward.org/"
    rel="nofollow">http://www.fast-downward.org/</a>.</p>

    <p>The following directories are not part of Fast Downward as covered by this

    license:</p>

    <ul>

    <li>./src/search/ext</li>

    </ul>

    <p>For the rest, the following license applies:</p>

    <pre><code>Fast Downward is free software: you can redistribute it and/or modify
    it under

    the terms of the GNU General Public License as published by the Free Software

    Foundation, either version 3 of the License, or (at your option) any later

    version.


    Fast Downward is distributed in the hope that it will be useful, but WITHOUT ANY

    WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
    A

    PARTICULAR PURPOSE. See the GNU General Public License for more details.


    You should have received a copy of the GNU General Public License along with

    this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619778901.0
cyang31/containers:
  data_format: 2
  description: null
  filenames:
  - Singularity.centos_tf
  - Singularity.pytorch
  - Singularity.ExplainAI
  - Singularity.centos_torch3
  - Singularity.centos_tf2
  - Singularity.ubuntu_tf
  - Singularity.torch
  - Singularity.torch_mmf
  - Singularity.jax
  - Singularity.physio
  - Singularity.mac_local
  - Singularity.Spektral
  - Singularity.centos_torch2
  - Singularity.ubuntu_pre
  - Singularity.centos_torch
  - Singularity.ubuntu_torch
  - Singularity.ExplainAI2
  full_name: cyang31/containers
  latest_release: null
  readme: '<p>Singulairy container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620037627.0
d-bohn/rstudio_aci:
  data_format: 2
  description: get rstudio on PSU ACI
  filenames:
  - Singularity
  - Singularity.ml
  full_name: d-bohn/rstudio_aci
  latest_release: null
  readme: '<h1>

    <a id="user-content-rstudio_aci" class="anchor" href="#rstudio_aci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>rstudio_aci</h1>

    <p>Code and workflow for building <a href="https://www.rocker-project.org/" rel="nofollow">rocker/verse</a>

    in Docker Hub and modifying it with Singularity Hub for use with PSU

    ACI HPC clusters.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p><code>ssh</code> into the PSU ACI HPC with X11 flags.</p>

    <pre><code>ssh USERID@aci-b.aci.ics.psu.edu -X -Y

    </code></pre>

    <p>Start an interactive session using <code>qsub</code>.</p>

    <pre><code>qsub -A open -I -X -l walltime=24:00:00 -l nodes=2:ppn=20 -l pmem=10gb

    </code></pre>

    <p>From ACI, start <code>screen</code> and then execute the following code to

    create an <code>RStudio</code> image running at address <code>127.0.0.1:8787</code>.</p>

    <pre><code>screen


    singularity pull -n rstudio_aci.simg shub://d-bohn/rstudio_aci


    singularity exec rstudio_aci.simg rserver --www-address=127.0.0.1

    </code></pre>

    <p>Next, press <code>CTRL+A+D</code> to detach the screen while allowing the process
    to continue running in the background.</p>

    <p>Finally, start your preferred browser and navigate to <code>127.0.0.1</code>.
    For

    example, firefox:</p>

    <pre><code>singularity pull shub://jpetucci-firefox_icsaci


    singularity exec jpetucci-firefox_icsaci-master-latest.simg /opt/firefox/./firefox

    </code></pre>

    <h2>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <p>1). A <code>shiny</code> server should also start when executing this image,

    the server should be running on port <code>3838</code></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1562671274.0
dagonzalezfo/HPCPlayground:
  data_format: 2
  description: This repository should contain all the works related to HPC
  filenames:
  - singularity/lolcow/Singularity
  full_name: dagonzalezfo/HPCPlayground
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpcplayground" class="anchor" href="#hpcplayground" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HPCPlayground</h1>

    <p>This repository should contain all the works related to HPC</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610561146.0
darachm/bartender_munge:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: darachm/bartender_munge
  latest_release: null
  readme: '<p>This container is for providing <code>bartender</code> for some bioinformatics
    pipelines.</p>

    <p>It''s super mungy</p>

    <p>The primary reason for this is so that it flows nicely through SingularityHub

    for Nextflow pipelines.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1554263689.0
darachm/containers:
  data_format: 2
  description: metarepo for tidying up container recipes, currently Singularity
  filenames:
  - tensorflow/Singularity.tensorflow-v2.4.0-rc4-compiled
  - tensorflow/Singularity.tensorflow-v2.0.3-compiled
  - tensorflow/Singularity.tensorflow-v1.15.4-compiled-partial
  - tensorflow/Singularity.tensorflow-v2.2.0-compiled
  - bioconda/Singularity.bioconda
  - bioinfmunger/Singularity.bioinfmunger
  - lh3-aligners/Singularity.lh3-aligners
  - base/Singularity.base
  - pacbio/Singularity.pacbio
  - starcode/Singularity.starcode-v0.1.1
  - jupyter/Singularity.jupyter-plus-tensorflow-v2.2.0-compiled
  - jupyter/Singularity.jupyter-plus-alignparse
  - jupyter/Singularity.jupyter
  - jupyter/Singularity.jupyter-plus-bioconda
  - jupyter/Singularity.jupyter-plus-tensorflow-v2.4.0-rc4-compiled
  - jupyter/Singularity.jupyter-plus
  - r/Singularity.r
  - r/Singularity.r-plus
  - ubuntu/Singularity.ubuntu2004
  - shell/Singularity.shell-plus
  full_name: darachm/containers
  latest_release: null
  readme: '<p>This is for tracking, hosting recipes for Singularity containers, such
    that

    it can get mirrored on Github and singularity-hub can get it.</p>

    <p>Organzation copied from <a href="https://github.com/jlboat/BioinfoContainers">jlboat</a>.

    (Of course, makes total sense to just use tags to organize things!)</p>

    <p>Some recipes are for individual tools, some are for workflows and so are

    combos.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619081056.0
darachm/itermae:
  data_format: 2
  description: null
  filenames:
  - Singularity.itermae
  - Singularity.latest
  - Singularity.test_base
  - Singularity.test
  - Singularity.itermae-plus
  full_name: darachm/itermae
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-itermae\" class=\"anchor\" href=\"#itermae\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>itermae</h1>\n<p>See the <a href=\"https://darachm.gitlab.io/itermae/concept.html\"\
    \ rel=\"nofollow\">concept here</a> and\n<a href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\"\
    \ rel=\"nofollow\">tutorial here</a>.</p>\n<p><code>itermae</code> is a command-line\
    \ utility to recognize patterns in input sequences\nand generate outputs from\
    \ groups recognized. Basically, it uses fuzzy regular\nexpression operations to\
    \ (primarily) DNA sequence for purposes of DNA\nbarcode/tag/UMI parsing, sequence\
    \ and quality -based filtering,\nand general output re-arrangment.</p>\n<p><a\
    \ href=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\"\
    \ alt=\"itermae diagram\" data-canonical-src=\"https://darachm.gitlab.io/itermae/_images/parse_diagram_1.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><code>itermae</code> reads and makes\
    \ FASTQ, FASTA, text-file, and SAM (tab-delimited)\nfiles using <a href=\"https://pypi.org/project/biopython/\"\
    \ rel=\"nofollow\"><code>Biopython</code></a> sequence records\nto represent slice,\
    \ and read/output formats.\nPattern matching uses the <a href=\"https://pypi.org/project/regex/\"\
    \ rel=\"nofollow\"><code>regex</code></a> library,\nand the tool is designed to\
    \ function in command-line pipes from tools like\n<a href=\"https://www.gnu.org/software/parallel/\"\
    \ rel=\"nofollow\">GNU <code>parallel</code></a>\nto permit light-weight parallelization.</p>\n\
    <p>It's usage might look something like this:</p>\n<pre><code>zcat seq_data.fastqz\
    \ | itermae --config my_config.yml -v &gt; output.sam\n</code></pre>\n<p>or</p>\n\
    <pre><code>zcat seq_data.fastqz \\\n    | parallel --quote --pipe -l 4 --keep-order\
    \ -N 10000 \\\n        itermae --config my_config.yml -v &gt; output.sam\n</code></pre>\n\
    <p>with a <code>my_config.yml</code> file that may look something like this:</p>\n\
    <pre><code>matches:\n    - use: input\n      pattern: NNNNNGTCCTCGAGGTCTCTNNNNNNNNNNNNNNNNNNNNCGTACGCTGCAGGTC\n\
    \      marking: aaaaaBBBBBBBBBBBBBBBccccccccccccccccccccDDDDDDDDDDDDDDD\n    \
    \  marked_groups:\n          a:\n              name: sampleIndex\n           \
    \   repeat: 5\n          B:\n              allowed_errors: 2\n          c:\n \
    \             name: barcode\n              repeat_min: 18\n              repeat_max:\
    \ 22\n          D:\n              allowed_insertions: 1\n              allowed_deletions:\
    \ 2\n              allowed_substititions: 2\noutput_list:\n    -   name: 'barcode'\n\
    \        description: 'description+\" sample=\"+sampleIndex'\n        seq: 'barcode'\n\
    \        filter: 'statistics.median(barcode.quality) &gt;= 35'\n</code></pre>\n\
    <h1>\n<a id=\"user-content-availability-installation-installation\" class=\"anchor\"\
    \ href=\"#availability-installation-installation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Availability,\
    \ installation, 'installation'</h1>\n<p>Options:</p>\n<ol>\n<li>\n<p>Use pip to\
    \ install <code>itermae</code>, so</p>\n<p>python3 -m pip install itermae</p>\n\
    </li>\n<li>\n<p>You can clone this repo, and install it locally. Dependencies\
    \ are in\n<code>requirements.txt</code>, so\n<code>python3 -m pip install -r requirements.txt</code>\
    \ will install those.</p>\n</li>\n<li>\n<p>You can use <a href=\"https://syslab.org\"\
    \ rel=\"nofollow\">Singularity</a> to pull and run a\n<a href=\"https://singularity-hub.org/collections/4537\"\
    \ rel=\"nofollow\">Singularity image of itermae.py</a>,\nwhere everything is already\
    \ installed.\nThis is the recommended usage.</p>\n<p>This image is built with\
    \ a few other tools,\nlike g/mawk, perl, and parallel, to make command line munging\
    \ easier.</p>\n</li>\n</ol>\n<h1>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h1>\n<p><code>itermae</code> is envisioned\
    \ to be used in a pipe-line where you just got your\nDNA sequencing FASTQ reads\
    \ back, and you want to parse them.\nThe recommended interface is the YAML config\
    \ file, as demonstrated\nin <a href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\"\
    \ rel=\"nofollow\">the tutorial</a>\nand detailed again in the\n<a href=\"https://darachm.gitlab.io/itermae/usage/config.html\"\
    \ rel=\"nofollow\">configuration details</a>.\nYou can also use a command-line\
    \ argument interface as detailed more\n<a href=\"https://darachm.gitlab.io/itermae/usage/examples.html\"\
    \ rel=\"nofollow\">in the examples</a>.</p>\n<p>I recommend you test this on small\
    \ batches of data,\nthen stick it behind GNU <code>parallel</code> and feed the\
    \ whole FASTQ file via\n<code>zcat</code> in on standard input.\nThis parallelizes\
    \ with a small memory footprint, then\nyou write it out to disk (or stream into\
    \ another tool).</p>\n<h1>\n<a id=\"user-content-thanks\" class=\"anchor\" href=\"\
    #thanks\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Thanks</h1>\n<p>Again, the tool is built upon on the excellent work\
    \ of</p>\n<ul>\n<li><a href=\"https://pypi.org/project/regex/\" rel=\"nofollow\"\
    ><code>regex</code></a></li>\n<li><a href=\"https://pypi.org/project/biopython/\"\
    \ rel=\"nofollow\"><code>Biopython</code></a></li>\n<li><a href=\"https://www.gnu.org/software/parallel/\"\
    \ rel=\"nofollow\"><code>parallel</code></a></li>\n</ul>\n<h1>\n<a id=\"user-content-development-helping\"\
    \ class=\"anchor\" href=\"#development-helping\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development, helping</h1>\n<p>Any\
    \ issues or advice are welcome as an\n<a href=\"https://gitlab.com/darachm/itermae/-/issues\"\
    \ rel=\"nofollow\">issue on the gitlab repo</a>.\nComplaints are especially welcome.</p>\n\
    <p>For development, see the\n<a href=\"https://darachm.gitlab.io/itermae/package.html\"\
    \ rel=\"nofollow\">documentation as rendered from docstrings</a>.</p>\n<p>A set\
    \ of tests is written up with <code>pytest</code> module, and can be run from\
    \ inside\nthe cloned repo with <code>make test</code>.\nSee <code>make help</code>\
    \ for more options, such as building, installing, and uploading.</p>\n<p>There's\
    \ also a bash script with some longer runs in\n<code>profiling_tests</code>, these\
    \ generate longer runs for profiling purposes\nwith <code>cProfile</code> and\
    \ <code>snakeviz</code>.\nBut is out of date. Todo is to re-configure and retest\
    \ that for speed.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619159484.0
darachm/singularity_jupyter:
  data_format: 2
  description: null
  filenames:
  - Singularity.v0.0.1
  - Singularity.v1.0.0
  - Singularity.v2.0.0
  full_name: darachm/singularity_jupyter
  latest_release: null
  readme: '<p>This is just a singularity file for yoinking in the Docker file for
    NCBI BLAST as a simg</p>

    <p>This is an example of poor documentation</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1584671066.0
darachm/singularity_miniconda3:
  data_format: 2
  description: null
  filenames:
  - Singularity.v0.0.0
  full_name: darachm/singularity_miniconda3
  latest_release: null
  readme: "<h2>\n<a id=\"user-content-emews-project-template\" class=\"anchor\" href=\"\
    #emews-project-template\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>EMEWS project template</h2>\n<p>You have just\
    \ created an EMEWS project.</p>\n<p>This project is compatible with swift-t v.\
    \ 1.3+. Earlier\nversions will NOT work.</p>\n<p>The project consists of the following\
    \ directories:</p>\n<pre><code>EMEWS-PhysiBoSSa/\n  data/\n  ext/\n  etc/\n  python/\n\
    \    test/\n  R/\n    test/\n  scripts/\n  swift/\n  README.md\n</code></pre>\n\
    <p>The directories are intended to contain the following:</p>\n<ul>\n<li>\n<code>data</code>\
    \ - model input etc. data</li>\n<li>\n<code>etc</code> - additional code used\
    \ by EMEWS</li>\n<li>\n<code>ext</code> - swift-t extensions such as eqpy, eqr</li>\n\
    <li>\n<code>python</code> - python code (e.g. model exploration algorithms written\
    \ in python)</li>\n<li>\n<code>python/test</code> - tests of the python code</li>\n\
    <li>\n<code>R</code> - R code (e.g. model exploration algorithms written R)</li>\n\
    <li>\n<code>R/test</code> - tests of the R code</li>\n<li>\n<code>scripts</code>\
    \ - any necessary scripts (e.g. scripts to launch a model), excluding\nscripts\
    \ used to run the workflow.</li>\n<li>\n<code>swift</code> - swift code</li>\n\
    </ul>\n<p>Use the subtemplates to customize this structure for particular types\
    \ of\nworkflows. These are: sweep, eqpy, and eqr.</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1581321093.0
darachm/singularity_ncbi-blast:
  data_format: 2
  description: null
  filenames:
  - Singularity.v0.0.0
  full_name: darachm/singularity_ncbi-blast
  latest_release: null
  readme: '<p>This is just a singularity file for yoinking in the Docker file for
    NCBI BLAST as a simg</p>

    <p>This is an example of poor documentation</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1603609657.0
darachm/singularity_ncbi-entrez:
  data_format: 2
  description: null
  filenames:
  - Singularity.v0.0.0
  full_name: darachm/singularity_ncbi-entrez
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_ncbi-entrez" class="anchor" href="#singularity_ncbi-entrez"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_ncbi-entrez</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1603628264.0
darachm/singularity_runningJobs:
  data_format: 2
  description: null
  filenames:
  - Singularity.v0.5.0
  - Singularity
  - Singularity.v0.2.0
  - Singularity.v0.1.0
  - Singularity.v0.4.0
  full_name: darachm/singularity_runningJobs
  latest_release: v0.1.0
  readme: '<h1>

    <a id="user-content-rstudio_aci" class="anchor" href="#rstudio_aci" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>rstudio_aci</h1>

    <p>Code and workflow for building <a href="https://www.rocker-project.org/" rel="nofollow">rocker/verse</a>

    in Docker Hub and modifying it with Singularity Hub for use with PSU

    ACI HPC clusters.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p><code>ssh</code> into the PSU ACI HPC with X11 flags.</p>

    <pre><code>ssh USERID@aci-b.aci.ics.psu.edu -X -Y

    </code></pre>

    <p>Start an interactive session using <code>qsub</code>.</p>

    <pre><code>qsub -A open -I -X -l walltime=24:00:00 -l nodes=2:ppn=20 -l pmem=10gb

    </code></pre>

    <p>From ACI, start <code>screen</code> and then execute the following code to

    create an <code>RStudio</code> image running at address <code>127.0.0.1:8787</code>.</p>

    <pre><code>screen


    singularity pull -n rstudio_aci.simg shub://d-bohn/rstudio_aci


    singularity exec rstudio_aci.simg rserver --www-address=127.0.0.1

    </code></pre>

    <p>Next, press <code>CTRL+A+D</code> to detach the screen while allowing the process
    to continue running in the background.</p>

    <p>Finally, start your preferred browser and navigate to <code>127.0.0.1</code>.
    For

    example, firefox:</p>

    <pre><code>singularity pull shub://jpetucci-firefox_icsaci


    singularity exec jpetucci-firefox_icsaci-master-latest.simg /opt/firefox/./firefox

    </code></pre>

    <h2>

    <a id="user-content-notes" class="anchor" href="#notes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h2>

    <p>1). A <code>shiny</code> server should also start when executing this image,

    the server should be running on port <code>3838</code></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1593763396.0
darachm/singularity_ubuntu:
  data_format: 2
  description: null
  filenames:
  - Singularity.v1.0.0
  full_name: darachm/singularity_ubuntu
  latest_release: null
  readme: '<p>This is a base updated singularity image of ubuntu 1804, updated on
    200208,

    to make it faster to debug Singularity builds.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1581237176.0
darachm/slapchop:
  data_format: 2
  description: Barcode/amplicon sequencing bioinformatics tool, chops up FASTQ reads
    into capture groups using fuzzy regular expressions instead of base-positions.
    Very flexible, very parallelized, order now !
  filenames:
  - Singularity.v0.5.1-alpha
  - Singularity.v0.5.0-alpha
  - Singularity.v0.3.0-alpha
  full_name: darachm/slapchop
  latest_release: v0.2.0-alpha
  readme: "<h1>\n<a id=\"user-content-slapchop\" class=\"anchor\" href=\"#slapchop\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>SLAPCHOP</h1>\n<p>SLAPCHOP.py parses Illumina reads using patterns\
    \ to extract barcodes.</p>\n<p>By using fuzzy regular expressions we can chop\
    \ robustly barcodes from\nindeterminate positions, filter the results based on\
    \ sequence or match\nproperties, and reassemble a fastq record from the results.</p>\n\
    <p>Available as\n<a href=\"https://www.singularity-hub.org/collections/1361\"\
    \ rel=\"nofollow\">a singularity containter</a>!\nSo you if you have\n<a href=\"\
    https://github.com/sylabs/singularity/releases\">Singularity</a>\ninstalled you\
    \ can just use it (without worrying about dependencies) with:\n<code>singularity\
    \ run shub://darachm/slapchop:latest -h</code> (to download and show the\nargument\
    \ help message for example). Then you use it like\n<code>singularity run shub://darachm/slapchop:latest\
    \ whatever.fastq arguments added</code></p>\n<p>More completely, this tool is\
    \ a python script. You give it a FASTQ(Z) file and\nsome operations to do, and\
    \ it'll do the following:</p>\n<pre><code>- read chunks of Illumina-format sequencing\
    \ reads\n- apply a series of operations:\n    - match fuzzy regular expression\
    \ to original sequence or previous\n        capture groups\n    - extract capture\
    \ groups and start next operation\n- apply pythonic filters (pass/fail) on sequence\
    \ or quality properties \n    (like average quality or group length)\n- apply\
    \ pythonic constructors to construct new FASTQ read from the capture\n    groups\
    \ (so ID plus the last four bases of the UMI plus length of whatever)\n- write\
    \ out these reads to new files of pass and fail\n</code></pre>\n<p>For tuning/debugging/designing\
    \ it has some verbosity modes to spill the gory\ndetails of each operation in\
    \ stats files, and should still have some memory\nprofiling functionality to debug\
    \ memory leaks (fixed that one).</p>\n<p>For a very very verbose example of a\
    \ debugging run:</p>\n<pre><code>./slapchop.py \\\n    input.fastqz -z \\\n  \
    \  output_basename \\\n    --bite-size 10 --processes 3  \\\n    --write-report\
    \ --limit 10000 \\\n    -o \"Sample:  input   &gt; (?P&lt;sample&gt;[ATCG]{5})(?P&lt;fixed1&gt;GTCCACGAGGTC){e&lt;=1}(?P&lt;rest&gt;TCT.*){e&lt;=1}\"\
    \ \\\n    -o \"Strain:  rest    &gt; (?P&lt;tag&gt;TCT){e&lt;=1}(?P&lt;strain&gt;[ATCG]{10,26})CGTACGCTGCAGGTCGAC\"\
    \  \\\n    -o \"UMITail: rest    &gt; (?P&lt;fixed2&gt;CGTACGCTGCAGGTC)(?&lt;UMItail&gt;GAC[ATCG]G[ATCG]A[ATCG]G[ATCG]G[ATCG]G[ATCG]GAT){s&lt;=2}\"\
    \  \\\n    -o \"UMI:     UMItail &gt; (GAC(?P&lt;umi1&gt;[ATCG])G(?&lt;umi2&gt;[ATCG])A(?&lt;umi3&gt;[ATCG])G(?&lt;umi4&gt;[ATCG])G(?&lt;umi5&gt;[ATCG])G(?&lt;umi6&gt;[ATCG])G){e&lt;=2}\"\
    \  \\\n    --output-seq \"strain\" \\\n    --output-id \"input.id+'_umi='+umi1.seq+umi2.seq+umi3.seq+\
    \ \\\n        umi4.seq+umi5.seq+umi6.seq+'_sample='+sample.seq\" \\\n    --filter\
    \ \"sample_length == 5 and rest_start &gt;= 16 and ( min(strain.letter_annotations['phred_quality'])\
    \ &gt;= 30 )\"\\\n    --verbose --verbose --verbose\n</code></pre>\n<p>That invocation:</p>\n\
    <pre><code>- Takes records from the `input.fastq`\n- Starts three processes that\
    \ each take bites of 10 records\n- Applies the four operations to cut up the read\n\
    - Writes the full detailed report including json reports for \n    each read,\
    \ so we limit it to the first 10,000 bytes\n    of the file (about 50 records).\
    \ This is for debugging.\n- Filters the records on having a `sample` barcode of\
    \ 5 bases \n    and having the `rest` sequence match starting at least past\n\
    \    index 16 (so base 15 in english).\n- Re-assembles the records that pass this\
    \ filter, making the ID\n    of the fastq record having the original ID plus a\
    \ UMI \n    sequence and the sample barcode, then the sequence is just\n    the\
    \ match to the strain barcode context. This is suitable for\n    feeding into\
    \ `bwa` for example.\n- We've got three levels of verbosity, so a per-record verbosity\n\
    \    for debugging purposes.\n</code></pre>\n<p>Note that the output formatting\
    \ is a bit funny. This is directly evaluated\n(because security is what?) on BioPython\
    \ SequenceRecords, so you need to specify\njust the name of the capture group(s)\
    \ for the outputs so it can access the\n<code>.seq</code> and qualities. For the\
    \ ID, etc, you can access <code>.seq</code> or <code>.id</code>.</p>\n<p>Then\
    \ if we like our thresholds we'd re-run, and drop the <code>--limit</code>\nand\
    \ <code>--write-report</code> flags. This will turn records like this:</p>\n<pre><code>@NB501157:100:H5J5LBGX2:1:11101:10000:6068\
    \ 1:N:0:\nCTACTGTCCACGAGGTCTCTGCAGATAATACACTGTCACCCGTACGCTGCAGGTCGACCGTAGGAGGGAGATGTG\n\
    +\nAAAAAEEEE/AEE&lt;EEEEEEEEAEEAEEAEEEEE/EEE/EEEEEEEEE/EEEEEEEEEEEEE/EEEEEEEEEEEE\n\
    </code></pre>\n<p>into records like this:</p>\n<pre><code>@NB501157:100:H5J5LBGX2:1:11101:10000:6068_umi=CTGAGA_sample=CTACT\n\
    GCAGATAATACACTGTCACC\n+\nEEAEEAEEAEEEEE/EEE/E\n</code></pre>\n<p>The sample barcode\
    \ is the first five, the strain barcode starts after\nthe <code>TCT</code>, and\
    \ the UMI is interspersed downstream. This is modified\nyeast BarSeq, btw.</p>\n\
    <hr>\n<p>This script depends strongly upon (uses) the work of\n<a href=\"https://pypi.org/project/regex/\"\
    \ rel=\"nofollow\">regex</a>\nand\n<a href=\"https://pypi.org/project/biopython/\"\
    \ rel=\"nofollow\">Biopython</a>. Thanks! Check them out...</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1594367782.0
darachm/ubuntu-2004:
  data_format: 2
  description: null
  filenames:
  - Singularity.v1.0.0
  full_name: darachm/ubuntu-2004
  latest_release: null
  readme: '<h1>

    <a id="user-content-generate-plans-using-forbid-iterative-planners-top-k-diverse-etc"
    class="anchor" href="#generate-plans-using-forbid-iterative-planners-top-k-diverse-etc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Generate
    plans using forbid iterative planners (top-k, diverse, etc)</h1>

    <p>This is a tool for creating synthetic plans, the generated plans will be converted
    to event logs (<code>.xes</code> files). Basically, this is a script which wrapped
    up current exisiting planners and the output data are in a suitable structure
    and format for our goal recognition experiments.</p>

    <h3>

    <a id="user-content-things-need-to-be-prepared" class="anchor" href="#things-need-to-be-prepared"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Things
    need to be prepared</h3>

    <ol>

    <li>

    <p>For running diverse planner, CPLEX and an recommended fast-downward planner
    is required.</p>

    </li>

    <li>

    <p>Required python 3 runtime environment, I recommend to build a python 3 virtual
    environment if you both have python 2 and python 3 installed.</p>

    </li>

    <li>

    <p>Need to complie C++ source codes, by following instuctions (CPLEX need to be
    pre-installed).</p>

    </li>

    <li>

    <p>To configure the command line environment variables in absolute path</p>

    <p>export DIVERSE_SCORE_COMPUTATION_PATH=/home/ubuntu/plan_generators/diversescore
    (VM)</p>

    <p>export DIVERSE_FAST_DOWNWARD_PLANNER_PATH=/home/ubuntu/plan_generators/fd-red-black-postipc2018
    (VM)</p>

    <p>export DIVERSE_SCORE_COMPUTATION_PATH=/Users/zihangs/plan_generators/diversescore
    (Mac)</p>

    </li>

    <li>

    <p>Download the dataset and put the downloaded dataset in this directory.</p>

    </li>

    </ol>

    <h3>

    <a id="user-content-commands-for-running-the-script" class="anchor" href="#commands-for-running-the-script"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Commands
    for running the script</h3>

    <p>Before run the commands, you have to check the parameter configrations, the
    parameters at the top of <code>run.py</code>. Then just run the following commands
    to starts.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    activate the python 3 venv (if you don''t using venv, ignore this step)</span>

    <span class="pl-c1">source</span> <span class="pl-k">&lt;</span>venv<span class="pl-k">&gt;</span>/bin/activate


    <span class="pl-c"><span class="pl-c">#</span> run script</span>

    python run.py</pre></div>

    <p>It will take a long time to run.</p>

    <h3>

    <a id="user-content-outputs" class="anchor" href="#outputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Outputs</h3>

    <p>Once the process is completed, check this directory, there will be a sub-directory
    <code>gene_data/</code>. All the domains, problems, tests and generated plans
    will be there. Then this directory will be used for our next steps for mining
    process models.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1597826109.0
davidhin/singularity-example:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: davidhin/singularity-example
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-builder---github-actions\" class=\"\
    anchor\" href=\"#singularity-builder---github-actions\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Singularity Builder\
    \ - Github Actions</h1>\n<p>This has been converted from the Gitlab Project here:\
    \  <a href=\"https://gitlab.com/singularityhub/gitlab-ci\" rel=\"nofollow\">https://gitlab.com/singularityhub/gitlab-ci</a></p>\n\
    <p>This is a simple example of how you can achieve:</p>\n<ul>\n<li>version control\
    \ of your recipes</li>\n<li>versioning to include image hash <em>and</em> commit\
    \ id</li>\n<li>build of associated container and</li>\n<li>push to a storage endpoint\
    \ (or <del>GitLab</del> Github artifact)</li>\n</ul>\n<p>for a reproducible build\
    \ workflow.</p>\n<p><strong>Why should this be managed via <del>GitLab</del> Github?</strong></p>\n\
    <p><del>GitLab</del> Github, by way of easy integration with continuous integration,\
    \ is an easy way\nto have a workflow set up where multiple people can collaborate\
    \ on a container recipe,\nthe recipe can be tested (with whatever testing you\
    \ need), discussed in pull requests,\nand then finally pushed to be a <del>GitLab</del>\
    \ Github artifact, to your storage of choice\nor to Singularity Registry.</p>\n\
    <p><strong>Why should I use this instead of a service?</strong></p>\n<p>You could\
    \ use a remote builder, but if you do the build in a continuous integration\n\
    service you get complete control over it. This means everything from the version\
    \ of\nSingularity to use, to the tests that you run for your container. You have\
    \ a lot more\nfreedom in the rate of building, and organization of your repository,\
    \ because it's you\nthat writes the configuration. Although the default would\
    \ work for most, you can\nedit the build, setup, and circle configuration file\
    \ in the\n<a href=\".gitlabci\">.gitlabci</a> folder to fit your needs.</p>\n\
    <h2>\n<a id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quick Start</h2>\n<p>Add your Singularity recipes to this repository,\
    \ and edit the build commands in\nthe <a href=\".gitlabci/build.sh\">build.sh</a>\
    \ file. This is where you can specify endpoints\n(Singularity Registry, Dropbox,\
    \ Google Storage, AWS) along with container names\n(the uri) and tag. You can\
    \ build as many recipes as you like, just add another line!</p>\n<div class=\"\
    highlight highlight-source-yaml\"><pre>                               <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> recipe relative to repository base</span>\n\
    \  - <span class=\"pl-s\">/bin/bash .gitlabci/build.sh Singularity</span>\n  -\
    \ <span class=\"pl-s\">/bin/bash .gitlabci/build.sh --uri collection/container\
    \ --tag tacos --cli google-storage Singularity</span>\n  - <span class=\"pl-s\"\
    >/bin/bash .gitlabci/build.sh --uri collection/container --cli google-drive Singularity</span>\n\
    \  - <span class=\"pl-s\">/bin/bash .gitlabci/build.sh --uri collection/container\
    \ --cli globus Singularity</span>\n  - <span class=\"pl-s\">/bin/bash .gitlabci/build.sh\
    \ --uri collection/container --cli registry Singularity</span></pre></div>\n<p>For\
    \ each client that you use, required environment variables (e.g., credentials\
    \ to push,\nor interact with the API) must be defined in the (encrypted) Travis\
    \ environment. To\nknow what variables to define, along with usage for the various\
    \ clients, see\nthe <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">client specific pages</a></p>\n<h2>\n<a id=\"user-content-detailed-started\"\
    \ class=\"anchor\" href=\"#detailed-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Detailed Started</h2>\n<h3>\n\
    <a id=\"user-content-0-fork-this-repository\" class=\"anchor\" href=\"#0-fork-this-repository\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>0. Fork this repository</h3>\n<p>You can clone and tweak, but it's\
    \ easiest likely to get started with our example\nfiles and edit them as you need.</p>\n\
    <h3>\n<a id=\"user-content-1-get-to-know-gitlab-github-actions\" class=\"anchor\"\
    \ href=\"#1-get-to-know-gitlab-github-actions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>1. Get to Know <del>GitLab</del>\
    \ Github Actions</h3>\n<p>Github has a built-in Continuous Integration service\
    \ called Github Actions you should be able to use for free. You can get started\
    \ here <a href=\"https://github.com/features/actions\">https://github.com/features/actions</a>\
    \ and take a look at the <code>.github\\workflows\\build.yml</code>.</p>\n<p>Artifacts\
    \ will be found in the <code>/home/runner/work/REPO-NAME/REPO-NAME/</code> directory.</p>\n\
    <h3>\n<a id=\"user-content-2-add-your-recipes\" class=\"anchor\" href=\"#2-add-your-recipes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2. Add your Recipe(s)</h3>\n<p>For the example here, we have a single\
    \ recipe named \"Singularity\" that is provided\nas an input argument to the <a\
    \ href=\".gitlabci/build.sh\">build script</a>. You could add another\nrecipe,\
    \ and then of course call the build to happen more than once.\nThe build script\
    \ will name the image based on the recipe, and you of course\ncan change this.\
    \ Just write the path to it (relative to the repository base) in\nyour <a href=\"\
    .github/workflows/build.yml\">.github/workflows/build.yml</a>.</p>\n<h3>\n<a id=\"\
    user-content-3-configure-singularity\" class=\"anchor\" href=\"#3-configure-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>3. Configure Singularity</h3>\n<p>We previously used <a href=\".gitlabci/setup.sh\"\
    >setup</a> to setup the build, but now use a base image instead.\nThe previous\
    \ instructions are provided for posterity.</p>\n<ul>\n<li>\n<p><del>Install Singularity,\
    \ we use the release 2.6 branch as it was the last to not be written in GoLang.\
    \ You could of course change the lines in <a href=\".gitlabci/setup.sh\">setup.sh</a>\
    \ to use a specific tagged release, an older version, or development version.</del></p>\n\
    </li>\n<li>\n<p>The current Github Action is using the <code>quay.io/singularity/singularity:v3.7.3</code>\
    \ image - at present the slim images are missing /bin/bash - the version can be\
    \ changed at will as per your production environment versioning.</p>\n</li>\n\
    <li>\n<p>Install the sregistry client, if needed. The <a href=\"https://singularityhub.github.io/sregistry-cli\"\
    \ rel=\"nofollow\">sregistry client</a> allows you to issue a command like \"\
    sregistry push ...\" to upload a finished image to one of your cloud / storage\
    \ endpoints. By default, the push won't happen, and you will just build an image\
    \ using the CI.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-4-configure-the-build\"\
    \ class=\"anchor\" href=\"#4-configure-the-build\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>4. Configure\
    \ the Build</h3>\n<p>The basic steps for the <a href=\".gitlabci/build.sh\">build</a>\
    \ are the following:</p>\n<ul>\n<li>Running build.sh with no inputs will default\
    \ to a recipe called \"Singularity\" in the base of the repository. You can provide\
    \ an argument to point to a different recipe path, always relative to the base\
    \ of your repository.</li>\n<li>If you want to define a particular unique resource\
    \ identifier for a finished container (to be uploaded to your storage endpoint)\
    \ you can do that with <code>--uri collection/container</code>. If you don't define\
    \ one, a robot name will be generated.</li>\n<li>You can add <code>--uri</code>\
    \ to specify a custom name, and this can include the tag, OR you can specify <code>--tag</code>\
    \ to go along with a name without one. It depends on which is easier for you.</li>\n\
    <li>If you add <code>--cli</code> then this is telling the build script that you\
    \ have defined the <a href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"\
    nofollow\">needed environment variables</a> for your <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">client of choice</a> and you want successful builds to be pushed\
    \ to your storage endpoint. See <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">here</a> for a list of current client endpoints, or roll your\
    \ own!</li>\n</ul>\n<p>See the <a href=\".gitlab-ci.yml\">.gitlab-ci.yml</a> for\
    \ examples of this build.sh command (commented out). If there is some cloud service\
    \ that you'd like that is not provided, please <a href=\"https://www.github.com/singularityhub/sregistry-cli/issues\"\
    >open an issue</a>.</p>\n<h3>\n<a id=\"user-content-5-pull--download-your-container\"\
    \ class=\"anchor\" href=\"#5-pull--download-your-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>5. Pull\
    \ / Download your Container</h3>\n<p>You can access the artifacts from the Actions\
    \ tab, under the build action for any given run. You may wish to edit the <code>build.yml</code>\
    \ to export individual .sif images rather than a zip file of artifacts.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620391274.0
dcgc-bfx/dcgc-single-cell:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity.0.2.2
  - Singularity.0.1-alpha
  - Singularity.0.2.1
  - Singularity.0.2.0
  full_name: dcgc-bfx/dcgc-single-cell
  latest_release: null
  readme: '<p><a href="https://github.com/dcgc-bfx/dcgc-single-cell/workflows/Build/badge.svg?branch=main"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/dcgc-bfx/dcgc-single-cell/workflows/Build/badge.svg?branch=main"
    alt="Build" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/5095" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-dcgc-single-cell" class="anchor" href="#dcgc-single-cell"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>dcgc-single-cell</h1>

    <p>DCGC singularity recipe for single cell analysis</p>

    <p>Pull it from the singularity hub: <a href="https://singularity-hub.org/collections/5095"
    rel="nofollow">https://singularity-hub.org/collections/5095</a></p>

    <p>Start jupyter lab:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run --writable-tmpfs
    --app jupyter shub://dcgc-bfx/dcgc-single-cell</pre></div>

    <p>Start rstudio server listening on port 8787:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run --writable-tmpfs
    --app rserver shub://dcgc-bfx/dcgc-single-cell 8787</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620324842.0
dchaimow/gfae:
  data_format: 2
  description: generic fmri analysis environment
  filenames:
  - Singularity.bak
  - Singularity
  - Singularity_fsl
  full_name: dchaimow/gfae
  latest_release: null
  readme: '<p>generic fmri analysis environment</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620511929.0
ddcap/singularity-halvade:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ddcap/singularity-halvade
  latest_release: hadoop_conf_halvade
  readme: "<h1>\n<a id=\"user-content-use-singularity-image\" class=\"anchor\" href=\"\
    #use-singularity-image\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Use singularity image</h1>\n<p>Singularity image:\n\
    <a href=\"https://singularity-hub.org/collections/5392\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity instance start \\\n  --bind <span class=\"pl-s\"><span class=\"\
    pl-pds\">$(</span>mktemp -d run/hostname_XXXX<span class=\"pl-pds\">)</span></span>:/run\
    \ \\\n  --bind dropbear/:/etc/dropbear \\\n  --bind log/:/usr/local/spark/logs\
    \ \\\n  --bind work/:/usr/local/spark/work \\\n  halvade.sif halvade-single\n\n\
    \  <span class=\"pl-c\"><span class=\"pl-c\">#</span> start spark inside the started\
    \ image</span>\n  singularity <span class=\"pl-c1\">exec</span> instance://halvade-single\
    \ /usr/local/spark/sbin/start-all.sh\n\n  <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> if pwless ssh doesn't work, you can specify the used key in the\
    \ `bashrc` file in the same directory as your halvade.sif file by adding this\
    \ line:</span>\n  SPARK_SSH_OPTS=<span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span><span class=\"pl-smi\">$SPARK_SSH_OPTS</span> -i /home/username/.ssh/pwless_rsa<span\
    \ class=\"pl-pds\">\"</span></span>\n</pre></div>\n<h1>\n<a id=\"user-content-extra-variables-to-use-in-script\"\
    \ class=\"anchor\" href=\"#extra-variables-to-use-in-script\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>extra variables\
    \ to use in script:</h1>\n<div class=\"highlight highlight-source-shell\"><pre>HALVADE_HOME\n\
    HALVADE_OPTS=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-option0 value\
    \ --option1 value -option2<span class=\"pl-pds\">\"</span></span>\nRESOURCES <span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> should have all these options:\
    \ --driver-memory ${DRIVER_MEM} --executor-memory ${EXECUTOR_MEMORY} --executor-cores\
    \ ${EXECUTOR_CORES} --conf spark.task.cpus=${TASK_CPUS} --conf spark.executor.memoryOverhead=${OVERHEAD_MEMORY}</span>\n\
    \nSPARK_SSH_OPTS <span class=\"pl-c\"><span class=\"pl-c\">#</span> can add -i\
    \ here if pwless_rsa is not found</span>\nHADOOP_SSH_OPTS</pre></div>\n<h1>\n\
    <a id=\"user-content-install-locally-without-the-singularity-image\" class=\"\
    anchor\" href=\"#install-locally-without-the-singularity-image\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Install\
    \ locally without the singularity image:</h1>\n<h2>\n<a id=\"user-content-start-spark-without-yarn\"\
    \ class=\"anchor\" href=\"#start-spark-without-yarn\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>start Spark without\
    \ YARN</h2>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> use default settings -&gt; all cpus/memory\
    \ in spark</span>\nstart-all.sh\nstop-all.sh\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span>To set memory/CPU per node:</span>\nstart-master.sh\nstart-slave.sh\
    \ spark://node001:7077 -c 6 -m 50 <span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ on every worker node</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ stop</span>\nstop-slave.sh <span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ on every worker node</span>\nstop-master.sh</pre></div>\n<p><code>spark-submit</code>\
    \ will need <code>--master</code> option to be set to <code>spark://master:7077</code>\
    \ to use the standalone Spark.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619643326.0
dilkas/wmc-without-parameters:
  data_format: 2
  description: null
  filenames:
  - deps/DPMC/DMC/Singularity
  - deps/DPMC/HTB/Singularity
  - deps/DPMC/tensor/Singularity
  - deps/DPMC/lg/Singularity
  full_name: dilkas/wmc-without-parameters
  latest_release: null
  readme: '<p>This repository holds all source code (that I am legally allowed to
    distribute) related to the following publications:</p>

    <ul>

    <li>Dilkas P., Belle V. <strong>Weighted Model Counting with Conditional Weights
    for Bayesian Networks</strong>. UAI 2021.</li>

    <li>Dilkas P., Belle V. <strong>Weighted Model Counting Without Parameter Variables</strong>.
    SAT 2021.</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621730220.0
diplomacy/research:
  data_format: 2
  description: Supervised and RL Models for No Press Diplomacy
  filenames:
  - diplomacy_research/containers/redis/Singularity
  - diplomacy_research/containers/ubuntu-cuda10/Singularity
  - diplomacy_research/containers/albert-ai/Singularity
  - diplomacy_research/containers/research/Singularity
  - diplomacy_research/containers/tensorflow-serving/Singularity
  full_name: diplomacy/research
  latest_release: v1.0.0
  readme: "<h1>\n<a id=\"user-content-supervised-and-rl-models-for-no-press-diplomacy\"\
    \ class=\"anchor\" href=\"#supervised-and-rl-models-for-no-press-diplomacy\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Supervised\
    \ and RL Models for No Press Diplomacy</h1>\n<p>This repository contains the source\
    \ code used to develop a supervised and RL agent that can play the No Press version\
    \ of Diplomacy.</p>\n<p align=\"center\">\n  <a href=\"docs/images/map_overview.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img width=\"500\" src=\"docs/images/map_overview.png\"\
    \ alt=\"Diplomacy Map Overview\" style=\"max-width:100%;\"></a>\n</p>\n<h2>\n\
    <a id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>This project is licensed under the MIT License - see the <a href=\"LICENSE\"\
    >LICENSE</a> file for details.</p>\n<p><strong>Restrictions: The trained weights\
    \ provided with this repository are for research purposes only and cannot be used\
    \ to power any bots on any website without my prior written consent, which may\
    \ be withheld without reasons.</strong></p>\n<p><strong>The data provider also\
    \ prevents using its data to train any bots accessible on any website.</strong></p>\n\
    <p><strong>You can play against the trained model by playing against \"KestasBot\"\
    \ on webdiplomacy.net</strong></p>\n<h2>\n<a id=\"user-content-dataset\" class=\"\
    anchor\" href=\"#dataset\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Dataset</h2>\n<p>The model was trained by using\
    \ a dataset of 156,468 games (diplomacy-v1-27k-msgs.zip), which consists of:</p>\n\
    <ul>\n<li>16,633 games on non-standard maps (e.g. modern and ancmed) (other_maps.jsonl)</li>\n\
    <li>33,279 no-press games on the standard map (standard_no_press.jsonl)</li>\n\
    <li>50 press games on the standard map with messages (standard_press_with_msgs.jsonl)</li>\n\
    <li>106,456 press games on the standard map without messages (standard_press_without_msgs.jsonl)</li>\n\
    <li>50 public press games on the standard map with messages (standard_public_press.jsonl)</li>\n\
    </ul>\n<p>A dataset of 156,458 games with 13,469,536 messages is also being prepared,\
    \ but it is not yet available.</p>\n<p>Access to the dataset used to train the\
    \ model can be requested by sending an email to <a href=\"mailto:webdipmod@gmail.com\"\
    >webdipmod@gmail.com</a>.</p>\n<h2>\n<a id=\"user-content-getting-started\" class=\"\
    anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Getting Started</h2>\n<h3>\n<a id=\"\
    user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h3>\n\
    <p>The repository can be installed in a conda environment with:</p>\n<div class=\"\
    highlight highlight-source-python\"><pre><span class=\"pl-s1\">conda</span> <span\
    \ class=\"pl-s1\">create</span> <span class=\"pl-c1\">-</span><span class=\"pl-s1\"\
    >n</span> <span class=\"pl-s1\">diplomacy</span>\n<span class=\"pl-s1\">conda</span>\
    \ <span class=\"pl-s1\">activate</span> <span class=\"pl-s1\">diplomacy</span>\n\
    <span class=\"pl-s1\">pip</span> <span class=\"pl-s1\">install</span> <span class=\"\
    pl-c1\">-</span><span class=\"pl-s1\">r</span> <span class=\"pl-s1\">requirements</span>.<span\
    \ class=\"pl-s1\">txt</span>\n<span class=\"pl-s1\">pip</span> <span class=\"\
    pl-s1\">install</span> <span class=\"pl-c1\">-</span><span class=\"pl-s1\">r</span>\
    \ <span class=\"pl-s1\">requirements_dev</span>.<span class=\"pl-s1\">txt</span></pre></div>\n\
    <p>This package depends on Redis and singularity 3+. Singularity can be installed\
    \ with:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Installing Singularity v3.2.0</span>\n<span\
    \ class=\"pl-k\">export</span> VERSION=v3.2.0\nsudo apt-get update -y\nsudo apt-get\
    \ install -y build-essential libssl-dev uuid-dev libgpgme11-dev libseccomp-dev\
    \ pkg-config squashfs-tools\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ Installing GO 1.12.5</span>\n<span class=\"pl-k\">export</span> GO_VERSION=1.12.5\
    \ OS=linux ARCH=amd64\nwget -nv https://dl.google.com/go/go<span class=\"pl-smi\"\
    >$GO_VERSION</span>.<span class=\"pl-smi\">$OS</span>-<span class=\"pl-smi\">$ARCH</span>.tar.gz\n\
    sudo tar -C /usr/local -xzf go<span class=\"pl-smi\">$GO_VERSION</span>.<span\
    \ class=\"pl-smi\">$OS</span>-<span class=\"pl-smi\">$ARCH</span>.tar.gz\nrm -f\
    \ go<span class=\"pl-smi\">$GO_VERSION</span>.<span class=\"pl-smi\">$OS</span>-<span\
    \ class=\"pl-smi\">$ARCH</span>.tar.gz\n<span class=\"pl-k\">export</span> GOPATH=<span\
    \ class=\"pl-smi\">$HOME</span>/.go\n<span class=\"pl-k\">export</span> PATH=/usr/local/go/bin:<span\
    \ class=\"pl-smi\">${PATH}</span>:<span class=\"pl-smi\">${GOPATH}</span>/bin\n\
    mkdir -p <span class=\"pl-smi\">$GOPATH</span>\ngo get github.com/golang/dep/cmd/dep\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Building from source</span>\n\
    mkdir -p <span class=\"pl-smi\">$GOPATH</span>/src/github.com/sylabs\n<span class=\"\
    pl-c1\">cd</span> <span class=\"pl-smi\">$GOPATH</span>/src/github.com/sylabs\n\
    git clone https://github.com/sylabs/singularity.git\n<span class=\"pl-c1\">cd</span>\
    \ singularity\ngit checkout <span class=\"pl-smi\">$VERSION</span>\n./mconfig\
    \ -p /usr/local\n<span class=\"pl-c1\">cd</span> ./builddir\nmake\nsudo make install</pre></div>\n\
    <p>The package is compatible with Python 3.5, 3.6, and 3.7.</p>\n<h3>\n<a id=\"\
    user-content-training-models\" class=\"anchor\" href=\"#training-models\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Training\
    \ models</h3>\n<p>To train a model:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ <span class=\"pl-k\">export</span> WORKING_DIR=/path/to/some/directory\n\
    $ cp diplomacy-v1-27k-msgs.zip <span class=\"pl-smi\">$WORKING_DIR</span>\n$ conda\
    \ activate diplomacy\n$ python diplomacy_research/scripts/build_dataset.py\n$\
    \ python diplomacy_research/models/policy/order_based/train.py --model_id 12</pre></div>\n\
    <h3>\n<a id=\"user-content-playing-against-the-sl-and-rl-agents\" class=\"anchor\"\
    \ href=\"#playing-against-the-sl-and-rl-agents\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Playing against the SL and RL\
    \ agents</h3>\n<p>It is possible to play against the published results by using\
    \ the <code>DipNetSLPlayer</code> and <code>DipNetRLPlayer</code> players in <code>diplomacy_research.players.benchmark_player</code>.</p>\n\
    <p>These players will automatically download a singularity container with the\
    \ trained weights, and then launch a TF serving server to handle the requests.</p>\n\
    <p>A simple example on how to play a 7 bots game is:</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">tornado</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\"\
    >gen</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">ujson</span>\
    \ <span class=\"pl-k\">as</span> <span class=\"pl-s1\">json</span>\n<span class=\"\
    pl-k\">from</span> <span class=\"pl-s1\">diplomacy</span> <span class=\"pl-k\"\
    >import</span> <span class=\"pl-v\">Game</span>\n<span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">diplomacy</span>.<span class=\"pl-s1\">utils</span>.<span\
    \ class=\"pl-s1\">export</span> <span class=\"pl-k\">import</span> <span class=\"\
    pl-s1\">to_saved_game_format</span>\n<span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">diplomacy_research</span>.<span class=\"pl-s1\">players</span>.<span class=\"\
    pl-s1\">benchmark_player</span> <span class=\"pl-k\">import</span> <span class=\"\
    pl-v\">DipNetSLPlayer</span>\n<span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">diplomacy_research</span>.<span class=\"pl-s1\">utils</span>.<span class=\"\
    pl-s1\">cluster</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\"\
    >start_io_loop</span>, <span class=\"pl-s1\">stop_io_loop</span>\n\n<span class=\"\
    pl-en\">@<span class=\"pl-s1\">gen</span>.<span class=\"pl-s1\">coroutine</span></span>\n\
    <span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span\
    \ class=\"pl-s\">\"\"\" Plays a local game with 7 bots \"\"\"</span>\n    <span\
    \ class=\"pl-s1\">player</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-v\">DipNetSLPlayer</span>()\n    <span class=\"pl-s1\">game</span> <span class=\"\
    pl-c1\">=</span> <span class=\"pl-v\">Game</span>()\n\n    <span class=\"pl-c\"\
    ># Playing game</span>\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\"\
    >not</span> <span class=\"pl-s1\">game</span>.<span class=\"pl-s1\">is_game_done</span>:\n\
    \        <span class=\"pl-s1\">orders</span> <span class=\"pl-c1\">=</span> <span\
    \ class=\"pl-k\">yield</span> {<span class=\"pl-s1\">power_name</span>: <span\
    \ class=\"pl-s1\">player</span>.<span class=\"pl-en\">get_orders</span>(<span\
    \ class=\"pl-s1\">game</span>, <span class=\"pl-s1\">power_name</span>) <span\
    \ class=\"pl-k\">for</span> <span class=\"pl-s1\">power_name</span> <span class=\"\
    pl-c1\">in</span> <span class=\"pl-s1\">game</span>.<span class=\"pl-s1\">powers</span>}\n\
    \        <span class=\"pl-k\">for</span> <span class=\"pl-s1\">power_name</span>,\
    \ <span class=\"pl-s1\">power_orders</span> <span class=\"pl-c1\">in</span> <span\
    \ class=\"pl-s1\">orders</span>.<span class=\"pl-en\">items</span>():\n      \
    \      <span class=\"pl-s1\">game</span>.<span class=\"pl-en\">set_orders</span>(<span\
    \ class=\"pl-s1\">power_name</span>, <span class=\"pl-s1\">power_orders</span>)\n\
    \        <span class=\"pl-s1\">game</span>.<span class=\"pl-en\">process</span>()\n\
    \n    <span class=\"pl-c\"># Saving to disk</span>\n    <span class=\"pl-k\">with</span>\
    \ <span class=\"pl-en\">open</span>(<span class=\"pl-s\">'game.json'</span>, <span\
    \ class=\"pl-s\">'w'</span>) <span class=\"pl-k\">as</span> <span class=\"pl-s1\"\
    >file</span>:\n        <span class=\"pl-s1\">file</span>.<span class=\"pl-en\"\
    >write</span>(<span class=\"pl-s1\">json</span>.<span class=\"pl-en\">dumps</span>(<span\
    \ class=\"pl-en\">to_saved_game_format</span>(<span class=\"pl-s1\">game</span>)))\n\
    \    <span class=\"pl-en\">stop_io_loop</span>()\n\n<span class=\"pl-k\">if</span>\
    \ <span class=\"pl-s1\">__name__</span> <span class=\"pl-c1\">==</span> <span\
    \ class=\"pl-s\">'__main__'</span>:\n    <span class=\"pl-en\">start_io_loop</span>(<span\
    \ class=\"pl-s1\">main</span>)</pre></div>\n<h3>\n<a id=\"user-content-playing-against-a-model\"\
    \ class=\"anchor\" href=\"#playing-against-a-model\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Playing against\
    \ a model</h3>\n<p>It is also possible for humans to play against bots using the\
    \ web interface. The player can be changed in <code>diplomacy_research.scripts.launch_bot</code></p>\n\
    <div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span\
    \ class=\"pl-c\">#</span> In a terminal window or tab - Launch React server (from\
    \ diplomacy/diplomacy)</span>\nnpm start\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> In another terminal window or tab - Launch diplomacy server</span>\n\
    python -m diplomacy.server.run\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ In a third terminal window or tab - Launch the bot script</span>\npython diplomacy_research/scripts/launch_bot.py</pre></div>\n\
    <h3>\n<a id=\"user-content-trained-weights-and-experiment-logs\" class=\"anchor\"\
    \ href=\"#trained-weights-and-experiment-logs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Trained weights and experiment\
    \ logs</h3>\n<p>To facilitate reproducibility, the experiments can be downloaded\
    \ using the following links. These include hyperparameters, tensorboard graphs,\
    \ output logs, and weights for each epoch.</p>\n<ul>\n<li>Order based LSTM model\
    \ (order-based v12 - Accuracy of 61.3% - <strong>DipNet SL</strong>) <a href=\"\
    https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/order-based-lstm.zip\"\
    \ rel=\"nofollow\">Download - 5.4GB</a>\n</li>\n<li>Order based Transformer model\
    \ (order-based v15 - Accuracy of 60.7%) <a href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/order-based-trsf.zip\"\
    \ rel=\"nofollow\">Download - 8.2GB</a>\n</li>\n<li>Token based LSTM model (token-based\
    \ v10 - Accuracy of 60.3%) <a href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/token-based-lstm.zip\"\
    \ rel=\"nofollow\">Download - 6.0GB</a>\n</li>\n<li>Token based Transformer model\
    \ (token-based v11 - Accuracy of 58.9%) <a href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/token-based-trsf.zip\"\
    \ rel=\"nofollow\">Download - 3.5GB</a>\n</li>\n<li>RL Model (Bootstrapped from\
    \ order-based v12 and value v1 - <strong>DipNet RL</strong>) <a href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/rl-model.zip\"\
    \ rel=\"nofollow\">Download - 11.1GB</a>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-games-against-albert-daide\"\
    \ class=\"anchor\" href=\"#games-against-albert-daide\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Games against\
    \ Albert (DAIDE)</h3>\n<p>The 1v6 and 6v1 games played between DipNet SL and Albert\
    \ (DAIDE) can be downloaded below:</p>\n<ul>\n<li>List of games with power assignments\
    \ <a href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/daide_albert_results.xlsx\"\
    \ rel=\"nofollow\">Download - 53KB</a>\n</li>\n<li>Visualisation of each game\
    \ (svg and json) <a href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/daide_albert_games.zip\"\
    \ rel=\"nofollow\">Download - 2.3GB</a>\n</li>\n</ul>\n"
  stargazers_count: 31
  subscribers_count: 13
  topics: []
  updated_at: 1620651034.0
dl-container-registry/container-cookiecutter:
  data_format: 2
  description: 'WIP: CookieCutter Docker/Singularity container project template'
  filenames:
  - '{{ cookiecutter.repo_name }}/Singularity'
  full_name: dl-container-registry/container-cookiecutter
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc-container-cookiecutter" class="anchor" href="#hpc-container-cookiecutter"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>HPC
    Container CookieCutter</h1>

    <p>Container template repo for building joint docker and singularity images. The

    template assumes the docker image contains the container, and the singularity

    file simply pulls down the docker image and converts it to a singularity

    container.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ul>

    <li>

    <p>Instantiate template using <code>cookiecutter</code>:

    <code>cookiecutter https://github.com/dl-container-registry/container-cookiecutter</code></p>

    </li>

    <li>

    <p>Create a <a href="https://github.com/new">new github repository</a> with the
    same name

    that you entered for <code>repo_name</code> in the template.</p>

    </li>

    <li>

    <p>Set up GitHub Auto-Deployment integration in settings.</p>

    <ul>

    <li>Github Token: Your GitHub token (obtained from <a href="https://github.com/settings/tokens">developer

    settings</a> with <code>repo_deployment</code> privileges)</li>

    <li>Environment: <code>singularity</code>

    </li>

    <li>Deploy on status: tick</li>

    <li>GitHub api url: empty</li>

    </ul>

    </li>

    <li>

    <p>Set up Singularity hub</p>

    <ul>

    <li>Create a <a href="https://www.singularity-hub.org/collections/new" rel="nofollow">new
    container collection</a>.</li>

    <li>Change settings:

    <ul>

    <li>Build Trigger: Deployment</li>

    </ul>

    </li>

    <li>Save.</li>

    <li>Note Singularity Hub ID, replace <code>SH_ID</code> with your ID in the README.</li>

    </ul>

    </li>

    <li>

    <p>Set up <a href="https://travis-ci.org/" rel="nofollow">Travis</a></p>

    <ul>

    <li>

    <a href="https://travis-ci.org/profile/" rel="nofollow">Enable travis on repository</a>.</li>

    <li>Click the cog next to the repository to go to the settings.</li>

    <li>Set <code>DOCKER_USERNAME</code> to your docker username.</li>

    <li>Set <code>DOCKER_PASSWORD</code> to your docker password (make sure you to
    quote the full

    password in case your password has special characters).</li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics:
  - cookiecutter
  - template
  - docker
  - singularity
  - container
  - hpc
  - travis
  updated_at: 1517774874.0
dl-container-registry/ffmpeg:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: dl-container-registry/ffmpeg
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-nvidia-accelerated-ffmpeg\" class=\"anchor\"\
    \ href=\"#nvidia-accelerated-ffmpeg\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>NVIDIA accelerated ffmpeg</h1>\n\
    <p><a href=\"https://travis-ci.org/dl-container-registry/ffmpeg\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/7976a6946b4090b514092b4f29d50f2b3ee4b012d5fa9f485f10552d81fe6284/68747470733a2f2f7472617669732d63692e6f72672f646c2d636f6e7461696e65722d72656769737472792f66666d7065672e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/dl-container-registry/ffmpeg.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/r/willprice/nvidia-ffmpeg/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/99bb6090faef97032d3bfd80b4d0cdb9d984e9e97aeb1d2750bc3e442fb117f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65726875622d3232623865622e737667\"\
    \ alt=\"Dockerhub link\" data-canonical-src=\"https://img.shields.io/badge/hosted-dockerhub-22b8eb.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/521\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"Singularity hub link\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-features\" class=\"\
    anchor\" href=\"#features\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Features</h2>\n<ul>\n<li><a href=\"https://developer.nvidia.com/nvidia-video-codec-sdk#NVENCFeatures\"\
    \ rel=\"nofollow\">NVENCODE acceleration</a></li>\n<li><a href=\"https://developer.nvidia.com/nvidia-video-codec-sdk#NVDECFeatures\"\
    \ rel=\"nofollow\">NVDECODE acceleration</a></li>\n<li><a href=\"https://www.videolan.org/developers/x264.html\"\
    \ rel=\"nofollow\">video codec: x264</a></li>\n<li><a href=\"https://www.videolan.org/developers/x265.html\"\
    \ rel=\"nofollow\">video codec: x265</a></li>\n<li><a href=\"https://github.com/mstorsjo/fdk-aac\"\
    >audio codec: AAC</a></li>\n</ul>\n<p>NVENCODE (nvenc) and NVDECODE (formerly\
    \ CUVID) are packaged in the <a href=\"https://developer.nvidia.com/nvidia-video-codec-sdk\"\
    \ rel=\"nofollow\">NVIDIA Video Codec\nSDK</a>.</p>\n<h3>\n<a id=\"user-content-hardware-accelerated-encoders\"\
    \ class=\"anchor\" href=\"#hardware-accelerated-encoders\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Hardware\
    \ Accelerated Encoders:</h3>\n<p>List options of an encoder using <code>ffmpeg\
    \ -h encoder=XXXX</code></p>\n<ul>\n<li>\n<code>h264_nvenc</code>, <code>nvenc</code>,\
    \ <code>nvenc_h264</code>\n</li>\n<li>\n<code>nvenc_hevc</code>, <code>hevc_nvenc</code>\n\
    </li>\n</ul>\n<h3>\n<a id=\"user-content-hardware-accelerated-decoders\" class=\"\
    anchor\" href=\"#hardware-accelerated-decoders\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Hardware Accelerated Decoders:</h3>\n\
    <p>List options of a decoder using <code>ffmpeg -h decoder=XXXX</code></p>\n<ul>\n\
    <li><code>h264_cuvid</code></li>\n<li><code>hevc_cuvid</code></li>\n<li><code>mjpeg_cuvid</code></li>\n\
    <li><code>mpeg1_cuvid</code></li>\n<li><code>mpeg2_cuvid</code></li>\n<li><code>mpeg4_cuvid</code></li>\n\
    <li><code>vc1_cuvid</code></li>\n<li><code>vp8_cuvid</code></li>\n<li><code>vp9_cuvid</code></li>\n\
    </ul>\n<h3>\n<a id=\"user-content-hardware-accelerated-filters\" class=\"anchor\"\
    \ href=\"#hardware-accelerated-filters\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Hardware Accelerated Filters:</h3>\n\
    <p>List options of a filter using <code>ffmpeg -h filter=XXXX</code></p>\n<ul>\n\
    <li><code>hwupload_cuda</code></li>\n<li><code>scale_cuda</code></li>\n<li><code>scale_npp</code></li>\n\
    <li><code>thumnail_cuda</code></li>\n</ul>\n<h2>\n<a id=\"user-content-usage\"\
    \ class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h2>\n<p>Run the container mounting\
    \ the current directory to <code>/workspace</code> processing\n<code>input.mp4</code>\
    \ to <code>output.mp4</code> without any hardware acceleration</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>$ docker run --rm -it --runtime=nvidia\
    \ \\\n    --volume <span class=\"pl-smi\">$PWD</span>:/workspace \\\n    willprice/nvidia-ffmpeg\
    \ -i input.mp4 output.avi</pre></div>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ docker run --rm -it --runtime=nvidia \\\n    --volume <span class=\"pl-smi\"\
    >$PWD</span>:/workspace \\\n    willprice/nvidia-ffmpeg \\\n      -hwaccel_device\
    \ 0 \\\n      -hwaccel cuvid \\\n      -c:v h264_cuvid \\\n      -i input.mp4\
    \ \\\n      -c:v hevc_nvenc\n      out.mkv</pre></div>\n<p>Get a shell prompt\
    \ inside the container, useful for debugging:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ docker run --rm -it --runtime=nvidia \\\n    --volume <span class=\"pl-smi\"\
    >$PWD</span>:/workspace \\\n    --entrypoint bash\n    willprice/nvidia-ffmpeg</pre></div>\n\
    <h2>\n<a id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Build</h2>\n\
    <p>The docker image is a multistage build. The initial stage, the <em>build</em>\
    \ stage, builds a statically linked ffmpeg binary\nthat is then copied over into\
    \ the runtime image. By statically linking we minimize the number of external\
    \ dependencies\nand shrink the runtime image.</p>\n<h2>\n<a id=\"user-content-resources\"\
    \ class=\"anchor\" href=\"#resources\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Resources</h2>\n<ul>\n<li><a\
    \ href=\"https://trac.ffmpeg.org/wiki/HWAccelIntro\" rel=\"nofollow\">FFmpeg hardware\
    \ acceleration guide with examples</a></li>\n<li><a href=\"https://gist.github.com/jniltinho/9c009e9771651aa4a004ad3d1f6857e3\"\
    >Static FFmpeg build on Ubuntu 16.04 guide</a></li>\n<li><a href=\"https://gist.github.com/Brainiarc7/18fca697891aea0e879f13ed092cb213\"\
    >Using FFmpeg with GNU parallel</a></li>\n<li><a href=\"https://gist.github.com/Brainiarc7/c6164520f082c27ae7bbea9556d4a3ba\"\
    >Listing NVENC and NPP capabilities of FFmpeg</a></li>\n<li><a href=\"https://gist.github.com/Brainiarc7/8b471ff91319483cdb725f615908286e\"\
    >Encoding HEVC using FFmpeg with NVENC</a></li>\n<li><a href=\"https://gist.github.com/Brainiarc7/ebf3091efd2bf0a0ded0f9715cd43a38\"\
    >FFmpeg cheatsheet</a></li>\n<li><a href=\"https://github.com/zimbatm/ffmpeg-static\"\
    >FFmpeg-static build scripts</a></li>\n</ul>\n"
  stargazers_count: 6
  subscribers_count: 3
  topics: []
  updated_at: 1612197004.0
dl-container-registry/furnari-flow:
  data_format: 2
  description: Antonino Furnari's fork of Feichtenhofer's gpu_flow, with temporal
    dilation.
  filenames:
  - Singularity
  full_name: dl-container-registry/furnari-flow
  latest_release: null
  readme: '<h1>

    <a id="user-content-gpu-based-optical-flow-extraction-from-videos" class="anchor"
    href="#gpu-based-optical-flow-extraction-from-videos" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GPU based optical flow
    extraction from videos</h1>

    <p>Forked from <a href="https://github.com/feichtenhofer/gpu_flow">https://github.com/feichtenhofer/gpu_flow</a>
    by Antonino Furnari</p>

    <p><a href="https://travis-ci.org/dl-container-registry/furnari-flow" rel="nofollow"><img
    src="https://camo.githubusercontent.com/22af16742ba115e53d8c72ecae46310b24dacb32e78ec3f7172c231c7cbc7c73/68747470733a2f2f7472617669732d63692e6f72672f646c2d636f6e7461696e65722d72656769737472792f6675726e6172692d666c6f772e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/dl-container-registry/furnari-flow.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/willprice/furnari-flow/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/99bb6090faef97032d3bfd80b4d0cdb9d984e9e97aeb1d2750bc3e442fb117f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65726875622d3232623865622e737667"
    alt="Docker Hub" data-canonical-src="https://img.shields.io/badge/hosted-dockerhub-22b8eb.svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/575" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="Singularity Hub" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-news" class="anchor" href="#news" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>News</h2>

    <h3>

    <a id="user-content-2020-01-09" class="anchor" href="#2020-01-09" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>2020-01-09</h3>

    <p>The semantics of the dilation parameter have changed to allow finer grained
    configuration. Previously optical flow was

    computed between frames I_{st} and I_{s(t+d)} where s is the stride and d the
    dilation. The code now computes flow

    between I_{st} and I_{st+d}--this makes the stride and dilation parameters completely
    independent which is more intuitive.

    If you wish to continue using the old code then use the docker image tagged with
    <code>v1</code>. All subsequent images and the

    <code>latest</code> tag will adopt the new behaviour described above.</p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>We support running via docker and singularity.</p>

    <h3>

    <a id="user-content-docker" class="anchor" href="#docker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Docker</h3>

    <ul>

    <li>Ensure you''re running

    <a href="https://github.com/NVIDIA/nvidia-docker"><code>nvidia-docker</code></a>
    as this software is

    GPU accelerated. If using docker 19.03 or above then you can use the native docker
    nvidia GPU support.</li>

    <li>Pull the docker image:

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">docker
    pull willprice/furnari-flow</span></pre></div>

    </li>

    <li>Dump out frames from the video you wish to compute flow for:

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">mkdir
    my_video<span class="pl-k">;</span> ffmpeg -i my_video.mp4 -qscale 3 my_video/img_%06d.jpg</span></pre></div>

    </li>

    <li>Compute the flow using <code>furnari-flow</code>:

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">mkdir
    my_video_flow</span>

    $ <span class="pl-s1">docker run \</span>

    <span class="pl-c1">    --runtime=nvidia \</span>

    <span class="pl-c1">    --rm \</span>

    <span class="pl-c1">    --mount "type=bind,source=$PWD/my_video,target=/input"
    \</span>

    <span class="pl-c1">    --mount "type=bind,source=$PWD/my_video_flow,target=/output"
    \</span>

    <span class="pl-c1">    --mount "type=bind,source=$HOME/.nv,target=/cache/nv"
    \</span>

    <span class="pl-c1">    willprice/furnari-flow \</span>

    <span class="pl-c1">    img_%06d.jpg</span>

    $ <span class="pl-s1">ls my_video_flow</span>

    <span class="pl-c1">u v</span>

    $ <span class="pl-s1">ls my_video_flow/u</span>

    <span class="pl-c1">img_0000001.jpg</span>

    <span class="pl-c1">img_0000002.jpg</span>

    <span class="pl-c1">...</span></pre></div>

    </li>

    </ul>

    <h3>

    <a id="user-content-details" class="anchor" href="#details" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Details</h3>

    <p>The software assumes that all video frames have been extracted in a directory.
    Files should be named according to some pattern, e.g., <code>img_%07d.jpg</code>.
    The software will put flow files in the same directory using a provided filename
    pattern, e.g., <code>flow_%s_%07d.jpg</code>, where the %s will be subsituted
    with "x" for the x flows and "y" for the y flows. For example, if DIR is a directory
    containing 4 images:</p>

    <p>DIR:</p>

    <ul>

    <li><code>img_0000001.jpg</code></li>

    <li><code>img_0000002.jpg</code></li>

    <li><code>img_0000003.jpg</code></li>

    <li><code>img_0000004.jpg</code></li>

    </ul>

    <p>the command <code>compute_flow DIR img_%07d.jpg flow_%s_%07d.jpg</code> will
    read the images in order and compute optical flows. The content of DIR will be
    as follows after the execution of the command:</p>

    <p>DIR:</p>

    <ul>

    <li><code>img_0000001.jpg</code></li>

    <li><code>img_0000002.jpg</code></li>

    <li><code>img_0000003.jpg</code></li>

    <li><code>img_0000004.jpg</code></li>

    <li><code>flow_x_0000001.jpg</code></li>

    <li><code>flow_x_0000002.jpg</code></li>

    <li><code>flow_x_0000003.jpg</code></li>

    <li><code>flow_x_0000004.jpg</code></li>

    <li><code>flow_y_0000001.jpg</code></li>

    <li><code>flow_y_0000002.jpg</code></li>

    <li><code>flow_y_0000003.jpg</code></li>

    <li><code>flow_y_0000004.jpg</code></li>

    </ul>

    <p>where <code>flow_x_{n}.jpg</code> is the x flow computed between <code>img_{n}.jpg</code>
    and <code>img_{n+1}.jpg</code> (if no dilation is used - see help).</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You only need to build this software if you intend on tweaking the source,
    otherwise you

    should just use the pre-built docker images.</p>

    <h3>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies:</h3>

    <ul>

    <li><a href="http://opencv.org/downloads.html" rel="nofollow">OpenCV 2.4</a></li>

    <li><a href="https://cmake.org/" rel="nofollow">cmake</a></li>

    </ul>

    <h3>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h3>

    <p>First, build opencv with gpu support. To do so, download opencv 2.4.x sources

    from <a href="https://opencv.org/releases.html" rel="nofollow">https://opencv.org/releases.html</a>.
    Unzip the downloaded archive, then enter

    the opencv folder and issue the following commands:</p>

    <ul>

    <li><code>mkdir build</code></li>

    <li><code>cd build</code></li>

    <li>

    <code>cmake -DCUDA_USE_STATIC_CUDA_RUNTIME=OFF ..</code> (inspect the <a href="./Dockerfile"><code>Dockerfile</code></a>
    for further flags that might

    be of use)</li>

    <li><code>make -j $(nproc)</code></li>

    </ul>

    <p>Then clone the current repository and enter the <code>compute_flow_video</code>
    folder. Type:</p>

    <ul>

    <li><code>export OpenCV_DIR=path_to_opencv_build_directory</code></li>

    <li><code>mkdir build</code></li>

    <li><code>cd build</code></li>

    <li><code>cmake -DCUDA_USE_STATIC_CUDA_RUNTIME=OFF ..</code></li>

    <li><code>make -j $(nproc)</code></li>

    </ul>

    '
  stargazers_count: 7
  subscribers_count: 1
  topics: []
  updated_at: 1600461582.0
dl-container-registry/gpu-flow:
  data_format: 2
  description: Feichtenhofer's gpu_flow
  filenames:
  - Singularity
  full_name: dl-container-registry/gpu-flow
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-gpu_flow-container\" class=\"anchor\" href=\"\
    #gpu_flow-container\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a><a href=\"https://github.com/uob-epic/gpu_flow\"\
    >gpu_flow</a> container</h1>\n<h2>\n<a id=\"user-content-details\" class=\"anchor\"\
    \ href=\"#details\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Details</h2>\n<ul>\n<li>CUDA 8.0</li>\n<li>OpenCV\
    \ 2.4.13.4 built from <code>opencv2-cuda8</code> container.</li>\n<li>Qt5</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <pre><code>$ docker run --runtime=nvidia --rm -t willprice/gpu_flow -h\n\nUsage:\
    \ /src/gpu_flow_build/compute_flow [options]\nAvaible options:\n  -f, --type=[1]\
    \                flow method\n  -g, --gpuID=[1]               gpu ID\n  -h, --help=[true]\
    \             print help message\n  -i, --in_dir                  input directory\
    \ of videos\n  -j, --frame_root_dir=[frames] path to directory to store jpegs\n\
    \  -o, --flow_root_dir=[flow]    path to directory to store flow\n  -s, --skip=[1]\
    \                frame skip\n  -v, --start_video=[1]         start video ID\n\n\
    $ docker run --runtime=nvidia --rm -t willprice/gpu_flow \\\n    --gpuID=0 \\\n\
    \    --in_dir=$HOME/input_videos \\\n    --frame_root_dir=$HOME/output/frames\
    \ \\\n    --flow_root_dir=$HOME/output/flow\n</code></pre>\n"
  stargazers_count: 2
  subscribers_count: 0
  topics: []
  updated_at: 1555106449.0
dl-container-registry/opencv2:
  data_format: 2
  description: OpenCV 2 built with NVIDIA acceleration
  filenames:
  - Singularity
  full_name: dl-container-registry/opencv2
  latest_release: null
  readme: '<h1>

    <a id="user-content-opencv2-dockerfile" class="anchor" href="#opencv2-dockerfile"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OpenCV2
    Dockerfile</h1>

    <p><a href="https://circleci.com/gh/dl-container-registry/opencv2" rel="nofollow"><img
    src="https://camo.githubusercontent.com/4cb047fbd7f8a38c0fd7c817c86e59892ff4db6f64e32a0fe2f310b40905dbd8/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f70726f6a6563742f6769746875622f646c2d636f6e7461696e65722d72656769737472792f6f70656e6376322f6d61737465722e737667"
    alt="CircleCI branch" data-canonical-src="https://img.shields.io/circleci/project/github/dl-container-registry/opencv2/master.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/willprice/opencv2-cuda8/~/settings/" rel="nofollow"><img
    src="https://camo.githubusercontent.com/99bb6090faef97032d3bfd80b4d0cdb9d984e9e97aeb1d2750bc3e442fb117f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65726875622d3232623865622e737667"
    alt="Dockerhub link" data-canonical-src="https://img.shields.io/badge/hosted-dockerhub-22b8eb.svg"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/530" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="Singularity hub hosted" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>Build other containers from this base image, it contains a prebuilt version
    of

    OpenCV 2.4.13.4 at <code>/src/opencv_build</code> installed to <code>/usr/local/OpenCV</code>

    (containing CMake files for building other projects).</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1589337912.0
dmorrill10/hr_edl_experiments:
  data_format: 2
  description: Experiments, data, and results for MAL and general sum game experiments
    with CFR.
  filenames:
  - hr_edl/Singularity.def
  full_name: dmorrill10/hr_edl_experiments
  latest_release: null
  readme: '<h1>

    <a id="user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    class="anchor" href="#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hindsight
    Rationality, and Efficient Deviation Types and Learning Experiments</h1>

    <p>Experiment code for <a href="https://arxiv.org/abs/2012.05874" rel="nofollow">Hindsight
    and Sequential Rationality of Correlated Play</a> and <a href="https://arxiv.org/abs/2102.06973"
    rel="nofollow">Efficient Deviation Types and Learning for Hindsight Rationality
    in Extensive-Form Games</a> conference papers (AAAI-21 and ICML 2021, respectively).</p>

    <p>This repository has a number of different components that work together to
    generate the experimental results.</p>

    <p>The pipeline begins with <code>hr_edl</code>. This is C++ code built over <a
    href="https://github.com/deepmind/open_spiel">OpenSpiel</a> that defines the experiments.
    See <a href="hr_edl/README.md"><code>hr_edl/README.md</code></a> for more information.</p>

    <p>The <code>hr_edl</code> code allows us to run experiments, but experiments
    are run with help from the Python3 library, <code>hr_edl_data</code>. Run <code>pip
    install .</code> to install it.

    With <code>hr_edl_data</code> installed, you can run <code>bin/run_experiment.py</code>
    to run an experiment.</p>

    <p>The experiment configurations used in the papers are defined in <code>Makefile</code>.
    Assuming that <code>hr_edl_data</code> is installed, running <code>make</code>
    should compile <code>hr_edl</code> and run all experiments, updating data files
    in <code>data</code> as necessary, and depositing Numpy data files in <code>results</code>.</p>

    <p>Finally, the Python Jupyter notebooks in <code>notebooks</code> process the
    Numpy data files into the final results, which are also saved in <code>results</code>.</p>

    <h2>

    <a id="user-content-virtual-machines-and-containers" class="anchor" href="#virtual-machines-and-containers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Virtual
    Machines and Containers</h2>

    <p>A <a href="https://www.vagrantup.com/" rel="nofollow">Vagrant</a> virtual machine
    configuration to help run these experiments is defined in <code>Vagrantfile</code>.

    If you already have Python3 installed though, you may not need to use it.

    Typically, the most onerous part of the installation procedure is building <code>hr_edl</code>
    and its dependencies.

    For this, you can use the <a href="https://sylabs.io/" rel="nofollow">Singularity</a>
    container defined by <code>hr_edl/Singularity.def</code>.

    Once you have Singularity installed or you are using the Vagrant virtual machine,
    you can run</p>

    <p><code>sudo singularity build ./hr_edl.sif Singularity.def</code></p>

    <p>to create a Singularity image. Then you can run</p>

    <p><code>singularity exec hr_edl.sif /code/build.optimized/bin/&lt;command&gt;
    [command options]</code></p>

    <p>to run an <code>hr_edl</code> executable inside the container.

    <code>bin/run_experiment.py</code> has a <code>--sif</code> option so you can
    specify a container image in which the experiment should be run.

    You can set the variable <code>SIF</code> and <code>HR_EDL_DIR =/code</code> in
    <code>Makefile</code> (either in the file or in the command like <code>make SIF=my_image.sif
    HR_EDL_DIR=/code</code>) to run all experiments in a given container image.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621058061.0
duke-chsi-informatics/singularity-rnaseq:
  data_format: 2
  description: Singularity Image for RNA-Seq analysis
  filenames:
  - Singularity
  - Singularity.test_jupyter
  full_name: duke-chsi-informatics/singularity-rnaseq
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"\
    #singularity-rnaseq\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-rnaseq</h1>\n<h2>\n<a id=\"user-content-running-jupyter\"\
    \ class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running Jupyter</h2>\n<p>Run\
    \ this to start Jupyter:</p>\n<pre><code>singularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\
    </code></pre>\n<p>Then follow the instructions that Jupyter printed to the terminal\
    \ when you started it up to access Jupyter in your web browser</p>\n<h3>\n<a id=\"\
    user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Accessing Jupyter on a remote server</h3>\n<p>If you are running the\
    \ container on a remote server, you will need to set up port forwarding with ssh\
    \ to be able to access Jupyter.  Run this command to forward the default Jupyter\
    \ port (8888)</p>\n<pre><code>ssh -L 8888:localhost:8888 bug\n</code></pre>\n\
    <blockquote>\n<p>Note if the default Jupyter port is not available, Jupyter will\
    \ choose a different port.  In this case you will need to substitute the port\
    \ that Jupyter outputs for 8888 in the ssh port forwarding command above.</p>\n\
    </blockquote>\n<h2>\n<a id=\"user-content-running-on-a-slurm-cluster\" class=\"\
    anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Running on a SLURM Cluster</h2>\n\
    <p>You can use this image interactively on a SLURM-managed cluster by running\
    \ launching RStudio or Jupyter. The following instructions work on the Duke Compute\
    \ Cluster (DCC).  Doing this on other cluster will require some modification and\
    \ may not work, depending on how the cluster is configured.</p>\n<h3>\n<a id=\"\
    user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RStudio</h3>\n\
    <ol>\n<li>ssh to DCC login node: <code>ssh NETID@dcc-login-01.rc.duke.edu</code>\n\
    </li>\n<li>run tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n\
    <li>Run this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty\
    \ bash -i</code>\n</li>\n<li>Run <code>hostname -A</code> on compute node and\
    \ record results</li>\n<li>Run on the following on a compute node and note the\
    \ port, username, and password that the command prints:</li>\n</ol>\n<pre><code>mkdir\
    \ -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\n\
    singularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind\
    \ /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\
    </code></pre>\n<ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the port returned\
    \ but the \"singularity run\" commmand</li>\n<li>Where COMPUTE_HOSTNAME is the\
    \ hostname returned by running \"hostname -A\" on the compute node</li>\n<li>Where\
    \ NETID is your NetID</li>\n</ul>\n</li>\n<li>Go to \"localhost:PORT\" in a webrowser\
    \ and enter the username and password printed by the \"singularity run\" commmand</li>\n\
    <li>Have fun!!</li>\n<li>At the end of an analysis you will probably want to copy\
    \ results to your directory in <code>/work</code> or <code>/hpc/group</code>\n\
    </li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Jupyter</h3>\n<ol>\n<li>ssh to dcc-login-01.rc.duke.edu</li>\n<li>run\
    \ tmux on login node: <code>tmux new -s container_demo</code>\n</li>\n<li>Run\
    \ this on login node: <code>srun -A chsi -p chsi --mem=100G -c 30 --pty bash -i</code>\n\
    </li>\n<li>Run on compute node:</li>\n</ol>\n<pre><code>mkdir -p /scratch/josh/rnaseq_demo/rawdata\
    \ /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\
    \n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace\
    \ \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n</code></pre>\n\
    <ol start=\"6\">\n<li>Run on local machine: <code>ssh -L PORT:COMPUTE_HOSTNAME:PORT\
    \ NETID@dcc-login-01.rc.duke.edu</code>\n<ul>\n<li>Where PORT is the number after\
    \ <code>http://127.0.0.1:</code> in the URL given by Jupyter (defaults to 8888,\
    \ but Jupyter will use a different one if the default is in use, or if a different\
    \ port is supplied as an argument using <code>--port</code> when running the singularity\
    \ container</li>\n<li>Where COMPUTE_HOSTNAME is the hostname returned by running\
    \ \"hostname -A\" on the compute node</li>\n<li>Where NETID is your NetID</li>\n\
    </ul>\n</li>\n<li>Copy the URL supplied by jupyter that starts <code>http://127.0.0.1:</code>\
    \ and paste it in a webbrowser</li>\n<li>Have fun!!</li>\n<li>At the end of an\
    \ analysis you will probably want to copy results to your directory in <code>/work</code>\
    \ or <code>/hpc/group</code>\n</li>\n</ol>\n<h3>\n<a id=\"user-content-jupyter-on-gpu-node\"\
    \ class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Jupyter on GPU node</h3>\n<p>Same\
    \ as above, but the srun command should use the <code>chsi-gpu</code> partition\
    \ and request a gpu, but less CPUs and Memory:</p>\n<p><code>srun -A chsi -p chsi-gpu\
    \ --gres=gpu:1 --mem=15866 -c 2 --pty bash -i</code></p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1619748236.0
dune-universe/dune-universe:
  data_format: 2
  description: All opam packages using dune
  filenames:
  - packages/bistro.0.5.0/etc/singularity-images/bowtie/1.1.2/Singularity
  full_name: dune-universe/dune-universe
  latest_release: null
  readme: '<h1>

    <a id="user-content-dune-universe" class="anchor" href="#dune-universe" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dune universe</h1>

    <p>This repository contains a snapshot of the latest versions of all opam

    packages depending on Dune (Jbuilder). It is updated daily. It is used

    to analyse the usage of Dune in opam.</p>

    '
  stargazers_count: 21
  subscribers_count: 7
  topics: []
  updated_at: 1621838498.0
dvav/singularity-rstudio-server:
  data_format: 2
  description: A Singularity container with R and Rstudio
  filenames:
  - Singularity
  full_name: dvav/singularity-rstudio-server
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rstudio-server\" class=\"anchor\"\
    \ href=\"#singularity-rstudio-server\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>singularity-rstudio-server</h1>\n\
    <p>This is a <code>singularity</code> definition file and associated files for\
    \ building a container\nwith the most recent (at the time of this writing) version\
    \ of <code>R</code> and <code>Rstudio Server</code>.</p>\n<p>To build the container,\
    \ use the following command:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity build rstudio.sif Singularity</pre></div>\n<p>To run the\
    \ container, do the following:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity run --bind {host_dir1}:/var/lib/,{host_dir2}:/var/run rstudio.sif\
    \ --www-port {your port}</pre></div>\n<p>You can run the container on <code>humbug</code>\
    \ (if you don't know what this is, skip this paragraph) using the following:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity run --bind /well,/gpfs0,/gpfs1,/gpfs2,{host_dir1}:/var/lib/,{host_dir2}:/var/run\
    \ rstudio.sif --www-port {remote port}</pre></div>\n<p>and setting up an <code>ssh</code>\
    \ tunnel from your local machine (e.g. your laptop), for example:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>ssh -N -f -L {local port}:localhost:{remote\
    \ port} {your username}@humbug.well.ox.ac.uk</pre></div>\n<p>You need to be connected\
    \ to the centre's VPN for this work.</p>\n<p>The following, does not work (yet):</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>RSTUDIO_PASSWORD=<span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>password<span class=\"pl-pds\">\"</span></span>\
    \ \\\n  singularity run --bind {host_dir1}:/var/lib/,{host_dir2}:/var/run rstudio.sif\
    \ \\\n    --auth-none 0 \\\n    --auth-pam-helper rstudio_auth</pre></div>\n<p>For\
    \ more info, check the definition file <code>Singularity</code>.</p>\n<p>I used\
    \ code from <a href=\"https://github.com/nickjer/singularity-rstudio\">https://github.com/nickjer/singularity-rstudio</a>\
    \ as a point of departure. Bits of this code still remain in this repository.</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1612500240.0
eliasarnold/image-generator-for-BScThesis:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: eliasarnold/image-generator-for-BScThesis
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-parametric-face-image-generatorextension-by-arneli00\"\
    \ class=\"anchor\" href=\"#parametric-face-image-generatorextension-by-arneli00\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>parametric-face-image-generator(extension by arneli00)</h1>\n<p>This\
    \ software is based on the parametric-face-image-generator of the gravis group.\
    \ The <a href=\"https://github.com/unibas-gravis/parametric-face-image-generator\"\
    >original Software</a> enables you to generate fully parametric face images from\
    \ the Basel Face Model 2017 as proposed in:</p>\n<ul>\n<li>\n<p>[1] Adam Kortylewski,\
    \ Andreas Schneider, Thomas Gerig, Bernhard Egger, Andreas Morel-Forster and Thomas\
    \ Vetter\n<a href=\"https://arxiv.org/abs/1802.05891\" rel=\"nofollow\">\"Training\
    \ Deep Face Recognition Systems with Synthetic Data\"</a>,\nIN: arXiv preprint\
    \ (2018)</p>\n</li>\n<li>\n<p>[2] Adam Kortylewski, Bernhard Egger, Andreas Schneider,\
    \ Thomas Gerig, Andreas Morel-Forster and Thomas Vetter\n<a href=\"https://arxiv.org/abs/1712.01619\"\
    \ rel=\"nofollow\">\"Empirically Analyzing the Effect of Dataset Biases on Deep\
    \ Face Recognition Systems\"</a>,\nIN: arXiv preprint (2017)</p>\n</li>\n</ul>\n\
    <p>You can control the variation of parameters such as pose, shape, color, camera\
    \ and illumination based on your demand and application.\nThis dataset can be\
    \ used for training and comparing machine learning techniques such as CNNs on\
    \ a common ground as proposed in [1] by generating fully controlled training and\
    \ test data.</p>\n<h2>\n<a id=\"user-content-whats-new\" class=\"anchor\" href=\"\
    #whats-new\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>WHAT'S NEW</h2>\n<p>This version produces two images of the same face\
    \ at a time. For the first image, it uses the Basel Face Model \"face12\" and\
    \ for the other image the \"bfm\" version. This version only works for three occlusionModes.\
    \ These are:</p>\n<h3>\n<a id=\"user-content-random\" class=\"anchor\" href=\"\
    #random\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>random</h3>\n<p>This mode renders an arbitrary image of a hand or\
    \ a microphone over the output image</p>\n<h3>\n<a id=\"user-content-random-1\"\
    \ class=\"anchor\" href=\"#random-1\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>random-1</h3>\n<p>This mode renders\
    \ an image of a hand over the image</p>\n<h3>\n<a id=\"user-content-random-2\"\
    \ class=\"anchor\" href=\"#random-2\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>random-2</h3>\n<p>This mode renders\
    \ an image of a microphone over the image</p>\n<h3>\n<a id=\"user-content-box\"\
    \ class=\"anchor\" href=\"#box\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>box</h3>\n<p>This mode occludes the\
    \ image with a rectangle at a random position, with random shape and random color</p>\n\
    <h3>\n<a id=\"user-content-none\" class=\"anchor\" href=\"#none\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>none</h3>\n\
    <p>If you don't want an occlusion to be added to the image, specify this option</p>\n\
    <h2>\n<a id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>OUTPUT</h2>\n\
    <p>This modified version provides two new folders in the output (in this version,\
    \ the output is split in two: output/bfm and output/face12):</p>\n<h3>\n<a id=\"\
    user-content-img_masks\" class=\"anchor\" href=\"#img_masks\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>img_masks</h3>\n\
    <p>These images provide the ground truth segmentation of the image. It distinguishes\
    \ between face-region, occlusion-region, and background.</p>\n<h3>\n<a id=\"user-content-img_occlusion\"\
    \ class=\"anchor\" href=\"#img_occlusion\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>img_occlusion</h3>\n<p>The original\
    \ images overlaid with an occlusion of one of the above modes.</p>\n<h2>\n<a id=\"\
    user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Contributors</h2>\n\
    <ul>\n<li>Bernhard Egger</li>\n<li>Adam Kortylewski</li>\n<li>Andreas Morel-Forster</li>\n\
    <li>Andreas Schneider</li>\n</ul>\n<h2>\n<a id=\"user-content-maintainers\" class=\"\
    anchor\" href=\"#maintainers\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Maintainers</h2>\n<ul>\n<li>University\
    \ of Basel, Graphics and Vision research: <a href=\"https://github.com/unibas-gravis\"\
    >@unibas-gravis</a>, <a href=\"http://gravis.cs.unibas.ch\" rel=\"nofollow\">homepage</a>\n\
    </li>\n</ul>\n<h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>License</h2>\n<p><a href=\"https://www.apache.org/licenses/LICENSE-2.0\"\
    \ rel=\"nofollow\">Apache License, Version 2.0</a>, details see LICENSE</p>\n\
    <pre><code>Copyright 2017, University of Basel, Graphics and Vision Research\n\
    \nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not\
    \ use this file except in compliance with the License.\nYou may obtain a copy\
    \ of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless\
    \ required by applicable law or agreed to in writing, software\ndistributed under\
    \ the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS\
    \ OF ANY KIND, either express or implied.\nSee the License for the specific language\
    \ governing permissions and\nlimitations under the License.\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620184864.0
elimoss/lathe:
  data_format: 2
  description: A tool for generating bacterial genomes from metagenomes with nanopore
    long read sequencing
  filenames:
  - singularity/Singularity.longread
  - singularity/Singularity.htsbox
  - singularity/Singularity.quickmerge
  full_name: elimoss/lathe
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-lathe\" class=\"anchor\" href=\"#lathe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Lathe</h1>\n\
    <p>A tool for generating bacterial genomes from metagenomes with Nanopore long\
    \ read sequencing</p>\n<h1>\n<a id=\"user-content-installation\" class=\"anchor\"\
    \ href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Installation</h1>\n<p>First, install <a href=\"\
    https://conda.io/en/latest/miniconda.html\" rel=\"nofollow\">miniconda3</a></p>\n\
    <p>Then install snakemake.  This can be done with the following.</p>\n<pre><code>conda\
    \ install snakemake\nsnakemake --version #please ensure this is &gt;=5.4.3\n</code></pre>\n\
    <p>Next, clone this github directory to some location where it can be stored permanently.\
    \  Remember to keep it updated with <code>git pull</code>.</p>\n<pre><code>git\
    \ clone https://github.com/elimoss/lathe.git\n</code></pre>\n<p>Instructions to\
    \ enable cluster execution with SLURM can be found at <a href=\"https://github.com/bhattlab/slurm\"\
    >https://github.com/bhattlab/slurm</a></p>\n<h2>\n<a id=\"user-content-inputs\"\
    \ class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Inputs</h2>\n<h3>\n<a id=\"user-content-alter-configyaml-to-provide-the-following\"\
    \ class=\"anchor\" href=\"#alter-configyaml-to-provide-the-following\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Alter\
    \ config.yaml to provide the following:</h3>\n<ul>\n<li>\n<p><strong>sample_name</strong>:\
    \ Name of sample and output directory</p>\n</li>\n<li>\n<p><strong>fast5_dirs_list</strong>:\
    \ text file containing a list of absolute paths to run/fast5/* subfolders containing\
    \ .fast5 files.  A good way to generate this is with <code>find -maxdepth 2 -mindepth\
    \ 2 fast5_parent_dir &gt; fodn.txt</code></p>\n</li>\n<li>\n<p><strong>flowcell</strong>:\
    \ flowcell code, e.g. FLO-MIN106, passed to basecaller</p>\n</li>\n<li>\n<p><strong>kit</strong>:\
    \ kit code, e.g. SQK-LSK109, passed to basecaller</p>\n</li>\n<li>\n<p><strong>genome_size</strong>:\
    \ Estimated genome size, e.g. 50m, passed to Canu.</p>\n</li>\n<li>\n<p><strong>singularity</strong>:\
    \ location (including on the internet) of a singularity image to be used for the\
    \ workflow.  Don't change this.</p>\n</li>\n<li>\n<p><strong>short_reads</strong>:\
    \ location of short reads to be used for Pilon polishing, or empty quotes for\
    \ long-read polishing.</p>\n</li>\n<li>\n<p><strong>use_grid</strong>: should\
    \ Canu execute in distributed mode on a cluster?</p>\n</li>\n<li>\n<p><strong>grid_options</strong>:\
    \ Extra options for execution on a cluster</p>\n</li>\n<li>\n<p><strong>canu_args</strong>:\
    \ Extra options for Canu</p>\n</li>\n<li>\n<p><strong>skip_circularization</strong>:\
    \ Should circularization be omitted from the workflow?</p>\n</li>\n</ul>\n<p>Lathe\
    \ uses the Flye assembler by default. For Canu, please specify 'canu' for the\
    \ assembler parameter in the config. For cluster Canu execution, please note:\
    \ if set to True, you will need to install Canu, e.g. <code>conda install -c conda-forge\
    \ -c bioconda Canu=1.8</code> as well as provide any additional required parameters\
    \ for your job scheduler in the config.yaml file.  Please see the example config\
    \ file. When executing on a cluster, Canu will appear to fail, as the first process\
    \ does not produce an assembly and instead spawns subsequent jobs on the cluster.\
    \  Don't worry, just re-run Lathe when the assembly completes.</p>\n<p>To execute\
    \ please run the following.  Please note, you must substitute a parent directory\
    \ containing all of your data and working directories for <code>/labs/</code>.</p>\n\
    <pre><code>snakemake --use-singularity --singularity-args '--bind /labs/ --bind\
    \ /scratch/ --bind /scg/ ' -s /path/to/lathe/Snakefile \\\n--configfile path/to/modified_config.yaml\
    \ --restart-times 0 --keep-going --latency-wait 30\n# --profile scg #enable cluster\
    \ support, highly recommended.  See above.\n</code></pre>\n<h2>\n<a id=\"user-content-outputs\"\
    \ class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Outputs</h2>\n<p>The outputs\
    \ generated by this workflow will look like the following:</p>\n<pre><code>samplename/\n\
    \u251C\u2500\u2500 0.basecall\n\u2502\_\_ \u251C\u2500\u2500 samplename.fq\n\u2502\
    \_\_ \u2514\u2500\u2500 nanoplots\n\u251C\u2500\u2500 1.assemble\n\u2502\_\_ \u251C\
    \u2500\u2500 samplename_merged.fasta\n\u2502\_\_ \u251C\u2500\u2500 samplename_raw_assembly.fa\n\
    \u2502\_\_ \u251C\u2500\u2500 samplename_raw_assembly.fa.amb\n\u2502\_\_ \u251C\
    \u2500\u2500 samplename_raw_assembly.fa.ann\n\u2502\_\_ \u251C\u2500\u2500 samplename_raw_assembly.fa.bwt\n\
    \u2502\_\_ \u251C\u2500\u2500 samplename_raw_assembly.fa.fai\n\u2502\_\_ \u251C\
    \u2500\u2500 samplename_raw_assembly.fa.pac\n\u2502\_\_ \u251C\u2500\u2500 samplename_raw_assembly.fa.paf\n\
    \u2502\_\_ \u251C\u2500\u2500 samplename_raw_assembly.fa.sa\n\u2502\_\_ \u251C\
    \u2500\u2500 assemble_100m (if specified)\n\u2502\_\_ \u2514\u2500\u2500 assemble_250m\
    \ (if specified)\n\u251C\u2500\u2500 2.polish\n\u2502\_\_ \u251C\u2500\u2500 samplename_polished.corrected.fasta\n\
    \u2502\_\_ \u251C\u2500\u2500 samplename_polished.fasta\n\u2502\_\_ \u251C\u2500\
    \u2500 samplename_polished.fasta.bam\n\u2502\_\_ \u251C\u2500\u2500 samplename_polished.fasta.bam.bai\n\
    \u2502\_\_ \u251C\u2500\u2500 samplename_polished.fasta.fai\n\u2502\_\_ \u251C\
    \u2500\u2500 samplename_polished.fasta.misassemblies.tsv\n\u2502\_\_ \u251C\u2500\
    \u2500 medaka (if specified)\n\u2502\_\_ \u251C\u2500\u2500 pilon (if specified)\n\
    \u2502\_\_ \u2514\u2500\u2500 racon (if specified)\n\u251C\u2500\u2500 3.circularization\n\
    \u2502\_\_ \u251C\u2500\u2500 1.candidate_genomes\n\u2502\_\_ \u251C\u2500\u2500\
    \ 2.circularization\n\u2502\_\_ \u251C\u2500\u2500 3.circular_sequences #circularized\
    \ genomes\n\u2502\_\_ \u251C\u2500\u2500 4.samplename_circularized.corrected.fasta\n\
    \u2502\_\_ \u251C\u2500\u2500 4.samplename_circularized.fasta\n\u2502\_\_ \u251C\
    \u2500\u2500 4.samplename_circularized.fasta.bam\n\u2502\_\_ \u251C\u2500\u2500\
    \ 4.samplename_circularized.fasta.bam.bai\n\u2502\_\_ \u251C\u2500\u2500 4.samplename_circularized.fasta.fai\n\
    \u2502\_\_ \u2514\u2500\u2500 4.samplename_circularized.fasta.misassemblies.tsv\n\
    \u2514\u2500\u2500 5.final\n \_\_ \u251C\u2500\u2500 samplename_final.fa\n \_\_\
    \ \u2514\u2500\u2500 samplename_final.fa.fai\n</code></pre>\n"
  stargazers_count: 7
  subscribers_count: 0
  topics: []
  updated_at: 1619198094.0
fabianrost84/singularity-action-test:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: fabianrost84/singularity-action-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-ngs-assembly" class="anchor" href="#ngs-assembly" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NGS-Assembly</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611018483.0
fenz-org/OpenVino:
  data_format: 2
  description: Container recipes for OpenVINO
  filenames:
  - ubuntu18/2019/singularity/Singularity.2019_R3.1-tbb-cg-py36-gcc7.5-ubuntu18
  - ubuntu18/2019/singularity/Singularity.2019_R3.1-tbb-c-py36-gcc7.5-ubuntu18
  - ubuntu18/2019/singularity/Singularity.2019_R3.1-omp-c-py36-gcc7.5-ubuntu18
  - ubuntu18/2019/singularity/Singularity.2019_R3.1-omp-cg-py36-gcc7.5-ubuntu18
  full_name: fenz-org/OpenVino
  latest_release: 0.0.2
  readme: '<h1>

    <a id="user-content-openvino" class="anchor" href="#openvino" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>OpenVino</h1>

    <p>Container recipes for OpenVINO</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621303507.0
fenz-org/tensorflow:
  data_format: 2
  description: TensorFlow
  filenames:
  - 2.2.0-c_intelpython-Python-3.7/Singularity.2.2.0-c_intelpython-Python-3.7
  - 2.1.0-p-Python-3.7-gpu/Singularity.2.1.0-p-Python-3.7-gpu
  - 2.1.0-c_anaconda-Python-3.7/Singularity.2.1.0-c_anaconda-Python-3.7
  - 2.1.0-p-Python-3.7/Singularity.2.1.0-p-Python-3.7
  - 2.1.0-c_intel-Python-3.7/Singularity.2.1.0-c_intel-Python-3.7
  - 2.1.0-c_anaconda-Python-3.7-gpu/Singularity.2.1.0-c_anaconda-Python-3.7-gpu
  full_name: fenz-org/tensorflow
  latest_release: null
  readme: '<h1>

    <a id="user-content-container-recipes-for-tensorflow" class="anchor" href="#container-recipes-for-tensorflow"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    recipes for TensorFlow</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1604175891.0
fenz/OpenVINO:
  data_format: 2
  description: null
  filenames:
  - ubuntu18/2019/Singularity.2019_R3.1-c-py36-gcc7.5-ubuntu18
  - ubuntu18/2019/Singularity.2019_R3.1-TBB-c-py36-gcc7.5-ubuntu18
  full_name: fenz/OpenVINO
  latest_release: 0.0.2
  readme: '<h1>

    <a id="user-content-singularity-images-for-openvino" class="anchor" href="#singularity-images-for-openvino"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    images for OpenVINO</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621008787.0
fenz/node4prova:
  data_format: 2
  description: NodeJS container used for PROVA! webserver
  filenames:
  - Singularity.v14.16-buster-slim
  full_name: fenz/node4prova
  latest_release: null
  readme: '<h1>

    <a id="user-content-node4prova" class="anchor" href="#node4prova" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>node4prova</h1>

    <p>NodeJS container used for PROVA! webserver</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619154525.0
fml-fam/fml:
  data_format: 2
  description: Fused Matrix Library
  filenames:
  - containers/singularity/dev/Singularity
  - containers/singularity/dev-gpu/Singularity
  full_name: fml-fam/fml
  latest_release: v0.4-0
  readme: "<h1>\n<a id=\"user-content-fml\" class=\"anchor\" href=\"#fml\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>fml</h1>\n\
    <ul>\n<li>\n<strong>Version:</strong> 0.5-0</li>\n<li>\n<strong>Status:</strong>\
    \ <a href=\"https://travis-ci.org/fml-fam/fml\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8a8fb1ec27c4ea1a5dfbd38b1f0d7b8c2e6c6bf27e8c45a447acbda0b6fa5b57/68747470733a2f2f7472617669732d63692e6f72672f666d6c2d66616d2f666d6c2e706e67\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/fml-fam/fml.png\"\
    \ style=\"max-width:100%;\"></a>\n</li>\n<li>\n<strong>License:</strong> <a href=\"\
    http://opensource.org/licenses/BSL-1.0\" rel=\"nofollow\">BSL-1.0</a>\n</li>\n\
    <li>\n<strong>Project home</strong>: <a href=\"https://github.com/fml-fam/fml\"\
    >https://github.com/fml-fam/fml</a>\n</li>\n<li>\n<strong>Bug reports</strong>:\
    \ <a href=\"https://github.com/fml-fam/fml/issues\">https://github.com/fml-fam/fml/issues</a>\n\
    </li>\n<li>\n<strong>Documentation</strong>: <a href=\"https://fml-fam.github.io/fml\"\
    \ rel=\"nofollow\">https://fml-fam.github.io/fml</a>\n</li>\n</ul>\n<p><a href=\"\
    ./docs/logo/fml_med.png\" target=\"_blank\" rel=\"noopener noreferrer\"><img align=\"\
    right\" src=\"./docs/logo/fml_med.png\" style=\"max-width:100%;\"></a></p>\n<p>fml\
    \ is the Fused Matrix Library, a multi-source, header-only C++ library for dense\
    \ matrix computing. The emphasis is on real-valued matrix types (<code>float</code>,\
    \ <code>double</code>, and <code>__half</code>) for numerical operations useful\
    \ for data analysis.</p>\n<p>The goal of fml is to be \"medium-level\". That is,\
    \ high-level compared to working directly with e.g. the BLAS or CUDA\u2122, but\
    \ low(er)-level compared to other C++ matrix frameworks. Some knowledge of the\
    \ use of LAPACK will make many choices in fml make more sense.</p>\n<p>The library\
    \ provides 4 main classes: <code>cpumat</code>, <code>gpumat</code>, <code>parmat</code>,\
    \ and <code>mpimat</code>. These are mostly what they sound like, but the particular\
    \ details are:</p>\n<ul>\n<li>CPU: Single node cpu computing (multi-threaded if\
    \ using multi-threaded BLAS and linking with OpenMP).</li>\n<li>GPU: Single gpu\
    \ computing.</li>\n<li>MPI: Multi-node computing via ScaLAPACK (+gpus if using\
    \ <a href=\"http://icl.utk.edu/slate/\" rel=\"nofollow\">SLATE</a>).</li>\n<li>PAR:\
    \ Multi-node and/or multi-gpu computing.</li>\n</ul>\n<p>There are some differences\
    \ in how objects of any particular type are constructed. But the high level APIs\
    \ are largely the same between the objects. The goal is to be able to quickly\
    \ create laptop-scale prototypes that are then easily converted into large scale\
    \ gpu/multi-node/multi-gpu/multi-node+multi-gpu codes.</p>\n<h2>\n<a id=\"user-content-installation\"\
    \ class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n<p>The library\
    \ is header-only so no installation is strictly necessary. You can just include\
    \ a copy/submodule in your project. However, if you want some analogue of <code>make\
    \ install</code>, then you could do something like:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>ln -s ./src/fml /usr/include/</pre></div>\n<h2>\n\
    <a id=\"user-content-dependencies-and-other-software\" class=\"anchor\" href=\"\
    #dependencies-and-other-software\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Dependencies and Other Software</h2>\n\
    <p>There are no external header dependencies, but there are some shared libraries\
    \ you need to have (more information below):</p>\n<ul>\n<li>CPU code needs <a\
    \ href=\"http://performance.netlib.org/lapack/\" rel=\"nofollow\">LAPACK</a> (I\
    \ recommend <a href=\"https://github.com/xianyi/OpenBLAS\">OpenBLAS</a>)</li>\n\
    <li>GPU code needs <a href=\"https://developer.nvidia.com/cuda-downloads\" rel=\"\
    nofollow\">NVIDIA\xAE CUDA\u2122</a>\n</li>\n<li>MPI code needs <a href=\"http://performance.netlib.org/scalapack/\"\
    \ rel=\"nofollow\">ScaLAPACK</a>\n</li>\n<li>PAR code needs the libraries required\
    \ by the CPU and/or GPU features, noted above.</li>\n</ul>\n<p>Other software\
    \ we use:</p>\n<ul>\n<li>Tests use <a href=\"https://github.com/catchorg/Catch2\"\
    >catch2</a> (a copy of which is included under <code>tests/</code>).</li>\n</ul>\n\
    <p>You can find some examples of how to use the library in the <code>examples/</code>\
    \ tree. Right now there is no real build system beyond some ad hoc makefiles;\
    \ but ad hoc is better than no hoc.</p>\n<p>Depending on which class(es) you want\
    \ to use, here are some general guidelines for using the library in your own project:</p>\n\
    <ul>\n<li>CPU: <code>cpumat</code>\n<ul>\n<li>Compile with your favorite C++ compiler.</li>\n\
    <li>Link with LAPACK and BLAS (and ideally with OpenMP).</li>\n</ul>\n</li>\n\
    <li>GPU: <code>gpumat</code>\n<ul>\n<li>Compile with <code>nvcc</code>.</li>\n\
    <li>For most functionality, link with libcudart, libcublas, and libcusolver. Link\
    \ with libcurand if using the random generators.  Link with libnvidia-ml if using\
    \ nvml (if you're only using this, then you don't need <code>nvcc</code>; an ordinary\
    \ C++ compiler will do). If you have CUDA installed and do not know what to link\
    \ with, there is no harm in linking with all of these.</li>\n</ul>\n</li>\n<li>MPI:\
    \ <code>mpimat</code>\n<ul>\n<li>Compile with <code>mpicxx</code>.</li>\n<li>Link\
    \ with libscalapack.</li>\n</ul>\n</li>\n<li>PAR: <code>parmat</code>\n<ul>\n\
    <li>Compile with <code>mpicxx</code>.</li>\n<li>Link with CPU stuff if using <code>parmat_cpu</code>;\
    \ link with GPU stuff if using <code>parmat_gpu</code> (you can use both).</li>\n\
    </ul>\n</li>\n</ul>\n<p>Check the makefiles in the <code>examples/</code> tree\
    \ if none of that makes sense.</p>\n<h2>\n<a id=\"user-content-example\" class=\"\
    anchor\" href=\"#example\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Example</h2>\n<p>Here's a simple example computing\
    \ the SVD with some data held on a single CPU:</p>\n<div class=\"highlight highlight-source-c++\"\
    ><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"\
    pl-pds\">&lt;</span>fml/cpu.hh<span class=\"pl-pds\">&gt;</span></span>\n<span\
    \ class=\"pl-k\">using</span> <span class=\"pl-k\">namespace</span> <span class=\"\
    pl-en\">fml</span><span class=\"pl-k\">;</span>\n\n<span class=\"pl-k\">int</span>\
    \ <span class=\"pl-en\">main</span>()\n{\n  <span class=\"pl-c1\">len_t</span>\
    \ m = <span class=\"pl-c1\">3</span>;\n  <span class=\"pl-c1\">len_t</span> n\
    \ = <span class=\"pl-c1\">2</span>;\n  \n  cpumat&lt;<span class=\"pl-k\">float</span>&gt;\
    \ <span class=\"pl-c1\">x</span>(m, n);\n  x.<span class=\"pl-c1\">fill_linspace</span>(<span\
    \ class=\"pl-c1\">1</span>.<span class=\"pl-smi\">f</span>, (<span class=\"pl-k\"\
    >float</span>)m*n);\n  \n  x.<span class=\"pl-c1\">info</span>();\n  x.<span class=\"\
    pl-c1\">print</span>(<span class=\"pl-c1\">0</span>);\n  \n  cpuvec&lt;<span class=\"\
    pl-k\">float</span>&gt; s;\n  <span class=\"pl-c1\">linalg::svd</span>(x, s);\n\
    \  \n  s.<span class=\"pl-c1\">info</span>();\n  s.<span class=\"pl-c1\">print</span>();\n\
    \  \n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0</span>;\n}</pre></div>\n\
    <p>Save as <code>svd.cpp</code> and build with:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>g++ -I/path/to/fml/src -fopenmp svd.cpp -o svd -llapack -lblas</pre></div>\n\
    <p>You should see output like</p>\n<pre><code># cpumat 3x2 type=f\n1 4 \n2 5 \n\
    3 6 \n\n# cpuvec 2 type=f\n9.5080 0.7729 \n</code></pre>\n<p>The API is largely\
    \ the same if we change the object storage, but we have to change the object initialization.\
    \ For example, if <code>x</code> is an object of class <code>mpimat</code>, we\
    \ still call <code>linalg::svd(x, s)</code>. The differences lie in the creation\
    \ of the objects. Here is how we might change the above example to use distributed\
    \ data:</p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"\
    pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>fml/mpi.hh<span\
    \ class=\"pl-pds\">&gt;</span></span>\n<span class=\"pl-k\">using</span> <span\
    \ class=\"pl-k\">namespace</span> <span class=\"pl-en\">fml</span><span class=\"\
    pl-k\">;</span>\n\n<span class=\"pl-k\">int</span> <span class=\"pl-en\">main</span>()\n\
    {\n  grid g = <span class=\"pl-c1\">grid</span>(PROC_GRID_SQUARE);\n  g.<span\
    \ class=\"pl-c1\">info</span>();\n  \n  <span class=\"pl-c1\">len_t</span> m =\
    \ <span class=\"pl-c1\">3</span>;\n  <span class=\"pl-c1\">len_t</span> n = <span\
    \ class=\"pl-c1\">2</span>;\n  \n  mpimat&lt;<span class=\"pl-k\">float</span>&gt;\
    \ <span class=\"pl-c1\">x</span>(g, m, n, <span class=\"pl-c1\">1</span>, <span\
    \ class=\"pl-c1\">1</span>);\n  x.<span class=\"pl-c1\">fill_linspace</span>(<span\
    \ class=\"pl-c1\">1</span>.<span class=\"pl-smi\">f</span>, (<span class=\"pl-k\"\
    >float</span>)m*n);\n  \n  x.<span class=\"pl-c1\">info</span>();\n  x.<span class=\"\
    pl-c1\">print</span>(<span class=\"pl-c1\">0</span>);\n  \n  cpuvec&lt;<span class=\"\
    pl-k\">float</span>&gt; s;\n  <span class=\"pl-c1\">linalg::svd</span>(x, s);\n\
    \  \n  <span class=\"pl-k\">if</span> (g.<span class=\"pl-c1\">rank0</span>())\n\
    \  {\n    s.<span class=\"pl-c1\">info</span>();\n    s.<span class=\"pl-c1\"\
    >print</span>();\n  }\n  \n  g.<span class=\"pl-c1\">exit</span>();\n  g.<span\
    \ class=\"pl-c1\">finalize</span>();\n  \n  <span class=\"pl-k\">return</span>\
    \ <span class=\"pl-c1\">0</span>;\n}</pre></div>\n<p>In practice, using such small\
    \ block sizes for an MPI matrix is probably not a good idea; we only do so for\
    \ the sake of demonstration (we want each process to own some data). We can build\
    \ this new example via:</p>\n<div class=\"highlight highlight-source-shell\"><pre>mpicxx\
    \ -I/path/to/fml/src svd.cpp -fopenmp  svd.cpp -o svd -lscalapack-openmpi</pre></div>\n\
    <p>We can launch the example with multiple processes via</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>mpirun -np 4 ./svd</pre></div>\n<p>And here we\
    \ see:</p>\n<pre><code>## Grid 0 2x2\n\n# mpimat 3x2 on 2x2 grid type=f\n1 4 \n\
    2 5 \n3 6 \n\n# cpuvec 2 type=f\n9.5080 0.7729 \n</code></pre>\n<h2>\n<a id=\"\
    user-content-high-level-language-bindings\" class=\"anchor\" href=\"#high-level-language-bindings\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>High-Level Language Bindings</h2>\n<ul>\n<li>R bindings: <a href=\"\
    https://github.com/fml-fam/fmlr\">fmlr</a>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-header-and-api-stability\"\
    \ class=\"anchor\" href=\"#header-and-api-stability\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Header and API\
    \ Stability</h2>\n<p>tldr:</p>\n<ul>\n<li>Use the super headers (or read the long\
    \ explanation)\n<ul>\n<li>CPU - <code>fml/cpu.hh</code>\n</li>\n<li>GPU - <code>fml/gpu.hh</code>\n\
    </li>\n<li>MPI - <code>fml/mpi.hh</code>\n</li>\n<li>PAR still evolving</li>\n\
    </ul>\n</li>\n<li>Existing API's are largely stable. Most changes will be additions\
    \ rather than modifications.</li>\n</ul>\n<p>The project is young and things are\
    \ still mostly evolving. The current status is:</p>\n<h3>\n<a id=\"user-content-headers\"\
    \ class=\"anchor\" href=\"#headers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Headers</h3>\n<p>There are currently\
    \ \"super headers\" for CPU (<code>fml/cpu.hh</code>), GPU (<code>fml/gpu.hh</code>),\
    \ and MPI (<code>fml/mpi.hh</code>) backends. These include all relevant sub-headers.\
    \ These are \"frozen\" in the sense that they will not move and will always include\
    \ everything. However, as more namespaces are added, those too will be included\
    \ in the super headers. The headers one folder level deep (e.g. those in <code>fml/cpu</code>)\
    \ are similarly frozen, although more may be added over time. Headers two folder\
    \ levels</p>\n<p>Internals are evolving and subject to change at basically any\
    \ time. Notable changes will be mentioned in the changelog.</p>\n<h3>\n<a id=\"\
    user-content-api\" class=\"anchor\" href=\"#api\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>API</h3>\n<ul>\n<li>\n<strong>Frozen</strong>:\
    \ Existing APIs will not be developed further.\n<ul>\n<li>none</li>\n</ul>\n</li>\n\
    <li>\n<strong>Stable</strong>: Existing APIs are not expected to change. Some\
    \ new features may be added slowly.\n<ul>\n<li>cpumat/gpumat/mpimat classes</li>\n\
    <li>copy namespace functions</li>\n<li>linalg namespace functions (all but parmat)</li>\n\
    </ul>\n</li>\n<li>\n<strong>Stabilizing</strong>: Core class naming and construction/destruction\
    \ is probably finalized. Function/method names and arguments are solidifying,\
    \ but may change somewhat. New features are still being developed.\n<ul>\n<li>dimops\
    \ namespace functions</li>\n</ul>\n</li>\n<li>\n<strong>Evolving</strong>: Function/method\
    \ names and arguments are subject to change. New features are actively being developed.\n\
    <ul>\n<li>stats namespace functions</li>\n</ul>\n</li>\n<li>\n<strong>Experimental</strong>:\
    \ Nothing is remotely finalized.\n<ul>\n<li>parmat - all functions and methods</li>\n\
    </ul>\n</li>\n</ul>\n<p>Internals are evolving and subject to change at basically\
    \ any time. Notable changes will be mentioned in the changelog.</p>\n<h2>\n<a\
    \ id=\"user-content-philosophy-and-similar-projects\" class=\"anchor\" href=\"\
    #philosophy-and-similar-projects\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Philosophy and Similar Projects</h2>\n\
    <p>Some similar C/C++ projects worth mentioning:</p>\n<ul>\n<li><a href=\"http://arma.sourceforge.net/\"\
    \ rel=\"nofollow\">Armadillo</a></li>\n<li><a href=\"http://eigen.tuxfamily.org/\"\
    \ rel=\"nofollow\">Eigen</a></li>\n<li><a href=\"http://www.boost.org/\" rel=\"\
    nofollow\">Boost</a></li>\n<li><a href=\"https://www.mcs.anl.gov/petsc/\" rel=\"\
    nofollow\">PETSc</a></li>\n<li><a href=\"https://www.gnu.org/software/gsl/\" rel=\"\
    nofollow\">GSL</a></li>\n</ul>\n<p>These are all great libraries which have stood\
    \ the test of time. Armadillo in particular is worthy of a look, as it has a very\
    \ nice interface and very extensive set of functions. However, to my knowledge,\
    \ all of these focus exclusively on CPU computing. There are some extensions to\
    \ Armadillo and Eigen for GPU computing. And for gemm-heavy codes, you can use\
    \ <a href=\"https://docs.nvidia.com/cuda/nvblas/index.html\" rel=\"nofollow\"\
    >nvblas</a> to offload some work to the GPU, but this doesn't always achieve good\
    \ performance. And none of the above include distributed computing, except for\
    \ PETSc which focuses on sparse matrices.</p>\n<p>There are probably many other\
    \ C++ frameworks in this arena, but none to my knowledge have a similar scope\
    \ to fml.</p>\n<p>Probably the biggest influence on my thinking for this library\
    \ is the <a href=\"https://github.com/RBigData\">pbdR package ecosystem</a> for\
    \ HPC with the R language, which I have worked on for many years now. Some obvious\
    \ parallels are:</p>\n<ul>\n<li>\n<a href=\"https://github.com/wrathematics/float\"\
    >float</a> - CPU/GPU</li>\n<li>\n<a href=\"https://github.com/RBigData/kazaam\"\
    >kazaam</a> - PAR</li>\n<li>\n<a href=\"https://github.com/RBigData/pbdDMAT\"\
    >pbdDMAT</a> - MPI</li>\n</ul>\n<p>The basic philosophy of fml is:</p>\n<ul>\n\
    <li>Be relatively small and self-contained.</li>\n<li>Follow general C++ conventions\
    \ by default (like RAII and exceptions), but give the ability to break these for\
    \ the sake of performance.</li>\n<li>Changing a code from one object type to another\
    \ should be very simple, ideally with no changes to the source (the internals\
    \ will simply <strong>Do The Right Thing (tm)</strong>), with the exception of:\n\
    <ul>\n<li>object creation</li>\n<li>printing (e.g. printing on only one MPI rank)</li>\n\
    </ul>\n</li>\n<li>Use a permissive open source license.</li>\n</ul>\n"
  stargazers_count: 22
  subscribers_count: 2
  topics:
  - linear-algebra
  - matrix
  - blas
  - cuda
  - mpi
  - scalapack
  - hpc
  updated_at: 1600895582.0
fml-fam/fmlr:
  data_format: 2
  description: R bindings for the Fused Matrix Library (fml)
  filenames:
  - containers/singularity/dev/Singularity
  - containers/singularity/dev-gpu/Singularity
  full_name: fml-fam/fmlr
  latest_release: v0.3-0
  readme: '<h1>

    <a id="user-content-fmlr" class="anchor" href="#fmlr" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>fmlr</h1>

    <ul>

    <li>

    <strong>Version:</strong> 0.3-0</li>

    <li>

    <strong>License:</strong> <a href="http://opensource.org/licenses/BSL-1.0" rel="nofollow">BSL-1.0</a>

    </li>

    <li>

    <strong>Project home</strong>: <a href="https://github.com/fml-fam/fmlr">https://github.com/fml-fam/fmlr</a>

    </li>

    <li>

    <strong>Bug reports</strong>: <a href="https://github.com/fml-fam/fmlr/issues">https://github.com/fml-fam/fmlr/issues</a>

    </li>

    <li>

    <strong>Documentation</strong>: <a href="https://fml-fam.github.io/fmlr" rel="nofollow">https://fml-fam.github.io/fmlr</a>

    </li>

    </ul>

    <p>fmlr is an R interface to the <a href="https://github.com/fml-fam/fml">fml
    library</a>. It is a "medium-level" interface for multiple dense matrix types,
    principally CPU, GPU, and MPI. Each supports multiple fundamental types (int,
    float, double), and data is held externally to R and operations that modify data
    generally occur in-place. The interface largely tracks with the core ''fml'' interface.
    The interface is written such that generally an ''fmlr'' R code can be easily
    translated to an ''fml'' C++ code.</p>

    <p>Differences between fmlr and other matrix interfaces (including the core R
    interface):</p>

    <ul>

    <li>Single interface supporting multiple fundamental types (<code>__half</code>,
    <code>float</code>, <code>double</code>) and backends (CPU, GPU, MPI).</li>

    <li>Data is always held externally to R (although CPU objects can inherit R data
    without a copy).</li>

    <li>Operations modifying data occur in-place (make your own copy if you don''t
    want the data modified).</li>

    </ul>

    <p>For a high-level interface on top of fmlr, see the <a href="https://github.com/fml-fam/craze">craze
    package</a>.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>In principle, installation can be as simple as:</p>

    <div class="highlight highlight-source-r"><pre>install.packages(<span class="pl-s"><span
    class="pl-pds">"</span>fmlr<span class="pl-pds">"</span></span>, <span class="pl-v">repos</span><span
    class="pl-k">=</span>c(<span class="pl-s"><span class="pl-pds">"</span>https://hpcran.org<span
    class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>https://cran.rstudio.com<span
    class="pl-pds">"</span></span>))</pre></div>

    <p>This will build support for the CPU backend. If you want GPU or MPI support,
    please see the <a href="https://fml-fam.github.io/fmlr/html/articles/01-installation.html"
    rel="nofollow">Installation Guide</a>.</p>

    <h2>

    <a id="user-content-example-use" class="anchor" href="#example-use" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Example Use</h2>

    <p>Calculating singular values on CPU:</p>

    <div class="highlight highlight-source-r"><pre>suppressMessages(library(<span
    class="pl-smi">fmlr</span>))

    <span class="pl-v">x</span> <span class="pl-k">=</span> cpumat(<span class="pl-c1">3</span>,
    <span class="pl-c1">2</span>, <span class="pl-v">type</span><span class="pl-k">=</span><span
    class="pl-s"><span class="pl-pds">"</span>float<span class="pl-pds">"</span></span>)

    <span class="pl-smi">x</span><span class="pl-k">$</span>fill_linspace(<span class="pl-c1">1</span>,
    <span class="pl-c1">6</span>)

    <span class="pl-smi">x</span><span class="pl-k">$</span>info()

    <span class="pl-c"><span class="pl-c">#</span># # cpumat 3x2 type=f</span>

    <span class="pl-smi">x</span>

    <span class="pl-c"><span class="pl-c">#</span># 1.0000 4.0000 </span>

    <span class="pl-c"><span class="pl-c">#</span># 2.0000 5.0000 </span>

    <span class="pl-c"><span class="pl-c">#</span># 3.0000 6.0000 </span>


    <span class="pl-v">s</span> <span class="pl-k">=</span> cpuvec(<span class="pl-v">type</span><span
    class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>float<span
    class="pl-pds">"</span></span>)

    linalg_svd(<span class="pl-smi">x</span>, <span class="pl-smi">s</span>)


    <span class="pl-smi">s</span><span class="pl-k">$</span>info()

    <span class="pl-c"><span class="pl-c">#</span># # cpuvec 3 type=f</span>

    <span class="pl-smi">s</span>

    <span class="pl-c"><span class="pl-c">#</span># 9.5080 0.7729 </span></pre></div>

    <p>and on GPU:</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-v">c</span> <span
    class="pl-k">=</span> card()

    <span class="pl-smi">c</span>

    <span class="pl-c"><span class="pl-c">#</span># GPU 0 (GeForce GTX 1070 Ti) 1139/8116
    MB - CUDA 10.2</span>


    <span class="pl-v">x</span> <span class="pl-k">=</span> gpumat(<span class="pl-smi">c</span>,
    <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-v">type</span><span
    class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>float<span
    class="pl-pds">"</span></span>)

    <span class="pl-smi">x</span><span class="pl-k">$</span>fill_linspace(<span class="pl-c1">1</span>,
    <span class="pl-c1">6</span>)

    <span class="pl-smi">x</span><span class="pl-k">$</span>info()

    <span class="pl-c"><span class="pl-c">#</span># # gpumat 3x2 type=f </span>


    <span class="pl-v">s</span> <span class="pl-k">=</span> gpuvec(<span class="pl-smi">c</span>,
    <span class="pl-v">type</span><span class="pl-k">=</span><span class="pl-s"><span
    class="pl-pds">"</span>float<span class="pl-pds">"</span></span>)

    linalg_svd(<span class="pl-smi">x</span>, <span class="pl-smi">s</span>)


    <span class="pl-smi">s</span><span class="pl-k">$</span>info()

    <span class="pl-c"><span class="pl-c">#</span># # gpuvec 2 type=f </span>

    <span class="pl-smi">s</span>

    <span class="pl-c"><span class="pl-c">#</span># 9.5080 0.7729 </span></pre></div>

    <p>For more information and examples, see:</p>

    <ul>

    <li><a href="https://fml-fam.github.io/fmlr" rel="nofollow">Package documentation</a></li>

    <li>Articles:

    <ul>

    <li><a href="https://fml-fam.github.io/fmlr/html/articles/01-installation.html"
    rel="nofollow">Installation Guide</a></li>

    <li><a href="https://fml-fam.github.io/fmlr/html/articles/02-overview.html" rel="nofollow">Overview
    of fmlr</a></li>

    <li><a href="https://fml-fam.github.io/fmlr/html/articles/03-backends.html" rel="nofollow">Managing
    Backends</a></li>

    <li><a href="https://fml-fam.github.io/fmlr/html/articles/04-data.html" rel="nofollow">Data
    Management</a></li>

    </ul>

    </li>

    </ul>

    <h2>

    <a id="user-content-fml-from-c" class="anchor" href="#fml-from-c" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>fml from C++</h2>

    <p>A copy of the core fml library is included in the fmlr package source in <code>inst/include/fml</code>.
    If you wish to link with fml to create your own C++ kernels, you can add <code>LinkingTo:
    fml</code> to your R package DESCRIPTION file.</p>

    <p>Before you write your own C++ code using fml, you should check the <a href="https://github.com/fml-fam/fml#api-stability">fml
    API stability</a> progress, as some things may be subject to change.</p>

    <h2>

    <a id="user-content-similar-projects" class="anchor" href="#similar-projects"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Similar
    Projects</h2>

    <p>Some similar R projects worth mentioning:</p>

    <ul>

    <li>Martin Maechler''s (et al.) <a href="https://cran.r-project.org/web/packages/Matrix/index.html"
    rel="nofollow">Matrix package</a>

    </li>

    <li>Charles Determan''s <a href="https://github.com/cdeterman/gpuR">gpuR</a> and
    <a href="https://github.com/gpuRcore">gpuR-related packages</a>

    </li>

    <li>Norm Matloff''s <a href="https://github.com/Rth-org/Rth">Rth</a>

    </li>

    </ul>

    <p>Some related R packages I have worked on:</p>

    <ul>

    <li><a href="https://github.com/wrathematics/float">float</a></li>

    <li><a href="https://github.com/RBigData/kazaam">kazaam</a></li>

    <li><a href="https://github.com/RBigData/pbdDMAT">pbdDMAT</a></li>

    </ul>

    <p>For C/C++ projects, see <a href="https://github.com/fml-fam/fml#philosophy-and-similar-projects">the
    fml README</a>.</p>

    '
  stargazers_count: 10
  subscribers_count: 2
  topics:
  - r
  - linear-algebra
  - matrix
  - blas
  - cuda
  - mpi
  - scalapack
  - hpc
  updated_at: 1620718561.0
ftabaro/MethylSnake:
  data_format: 2
  description: A Snakemake pipeline for RRBS data analysis
  filenames:
  - singularity/Singularity.base
  - singularity/Singularity.methylkit
  full_name: ftabaro/MethylSnake
  latest_release: 1.1.1
  readme: "<p><a href=\"https://hackmd.io/cksQRWI5SKOrVTogW4DzMQ\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/8a507adfaf617171f3313cd95921c8d879cc5847e1a2bc0b0e4a114850c3f92b/68747470733a2f2f6861636b6d642e696f2f636b735152574935534b4f7256546f675734447a4d512f6261646765\"\
    \ alt=\"hackmd-github-sync-badge\" data-canonical-src=\"https://hackmd.io/cksQRWI5SKOrVTogW4DzMQ/badge\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://zenodo.org/badge/latestdoi/216578481\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/39f91651318725769151530a0bccc5499b25df00549bb2d18aac95a564760d08/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3231363537383438312e737667\"\
    \ alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/216578481.svg\" style=\"\
    max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-methylsnake\" class=\"\
    anchor\" href=\"#methylsnake\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>MethylSnake</h1>\n<p>A Snakemake pipeline\
    \ for RRBS data analysis</p>\n<h2>\n<a id=\"user-content-overview\" class=\"anchor\"\
    \ href=\"#overview\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Overview</h2>\n<p>This pipeline implements a\
    \ DNA methylation data analysis workflow. Currently, it is taylored toward RRBS\
    \ data analysis but it can be easity tweaked to support other data types.</p>\n\
    <p>Starting from raw reads in FASTQ format, it runs basic QC reporting, reads\
    \ trimming, alignment, methylation extraction, filtering of incomplete conversions\
    \ and differential methylation calls both at single base resolution and fixed-size\
    \ tiles. Moreover, it runs basic downstream  genomic annotation.</p>\n<p>The final\
    \ output of the pipeline are a set of files for alignment, detected features,\
    \ annotation and reports in a variety of formats (bam, bed, pdf, html). R objects\
    \ are also returned for retrospective inspection and further downstream analysis.</p>\n\
    <p><a href=\"rulegraph.png\" target=\"_blank\" rel=\"noopener noreferrer\"><img\
    \ src=\"rulegraph.png\" alt=\"\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a\
    \ id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Quick\
    \ start</h2>\n<ol>\n<li>Prepare genome indexes for the <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/Bismark_User_Guide.pdf\"\
    \ rel=\"nofollow\">Bismark aligner</a>.</li>\n<li>\n<a href=\"#how-to-write-the-sample-sheet\"\
    >Write a sample sheet</a> with per-sample specifications</li>\n<li><a href=\"\
    #how-to-generate-the-config-file\">Generate config file</a></li>\n<li><a href=\"\
    #how-to-run-the-pipeline\">Start the pipeline</a></li>\n</ol>\n<h3>\n<a id=\"\
    user-content-tools\" class=\"anchor\" href=\"#tools\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Tools</h3>\n\
    <ul>\n<li>QC: <strong>FastQC</strong>\n</li>\n<li>Trimming: <strong>Trim_galore</strong>\
    \ (Cutadapt wrapper)</li>\n<li>Alignment + methlyation extraction: <strong>Bismark</strong>\
    \ (using Bowtie2)</li>\n<li>Differential methylation detection: <strong>MethylKit</strong>\
    \ (Bioconductor package)</li>\n</ul>\n<h2>\n<a id=\"user-content-dependencies\"\
    \ class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Dependencies</h2>\n<ul>\n<li>\n\
    <code>snakemake</code> 5.20.1: <code>pip install --user snakemake</code> or via\
    \ Conda</li>\n<li>\n<code>pyaml</code> 20.4 (PyYAML 5.3.1): <code>pip install\
    \ --user pyaml</code>\n</li>\n<li>\n<code>singularity</code> 3.6.1 has to be installed\
    \ system wide, ask your admin</li>\n<li>\n<code>slurm</code> 18.08.8 has to be\
    \ installed system wide, ask your admin</li>\n</ul>\n<h2>\n<a id=\"user-content-how-to-write-the-sample-sheet\"\
    \ class=\"anchor\" href=\"#how-to-write-the-sample-sheet\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How to write\
    \ the sample sheet</h2>\n<p>The sample sheet has to be saved in csv format, quotes\
    \ are not mandatory but <strong>field separator has to a semicolon and headers\
    \ are mandatory</strong>.</p>\n<p>The table is composed of two columns:</p>\n\
    <ul>\n<li>\n<code>sample_name</code>: this field should match the input file name(s),\
    \ e.g. if reads for a sample are stored in <code>sample1.fq.gz</code>, the <code>sample_name</code>\
    \ column value should be <code>sample1</code>. In case of paired end reads, <code>sample1_R1.fq.gz</code>\
    \ and <code>sample1_R2.fq.gz</code> the value of this column should be <code>sample1</code>.</li>\n\
    <li>\n<code>treatment</code>: a numerical value representing the treatment. The\
    \ value 0 (zero) always represent control samples.</li>\n</ul>\n<p>An example\
    \ with two treatments, control and triplicate replicates per treatment:</p>\n\
    <pre><code>sample_name;treatment\nsample1;0\nsample2;0\nsample3;0\nsample4;1\n\
    sample5;1\nsample6;1\nsample7;2\nsample8;2\nsample9;2\n</code></pre>\n<p>In this\
    \ setup, samples 1, 2 and 3 are control; samples 4, 5, 6 correspond to treatment\
    \ 1 and samples 7, 8, 9 to treatment 2.</p>\n<h2>\n<a id=\"user-content-how-to-generate-the-config-file\"\
    \ class=\"anchor\" href=\"#how-to-generate-the-config-file\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How to generate\
    \ the config file</h2>\n<p>The Snakemake config file holds all tunable parameters\
    \ of the pipeline. Check <a href=\"#complete-list-of-config-parameters\">this\
    \ section</a> for a complete list and relative explanation. For simplicity, all\
    \ paths should be absolute.</p>\n<p>A wrapper script with an example invocation\
    \ can be found in <code>run_make_config.sh</code>, It can be tweaked and run to\
    \ generate a valid config file.</p>\n<h2>\n<a id=\"user-content-how-to-run-the-pipeline\"\
    \ class=\"anchor\" href=\"#how-to-run-the-pipeline\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>How to run the\
    \ pipeline</h2>\n<p>The pipeline runs inside a Singularity container. This container\
    \ should contain all the necessary tools to run the analysis. See <a href=\"#singularity-container\"\
    >below</a> for additional details.</p>\n<p>The pipeline can be started with the\
    \ wrapper script <code>run_snakemake.sh</code>.</p>\n<p>The scripts parses the\
    \ config YAML file searching for four folder to bind mount in the Singularity\
    \ container: working directory, the genome directory, the genome index directory,\
    \ the directory holding annotations and the temporary directory. All these paths\
    \ can be specified when running the config script.</p>\n<h4>\n<a id=\"user-content-slurm-configuration\"\
    \ class=\"anchor\" href=\"#slurm-configuration\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Slurm configuration</h4>\n<p>A\
    \ Slurm configuration is given in <code>cluster-config/cluster.json</code>. In\
    \ the json files, keys correspont to Snakemake rule names. Values are in turn\
    \ objects with keys corresponding to Slurm parameters and values corresponding\
    \ to desired values for the parameter:</p>\n<div class=\"highlight highlight-source-json\"\
    ><pre>...\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>trim<span class=\"\
    pl-pds\">\"</span></span>:\n    {\n        <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>      : <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>6:00:00<span class=\"pl-pds\">\"</span></span>,\n\
    \        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mem<span class=\"\
    pl-pds\">\"</span></span>       : <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>8G<span class=\"pl-pds\">\"</span></span>\n    },\n...</pre></div>\n\
    <p>The example above assigns maximum runtime of six hours and 8GB or memory to\
    \ the <code>trim</code> rule. The \"__default__\" key provides default parameters\
    \ applied to all rules:</p>\n<div class=\"highlight highlight-source-json\"><pre>...\n\
    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__default__<span class=\"\
    pl-pds\">\"</span></span>:\n    {\n        <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>jobname<span class=\"pl-pds\">\"</span></span>   : <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>{rule}<span class=\"pl-pds\">\"</span></span>,\n\
    \        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>log<span class=\"\
    pl-pds\">\"</span></span>       : <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>slurm-%x-%j.log<span class=\"pl-pds\">\"</span></span>,\n        <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>ntasks<span class=\"pl-pds\"\
    >\"</span></span>    : <span class=\"pl-c1\">1</span>,\n        <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>cpus<span class=\"pl-pds\">\"</span></span>\
    \      : <span class=\"pl-c1\">1</span>,\n        <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>mem<span class=\"pl-pds\">\"</span></span>       : <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>4G<span class=\"pl-pds\">\"</span></span>,\n\
    \        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"\
    pl-pds\">\"</span></span>      : <span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>00-00:20:00<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>partition<span class=\"pl-pds\">\"</span></span>\
    \ : <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>normal,parallel<span\
    \ class=\"pl-pds\">\"</span></span>\n    }\n...</pre></div>\n<p>By default, each\
    \ job is spawned with the rule name and logs to a file named after the rule and\
    \ the jobid generated by Slurm. Each job is assigned 1 task, 1 CPU and 4GB of\
    \ RAM. Maximum runtime is 20 minutes. Allowed partitions are normal and parallel.</p>\n\
    <h4>\n<a id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity container</h4>\n<p>The pipeline is designed to run within\
    \ a Singularity container. The path to the container can be configured with the\
    \ config script.</p>\n<p>A working container can be pulled from SingularityHub:</p>\n\
    <pre><code>singularity pull --name methylsnake.sif library://ftabaro/default/methylsnake\
    \ \n</code></pre>\n<p>Checkout the <a href=\"https://github.com/ftabaro/MethylSnake/tree/master/singularity\"\
    ><code>singularity</code></a> folder for more info.</p>\n<p>The container path\
    \ has be specified in the command line arguments of the config script.</p>\n<h2>\n\
    <a id=\"user-content-output-description\" class=\"anchor\" href=\"#output-description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Output description</h2>\n<p>This pipeline generates a number of output\
    \ files:</p>\n<ul>\n<li>\n<a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/good_sequence_short_fastqc.html\"\
    \ rel=\"nofollow\">FastQC reports</a> for input and trimmed reads</li>\n<li>Fastq\
    \ files for trimmed reads files with reports (Fastqc and text)</li>\n<li>Alignment\
    \ files with report and nucleotide statistics</li>\n<li>Alignment files with incomplete\
    \ conversions removed</li>\n<li>Methylation files in GpG, CHG and CHH contexts</li>\n\
    <li><a href=\"http://www.bioinformatics.babraham.ac.uk/projects/bismark/PE_report.html\"\
    \ rel=\"nofollow\">Bismark HTML reports</a></li>\n<li><a href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/bismark_summary_report.html\"\
    \ rel=\"nofollow\">Bismark HTML summary</a></li>\n<li>rds files with every object\
    \ computed by <a href=\"https://bioconductor.org/packages/release/bioc/vignettes/methylKit/inst/doc/methylKit.html\"\
    \ rel=\"nofollow\">MethylKit</a>\n</li>\n<li>bed files of differential methylation\
    \ calls</li>\n<li>Figures\n<ul>\n<li>prelimimary coverage and methylation state\
    \ histograms</li>\n<li>samples PCA and correlations</li>\n<li>number of differentially\
    \ methylated features per chromosome (DMC and DMR)</li>\n<li>pie charts of annotation\
    \ classes\n<ul>\n<li>all differentially methylated features</li>\n<li>hyper methylated</li>\n\
    <li>hypo methylated</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Tables\n<ul>\n<li>distance\
    \ to TSS for all, hyper and hypo methylated features (csv)</li>\n</ul>\n</li>\n\
    </ul>\n<h3>\n<a id=\"user-content-ouput-folder-hierarchy\" class=\"anchor\" href=\"\
    #ouput-folder-hierarchy\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Ouput folder hierarchy</h3>\n<p>An example of\
    \ output directory (all directory names can be changed from the config file)</p>\n\
    <pre><code>.\n\u251C\u2500\u2500 alignments\n\u251C\u2500\u2500 bed\n\u2502\_\_\
    \ \u251C\u2500\u2500 dmc\n\u2502\_\_ \u2514\u2500\u2500 dmr\n\u251C\u2500\u2500\
    \ fastqc\n\u251C\u2500\u2500 log\n\u251C\u2500\u2500 pictures\n\u2502\_\_ \u251C\
    \u2500\u2500 dmc\n\u2502\_\_ \u2514\u2500\u2500 dmr\n\u251C\u2500\u2500 reads\n\
    \u2502\_\_ \u2514\u2500\u2500 fastqc\n\u251C\u2500\u2500 RData\n\u2502\_\_ \u251C\
    \u2500\u2500 dmc\n\u2502\_\_ \u2514\u2500\u2500 dmr\n\u251C\u2500\u2500 reports\n\
    \u251C\u2500\u2500 tables\n\u2502\_\_ \u251C\u2500\u2500 dmc\n\u2502\_\_ \u2514\
    \u2500\u2500 dmr\n\u2514\u2500\u2500 trimmed\n\n</code></pre>\n<h2>\n<a id=\"\
    user-content-useful-links\" class=\"anchor\" href=\"#useful-links\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Useful\
    \ links</h2>\n<ul>\n<li><a href=\"https://www.nature.com/articles/nmeth.1828\"\
    \ rel=\"nofollow\">https://www.nature.com/articles/nmeth.1828</a></li>\n<li><a\
    \ href=\"https://www.epigenesys.eu/images/stories/protocols/pdf/20120720103700_p57.pdf\"\
    \ rel=\"nofollow\">https://www.epigenesys.eu/images/stories/protocols/pdf/20120720103700_p57.pdf</a></li>\n\
    <li><a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"\
    nofollow\">https://www.bioinformatics.babraham.ac.uk/projects/fastqc/</a></li>\n\
    <li><a href=\"https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\"\
    \ rel=\"nofollow\">https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/</a></li>\n\
    <li><a href=\"https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md\"\
    >https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md</a></li>\n\
    <li><a href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/\" rel=\"\
    nofollow\">https://www.bioinformatics.babraham.ac.uk/projects/bismark/</a></li>\n\
    <li><a href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/Bismark_User_Guide.pdf\"\
    \ rel=\"nofollow\">https://www.bioinformatics.babraham.ac.uk/projects/bismark/Bismark_User_Guide.pdf</a></li>\n\
    <li><a href=\"https://github.com/FelixKrueger/Bismark/tree/master/Docs\">https://github.com/FelixKrueger/Bismark/tree/master/Docs</a></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contributing</h2>\n<p>Bug reports and pull requests are welcome on\
    \ GitHub at <a href=\"https://github.com/ftabaro/rrbs-pipeline/issues\">https://github.com/ftabaro/rrbs-pipeline/issues</a></p>\n\
    <h2>\n<a id=\"user-content-complete-list-of-config-parameters\" class=\"anchor\"\
    \ href=\"#complete-list-of-config-parameters\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Complete list of config parameters</h2>\n\
    <pre><code>  --config-path CONFIG_PATH\n                        Path for the config\
    \ file\n                        \n  --wd WD               Working directory. Root\
    \ directory of the project\n  \n  --genome-path GENOME_PATH\n                \
    \        Path to a fasta file with genome sequence\n                        \n\
    \  --bismark-index-path BISMARK_INDEX_PATH\n                        Path to Bismark\
    \ index files (only index name required:\n                        /path/to/index/folder/hg38*)\n\
    \                        \n  --sample-sheet SAMPLE_SHEET\n                   \
    \     Path to csv file holding samples information\n                        \n\
    \  --annotation-file ANNOTATION_FILE\n                        Path to a gzipped\
    \ GTF file.\n                        \n  --dmr-window-size DMR_WINDOW_SIZE\n \
    \                       Window size for tiled differential methylation\n     \
    \                   analysis\n                        \n  --dmr-step-size DMR_STEP_SIZE\n\
    \                        Step size for tiled differential methylation analysis\n\
    \                        \n  --dmr-difference DMR_DIFFERENCE [DMR_DIFFERENCE ...]\n\
    \                        Difference in reads coverage threshold for\n        \
    \                differential methylation analysis\n                        \n\
    \  --dmr-qvalue DMR_QVALUE [DMR_QVALUE ...]\n                        Q-value threshold\
    \ for differential methylation\n                        analysis\n           \
    \             \n  --min-per-group MIN_PER_GROUP\n                        An integer\
    \ denoting minimum number of samples per\n                        replicate needed\
    \ to cover a region/base\n                        \n  --mate1-pattern MATE1_PATTERN\n\
    \                        Pattern to identify mate 1 in paired sequencing files\
    \ e.g. R1 or _R1 or _1\n                        \n  --mate2-pattern MATE2_PATTERN\n\
    \                        Pattern to identify mate 2 in paired sequencing files\
    \ e.g. R2 or _R2 or _2\n                        \n  --fastq-extension FASTQ_EXTENSION\n\
    \                        File extension of reads fastq files e.g. fastq.gz or\
    \ fq.gz or fastq\n                        \n  --genome-version GENOME_VERSION\n\
    \                        Label for genome version used in the analysis e.g. hg38\
    \ or hg19\n                        \n  --singularity-container SINGULARITY_CONTAINER\n\
    \                        Path to a singularity container with all necessary\n\
    \                        tools installed.\n                        \n  --tmp-folder\
    \ TMP_FOLDER\n                        Path to a temporary folder (possibly fast\
    \ storage) e.g. /scratch\n                        \n  --log-folder LOG_FOLDER\n\
    \                        Path to a log folder\n                        \n  --reads-folder\
    \ READS_FOLDER\n                        Path to a folder with reads to be processed\n\
    \                        \n  --trimmed-folder TRIMMED_FOLDER\n               \
    \         Path to folder to write trimmed reads in\n                        \n\
    \  --alignments-folder ALIGNMENTS_FOLDER\n                        Path to a folder\
    \ to write alignments in\n                        \n  --reports-folder REPORTS_FOLDER\n\
    \                        Path to a folder to write Bismark reports in\n      \
    \                  \n  --nucleotide-stats-folder NUCLEOTIDE_STATS_FOLDER\n   \
    \                     Path to write Bismark nucleotide report files in\n     \
    \                   \n  --methylkitdb-folder METHYLKITDB_FOLDER\n            \
    \            Path to write methylKit tabix files in\n                        \n\
    \  --rdata-folder RDATA_FOLDER\n                        Path to write RDS objects\
    \ used in methylKit analysis\n                        \n  --bed-folder BED_FOLDER\n\
    \                        Path to write BED files for DMR/DMC coordinates to\n\
    \                        \n  --pictures-folder PICTURES_FOLDER\n             \
    \           Path to a folder to write plots in\n                        \n  --tables-folder\
    \ TABLES_FOLDER\n                        Path to a folder to write tables in\n\
    \                        \n  --fastqc-folder FASTQC_FOLDER\n                 \
    \       FastQC results folder\n                        \n  --low-coverage-count\
    \ LOW_COVERAGE_COUNT\n                        Number of reads for low coverage\
    \ filtering\n                        \n  --high-coverage-percentage HIGH_COVERAGE_PERCENTAGE\n\
    \                        Percentage of the coverage distribution for high\n  \
    \                      coverage filtering (PCR duplicates)\n</code></pre>\n<h6>\n\
    <a id=\"user-content-tags-snakemake-dna-methylation-rrbs\" class=\"anchor\" href=\"\
    #tags-snakemake-dna-methylation-rrbs\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>tags: <code>snakemake</code>\
    \ <code>DNA-methylation</code> <code>rrbs</code>\n</h6>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - bioinformatics-pipeline
  - methylation
  - rrbs-pipeline
  - snakemake
  - rrbs-data-analysis
  updated_at: 1613470322.0
ftabaro/luxus-singularity:
  data_format: 2
  description: A Singularity container for the LuxUS tool
  filenames:
  - Singularity
  full_name: ftabaro/luxus-singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-a-singularity-container-for-the-luxus-tool\"\
    \ class=\"anchor\" href=\"#a-singularity-container-for-the-luxus-tool\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>A\
    \ Singularity container for the LuxUS tool</h1>\n<p>For details about the LuxUS\
    \ method see the <a href=\"https://www.biorxiv.org/content/10.1101/536722v2\"\
    \ rel=\"nofollow\">paper</a> and the <a href=\"https://github.com/hallav/LuxUS\"\
    >repository</a>.</p>\n<p><a href=\"https://singularity-hub.org/collections/4296\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-download-a-premade-luxus-image-from-singularity-hub\"\
    \ class=\"anchor\" href=\"#download-a-premade-luxus-image-from-singularity-hub\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Download a premade LuxUS image from Singularity Hub</h2>\n<pre><code>$\
    \ singularity pull shub://ftabaro/luxus-singularity\n</code></pre>\n<p>This will\
    \ download a premade image from the Singularity Hub into the current working directory\
    \ into <code>luxus-singularity_latest.sif</code>.</p>\n<h2>\n<a id=\"user-content-build-local-image\"\
    \ class=\"anchor\" href=\"#build-local-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build local image</h2>\n<ol>\n\
    <li>Clone this repository</li>\n<li>\n<code>cd</code> into che cloned repository</li>\n\
    <li>Build</li>\n</ol>\n<pre><code>$ singularity build Singularity luxus-singularity.sif\n\
    </code></pre>\n<p>This will build the image in the current folder into <code>luxus-singularity.sif</code>.</p>\n\
    <h2>\n<a id=\"user-content-run-luxus\" class=\"anchor\" href=\"#run-luxus\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run\
    \ LuxUS</h2>\n<ol>\n<li>Set variables</li>\n</ol>\n<pre><code>INPUT_FOLDER=LuxUS/data/\n\
    OUTPUT_FOLDER=test\nOUTPUT_FILE=test.txt\n</code></pre>\n<ol start=\"2\">\n<li>Prepare\
    \ data</li>\n</ol>\n<pre><code>$ singularity run --app prepare luxus-singularity.sif\
    \ \\\n  -i $INPUT_FOLDER/proportion_table_test_data_diff1.txt \\\n  -d $INPUT_FOLDER/design_matrix_test_data_diff1.txt\
    \ \\\n  -o $OUTPUT_FOLDER \\\n  -r 12 \\\n  -t 1 \\\n  -u 0.1 \\\n  -y $OUTPUT_FOLDER/window_mean_coverage_test_data_diff1.txt\
    \ \\\n  -z $OUTPUT_FOLDER/window_number_of_cytosines_test_data_diff1.txt\n</code></pre>\n\
    <ol start=\"3\">\n<li>Run the analysis</li>\n</ol>\n<pre><code>$ singularity run\
    \ --app luxus luxus-singularity.sif \\\n  -d input_for_luxus_1.txt \\\n  -o $OUTPUT_FOLDER\
    \ \\\n  -i $INPUT_FOLDER \\\n  -j $OUTPUT_FILE \\\n  -x 1 \\\n  -p 1 \\\n  -w\
    \ 1\n</code></pre>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1590164126.0
ftabaro/singularity-coderefinery2020:
  data_format: 2
  description: An extended JupyterLab container
  filenames:
  - Singularity
  full_name: ftabaro/singularity-coderefinery2020
  latest_release: null
  readme: '<h1>

    <a id="user-content-jupyterlab-container-for-coderefinery-2020" class="anchor"
    href="#jupyterlab-container-for-coderefinery-2020" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>JupyterLab container for CodeRefinery
    2020</h1>

    <h6>

    <a id="user-content-tags-code-refinery-singularity" class="anchor" href="#tags-code-refinery-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>tags:
    <code>code-refinery</code> <code>singularity</code>

    </h6>

    <p>This repository contains the recipe for the container used in CodeRefinery
    Workshop 2020 by the NykterLab team.</p>

    <p><a href="https://singularity-hub.org/collections/4356" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-whats-inside" class="anchor" href="#whats-inside" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>What''s inside</h1>

    <ul>

    <li>The base container is Jupyter datascience-notebook Docker container.</li>

    <li>Python packages have been added, e.g.:

    <ul>

    <li><code>sphinx</code></li>

    <li><code>pytest</code></li>

    <li><code>pycodestyle</code></li>

    </ul>

    </li>

    <li>Jupyter extensions:

    <ul>

    <li>JupyterLab Git</li>

    <li>JupyterLab GitHub</li>

    <li>ipywidgets</li>

    </ul>

    </li>

    <li>Snakemake</li>

    </ul>

    <h1>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h1>

    <h3>

    <a id="user-content-option-1-singularityhub" class="anchor" href="#option-1-singularityhub"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option
    1: SingularityHub</h3>

    <p>With this option, a copy of the pre-built container image will be downloaded
    from SingularityHub</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    start with defaults using runscript directive</span>

    singularity run shub://ftabaro/coderefinery.sif [options]


    <span class="pl-c"><span class="pl-c">#</span> exec a custom Jupyter command </span>

    singularity <span class="pl-c1">exec</span> -B /run/user shub://ftabaro/singularity-coderefinery2020
    jupyter lab [options]

    </pre></div>

    <h3>

    <a id="user-content-option-2-local-image" class="anchor" href="#option-2-local-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option
    2: local image</h3>

    <p>These commands will run an instance of a locally hosted container. Any of these
    options require a locally available container image file, see below.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    start with defaults</span>

    ./coderefinery.sif [options]


    <span class="pl-c"><span class="pl-c">#</span> start with defaults using runscript
    directive</span>

    singularity run coderefinery.sif [options]


    <span class="pl-c"><span class="pl-c">#</span> start custom command</span>

    singularity <span class="pl-c1">exec</span> -B /run/user coderefinery.sif jupyter
    lab [options]</pre></div>

    <p>In both scenarios, all the JupyterLab options are fully supported.</p>

    <h1>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h1>

    <ol>

    <li>Clone this repository</li>

    <li>Build</li>

    </ol>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build coderefinery.sif
    Singularity</pre></div>

    <h1>

    <a id="user-content-configure-the-github-integration" class="anchor" href="#configure-the-github-integration"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configure
    the GitHub integration</h1>

    <p>In order to use the GitHub integration, an authentication token needs to be
    generated from GitHub and Jupyter lab needs to be configured to use it.</p>

    <p>To generate a token:</p>

    <ol>

    <li>Navigate to profile settings on GitHub: <a href="https://github.com/settings/profile">https://github.com/settings/profile</a>

    </li>

    <li>On the menu on the left, click "Developer Settings"</li>

    <li>On the menu on the left, click "Personal Access Tokens"</li>

    <li>Click on top right button "Generate new token"</li>

    <li>In the "Note" field insert a suitable name, e.g. "Jupyter"</li>

    <li>In the "Select scopes" menu, click on "repo" to allow full control of private
    repositories</li>

    <li>At the bottom of the page, click on "Generate token"</li>

    <li>At this point take note of the code in the green box, copy it and paste it
    to some secure place</li>

    </ol>

    <p>To configure Jupyter to use the token:</p>

    <ol>

    <li>Connect to the machine running the Jupyter container instance (e.g. narvi)</li>

    <li>If the file <code>~/.jupyter/jupyter_notebook_config.py</code> exists, skip
    the next step</li>

    <li>Generate a config file with:</li>

    </ol>

    <pre><code>singularity exec containers/coderefinery.sif jupyter notebook --generate-config

    </code></pre>

    <ol start="4">

    <li>Open the <code>~/.jupyter/jupyter_notebook_config.py</code> file with a text
    editor</li>

    <li>Navigate to the bottom of the file and add the following line replacing <code>&lt;
    YOUR_ACCESS_TOKEN &gt;</code> with the token generated above</li>

    </ol>

    <pre><code>c.GitHubConfig.access_token = ''&lt; YOUR_ACCESS_TOKEN &gt;''

    </code></pre>

    <p>At this point, starting the JupyterLab instance, no error message will be displayed
    and the user will be able to navigate GitHub repositories from the JupyterLab
    UI.</p>

    <h1>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h1>

    <p>Bug reports and pull requests are welcome on GitHub at <a href="https://github.com/ftabaro/singularity-coderefinery2020">https://github.com/ftabaro/singularity-coderefinery2020</a>.</p>

    <h1>

    <a id="user-content-references" class="anchor" href="#references" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>References</h1>

    <ul>

    <li><a href="https://coderefinery.github.io/2020-05-25-online/" rel="nofollow">Online
    CodeRefinery workshop</a></li>

    <li><a href="https://sylabs.io/guides/3.3/user-guide/quick_start.html" rel="nofollow">Singularity</a></li>

    <li><a href="https://hub.docker.com/r/jupyter/datascience-notebook" rel="nofollow">Jupyter
    datascience-notebook Docker image</a></li>

    <li><a href="https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line">GitHub
    token creation</a></li>

    <li><a href="https://github.com/jupyterlab/jupyterlab-git">JupyterLab Git extension</a></li>

    <li><a href="https://github.com/jupyterlab/jupyterlab-github">JupyterLab GitHub
    extension</a></li>

    <li><a href="https://ipywidgets.readthedocs.io/en/latest/" rel="nofollow">ipywidgets</a></li>

    </ul>

    '
  stargazers_count: 3
  subscribers_count: 1
  topics:
  - jupyterlab-container
  - jupyter
  - singularity
  updated_at: 1591371942.0
ftabaro/singularity-test:
  data_format: 2
  description: Test SingularityHub cloud builders
  filenames:
  - Singularity
  full_name: ftabaro/singularity-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-test-singularityhub-builder" class="anchor" href="#test-singularityhub-builder"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Test
    SingularityHub builder</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1597165176.0
fubar2/toolfactory-galaxy-server:
  data_format: 2
  description: 'GTN ToolFactory flavour of https://github.com/bgruening/docker-galaxy-stable '
  filenames:
  - compose/toolfactory-configurator/files/Singularity.def
  full_name: fubar2/toolfactory-galaxy-server
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-galaxy-toolfactory-appliance\" class=\"anchor\"\
    \ href=\"#galaxy-toolfactory-appliance\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Galaxy ToolFactory Appliance</h1>\n\
    <p>This is a ToolFactory flavoured developer appliance in Docker extending the\
    \ basic <code>docker-galaxy-stable</code> composition.</p>\n<ol>\n<li>The ToolFactory\
    \ - a form driven code generator to make new Galaxy tools from scripts - is installed\
    \ with a testing tool.</li>\n<li>A container for post-install configuration is\
    \ added to create the ToolFactory flavour. The new container then runs a planemo\
    \ server\noutside the Galaxy tool execution environment to test tools, returning\
    \ the planemo test\nreports, log and updated archive to the user's current history.</li>\n\
    <li>A history containing 15 demonstration tool generation jobs to rerun and build\
    \ on. Samples use bash, python, perl (yes, even perl. Galaxy is a very welcoming\
    \ community..),\nRscript and for more discerning users, prolog and lisp. Any scriptable\
    \ language in Conda should work.</li>\n</ol>\n<h2>\n<a id=\"user-content-built-on-docker-galaxy-stable\"\
    \ class=\"anchor\" href=\"#built-on-docker-galaxy-stable\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Built on\
    \ docker-galaxy-stable</h2>\n<p>This flavour of the docker-compose infrastructure\
    \ is based on <a href=\"https://github.com/bgruening/docker-galaxy-stable\">https://github.com/bgruening/docker-galaxy-stable</a>\
    \ - there is excellent documentation at\nthat site. Respect. A few minor pointers\
    \ only are provided below - please refer to the original documentation for details\
    \ about this extensive infrastructure for Galaxy flavours including\n(untested)\
    \ cluster and other deployment options.</p>\n<h2>\n<a id=\"user-content-a-standalone-pop-up-desktop-galaxy-appliance\"\
    \ class=\"anchor\" href=\"#a-standalone-pop-up-desktop-galaxy-appliance\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>A\
    \ standalone, pop-up desktop Galaxy appliance</h2>\n<p>The Appliance supporting\
    \ the ToolFactory is a fully featured <code>docker-galaxy-stable</code> Galaxy\
    \ server, ideal for scientists and developers\nwho can use a private pop-up desktop\
    \ server for learning how Galaxy works, building new tools, using interactive\
    \ environments and any available toolshed tools to do real work,\npotentially\
    \ at scale. The Appliance adds only one specific container. The others are all\
    \ pulled from Bj\xF6rn's docker-galaxy-stable quay.io containers.</p>\n<h2>\n\
    <a id=\"user-content-private-desktop-use-only-is-recommended\" class=\"anchor\"\
    \ href=\"#private-desktop-use-only-is-recommended\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Private desktop\
    \ use only is recommended.</h2>\n<p>This Appliance has been configured to weaken\
    \ some of Galaxy's strict job-runner isolation features so it can install and\
    \ test tools conveniently.\nIt is safe to run on a private Linux laptop or workstation.</p>\n\
    <p><strong>Running it on any server accessible from the public internet exposes\
    \ it to potential miscreants. This is strongly discouraged</strong>.</p>\n<p>Although\
    \ Galaxy's job execution security is very tight and safe, allowing potentially\
    \ hostile users to build and then immediately run their own tools exposes any\
    \ production server\nto unwelcome security risk.</p>\n<p>The Appliance runs as\
    \ a set of Docker containers, so it is secured to that extent from the host system.\
    \ ToolFactory and other related source code is\nincluded in this repository for\
    \ the curious or the dubious. Specific security disclosure details are discussed\
    \ in the compose documentation. The mechanisms described offer a\nconvenient way\
    \ for tools to remotely execute tasks outside the Galaxy job execution environment\
    \ in suitably private environments such as this Appliance.\nThis may open up interesting\
    \ uses for Galaxy on the desktop with specialised tools accessing GPU or other\
    \ local resources.</p>\n<h2>\n<a id=\"user-content-tutorial-and-documentation\"\
    \ class=\"anchor\" href=\"#tutorial-and-documentation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Tutorial and\
    \ documentation</h2>\n<p>There is an accompanying GTN ToolFactory developer tutorial\
    \ PR in beta test.\n<a href=\"https://training.galaxy.lazarus.name/training-material/topics/dev/tutorials/tool-generators/tutorial.html\"\
    \ rel=\"nofollow\">It is temporarily available here on a private server</a>\n\
    to help explain how this Appliance can be used to generate Galaxy tools from working\
    \ command line scripts. There is a linked advanced tutorial.</p>\n<h2>\n<a id=\"\
    user-content-installation-and-startup\" class=\"anchor\" href=\"#installation-and-startup\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation and startup</h2>\n<pre><code>git clone https://github.com/fubar2/toolfactory-galaxy-server\n\
    cd toolfactory-galaxy-server/compose\ndocker-compose pull\ndocker-compose up\n\
    </code></pre>\n<ul>\n<li>\n<p>Your appliance should be running with a local Galaxy\
    \ on localhost:8080 after a fair bit of activity and about 5-10 minutes. Wait\
    \ until all is done before logging in.</p>\n</li>\n<li>\n<p>Watch the logs as\
    \ they scroll by on the terminal. They are very instructive and informative for\
    \ those who need to understand how Galaxy actually works.</p>\n</li>\n<li>\n<p>Keep\
    \ an eye out for Conda processes on your machine.</p>\n</li>\n<li>\n<p>Wait until\
    \ they <strong>all</strong> stop.</p>\n</li>\n<li>\n<p>Restarting is much faster.</p>\n\
    </li>\n<li>\n<p>Out of the box login is <code>admin@galaxy.org</code> and the\
    \ password is <code>password</code></p>\n</li>\n<li>\n<p>This is obviously insecure\
    \ but convenient and easily changed at first login.</p>\n</li>\n<li>\n<p>Change\
    \ it more permanently in docker-compose.yml if you intend to use this Appliance\
    \ for your own work.</p>\n<ul>\n<li>At present, the admin_key default (<code>fakekey</code>)\
    \ is hard-wired into the ToolFactory boot process, but should be changed at first\
    \ login to something less well known if\nthe appliance is exposed at all.</li>\n\
    <li>The API key is the administrative key for the appliance Galaxy so if the Appliance\
    \ is accessible on a network, it is\nexposed to easy API based remote mischief\
    \ until a new API key is generated. Another good reason not to expose the Appliance\
    \ anywhere.</li>\n</ul>\n</li>\n</ul>\n<p>The container <code>/export</code> directory\
    \ is mounted locally at <code>...compose/export</code> .</p>\n<h2>\n<a id=\"user-content-demonstration-tools-are-the-functional-documentation\"\
    \ class=\"anchor\" href=\"#demonstration-tools-are-the-functional-documentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Demonstration tools are the functional documentation</h2>\n<p>See\
    \ how they were built, by rerunning the generating job to recreate the ToolFactory\
    \ form for the run.\nLook at the form settings for each one to see what can be\
    \ done.</p>\n<p>To view the form that generated each job, open the toolshed archive\
    \ or the XML by clicking on it, and select the <code>rerun</code> button.\nEdit\
    \ the form and rerun to create an updated tool. The history has previous versions\
    \ so work is not entirely lost.\nChange the tool ID to change the tool name and\
    \ avoid overwriting previous versions.</p>\n<h2>\n<a id=\"user-content-generating-your-own-tools\"\
    \ class=\"anchor\" href=\"#generating-your-own-tools\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Generating your\
    \ own tools</h2>\n<p>Generated tools are installed on build. The whole process\
    \ takes a few seconds normally. Refreshing the Galaxy panel will be needed for\
    \ the tool menu to be updated after the\nnewly generated tool is installed. It\
    \ will be found in the <code>ToolFactory Generated Tools</code> section.</p>\n\
    <p>A new tool requiring Conda dependencies will take time as those must be installed\
    \ the first time it is run. After that first run or if the dependency is already\
    \ installed,\nthe tool will run without delay.</p>\n<p><strong>If two or more\
    \ new tools that need new dependencies on first run try to install them at the\
    \ same time, Conda will fail in interesting ways. It is not designed for multiple\
    \ simultaneous users</strong></p>\n<p>Run one new tool at a time to avoid this\
    \ causing problems. Once the dependency is installed, the tool will not need to\
    \ install it again unless the dependency is altered.</p>\n<p>Choose the names\
    \ thoughtfully and be warned: there are no checks on tool names - any existing\
    \ installed tool with the same name will be overwritten permanently. The history\n\
    will retain all the generating jobs if you accidentally overwrite a tool.</p>\n\
    <p>Refresh the page (by clicking the home icon or the \"analysis\" tab) to see\
    \ them in the <code>ToolFactory Generated Tools</code> section and try them out.</p>\n\
    <p>Rerun the job and adjust the form. Rinse and repeat until ready.</p>\n<p>The\
    \ generated tool has not been run to generate test outputs, so the archive is\
    \ not complete although the installed tool may work fine.</p>\n<p>To generate\
    \ a real, tested toolshed archive, use the companion <code>planemo_test</code>\
    \ tool. Planemo will be run in a separate\ncontainer. The generated test outputs\
    \ and the newly updated tested toolshed archive will appear when the job is done.\
    \ The very first test in a\nfresh Appliance takes a few minutes as Conda installs\
    \ some dependencies - only needed once.\nSubsequently more like a minute or two,\
    \ depending on Conda time to install all new dependencies needed\nfor the tool\
    \ to run.</p>\n<h2>\n<a id=\"user-content-to-safely-shut-the-appliance-down\"\
    \ class=\"anchor\" href=\"#to-safely-shut-the-appliance-down\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To\
    \ safely shut the appliance down</h2>\n<p>If you used <code>-d</code> to detach,</p>\n\
    <p><code>docker-compose down</code></p>\n<p>from the same place you started should\
    \ shut it down nicely</p>\n<p>Otherwise, CtrlC from the attached console will\
    \ stop the services.</p>\n<h2>\n<a id=\"user-content-if-things-go-wrong-or-if-the-appliance-is-no-longer-needed\"\
    \ class=\"anchor\" href=\"#if-things-go-wrong-or-if-the-appliance-is-no-longer-needed\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>If things go wrong or if the Appliance is no longer needed</h2>\n\
    <ol>\n<li>\n<p>Delete the <code>...compose/export</code> directory - you will\
    \ need <code>sudo rm -rf export/*</code></p>\n</li>\n<li>\n<p>Then you can delete\
    \ the parent git or wget <code>toolfactory-galaxy-server</code> directory</p>\n\
    </li>\n<li>\n<p>Use <code>docker system prune</code> and respond <code>y</code>\
    \ to the prompt to clean up any damaged containers.</p>\n</li>\n<li>\n<p>Remove\
    \ all the docker images in the usual way.</p>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-security---why-this-appliance-is-not-suitable-for-exposing-on-the-public-internet\"\
    \ class=\"anchor\" href=\"#security---why-this-appliance-is-not-suitable-for-exposing-on-the-public-internet\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Security - why this Appliance is not suitable for exposing on the\
    \ public internet</h2>\n<p>See <a href=\"https://github.com/fubar2/toolfactory-galaxy-server/tree/main/compose#readme\"\
    >the notes on Appliance security considerations.</a></p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621871144.0
fzimmermann89/idi:
  data_format: 2
  description: Simulating, Reconstructing and Analysing Data for FEL IDI Experiments
  filenames:
  - Singularity.py38
  - Singularity
  - Singularity.simple
  full_name: fzimmermann89/idi
  latest_release: '210514'
  readme: '<p>CAVE: Hic sunt dracones</p>

    <p><em>The code is a mess, undocumented and only certain code paths are tested.</em></p>

    <h1>

    <a id="user-content-idi---incoherent-diffraction-imaging" class="anchor" href="#idi---incoherent-diffraction-imaging"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>IDI
    - INCOHERENT DIFFRACTION IMAGING</h1>

    <p><a href="https://singularity-hub.org/collections/4824" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg"
    alt="tests" style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://www.travis-ci.com/fzimmermann89/idi.svg?branch=master"
    style="max-width:100%;"></a></p>

    <p>Singularity Image now at <a href="https://cloud.sylabs.io/library/_container/607b669a4ad4aa1fdea0c43c"
    rel="nofollow">library://fzimmermann89/idi/idi</a></p>

    <p>Conda Pacakges at <a href="https://anaconda.org/zimmf/idi" rel="nofollow">zimmf/idi</a></p>

    <p>PIP Source at <a href="https://pypi.org/project/idi/" rel="nofollow">idi</a></p>

    <p>Wheels at <a href="https://github.com/fzimmermann89/idi/releases/latest">Releases</a></p>

    <h2>

    <a id="user-content-content-of-the-repo" class="anchor" href="#content-of-the-repo"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>content
    of the repo</h2>

    <ul>

    <li>ipynb: example notebooks</li>

    <li>simulation: simulation of incoherent images</li>

    <li>reconstruction: direct and ft based reconstruction</li>

    <li>util: some small utilities for data analysis, geometry and random distributions,
    etc.</li>

    </ul>

    <h2>

    <a id="user-content-preparation-for-slac-sdf" class="anchor" href="#preparation-for-slac-sdf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>preparation
    for slac sdf:</h2>

    <p>Use Singulariy, if using OOD launcher, use the following to start a jupyterhub</p>

    <pre><code>    function jupyter() { singularity run --app jupyter --nv -B /sdf,/gpfs,/scratch,/lscratch
    library://fzimmermann89/idi/idi $@; }

    </code></pre>

    <h2>

    <a id="user-content-preparation-for-sacla" class="anchor" href="#preparation-for-sacla"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>preparation
    for sacla:</h2>

    <ul>

    <li>Download and install miniconda, setup ssh tunnel for web access.</li>

    <li><code>conda create -n local3 python=3.7 numpy mkl mkl-dev ipython ipykernel
    cython jinja2 numba numexpr matplotlib six scipy jupyterlab</code></li>

    <li><code>conda activate local3</code></li>

    <li><code>pip install https://github.com/fzimmermann89/idi/</code></li>

    <li><code>python -m ipykernel install --user --name local-simulation-env3 --display-name
    "local simulation(py37)"</code></li>

    </ul>

    <p>(C) Felix Zimmermann</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - idi
  - reconstruction
  - simulation
  - xray
  - incoherent-images
  - fel
  updated_at: 1621013018.0
gagandaroach/rosie:
  data_format: 2
  description: Milwaukee School of Engineering ROSIE Supercomputer User Guide and
    Compute Resources
  filenames:
  - singularity/Singularity.PyTorch.def
  - singularity/Singularity.DrivSim.tf1.def
  - singularity/Singularity.cs3450.def
  - singularity/Singularity.DataScience.def
  - singularity/Singularity.Tensorflow.v1.def
  - singularity/Singularity.DeepstreamSDK.def
  - singularity/Singularity.Tensorflow.v2.def
  - docs/cli/Singularity.md
  full_name: gagandaroach/rosie
  latest_release: null
  readme: '<h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <ol>

    <li>git clone --recurse-submodules <a href="https://github.com/mgroth0/dnn">https://github.com/mgroth0/dnn</a>

    </li>

    <li>install <a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow">miniconda</a>

    </li>

    <li><code>conda update conda</code></li>

    <li>

    <code>conda create --name dnn --file requirements.txt</code> (requirements.txt
    is currently not working, TODO)</li>

    <li>might need to separately <code>conda install -c mgroth0 mlib-mgroth0</code>--
    When updating, use <code>conda install --file requirements.txt;</code>

    </li>

    <li><code>conda activate dnn</code></li>

    </ol>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ul>

    <li>

    <p>./dnn</p>

    </li>

    <li>

    <p>Generate some images, train/test a model, run analyses, and generate plots.
    Tested on Mac, but not yet on linux/Windows.</p>

    </li>

    <li>

    <p><code>./dnn -cfg=gen_images --INTERACT=0</code></p>

    </li>

    <li>

    <p><code>./dnn -cfg=test_one --INTERACT=0</code></p>

    </li>

    </ul>

    <p>The second command will fail with a Mathematica-related error, but your results
    will be saved in <code>_figs</code>.</p>

    <p>TODO: have to also consider running and developing other executables here:
    human_exp_1 and human_analyze</p>

    <h2>

    <a id="user-content-configuration" class="anchor" href="#configuration" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h2>

    <p>-MODE: (default = FULL) is a string that can contain any combination of the
    following (example: "CLEAN JUSTRUN")</p>

    <ul>

    <li>CLEAN</li>

    <li>JUSTRUN</li>

    <li>GETANDMAKE</li>

    <li>MAKEREPORT</li>

    </ul>

    <p>Edit <a href="">cfg.yml</a> to save configuration options. Feel free to push
    these.</p>

    <p>If there is anything hardcoded that you''d like to be configurable, please
    submit an issue.</p>

    <h2>

    <a id="user-content-testing" class="anchor" href="#testing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Testing</h2>

    <p>todo</p>

    <h2>

    <a id="user-content-development" class="anchor" href="#development" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Development</h2>

    <ul>

    <li>TODO: have separate development and user modes. Developer mode has PYTHONPATH
    link to mlib and instructions for resolving and developing in ide in parallel.
    User mode has mlib as normal dependency. might need to use <code>conda uninstall
    mlib-mgroth0 --force</code>. Also in these public readmes or reqs.txt I have to
    require a specific mlib version</li>

    <li>./dnn build</li>

    </ul>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>Darius, Xavier, Pawan</p>

    <p>heuritech, raghakot, joel</p>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1615357040.0
ganprad/volcano-ingv:
  data_format: 2
  description: Feature engineering for INGV data
  filenames:
  - Singularity
  full_name: ganprad/volcano-ingv
  latest_release: null
  readme: '<h1>

    <a id="user-content-time-series-feature-engineering-with-ingv-volcano-prediction-competition-data"
    class="anchor" href="#time-series-feature-engineering-with-ingv-volcano-prediction-competition-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Time
    series feature engineering with INGV Volcano prediction competition data:</h1>

    <p><a href="https://www.kaggle.com/c/predict-volcanic-eruptions-ingv-oe" rel="nofollow">https://www.kaggle.com/c/predict-volcanic-eruptions-ingv-oe</a></p>

    <p>The goal of the competition is the build a regression model that can accurately

    identify time to an eruption event using time series recordings from 10 sensors.</p>

    <h3>

    <a id="user-content-summary" class="anchor" href="#summary" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Summary:</h3>

    <pre><code>+ Some of the sensors don''t have any data. These are filled with zeros.

    + Overall strategy is to build features using the time series data that will be
    used by a model.

    + Approach highlighted here is based on dynamic mode decomposition.

    </code></pre>

    <p>References:</p>

    <ul>

    <li><a href="http://www.databookuw.com/" rel="nofollow">http://www.databookuw.com/</a></li>

    <li><a href="http://dmdbook.com/" rel="nofollow">http://dmdbook.com/</a></li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics:
  - kaggle
  - kaggle-competition
  - kaggle-datasets
  - time-series
  updated_at: 1607669452.0
gearslaboratory/gears-singularity:
  data_format: 2
  description: Singularity containers generated by the GEARS Lab at the University
    of Nevada, Reno
  filenames:
  - singularity-definitions/general_use/Singularity.gears-general
  - singularity-definitions/general_use/Singularity.R
  - singularity-definitions/specialized_use/Singularity.gears-cloud-sdk
  - singularity-definitions/specialized_use/Singularity.gears-lastools
  - singularity-definitions/specialized_use/Singularity.gears-general-xenial
  - singularity-definitions/specialized_use/Singularity.gears-tls_fuels
  - singularity-definitions/specialized_use/Singularity.gears-pdal
  - singularity-definitions/development/Singularity.treeseg
  - singularity-definitions/development/Singularity.test_theo
  - singularity-definitions/development/Singularity.gears-treeseg-burt
  - singularity-definitions/development/Singularity.gears-taudem
  - singularity-definitions/development/Singularity.gears-general-focal
  - singularity-definitions/development/Singularity.gears-treeseg-calders
  - singularity-definitions/development/Singularity.gears-general-eoan
  - singularity-definitions/development/Singularity.gears-rfsrc-openmpi
  - singularity-definitions/development/Singularity.gears-lidR
  - singularity-definitions/development/Singularity.gears-computree
  - singularity-definitions/development/Singularity.gears-treeseg-greenberg
  - singularity-definitions/development/Singularity.gears-cloudcompare
  - singularity-definitions/courses/Singularity.pronghorn-tutorial
  - singularity-definitions/courses/Singularity.grad778-f19-module-09
  full_name: gearslaboratory/gears-singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-google-summer-of-code-2020\" class=\"anchor\"\
    \ href=\"#google-summer-of-code-2020\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Google Summer of code 2020</h1>\n\
    <p>This is my google summer of code project 2020. I worked with the Redhenlab\
    \ organization. My project is based on the Image and Audio clustering and to deploy\
    \ in Rapid Annotator.\nFor more details about the code and weekly progress during\
    \ Google Summer of Code visit my GSoC blog:\n<a href=\"https://himani2000.github.io/gsoc/index.html\"\
    \ rel=\"nofollow\">https://himani2000.github.io/gsoc/index.html</a></p>\n<h2>\n\
    <a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installation</h2>\n\
    <p>To install the dependencies use singualrity container .</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>\n1.Pull the singularity image using \n\
    \nsingularity pull --name clustering-image_and_audio.img shub://Himani2000/GSOC_2020:clustering\n\
    \n2. Run the image using \n\nsingularity <span class=\"pl-c1\">exec</span> -B\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">`</span>pwd<span class=\"pl-pds\"\
    >`</span></span> [singularity_image_name ].img python3 [python file]</pre></div>\n\
    <h2>\n<a id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Contributing</h2>\n<pre><code>Pull requests are welcome. For major\
    \ changes, please open an issue first to discuss what you would like to change.\n\
    \n</code></pre>\n<h2>\n<a id=\"user-content-acknowledgments\" class=\"anchor\"\
    \ href=\"#acknowledgments\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Acknowledgments</h2>\n<pre><code>1. Google summer\
    \ of code 2020\n2. Redhenlab \n</code></pre>\n"
  stargazers_count: 3
  subscribers_count: 2
  topics: []
  updated_at: 1613698198.0
gipert/Singularity.def:
  data_format: 2
  description: My Singularity recipe files
  filenames:
  - julia/Singularity.def
  - root-cern/Singularity.def
  - gerda-tgsend/Singularity.def
  - texlive/Singularity.def
  - bat/Singularity.def
  - lilypond/Singularity.def
  - itunes/Singularity.def
  - asciinema/Singularity.def
  - arch-base/Singularity.def
  - centos-base/Singularity.def
  full_name: gipert/Singularity.def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - singularity
  - containers
  updated_at: 1587880077.0
github/linguist:
  data_format: 2
  description: Language Savant. If your repository's language is being reported incorrectly,
    send us a pull request!
  filenames:
  - samples/Singularity/filenames/Singularity
  full_name: github/linguist
  latest_release: v7.14.0
  readme: "<h1>\n<a id=\"user-content-linguist\" class=\"anchor\" href=\"#linguist\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Linguist</h1>\n<p><a href=\"https://github.com/github/linguist/actions\"\
    ><img src=\"https://github.com/github/linguist/workflows/Run%20Tests/badge.svg\"\
    \ alt=\"Actions Status\" style=\"max-width:100%;\"></a></p>\n<p>This library is\
    \ used on GitHub.com to detect blob languages, ignore binary or vendored files,\
    \ suppress generated files in diffs, and generate language breakdown graphs.</p>\n\
    <h2>\n<a id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Documentation</h2>\n<ul>\n<li><a href=\"/docs/how-linguist-works.md\"\
    >How Linguist works</a></li>\n<li><a href=\"/docs/overrides.md\">Change Linguist's\
    \ behaviour with overrides</a></li>\n<li><a href=\"/docs/troubleshooting.md\"\
    >Troubleshooting</a></li>\n<li><a href=\"CONTRIBUTING.md\">Contributing guidelines</a></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installation</h2>\n<p>Install the gem:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>gem install github-linguist</pre></div>\n<h3>\n\
    <a id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Dependencies</h3>\n\
    <p>Linguist is a Ruby library so you will need a recent version of Ruby installed.\n\
    There are known problems with the macOS/XCode supplied version of Ruby that causes\
    \ problems installing some of the dependencies.\nAccordingly, we highly recommend\
    \ you install a version of Ruby using Homebrew, <code>rbenv</code>, <code>rvm</code>,\
    \ <code>ruby-build</code>, <code>asdf</code> or other packaging system, before\
    \ attempting to install Linguist and the dependencies.</p>\n<p>Linguist uses <a\
    \ href=\"https://github.com/brianmario/charlock_holmes\"><code>charlock_holmes</code></a>\
    \ for character encoding and <a href=\"https://github.com/libgit2/rugged\"><code>rugged</code></a>\
    \ for libgit2 bindings for Ruby.\nThese components have their own dependencies.</p>\n\
    <ol>\n<li>charlock_holmes\n<ul>\n<li>cmake</li>\n<li>pkg-config</li>\n<li><a href=\"\
    http://site.icu-project.org/\" rel=\"nofollow\">ICU</a></li>\n<li><a href=\"https://zlib.net/\"\
    \ rel=\"nofollow\">zlib</a></li>\n</ul>\n</li>\n<li>rugged\n<ul>\n<li><a href=\"\
    https://curl.haxx.se/libcurl/\" rel=\"nofollow\">libcurl</a></li>\n<li><a href=\"\
    https://www.openssl.org\" rel=\"nofollow\">OpenSSL</a></li>\n</ul>\n</li>\n</ol>\n\
    <p>You may need to install missing dependencies before you can install Linguist.\n\
    For example, on macOS with <a href=\"http://brew.sh/\" rel=\"nofollow\">Homebrew</a>:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>brew install cmake pkg-config\
    \ icu4c</pre></div>\n<p>On Ubuntu:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo apt-get install cmake pkg-config libicu-dev zlib1g-dev libcurl4-openssl-dev\
    \ libssl-dev ruby-dev</pre></div>\n<h2>\n<a id=\"user-content-usage\" class=\"\
    anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-application-usage\"\
    \ class=\"anchor\" href=\"#application-usage\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Application usage</h3>\n<p>Linguist\
    \ can be used in your application as follows:</p>\n<div class=\"highlight highlight-source-ruby\"\
    ><pre><span class=\"pl-en\">require</span> <span class=\"pl-s\">'rugged'</span>\n\
    <span class=\"pl-en\">require</span> <span class=\"pl-s\">'linguist'</span>\n\n\
    <span class=\"pl-s1\">repo</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-v\">Rugged</span>::<span class=\"pl-v\">Repository</span><span class=\"pl-kos\"\
    >.</span><span class=\"pl-en\">new</span><span class=\"pl-kos\">(</span><span\
    \ class=\"pl-s\">'.'</span><span class=\"pl-kos\">)</span>\n<span class=\"pl-s1\"\
    >project</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Linguist</span>::<span\
    \ class=\"pl-v\">Repository</span><span class=\"pl-kos\">.</span><span class=\"\
    pl-en\">new</span><span class=\"pl-kos\">(</span><span class=\"pl-s1\">repo</span><span\
    \ class=\"pl-kos\">,</span> <span class=\"pl-s1\">repo</span><span class=\"pl-kos\"\
    >.</span><span class=\"pl-en\">head</span><span class=\"pl-kos\">.</span><span\
    \ class=\"pl-en\">target_id</span><span class=\"pl-kos\">)</span>\n<span class=\"\
    pl-s1\">project</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">language</span>\
    \       <span class=\"pl-c\">#=&gt; \"Ruby\"</span>\n<span class=\"pl-s1\">project</span><span\
    \ class=\"pl-kos\">.</span><span class=\"pl-en\">languages</span>      <span class=\"\
    pl-c\">#=&gt; { \"Ruby\" =&gt; 119387 }</span></pre></div>\n<h3>\n<a id=\"user-content-command-line-usage\"\
    \ class=\"anchor\" href=\"#command-line-usage\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Command line usage</h3>\n<h4>\n\
    <a id=\"user-content-git-repository\" class=\"anchor\" href=\"#git-repository\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Git Repository</h4>\n<p>A repository's languages stats can also be\
    \ assessed from the command line using the <code>github-linguist</code> executable.\n\
    Without any options, <code>github-linguist</code> will output the breakdown that\
    \ correlates to what is shown in the language stats bar.\nThe <code>--breakdown</code>\
    \ flag will additionally show the breakdown of files by language.</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-c1\">cd</span> /path-to-repository/\n\
    github-linguist</pre></div>\n<p>You can try running <code>github-linguist</code>\
    \ on the root directory in this repository itself:</p>\n<div class=\"highlight\
    \ highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">github-linguist\
    \ --breakdown</span>\n<span class=\"pl-c1\">68.57%  Ruby</span>\n<span class=\"\
    pl-c1\">22.90%  C</span>\n<span class=\"pl-c1\">6.93%   Go</span>\n<span class=\"\
    pl-c1\">1.21%   Lex</span>\n<span class=\"pl-c1\">0.39%   Shell</span>\n\n<span\
    \ class=\"pl-c1\">Ruby:</span>\n<span class=\"pl-c1\">Gemfile</span>\n<span class=\"\
    pl-c1\">Rakefile</span>\n<span class=\"pl-c1\">bin/git-linguist</span>\n<span\
    \ class=\"pl-c1\">bin/github-linguist</span>\n<span class=\"pl-c1\">ext/linguist/extconf.rb</span>\n\
    <span class=\"pl-c1\">github-linguist.gemspec</span>\n<span class=\"pl-c1\">lib/linguist.rb</span>\n\
    <span class=\"pl-c1\">\u2026</span></pre></div>\n<h4>\n<a id=\"user-content-single-file\"\
    \ class=\"anchor\" href=\"#single-file\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Single file</h4>\n<p>Alternatively\
    \ you can find stats for a single file using the <code>github-linguist</code>\
    \ executable.</p>\n<p>You can try running <code>github-linguist</code> on files\
    \ in this repository itself:</p>\n<div class=\"highlight highlight-text-shell-session\"\
    ><pre>$ <span class=\"pl-s1\">github-linguist grammars.yml</span>\n<span class=\"\
    pl-c1\">grammars.yml: 884 lines (884 sloc)</span>\n<span class=\"pl-c1\">  type:\
    \      Text</span>\n<span class=\"pl-c1\">  mime type: text/x-yaml</span>\n<span\
    \ class=\"pl-c1\">  language:  YAML</span></pre></div>\n<h4>\n<a id=\"user-content-docker\"\
    \ class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Docker</h4>\n<p>If you have Docker\
    \ installed you can build an image and run Linguist within a container:</p>\n\
    <div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\"\
    >docker build -t linguist <span class=\"pl-c1\">.</span></span>\n$ <span class=\"\
    pl-s1\">docker run --rm -v <span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pwd<span\
    \ class=\"pl-pds\">)</span></span>:<span class=\"pl-s\"><span class=\"pl-pds\"\
    >$(</span>pwd<span class=\"pl-pds\">)</span></span> -w <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">$(</span>pwd<span class=\"pl-pds\">)</span></span> -t linguist</span>\n\
    <span class=\"pl-c1\">68.57%  Ruby</span>\n<span class=\"pl-c1\">22.90%  C</span>\n\
    <span class=\"pl-c1\">6.93%   Go</span>\n<span class=\"pl-c1\">1.21%   Lex</span>\n\
    <span class=\"pl-c1\">0.39%   Shell</span>\n$ <span class=\"pl-s1\">docker run\
    \ --rm -v <span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pwd<span class=\"\
    pl-pds\">)</span></span>:<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>pwd<span\
    \ class=\"pl-pds\">)</span></span> -w <span class=\"pl-s\"><span class=\"pl-pds\"\
    >$(</span>pwd<span class=\"pl-pds\">)</span></span> -t linguist github-linguist\
    \ --breakdown</span>\n<span class=\"pl-c1\">68.57%  Ruby</span>\n<span class=\"\
    pl-c1\">22.90%  C</span>\n<span class=\"pl-c1\">6.93%   Go</span>\n<span class=\"\
    pl-c1\">1.21%   Lex</span>\n<span class=\"pl-c1\">0.39%   Shell</span>\n\n<span\
    \ class=\"pl-c1\">Ruby:</span>\n<span class=\"pl-c1\">Gemfile</span>\n<span class=\"\
    pl-c1\">Rakefile</span>\n<span class=\"pl-c1\">bin/git-linguist</span>\n<span\
    \ class=\"pl-c1\">bin/github-linguist</span>\n<span class=\"pl-c1\">ext/linguist/extconf.rb</span>\n\
    <span class=\"pl-c1\">github-linguist.gemspec</span>\n<span class=\"pl-c1\">lib/linguist.rb</span>\n\
    <span class=\"pl-c1\">\u2026</span></pre></div>\n<h2>\n<a id=\"user-content-contributing\"\
    \ class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Contributing</h2>\n<p>Please\
    \ check out our <a href=\"CONTRIBUTING.md\">contributing guidelines</a>.</p>\n\
    <h2>\n<a id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>License</h2>\n\
    <p>The language grammars included in this gem are covered by their repositories'\
    \ respective licenses.\n<a href=\"/vendor/README.md\"><code>vendor/README.md</code></a>\
    \ lists the repository for each grammar.</p>\n<p>All other files are covered by\
    \ the MIT license, see <a href=\"./LICENSE\"><code>LICENSE</code></a>.</p>\n"
  stargazers_count: 8769
  subscribers_count: 385
  topics:
  - syntax-highlighting
  - language-grammars
  - language-statistics
  - linguistic
  updated_at: 1621976099.0
glentner/PostgreSQL-Singularity:
  data_format: 2
  description: Run PostgreSQL server within a Singularity container against isolated
    directory.
  filenames:
  - Singularity
  full_name: glentner/PostgreSQL-Singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-postgresql-singularity" class="anchor" href="#postgresql-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PostgreSQL-Singularity</h1>

    <p>Run PostgreSQL server within a Singularity container against isolated directory.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1620819429.0
gletort/PhysiBoSS:
  data_format: 2
  description: Multiscale simulation of multi-cellular system
  filenames:
  - MaBoSS-env-2.0/containers/singularity/Singularity
  full_name: gletort/PhysiBoSS
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-physiboss\" class=\"anchor\" href=\"#physiboss\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>PhysiBoSS</h1>\n<p>Multiscale simulation of multi-cellular system</p>\n\
    <p>Overview:</p>\n<ul>\n<li><a href=\"#presentation\">Presentation</a></li>\n\
    <li><a href=\"#usage\">Usage</a></li>\n<li><a href=\"#documentation\">Documentation</a></li>\n\
    <li><a href=\"#references\">References</a></li>\n<li><a href=\"#remarks\">Remarks</a></li>\n\
    </ul>\n<h2>\n<a id=\"user-content-presentation\" class=\"anchor\" href=\"#presentation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Presentation</h2>\n<p>PhysiBoSS (PhysiCell-MaBoSS) is C++ software\
    \ for multiscale simulation of heterogeneous multi-cellular system. It integrates\
    \ together cell's internal signalling pathway model (boolean formalism), physical\
    \ representation of cell (agent-based) and extra-cellular matrix diffusing or\
    \ fixed entities.\nIt is adapted from <a href=\"http://physicell.mathcancer.org\"\
    \ rel=\"nofollow\">PhysiCell</a> sources, with the boolean network computation\
    \ inside each cell from <a href=\"http://maboss.curie.fr\" rel=\"nofollow\">MaBoSS</a>\
    \ software.</p>\n<p><a href=\"./doc/imgs/hello.png?raw=true\" target=\"_blank\"\
    \ rel=\"noopener noreferrer\"><img src=\"./doc/imgs/hello.png?raw=true\" alt=\"\
    Hello world image\" title=\"PhysiBoSS simulation example\" style=\"max-width:100%;\"\
    ></a></p>\n<h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Usage</h2>\n<h3>\n<a id=\"user-content-compiling-physiboss\" class=\"\
    anchor\" href=\"#compiling-physiboss\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Compiling PhysiBoSS</h3>\n<p>PhysiBoSS\
    \ should run and be easily installed on Linux and MacOS system.\nIt requires moderatly\
    \ recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS\
    \ library requires <code>flex</code> and <code>bison</code> library, usually already\
    \ present (and can be easily installed on e.g. Linux ubuntu with <code>sudo apt-get\
    \ install bison flex</code>). We also provide a <a href=\"https://github.com/gletort/PhysiBoSS/tree/master/Docker\"\
    >Docker image</a> of PhysiBoSS that can be used if it cannot be installed in your\
    \ machine. It can also be used without any installation via a Web interface for\
    \ specific simulations on <a href=\"#nanohub\">nanohub</a>.</p>\n<p>To install\
    \ it on Linux system, from a Terminal:\nClone the repository on your local machine,\
    \ and go inside the main directory. Type <code>make install</code>, which will\
    \ install and compile MaBoSS then PhysiBoSS. The executables will be created in\
    \ the 'bin' directory if all goes well.\nIt can be compiled in 'Debug', 'Release'\
    \ or 'Proliling' modes, to set in the 'Makefile' file. Default is 'Release' mode\
    \ (fastest).\nYou might also have to change your c++ compiler in the Makefile\
    \ according to your operating system.</p>\n<p>Commands list:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>git clone https://github.com/gletort/PhysiBoSS.git\n\
    <span class=\"pl-c1\">cd</span> PhysiBoSS\nmake install</pre></div>\n<p>If errors\
    \ happened during the compilation, please refer to the <a href=\"https://github.com/gletort/PhysiBoSS/wiki/Installation\"\
    >installation</a> page.</p>\n<h3>\n<a id=\"user-content-running-one-simulation\"\
    \ class=\"anchor\" href=\"#running-one-simulation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running one simulation</h3>\n\
    <p>To run a simulation, you need (at least) a XML parameter file indicating the\
    \ conditions of the simulation, and the networks file (you can find some on <a\
    \ href=\"http://maboss.curie.fr\" rel=\"nofollow\">MaBoSS website</a> and on our\
    \ <a href=\"https://github.com/ArnauMontagud/Logical_modelling_pipeline\">logical\
    \ modelling pipeline repository</a>).\nOther options are possible, cf the code-documentation\
    \ or this repository wiki for more informations.</p>\n<p>Example of a parameter\
    \ file (with only few parameters shown):</p>\n<div class=\"highlight highlight-text-xml\"\
    ><pre>  &lt;?<span class=\"pl-ent\">xml</span><span class=\"pl-e\"> version</span>=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>1.0<span class=\"pl-pds\">\"\
    </span></span><span class=\"pl-e\"> encoding</span>=<span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>UTF-8<span class=\"pl-pds\">\"</span></span> ?&gt;\n\
    \ \n  &lt;<span class=\"pl-ent\">simulation</span>&gt;\n \t\t&lt;<span class=\"\
    pl-ent\">time_step</span>&gt; 0.2 &lt;/<span class=\"pl-ent\">time_step</span>&gt;\n\
    \ \t\t&lt;<span class=\"pl-ent\">mechanics_time_step</span>&gt; 0.1 &lt;/<span\
    \ class=\"pl-ent\">mechanics_time_step</span>&gt;\n \t\t....\n  &lt;/<span class=\"\
    pl-ent\">simulation</span>&gt;\n \n  &lt;<span class=\"pl-ent\">cell_properties</span>&gt;\n\
    \ \t\t&lt;<span class=\"pl-ent\">mode_motility</span>&gt; 1 &lt;/<span class=\"\
    pl-ent\">mode_motility</span>&gt;\n \t\t&lt;<span class=\"pl-ent\">polarity_coefficient</span>&gt;\
    \ 0.5 &lt;/<span class=\"pl-ent\">polarity_coefficient</span>&gt;\n \t\t...\n\
    \  &lt;/<span class=\"pl-ent\">cell_properties</span>&gt;\n \n  &lt;<span class=\"\
    pl-ent\">network</span>&gt;\n \t\t&lt;<span class=\"pl-ent\">network_update_step</span>&gt;\
    \ 10 &lt;/<span class=\"pl-ent\">network_update_step</span>&gt;\n \t\t...\n  &lt;/<span\
    \ class=\"pl-ent\">network</span>&gt;\n \n  &lt;<span class=\"pl-ent\">initial_configuration</span>&gt;\n\
    \ \t\t&lt;<span class=\"pl-ent\">load_cells_from_file</span>&gt; init.txt &lt;/<span\
    \ class=\"pl-ent\">load_cells_from_file</span>&gt;\n \t\t...\n  &lt;/<span class=\"\
    pl-ent\">initial_configuration</span>&gt;</pre></div>\n<h3>\n<a id=\"user-content-image-and-analyse-a-simulation\"\
    \ class=\"anchor\" href=\"#image-and-analyse-a-simulation\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Image and\
    \ analyse a simulation</h3>\n<p>To visualize graphically the result of a simulation,\
    \ with use the software Paraview (or you can also generate a <code>.svg</code>\
    \ snapshot of the simulation). Analysis of the result files were done with python\
    \ scripts proposed in this directory. For documentation on how to use Paraview\
    \ to set-up the rendering of PhysiBoSS outputs, see <a href=\"https://github.com/gletort/PhysiBoSS/wiki/Paraviewing\"\
    >here</a>, with the explication on how to draw spheres from a set of points (x,\
    \ y, z, radius).</p>\n<h3>\n<a id=\"user-content-nanohub\" class=\"anchor\" href=\"\
    #nanohub\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Nanohub</h3>\n<p>PhysiBoSS can be directly used via a Web interface\
    \ on nanohub. This allows to run it without any installation and running directly\
    \ on the server and can be used without any coding skills. The parameters of the\
    \ simulation can be entered in the interface and then the simulation will be runned\
    \ on the nanohub server. It just required a nanohub account.\nAvailable simulations\
    \ tools of PhysiBoSS can be found on <a href=\"https://nanohub.org/resources/tools\"\
    \ rel=\"nofollow\">https://nanohub.org/resources/tools</a>, under the keywords\
    \ PhysiBoSS or PhysiBoSSa.</p>\n<p>A model of tumors cell spheroid growing and\
    \ invading into the surrounding extra-cellular matrix (ECM) is currently available\
    \ <a href=\"https://proxy.nanohub.org/weber/1663662/howueVcBQ44wNqfD/12/apps/project_repo_ecm_simul.ipynb?\"\
    \ rel=\"nofollow\">PhysiBoSSa_ECM</a>. Various parameters as the density of the\
    \ extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta\
    \ production... can be tuned by the user.</p>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p>Code-oriented\
    \ documentation can be generated with Doxygen:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>make doc</pre></div>\n<p>in the main directory.\nIt can be configured in\
    \ the Doxyfile file present in this directory.\nIt will generate the html documentation\
    \ in the doc/html directory.\nYou can visualize it in a browser, e.g.:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>firefox doc/html/index.html\
    \ <span class=\"pl-k\">&amp;</span></pre></div>\n<p>You can also refer to (future)\
    \ publications with PhysiBoSS for scientific applications of this software and\
    \ description of the models.</p>\n<p>Step-by-step examples with the necessary\
    \ files to run them are also proposed in the 'examples' directory and on the <a\
    \ href=\"https://github.com/gletort/PhysiBoSS/wiki\">Wiki</a> of this repository.</p>\n\
    <h2>\n<a id=\"user-content-references\" class=\"anchor\" href=\"#references\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>References</h2>\n<p> For PhysiBoSS: </p>\n<ul>\n<li>PhysiBoSS publication:\
    \ Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A,\
    \ Calzone L . <a href=\"https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty766/5087713\"\
    \ rel=\"nofollow\"> PhysiBoSS: a multi-scale agent-based modelling framework integrating\
    \ physical dimension and cell signalling. </a> Bioinformatics, bty766, doi:10.1093/bioinformatics/bty766\n\
    </li>\n<br>\n </ul>\n<p>For PhysiCell: </p><ul>\n<li>\n<a href=\"http://physicell.mathcancer.org\"\
    \ rel=\"nofollow\">Paul Macklin's lab website </a> </li>\n<li>PhysiCell publication:\
    \ A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell:\
    \ an Open Source Physics-Based <a href=\"class_cell.html\" title=\"Dynamic (alive)\
    \ cell (move, interact, divide, die...) \">Cell</a> Simulator for 3-D Multicellular\
    \ Systems, bioRxiv 088773, 2016. DOI: 10.1101/088773. </li>\n<li>\n<a href=\"\
    http://biofvm.mathcancer.org\" rel=\"nofollow\">BioFVM website </a> </li>\n<li>\n\
    <a href=\"namespace_bio_f_v_m.html\">BioFVM</a> publication: A. Ghaffarizadeh,\
    \ S.H. Friedman, and P. Macklin. <a href=\"namespace_bio_f_v_m.html\">BioFVM</a>:\
    \ an efficient, parallelized diffusive transport solver for 3-D biological simulations.\
    \ Bioinformatics, 2015. </li>\n <br>\n </ul>\n<p>For MaBoSS:</p><ul>\n <li>\n\
    <a href=\"http://maboss.curie.fr\" rel=\"nofollow\">MaBoSS website </a> </li>\n\
    <li>MaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time\
    \ Boolean modeling for biological signaling: application of Gillespie algorithm.\
    \ BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186/1752-0509-6-116. </li>\n <br>\n\
    \ </ul>\n<h2>\n<a id=\"user-content-remarks\" class=\"anchor\" href=\"#remarks\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Remarks</h2>\n<p>Please, refer to the <a href=\"https://github.com/gletort/PhysiBoSS/wiki\"\
    >Wiki</a> of this repository for a much more extended documentation, with step\
    \ by step examples instructions.</p>\n<p>PhysiCell is developed in <a href=\"\
    http://mathcancer.org\" rel=\"nofollow\">Paul Macklin's lab</a>.\nMaBoSS and PhysiBoSS\
    \ are developed in the <a href=\"http://sysbio.curie.fr\" rel=\"nofollow\">Computational\
    \ Systems Biology of Cancer group</a> at Institut Curie (Paris, France).</p>\n\
    <p>We invite you to use PhysiBoSS for you research and give feedbacks to us. Any\
    \ help in developing it further is more than welcome.\nDo not hesitate to contact\
    \ us for any comments or difficulties in using PhysiBoSS: <a href=\"mailto:physiboss@gmail.com\"\
    >physiboss@gmail.com</a>.</p>\n<p>Wishing you to enjoy using PhysiBoSS,</p>\n\
    <p>PhysiBoSS's team.</p>\n"
  stargazers_count: 18
  subscribers_count: 5
  topics:
  - cell
  - biology
  - boolean-logic
  - agent-based-modeling
  - agent-based-simulation
  - multiscale-simulation
  updated_at: 1616791911.0
gparadis/spades-singularity:
  data_format: 2
  description: Build Singularity containers to run SpaDES simulations on HPC clusters.
  filenames:
  - Singularity.spades_base
  - Singularity.spades_github-development
  - Singularity.spades_github-master
  full_name: gparadis/spades-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-spades-singularity" class="anchor" href="#spades-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>spades-singularity</h1>

    <h2>

    <a id="user-content-about-this-project" class="anchor" href="#about-this-project"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>About
    this project</h2>

    <p>This project implements a scripted framework for automating the process of
    building Singularity containers for running SpaDES simulations on HPC clusters.</p>

    <h2>

    <a id="user-content-i-am-super-impatient-and-refuse-to-take-the-time-to-understand-what-i-am-doing-before-running-any-commands-just-tell-me-how-to-do-the-thing-right-now"
    class="anchor" href="#i-am-super-impatient-and-refuse-to-take-the-time-to-understand-what-i-am-doing-before-running-any-commands-just-tell-me-how-to-do-the-thing-right-now"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>I
    am super impatient, and refuse to take the time to understand what I am doing
    before running any commands. Just tell me how to do the thing right now!</h2>

    <p>To build, sign, and push the base container flavour to the cloud image repository,
    simply run <code>make all flavour=FLAVOUR</code>, where <code>FLAVOUR</code> is
    one of <code>base</code>, <code>github-master</code>, or <code>github-development</code>.</p>

    <p>Not sure which flavour to use? Read on!</p>

    <p>Note that, if you do not have Singularity installed yet, you will need to run
    <code>make install-singularity</code> first.</p>

    <h2>

    <a id="user-content-singularity-container-definition-files" class="anchor" href="#singularity-container-definition-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    container definition files</h2>

    <p>This Singularity container definition files follow standard Singularity definition
    file naming conventions (i.e., they are prefixed with <code>Singularity.</code>
    followed by a <em>tag</em> string). There are three flavours (tags) defined in
    this project: <code>base</code>, <code>github-master</code>, and <code>github-development</code>.
    Note that the R code that installs SpaDES packages for each flavour is contained
    in a script named <code>spades-setup_flavour.R</code></p>

    <p>You can also create new custom flavours by copying and modifying some files
    from an existing flavour. New flavours should be compatible with automated make
    targets (as long as you did not break the filename patterns).</p>

    <h3>

    <a id="user-content-base-flavour" class="anchor" href="#base-flavour" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Base flavour</h3>

    <p>The base container flavour includes the latest stable CRAN versions of core
    SpaDES R packages. This base can be used to run SpaDES models directly (for simpler
    projects, where the CRAN packages are all you need). The base image also serves
    as a <em>bootstrap</em> image for other flavours. The base container flavour is
    implemented in <code>Singularity.spades_base</code> and <code>spades-setup_base.R</code>.</p>

    <h3>

    <a id="user-content-github-flavours" class="anchor" href="#github-flavours" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GitHub flavours</h3>

    <p>There are two GitHub container flavours (<code>github-master</code>, <code>github-development</code>).
    These install core SpaDES R packages from the latest code pushed to GitHub repositories
    for <code>master</code> and <code>development</code> branches, respectively. The
    GitHub container flavours are implemented in the <code>Singularity.spades-github_BRANCH</code>
    and <code>spades-setup_github-BRANCH</code> (where <code>BRANCH</code> is one
    of <code>master</code> or <code>development</code>).</p>

    <p>The GitHub container flavours are <em>bootstrapped</em> from the base container
    flavour. Defintion file implementation assumes that a local base container image
    is available in path <code>build/spades.sif</code>, so the base container must
    be built first (the base container will automatically get built if not present
    if you run <code>make build flavour=FLAVOUR</code>, where <code>FLAVOUR</code>
    is any value except for <code>base</code>).</p>

    <h3>

    <a id="user-content-custom-flavours" class="anchor" href="#custom-flavours" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Custom flavours</h3>

    <p>You can create a custom container flavour but copying <code>Singularity.spades_github-master</code>
    and <code>spades-setup_github-master.R</code>---rename these to <code>Singularity.spades_foo</code>
    and <code>spades-setup_foo.R</code> (where <code>foo</code> is whatever unique
    flavour name you want) and modify as required. Minimally, you just need to edit
    one line of code in the Singularity definition file to point to <code>spades-setup_foo.R</code>,
    and edit the code in <code>spades-setup_foo.R</code> to install whatever versions
    of SpaDES R packages you need.</p>

    <h2>

    <a id="user-content-makefile-details" class="anchor" href="#makefile-details"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Makefile
    details</h2>

    <p>The <code>Makefile</code> implements a number of make targets.</p>

    <p>Run <code>make build flavour=FLAVOUR sandbox=true</code> to build a sandbox
    container (in <code>build/spades_FLAVOUR_sandbox</code>). See Singularity documentation
    for details on sandbox containers.</p>

    <p>Run <code>make build flavour=FLAVOUR</code> to build a container as a single
    <em>singularity image file</em> (in <code>build/spades_FLAVOUR.sif</code>). See
    Singularity documentation for details on SIF containers.</p>

    <p>Run <code>make push flavour=FLAVOUR</code> to sign your SIF image and push
    it to your Sylabs cloud image library account. See the <a href="https:%5Ccloud.sylabs.io">Sylabs
    Container Library</a> to create and configure your account.</p>

    <p>Run <code>make all flavour=FLAVOUR</code> to build and push your image in one
    step.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602060129.0
graneklab/rna_enrichment:
  data_format: 2
  description: null
  filenames:
  - compute_environment/singularity/Singularity
  full_name: graneklab/rna_enrichment
  latest_release: null
  readme: "<p>This repository contains the code for <a href=\"https://doi.org/10.1101/2021.03.01.433483\"\
    \ rel=\"nofollow\">Comparative analysis of RNA enrichment methods for preparation\
    \ of Cryptococcus neoformans RNA sequencing libraries</a>. Instructions for reproducing\
    \ the analysis described in the manuscript are below.</p>\n<h1>\n<a id=\"user-content-full-analysis-except-lncrna-discovery\"\
    \ class=\"anchor\" href=\"#full-analysis-except-lncrna-discovery\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Full\
    \ Analysis, except lncRNA discovery</h1>\n<p>To run the full analysis in the manuscript\
    \ (except lncRNA discovery):</p>\n<ol>\n<li>Clone this repository</li>\n<li>Run\
    \ the script <code>run_full_analysis.sh</code>, which is found in top level of\
    \ this repository</li>\n</ol>\n<p>The <code>run_full_analysis.sh</code> script\
    \ will download the singularity image from Sylabs Cloud, download the raw sequence\
    \ data from SRA, and run all the components of the analysis.</p>\n<h2>\n<a id=\"\
    user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\
    <ol>\n<li>\n<a href=\"https://sylabs.io/guides/3.7/user-guide/\" rel=\"nofollow\"\
    >Singularity</a>. It is known to work with Singularity version 3.6.4, but probably\
    \ works fine with version 3.0 and above</li>\n<li>Bash</li>\n<li>Create the directories\
    \ specified by <code>DATA_BASE_DIR</code>, <code>WORKSPACE_BASE_DIR</code> and\
    \ <code>SPACE_DIR</code> (see below)</li>\n</ol>\n<h2>\n<a id=\"user-content-recommended-configuration\"\
    \ class=\"anchor\" href=\"#recommended-configuration\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Recommended Configuration</h2>\n\
    <p>It is recommended that you make the configuration adjustments</p>\n<ol>\n<li>Check\
    \ the values in the <code>common_config.R</code> for these variables and adjust\
    \ appropriately for the machine you will be running on:\n<ul>\n<li>\n<code>total_threads</code>\
    \ should be less than the total number of cores you have available</li>\n<li>\n\
    <code>max_jobs</code> can be 1 or higher, ideally should be a factor of total_threads</li>\n\
    <li>\n<code>max_memory_mb</code> should be less than the total memory you have\
    \ available</li>\n</ul>\n</li>\n<li>Set values for the following environment variables\
    \ in your .bashrc, when running <code>run_full_analysis.sh</code>, or by editing\
    \ <code>setup.sh</code>:\n<ul>\n<li>\n<code>DATA_BASE_DIR</code>: where raw data\
    \ will be downloaded</li>\n<li>\n<code>WORKSPACE_BASE_DIR</code>: where intermediate\
    \ results and final results will be output</li>\n<li>\n<code>SPACE_DIR</code>\
    \ : directory used by lncRNA</li>\n<li>\n<code>SINGULARITY_CACHEDIR</code> described\
    \ in the <a href=\"https://sylabs.io/guides/2.6/user-guide/build_environment.html#cache\"\
    \ rel=\"nofollow\">Singularity User Manual</a>\n</li>\n<li>\n<code>SINGULARITY_PULLFOLDER</code>\
    \ described in the <a href=\"https://sylabs.io/guides/2.6/user-guide/build_environment.html#cache\"\
    \ rel=\"nofollow\">Singularity User Manual</a>\n</li>\n</ul>\n</li>\n</ol>\n<h2>\n\
    <a id=\"user-content-running-on-a-cluster\" class=\"anchor\" href=\"#running-on-a-cluster\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Running on a cluster</h2>\n<p>This script can be run on a SLURM cluster\
    \ with the following command</p>\n<pre><code>srun ./run_full_analysis.sh\n</code></pre>\n\
    <p>However a slightly more complex command might be desireable, which specifies\
    \ the partition, account, number of CPUs and amount of memory to allocate, such\
    \ as:</p>\n<pre><code>srun -A chsi -p chsi --cpus-per-task=60 --mem=300G ./run_full_analysis.sh\n\
    </code></pre>\n \n \n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621656631.0
h3abionet/chipimputation_evaluate_chips:
  data_format: 2
  description: null
  filenames:
  - singularity/Singularity.minimac4
  full_name: h3abionet/chipimputation_evaluate_chips
  latest_release: null
  readme: '<h1>

    <a id="user-content-chip-imputation-evaluation-workflow-h3abionetchipimputation_evaluate_chips"
    class="anchor" href="#chip-imputation-evaluation-workflow-h3abionetchipimputation_evaluate_chips"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Chip
    imputation evaluation Workflow h3abionet/chipimputation_evaluate_chips</h1>

    <p><a href="https://travis-ci.org/h3abionet/chipimputation" rel="nofollow"><img
    src="https://camo.githubusercontent.com/9bdc33815fd93626a153c75edc2679d736b504273526d14b1f30776900d72b8b/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f63686970696d7075746174696f6e2e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/h3abionet/chipimputation.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/17df893666bfa5cad487466d4476ab773ea5def560c04c50f45795017865e81c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33302e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.30.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/h3abionet/chipimputation" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a4a1755f55345897c83ea6a657c8396ebf4f9a4141d5b591d92241002094bc19/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f63686970696d7075746174696f6e2e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/chipimputation.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>The pipeline is to evaluate the imputation performance and accuracy of different
    arrays starting from sequence data.

    It masks non tag variants for each array, and then impute to a reference panel
    using Minimac.<br>

    It is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner.<br>

    It comes with singularity containers making installation trivial and results highly
    reproducible.</p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The evaluate_chips pipeline comes with documentation about the pipeline, found
    in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation and Configuration</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Configuration for other clusters</a></li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    </ol>

    <h3>

    <a id="user-content-setup-native-cluster" class="anchor" href="#setup-native-cluster"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup
    (native cluster)</h3>

    <h4>

    <a id="user-content-headnode" class="anchor" href="#headnode" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Headnode</h4>

    <ul>

    <li>

    <a href="https://www.nextflow.io/" rel="nofollow">Nextflow</a> (can be installed
    as local user)</li>

    <li>NXF_HOME needs to be set, and must be in the PATH</li>

    <li>Note that we''ve experienced problems running Nextflow when NXF_HOME is on
    an NFS mount.</li>

    <li>The Nextflow script also needs to be invoked in a non-NFS folder</li>

    <li>Java 1.8+</li>

    </ul>

    <h4>

    <a id="user-content-compute-nodes" class="anchor" href="#compute-nodes" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compute nodes</h4>

    <ul>

    <li>

    <p>The compute nodes need to have singularity installed.</p>

    </li>

    <li>

    <p>The compute nodes need access to shared storage for input, references, output</p>

    </li>

    <li>

    <p>The following commands need to be available in PATH on the compute nodes, in
    case of unavailabitity of singularity.</p>

    <ul>

    <li>

    <code>minimac4</code> from <a href="http://mathgen.stats.ox.ac.uk/impute/impute_v2.html"
    rel="nofollow">MINIMAC4</a>

    </li>

    <li>

    <code>vcftools</code> from <a href="https://vcftools.github.io/index.html" rel="nofollow">VCFtools</a>

    </li>

    <li>

    <code>bcftools</code>from <a href="https://samtools.github.io/bcftools/bcftools.html"
    rel="nofollow">bcftools</a>

    </li>

    <li>

    <code>bgzip</code> from <a href="http://www.htslib.org" rel="nofollow">htslib</a>

    </li>

    <li>

    <code>eagle</code> from <a href="https://data.broadinstitute.org/alkesgroup/Eagle/"
    rel="nofollow">Eagle</a>

    </li>

    <li><code>python2.7</code></li>

    <li>

    <code>R</code> with the following packages ...</li>

    </ul>

    </li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1621014196.0
h3abionet/h3arefgraph:
  data_format: 2
  description: H3A RefGraph Hackathon 2019
  filenames:
  - Singularity
  full_name: h3abionet/h3arefgraph
  latest_release: null
  readme: '<h1>

    <a id="user-content-h3abioneth3arefgraph" class="anchor" href="#h3abioneth3arefgraph"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>h3abionet/h3arefgraph</h1>

    <p><strong>RefGraph Workflows Hackathon</strong></p>

    <p><a href="https://travis-ci.org/h3abionet/h3arefgraph" rel="nofollow"><img src="https://camo.githubusercontent.com/1ac1f732e5b8f802a198a533b88e24608b4b10e38d8744373f7bcb5284832ca8/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f68336172656667726170682e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/h3abionet/h3arefgraph.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nfcore/h3arefgraph" rel="nofollow"><img src="https://camo.githubusercontent.com/910d36cd9eef9bfef42722b54a531cf2c72b7ed04f37e85d72b93d50b7a3e0c1/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f68336172656667726170682e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nfcore/h3arefgraph.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667"
    alt="Singularity Container available" data-canonical-src="https://img.shields.io/badge/singularity-available-7E4C74.svg"
    style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

    <p>This pipeline is for the use and testing of graph based methods for variant
    calling.</p>

    <p>The aim is to allow the user to choose the reference graph construction method
    and the alignment / variant calling methods separately.</p>

    <p>We also provide tools for reporting the results of the variant calling, that
    take advantage of the additional contextual information that using reference graphs
    provides.</p>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker / singularity containers making installation
    trivial and results highly reproducible.</p>

    <h3>

    <a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h3>

    <p>The aim of this project is to separate the different parts of the variant calling
    process to allow the development of

    task specific tools. This is more in line with traditional variant calling where
    specific alignment tools may preform

    better for different organisms, but should not require a different downstream
    analysis for each output.</p>

    <p><a href="assets/images/Overview_slide.jpeg" target="_blank" rel="noopener noreferrer"><img
    src="assets/images/Overview_slide.jpeg" alt="Overview slide" style="max-width:100%;"></a></p>

    <h3>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h3>

    <p>The h3abionet/h3arefgraph pipeline comes with documentation about the pipeline,
    found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="docs/installation.md">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="docs/configuration/local.md">Local installation</a></li>

    <li><a href="docs/configuration/adding_your_own.md">Adding your own system</a></li>

    <li><a href="docs/configuration/reference_genomes.md">Reference genomes</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>


    <h3>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h3>

    <p>h3abionet/h3arefgraph was originally written by the H3ABioNet RefGraph Team.</p>

    '
  stargazers_count: 3
  subscribers_count: 13
  topics: []
  updated_at: 1569710209.0
h3abionet/h3avarcall:
  data_format: 2
  description: H3A variant calling pipeline
  filenames:
  - containers/Singularity.gatk
  - containers/Singularity.trimmomatic
  - containers/Singularity.multiqc
  - containers/Singularity.fastqc
  - containers/Singularity.bwa
  full_name: h3abionet/h3avarcall
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-h3avarcall---h3abionet-variant-calling-pipeline\"\
    \ class=\"anchor\" href=\"#h3avarcall---h3abionet-variant-calling-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><code>h3avarcall</code>\
    \ - H3ABioNet Variant Calling Pipeline</h1>\n<p><code>h3avarcall</code> is a <a\
    \ href=\"https://www.nextflow.io/\" rel=\"nofollow\"><code>Nextflow</code></a>\
    \ pipeline developed by <a href=\"https://www.h3abionet.org/\" rel=\"nofollow\"\
    ><code>H3ABioNet</code></a> for genomic Variant Calling allowing to detect SNPs\
    \ and Indels giving raw sequence reads (fastq files) as input. <code>h3avarcall</code>\
    \ includes the different steps from aligning raw sequence reads to variant calling\
    \ and filtering using GATK. <br>\nFor more details about the different steps of\
    \ the pipeline, check the [H3ABionet SOPs pages] <a href=\"https://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling\"\
    \ rel=\"nofollow\">https://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling</a>\n\
    <code>h3avarcall</code> is a modular and extensible tool allowing users to run\
    \ the whole pipeline, use only parts of it and also to easily enrich it and adapt\
    \ it to their needs. <code>h3avarcall</code> generates a number of intermediate\
    \ files where results from various steps of the pipeline are stored.</p>\n<h2>\n\
    <a id=\"user-content-1-obtaining-pipeline-and-preparing-data\" class=\"anchor\"\
    \ href=\"#1-obtaining-pipeline-and-preparing-data\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1. Obtaining\
    \ pipeline and preparing Data</h2>\n<p>First, you need to clone the <code>h3avarcall</code>\
    \ repository onto you machine:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>git clone https://github.com/h3abionet/h3avarcall.git\n<span class=\"pl-c1\"\
    >cd</span> h3avarcall</pre></div>\n<p>Content of the repository:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>h3avarcall\n  <span class=\"pl-k\">|</span>--containers\
    \                       <span class=\"pl-c\"><span class=\"pl-c\">#</span># Folder\
    \ for Singularity images and recipes (in case you want to build yourself). All\
    \ downloaded images go here!</span>\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--Singularity.bwa               <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span># Singularity recipe file for BWA and Samtools.</span>\n  <span\
    \ class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--Singularity.fastqc \
    \           <span class=\"pl-c\"><span class=\"pl-c\">#</span># Singularity recipe\
    \ file for FastQC.</span>\n  <span class=\"pl-k\">|</span>  <span class=\"pl-k\"\
    >|</span>--Singularity.gatk              <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span># Singularity recipe file for GATK and tabix.</span>\n  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--Singularity.trimmomatic      \
    \ <span class=\"pl-c\"><span class=\"pl-c\">#</span># Singularity recipe file\
    \ for Trimmimatic.</span>\n  <span class=\"pl-k\">|</span>--gatk-b37-bundle  \
    \                <span class=\"pl-c\"><span class=\"pl-c\">#</span># Folder for\
    \ stoding downloaded GATK-b37-bundle files.</span>\n  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>--b37_files_minimal.txt         <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span># LList of GATK-b37-bundle files to be downloaded\
    \ (bundle TOO BIG! Only selected files needed for the pipeline). </span>\n  <span\
    \ class=\"pl-k\">|</span>--templates                        <span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span># Folder for extra scripts for the pipeline.</span>\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--download_bundles.sh\
    \           <span class=\"pl-c\"><span class=\"pl-c\">#</span># Script for downloading\
    \ GATK-b37-bundle.</span>\n  <span class=\"pl-k\">|</span>--LICENSE          \
    \                <span class=\"pl-c\"><span class=\"pl-c\">#</span># Duh!</span>\n\
    \  <span class=\"pl-k\">|</span>--README.md                        <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span># Duh!</span>\n  <span class=\"pl-k\">|</span>--main.config\
    \                      <span class=\"pl-c\"><span class=\"pl-c\">#</span># User\
    \ configuration file! All inputs, outputs and options GO HERE!! ONLY file that\
    \ SHOULD be modified by user!</span>\n  <span class=\"pl-k\">|</span>--main.nf\
    \                          <span class=\"pl-c\"><span class=\"pl-c\">#</span>#\
    \ Main h3avarcall nextflow scripts.</span>\n  <span class=\"pl-k\">|</span>--nextflow.config\
    \                  <span class=\"pl-c\"><span class=\"pl-c\">#</span># Pipeline\
    \ configuration file! DO NOT EDIT!!!</span></pre></div>\n<p>The <code>main.config</code>\
    \ file:</p>\n<div class=\"highlight highlight-source-groovy\"><pre><span class=\"\
    pl-c\"><span class=\"pl-c\">/*</span>==================================================================================================</span>\n\
    <span class=\"pl-c\"> * THIS FILE IS USED TO SPECIFY INPUT, OUTPUTS AND PARAMETERS.\
    \ THE FOLLOWING OPTIONS ARE THE ALLOWED:</span>\n<span class=\"pl-c\"> * ==================================================================================================</span>\n\
    <span class=\"pl-c\"> * data         : Path to where the data is (FASTQ files).</span>\n\
    <span class=\"pl-c\"> * out          : Path to store output results.</span>\n\
    <span class=\"pl-c\"> * bundle       : GATK-b37-bundle list file.</span>\n<span\
    \ class=\"pl-c\"> * mode         : Worflow step to perform. Can be any of [ do.GetContainers\
    \ | do.GenomeIndexing | do.QC | do.ReadTrimming | do.ReadAlignment | do.VarianCalling\
    \ | do.VariantFiltering | do.MultiQC].</span>\n<span class=\"pl-c\"> * trim  \
    \       : Trimming options for Trimmomatic.</span>\n<span class=\"pl-c\"> * resources\
    \    : Location of the GATK-b37-bundle folder.</span>\n<span class=\"pl-c\"> *\
    \ from         : pipeline step to resume pipeline from. Can be any of [ do.QC\
    \ | do.ReadTrimming | do.ReadAlignment | do.VarianCalling | do.VariantFiltering\
    \ ].</span>\n<span class=\"pl-c\"> * params.help  : Print help menu.</span>\n\
    <span class=\"pl-c\"> * ==================================================================================================</span>\n\
    <span class=\"pl-c\"> * BELOW ARE THE DEFAULT PARAMETERS! YOU'RE MORE THAN WELCOME\
    \ TO CHANGE AS DESIRED!</span>\n<span class=\"pl-c\"> * ==================================================================================================</span>\n\
    <span class=\"pl-c\"> <span class=\"pl-c\">*/</span></span>\n\nparams {\n    data\
    \         <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">$b<span class=\"pl-smi\">aseDir</span></span>/data<span\
    \ class=\"pl-pds\">\"</span></span>\n    out          <span class=\"pl-k\">=</span>\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >$b<span class=\"pl-smi\">aseDir</span></span>/results<span class=\"pl-pds\">\"\
    </span></span>\n    bundle       <span class=\"pl-k\">=</span> <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">$b<span class=\"\
    pl-smi\">aseDir</span></span>/gatk-b37-bundle/b37_files_minimal.txt<span class=\"\
    pl-pds\">\"</span></span>\n    mode         <span class=\"pl-k\">=</span> <span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>do.QC<span class=\"pl-pds\">\"\
    </span></span>\n    trim         <span class=\"pl-k\">=</span> <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:8:true\
    \ TRAILING:28 MINLEN:40<span class=\"pl-pds\">\"</span></span>\n    resources\
    \    <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span><span class=\"pl-smi\">$b<span class=\"pl-smi\">aseDir</span></span>/gatk-b37-bundle<span\
    \ class=\"pl-pds\">\"</span></span>\n    from         <span class=\"pl-k\">=</span>\
    \ <span class=\"pl-c1\">null</span>\n    params<span class=\"pl-k\">.</span>help\
    \  <span class=\"pl-k\">=</span> <span class=\"pl-c1\">null</span>\n}\n</pre></div>\n\
    <h3>\n<a id=\"user-content-11-download-test-datasets\" class=\"anchor\" href=\"\
    #11-download-test-datasets\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>1.1. Download test datasets:</h3>\n<p>Create\
    \ a data directory under the <code>h3avarcall</code> repository:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>mkdir data\n<span class=\"pl-c1\">cd</span>\
    \ data</pre></div>\n<p>Download the test data from <a href=\"http://thesite.com\"\
    \ rel=\"nofollow\">THIS_SITE</a> using one of the commands bellow:</p>\n<h4>\n\
    <a id=\"user-content-112-using-lftp-faster\" class=\"anchor\" href=\"#112-using-lftp-faster\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1.1.2. Using LFTP (faster)</h4>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>lftp -e <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pget -n 20\
    \ ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R1_001.fastq.gz;\
    \ bye<span class=\"pl-pds\">\"</span></span>\nlftp -e <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>pget -n 20 ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R2_001.fastq.gz;\
    \ bye<span class=\"pl-pds\">\"</span></span></pre></div>\n<h4>\n<a id=\"user-content-113-using-wget-slower\"\
    \ class=\"anchor\" href=\"#113-using-wget-slower\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.1.3. Using\
    \ WGET (slower)</h4>\n<div class=\"highlight highlight-source-shell\"><pre>wget\
    \ ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R1_001.fastq.gz\n\
    wget ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R2_001.fastq.gz</pre></div>\n\
    <h3>\n<a id=\"user-content-12-download-the-singularity-containers-required-to-execute-the-pipeline\"\
    \ class=\"anchor\" href=\"#12-download-the-singularity-containers-required-to-execute-the-pipeline\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1.2. Download the <code>Singularity</code> containers (required to\
    \ execute the pipeline):</h3>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>nextflow run main.nf -profile slurm --mode do.GetContainers</pre></div>\n\
    <h3>\n<a id=\"user-content-13-download-the-gatk-b37-bundle-required-to-execute-the-pipeline\"\
    \ class=\"anchor\" href=\"#13-download-the-gatk-b37-bundle-required-to-execute-the-pipeline\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1.3. Download the GATK b37 bundle (required to execute the pipeline):</h3>\n\
    <p>This step takes <strong>FOREVER</strong> to run - run it only once!</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>nextflow run main.nf -profile\
    \ slurm --mode do.GenomeIndexing</pre></div>\n<p>If by some miracle you happen\
    \ to have access to the WITS Cluster, you do not need to download the GATK-b37-bundle!\
    \ Simply <code>cd</code> into the <code>gatk-b37-bundle</code> folder of the <code>h3avarcall</code>\
    \ repo and soft-link the GATK-b37-bundle data as follows:</p>\n<pre><code>cd gatk-b37-bundle\n\
    ln -s /global/blast/gatk-bundle/b37/* .\n</code></pre>\n<h2>\n<a id=\"user-content-2-executing-the-main-h3avarcall-pipeline\"\
    \ class=\"anchor\" href=\"#2-executing-the-main-h3avarcall-pipeline\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2.\
    \ Executing the main <code>h3avarcall</code> pipeline</h2>\n<h3>\n<a id=\"user-content-21-read-qc-optional-\"\
    \ class=\"anchor\" href=\"#21-read-qc-optional-\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2.1. Read QC (optional): <br>\n\
    </h3>\n<p>Before getting started with the downstream analysis, it's always good\
    \ to do some quality checks on your raw sequences to assess the quality of raw\
    \ sequence data, the fastq files. FastQC tool has been used in this workflow.\
    \ An html report page will be automatically created for each fastq file. You can\
    \ load up these html pages in your browser to assess your data through graphs\
    \ and summary tables.<br>\nTo perform the QC of your fastq files, you can use\
    \ this command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>nextflow\
    \ run main.nf -profile slurm --mode do.QC</pre></div>\n<h3>\n<a id=\"user-content-22-read-trimming-optional\"\
    \ class=\"anchor\" href=\"#22-read-trimming-optional\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2.2. Read Trimming\
    \ (optional):<br>\n</h3>\n<p>After performing the QC of your fastq files, you\
    \ have an idea about the quality of your reads: some of your reads might not be\
    \ of a very good quality or the quality might drop at some positions (near the\
    \ begining or end of reads) across all reads and this requires to clean up your\
    \ library to minimize biaises in your analysis by filtering poor quality reads\
    \ and/or trim poor quality bases from our samples. Trimmomatic is the trimming\
    \ tool that has been used here. <br>\nFor more information about reads preprocessing,\
    \ check this <a href=\"https://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling#phase-1-preprocessing-of-the-raw-reads\"\
    \ rel=\"nofollow\">page</a>. <br>\nTo run the trimming step of the <code>h3avarcall</code>\
    \ pipeline, you can use this command:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>nextflow run main.nf -profile slurm --mode do.ReadTrimming</pre></div>\n\
    <h3>\n<a id=\"user-content-23-read-alignment-\" class=\"anchor\" href=\"#23-read-alignment-\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2.3. Read Alignment <br>\n</h3>\n<p>Once you have good raw sequences\
    \ quality, the next step is to map your reads to a reference genome to determine\
    \ where in the genome the reads originated from. The mapper used in this workflow\
    \ is BWA.  For more information about the read alignement step, check this <a\
    \ href=\"https://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling#phase-2-initial-variant-discovery\"\
    \ rel=\"nofollow\">page</a></p>\n<p>Can be run with <code>--from do.ReadTrimming</code>\
    \ or <code>--from do.QC</code> depending on whether these steps were run!</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>nextflow run main.nf -profile\
    \ slurm --mode do.ReadAlignment</pre></div>\n<h3>\n<a id=\"user-content-24-variant-calling\"\
    \ class=\"anchor\" href=\"#24-variant-calling\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2.4. Variant Calling</h3>\n<p>This\
    \ step uses the outputs generated by the Read Alignment STEP! <strong>MUST</strong>\
    \ run STEP 2.3 (<code>--mode do.ReadAlignment</code>) before running this step.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>nextflow run main.nf -profile\
    \ slurm --mode do.VariantCalling </pre></div>\n<h3>\n<a id=\"user-content-25-variant-filtering\"\
    \ class=\"anchor\" href=\"#25-variant-filtering\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>2.5. Variant Filtering</h3>\n\
    <p>This step uses the outputs generated by the Variant Calling STEP! <strong>MUST</strong>\
    \ run STEP 2.4 (<code>--mode do.VariantCalling</code>) before running this step.</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>nextflow run main.nf -profile\
    \ slurm --mode do.VariantFiltering </pre></div>\n<h3>\n<a id=\"user-content-26-workflow-qc-multiqc---optional\"\
    \ class=\"anchor\" href=\"#26-workflow-qc-multiqc---optional\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2.6.\
    \ Workflow QC (MultiQC - Optional)</h3>\n<p>This step performs a Quality Check\
    \ of the different pipeline steps that have been ran. You need to run at least\
    \ ONE step of the pipeline to be able to run this MultiQC step!</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>nextflow run main.nf -profile slurm --mode\
    \ do.MultiQC </pre></div>\n<h2>\n<a id=\"user-content-3-explore-h3avarcall-results\"\
    \ class=\"anchor\" href=\"#3-explore-h3avarcall-results\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3. Explore\
    \ <code>h3avarcall</code> results</h2>\n<p>Assuming you did not change the  default\
    \ output folder (in the <code>main.config</code> file), the resulting files will\
    \ be found in the <code>results</code> folder of the <code>h3avarcall</code> repository.\
    \ Resulting files for each of the main pipeline steps (<code>2.1</code> - <code>2.5</code>)\
    \ are grouped in different folders as follows:</p>\n<pre><code>- [1] Read QC (optional)\
    \         =&gt;    `results/1_QC`\n- [2] Read Trimming (optional)   =&gt;    `results/2_Read_Trimming`\n\
    - [3] Read Alignment             =&gt;    `results/3_Read_Alignment`\n- [4] Variant\
    \ Calling            =&gt;    `results/4_Variant_Calling`\n- [5] Variant Filtering\
    \          =&gt;    `results/5_Variant_Filtering`\n- [6] MultiQC             \
    \       =&gt;    `results/MultiQC`\n</code></pre>\n<p>In each of these folders,\
    \ a sub-folder \"<code>workflow_report</code>\"  is created. It  contains 4 different\
    \ files (<code>h3avarcall_report.html</code>, <code>h3avarcall_timeline.html</code>,\
    \ <code>h3avarcall_workflow.dot</code> and <code>h3avarcall_trace.txt</code>)\
    \ containing detailed information on the resources (CPU, MEMORY and TIME) usage\
    \ of each process in the different pipeline steps. <br>\nThe <code>results</code>\
    \ directory structure within <code>h3avarcall</code> repository can be summarized\
    \ as below:</p>\n<div class=\"highlight highlight-source-shell\"><pre>h3avarcall\n\
    \  <span class=\"pl-k\">|</span>--results\n  <span class=\"pl-k\">|</span>  <span\
    \ class=\"pl-k\">|</span>--1_QC\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--workflow_report\n  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>--h3avarcall_report.html\n  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span\
    \ class=\"pl-k\">|</span>--h3avarcall_timeline.html\n  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--h3avarcall_workflow.dot\n  <span class=\"pl-k\">|</span>  <span\
    \ class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\"\
    >|</span>--h3avarcall_trace.txt\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--<span class=\"pl-k\">&lt;</span>sample_<span\
    \ class=\"pl-k\">1&gt;</span>_R1.fastqc.html .. <span class=\"pl-k\">&lt;</span>sample_N<span\
    \ class=\"pl-k\">&gt;</span>_R1.fastqc.html\n  <span class=\"pl-k\">|</span> \
    \ <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--<span class=\"\
    pl-k\">&lt;</span>sample_<span class=\"pl-k\">1&gt;</span>_R2.fastqc.html .. <span\
    \ class=\"pl-k\">&lt;</span>sample_N<span class=\"pl-k\">&gt;</span>_R1.fastqc.html\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--2_Read_Trimming\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--workflow_report\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--h3avarcall_report.html\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--h3avarcall_timeline.html\n  <span\
    \ class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>--h3avarcall_workflow.dot\n  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>--h3avarcall_trace.txt\n  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--<span\
    \ class=\"pl-k\">&lt;</span>sample_<span class=\"pl-k\">1&gt;</span>.1P.fastq.gz\
    \ .. <span class=\"pl-k\">&lt;</span>sample_N<span class=\"pl-k\">&gt;</span>.1P.fastq.gz\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--<span class=\"pl-k\">&lt;</span>sample_<span class=\"pl-k\">1&gt;</span>.2P.fastq.gz\
    \ .. <span class=\"pl-k\">&lt;</span>sample_N<span class=\"pl-k\">&gt;</span>.2P.fastq.gz\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--3_Read_Alignment\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--workflow_report\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--h3avarcall_report.html\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--h3avarcall_timeline.html\n  <span\
    \ class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>--h3avarcall_workflow.dot\n  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>--h3avarcall_trace.txt\n  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--<span\
    \ class=\"pl-k\">&lt;</span>sample_<span class=\"pl-k\">1&gt;</span>_md.recal.bam\
    \ .. <span class=\"pl-k\">&lt;</span>sample_N<span class=\"pl-k\">&gt;</span>_md.recal.bam\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--<span class=\"pl-k\">&lt;</span>sample_<span class=\"pl-k\">1&gt;</span>_md.recal.bai\
    \ .. <span class=\"pl-k\">&lt;</span>sample_N<span class=\"pl-k\">&gt;</span>_md.recal.bai\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--4_Variant_Calling\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--workflow_report\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--h3avarcall_report.html\n\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--h3avarcall_timeline.html\n  <span\
    \ class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>--h3avarcall_workflow.dot\n  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>--h3avarcall_trace.txt\n  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--chr_1_genotyped.vcf.gz\
    \ .. chr_22_genotyped.vcf.gz\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--chr_1_genotyped.vcf.gz.tbi ..\
    \ chr_22_genotyped.vcf.gz.tbi\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--5_Variant_Filtering\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--workflow_report\n  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>--h3avarcall_report.html\n  <span class=\"pl-k\"\
    >|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span\
    \ class=\"pl-k\">|</span>--h3avarcall_timeline.html\n  <span class=\"pl-k\">|</span>\
    \  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>--h3avarcall_workflow.dot\n  <span class=\"pl-k\">|</span>  <span\
    \ class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\"\
    >|</span>--h3avarcall_trace.txt\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--genome.SNP-recal.vcf.gz\n  <span\
    \ class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\"\
    >|</span>--genome.SNP-recal.vcf.gz.tbi\n  <span class=\"pl-k\">|</span>  <span\
    \ class=\"pl-k\">|</span>--MultiQC\n  <span class=\"pl-k\">|</span>  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>--multiqc_data\n  <span class=\"\
    pl-k\">|</span>  <span class=\"pl-k\">|</span>  <span class=\"pl-k\">|</span>--multiqc_report.html\n\
    \  <span class=\"pl-k\">|</span>--work\n  <span class=\"pl-k\">|</span>  <span\
    \ class=\"pl-k\">|</span>--<span class=\"pl-k\">&lt;</span>There<span class=\"\
    pl-s\"><span class=\"pl-pds\">'</span>s a lot of folders here! Lets not worry\
    \ about them for today!&gt;</span></pre></div>\n<p>##We're working on further\
    \ improving the pipleine and the associated documentation, feel free to share\
    \ comments and suggestions!</p>\n"
  stargazers_count: 4
  subscribers_count: 8
  topics: []
  updated_at: 1618936209.0
here0009/SarsCov2_Snakemake_Pipeline:
  data_format: 2
  description: null
  filenames:
  - envs/illumina/Singularity
  full_name: here0009/SarsCov2_Snakemake_Pipeline
  latest_release: null
  readme: '<h1>

    <a id="user-content-sarscov2_snakemake_pipeline" class="anchor" href="#sarscov2_snakemake_pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SarsCov2_Snakemake_Pipeline</h1>

    <p>This is a snakemake pipeline used for analyse SarsCov2 sequence data generated
    by illumina machine.

    This pipelien was based on <a href="https://github.com/artic-network/fieldbioinformatics">ARTIC
    network''s fieldbioinformatics tools</a>, <a href="https://github.com/dridk/Sars-CoV-2-NGS-pipeline">Sars-CoV-2-NGS-pipeline</a>
    and <a href="https://github.com/connor-lab/ncov2019-artic-nf">ncov2019-artic-nf</a>
    with some updates:</p>

    <ol>

    <li>

    <code>fastqc</code> and  was used to generate the qc report of input data.</li>

    <li>

    <code>quast</code> was used to generate the sequence assembly report.</li>

    <li>

    <a href="https://github.com/cov-lineages/pangolin">pangolin</a> was used for the
    typing of SarsCov-2</li>

    <li>

    <code>CorGat</code> was used to annotate the sequence, and generate alle frequency
    reports

    You need to clone <a href="https://github.com/matteo14c/CorGAT">CorGat</a> and
    specify the directory in the config files.</li>

    <li>

    <code>multiqc</code> was used to generate the final report.</li>

    </ol>

    <p>The workflow shows like below:</p>

    <p>A test_data file was provided to test the pipeline.

    You may test the pipeline by dry-run

    <code>snakemake -s sars2.smk -n</code>

    then run the pipeline:

    <code>snakemake -s sars2.smk -j 4 --use-conda</code></p>

    <p>WARNING - THIS REPO IS UNDER ACTIVE DEVELOPMENT AND ITS BEHAVIOUR MAY CHANGE
    AT <strong>ANY</strong> TIME.</p>

    <p>PLEASE ENSURE THAT YOU READ BOTH THE README AND THE CONFIG FILE AND UNDERSTAND
    THE EFFECT OF THE OPTIONS ON YOUR DATA!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621527734.0
hexmek/container:
  data_format: 2
  description: null
  filenames:
  - Singularity.dRep
  - Singularity.eukcc_vanilla
  - Singularity.cmseq_conda
  - Singularity.metabat2
  - Singularity.nQuire
  - Singularity.megahit
  - Singularity.R
  - Singularity.kraken2
  - Singularity.sourmash
  - Singularity.seqtk
  - Singularity.metawap_docker
  - Singularity.puntseq
  - Singularity.mummer
  - Singularity.euk_decide
  - Singularity.CAT_update
  - Singularity.BUSCO4
  - Singularity.cmseq
  - Singularity.art
  - Singularity.snakemake
  - Singularity.EukRep
  - Singularity.qiime2
  - Singularity.biopython
  - Singularity.repeatmasker
  - Singularity.deeptools
  - Singularity.antismash_standalone
  - Singularity.mashmap
  - Singularity.nanofilt
  - Singularity.METAMVGL
  - Singularity.VAMB_10.1
  - Singularity.bamm
  - Singularity.BUSCO5
  - Singularity.bioconvert
  - Singularity.mafft
  - Singularity.metawrap
  - Singularity.raxml-ng
  - Singularity.CAT
  - Singularity.krona
  - Singularity.minimap2
  - Singularity.pysam
  - Singularity.ete3
  - Singularity.dbcan
  - Singularity.spades_3.15
  - Singularity.sepp
  - Singularity.spades
  - Singularity.ncbi-downloader
  - Singularity.metaeuk
  - Singularity.BUSCO414
  - Singularity.dRep3
  - Singularity.pasta
  - Singularity.VAMB
  - Singularity.mash
  - Singularity.ploidyNGS
  - Singularity.bbmap
  - Singularity.VAMP
  - Singularity.bioinfo
  - Singularity.tree
  - Singularity.spades_3.13
  - Singularity.mmseq2
  - Singularity.bwa
  - Singularity.iqtree
  - Singularity.famsa
  - Singularity.fastani
  - Singularity.trimal
  - Singularity.comparem
  full_name: hexmek/container
  latest_release: null
  readme: '<p>Container</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1619722202.0
hmgu-itg/peakplotter:
  data_format: 2
  description: A tool to find and annotate signals in next-generation association
    studies
  filenames:
  - Singularity
  full_name: hmgu-itg/peakplotter
  latest_release: v.2.0
  readme: "<h1>\n<a id=\"user-content-peakplotter--automatically-annotate-hits-from-genome-wide-association-results\"\
    \ class=\"anchor\" href=\"#peakplotter--automatically-annotate-hits-from-genome-wide-association-results\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>PeakPlotter : automatically annotate hits from genome-wide association\
    \ results</h1>\n<p>PeakPlotter takes away the annoying task of running regional\
    \ association plots and annotating variants for your association studies results.\
    \ It is compatible with sequencing as well as GWAS data. It is compatible with\
    \ any format (GEMMA, SNPTEST, Bolt-LMM...) that produces the relevant columns:\
    \ chromosome, position, unique ID, P-value, reference and non-reference alleles.</p>\n\
    <h2>\n<a id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Install</h2>\n\
    <p>After installing the prerequisites (see below), clone the repository and install\
    \ using <code>pip</code>.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>git clone https://github.com/hmgu-itg/peakplotter.git\n\n<span class=\"\
    pl-c1\">cd</span> peakplotter\n\npython3 -m pip install <span class=\"pl-c1\"\
    >.</span>\n\npeakplotter-data-setup <span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> This only needs to be run once</span>\n\npeakplotter --help\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> or </span>\npython3 -m peakplotter --help</pre></div>\n\
    <p>A <code>Singularity</code> definition file is also available in the repository\
    \ if you wish to build a container to use <code>peakplotter</code>.</p>\n<h2>\n\
    <a id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Prerequisites</h2>\n<p>PeakPlotter has has non-python dependencies.<br>\n\
    In order to run PeakPlotter you need to install the following tools and add the\
    \ executables to your <code>PATH</code>:</p>\n<ul>\n<li>Plink 1.9 or newer (<a\
    \ href=\"https://www.cog-genomics.org/plink2/index\" rel=\"nofollow\">available\
    \ here</a>)</li>\n<li>LocusZoom Standalone 1.4 or newer (<a href=\"http://genome.sph.umich.edu/wiki/LocusZoom_Standalone\"\
    \ rel=\"nofollow\">available here</a>)</li>\n<li>BedTools (<a href=\"http://bedtools.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">available here</a>)</li>\n<li>Tabix (<a href=\"https://github.com/samtools/htslib\"\
    >available here</a>)</li>\n<li>Moreutils (for <code>sponge</code>)</li>\n</ul>\n\
    <p>PeakPlotter will throw a <code>MissingExecutableError</code> if you have any\
    \ of the above tools missing in your <code>PATH</code> environment variable.<br>\n\
    Add the necessary tools to your <code>PATH</code> like below:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> PATH=/path/to/locuszoom:/path/to/plink:<span\
    \ class=\"pl-smi\">$PATH</span></pre></div>\n<p>If you want to make these changes\
    \ permanent, do:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span\
    \ class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>export\
    \ PATH=/path/to/locuszoom:/path/to/plink:$PATH<span class=\"pl-pds\">'</span></span>\
    \ <span class=\"pl-k\">&gt;&gt;</span> <span class=\"pl-k\">~</span>/.bashrc</pre></div>\n\
    <h2>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h2>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ peakplotter --help\nUsage:\
    \ peakplotter [OPTIONS]\n\n  PeakPlotter\n\nOptions:\n  -a, --assoc-file FILE\
    \    Path to the association file. It can be gzipped,\n                      \
    \     provided that it bears the .gz extension. Its first\n                  \
    \         line must be a header, coherent with the name\n                    \
    \       arguments below. It must be tab-separated, bgzipped\n                \
    \           and tabixed (tabix is available as part of\n                     \
    \      bcftools)  [required]\n  -f, --bfiles TEXT        Binary PLINK (.bed/.bim/.fam)\
    \ file base name. This\n                           should contain the genotypes\
    \ <span class=\"pl-k\">for</span> at least all the\n                         \
    \  variants <span class=\"pl-k\">in</span> the assoc_file, but it can contain\n\
    \                           more. Please note that this is the base name,\n  \
    \                         without the .bed/.bim/.fam extension.  [required]\n\
    \  -o, --out DIRECTORY      Output directory to store all output files.\n    \
    \                       [required]\n  -chr, --chr-col TEXT     Name of the column\
    \ <span class=\"pl-k\">for</span> chromosome names.\n                        \
    \   [required]\n  -ps, --pos-col TEXT      Name of the column <span class=\"pl-k\"\
    >for</span> chromosomal position.\n                           [required]\n  -rs,\
    \ --rs-col TEXT       Name of the column <span class=\"pl-k\">for</span> unique\
    \ SNP ids (RS-id or\n                           chr:pos).  [required]\n  -p, --pval-col\
    \ TEXT      Name of the column <span class=\"pl-k\">for</span> p-values.  [required]\n\
    \  -a1, --a1-col TEXT       Name of the column <span class=\"pl-k\">for</span>\
    \ reference or major allele\n                           (used <span class=\"pl-k\"\
    >for</span> predicting consequence).  [required]\n  -a2, --a2-col TEXT       Name\
    \ of the column <span class=\"pl-k\">for</span> alternate or minor allele.\n \
    \                          [required]\n  -maf, --maf-col TEXT     Name of the\
    \ column <span class=\"pl-k\">for</span> non-reference or minor\n            \
    \               allele frequency.  [required]\n  -b, --build INTEGER      Assembly\
    \ build (37 or 38)  [default: 38]\n  -s, --signif FLOAT       The significance\
    \ level above which to <span class=\"pl-k\">declare</span> a\n               \
    \            variant significant. Scientific notation (such as\n             \
    \              5e-8) is fine.\n  -bp, --flank-bp INTEGER  Flanking size <span\
    \ class=\"pl-k\">in</span> base pairs <span class=\"pl-k\">for</span> drawing\
    \ plots\n                           (defaults to 500kb, i.e. 1Mbp plots) around\
    \ lead\n                           SNPs.\n  --overwrite              Overwrite\
    \ output directory <span class=\"pl-k\">if</span> it already exists.\n  --help\
    \                   Show this message and exit.</pre></div>\n<h2>\n<a id=\"user-content-testing\"\
    \ class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Testing</h2>\n<p>Run <code>pytest</code>\
    \ at the root of the repository to run the testsuite.</p>\n<p>There aren't a lot\
    \ of tests right now, and this is a work in progress. If you encounter any bugs,\
    \ please raise an issue at the <a href=\"https://github.com/hmgu-itg/peakplotter/issues\"\
    >issue page</a>.</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1621639878.0
hqhv/oneapi:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity.hpc
  full_name: hqhv/oneapi
  latest_release: null
  readme: '<h1>

    <a id="user-content-metaprokka" class="anchor" href="#metaprokka" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>metaprokka</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611091491.0
huynhngoc/cnn-template:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: huynhngoc/cnn-template
  latest_release: null
  readme: "<p>After forking this repository, replace <code>orion_username</code> in\
    \ the file <code>2d_unet_CT_W_PET.json</code> with your actual orion username.</p>\n\
    <h1>\n<a id=\"user-content-setup-in-orion-cluster\" class=\"anchor\" href=\"#setup-in-orion-cluster\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Setup in Orion cluster</h1>\n<p>The <code>$HOME</code> directory of\
    \ your login machine (<code>[username@login ~]</code>) should have the following\
    \ structure</p>\n<pre><code>$HOME\n   \u251C\u2500\u2500 cnn_template (the forked\
    \ repository)\n   \u2502   \u251C\u2500\u2500 config\n   \u2502   |   \u251C\u2500\
    \u2500 2d_unet.json\n   \u2502   |   \u2514\u2500\u2500 (other configurations)\n\
    \   |   \u251C\u2500\u2500 outputs\n   \u2502   |   \u2514\u2500\u2500 README.md\n\
    \   \u2502   \u251C\u2500\u2500 customize_obj.py\n   \u2502   \u251C\u2500\u2500\
    \ experiment.py\n   \u2502   \u251C\u2500\u2500 slurm.sh\n   \u2502   \u251C\u2500\
    \u2500 setup.sh\n   \u2502   \u2514\u2500\u2500 (other files)\n   \u251C\u2500\
    \u2500 datasets (Put your datasets in here)\n   \u2502   \u2514\u2500\u2500 headneck\n\
    \   \u2502       \u251C\u2500\u2500 full_dataset_singleclass.h5\n   \u2502   \
    \    \u2514\u2500\u2500 (other datasets)\n   \u251C\u2500\u2500 hnperf (log files\
    \ will be saved in here)\n   \u2502\n</code></pre>\n<p>Start by running <code>setup.sh</code>\
    \ to download the singularity container</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> cnn-template\n./setup.sh</pre></div>\n<p>Alternative\
    \ you can directly download the image file</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> cnn-template\nsingularity pull --name deoxys.sif\
    \ shub://huynhngoc/head-neck-analysis</pre></div>\n<h1>\n<a id=\"user-content-run-experiments-on-orion\"\
    \ class=\"anchor\" href=\"#run-experiments-on-orion\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run experiments\
    \ on Orion</h1>\n<h2>\n<a id=\"user-content-submit-jobs\" class=\"anchor\" href=\"\
    #submit-jobs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Submit jobs</h2>\n<p>Submit slurm jobs like this:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>sbatch slurm.sh config/2d_unet.json\
    \ 2d_unet 200</pre></div>\n<p>Which will load the setup from the <code>config/2d_unet.json</code>\
    \ file, train for 200 epochs\nand store the results in the folder <code>$HOME/hnperf/2d_unet/</code>.</p>\n\
    <p>To customize model and prediction checkpoints, add the <code>model_checkpoint_period</code>\
    \ and <code>prediction_checkpoint_period</code> as arguments</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm.sh config/2d_unet_CT_W_PET.json\
    \ 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period\
    \ 5\n</pre></div>\n<p>Which will save the trained model every 5 epochs and predict\
    \ the validation set every 5 epoch</p>\n<h2>\n<a id=\"user-content-continue-experiments-and-run-test\"\
    \ class=\"anchor\" href=\"#continue-experiments-and-run-test\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Continue\
    \ experiments and run test</h2>\n<p>To continue an experiment</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_cont.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period\
    \ 5</pre></div>\n<p>Which will load the saved model and continue training 100\
    \ more epochs</p>\n<p>In the case the job ended unexpectedly before plotting the\
    \ performance:</p>\n<div class=\"highlight highlight-source-shell\"><pre>sbatch\
    \ slurm_vis.sh 2d_unet_CT_W_PET</pre></div>\n<p>To run test</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_test.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET</pre></div>\n<p>To run external validation</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_external.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET maastro.json</pre></div>\n<h1>\n<a id=\"user-content-misc\"\
    \ class=\"anchor\" href=\"#misc\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Misc</h1>\n<p>Manually build the singularity\
    \ image file</p>\n<pre><code>singularity build --fakeroot Singularity deoxys.sif\n\
    </code></pre>\n<p>Login to a gpu session to use the gpu</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>qlogin --partition=gpu --gres=gpu:1\nsingularity\
    \ <span class=\"pl-c1\">exec</span> --nv deoxys.sif ipython</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618886215.0
huynhngoc/head-neck-analysis:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity.beta
  full_name: huynhngoc/head-neck-analysis
  latest_release: null
  readme: '<h1>

    <a id="user-content-head-and-neck-cancer-analysis" class="anchor" href="#head-and-neck-cancer-analysis"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Head
    and Neck cancer analysis</h1>

    <p>Start by running <code>setup.sh</code> to download the singularity container

    Then, submit slurm jobs like this:</p>

    <div class="highlight highlight-source-shell"><pre>sbatch slurm.sh config/2d_unet.json
    2d_unet 200</pre></div>

    <p>Which will load the setup from the <code>config/2d_unet.json</code> file, train
    for 200 epochs

    and store the results in the folder <code>$HOME/logs/hn_perf/2d_unet/</code>.</p>

    <p>To customize model and prediction checkpoints</p>

    <pre><code>sbatch slurm.sh config/3d_vnet_32_normalize.json 3d_vnet_32_normalize
    100 --model_checkpoint_period 5 --prediction_checkpoint_period 5


    </code></pre>

    <p>To continue an experiment</p>

    <pre><code>sbatch slurm_cont.sh config/3d_vnet_32_normalize/model/model.030.h5
    3d_vnet_32_normalize 100 --model_checkpoint_period 5 --prediction_checkpoint_period
    5

    </code></pre>

    <p>To plot performance</p>

    <pre><code>sbatch slurm_vis.sh 3d_vnet_32_normalize

    </code></pre>

    <p>To run test</p>

    <pre><code>sbatch slurm_test.sh 3d_vnet_32/model/model.030.h5 3d_vnet_32

    </code></pre>

    <p>Alternatively, if your cluster does not have slurm installed, simply omit the
    <code>sbatch</code>

    part of the call above, thus running</p>

    <div class="highlight highlight-source-shell"><pre>./slurm.sh config/2d_unet.json
    2d_unet 200</pre></div>

    <p>Manually build</p>

    <pre><code>singularity build --fakeroot Singularity deoxys.sif

    </code></pre>

    <p>Remember to login to a gpu session to use the gpu</p>

    <pre><code>qlogin --partition=gpu --gres=gpu:1

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620348200.0
icaoberg/singularity-asciinema:
  data_format: 2
  description: Singularity recipe for asciinema
  filenames:
  - 2.0.2/Singularity
  full_name: icaoberg/singularity-asciinema
  latest_release: v2.0.2-r3
  readme: "<h1>\n<a id=\"user-content-singularity-asciinema\" class=\"anchor\" href=\"\
    #singularity-asciinema\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-asciinema</h1>\n<p><a href=\"https://www.travis-ci.com/icaoberg/singularity-asciinema\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7af8220113fb0c457eda3ba87d518d1a80a3bfc8eab5f60a14fac181d20e05c3/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d61736369696e656d612e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-asciinema.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for asciinema.</p>\n\
    <h2>\n<a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\"\
    \ href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-or-similar\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges (or similar)</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>asciinema</code> script</li>\n</ul>\n<p>to <code>/opt/packages/asciinema/2.0.2</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/asciinema</code>\
    \ as <code>2.0.2.lua</code>.</p>\n<hr>\n<p>Copyright \xA9 2021 Pittsburgh Supercomputing\
    \ Center. All Rights Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\"\
    \ rel=\"nofollow\">icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\"\
    >Pittsburgh Supercomputing Center</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - asciinema
  - singularity-recipe
  updated_at: 1619661412.0
icaoberg/singularity-boxes:
  data_format: 2
  description: Singularity recipe for boxes.
  filenames:
  - 1.3/Singularity
  full_name: icaoberg/singularity-boxes
  latest_release: '1.3'
  readme: "<h1>\n<a id=\"user-content-singularity-boxes\" class=\"anchor\" href=\"\
    #singularity-boxes\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-boxes</h1>\n<p>Singularity recipe\
    \ for <a href=\"https://boxes.thomasjensen.com/\" rel=\"nofollow\">boxes</a>.</p>\n\
    <h2>\n<a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\"\
    \ href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n<p><a\
    \ href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a> at\
    \ the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\n\
    Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  - boxes
  updated_at: 1619480922.0
icaoberg/singularity-chalk-cli:
  data_format: 2
  description: Singularity recipe for chalk-cli.
  filenames:
  - 4.1.0/Singularity
  full_name: icaoberg/singularity-chalk-cli
  latest_release: v4.1.0
  readme: "<h1>\n<a id=\"user-content-singularity-chalk-cli\" class=\"anchor\" href=\"\
    #singularity-chalk-cli\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-chalk-cli</h1>\n<p><a href=\"https://www.travis-ci.com/icaoberg/singularity-chalk-cli\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/edef36a43b2f6e93623b669084ecd8c07f14abb4e6a940915e33cbf76670c50c/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c692e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-chalk-cli.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for <a href=\"https://github.com/chalk/chalk-cli\"\
    >chalk-cli</a>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-or-similar\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges (or similar)</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>chalk-cli</code> script</li>\n</ul>\n<p>to <code>/opt/packages/chalk-cli/4.1.0</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/chalk-cli</code>\
    \ as <code>4.1.0.lua</code>.</p>\n<h3>\n<a id=\"user-content-example\" class=\"\
    anchor\" href=\"#example\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Example</h3>\n<pre><code>singularity exec singularity-chalk-cli-4.1.0.sif\
    \ chalk -t '{red.bold Dungeons and Dragons {~bold.blue (with added fairies)}}'\n\
    </code></pre>\n<p><a href=\"images/screenshot.png\" target=\"_blank\" rel=\"noopener\
    \ noreferrer\"><img src=\"images/screenshot.png\" alt=\"Screenshot\" style=\"\
    max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-alternative-installation\"\
    \ class=\"anchor\" href=\"#alternative-installation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Alternative Installation</h2>\n\
    <pre><code>spack install npm\nspack load npm\nnpm install -g chalk-cli\n</code></pre>\n\
    <hr>\n<p>Copyright \xA9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a>\
    \ at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\
    \ Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - cli-utilities
  - utilities
  updated_at: 1619482220.0
icaoberg/singularity-fastani:
  data_format: 2
  description: Singularity recipe for fastANI
  filenames:
  - 1.33/Singularity
  full_name: icaoberg/singularity-fastani
  latest_release: v1.3.3
  readme: "<h1>\n<a id=\"user-content-singularity-fastani\" class=\"anchor\" href=\"\
    #singularity-fastani\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-fastani</h1>\n<p>Singularity recipe\
    \ for fastANI.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n<p><a\
    \ href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a> at\
    \ the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\
    \ Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  updated_at: 1621557973.0
icaoberg/singularity-gimp:
  data_format: 2
  description: Singularity recipe for Gimp.
  filenames:
  - 2.10.8/Singularity
  full_name: icaoberg/singularity-gimp
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-gimp\" class=\"anchor\" href=\"\
    #singularity-gimp\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>singularity-gimp</h1>\n<p>Singularity recipe for <a\
    \ href=\"https://www.gimp.org\" rel=\"nofollow\">Gimp</a>.</p>\n<h2>\n<a id=\"\
    user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Building the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a>\
    \ at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\n\
    Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - gimp
  updated_at: 1619080700.0
icaoberg/singularity-pandoc:
  data_format: 2
  description: Singularity recipe for pandoc.
  filenames:
  - 2.2.1/Singularity
  full_name: icaoberg/singularity-pandoc
  latest_release: 2.2.1
  readme: "<h1>\n<a id=\"user-content-singularity-pandoc\" class=\"anchor\" href=\"\
    #singularity-pandoc\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-pandoc</h1>\n<p>Singularity recipe\
    \ for <a href=\"https://pandoc.org/\" rel=\"nofollow\">pandoc</a>.</p>\n<h2>\n\
    <a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"\
    #building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n<p><a\
    \ href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a> at\
    \ the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\n\
    Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - utilities
  - pandoc
  updated_at: 1619481049.0
icaoberg/singularity-phylip-suite:
  data_format: 2
  description: null
  filenames:
  - 3.697/Singularity
  full_name: icaoberg/singularity-phylip-suite
  latest_release: null
  readme: '<h1>

    <a id="user-content-nf-upcoast-v" class="anchor" href="#nf-upcoast-v" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>nf-upcoast-v</h1>

    <p>Nextflow based WGS workflow for Vp (Will be updated later)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619486665.0
iferres/pagoo_publication_scripts:
  data_format: 2
  description: Scripts supporting pagoo's publication
  filenames:
  - Singularity
  full_name: iferres/pagoo_publication_scripts
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/5123" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-pagoo---publication-scripts" class="anchor" href="#pagoo---publication-scripts"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pagoo
    - Publication Scripts</h1>

    <p>Scripts supporting pagoo''s publication.</p>

    <h2>

    <a id="user-content-description" class="anchor" href="#description" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Description</h2>

    <p>This repo contains 2 scripts to reproduce the analyses described in the pagoo
    manuscript. <code>timing_benchmark.R</code> runs roary and evaluates timings over
    a set of pagoo''s operations. <code>Cfetus_pangenome_example.R</code> downloads
    a Campylobacter fetus dataset and uses pagoo along with other R packages to perform
    a series of analyses.</p>

    <h2>

    <a id="user-content-data" class="anchor" href="#data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

    <p>Data required by <code>timing_benchmark.R</code> is hosted at: <a href="https://zenodo.org/record/3341535#.YA7_8ZqvHJE"
    rel="nofollow">https://zenodo.org/record/3341535#.YA7_8ZqvHJE</a> (Publication:
    <a href="https://www.nature.com/articles/s41598-019-54004-5" rel="nofollow">Decano
    &amp; Downing, 2019</a>, dataset doi:10.5281/zenodo.3341534). The script automatically
    downloads and decompress it in the working directory.</p>

    <p>Data required by <code>Cfetus_pangenome_example.R</code> is hosted at: <a href="https://figshare.com/articles/dataset/Campylobacter_fetus_genomes_and_pangenome_for_pagoo_demo/13622354"
    rel="nofollow">https://figshare.com/articles/dataset/Campylobacter_fetus_genomes_and_pangenome_for_pagoo_demo/13622354</a>
    . The script automatically downloads and decompress it in the working directory.</p>

    <h2>

    <a id="user-content-singularity-container" class="anchor" href="#singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    container</h2>

    <p>This repo also contains a Singularity file to build a Singularity image with
    all dependencies needed to run the scripts.</p>

    <h3>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h3>

    <p>To manually build the container:</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/iferres/pagoo_publication_scripts

    <span class="pl-c1">cd</span> ./pagoo_publication_scripts

    sudo singularity build pagoo_publicaction_container.sif Singularity</pre></div>

    <h3>

    <a id="user-content-pull" class="anchor" href="#pull" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pull</h3>

    <p>To pull the prebuilt container hosted at singularity-hub:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull pagoo_publication_container.sif
    shub://iferres/pagoo_publication_scripts</pre></div>

    <h3>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h3>

    <p>To run the C. fetus script, all at once:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    The following expects the script (.R) and the container (.sif) </span>

    <span class="pl-c"><span class="pl-c">#</span> in the current working directory.</span>

    singularity <span class="pl-c1">exec</span> pagoo_publication_container.sif Rscript
    --vanilla Cfetus_pangenome_example.R</pre></div>

    <p>If you want to play more freely with the package, you can shell into the container,
    start an interactive R session, and load pagoo:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Start container</span>

    singularity shell pagoo_publication_container.sif

    <span class="pl-c"><span class="pl-c">#</span> Start R</span>

    R</pre></div>

    <div class="highlight highlight-source-r"><pre><span class="pl-c"><span class="pl-c">#</span>
    Inside R, load pagoo</span>

    library(<span class="pl-smi">pagoo</span>)</pre></div>

    <h4>

    <a id="user-content-known-limitations" class="anchor" href="#known-limitations"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Known
    limitations</h4>

    <p>Shiny app doesn''t work from within the container, although it is not needed
    by the scripts.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620083275.0
ifremer-bioinformatics/orson:
  data_format: 2
  description: Workflow for prOteome and tRanScriptome functiOnal aNnotation. (Mirror
    from Ifremer's Gitlab).
  filenames:
  - containers/Singularity.interproscan-5.51-85.0
  full_name: ifremer-bioinformatics/orson
  latest_release: v1.0.0
  readme: '<h1>

    <a id="user-content-orson-workflow-for-proteome-and-transcriptome-functional-annotation"
    class="anchor" href="#orson-workflow-for-proteome-and-transcriptome-functional-annotation"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>ORSON:
    workflow for prOteome and tRanScriptome functiOnal aNnotation</strong>.</h1>

    <p><a href="https://github.com/ifremer-bioinformatics/orson"><img src="https://camo.githubusercontent.com/68d7de18944473ac497abf10c6474d22ea7670f99506880609ed17327999e012/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f52534f4e25323076657273696f6e2d312e302e302d7265643f6c6162656c436f6c6f723d303030303030"
    alt="ORSON version" data-canonical-src="https://img.shields.io/badge/ORSON%20version-1.0.0-red?labelColor=000000"
    style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/a1e0f23194e457dfe0e30e723e75e3d231e038aa553636a5de68b76c877496ff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413532302e31302e302d3233616136322e7376673f6c6162656c436f6c6f723d303030303030"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A520.10.0-23aa62.svg?labelColor=000000"
    style="max-width:100%;"></a>

    <a href="https://sylabs.io/docs/" rel="nofollow"><img src="https://camo.githubusercontent.com/0b0568e8f684f1ea04320511f0635c70c144cad7fb7daec19a8e605f02933b01/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72756e253230776974682d73696e67756c61726974792d3164333535632e7376673f6c6162656c436f6c6f723d303030303030"
    alt="Run with with singularity" data-canonical-src="https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000"
    style="max-width:100%;"></a>

    <a href="https://ifremer-bioinformatics.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/31a937a688093f0712fada8741813c83b48db9380ba74f25f296c9dc503c9730/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446576656c6f706572732d536542694d45522d79656c6c6f773f6c6162656c436f6c6f723d303030303030"
    alt="Developers" data-canonical-src="https://img.shields.io/badge/Developers-SeBiMER-yellow?labelColor=000000"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

    <p>The ORSON pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with singularity containers making installation trivial
    and results highly reproducible.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>i. Install <a href="https://www.nextflow.io/docs/latest/getstarted.html#installation"
    rel="nofollow"><code>nextflow</code></a></p>

    <p>ii. Install <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow"><code>Singularity</code></a>
    for full pipeline reproducibility</p>

    <p>iii. Download the pipeline and test it on a minimal dataset with a single command</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run main.nf -profile
    test,singularity</pre></div>

    <blockquote>

    <p>To use this workflow on a computing cluster, it is necessary to provide a configuration
    file for your system. For some institutes, this one already exists and is referenced
    on <a href="https://github.com/nf-core/configs#documentation">nf-core/configs</a>.
    If so, you can simply download your institute custom config file and simply use
    <code>-c &lt;institute_config_file&gt;</code> in your command. This will enable
    either <code>docker</code> or <code>singularity</code> and set the appropriate
    execution settings for your local compute environment.</p>

    </blockquote>

    <p>iv. Start running your own analysis!</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run main.nf -profile
    custom,singularity [-c <span class="pl-k">&lt;</span>institute_config_file<span
    class="pl-k">&gt;</span>]</pre></div>

    <p>See <a href="docs/usage.md">usage docs</a> for a complete description of all
    of the options available when running the pipeline.</p>

    <h2>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>This workflow comes with documentation about the pipeline, found in the <code>docs/</code>
    directory:</p>

    <ol>

    <li><a href="docs/usage.md#introduction">Introduction</a></li>

    <li>

    <a href="docs/usage.md#install-the-pipeline">Pipeline installation</a>

    <ul>

    <li><a href="docs/usage.md#local-installation">Local installation</a></li>

    <li><a href="docs/usage.md#your-own-config">Adding your own system config</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md#running-the-pipeline">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="docs/troubleshooting.md">Troubleshooting</a></li>

    </ol>

    <p>Here is an overview of the many steps available in orson pipeline:</p>

    <p><a href="docs/images/ORSON_workflow.png" target="_blank" rel="noopener noreferrer"><img
    src="docs/images/ORSON_workflow.png" alt="ORSON" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Requirements</h2>

    <p>To use ORSON, all tools are automatically installed via pre-built singularity
    images available at SeBiMER ftp; these images are built from recipes available
    <a href="/containers">here</a>.</p>

    <p>Databases are also automatically download according to user''s choice (default:
    Enzyme, UniProt SwissProt). See the <a href="/docs/usage.md#installing-annotated-sequence-banks">Installing
    annotated sequence banks</a> section of the usage documentation.</p>

    <p>However, you must have local access to the BUSCO lineage databases. To download
    them, please refer to the tool''s documentation to <a href="https://busco.ezlab.org/busco_userguide.html#download-and-automated-update"
    rel="nofollow">download the lineage databases</a></p>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>ORSON is written by <a href="https://ifremer-bioinformatics.github.io/" rel="nofollow">SeBiMER</a>,
    the Bioinformatics Core Facility of <a href="https://wwz.ifremer.fr/en/" rel="nofollow">IFREMER</a>.</p>

    <h2>

    <a id="user-content-contributions" class="anchor" href="#contributions" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributions</h2>

    <p>We welcome contributions to the pipeline. If such case you can do one of the
    following:</p>

    <ul>

    <li>Use issues to submit your questions</li>

    <li>Fork the project, do your developments and submit a pull request</li>

    <li>Contact us (see email below)</li>

    </ul>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>For further information or help, don''t hesitate to get in touch with the orson
    developpers:</p>

    <p><a href="assets/sebimer-email.png" target="_blank" rel="noopener noreferrer"><img
    src="assets/sebimer-email.png" alt="sebimer email" style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics:
  - workflow
  - annotation
  - transcriptome
  - proteome
  - nextflow-pipelines
  updated_at: 1621527121.0
ifurther/flair-def:
  data_format: 2
  description: singularity def file for flair(fluka)
  filenames:
  - flair.def
  - flair-cern.def
  full_name: ifurther/flair-def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619708213.0
ifurther/geant4-docker:
  data_format: 2
  description: geant4 in contianer.
  filenames:
  - Singularity
  full_name: ifurther/geant4-docker
  latest_release: 10.7.1
  readme: '<h1>

    <a id="user-content-a-singularity-container-for-installing-manta" class="anchor"
    href="#a-singularity-container-for-installing-manta" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>A singularity container
    for installing manta</h1>

    <h2>

    <a id="user-content-build-with" class="anchor" href="#build-with" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build with:</h2>

    <p>sudo singularity build manta.simg Singularity</p>

    <ul>

    <li>Build files are found in ./build</li>

    <li>Install files are found in ./install</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1615754654.0
ikmb/ensembl-api:
  data_format: 2
  description: Docker container recipe for EnsEMBL API
  filenames:
  - Singularity
  - Singularity.100
  - Singularity.99
  full_name: ikmb/ensembl-api
  latest_release: null
  readme: '<p><a href="images/ikmb_bfx_logo.png" target="_blank" rel="noopener noreferrer"><img
    src="images/ikmb_bfx_logo.png" alt="" style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-ensembl-perl-api" class="anchor" href="#ensembl-perl-api"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>EnsEMBL
    Perl API</h1>

    <p>Container recipe for EnsEMBL API</p>

    <p>The corresponding Singularity Hub URL</p>

    <p>shub://ikmb/ensembl-api:97

    shub://ikmb/ensembl-api:99</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1592419007.0
ikmb/teaching-med-assembly:
  data_format: 2
  description: Repo for the singularitx container used in the assembly and annotation
    exercise of the msc course on bioinformatics
  filenames:
  - Singularity
  full_name: ikmb/teaching-med-assembly
  latest_release: null
  readme: '<h1>

    <a id="user-content-teaching-med-assembly" class="anchor" href="#teaching-med-assembly"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>teaching-med-assembly</h1>

    <p>Repo for the singularity container used in the assembly and annotation exercise
    of the msc course on bioinformatics</p>

    <p>This needs to be re-build/updated each year since Prokka stops working after
    some time.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1606743227.0
inextvir/Docker:
  data_format: 2
  description: Docker image of virus related tools
  filenames:
  - Viromescan/Singularity
  full_name: inextvir/Docker
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_ncbi-entrez" class="anchor" href="#singularity_ncbi-entrez"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_ncbi-entrez</h1>

    '
  stargazers_count: 0
  subscribers_count: 4
  topics: []
  updated_at: 1609869497.0
intel/HPC-containers-from-Intel:
  data_format: 2
  description: Intel HPC Containers using Singularity
  filenames:
  - definitionFiles/namd/namdBuild.def
  - definitionFiles/namd/namdRun.def
  - definitionFiles/lammps/lammpsBuild.def
  - definitionFiles/lammps/lammpsRun.def
  - definitionFiles/WRF/wrfRun.def
  - definitionFiles/WRF/wrfBuild.def
  - definitionFiles/base/base.def
  - definitionFiles/gromacs/gromacsRun.def
  - definitionFiles/gromacs/gromacsBuild.def
  full_name: intel/HPC-containers-from-Intel
  latest_release: null
  readme: '<h1>

    <a id="user-content-goal" class="anchor" href="#goal" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Goal:</h1>

    <p>Create containers using Singularity definition file for HPC apps and run them
    on the cloud or bare metal for Single and Cluster runs.</p>

    <p>This repo should have definition files only for few HPC applications. Users
    can utilize them to generate containers.</p>

    <h2>

    <a id="user-content-get-help" class="anchor" href="#get-help" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Get Help</h2>

    <ul>

    <li>

    <a href="https://github.com/intel/HPC-containers-from-Intel/issues">Post an issue</a>
    if you face any problem building or running a container</li>

    </ul>

    '
  stargazers_count: 16
  subscribers_count: 9
  topics:
  - hpc
  - cluster
  - singularity-containers
  - cloud
  updated_at: 1619733161.0
iqbal-lab-org/viridian:
  data_format: 2
  description: Virus assembler from amplicon sequencing reads
  filenames:
  - Singularity.def
  full_name: iqbal-lab-org/viridian
  latest_release: v0.1.0
  readme: "<p><a href=\"https://www.travis-ci.com/iqbal-lab-org/viridian\" rel=\"\
    nofollow\"><img src=\"https://camo.githubusercontent.com/6ff381f585a29ae518070fb3871e067b0ed88ed137f3a6e7dbd32f72b1f73e00/68747470733a2f2f7777772e7472617669732d63692e636f6d2f697162616c2d6c61622d6f72672f766972696469616e2e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/iqbal-lab-org/viridian.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-viridian\" class=\"\
    anchor\" href=\"#viridian\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>viridian</h1>\n<p>Virus assembler from amplicon\
    \ sequencing reads</p>\n<h2>\n<a id=\"user-content-install\" class=\"anchor\"\
    \ href=\"#install\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Install</h2>\n<h3>\n<a id=\"user-content-from-source\"\
    \ class=\"anchor\" href=\"#from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>From source</h3>\n<p>These must\
    \ be installed and in your <code>$PATH</code>:</p>\n<ul>\n<li>\n<code>racon</code>\
    \ (<a href=\"https://github.com/lbcb-sci/racon\">https://github.com/lbcb-sci/racon</a>)</li>\n\
    <li>\n<code>minimap2</code> (<a href=\"https://github.com/lh3/minimap2/\">https://github.com/lh3/minimap2/</a>).</li>\n\
    </ul>\n<p>Clone this repository and run:</p>\n<pre><code>python3 -m pip install\
    \ .\n</code></pre>\n<h3>\n<a id=\"user-content-singularity-container\" class=\"\
    anchor\" href=\"#singularity-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity container</h3>\n\
    <p>Clone this repository and run:</p>\n<pre><code>sudo singularity build viridian.img\
    \ Singularity.def\n</code></pre>\n<p>to build the container <code>viridian.img</code>.</p>\n\
    <h2>\n<a id=\"user-content-example-usage\" class=\"anchor\" href=\"#example-usage\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Example usage</h2>\n<p>Required input:</p>\n<ol>\n<li>Reference FASTA\
    \ file</li>\n<li>BED file of amplicons. Column 1 = name of amplicon, columns 2\
    \ and 3 are start\nend end positions of the amplicons</li>\n<li>Reads, either\
    \ in a sorted mapped indexed BAM file, or in a FASTA/FASTQ file\n(or two FASTA/FASTQ\
    \ files for paired reads).</li>\n</ol>\n<p>Run using a mapped BAM file of ONT\
    \ reads:</p>\n<pre><code>viridian assemble --bam reads.bam ont ref.fasta amplicons.bed\
    \ outdir\n</code></pre>\n<p>Run using a FASTQ file of ONT reads:</p>\n<pre><code>viridian\
    \ assemble --reads_to_map reads.fastq ont ref.fasta amplicons.bed outdir\n</code></pre>\n\
    <p>Run using two FASTQ files of paired Illumina reads:</p>\n<pre><code>viridian\
    \ assemble \\\n  --reads_to_map reads1.fastq --mates_to_map reads2.fastq \\\n\
    \  illumina ref.fasta amplicons.bed outdir\n</code></pre>\n<h2>\n<a id=\"user-content-output\"\
    \ class=\"anchor\" href=\"#output\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Output</h2>\n<p>The important\
    \ files are:</p>\n<ul>\n<li>\n<code>consensus.final_assembly.fa</code>: this contains\
    \ the consensus sequence.</li>\n<li>\n<code>amplicon_data.json</code>: JSON file\
    \ containing details of what happened when\ntrying to make a consensus sequence\
    \ of each amplicon.</li>\n</ul>\n"
  stargazers_count: 1
  subscribers_count: 5
  topics: []
  updated_at: 1621027587.0
j23414/singularity_event:
  data_format: 2
  description: Review how to write a singularity image
  filenames:
  - Singularity
  full_name: j23414/singularity_event
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity_event" class="anchor" href="#singularity_event"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity_event</h1>

    <p><a href="https://singularity-hub.org/collections/4858" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Review how to write a singularity image</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602550496.0
jackjamend/CaptionContainer:
  data_format: 2
  description: Container for captioning project using pytorch and java
  filenames:
  - torch_java/Singularity.simple_v2
  - torch_java/Singularity.simple_v3
  - torch_java/Singularity.torch_java11_w_py
  - torch_java/Singularity.simple_torch:0.4
  - torch_java/Singularity.torch_java11
  - torch_java/Singularity.simple
  full_name: jackjamend/CaptionContainer
  latest_release: null
  readme: '<h1>

    <a id="user-content-evolutionary-algorithm" class="anchor" href="#evolutionary-algorithm"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evolutionary
    Algorithm</h1>

    <p><a href="https://github.com/anyaevostinar/evo-algo/releases"><img src="https://camo.githubusercontent.com/ff63e2cc80517f7b8e8246b33025f569d757ede0ae65c7ea57418d79e5a3709d/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f25324676657273696f6e2d62616467652e6a736f6e"
    alt="version" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fversion-badge.json"
    style="max-width:100%;"></a>

    <a href="https://travis-ci.com/anyaevostinar/evo-algo" rel="nofollow"><img src="https://camo.githubusercontent.com/2bde10efc09d993aad000b3101be56d5410d0ced348c4c7bbedbc7ffce79d630/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f616e796165766f7374696e61722f65766f2d616c676f2e737667"
    alt="" data-canonical-src="https://img.shields.io/travis/anyaevostinar/evo-algo.svg"
    style="max-width:100%;"></a>

    <a href="https://evo-algo.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img
    src="https://camo.githubusercontent.com/2af827df5df15ff6f5c6a9fff5021e631a0049aa2274f98e983135bb66f0ed81/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f65766f2d616c676f2f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/evo-algo/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="https://evo-algo.readthedocs.io/en/latest/" rel="nofollow"><img src="https://camo.githubusercontent.com/0e035446b6d1c7a911bd4b203bd581f2116c7873352a90d4797dc08119abbd0e/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f253246646f63756d656e746174696f6e2d636f7665726167652d62616467652e6a736f6e"
    alt="documentation coverage" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fdocumentation-coverage-badge.json"
    style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/anyaevostinar/evo-algo" rel="nofollow"><img src="https://camo.githubusercontent.com/869814fe0b90d0baadc37140ac31d6d9c0e4e00c2854a51f1030c40678bc13a4/68747470733a2f2f636f6465636f762e696f2f67682f616e796165766f7374696e61722f65766f2d616c676f2f6272616e63682f6d61737465722f67726170682f62616467652e737667"
    alt="code coverage status" data-canonical-src="https://codecov.io/gh/anyaevostinar/evo-algo/branch/master/graph/badge.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/anyaevostinar/evo-algo/search?q=todo+OR+fixme&amp;type="><img
    src="https://camo.githubusercontent.com/4455f1d1625d643fe17506cb6ad50a9a6612ce62ab4667dc60b75616241c7534/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e636f6d25324665766f2d616c676f253246646f746f2d62616467652e6a736f6e"
    alt="dotos" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.com%2Fevo-algo%2Fdoto-badge.json"
    style="max-width:100%;"></a>

    <a href="https://github.com/anyaevostinar/evo-algo"><img src="https://camo.githubusercontent.com/db51ac0d7785eb561dcfc0cbccfc0cfed9872513120f8f732b1301504c4eb32e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e796165766f7374696e61722f65766f2d616c676f2e7376673f7374796c653d666c61742d737175617265266c6f676f3d676974687562266c6162656c3d5374617273266c6f676f436f6c6f723d7768697465"
    alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/anyaevostinar/evo-algo.svg?style=flat-square&amp;logo=github&amp;label=Stars&amp;logoColor=white"
    style="max-width:100%;"></a></p>

    <p>An evolutionary algorithm</p>

    <p>Check out the live in-browser web app at <a href="https://anyaevostinar.github.io/evo-algo"
    rel="nofollow">https://anyaevostinar.github.io/evo-algo</a>.</p>

    <ul>

    <li>Free software: MIT license</li>

    <li>Documentation: <a href="https://evo-algo.readthedocs.io" rel="nofollow">https://evo-algo.readthedocs.io</a>.</li>

    </ul>

    <h2>

    <a id="user-content-features" class="anchor" href="#features" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Features</h2>

    <ul>

    <li>TODO</li>

    </ul>

    <p><a href="docs/assets/cookie.gif" target="_blank" rel="noopener noreferrer"><img
    src="docs/assets/cookie.gif" alt="cookie monster example" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>This package was created with <a href="https://github.com/audreyr/cookiecutter">Cookiecutter</a>
    and the <a href="https://github.com/devosoft/cookiecutter-empirical-project">devosoft/cookiecutter-empirical-project</a>
    project template.</p>

    <p>This package uses <a href="https://github.com/devosoft/Empirical#readme">Empirical</a>,
    a library of tools for scientific software development, with emphasis on also
    being able to build web interfaces using Emscripten.</p>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <p>To install <a href="https://github.com/devosoft/Empirical">Empirical</a>, pull
    down a clone of the Empirical repository.  See <a href="https://empirical.readthedocs.io/en/latest/QuickStartGuides"
    rel="nofollow">Quick Start Guides</a> for directions on cloning and using the
    library.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616709684.0
jchrist27/singularity-test:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jchrist27/singularity-test
  latest_release: null
  readme: '<h2>

    <a id="user-content-annotsv-container-for-wgs-pipeline" class="anchor" href="#annotsv-container-for-wgs-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>AnnotSV
    container for WGS pipeline</h2>

    <p>This is a singularity recipe for AnnotSV 2.3 and 2.2.</p>

    <p>sudo singularity build annotsv2.3.sif Singularity</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1618801208.0
jcoreyes/erl:
  data_format: 2
  description: null
  filenames:
  - docker/Singularity
  - docker/railrl_hand_v3/Singularity
  - docker/railrl_hand_v3/Singularity_cpu
  - docker/railrl_hand_tf_v1/Singularity
  - docker/railrl_hand_tf_v1/Singularity_cpu
  - docker/railrl_v10_cuda10-1_mj2-0-2-2_torch0-4-1_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_hand_v1/Singularity
  - docker/railrl_hand_v1/Singularity_cpu
  - docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity
  - docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity_cpu
  - docker/railrl_v6_cuda9/Singularity
  - docker/railrl_ray_gym-0-12-0/Singularity_from_scratch_cuda8
  - docker/railrl_ray_gym-0-12-0/Singularity_from_scratch
  - docker/railrl_v9_cuda10-1_mj1-50-1-59_torch0-4-1_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_v7/Singularity
  - docker/railrl_v11_cuda10-1_mj2-0-2-2_torch0-3-1_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_v6_cuda8/Singularity
  - docker/metac_railrl_v12_cuda10-1_mj2-0-2-2_torch1-4-0_gym0-12-5_py3-6-5/Singularity
  - docker/metac_railrl_v12_cuda10-1_mj2-0-2-2_torch1-4-0_gym0-12-5_py3-6-5/Singularity_cpu
  - docker/railrl_ray/Singularity
  - docker/railrl_v7_cuda8/Singularity
  - docker/railrl_v5/singularity/Singularity
  - docker/railrl_v9-5_cuda10-1_mj1-50-1-59_torch1-1-0_gym0-10-5_py3-5-2/Singularity
  - docker/railrl_hand_v2/Singularity
  - docker/railrl_hand_v2/Singularity_cpu
  - docker/railrl_v8_cuda10-1/Singularity
  - docker/railrl_gpu_mujoco1-5-v4/singularity/Singularity
  full_name: jcoreyes/erl
  latest_release: null
  readme: '<p>README last updated on: 01/24/2018</p>

    <h1>

    <a id="user-content-railrl" class="anchor" href="#railrl" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>railrl</h1>

    <p>Reinforcement learning framework.

    Some implemented algorithms:</p>

    <ul>

    <li><a href="examples/ddpg.py">Deep Deterministic Policy Gradient (DDPG)</a></li>

    <li><a href="examples/sac.py">Soft Actor Critic</a></li>

    <li><a href="examples/dqn_and_double_dqn.py">(Double) Deep Q-Network (DQN)</a></li>

    <li><a href="examples/her.py">Hindsight Experience Replay (HER)</a></li>

    <li><a href="examples/model_based_dagger.py">MPC with Neural Network Model</a></li>

    <li>

    <a href="examples/naf.py">Normalized Advantage Function (NAF)</a>

    <ul>

    <li>WARNING: I haven''t tested this NAF implementation much, so it may not match
    the paper''s performance. I''m pretty confident about the other two implementations
    though.</li>

    </ul>

    </li>

    </ul>

    <p>To get started, checkout the example scripts, linked above.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <h3>

    <a id="user-content-some-dependancies" class="anchor" href="#some-dependancies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Some
    dependancies</h3>

    <ul>

    <li><code>sudo apt-get install swig</code></li>

    </ul>

    <h3>

    <a id="user-content-create-conda-env" class="anchor" href="#create-conda-env"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    Conda Env</h3>

    <p>Install and use the included ananconda environment</p>

    <pre><code>$ conda env create -f docker/railrl/railrl-env.yml

    $ source activate railrl-env

    (railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py

    </code></pre>

    <p>Or if you want you can use the docker image included.</p>

    <h3>

    <a id="user-content-download-simulation-env-code" class="anchor" href="#download-simulation-env-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download
    Simulation Env Code</h3>

    <ul>

    <li>

    <a href="https://github.com/vitchyr/multiworld">multiworld</a> (contains environments):<code>git
    clone https://github.com/vitchyr/multiworld</code>

    </li>

    </ul>

    <h3>

    <a id="user-content-optional-install-doodad" class="anchor" href="#optional-install-doodad"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>(Optional)
    Install doodad</h3>

    <p>I recommend installing <a href="https://github.com/justinjfu/doodad">doodad</a>
    to

    launch jobs. Some of its nice features include:</p>

    <ul>

    <li>Easily switch between running code locally, on a remote compute with

    Docker, on EC2 with Docker</li>

    <li>Easily add your dependencies that can''t be installed via pip (e.g. you

    borrowed someone''s code)</li>

    </ul>

    <p>If you install doodad, also modify <code>CODE_DIRS_TO_MOUNT</code> in <code>config.py</code>
    to

    include:</p>

    <ul>

    <li>Path to rllab directory</li>

    <li>Path to railrl directory</li>

    <li>Path to other code you want to juse</li>

    </ul>

    <p>You''ll probably also need to update the other variables besides the docker

    images/instance stuff.</p>

    <h3>

    <a id="user-content-setup-config-file" class="anchor" href="#setup-config-file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup
    Config File</h3>

    <p>You must setup the config file for launching experiments, providing paths to
    your code and data directories. Inside <code>railrl/config/launcher_config.py</code>,
    fill in the appropriate paths. You can use <code>railrl/config/launcher_config_template.py</code>
    as an example reference.</p>

    <p><code>cp railrl/launchers/config-template.py railrl/launchers/config.py</code></p>

    <h2>

    <a id="user-content-visualizing-a-policy-and-seeing-results" class="anchor" href="#visualizing-a-policy-and-seeing-results"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visualizing
    a policy and seeing results</h2>

    <p>During training, the results will be saved to a file called under</p>

    <pre><code>LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;

    </code></pre>

    <ul>

    <li>

    <code>LOCAL_LOG_DIR</code> is the directory set by <code>railrl.launchers.config.LOCAL_LOG_DIR</code>

    </li>

    <li>

    <code>&lt;exp_prefix&gt;</code> is given either to <code>setup_logger</code>.</li>

    <li>

    <code>&lt;foldername&gt;</code> is auto-generated and based off of <code>exp_prefix</code>.</li>

    <li>inside this folder, you should see a file called <code>params.pkl</code>.
    To visualize a policy, run</li>

    </ul>

    <pre><code>(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;/params.pkl

    </code></pre>

    <p>If you have rllab installed, you can also visualize the results

    using <code>rllab</code>''s viskit, described at

    the bottom of <a href="http://rllab.readthedocs.io/en/latest/user/cluster.html"
    rel="nofollow">this page</a></p>

    <p>tl;dr run</p>

    <div class="highlight highlight-source-shell"><pre>python rllab/viskit/frontend.py
    LOCAL_LOG_DIR/<span class="pl-k">&lt;</span>exp_prefix<span class="pl-k">&gt;</span>/</pre></div>

    <h3>

    <a id="user-content-add-paths" class="anchor" href="#add-paths" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Add paths</h3>

    <pre><code>export PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo

    </code></pre>

    <h2>

    <a id="user-content-credit" class="anchor" href="#credit" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credit</h2>

    <p>A lot of the coding infrastructure is based on <a href="https://github.com/rll/rllab">rllab</a>.

    Also, the serialization and logger code are basically a carbon copy.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1603507389.0
jekriske/atom:
  data_format: 2
  description: Atom.io editor in a singularity container.
  filenames:
  - Singularity
  - Singularity.1.24.0
  full_name: jekriske/atom
  latest_release: null
  readme: '<h1>

    <a id="user-content-cc-camp" class="anchor" href="#cc-camp" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cc-camp</h1>

    <p>Container Camp test repo</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1519814161.0
jekriske/git:
  data_format: 2
  description: Singularity container for git
  filenames:
  - Singularity
  - git-2.16.1/Singularity.2.16.1
  - git-2.15.0/Singularity.2.15.0
  full_name: jekriske/git
  latest_release: null
  readme: '<h1>

    <a id="user-content-a-singularity-container-for-installing-manta" class="anchor"
    href="#a-singularity-container-for-installing-manta" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>A singularity container
    for installing manta</h1>

    <h2>

    <a id="user-content-build-with" class="anchor" href="#build-with" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build with:</h2>

    <p>sudo singularity build manta.simg Singularity</p>

    <ul>

    <li>Build files are found in ./build</li>

    <li>Install files are found in ./install</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1517474496.0
jekriske/lolcow:
  data_format: 2
  description: A low fat version of lolcow
  filenames:
  - Singularity
  - Singularity.lowfat
  full_name: jekriske/lolcow
  latest_release: null
  readme: '<h1>

    <a id="user-content-work-in-progress" class="anchor" href="#work-in-progress"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>work
    in progress</h1>

    <p>attempt to build a base singularity image with spack that can be used as the
    bootstrap for

    other singularity images that would perform the spack install of a particular
    package</p>

    <p>currently having an issue with stage directory for spack attempting to write
    to

    the immutable squashfs</p>

    <p>as expected, the child container will happily install during %post since it
    can write</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1524977683.0
jekriske/r-base:
  data_format: 2
  description: A base R build
  filenames:
  - Singularity
  - Singularity.3.4.4
  - Singularity.3.4.3
  - Singularity.3.4.2
  full_name: jekriske/r-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-stahb" class="anchor" href="#stahb" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>staHB</h1>

    <p>Hierarchical Bayesian State Trace Analysis</p>

    <p>This repository aims to compare a few different methods of conducting an STA.
    As such, it includes some code from both <a href="https://github.com/michaelkalish/STA">Kalish,
    M. L., Dunn, J. C., Burdakov, O. P., &amp; Sysoev, O. (2016). A statistical test
    of the equality of latent orders. Journal of Mathematical Psychology, 70, 1-11.</a>
    and <a href="http://dx.doi.org/10.1016/j.jmp.2015.08.004" rel="nofollow">Davis-Stober,
    C. P., Morey, R. D., Gretton, M., &amp; Heathcote, A. (2016). Bayes factors for
    state-trace analysis. Journal of Mathematical Psychology, 72, 116-129.</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1521975422.0
jekriske/rstudio:
  data_format: 2
  description: Rstudio Singularity Recipes
  filenames:
  - Singularity
  - Rstudio_Desktop-1.1.442/Singularity.1.1.442_desktop
  - Rstudio_Desktop-1.1.447/Singularity.1.1.447_desktop
  - Rstudio_Desktop-1.1.419/Singularity.1.1.419_desktop
  - Rstudio_Desktop-1.1.414/Singularity.1.1.414_desktop
  - Rstudio_Desktop-1.1.423/Singularity.1.1.423_desktop
  full_name: jekriske/rstudio
  latest_release: null
  readme: '<h1>

    <a id="user-content-stahb" class="anchor" href="#stahb" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>staHB</h1>

    <p>Hierarchical Bayesian State Trace Analysis</p>

    <p>This repository aims to compare a few different methods of conducting an STA.
    As such, it includes some code from both <a href="https://github.com/michaelkalish/STA">Kalish,
    M. L., Dunn, J. C., Burdakov, O. P., &amp; Sysoev, O. (2016). A statistical test
    of the equality of latent orders. Journal of Mathematical Psychology, 70, 1-11.</a>
    and <a href="http://dx.doi.org/10.1016/j.jmp.2015.08.004" rel="nofollow">Davis-Stober,
    C. P., Morey, R. D., Gretton, M., &amp; Heathcote, A. (2016). Bayes factors for
    state-trace analysis. Journal of Mathematical Psychology, 72, 116-129.</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1524977518.0
jekriske/shinyapp:
  data_format: 2
  description: A Containerized Shiny App
  filenames:
  - Singularity
  full_name: jekriske/shinyapp
  latest_release: null
  readme: '<p>This is a proof of concept demonstrating a Shiny application within
    a container.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1519301630.0
jekriske/svn2git:
  data_format: 2
  description: Singularity container for svn2git
  filenames:
  - Singularity
  full_name: jekriske/svn2git
  latest_release: null
  readme: '<h1>

    <a id="user-content-work-in-progress" class="anchor" href="#work-in-progress"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>work
    in progress</h1>

    <p>attempt to build a base singularity image with spack that can be used as the
    bootstrap for

    other singularity images that would perform the spack install of a particular
    package</p>

    <p>currently having an issue with stage directory for spack attempting to write
    to

    the immutable squashfs</p>

    <p>as expected, the child container will happily install during %post since it
    can write</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1511601734.0
jekriske/vscode:
  data_format: 2
  description: Visual Studio Code base container
  filenames:
  - Singularity
  full_name: jekriske/vscode
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>singularity</h1>

    <p>Development Branch</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1535667885.0
jesus-gorronogoitia/easy:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jesus-gorronogoitia/easy
  latest_release: null
  readme: '<h1>

    <a id="user-content-easy" class="anchor" href="#easy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>easy</h1>

    <p>Example of Singularity container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1615937293.0
jintonic/sing:
  data_format: 2
  description: Singularity Recipe for HPC
  filenames:
  - Singularity
  full_name: jintonic/sing
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/5087" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-sing" class="anchor" href="#sing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>sing</h1>

    <p>Singularity Recipe for HPC</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610801957.0
jkwmoore/singularity-gha-docker:
  data_format: 2
  description: A not working Github actions docker example
  filenames:
  - Singularity
  full_name: jkwmoore/singularity-gha-docker
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-builder---github-actions\" class=\"\
    anchor\" href=\"#singularity-builder---github-actions\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Singularity Builder\
    \ - Github Actions</h1>\n<p>This has been converted from the Gitlab Project here:\
    \  <a href=\"https://gitlab.com/singularityhub/gitlab-ci\" rel=\"nofollow\">https://gitlab.com/singularityhub/gitlab-ci</a></p>\n\
    <p>This is a simple example of how you can achieve:</p>\n<ul>\n<li>version control\
    \ of your recipes</li>\n<li>versioning to include image hash <em>and</em> commit\
    \ id</li>\n<li>build of associated container and</li>\n<li>push to a storage endpoint\
    \ (or <del>GitLab</del> Github artifact)</li>\n</ul>\n<p>for a reproducible build\
    \ workflow.</p>\n<p><strong>Why should this be managed via <del>GitLab</del> Github?</strong></p>\n\
    <p><del>GitLab</del> Github, by way of easy integration with continuous integration,\
    \ is an easy way\nto have a workflow set up where multiple people can collaborate\
    \ on a container recipe,\nthe recipe can be tested (with whatever testing you\
    \ need), discussed in pull requests,\nand then finally pushed to be a <del>GitLab</del>\
    \ Github artifact, to your storage of choice\nor to Singularity Registry.</p>\n\
    <p><strong>Why should I use this instead of a service?</strong></p>\n<p>You could\
    \ use a remote builder, but if you do the build in a continuous integration\n\
    service you get complete control over it. This means everything from the version\
    \ of\nSingularity to use, to the tests that you run for your container. You have\
    \ a lot more\nfreedom in the rate of building, and organization of your repository,\
    \ because it's you\nthat writes the configuration. Although the default would\
    \ work for most, you can\nedit the build, setup, and circle configuration file\
    \ in the\n<a href=\".gitlabci\">.gitlabci</a> folder to fit your needs.</p>\n\
    <h2>\n<a id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quick Start</h2>\n<p>Add your Singularity recipes to this repository,\
    \ and edit the build commands in\nthe <a href=\".gitlabci/build.sh\">build.sh</a>\
    \ file. This is where you can specify endpoints\n(Singularity Registry, Dropbox,\
    \ Google Storage, AWS) along with container names\n(the uri) and tag. You can\
    \ build as many recipes as you like, just add another line!</p>\n<div class=\"\
    highlight highlight-source-yaml\"><pre>                               <span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> recipe relative to repository base</span>\n\
    \  - <span class=\"pl-s\">/bin/bash .gitlabci/build.sh Singularity</span>\n  -\
    \ <span class=\"pl-s\">/bin/bash .gitlabci/build.sh --uri collection/container\
    \ --tag tacos --cli google-storage Singularity</span>\n  - <span class=\"pl-s\"\
    >/bin/bash .gitlabci/build.sh --uri collection/container --cli google-drive Singularity</span>\n\
    \  - <span class=\"pl-s\">/bin/bash .gitlabci/build.sh --uri collection/container\
    \ --cli globus Singularity</span>\n  - <span class=\"pl-s\">/bin/bash .gitlabci/build.sh\
    \ --uri collection/container --cli registry Singularity</span></pre></div>\n<p>For\
    \ each client that you use, required environment variables (e.g., credentials\
    \ to push,\nor interact with the API) must be defined in the (encrypted) Travis\
    \ environment. To\nknow what variables to define, along with usage for the various\
    \ clients, see\nthe <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">client specific pages</a></p>\n<h2>\n<a id=\"user-content-detailed-started\"\
    \ class=\"anchor\" href=\"#detailed-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Detailed Started</h2>\n<h3>\n\
    <a id=\"user-content-0-fork-this-repository\" class=\"anchor\" href=\"#0-fork-this-repository\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>0. Fork this repository</h3>\n<p>You can clone and tweak, but it's\
    \ easiest likely to get started with our example\nfiles and edit them as you need.</p>\n\
    <h3>\n<a id=\"user-content-1-get-to-know-gitlab-github-actions\" class=\"anchor\"\
    \ href=\"#1-get-to-know-gitlab-github-actions\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>1. Get to Know <del>GitLab</del>\
    \ Github Actions</h3>\n<p>Github has a built-in Continuous Integration service\
    \ called Github Actions you should be able to use for free. You can get started\
    \ here <a href=\"https://github.com/features/actions\">https://github.com/features/actions</a>\
    \ and take a look at the <code>.github\\workflows\\build.yml</code>.</p>\n<p>Artifacts\
    \ will be found in the <code>/home/runner/work/REPO-NAME/REPO-NAME/</code> directory.</p>\n\
    <h3>\n<a id=\"user-content-2-add-your-recipes\" class=\"anchor\" href=\"#2-add-your-recipes\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2. Add your Recipe(s)</h3>\n<p>For the example here, we have a single\
    \ recipe named \"Singularity\" that is provided\nas an input argument to the <a\
    \ href=\".gitlabci/build.sh\">build script</a>. You could add another\nrecipe,\
    \ and then of course call the build to happen more than once.\nThe build script\
    \ will name the image based on the recipe, and you of course\ncan change this.\
    \ Just write the path to it (relative to the repository base) in\nyour <a href=\"\
    .github/workflows/build.yml\">.github/workflows/build.yml</a>.</p>\n<h3>\n<a id=\"\
    user-content-3-configure-singularity\" class=\"anchor\" href=\"#3-configure-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>3. Configure Singularity</h3>\n<p>We previously used <a href=\".gitlabci/setup.sh\"\
    >setup</a> to setup the build, but now use a base image instead.\nThe previous\
    \ instructions are provided for posterity.</p>\n<ul>\n<li>\n<p><del>Install Singularity,\
    \ we use the release 2.6 branch as it was the last to not be written in GoLang.\
    \ You could of course change the lines in <a href=\".gitlabci/setup.sh\">setup.sh</a>\
    \ to use a specific tagged release, an older version, or development version.</del></p>\n\
    </li>\n<li>\n<p>The current Github Action is using the <code>quay.io/singularity/singularity:v3.7.3</code>\
    \ image - at present the slim images are missing /bin/bash - the version can be\
    \ changed at will as per your production environment versioning.</p>\n</li>\n\
    <li>\n<p>Install the sregistry client, if needed. The <a href=\"https://singularityhub.github.io/sregistry-cli\"\
    \ rel=\"nofollow\">sregistry client</a> allows you to issue a command like \"\
    sregistry push ...\" to upload a finished image to one of your cloud / storage\
    \ endpoints. By default, the push won't happen, and you will just build an image\
    \ using the CI.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-4-configure-the-build\"\
    \ class=\"anchor\" href=\"#4-configure-the-build\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>4. Configure\
    \ the Build</h3>\n<p>The basic steps for the <a href=\".gitlabci/build.sh\">build</a>\
    \ are the following:</p>\n<ul>\n<li>Running build.sh with no inputs will default\
    \ to a recipe called \"Singularity\" in the base of the repository. You can provide\
    \ an argument to point to a different recipe path, always relative to the base\
    \ of your repository.</li>\n<li>If you want to define a particular unique resource\
    \ identifier for a finished container (to be uploaded to your storage endpoint)\
    \ you can do that with <code>--uri collection/container</code>. If you don't define\
    \ one, a robot name will be generated.</li>\n<li>You can add <code>--uri</code>\
    \ to specify a custom name, and this can include the tag, OR you can specify <code>--tag</code>\
    \ to go along with a name without one. It depends on which is easier for you.</li>\n\
    <li>If you add <code>--cli</code> then this is telling the build script that you\
    \ have defined the <a href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"\
    nofollow\">needed environment variables</a> for your <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">client of choice</a> and you want successful builds to be pushed\
    \ to your storage endpoint. See <a href=\"https://singularityhub.github.io/sregistry-cli/clients\"\
    \ rel=\"nofollow\">here</a> for a list of current client endpoints, or roll your\
    \ own!</li>\n</ul>\n<p>See the <a href=\".gitlab-ci.yml\">.gitlab-ci.yml</a> for\
    \ examples of this build.sh command (commented out). If there is some cloud service\
    \ that you'd like that is not provided, please <a href=\"https://www.github.com/singularityhub/sregistry-cli/issues\"\
    >open an issue</a>.</p>\n<h3>\n<a id=\"user-content-5-pull--download-your-container\"\
    \ class=\"anchor\" href=\"#5-pull--download-your-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>5. Pull\
    \ / Download your Container</h3>\n<p>You can access the artifacts from the Actions\
    \ tab, under the build action for any given run. You may wish to edit the <code>build.yml</code>\
    \ to export individual .sif images rather than a zip file of artifacts.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620413122.0
jlabsolid/container:
  data_format: 2
  description: null
  filenames:
  - Singularity_jlabsolidbase_devel
  - Singularity.1.0.2
  full_name: jlabsolid/container
  latest_release: null
  readme: '<p>Container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1521257911.0
jmurga/Analytical.jl:
  data_format: 2
  description: Analytical MK approach and ABC inference based on Julia
  filenames:
  - scripts/singularity/Singularity
  full_name: jmurga/Analytical.jl
  latest_release: null
  readme: '<h1>

    <a id="user-content-abc-mk" class="anchor" href="#abc-mk" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ABC-MK</h1>

    <p><a href="https://jmurga.github.io/Analytical.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667"
    alt="" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg"
    style="max-width:100%;"></a></p>

    <p>Extended ABC-MK calculations accounting for background selection and weak adaptation
    based on Julia language.</p>

    <p>Included in this repository are the scripts used to simulate and infer parameters
    from ABC software.</p>

    <p>To install the module we highly recommend to use <a href="https://julialang.org/downloads/"
    rel="nofollow">LTS official Julia binaries</a>. You can easily export the Julia
    bin through <code>export PATH="/path/to/directory/julia-1.0.5/bin:$PATH"</code>.
    In addition, since the package use <em>scipy</em> functions, we recommend executing
    Julia activating the following <a href="https://github.com/jmurga/Analytical.jl/tree/master/scripts/abc-mk.yml">conda
    enviroment</a> (or you can just just install <em>scipy</em> in your default python).
    Once Julia is installed just run:</p>

    <div class="highlight highlight-source-shell"><pre>julia -e <span class="pl-s"><span
    class="pl-pds">''</span>using Pkg;Pkg.add(PackageSpec(path="https://github.com/jmurga/Analytical.jl"))<span
    class="pl-pds">''</span></span></pre></div>

    <p>We provide a Docker image based on Debian including Julia and all packages
    needed to run the estimations. It also includes jupyterlab and several data science
    packages. You can run the whole Debian image or just the jupyterlab instance pulling
    the image from dockerhub:</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    Pull the image</span>

    docker pull jmurga/mktest

    <span class="pl-c"><span class="pl-c">#</span> Run docker bash interactive session
    linking to some local volume to export data</span>

    docker run -i -t -v <span class="pl-smi">${HOME}</span>/<span class="pl-k">&lt;</span>anyData<span
    class="pl-k">&gt;</span>:/data/<span class="pl-k">&lt;</span>anyData<span class="pl-k">&gt;</span>
    jmurga/abcmk julia -J /root/mktest.so

    <span class="pl-c"><span class="pl-c">#</span> Run only jupyter notebook from
    docker image. Change the port if 8888 is already used</span>

    docker run -i -t -p 8888:8888 jmurga/abcmk /bin/bash -c <span class="pl-s"><span
    class="pl-pds">"</span>jupyter-lab --ip=''*'' --port=8888 --no-browser --allow-root<span
    class="pl-pds">"</span></span></pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621523421.0
jmurga/bgd-pic:
  data_format: 2
  description: null
  filenames:
  - Singularity.breakseq
  - Singularity.pophuman
  - Singularity.abcmk
  full_name: jmurga/bgd-pic
  latest_release: null
  readme: '<h1>

    <a id="user-content-pythoncourse" class="anchor" href="#pythoncourse" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PythonCourse</h1>

    <p>From A to Z. For fun.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620435632.0
jolars/HessianScreening:
  data_format: 2
  description: Code for academic paper The Hessian Screening Rule
  filenames:
  - Singularity
  full_name: jolars/HessianScreening
  latest_release: v0.1.0
  readme: '

    <h1>

    <a id="user-content-code-for-the-hessian-screening-rule" class="anchor" href="#code-for-the-hessian-screening-rule"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Code
    for the Hessian Screening Rule</h1>


    <p><a href="https://github.com/jolars/HessianScreening/actions"><img src="https://github.com/jolars/HessianScreening/workflows/R-CMD-check/badge.svg"
    alt="R-CMD-check" style="max-width:100%;"></a>

    <a href="https://arxiv.org/abs/2104.13026" rel="nofollow"><img src="https://camo.githubusercontent.com/0520bb4d232c0f8289fb80a5e50688bb68c43b90fd380490f1a59a25616be4b0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d313233342e35363738392d6233316231622e737667"
    alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-1234.56789-b31b1b.svg"
    style="max-width:100%;"></a></p>


    <h2>

    <a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Results</h2>

    <p>The results from the simulations, which were run on a dedicated HPC

    cluster, are stored in the <a href="results/">results folder</a>. The figures
    and

    tables in the paper, generated from these results, are stored in

    <a href="figures/"><code>figures/</code></a> and <a href="tables/"><code>tables/</code></a>
    respectively.</p>

    <h2>

    <a id="user-content-reproducing-the-results" class="anchor" href="#reproducing-the-results"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Reproducing
    the Results</h2>

    <p>The results from our paper were run through a singularity container.

    Check the releases for pre-built singularity containers that you can

    download and use.</p>

    <p>To reproduce the results, <strong>always</strong> use the singularity container.
    To

    run an experiment from the singularity container, call</p>

    <div class="highlight highlight-source-shell"><pre>singularity run --bind results:/Project/results
    container.sif <span class="pl-k">&lt;</span>script<span class="pl-k">&gt;</span></pre></div>

    <p>where <code>&lt;script&gt;</code> should be a name of a script in the <a href="experiments/">experiments

    folder</a>, such as <code>simulateddata.R</code>.</p>

    <h3>

    <a id="user-content-re-building-the-singularity-container" class="anchor" href="#re-building-the-singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Re-building
    the Singularity Container</h3>

    <p>If you want to re-build the singularity container from scratch (or

    simply want to clone the repo to your local drive), you can do so via

    the following steps.</p>

    <ol>

    <li>

    <p>Make sure you have installed and enabled

    <a href="https://git-lfs.github.com/">Git-LFS</a>. On ubuntu, for instance, you

    can install Git-LFS by calling</p>

    <div class="highlight highlight-source-shell"><pre>sudo apt update

    sudo apt install git-lfs</pre></div>

    <p>Then activate git-lfs by calling</p>

    <div class="highlight highlight-source-shell"><pre>git lfs install</pre></div>

    </li>

    <li>

    <p>Clone the repository to your local hard drive. On linux, using SSH

    authentication, run</p>

    <div class="highlight highlight-source-shell"><pre>git clone git@github.com:jolars/HessianScreening.git</pre></div>

    </li>

    <li>

    <p>Navigate to the root of the repo and build the singularity container

    by calling</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    HessianScreening

    sudo singularity build container.sif Singularity</pre></div>

    </li>

    </ol>

    <p>Then proceed as in <a href="#reproducing-the-results">Reproducing the Results</a>

    to run the experiments.</p>

    <h3>

    <a id="user-content-running-experiments-without-singularity-not-recommended" class="anchor"
    href="#running-experiments-without-singularity-not-recommended" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running Experiments
    without Singularity (Not Recommended!)</h3>

    <p>Alternatively, you may also reproduce the results by cloning this

    repository, then either opening the <code>HessianScreening.Rproj</code> file in
    R

    Studio or starting R in the root directory of this folder (which will

    activate the renv repository) and then run</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">renv</span><span
    class="pl-k">::</span>restore()</pre></div>

    <p>to restore the project library. Then build the R package (see below) and

    run the simulations directly by running the scripts in the experiments

    folder. This is <strong>not recommended</strong>, however, since it, unlike the

    Singularity container approach, does not exactly reproduce the software

    environment used when these simulations where originally run and may

    result in discrepancies due to differences in for instance operating

    systems, compilers, and BLAS/LAPACK implementations.</p>

    <h2>

    <a id="user-content-r-package" class="anchor" href="#r-package" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>R Package</h2>

    <p>If you want to build and experiment with the package, you can do so by

    calling</p>

    <div class="highlight highlight-source-shell"><pre> R CMD INSTALL  <span class="pl-c1">.</span></pre></div>

    <p>provided you have <code>cd</code>ed to the root folder of this repository.
    First

    ensure, however, that you have enabled the renv project library by

    calling <code>renv::restore()</code> (see the section above).</p>

    <h2>

    <a id="user-content-data" class="anchor" href="#data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

    <p>The datasets used in these simulations are stored in the <a href="data/">data

    folder</a>. Scripts to retrieve these datasets from their original

    sources can be found in <a href="data-raw/"><code>data-raw/</code></a>.</p>

    <h2>

    <a id="user-content-forking-and-git-lfs" class="anchor" href="#forking-and-git-lfs"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Forking
    and Git-LFS</h2>

    <p>Note that pushing large files using Git-LFS against forks of this repo

    <a href="https://docs.github.com/en/github/managing-large-files/collaboration-with-git-large-file-storage">counts
    against the bandwidth limits of this

    repo</a>,

    and so may fail if these limits are exceeded. If you for some reason

    need to do this and it fails, please file as issue here.</p>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics:
  - screening
  - singularity-container
  - simulations
  - renv
  - statistics
  - machine-learning
  - lasso
  updated_at: 1621967868.0
jolars/LookAheadScreening:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jolars/LookAheadScreening
  latest_release: v0.1.0
  readme: '

    <h1>

    <a id="user-content-code-for-look-ahead-screening-rules-for-the-lasso" class="anchor"
    href="#code-for-look-ahead-screening-rules-for-the-lasso" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Code for <em>Look-Ahead
    Screening Rules for the Lasso</em>

    </h1>


    <p><a href="https://github.com/jolars/HessianScreening/actions"><img src="https://github.com/jolars/LookAheadScreening/workflows/R-CMD-check/badge.svg"
    alt="R-CMD-check" style="max-width:100%;"></a></p>


    <h2>

    <a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Results</h2>

    <p>The results from the simulations, which were run on a dedicated HPC

    cluster, are stored in the <a href="results/">results folder</a>. The figures
    in

    the paper, generated from these results, are stored in

    <a href="figures/"><code>figures/</code></a>.</p>

    <h2>

    <a id="user-content-reproducing-the-results" class="anchor" href="#reproducing-the-results"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Reproducing
    the Results</h2>

    <p>The results from our paper were run through a singularity container.

    Check the releases for pre-built singularity containers that you can

    download and use.</p>

    <p>To reproduce the results, <strong>always</strong> use the singularity container.
    To

    run an experiment from the singularity container, call</p>

    <div class="highlight highlight-source-shell"><pre>singularity run --bind results:/Project/results
    container.sif <span class="pl-k">&lt;</span>script<span class="pl-k">&gt;</span></pre></div>

    <p>where <code>&lt;script&gt;</code> should be a name of a script in the <a href="experiments/">experiments

    folder</a>, such as <code>experiments/simulateddata.R</code>.</p>

    <h3>

    <a id="user-content-re-building-the-singularity-container" class="anchor" href="#re-building-the-singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Re-building
    the Singularity Container</h3>

    <p>If you want to re-build the singularity container from scratch (or

    simply want to clone the repo to your local drive), you can do so via

    the following steps.</p>

    <ol>

    <li>

    <p>Clone the repository to your local hard drive. On linux, using SSH

    authentication, run</p>

    <div class="highlight highlight-source-shell"><pre>git clone git@github.com:jolars/LookAheadScreening.git</pre></div>

    </li>

    <li>

    <p>Navigate to the root of the repo and build the singularity container

    by calling</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span>
    LookAheadScreening

    sudo singularity build container.sif Singularity</pre></div>

    </li>

    </ol>

    <p>Then proceed as in <a href="#reproducing-the-results">Reproducing the Results</a>

    to run the experiments.</p>

    <h3>

    <a id="user-content-running-experiments-without-singularity-not-recommended" class="anchor"
    href="#running-experiments-without-singularity-not-recommended" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running Experiments
    without Singularity (Not Recommended!)</h3>

    <p>Alternatively, you may also reproduce the results by cloning this

    repository and starting R in the root directory of this folder (which

    will activate the renv repository) and then run</p>

    <div class="highlight highlight-source-r"><pre><span class="pl-e">renv</span><span
    class="pl-k">::</span>restore()</pre></div>

    <p>to restore the project library. Then build the R package (see below) and

    run the simulations directly by running the scripts in the experiments

    folder. This is <strong>not recommended</strong>, however, since it, unlike the

    Singularity container approach, does not exactly reproduce the software

    environment used when these simulations where originally run and may

    result in discrepancies due to differences in for instance operating

    systems, compilers, and BLAS/LAPACK implementations.</p>

    <h2>

    <a id="user-content-r-package" class="anchor" href="#r-package" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>R Package</h2>

    <p>If you want to build and experiment with the package, you can do so by

    calling</p>

    <div class="highlight highlight-source-shell"><pre> R CMD INSTALL  <span class="pl-c1">.</span></pre></div>

    <p>provided you have <code>cd</code>ed to the root folder of this repository.
    First

    ensure, however, that you have enabled the renv project library by

    calling <code>renv::restore()</code> (see the section above).</p>

    <h2>

    <a id="user-content-data" class="anchor" href="#data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

    <p>The datasets used in these simulations are stored in the <a href="data/">data

    folder</a>. Scripts to retrieve these datasets from their original

    sources can be found in <a href="data-raw/"><code>data-raw/</code></a>.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1621967863.0
jolars/ReproduciblePythonProject:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jolars/ReproduciblePythonProject
  latest_release: null
  readme: '<h1>

    <a id="user-content-reproduciblepythonproject" class="anchor" href="#reproduciblepythonproject"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ReproduciblePythonProject</h1>

    <p>A template for reproducible projects using python, singularity, and c++ (via

    pybind11).</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1621967866.0
jolars/ReproducibleRProject:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: jolars/ReproducibleRProject
  latest_release: null
  readme: '<h1>

    <a id="user-content-reproduciblerproject" class="anchor" href="#reproduciblerproject"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ReproducibleRProject</h1>



    <p>A template for a reproducible research project using Singularity, R, and

    C++, with package dependency management in R based on renv.</p>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1621967866.0
josephwkania/radio_transients:
  data_format: 2
  description: 'Singularity containers with common radio transient search software.  '
  filenames:
  - Singularity.arm
  - Singularity
  - Singularity.cpu
  - Singularity.gpu
  full_name: josephwkania/radio_transients
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-radio_transients\" class=\"anchor\" href=\"\
    #radio_transients\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>radio_transients</h1>\n<p><a href=\"\"><img src=\"\
    https://camo.githubusercontent.com/88694793dd1e428a0aab6788e9bbd21141580f62cbc4d6310bbcc74fde83ab22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"\"><img src=\"https://camo.githubusercontent.com/ff5d91d6824296a5d7ffaad36635c5ef0688d2c0e21e50ccc94735fe8387faa7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"Forks\" data-canonical-src=\"https://img.shields.io/github/forks/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"\"><img src=\"https://camo.githubusercontent.com/6e6ccae69f7f4df8c46d4d56b7e36d27fd932cc463a486a3111796543c271ab9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"\"><img src=\"https://camo.githubusercontent.com/d5708d5c2bcd6f7d5f3565d9e75135e1eaa086ff847e198c14d187c5612f8203/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\"\
    \ alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/josephwkania/radio_transients?style=flat-square\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/5231\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-overview\" class=\"\
    anchor\" href=\"#overview\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Overview</h2>\n<p>These are my Singularity Recipes\
    \ for common radio transient software.\nThere are three containers</p>\n<h3>\n\
    <a id=\"user-content-radio_transients-1\" class=\"anchor\" href=\"#radio_transients-1\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>radio_transients</h3>\n<p>Contains everything (CPU+GPU)</p>\n<pre><code>CUDA\
    \ 10.2\nFETCH          https://github.com/devanshkv/fetch  -- In Conda environment\
    \ `FE`\nheimdall       https://sourceforge.net/p/heimdall-astro/wiki/Use/\n- dedisp\
    \       https://github.com/ajameson/dedisp\nhtop           https://htop.dev/\n\
    iqrm_apollo    https://gitlab.com/kmrajwade/iqrm_apollo\njupyterlab     https://jupyter.org/\n\
    PRESTO         https://www.cv.nrao.edu/~sransom/presto/\npsrdada        http://psrdada.sourceforge.net/\n\
    psrdada-python https://github.com/TRASAL/psrdada-python\npsrcat         https://www.atnf.csiro.au/people/pulsar/psrcat/download.html\n\
    pysigproc      https://github.com/devanshkv/pysigproc\nriptide        https://github.com/v-morello/riptide\n\
    sigproc        https://github.com/SixByNine/sigproc\nTempo          http://tempo.sourceforge.net/\n\
    RFIClean       https://github.com/ymaan4/RFIClean\nYAPP           https://github.com/jayanthc/yapp\n\
    your           https://github.com/thepetabyteproject/your\n</code></pre>\n<p>Get\
    \ with\n<code>singularity pull shub://josephwkania/radio_transients</code></p>\n\
    <p>*One of FETCH's dependencies causes PRESTO's Python scripts to fail.\nThis\
    \ necessitated putting them in different environments.\nEverything except for\
    \ PRESTO is in <code>RT</code>, which is loaded by default.\nPRESTO is in <code>PE</code>,\
    \ in the shell you can activate this\nwith <code>conda activate PE</code>. If\
    \ you need access outside the container,\nyou should use radio_transients:cpu,\
    \ which has PRESTO in the default environment.</p>\n<h3>\n<a id=\"user-content-radio_transients_cpu\"\
    \ class=\"anchor\" href=\"#radio_transients_cpu\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>radio_transients_cpu</h3>\n<p>Contains\
    \ CPU based programs</p>\n<pre><code>htop\niqrm_apollo\njupyterlab   \nPRESTO\n\
    psrcat\npysigproc\nriptide\nsigproc\nTempo \nRFIClean\nYAPP  \nyour\n</code></pre>\n\
    <p>Get with\n<code>singularity pull shub://josephwkania/radio_transients:cpu</code><br>\n\
    There is an arm version <code>Singularity.arm</code>, you can build yourself.</p>\n\
    <h3>\n<a id=\"user-content-radio_transients_gpu\" class=\"anchor\" href=\"#radio_transients_gpu\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>radio_transients_gpu</h3>\n<p>Contains gpu based programs</p>\n<pre><code>CUDA\
    \ 10.2\nFETCH      \njupyterlab\nheimdall\n- dedisp\nhtop \npsrdada \npsrdada-python\n\
    your\n</code></pre>\n<p>Get with\n<code>singularity pull shub://josephwkania/radio_transients:gpu</code></p>\n\
    <h3>\n<a id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>How to use</h3>\n<p>Your <code>$HOME</code> automatically gets mounted.\n\
    You can mount a directory with <code>-B /dir/on/host:/mnt</code>, which will mount\
    \ <code>/dir/on/host</code> to <code>/mnt</code> in the container.</p>\n<p>For\
    \ the gpu processes, you must pass <code>--nv</code> when running singularity.</p>\n\
    <p><code>singularity shell --nv -B /data:/mnt radio_transients_gpu.sif</code>\n\
    will mount <code>/data</code> to <code>/mnt</code>, give you GPU access, and drop\
    \ you into the interactive shell.</p>\n<p><code>singularity exec --nv -B /data:/mnt\
    \ radio_transients_gpu.sif your_heimdall.py -f /mnt/data.fil</code>\nwill mount\
    \ <code>/data</code> to <code>/mnt</code>, give you GPU access, and run your_heimdall.py\
    \ without entering the container.</p>\n<p>All the Python scripts are installed\
    \ in a Conda environment <code>RT</code>, this environment is automatically loaded.</p>\n\
    <p>You can see the commits and corresponding dates by running <code>singularity\
    \ inspect radio_transients.sif</code></p>\n<h3>\n<a id=\"user-content-shub\" class=\"\
    anchor\" href=\"#shub\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>shub</h3>\n<p>These are built on commit by Singularity\
    \ Hub at: <a href=\"https://singularity-hub.org/collections/5231\" rel=\"nofollow\"\
    >https://singularity-hub.org/collections/5231</a>\nThey where last built on 15-May-2021</p>\n\
    <h3>\n<a id=\"user-content-improvements\" class=\"anchor\" href=\"#improvements\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Improvements</h3>\n<p>If you come accross bug or have suggestions\
    \ for improvements, let me know or submit a pull request.</p>\n<h3>\n<a id=\"\
    user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Thanks</h3>\n\
    <p>To Kshitij Aggarwal for bug reports and suggestions.</p>\n"
  stargazers_count: 4
  subscribers_count: 2
  topics:
  - pulsars
  - fast-radio-bursts
  - psrdada
  - radio-astronomy
  updated_at: 1621141428.0
jsmentch/analyze-fmri:
  data_format: 2
  description: singularity recipes for naturalistic imaging / neuroscout project
  filenames:
  - Singularity.pliersgoogle
  - Singularity.room2reverb
  - Singularity.cifti
  - Singularity.googlespeech
  - Singularity.nat_img
  - Singularity.debian
  - Singularity_gpu
  - Singularity.nat_img_gpu
  - Singularity.centosslim
  - Singularity.centospliers
  - Singularity.centosdyneusr
  - Singularity.feature_extraction
  - Singularity.centos
  - Singularity.r2r
  full_name: jsmentch/analyze-fmri
  latest_release: null
  readme: '<h1>

    <a id="user-content-analyze-fmri" class="anchor" href="#analyze-fmri" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>analyze-fmri</h1>

    <p>singularity recipes for naturalistic imaging / neuroscout project</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619637484.0
jtevns/AnnotationPipeline:
  data_format: 2
  description: An annotation pipeline using snakemake
  filenames:
  - Singularity/Singularity.def
  full_name: jtevns/AnnotationPipeline
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-annotationpipeline\" class=\"anchor\" href=\"\
    #annotationpipeline\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>AnnotationPipeline</h1>\n<p>An annotation pipeline\
    \ using snakemake that curently supports pfam, eggnog, tigrfam annotations.</p>\n\
    <p>To run:<br>\n-Create a directory for annotation<br>\n-Create a folder and put\
    \ your original fasta files in\n- module load python3.7-anaconda\n-Run filtering\
    \ script on a directory of bins<br>\n\_\_\_\_-This is required to filter out bins\
    \ that do not meet the length requirement for running prodigal (20000 bases and\
    \ prep the directory for the pipeline)<br>\n\_\_\_\_- Run: <code>/nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/scripts/prep_annotation.py\
    \ folder_of_fastas</code><br>\n\_\_\_\_- the script results in the creation of\
    \ a directory called Passing_Bins in the directory the script is run from with\
    \ all\n\_\_\_\_bins linked in the directory that meet the length threshold.<br>\n\
    - copy the config template from /nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/config_template.yaml\
    \ and name it config.yaml.<br>\n- use the file extension in the Passing_Bins dir\
    \ (fa or fna or fasta etc).<br>\n- module load singularity.<br>\n-Run Annotation.smk\
    \  (I recommend putting this in a slurm script with 36 cores and 180gb mem for\
    \ fastest run time).<br>\n\_\_\_\_ <code>singularity exec /nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/Singularity/annotation_tools.sif\
    \ snakemake -s /nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/Annotation.smk\
    \ --cores</code><br>\nNote on headers:\nfasta headers cannot contain \":\"</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621642145.0
kevin-harms/sycltrain:
  data_format: 2
  description: Training examples for SYCL
  filenames:
  - Singularity
  full_name: kevin-harms/sycltrain
  latest_release: null
  readme: '<h1>

    <a id="user-content-sycltrain" class="anchor" href="#sycltrain" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>sycltrain</h1>

    <p>This work has been moved to the github repo:</p>

    <p><a href="https://github.com/alcf-perfengr/sycltrain">https://github.com/alcf-perfengr/sycltrain</a></p>

    '
  stargazers_count: 8
  subscribers_count: 3
  topics: []
  updated_at: 1613349330.0
kkelchte/simulation_supervised:
  data_format: 2
  description: The simulation-supervised package combines different sets of code needed
    to train a DNN policy to fly a drone.
  filenames:
  - Singularity
  full_name: kkelchte/simulation_supervised
  latest_release: null
  readme: '<h1>

    <a id="user-content-simulation-supervised" class="anchor" href="#simulation-supervised"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>simulation-supervised</h1>

    <p>The simulation-supervised package combines different sets of code needed to
    train a DNN policy to fly a drone.</p>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <ul>

    <li>

    <a href="https://www.github.com/kkelchte/pilot_online">online_training</a>: the
    tensorflow code used for training and running the DNN policy.</li>

    <li>

    <a href="https://www.github.com/kkelchte/hector_quadrotor">drone_simulator</a>:
    a simulated drone model for Gazebo. This is a copy of the original <a href="http://wiki.ros.org/hector_quadrotor"
    rel="nofollow">hector quadrotor model</a>.

    OR</li>

    <li>

    <a href="https://github.com/kkelchte/bebop_autonomy">bebop_autonomy</a>: a copy
    of the bebop autonomy package of ROS. This package allows you to test the DNN
    in the real-world. The copy is only for ensuring stability while performing research.
    There are no significant modifications so it is probably best to use <a href="https://github.com/AutonomyLab/bebop_autonomy">the
    original</a>. If you are using the Doshico docker image, it is already installed
    globally.</li>

    </ul>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <p>This package is best build in a separate <a href="http://wiki.ros.org/catkin/Tutorials/create_a_workspace"
    rel="nofollow">catkin workspace</a>.</p>

    <div class="highlight highlight-source-shell"><pre>mkdir -p <span class="pl-k">~</span>/simsup_ws/src

    <span class="pl-c1">cd</span> <span class="pl-k">~</span>/simsup_ws <span class="pl-k">&amp;&amp;</span>
    catkin_make

    <span class="pl-c1">cd</span> <span class="pl-k">~</span>/simsup_ws/src

    git clone https://github.com/kkelchte/simulation_supervised

    <span class="pl-c1">cd</span> <span class="pl-k">~</span>/simsup_ws <span class="pl-k">&amp;&amp;</span>
    catkin_make</pre></div>

    <p>You will have to set the correct path to your tensorflow pilot_online package.</p>

    <p>In case you are using different drone models, you will have to adjust the <a
    href="https://github.com/kkelchte/simulation-supervised/blob/master/simulation_supervised/config/sim_drone.yaml">config.yaml</a>
    file in order to set the correct rosparams.</p>

    <h2>

    <a id="user-content-run-some-experiments" class="anchor" href="#run-some-experiments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Run
    some experiments</h2>

    <p>Here are some common used setting combinations in order to remember them:</p>

    <pre><code># Test online the performance of a heuristic defined in tensorflow/gt_depth_online/../rosinterface.py
    using for instance recovery cameras flying 3 times through a generated canyon

    $ ./scripts/evaluate_model.sh -m auxd -s start_python_sing_gtdepth.sh -t test_depth_online
    -r true -w canyon -n 3

    # Train online in canyon, forest, sandbox

    $ ./scripts/train_model.sh -m mobilenet_025


    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1560186991.0
lhutton1/ga-dqn-tuner:
  data_format: 2
  description: Implements GA-DQN tuner which consists of a genetic algorithm that
    uses two deep Q-network agents.
  filenames:
  - Singularity
  full_name: lhutton1/ga-dqn-tuner
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-generating-high-performance-code-for-deep-learning-workloads-a-reinforcement-learning-based-approach\"\
    \ class=\"anchor\" href=\"#generating-high-performance-code-for-deep-learning-workloads-a-reinforcement-learning-based-approach\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Generating high-performance code for deep learning workloads: a reinforcement\
    \ learning based approach.</h1>\n<p><em>Implemented as part of a final year dissertation.\
    \ Should not be considered for production use.</em></p>\n<p>This project aims\
    \ to apply reinforcement learning to auto-tuning in AutoTVM (part of the TVM machine\
    \ learning compiler),\nin order to improve the experience of the end user. Currently,\
    \ reinforcement learning is applied to the GATuner - a genetic algorithm\nthat\
    \ repeatedly applies elitism, 2-point crossover and mutation to a population.\
    \ Named <strong>GA-DQN</strong>, the new tuner uses two independent\ndeep Q-network\
    \ (DQN)'s that are applied to crossover and mutation. Crossover is completed by\
    \ allowing DQN to suggest the point at\nwhich to crossover a gene, while, mutation\
    \ is completed by allowing DQN to select which detail to randomly mutate. In addition,\
    \ an evaluation\nframework is provided to assess the performance of GA-DQN.</p>\n\
    <p><a href=\"/assets/ga-dqn-pipeline.png\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img src=\"/assets/ga-dqn-pipeline.png\" alt=\"GA-DQN tuning pipeline\" title=\"\
    GA-DQN tuning pipeline\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-usage\"\
    \ class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Usage</h2>\n<p>To use the tuner, TVM\
    \ must be installed and visible within your python environment. Due to needing\
    \ additional features not available in a released\nversion of TVM, a forked version\
    \ of TVM is used which applies a small amount debugging code and a fix to the\
    \ PyTorch front-end parser. A pinned\nversion is also used as TVM is mostly in\
    \ a development stage and the API's used are unstable. Consequently, the GA-DQN\
    \ tuner has only been tested\nwith this specific commit, along with small modifications\
    \ ontop. The required version can be pulled from git like so:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>git clone --recursive https://github.com/lhutton1/tvm.git\
    \ tvm\n<span class=\"pl-c1\">cd</span> tvm\ngit checkout autotvm-measure-remote-time\n\
    git checkout d2452502b9486a7993d9dec3d04e449efdd81cf7</pre></div>\n<p>TVM also\
    \ requires a number of dependencies such as: Cuda, Python3.6, LLVM, XGBoost (for\
    \ the XGBTuner) and PyTorch (for the GA-DQN tuner). As such, we recommend using\
    \ a containerised environment powered by Singularity. Similar to docker, an image\
    \ must be built from which containers can be run based on the image. First install\
    \ Singularity, then build the image using a simple script provided:</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Install Singularity</span>\nsudo wget -O- http://neuro.debian.net/lists/xenial.us-ca.full\
    \ <span class=\"pl-k\">|</span> sudo tee /etc/apt/sources.list.d/neurodebian.sources.list\
    \ <span class=\"pl-k\">&amp;&amp;</span> \\\n    sudo apt-key adv --recv-keys\
    \ --keyserver hkp://pool.sks-keyservers.net:80 0xA5D32F012649A5A9 <span class=\"\
    pl-k\">&amp;&amp;</span> \\\n    sudo apt-get update\n    \nsudo apt-get install\
    \ -y singularity-container\n    \n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Build image</span>\n./create_image.sh</pre></div>\n<p>From this a container\
    \ can be created and GA-DQN can be run from within this container using the presented\
    \ shell:</p>\n<div class=\"highlight highlight-source-shell\"><pre>./create_container.sh\
    \ rl-tuner.simg</pre></div>\n<p>Now in the shell, test your container works correctly\
    \ by attempting to run the evaluation framework help prompt:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>python driver.py --help</pre></div>\n\
    <p><em>Note: This has been tested on a Ubuntu 18.04 setup and is not guaranteed\
    \ to work with other operating systems. These scripts have also been tested on\
    \ the University of Leeds HPC cluster, ARC.</em></p>\n<p><em>Note: it is possible\
    \ to build TVM and install its dependencies from scratch, although this is not\
    \ recommended due to the number of packages required. The process required should\
    \ be similar to that provided in <code>create_image.sh</code> script. However,\
    \ it is recommended you create a new virtual environment for python in this process.</em></p>\n\
    <h2>\n<a id=\"user-content-rl-tuner\" class=\"anchor\" href=\"#rl-tuner\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>RL\
    \ Tuner</h2>\n<p>GA-DQN is a tuner that combines advancements in reinforcement\
    \ learning and the genetic algorithm tuner that currently exists in TVM. Two independent\
    \ deep Q-network (DQN)'s are used to suggest where to crossover genes and which\
    \ detail of a gene to mutate.</p>\n<h2>\n<a id=\"user-content-ga-tuner\" class=\"\
    anchor\" href=\"#ga-tuner\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>GA Tuner</h2>\n<p>The GA tuner is code obtained\
    \ from the open source TVM compiler. It is here for convenience and to allow a\
    \ small amount of debug code to be added so that it can be evaluated. This work\
    \ is not my own.</p>\n<h2>\n<a id=\"user-content-evaluation-framework-tools\"\
    \ class=\"anchor\" href=\"#evaluation-framework-tools\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Evaluation framework\
    \ (tools)</h2>\n<p>Provides a series of tools and experiments to quickly test\
    \ various tuning algorithms in AutoTVM. Use tune and benchmark commands on a series\
    \ of pre-trained models to evaluate random, genetic algorithm, extreme gradient\
    \ boost and GA-DQN algorithms. Use the experiment framework to evaluate various\
    \ aspects of GA-DQN, with graphical monitoring.</p>\n<p>A command line driver\
    \ is provided for this framework:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>python driver.py -m=tune -c=../config-example.json\npython driver.py -m=benchmark\
    \ -c=../config-example.json\npython driver.py -m=experiment -c=../config-example.json</pre></div>\n\
    <h2>\n<a id=\"user-content-ga-dqn-pipeline-example\" class=\"anchor\" href=\"\
    #ga-dqn-pipeline-example\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>GA-DQN pipeline example</h2>\n<p><a href=\"\
    /assets/ga-dqn-pipeline-example.png\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img src=\"/assets/ga-dqn-pipeline-example.png\" alt=\"GA-DQN pipeline example\"\
    \ title=\"GA-DQN pipeline example\" style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1620668898.0
libAtoms/docker-quip-base:
  data_format: 2
  description: Deprecated repo. If you think you anything need from this, look at
    quip-docker and scream if it's not there
  filenames:
  - Singularity
  full_name: libAtoms/docker-quip-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-quip-base" class="anchor" href="#quip-base" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>quip-base</h1>

    <p><a href="https://travis-ci.org/libAtoms/docker-quip-base" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a85436fd275dfe3b0867b90fdc6c7637a2d9df2d26fccc8d9f42b2e1f09becba/68747470733a2f2f7472617669732d63692e6f72672f6c696241746f6d732f646f636b65722d717569702d626173652e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/libAtoms/docker-quip-base.svg?branch=master"
    style="max-width:100%;"></a></p>

    <p><a href="https://travis-ci.org/libAtoms/docker-quip-base.svg?branch=master"
    rel="nofollow">https://travis-ci.org/libAtoms/docker-quip-base.svg?branch=master</a></p>

    <p>A Docker image with a scientific stack that is used for building <code>QUIP</code>.

    The image is hosted (and automatically built) on Docker hub as

    <a href="https://hub.docker.com/r/libatomsquip/quip-base/" rel="nofollow">libatomsquip/quip-base</a>.

    You probably don''t want to use this image directly, instead look for

    one of the QUIP images on <a href="https://hub.docker.com/u/libatomsquip/" rel="nofollow">https://hub.docker.com/u/libatomsquip/</a>,

    probably <a href="https://hub.docker.com/r/libatomsquip/quip/" rel="nofollow">libatomsquip/quip</a>.

    or use it in your <code>FROM</code> line. See also:</p>

    <ul>

    <li><a href="https://github.com/libAtoms/QUIP">https://github.com/libAtoms/QUIP</a></li>

    <li><a href="https://github.com/libAtoms/QUIP/tree/public/docker">https://github.com/libAtoms/QUIP/tree/public/docker</a></li>

    <li><a href="https://www.libatoms.org" rel="nofollow">https://www.libatoms.org</a></li>

    </ul>

    <h2>

    <a id="user-content-contents" class="anchor" href="#contents" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contents</h2>

    <p>This image does not contain QUIP, but everything needed to build it

    plus many tools and codes that we find useful.</p>

    <p>Stack contains:</p>

    <ul>

    <li>Python 2.7 image (based on Debian)</li>

    <li>Build tools (gcc, gfortran)</li>

    <li>OpenMP compiled version of OpenBLAS as default maths libraries</li>

    <li>Numpy, SciPy, Matplotlib, ase...</li>

    <li>Julia in <code>/opt/julia</code> with IJulia, PyCall, PyPlot, JuLIP...</li>

    </ul>

    <h2>

    <a id="user-content-data" class="anchor" href="#data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Data</h2>

    <p>The image includes interatomic potentials in <code>/opt/share/potentials</code>

    published on <a href="http://www.libatoms.org/Home/DataRepository" rel="nofollow">http://www.libatoms.org/Home/DataRepository</a>
    which has Gaussian

    Approximation Potentials for:</p>

    <ul>

    <li>Tungsten</li>

    <li>Iron</li>

    <li>Water</li>

    <li>Amorphous carbon</li>

    </ul>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>To make or request changes, open a merge request or issue in the

    <a href="https://github.com/libAtoms/docker-quip-base">GitHub repository</a></p>

    <p>Packages should be added to where the usual istallation commands

    (e.g. <code>apt-get</code>, <code>pip</code>, ...) are in the Dockerfile, with
    the exception

    that Julia pacakes are listed at the beginning of the Julia section.</p>

    <p>Small software package builds can be added at the end of the Dockerfile.

    Larger software applications are included in the

    <a href="https://hub.docker.com/r/libatomsquip/quip-base-software/" rel="nofollow">libatomsquip/quip-base-software</a>

    image in the <code>Software</code> subdirectory.</p>

    '
  stargazers_count: 1
  subscribers_count: 15
  topics: []
  updated_at: 1602585132.0
lkirk/toil-demo:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: lkirk/toil-demo
  latest_release: null
  readme: '<h1>

    <a id="user-content-demo-repo-for-messing-around-with-toil-workflows" class="anchor"
    href="#demo-repo-for-messing-around-with-toil-workflows" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Demo repo for messing
    around with toil workflows</h1>

    <h3>

    <a id="user-content-running-the-workflow" class="anchor" href="#running-the-workflow"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    the workflow</h3>

    <pre><code>singularity run library://lkirk/default/toil-demo:latest

    </code></pre>

    <h3>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h3>

    <pre><code>./scripts/build-sandbox library://lkirk/default/toil-demo:latest toil-sandbox

    </code></pre>

    <h3>

    <a id="user-content-install-for-dev-purposes" class="anchor" href="#install-for-dev-purposes"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    (for dev purposes)</h3>

    <pre><code>./scripts/run-sandbox toil-sandbox pip install -e .

    </code></pre>

    <h3>

    <a id="user-content-play" class="anchor" href="#play" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Play</h3>

    <pre><code>./scripts/run-sandbox toil-sandbox ipython

    </code></pre>

    <h3>

    <a id="user-content-test" class="anchor" href="#test" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Test</h3>

    <pre><code>./scripts/run-sandbox toil-sandbox pytest demo

    </code></pre>

    <h3>

    <a id="user-content-useful-links" class="anchor" href="#useful-links" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Useful links</h3>

    <ul>

    <li><a href="https://toil.readthedocs.io/en/latest/developingWorkflows/developing.html#workflows-with-multiple-jobs"
    rel="nofollow">multiple jobs, child, follow on</a></li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620259690.0
longgangfan/ubuntu2004uwgeo-sig:
  data_format: 2
  description: null
  filenames:
  - Singularity.snowflake
  full_name: longgangfan/ubuntu2004uwgeo-sig
  latest_release: null
  readme: '<h1>

    <a id="user-content-ubuntu2004uwgeo-sig" class="anchor" href="#ubuntu2004uwgeo-sig"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ubuntu2004uwgeo-sig</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621608269.0
lukegun/BIOMEC:
  data_format: 2
  description: Bayesian Inference and Optimisation for the Monash Electrochemical
    Simulator
  filenames:
  - Singularity.def
  full_name: lukegun/BIOMEC
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4983" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-biomec" class="anchor" href="#biomec" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>BIOMEC</h1>

    <p>Bayesian Inference and Optimisation for the Monash electrochemical Simulator
    (MECSim) is the application developed by the

    monash electrochemistry group with the assistance of Assosiate Professor Jie Zhang,
    Emeratious Professor Alan Bond and technical assistance from Gareth Kennedy And
    Martin Robinson.</p>

    <p>It is an automatic plaform for parameterisation that uses mathmatical optimisation
    and Bayesian Inference to calculate parameters involved in the electrochemical
    simulation.

    Built around <a href="http://www.garethkennedy.net/MECSim.html" rel="nofollow">MECSim</a>
    and first applied in the PAPER. BIOMEC allows for automated parameterisation of
    DC and FTAC voltammetery, allowing highly dimensional fits of the posteriour distrabution.</p>

    <p>BIOMEC uses <a href="https://github.com/pints-team/pints">PINTS</a> for univariant
    Bayesian inference.</p>

    <p>For information of current uses see the original <a href="https://chemistry-europe.onlinelibrary.wiley.com/doi/abs/10.1002/celc.202100391"
    rel="nofollow">BIOMEC paper</a>, the <a href="https://doi.org/10.1002/celc.201700678"
    rel="nofollow">original Bayesian inference paper</a> for AC voltammetry or our
    most recent <a href="https://doi.org/10.1039/D0CC07549C" rel="nofollow">featured
    article</a>.</p>

    <h2>

    <a id="user-content-installing-biomec-image" class="anchor" href="#installing-biomec-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installing
    BIOMEC image</h2>

    <p>The code is run in a singularity container which works for Ubuntu/UNIX and
    MAC (untested) OS systems.

    Singularity will need to be installed to use the image. Where the guide is seen
    in the following <a href="https://sylabs.io/guides/3.6/user-guide/quick_start.html"
    rel="nofollow">website</a> or downloaded from connected singularity hub.</p>

    <p>Once singularity has been installed, download the BIOMEC file and run the code
    to create the BIOMEC container (which should be around 580MB).</p>

    <pre><code>$ sudo singularity build BIOMEC.simg Singularity.def

    </code></pre>

    <p>Once the image is built the imput file (input.txt) can be passed to the image
    by using the following command.</p>

    <pre><code>$ ./BIOMEC.simg input.txt

    </code></pre>

    <p>This will generate and ouput file with plots and results once completed.</p>

    <h2>

    <a id="user-content-generating-input-files" class="anchor" href="#generating-input-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Generating
    input files</h2>

    <p>inputwritter.py can guide users unfamilaur with generating input files to create
    an input file for the BIOMEC container, this program is contained in the BIOMEC_inputwritter.

    Simply run the file using the following command and follow the prompts and an
    input file will be generated.</p>

    <pre><code>$ python3 inputwritter.py

    </code></pre>

    <p>The output of this file will then be of the form &lt;input.txt&gt; though other
    names will work.

    It is important that a copy of the MECSim Master.inp file is present in the folder
    you run inputwritter.py as the MECSim input file is required for BIOMEC to run.</p>

    <p>Once comfortable with writting the input file it is recommended to use any
    text editor.</p>

    <h2>

    <a id="user-content-running-biomec" class="anchor" href="#running-biomec" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running BIOMEC</h2>

    <p>PDF tutorial or youtube videos to come.</p>

    <h2>

    <a id="user-content-supporting-code" class="anchor" href="#supporting-code" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Supporting Code</h2>

    <ul>

    <li>BIOMEC_inputwritter: Basic terminal/ .exe code for guiding uses in writting
    the input files for BIOMEC</li>

    <li>MCMC PLOTTER: code to plot the mcmc output chains from the Iter_log.txt to
    images</li>

    </ul>

    <h2>

    <a id="user-content-known-issues" class="anchor" href="#known-issues" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Known Issues</h2>

    <ul>

    <li>Custom waveforms have not been tested and Estart and End cannot equal zero.</li>

    <li>Number of data poins in experimental data must be a multiple of two.</li>

    </ul>

    <h2>

    <a id="user-content-citing" class="anchor" href="#citing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Citing</h2>

    <p>Please, cite the original <a href="https://chemistry-europe.onlinelibrary.wiley.com/doi/abs/10.1002/celc.202100391"
    rel="nofollow">BIOMEC paper</a> if you have used this package in a publication.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>BIOMEC analysis/python code is open source under the GPL-3.0 License, with
    MECSim developed by Gareth Kennedy and contaned in the mecsim.cpython-37m-x86_64-linux-gnu.so
    shared object is under the Creative Commons Attribution-NonCommercial-ShareAlike
    4.0 International License.</p>

    <h2>

    <a id="user-content-get-in-touch" class="anchor" href="#get-in-touch" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Get in touch</h2>

    <p>For Questions/Bugs Email me at <a href="mailto:luke.gundry1@monash.edu">luke.gundry1@monash.edu</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics:
  - baysian-inference
  - optimization
  - voltammetry
  - electrochemistry
  updated_at: 1621949924.0
lumpiluk/pogona-container:
  data_format: 2
  description: Singularity container definition for the upcoming Pogona simulator
  filenames:
  - Singularity
  full_name: lumpiluk/pogona-container
  latest_release: null
  readme: '<h1>

    <a id="user-content-pogona-container" class="anchor" href="#pogona-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>pogona-container</h1>

    <p>Singularity container definition for the upcoming Pogona simulator</p>

    <p><a href="https://singularity-hub.org/collections/4128" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>To use a pre-built image, first pull it from Singularity Hub:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull shub://lumpiluk/pogona-container</pre></div>

    <p>Afterwards, you can open a subshell using the image by running</p>

    <div class="highlight highlight-source-shell"><pre>./singularity_shell.sh</pre></div>

    <p>From there, all prerequisites for running OpenFOAM simulations or setting up
    a Python Pipenv/Virtualenv for the Pogona simulator should be available.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619194569.0
mahwisharif/singularity-cinnamon:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: mahwisharif/singularity-cinnamon
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-cinnamon" class="anchor" href="#singularity-cinnamon"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-cinnamon</h1>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1619586956.0
manuel-munoz-aguirre/singularity-pytorch-gpu:
  data_format: 2
  description: Singularity image for a deep learning (pytorch) environment + GPU support
  filenames:
  - Singularity.1.0.0
  full_name: manuel-munoz-aguirre/singularity-pytorch-gpu
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-pytorch-gpu" class="anchor" href="#singularity-pytorch-gpu"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-pytorch-gpu</h1>

    <p><a href="https://singularity-hub.org/collections/4969" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for a deep learning (pytorch) environment + GPU support (cuda-10.2).
    Contains libraries to perform common ML tasks. <code>Openslide</code> is included
    to manipulate whole-slide histology images, <code>imagemagick</code> for general
    image manipulation. <code>JupyterLab</code> and <code>code-server</code> (VS Code)
    are also included in the image. This image has been tested in an HPC (SGE) with
    distributed pytorch applications.</p>

    <h2>

    <a id="user-content-installing-singularity" class="anchor" href="#installing-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installing
    singularity</h2>

    <p>To install singularity, see the <a href="https://sylabs.io/guides/3.6/admin-guide/installation.html#installation-on-linux"
    rel="nofollow">official docs</a>.</p>

    <h2>

    <a id="user-content-buildingdownloading-the-image" class="anchor" href="#buildingdownloading-the-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building/downloading
    the image</h2>

    <p>To build an image called <code>torchenv.sif</code> based on the definition
    file <code>Singularity.1.0.0</code>, an NVIDIA GPU and <code>cuda-10.2</code>
    drivers must be available on the host system. Clone this repository, move into
    it and run the singularity build command.</p>

    <pre><code>git clone https://github.com/manuel-munoz-aguirre/singularity-pytorch-gpu.git
    &amp;&amp; \

    cd singularity-pytorch-gpu &amp;&amp; \

    sudo singularity build torchenv.sif Singularity.1.0.0

    </code></pre>

    <p>Otherwise, the image can be pulled directly from singularity hub:</p>

    <pre><code>singularity pull torchenv.sif shub://manuel-munoz-aguirre/singularity-pytorch-gpu:1.0.0

    </code></pre>

    <h2>

    <a id="user-content-using-the-container" class="anchor" href="#using-the-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Using
    the container</h2>

    <p>To spawn an interactive shell within the container, use the command below.
    The <code>--nv</code> flag setups the container to use NVIDIA GPUs (read more
    <a href="https://sylabs.io/guides/3.6/user-guide/gpu.html" rel="nofollow">here</a>).</p>

    <pre><code>singularity shell --nv torchenv.sif

    </code></pre>

    <p>To run a script (for example, <code>script.py</code>) using the container without
    starting an interactive shell:</p>

    <pre><code>singularity exec --nv torchenv.sif python3 script.py

    </code></pre>

    <p>The container can also be launched and used on a system without a GPU, but
    upon startup it will display a warning about missing NVIDIA binaries on the host.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - pytorch
  - deep-learning
  - machine-learning
  - environment
  updated_at: 1612808313.0
markmenezes11/COMPM091:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: markmenezes11/COMPM091
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-on-the-importance-of-the-choice-of-downstream-models-for-evaluating-sentence-representations\"\
    \ class=\"anchor\" href=\"#on-the-importance-of-the-choice-of-downstream-models-for-evaluating-sentence-representations\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>On the Importance of the Choice of Downstream Models for Evaluating\
    \ Sentence Representations</h1>\n<p>This repository contains the code created/used\
    \ for my MEng Computer Science Undergraduate Final Year Individual Projct (COMPM091)\
    \ at University College London.</p>\n<p>My project primarily involved looking\
    \ at sentence representations, specifically InferSent (Conneau et al., 2017) and\
    \ CoVe (McCann et al., 2017) and using SentEval (Conneau et al., 2017) and the\
    \ Biattentive Classification Network (McCann et al., 2017) to evaluate them.</p>\n\
    <p>The code for InferSent, SentEval and CoVe were cloned from the following repositories:</p>\n\
    <ul>\n<li>InferSent: <a href=\"https://github.com/facebookresearch/InferSent\"\
    >https://github.com/facebookresearch/InferSent</a>\n</li>\n<li>SentEval: <a href=\"\
    https://github.com/facebookresearch/SentEval\">https://github.com/facebookresearch/SentEval</a>\n\
    </li>\n<li>CoVe: <a href=\"https://github.com/salesforce/cove\">https://github.com/salesforce/cove</a>\
    \ and <a href=\"https://github.com/rgsachin/CoVe\">https://github.com/rgsachin/CoVe</a>\n\
    </li>\n</ul>\n<p>In the <code>libs.zip</code> file, you will find snapshots of\
    \ these repositories - the versions of them that were used in my project, at this\
    \ time of writing. If you have problems with any newer versions of these libraries,\
    \ it is therefore recommended that you use the versions contained in <code>libs.zip</code>.</p>\n\
    <p><em>Note: A CUDA-enabled GPU is required, and at least 16GB RAM is recommended\
    \ to run most of the scripts. A small minority of scripts will need more than\
    \ this, hence why they were run on UCL's HPC cluster.</em></p>\n<h2>\n<a id=\"\
    user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Install</h2>\n\
    <p>There are two ways to install. You can use the provided Singularity container,\
    \ which already contains all of the required software, or you can install the\
    \ requirements locally. It is highly recommended that you use the Singularity\
    \ container.</p>\n<p>For the Singularity method, use steps 1, 2a and 3. For local\
    \ install, use steps 1, 2b and 3.</p>\n<p>For common problems, see the <code>Troubleshooting</code>\
    \ section at the bottom of this page.</p>\n<h3>\n<a id=\"user-content-step-1-clone-infersent-and-senteval\"\
    \ class=\"anchor\" href=\"#step-1-clone-infersent-and-senteval\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step\
    \ 1) Clone InferSent and SentEval</h3>\n<p>You can either clone them from here:</p>\n\
    <ul>\n<li>InferSent: <a href=\"https://github.com/facebookresearch/InferSent\"\
    >https://github.com/facebookresearch/InferSent</a>\n</li>\n<li>SentEval: <a href=\"\
    https://github.com/facebookresearch/SentEval\">https://github.com/facebookresearch/SentEval</a>\n\
    </li>\n</ul>\n<p>Or, in case any changes are made to these repositories that stop\
    \ this code from working, snapshots of these repositories used at this time of\
    \ writing have been provided in the <code>libs.zip</code> file, but obviously\
    \ they may not be the latest version. A modified version of SentEval which uses\
    \ far less memory can be found in the <code>SentEval-modified</code> folder, but\
    \ only use this if you really have to. Read the README in that folder first. It\
    \ works identically to the version found in <code>libs.zip</code> but contains\
    \ various performance tweaks. So, instead of cloning directly from the InferSent\
    \ and SentEval repositories, you can use the versions provided in <code>libs.zip</code>\
    \ or the version of SentEval provided in the <code>SentEval-modified</code> folder.</p>\n\
    <h3>\n<a id=\"user-content-step-2a-singularity-container-recommended\" class=\"\
    anchor\" href=\"#step-2a-singularity-container-recommended\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 2a)\
    \ Singularity Container (Recommended)</h3>\n<p>Pull and run the latest Singularity\
    \ container for this repository from Singularity Hub.</p>\n<pre><code>singularity\
    \ run --nv shub://markmenezes11/COMPM091\n</code></pre>\n<p><code>--nv</code>\
    \ runs it in nvidia mode. See <code>singularity run -h</code> for more options.\
    \ You can also use the same command above to pull any later versions of the container\
    \ in future, making sure to delete the old <code>.simg</code> file first.</p>\n\
    <p>The container uses nvidia/cuda:8.0-cudnn6-runtime-ubuntu16.04. Make sure your\
    \ host machine's CUDA and cuDNN versions match this*. If they don't, you might\
    \ need to tweak the <code>Singularity</code> file and re-build the image locally.</p>\n\
    <h3>\n<a id=\"user-content-step-2b-local-install\" class=\"anchor\" href=\"#step-2b-local-install\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Step 2b) Local Install</h3>\n<p>The <code>Singularity</code> file\
    \ contains a list of commands in the <code>%post</code> section to install the\
    \ required libraries. You will need to run each of these commands. Python 2.7\
    \ was used for the majority of this work, and so you should only need to do it\
    \ for Python 2.7 (i.e. ignore the <code>pip3</code> commands) and use Python 2.7\
    \ when running the scripts later. You will also need CUDA and cuDNN to make use\
    \ of your GPU. Depending on where/how you are running this, you might need to\
    \ run the commands as <code>sudo</code> or <code>--user</code>. You will also\
    \ need to find/use a different URL for PyTorch and TensorFlow if you are not using\
    \ Linux and CUDA 8.</p>\n<h3>\n<a id=\"user-content-step-3-download-datasets\"\
    \ class=\"anchor\" href=\"#step-3-download-datasets\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Step 3) Download\
    \ Datasets</h3>\n<p>Run all of the required scripts** and curl commands** to download\
    \ the required datasets. These scripts can be found in the repositories you just\
    \ cloned (see Step 1), and instructions can be found in their READMEs.</p>\n<h2>\n\
    <a id=\"user-content-note-for-users-of-ucls-hpc-cluster\" class=\"anchor\" href=\"\
    #note-for-users-of-ucls-hpc-cluster\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Note for Users of UCL's HPC Cluster</h2>\n\
    <p>If using UCL's HPC cluster, it is recommended to follow the Singularity instructions\
    \ above. Additionally, assuming it hasn't been moved, some outputs of running\
    \ some of my code, as well as the Singularity image and the InferSent and SentEval\
    \ libraries with all of the requirements already downloaded, should be in <code>/cluster/project2/ishi_storage_1/mmenezes</code>.\
    \ If so, you can save time by just using the following Singularity command which\
    \ will bind my folder to <code>/mnt/mmenezes</code> in the Singularity image for\
    \ you, with all of the requirements pre-installed and pre-downloaded:</p>\n<pre><code>singularity\
    \ run --nv --bind /cluster/project2/ishi_storage_1:/mnt /cluster/project2/ishi_storage_1/mmenezes/markmenezes11-COMPM091-master.simg\n\
    </code></pre>\n<p>When following the \"Run\" instructions later, you can then\
    \ use the above command instead of <code>singularity run --nv shub://markmenezes11/COMPM091</code>.</p>\n\
    <h2>\n<a id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run</h2>\n\
    <p>For \"Run\" instructions, see the relevant README(s) in the subfolder(s) within\
    \ this repository. They all require all of the above installation instructions\
    \ to be competed first. The folders containing the runnable scripts are as follows:</p>\n\
    <ul>\n<li>The <code>CoVe-BCN</code> folder contains my replication of the Biattentive\
    \ Classification Network model by McCann et al. (2017). It also includes a script\
    \ for running a parameter sweep on this model, and evaluaitng both InferSent and\
    \ CoVe on different transfer tasks.</li>\n<li>The <code>CoVe-ported</code> folder\
    \ contains a script (and some test scripts) for porting the Python 3 version of\
    \ the Keras port of CoVe into a Python 2 compatible version. It is unlikely that\
    \ you will need to run any of these scripts.</li>\n<li>The <code>InferSent-sweep</code>\
    \ folder contains code for performing a parameter sweep on InferSent, including\
    \ training the InferSent model based on many different parameters, and evaluating\
    \ the model using SentEval. The sweep script can either use qsub job submissions\
    \ for running jobs in parallel or it can be run locally.</li>\n<li>The <code>SentEval-evals</code>\
    \ folder contains scripts and test results from evaluating InferSent (Conneau\
    \ et al. (2017)), CoVe (McCann et al. (2017)) and GloVe (Pennington et al. (2014))\
    \ sentence representations using SentEval for evaluation on various transfer tasks.</li>\n\
    <li>The <code>Utils</code> folder contains helper scripta, for example for displaying\
    \ RAM usage, for giving summaries based on lots of results files, and for manipulating\
    \ datasets.</li>\n</ul>\n<p>Also have a look at the <code>qsub_helper</code> scripts\
    \ in these folders if you want to use <code>qsub</code> to submit HPC jobs. They\
    \ can mostly be run using <code>qsub &lt;qsub-params&gt; &lt;qsub_helper-script&gt;\
    \ &lt;command&gt;</code>. The  bit can use singularity by running the <code>singularity\
    \ exec</code> command instead of <code>singularity run</code>. For example:</p>\n\
    <pre><code>qsub -cwd -o $PWD/se_log_STS12.txt -e $PWD/se_error_STS12.txt ../qsub_helper.sh\
    \ singularity exec --nv --bind /cluster/project2/ishi_storage_1:/mnt /cluster/project2/ishi_storage_1/mmenezes/markmenezes11-COMPM091-master.simg\
    \ python eval.py --transfertask STS12\n</code></pre>\n<h2>\n<a id=\"user-content-troubleshooting--common-problems\"\
    \ class=\"anchor\" href=\"#troubleshooting--common-problems\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Troubleshooting\
    \ / Common Problems</h2>\n<p>* If your CUDA and cuDNN versions do not match, you\
    \ might have to create your own Singularity file, changing the <code>from:</code>\
    \ line and also the line that installs PyTorch. You can then host it in a similar\
    \ way, or build it in another way, then pull/run it.</p>\n<p>**  Keep an eye on\
    \ the InferSent/SentEval download scripts because curl sometimes gives an SSL\
    \ error on UCL's cluster machines. If you can't update curl on your machine, you\
    \ could either use the Singularity image (which has a working version of curl)\
    \ or download the file(s) on a different machine and use <code>scp</code> to send\
    \ them across.</p>\n<p>*** You may have problems when running things later, where\
    \ Singularity looks at installations in your <code>/home</code> folder and these\
    \ conflict with things that Singularity has installed within the container. You\
    \ might have to remove some <code>/home</code> installs to get it to work.</p>\n\
    <p>**** Newer PyTorch versions have a few differences in how you call certain\
    \ functions - if you have some weird PyTorch errors, this is likely the reason.\
    \ This code has been tested with PyTorch version 0.2.0_3. At this time of writing,\
    \ the examples in the InferSent, SentEval and CoVe repositories are all out of\
    \ date and need minor tweaks to get them to work with the latest PyTorch version.\
    \ See their GitHub issues sections for more details.</p>\n<h2>\n<a id=\"user-content-references\"\
    \ class=\"anchor\" href=\"#references\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>References</h2>\n<ul>\n<li><a\
    \ href=\"https://arxiv.org/pdf/1705.02364.pdf\" rel=\"nofollow\">Conneau, Alexis,\
    \ Kiela, Douwe, Schwenk, Holger, Barrault, Lo\xEFc, and Bordes, Antoine. Supervised\
    \ learning of universal sentence representations from natural language inference\
    \ data. In Proceedings of the 2017 Conference on Empirical Methods in Natural\
    \ Language Processing, pp. 670\u2013680. Association for Computational Linguistics,\
    \ 2017.</a></li>\n<li><a href=\"https://arxiv.org/pdf/1708.00107.pdf\" rel=\"\
    nofollow\">McCann, Bryan, Bradbury, James, Xiong, Caiming, and Socher, Richard.\
    \ Learned in translation: Contextualized word vectors. In Advances in Neural Information\
    \ Processing Systems 30, pp. 6297\u20136308. Curran Associates, Inc., 2017.</a></li>\n\
    <li><a href=\"https://nlp.stanford.edu/pubs/glove.pdf\" rel=\"nofollow\">Pennington,\
    \ Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: Global vectors\
    \ for word representation. In Empirical Methods in Natural Language Processing\
    \ (EMNLP), pp. 1532\u20131543, 2014.</a></li>\n<li>Martin Abadi, Ashish Agarwal,\
    \ Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy\
    \ Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\
    \ Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz\
    \ Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Mike Schuster, Rajat Monga,\
    \ Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya\
    \ Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda\
    \ Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,\
    \ and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous\
    \ systems, 2015. Software available from tensorflow.org.</li>\n<li>Pytorch. [online].\
    \ Available at: <a href=\"https://github.com/pytorch/pytorch\">https://github.com/pytorch/pytorch</a>.</li>\n\
    <li>Chollet, Francois et al. Keras. [online]. Available at: <a href=\"https://github.com/keras-team/keras\"\
    >https://github.com/keras-team/keras</a>, 2015.</li>\n<li>Richard Socher, Alex\
    \ Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng and Christopher\
    \ Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment\
    \ Treebank. In Conference on Empirical Methods in Natural Language Processing\
    \ (EMNLP 2013).</li>\n<li>Xin Li, Dan Roth, Learning Question Classifiers. COLING'02,\
    \ Aug., 2002.</li>\n<li>E. M. Voorhees and D. M. Tice. The TREC-8 question answering\
    \ track evaluation. In TREC, volume 1999, page 82, 1999.</li>\n</ul>\n<p>Libraries\
    \ and algorithms are referenced in the files they are used.</p>\n"
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1539289538.0
martinobertoni/BBBpredictor:
  data_format: 2
  description: "Predict blood\u2013brain barrier (BBB) permeability of a compound"
  filenames:
  - singularity/Singularity
  full_name: martinobertoni/BBBpredictor
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/5286" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-blood-brain-barrier-bbb-predictor" class="anchor" href="#blood-brain-barrier-bbb-predictor"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Blood-brain
    barrier (BBB) predictor:</h1>

    <p>Classifier that will predict if a chemical coumpound will pass the blood-brain
    barrier.

    SBNB lab (IRB Barcelona) - Nov 2020.</p>

    <p>USAGE: BBBpredictor myfile.tsv [SMILES or INCHI]</p>

    <p>Where myfile.tsv is an input text file that contains two columns separated
    by TABS.

    COLUMN 1: compound unique id (ex:1 or a molecule name without tab characters inside).

    COLUMN 2: compound SMILES string.</p>

    <p>NOTE: The default format is SMILES and can be set to InChI if the second argument
    INCHI is provided.

    NOTE: Lines starting with ''#'' as well as empty lines are ignored.</p>

    <p>OUTPUT: tsv file containing the prediction for each compound.</p>

    <p>PREDICTION LEGEND: the molecule pass the BBB?</p>

    <p>0: no</p>

    <p>1: yes</p>

    <p>-1: the molecular signature could not be calculated for this coumpound</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1616100804.0
mattocci27/r-containers:
  data_format: 2
  description: docker and singularity containers for R
  filenames:
  - images/rmd-light_4.1.0/Singularity.def
  - images/rstan_4.1.0/Singularity.def
  - images/myenv_4.1.0/Singularity.def
  full_name: mattocci27/r-containers
  latest_release: null
  readme: '<h1>

    <a id="user-content-docker-and-singularity-images-for-r" class="anchor" href="#docker-and-singularity-images-for-r"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Docker
    and singularity images for R</h1>

    <p><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-images" class="anchor" href="#images" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Images</h2>

    <table>

    <thead>

    <tr>

    <th>docker</th>

    <th>singularity</th>

    <th>description</th>

    <th>r-ver</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><a href="https://hub.docker.com/repository/docker/mattocci/rstan" rel="nofollow">rstan</a></td>

    <td><a href="https://cloud.sylabs.io/library/mattocci27/default/rstan" rel="nofollow">rstan</a></td>

    <td>adds rstan on <a href="https://hub.docker.com/r/rocker/geospatial" rel="nofollow">geospatial</a>

    </td>

    <td>3.6.3, 4.0.5</td>

    </tr>

    <tr>

    <td><a href="https://hub.docker.com/repository/docker/mattocci/myenv" rel="nofollow">myenv</a></td>

    <td><a href="https://cloud.sylabs.io/library/mattocci27/default/myenv" rel="nofollow">myenv</a></td>

    <td>adds a bunch of packages on ''rstan''</td>

    <td>3.6.3, 4.0.5</td>

    </tr>

    <tr>

    <td><a href="https://hub.docker.com/repository/docker/mattocci/rmd-light" rel="nofollow">rmd-light</a></td>

    <td>-</td>

    <td>R markdown + TinyTex + pandoc-crossref without Rstudio and Tidyverse</td>

    <td>4.0.5</td>

    </tr>

    </tbody>

    </table>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1621973368.0
maxemil/PhyloMagnet:
  data_format: 2
  description: screening metagenomes for arbitrary lineages, using gene-centric assembly
    methods and phylogenetics
  filenames:
  - Singularity
  full_name: maxemil/PhyloMagnet
  latest_release: null
  readme: "<p><a href=\"http://phylomagnet.readthedocs.io/en/latest/\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/29e96fe88e81eead9542ac2fcef38f609b35c29fcecfdf5ed2c5517a534d9b20/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7068796c6f6d61676e65742f62616467652f3f76657273696f6e3d6c6174657374\"\
    \ alt=\"Docs Status\" data-canonical-src=\"https://readthedocs.org/projects/phylomagnet/badge/?version=latest\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://travis-ci.org/maxemil/PhyloMagnet\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/4fe367ba9e712aa4068f6e691da8264f15ce96872f20bdfed1656d9b6eba78b7/68747470733a2f2f7472617669732d63692e6f72672f6d6178656d696c2f5068796c6f4d61676e65742e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/maxemil/PhyloMagnet.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://www.singularity-hub.org/collections/978\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/9b0b8670bab3cab652cf5c31fdae614cf89b2ceb2e013cd2d7dd570e9f8530f2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d73696e67756c61726974792d2d6875622d626c75652e737667\"\
    \ alt=\"Hosted\" data-canonical-src=\"https://img.shields.io/badge/hosted-singularity--hub-blue.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-phylomagnet\"\
    \ class=\"anchor\" href=\"#phylomagnet\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>PhyloMagnet</h1>\n<h2>\n<a id=\"\
    user-content-pipeline-for-screening-metagenomes-looking-for-arbitrary-lineages-using-gene-centric-assembly-methods-and-phylogenetics\"\
    \ class=\"anchor\" href=\"#pipeline-for-screening-metagenomes-looking-for-arbitrary-lineages-using-gene-centric-assembly-methods-and-phylogenetics\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pipeline for screening metagenomes, looking for arbitrary lineages,\
    \ using gene-centric assembly methods and phylogenetics</h2>\n<h2>\n<a id=\"user-content-abstract\"\
    \ class=\"anchor\" href=\"#abstract\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Abstract</h2>\n<p>Motivation:\
    \ Metagenomic and metatranscriptomic sequencing analyses have become increasingly\
    \ popular tools for producing massive amounts of short-read data, often used for\
    \ the reconstruction of draft genomes or the detection of (active) genes in microbial\
    \ communities. Unfortunately, sequence assemblies of such datasets generally remain\
    \ a computationally challenging task. Frequently, researchers are only interested\
    \ in a specific group of organisms or genes; yet, the assembly of multiple datasets\
    \ only to identify candidate sequences for a specific question is sometimes prohibitively\
    \ slow, forcing researchers to select a subset of available datasets to address\
    \ their question. Here we present PhyloMagnet, a workflow to screen meta-omics\
    \ datasets for taxa and genes of interest using gene-centric assembly and phylogenetic\
    \ placement of sequences.\nResults: Using PhyloMagnet, we could identify up to\
    \ 87% of the genera in an in vitro mock community with variable abundances, while\
    \ the false positive predictions per single gene tree ranged from 0% to 23%. When\
    \ applied to a group of metagenomes for which a set of MAGs have been published,\
    \ we could detect the majority of the taxonomic labels that the MAGs had been\
    \ annotated with. In a metatranscriptomic setting the phylogenetic placement of\
    \ assembled contigs corresponds to that of transcripts obtained from transcriptome\
    \ assembly. See <a href=\"https://github.com/maxemil/PhyloMagnet-benchmarks\"\
    >https://github.com/maxemil/PhyloMagnet-benchmarks</a> for benchmark experiments.</p>\n\
    <h2>\n<a id=\"user-content-quick-installation--usage\" class=\"anchor\" href=\"\
    #quick-installation--usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Quick installation &amp; usage</h2>\n<p>For\
    \ detailed documentation, please visit <a href=\"http://phylomagnet.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">http://phylomagnet.readthedocs.io/en/latest/</a></p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> download the image with all tools installed using singularity\
    \ 3x</span>\nsingularity pull --name PhyloMagnet.sif shub://maxemil/PhyloMagnet:latest\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> get versions of tools used\
    \ in the pipeline:</span>\nsingularity <span class=\"pl-c1\">exec</span> PhyloMagnet.sif\
    \ conda list -n PhyloMagnet-<span class=\"pl-k\">&lt;</span>version<span class=\"\
    pl-k\">&gt;</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> execute\
    \ the test pipeline with nextflow</span>\nnextflow run main.nf \\\n          -with-singularity\
    \ PhyloMagnet.sif \\\n          --is_runs <span class=\"pl-c1\">true</span> \\\
    \n          --fastq <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test/*rpoB.fastq.gz<span\
    \ class=\"pl-pds\">\"</span></span> \\\n          --reference_packages <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span>test/rpkgs/*<span class=\"pl-pds\">\"</span></span>\
    \ \\\n          --lineage <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>order<span\
    \ class=\"pl-pds\">\"</span></span> \\\n          --megan_eggnog_map eggnog.map\
    \ \\\n          --cpus 2 \\\n          --is_runs <span class=\"pl-c1\">true</span>\
    \ \\\n          --queries_dir test/queries \\\n          --reference_dir test/references\
    \ \\\n          --phylo_method <span class=\"pl-s\"><span class=\"pl-pds\">'</span>fasttree<span\
    \ class=\"pl-pds\">'</span></span> \\\n          --align_method <span class=\"\
    pl-s\"><span class=\"pl-pds\">'</span>mafft-fftnsi<span class=\"pl-pds\">'</span></span>\
    \ \\\n          -w test/work -resume</pre></div>\n<h2>\n<a id=\"user-content-citing-phylomagnet\"\
    \ class=\"anchor\" href=\"#citing-phylomagnet\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Citing PhyloMagnet</h2>\n<p>PhyloMagnet\
    \ is published in Bioinformatics:\nMax E Sch\xF6n, Laura Eme, Thijs J G Ettema,\
    \ PhyloMagnet: fast and accurate screening of short-read meta-omics data using\
    \ gene-centric phylogenetics, Bioinformatics, btz799, <a href=\"https://doi.org/10.1093/bioinformatics/btz799\"\
    \ rel=\"nofollow\">https://doi.org/10.1093/bioinformatics/btz799</a></p>\n<p>Please\
    \ make sure to also cite all tools that are used in the pipeline if you use it\
    \ for your research! Visit <a href=\"http://phylomagnet.readthedocs.io/en/latest/\"\
    \ rel=\"nofollow\">http://phylomagnet.readthedocs.io/en/latest/</a> or see the\
    \ startup message for details.</p>\n"
  stargazers_count: 4
  subscribers_count: 1
  topics:
  - metagenomics
  - next-generation-sequencing
  - phylogenetics
  - nextflow
  - singularity-container
  - evolution
  updated_at: 1619042816.0
maxemil/originations-placement:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: maxemil/originations-placement
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-recipe-for-craig\" class=\"anchor\"\
    \ href=\"#singularity-recipe-for-craig\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity Recipe for CraiG</h1>\n\
    <h2>\n<a id=\"user-content-craig-source\" class=\"anchor\" href=\"#craig-source\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>CraiG Source</h2>\n<p><a href=\"https://github.com/axl-bernal/CraiG\"\
    >https://github.com/axl-bernal/CraiG</a></p>\n<h2>\n<a id=\"user-content-build-image\"\
    \ class=\"anchor\" href=\"#build-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Build image</h2>\n<p>The <a href=\"\
    https://github.com/EuPathDB/vagrant-rpmbuild\"><code>vagrant-rpmbuild</code></a>\n\
    virtual machine provisions Singularity suitable for building.</p>\n<p>Typically\
    \ you should remove any existing image so you get clean build.</p>\n<pre><code>$\
    \ rm -f craig.simg\n$ sudo singularity build craig.simg Singularity \n</code></pre>\n\
    <h2></h2>\n<p>$ singularity pull --name craig.simg shub://mheiges/singularity-craig</p>\n\
    <h2>\n<a id=\"user-content-exec\" class=\"anchor\" href=\"#exec\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Exec</h2>\n\
    <pre><code>$ singularity exec --bind /eupath craig.simg ls  /eupath\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1603131331.0
maxemil/singularity-container:
  data_format: 2
  description: null
  filenames:
  - Singularity.discordance
  - Singularity.malt
  - Singularity.gappa
  - Singularity.rp15
  - Singularity.PhyloBayesMPI
  - Singularity.PhyloBayes
  - Singularity.megan6-ce
  - Singularity.checkM
  - Singularity.iqtree.1.6.1
  - Singularity.papara
  - Singularity.megan5
  - Singularity.epa-ng
  - Singularity.ale
  - Singularity.hifix
  - COME2018/Singularity.fsa
  - COME2018/Singularity.muscle
  - COME2018/Singularity.beast2
  - COME2018/Singularity.phyml
  - COME2018/Singularity.gnuplot
  - COME2018/Singularity.prank
  - COME2018/Singularity.mummer
  - COME2018/Singularity.fasttree
  - COME2018/Singularity.clustalx
  - COME2018/Singularity.bpp
  - COME2018/Singularity.seaview
  - COME2018/Singularity.codonphyml
  - COME2018/Singularity.mcl
  - COME2018/Singularity.probcons
  - COME2018/Singularity.paml
  - COME2018/Singularity.jmodeltest2
  - COME2018/Singularity.iqtree.1.6.3
  - COME2018/Singularity.blast
  - COME2018/Singularity.mafft
  - COME2018/Singularity.raxml-ng
  - COME2018/Singularity.swipe
  - COME2018/Singularity.clustalw
  - COME2018/Singularity.bali-phy
  - COME2018/Singularity.amap-align
  - COME2018/Singularity.modeltest-ng
  - COME2018/Singularity.standard-raxml
  full_name: maxemil/singularity-container
  latest_release: null
  readme: '<h2>

    <a id="user-content-singularity-container" class="anchor" href="#singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Container</h2>

    <p>To use the containers in this repository, install the latest Singularity app
    (taken from <a href="http://singularity.lbl.gov/install-linux" rel="nofollow">http://singularity.lbl.gov/install-linux</a>):</p>

    <div class="highlight highlight-source-shell"><pre>VERSION=2.4.5

    wget https://github.com/singularityware/singularity/releases/download/<span class="pl-smi">$VERSION</span>/singularity-<span
    class="pl-smi">$VERSION</span>.tar.gz

    tar xvf singularity-<span class="pl-smi">$VERSION</span>.tar.gz

    <span class="pl-c1">cd</span> singularity-<span class="pl-smi">$VERSION</span>

    ./configure --prefix=/usr/local

    make

    sudo make install</pre></div>

    <p>Then you can build and use the containers using the following commands:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build <span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span>.simg Singularity.<span
    class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span>

    singularity <span class="pl-c1">exec</span> -b <span class="pl-smi">$PWD</span>
    <span class="pl-k">&lt;</span>name<span class="pl-k">&gt;</span>.simg <span class="pl-k">&lt;</span>command<span
    class="pl-k">&gt;</span> <span class="pl-k">&lt;</span>arguments<span class="pl-k">&gt;</span>

    <span class="pl-c"><span class="pl-c">#</span> or if only a single command is
    available:</span>

    singularity run -b <span class="pl-smi">$PWD</span> <span class="pl-k">&lt;</span>name<span
    class="pl-k">&gt;</span>.simg <span class="pl-k">&lt;</span>arguments<span class="pl-k">&gt;</span></pre></div>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1613209350.0
mcw-rcc/pacbioapps:
  data_format: 2
  description: Singularity container for PacBio applications.
  filenames:
  - Singularity
  full_name: mcw-rcc/pacbioapps
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-pacbio-apps-container\" class=\"anchor\" href=\"\
    #pacbio-apps-container\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>PacBio Apps Container</h1>\n<p><a href=\"https://singularity-hub.org/collections/1263\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>A container for installing and running\
    \ PacBio applications using Conda.</p>\n<p>To use the container:</p>\n<pre><code>$\
    \ module load pacbioapps/latest\n</code></pre>\n<p>Use the included wrapper script\
    \ <code>runpbapps</code> to run commands (e.g., blasr):</p>\n<pre><code>$ runpbapps\
    \ blasr --version\nblasr\t5.3.2\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity-container
  updated_at: 1586494536.0
mdellalma/cellrangerv6.0.0:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: mdellalma/cellrangerv6.0.0
  latest_release: null
  readme: '<h1>

    <a id="user-content-cellranger--v600" class="anchor" href="#cellranger--v600"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>CellRanger  (v6.0.0)</h1>

    <p>Container CellRanger 10X v6.0.0</p>

    <p>The download files for the last vesion of CellRanger need to be updated but
    the recipe stays the same.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1617230630.0
mfschmidt/PyGEST:
  data_format: 2
  description: Python Gene Expression Spatial Toolkit
  filenames:
  - singularity/Singularity.stretch
  full_name: mfschmidt/PyGEST
  latest_release: null
  readme: "<p><a href=\"https://singularity-hub.org/collections/711\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-remote-sensing-environments-in-scalable-hpc-singularity-images\"\
    \ class=\"anchor\" href=\"#remote-sensing-environments-in-scalable-hpc-singularity-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Remote Sensing Environments in Scalable HPC Singularity Images</h1>\n\
    <p>The purpose of this singularity images and setup is to allow for development\
    \ of an active pipeline between image resources, hosting, ingestion protocol into\
    \ Google Earth Engine and retentation on volume as needed.</p>\n<h2>\n<a id=\"\
    user-content-main-build-components--should-include\" class=\"anchor\" href=\"\
    #main-build-components--should-include\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Main Build components ( Should\
    \ include)</h2>\n<ul>\n<li>Planet API Download Client to download images from\
    \ Planet API <strong>Included</strong>\n</li>\n<li>FTP client to pull images from\
    \ existing sftp or ftp addresses <strong>In progress</strong>\n</li>\n<li>Include\
    \ additional tools (shapely, pdal, rasterio) <strong>In progress</strong>\n</li>\n\
    <li>Singularity container which consists of further tools to manipulate and preprocess\
    \ imagery <strong>In progress</strong>\n</li>\n<li>Singularity container which\
    \ is mounted with large shared volume but individual user folder <strong>In progress</strong>\n\
    </li>\n<li>Earth Engine Upload and processing client including Earth Engine API\
    \ client <strong>Included</strong>\n</li>\n<li>Jupyter notebook to connnect to\
    \ mounted volume for local analysis and export of image or remote analysis and\
    \ export from Google Earth Engine <strong>In progress</strong>\n</li>\n<li>Drive\
    \ to access google drive: Check if all tasks have completed and download from\
    \ google drive, Verify and delete to preserve google drive storage <strong>In\
    \ progress</strong>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-possible-integrations-and-enhancements\"\
    \ class=\"anchor\" href=\"#possible-integrations-and-enhancements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Possible\
    \ Integrations and Enhancements</h2>\n<ul>\n<li>Leaflet based imagery and vector\
    \ visualization</li>\n<li>Image tiling so user can visualize on the leaflet window\
    \ within the Jupyter notebooks</li>\n<li>End results include QGIS with X11 support\
    \ to allow user to generate maps and export as images.</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-planet-cli-in-a-docker-container\" class=\"anchor\" href=\"#planet-cli-in-a-docker-container\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Planet CLI in a Docker Container</h2>\n<p><em>In Progress</em></p>\n\
    <h2>\n<a id=\"user-content-planet-cli-in-a-singularity-container\" class=\"anchor\"\
    \ href=\"#planet-cli-in-a-singularity-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Planet CLI in a Singularity Container</h2>\n\
    <p>Singularity files are in the <code>/basic</code> and <code>/osgeo</code> folders.\
    \ The <code>/osgeo</code> container is maintained by Tyson Swetnam on <a href=\"\
    \">Singularity Hub</a></p>\n<p>To build a container:</p>\n<pre><code>cd basic/\n\
    sudo singularity build basic.simg Singularity\n</code></pre>\n<pre><code>cd osgeo/\n\
    sudo singularity build osgeo.simg Singularity\n</code></pre>\n<h3>\n<a id=\"user-content-moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\"\
    \ class=\"anchor\" href=\"#moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Moving files from Drive.Google into an Atmosphere or Jetstream VM</h3>\n\
    <p>In the US, academic institutions have increasingly established email accounts\
    \ through Google.\nSome institutions have unlimited storage on <a href=\"https://drive.google.com\"\
    \ rel=\"nofollow\">Drive</a>\nas a service for faculty and students.</p>\n<p>One\
    \ of the issues with uploading / downloading a large number of images to or from\
    \ a Drive\naccount through the web browser (Chrome or Firefox) is the number of\
    \ files allowed,\nthe speed of the uploads, and the size of the downloads.</p>\n\
    <p>Typically a download directly from a Drive account through Chrome is limited\
    \ to &lt;2Gb\nand is zipped by Google before starting.</p>\n<p>While the browser\
    \ can work well for easily uploading a large number sUAS images from a collection,\n\
    downloading the images as .zip files from the Google.drive can become tiresome\
    \ and difficult.</p>\n<p>To get around these problems we can use 3rd party applications\
    \ like FUSE and\n<a href=\"https://github.com/odeke-em/drive\"><code>drive</code></a>.</p>\n\
    <h3>\n<a id=\"user-content-fuse-client-ocamfluse\" class=\"anchor\" href=\"#fuse-client-ocamfluse\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>FUSE client <code>ocamfluse</code>\n</h3>\n<p><a href=\"https://github.com/astrada/google-drive-ocamlfuse\"\
    ><code>google-drive-ocamlfuse</code></a> is a Google Drive Client written in OCaml.\
    \ This is tested for Ubuntu systems.</p>\n<p>It can mount your google drive as\
    \ a folder visible in the file tree. FUSE is slower than other methods like iRODS\
    \ or <code>Drive</code>, but allows for GUI based drag and drop transfers.</p>\n\
    <pre><code>sudo add-apt-repository ppa:alessandro-strada/ppa\n</code></pre>\n\
    <pre><code>sudo apt-get update\nsudo apt-get install google-drive-ocamlfuse\n\
    </code></pre>\n<p>Run the app the first time to get the authentication certificate\
    \ from Google</p>\n<pre><code>google-drive-ocamlfuse\n</code></pre>\n<p>Create\
    \ a folder:</p>\n<pre><code>mkdir ~/googledrive\n</code></pre>\n<p>Mount the new\
    \ googledrive</p>\n<pre><code>google-drive-ocamlfuse ~/googledrive\n</code></pre>\n\
    <p>Open in your favorite file explorer.</p>\n<h3>\n<a id=\"user-content-install-go\"\
    \ class=\"anchor\" href=\"#install-go\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install <code>Go</code>\n</h3>\n\
    <p>Drive uses the <code>go</code> language. In order to work with it you need\
    \ to <a href=\"https://golang.org/doc/install\" rel=\"nofollow\">install <code>go</code></a>\
    \ onto the VM.</p>\n<p>Remove any other go packages (particularly gccgo-go)</p>\n\
    <pre><code>sudo apt-get -y autoremove gccgo-go\n</code></pre>\n<pre><code>wget\
    \ https://storage.googleapis.com/golang/go1.8.1.linux-amd64.tar.gz\nsudo tar -C\
    \ /usr/local -xzf go1.8.1.linux-amd64.tar.gz\n</code></pre>\n<p>In <code>/etc/profile</code>\
    \ add: <code>export PATH=$PATH:/usr/local/go/bin</code></p>\n<pre><code>cat &lt;&lt;\
    \ ! &gt;&gt; /etc/profile\nexport PATH=$PATH:/usr/local/go/bin\n!\n</code></pre>\n\
    <p>In <code>~/.bashrc</code> - <code>sudo nano ~/.bashrc</code></p>\n<p>Add the\
    \ GOPATH directly from terminal:</p>\n<pre><code>cat &lt;&lt; ! &gt;&gt; ~/.bashrc\n\
    export GOPATH=\\$HOME/go\nexport PATH=\\$GOPATH:\\$GOPATH/bin:\\$PATH\n!\n</code></pre>\n\
    <p>Source the new <code>~/.bashrc</code> close the terminal and reopen</p>\n<pre><code>source\
    \ ~/.bashrc \n</code></pre>\n<p>Follow the <code>go</code>instructions to <a href=\"\
    https://golang.org/doc/install#testing\" rel=\"nofollow\">test the installation</a></p>\n\
    <h3>\n<a id=\"user-content-install-drive-a-drivegoogle-client-for-commandline\"\
    \ class=\"anchor\" href=\"#install-drive-a-drivegoogle-client-for-commandline\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Install <code>drive</code> a drive.google client for commandline</h3>\n\
    <p><a href=\"https://github.com/odeke-em/drive#installing\"><code>drive</code></a>\
    \ is a command line client using Go language</p>\n<p>Install <code>drive</code>\
    \ using <code>go</code></p>\n<pre><code>#install git\nsudo apt-get install git\n\
    </code></pre>\n<pre><code>cd ~/go\ngo get -u github.com/odeke-em/drive/cmd/drive\n\
    </code></pre>\n<h3>\n<a id=\"user-content-initialize-drive-with-your-google-account\"\
    \ class=\"anchor\" href=\"#initialize-drive-with-your-google-account\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Initialize\
    \ <code>drive</code> with your Google account</h3>\n<pre><code>mkdir ~/gdrive\n\
    drive init ~/gdrive\n</code></pre>\n<p>Follow the instructions for copying and\
    \ pasting the html for authentication</p>\n<p>Test your installation</p>\n<pre><code>drive\
    \ ls\n</code></pre>\n<h2>\n<a id=\"user-content-pull-data-from-your-googledrive-account-onto-the-vm\"\
    \ class=\"anchor\" href=\"#pull-data-from-your-googledrive-account-onto-the-vm\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pull data from your Google.drive account onto the VM</h2>\n<pre><code>drive\
    \ pull your/google/drive/folders/here\n</code></pre>\n<h3>\n<a id=\"user-content-using-drive-on-ua-hpc\"\
    \ class=\"anchor\" href=\"#using-drive-on-ua-hpc\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using Drive on\
    \ UA HPC</h3>\n<p>Drive is currently available on the ElGato login node</p>\n\
    <p>Load <code>go</code> and <code>drive</code></p>\n<pre><code>module load go\n\
    module load drive\n</code></pre>\n<p>Create a directory where you want to initiate\
    \ <code>drive</code> - preferrably on your <code>/xdisk/</code></p>\n<pre><code>cd\
    \ /xdisk/uid/\nmkdir gdrive\n</code></pre>\n<p>Initiate the drive</p>\n<pre><code>drive\
    \ init /xdisk/uid/gdrive\n</code></pre>\n<p>You will get a request to <code>Visit\
    \ this URL to get an authorization code</code> with a link to a long <code>https://accounts.google.com/o/oauth2/xxxx</code>\
    \ URL - copy paste that link into your preferred browser.</p>\n<p>You will be\
    \ taken to a Google login page, type in your email address (<a href=\"mailto:uid@email.arizona.edu\"\
    >uid@email.arizona.edu</a>) and password.</p>\n<p>Copy/Paste the code provided\
    \ by Google back in your Terminal window where prompted: <code>Paste the authorization\
    \ code:</code></p>\n<p>Now, check to see if your <code>drive.google.com</code>\
    \ acount is active:</p>\n<pre><code>drive ls\n</code></pre>\n<p>A list of the\
    \ directories in your <code>drive.google.com</code> account should be listed,\
    \ e.g.</p>\n<pre><code>/project1\n/project2\n/reports1\n/pictures1\n</code></pre>\n\
    <p>You can <code>pull</code> files or directories from your <code>drive.google.com</code>\
    \ now using commands like:</p>\n<pre><code>drive pull project1/subfolder/\n</code></pre>\n\
    <p>You will see:</p>\n<pre><code>Resolving...\n</code></pre>\n<p>followed by a\
    \ spinning <code>\\</code> <code>|</code> <code>/</code> <code>-</code> set of\
    \ symbols</p>\n<p>The folder contents will be displayed:</p>\n<pre><code> /project1/subfolder/file1.csv\n\
    ...\n+ /project1/subfolder/file955.csv\n+ /project1/subfolder/file966.csv\nAddition\
    \ count 966 src: 5.02GB\nProceed with the changes? [Y/n]:\n</code></pre>\n<p>Select\
    \ <code>y</code> and the download will proceed.</p>\n<p>Typical speeds are between\
    \ 10 and 50 Mb/s.</p>\n<pre><code>Proceed with the changes? [Y/n]:y\n 5392682146\
    \ / 5392682146 [==========================================================================================================================]\
    \ 100.00% 1m55s\n</code></pre>\n<h1>\n<a id=\"user-content-setting-up-cyverse-data-store-and-irods-icommands\"\
    \ class=\"anchor\" href=\"#setting-up-cyverse-data-store-and-irods-icommands\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Setting up CyVerse Data Store and iRods iCommands</h1>\n<p><a href=\"\
    https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\" rel=\"\
    nofollow\">Instructions</a></p>\n<pre><code>$ iinit\n</code></pre>\n<p>You will\
    \ be queried to set up your <code>irods_environment.json</code></p>\n<p>Enter\
    \ the following:</p>\n<table>\n<thead>\n<tr>\n<th>statement</th>\n<th>input</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>DNS</td>\n<td><em>data.cyverse.org</em></td>\n\
    </tr>\n<tr>\n<td>port number</td>\n<td><em>1247</em></td>\n</tr>\n<tr>\n<td>user\
    \ name</td>\n<td><em>your user name</em></td>\n</tr>\n<tr>\n<td>zone</td>\n<td><em>iplant</em></td>\n\
    </tr>\n</tbody>\n</table>\n<p>Set up auto-complete for iCommands\n<a href=\"https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\"\
    \ rel=\"nofollow\">instructions</a></p>\n<p>Download <a href=\"https://pods.iplantcollaborative.org/wiki/download/attachments/6720192/i-commands-auto.bash\"\
    \ rel=\"nofollow\">i-commands-auto.bash</a>.\nIn your home directory, rename i-commands-auto.bash\
    \ to .i-commands-auto.bash\nIn your .bashrc or .bash_profile, enter the following:\n\
    source .i-commands-auto.bash</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1611393577.0
mgoubran/MIRACL:
  data_format: 2
  description: Multi-modal Image Registration And Connectivity anaLysis
  filenames:
  - Singularity
  full_name: mgoubran/MIRACL
  latest_release: null
  readme: "<p><a href=\"https://github.com/mgoubran/MIRACL/blob/master/LICENSE.md\"\
    ><img src=\"https://camo.githubusercontent.com/1075b8b9dd5e5633cedcda2b5117cd7f11935b060bbd3469fcc1e8dbb3d4a302/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d676f756272616e2f4d495241434c\"\
    \ alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/mgoubran/MIRACL\"\
    \ style=\"max-width:100%;\"></a> <a href=\"https://camo.githubusercontent.com/baa20110996179dced47b627519e4f6abf39a884364dae9b6761089520ae0cb5/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d676f756272616e2f6d697261636c\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/baa20110996179dced47b627519e4f6abf39a884364dae9b6761089520ae0cb5/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d676f756272616e2f6d697261636c\"\
    \ alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/mgoubran/miracl\"\
    \ style=\"max-width:100%;\"></a></p>\n<p align=\"center\">\n  <a href=\"docs/gallery/icon.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"docs/gallery/icon.png\"\
    \ alt=\"alt text\" width=\"400\" height=\"250\" style=\"max-width:100%;\"></a>\n\
    </p>\n<hr>\n<p>MIRACL (Multi-modal Image Registration And Connectivity anaLysis)\n\
    is a general-purpose, open-source pipeline for automated:</p>\n<pre><code>1) Registration\
    \ of mice clarity data to the Allen reference atlas\n2) Segmentation &amp; feature\
    \ extraction of mice clarity data in 3D (Sparse &amp; nuclear staining)\n3) Registration\
    \ of mice multimodal imaging data (MRI &amp; CT, in-vivo &amp; ex-vivo) to Allen\
    \ reference atlas\n4) Tract or label specific connectivity analysis based on the\
    \ Allen connectivity atlas\n5) Comparison of diffusion tensort imaging (DTI)/tractography,\
    \ virus tracing using CLARITY &amp;\n   Allen connectivity atlas\n6) Statistical\
    \ analysis of CLARITY &amp; Imaging data\n7) Atlas generation &amp; Label manipulation\n\
    </code></pre>\n<p>Copyright (c) 2019 Maged Goubran, <a href=\"mailto:maged.goubran@utoronto.ca\"\
    >maged.goubran@utoronto.ca</a></p>\n<p>All Rights Reserved.</p>\n<hr>\n<p>We provide\
    \ containers for using the software (Docker and Singularity) as well as\nlocal\
    \ install instructions. For more details, see our <a href=\"https://miracl.readthedocs.io\"\
    \ rel=\"nofollow\">docs</a>.\nNote that the base image for the docker container\
    \ can be found in <a href=\"docker\">docker</a> and\nthe container <code>mgoubran/miracl</code>\
    \ is built on top of that.</p>\n"
  stargazers_count: 13
  subscribers_count: 8
  topics:
  - neuroscience
  - brain
  - atlases
  - connectivity
  - network
  - clarity
  - mri
  - neuroimaging
  - biomedical
  - image-registration
  - image-segmentation
  - python
  updated_at: 1621933238.0
mgroth0/dnn:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity_debug
  full_name: mgroth0/dnn
  latest_release: null
  readme: '<h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <ol>

    <li>git clone --recurse-submodules <a href="https://github.com/mgroth0/dnn">https://github.com/mgroth0/dnn</a>

    </li>

    <li>install <a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow">miniconda</a>

    </li>

    <li><code>conda update conda</code></li>

    <li>

    <code>conda create --name dnn --file requirements.txt</code> (requirements.txt
    is currently not working, TODO)</li>

    <li>might need to separately <code>conda install -c mgroth0 mlib-mgroth0</code>--
    When updating, use <code>conda install --file requirements.txt;</code>

    </li>

    <li><code>conda activate dnn</code></li>

    </ol>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ul>

    <li>

    <p>./dnn</p>

    </li>

    <li>

    <p>Generate some images, train/test a model, run analyses, and generate plots.
    Tested on Mac, but not yet on linux/Windows.</p>

    </li>

    <li>

    <p><code>./dnn -cfg=gen_images --INTERACT=0</code></p>

    </li>

    <li>

    <p><code>./dnn -cfg=test_one --INTERACT=0</code></p>

    </li>

    </ul>

    <p>The second command will fail with a Mathematica-related error, but your results
    will be saved in <code>_figs</code>.</p>

    <p>TODO: have to also consider running and developing other executables here:
    human_exp_1 and human_analyze</p>

    <h2>

    <a id="user-content-configuration" class="anchor" href="#configuration" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h2>

    <p>-MODE: (default = FULL) is a string that can contain any combination of the
    following (example: "CLEAN JUSTRUN")</p>

    <ul>

    <li>CLEAN</li>

    <li>JUSTRUN</li>

    <li>GETANDMAKE</li>

    <li>MAKEREPORT</li>

    </ul>

    <p>Edit <a href="">cfg.yml</a> to save configuration options. Feel free to push
    these.</p>

    <p>If there is anything hardcoded that you''d like to be configurable, please
    submit an issue.</p>

    <h2>

    <a id="user-content-testing" class="anchor" href="#testing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Testing</h2>

    <p>todo</p>

    <h2>

    <a id="user-content-development" class="anchor" href="#development" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Development</h2>

    <ul>

    <li>TODO: have separate development and user modes. Developer mode has PYTHONPATH
    link to mlib and instructions for resolving and developing in ide in parallel.
    User mode has mlib as normal dependency. might need to use <code>conda uninstall
    mlib-mgroth0 --force</code>. Also in these public readmes or reqs.txt I have to
    require a specific mlib version</li>

    <li>./dnn build</li>

    </ul>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>Darius, Xavier, Pawan</p>

    <p>heuritech, raghakot, joel</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1617456499.0
miguelcarcamov/container_docker:
  data_format: 2
  description: null
  filenames:
  - Singularity
  - Singularity.casacore.gpuvmem.11.0
  - Singularity.casacore.gpuvmem.9.2
  - Singularity.casacore.gpuvmem.10.0.ubuntu1604
  - Singularity.HPC
  - Singularity.casacore.gpuvmem.10.0
  - Singularity.casacore.gpuvmem.9.2.ubuntu1604
  full_name: miguelcarcamov/container_docker
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-container_docker\" class=\"anchor\" href=\"\
    #container_docker\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>container_docker</h1>\n<p>Useful containers to work\
    \ with radio astronomical data</p>\n<p>Miguel C\xE1rcamo.</p>\n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1615461418.0
mjboos/voxelwiseencoding:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: mjboos/voxelwiseencoding
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-voxel-wise-encoding-models-for-bids-datasets-with-naturalistic-stimuli\"\
    \ class=\"anchor\" href=\"#voxel-wise-encoding-models-for-bids-datasets-with-naturalistic-stimuli\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Voxel-wise encoding models for BIDS datasets with naturalistic stimuli</h1>\n\
    <blockquote>\n<p>This BIDS App lets you train voxelwise-encoding models for continuous\
    \ (naturalistic) stimuli provided as a BIDS-compliant continuous recording file.</p>\n\
    </blockquote>\n<p><a href=\"https://github.com/mjboos/voxelwiseencoding/workflows/CI/badge.svg\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/mjboos/voxelwiseencoding/workflows/CI/badge.svg\"\
    \ alt=\"\" style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://raw.githubusercontent.com/mjboos/voxelwiseencoding/master/scheme_BIDS_encoding.png\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/mjboos/voxelwiseencoding/master/scheme_BIDS_encoding.png\"\
    \ alt=\"Schema for voxel-wise encoding models using a BIDS dataset\" style=\"\
    max-width:100%;\"></a></p>\n<p>For more information about the specification of\
    \ BIDS Apps see <a href=\"https://docs.google.com/document/d/1E1Wi5ONvOVVnGhj21S1bmJJ4kyHFT7tkxnV3C23sjIE/\"\
    \ rel=\"nofollow\">here</a>.\nFor auditory stimuli <a href=\"https://github.com/mjboos/audio2bidsstim/\"\
    >this</a> module can help you convert your wav file to a BIDS stimulus representation.</p>\n\
    <h2>\n<a id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Install</h2>\n\
    <p>If you are only interested in using the Python module for preprocessing fMRI,\
    \ lagging the stimulus, and training encoding models without the BIDS app, you\
    \ can install this library by running <code>python setup.py</code> or <code>pip\
    \ install -e voxelwiseencoding</code>.\nYou can use the BIDS app either via Docker\
    \ or directly by calling <code>run.py</code>.</p>\n<h2>\n<a id=\"user-content-description\"\
    \ class=\"anchor\" href=\"#description\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Description</h2>\n<p>This library\
    \ allows you to train and validate voxel-wise encoding models for a BIDS dataset\
    \ with a BIDS-compliant stimulus representation. See below for an example on how\
    \ to use it.</p>\n<h3>\n<a id=\"user-content-example\" class=\"anchor\" href=\"\
    #example\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Example</h3>\n<p>We are going to use <a href=\"https://openneuro.org/datasets/ds002322/versions/1.0.4\"\
    \ rel=\"nofollow\">this</a> dataset to demonstrate an example workflow using the\
    \ Python package.</p>\n<p>First we need to download the data and extract a stimulus\
    \ representation:</p>\n<div class=\"highlight highlight-source-python\"><pre><span\
    \ class=\"pl-c\">#hide_output</span>\n!a<span class=\"pl-s1\">ws</span> <span\
    \ class=\"pl-s1\">s3</span> <span class=\"pl-s1\">sync</span> <span class=\"pl-c1\"\
    >-</span><span class=\"pl-c1\">-</span><span class=\"pl-s1\">no</span><span class=\"\
    pl-c1\">-</span><span class=\"pl-s1\">sign</span><span class=\"pl-c1\">-</span><span\
    \ class=\"pl-s1\">request</span> <span class=\"pl-s1\">s3</span>:<span class=\"\
    pl-c1\">//</span><span class=\"pl-s1\">openneuro</span>.<span class=\"pl-s1\"\
    >org</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">ds002322</span>\
    \ <span class=\"pl-c1\">/</span><span class=\"pl-s1\">data</span><span class=\"\
    pl-c1\">/</span><span class=\"pl-s1\">ds002322</span><span class=\"pl-c1\">-</span><span\
    \ class=\"pl-s1\">download</span><span class=\"pl-c1\">/</span>\n<span class=\"\
    pl-s1\">import</span> <span class=\"pl-s1\">json</span>\n<span class=\"pl-c\"\
    ># these are the parameters for extracting a Mel spectrogram</span>\n<span class=\"\
    pl-c\"># for computational ease in this example we want 1 sec segments of 31 Mel\
    \ frequencies with a max frequency of * KHz</span>\n<span class=\"pl-s1\">mel_params</span>\
    \ <span class=\"pl-c1\">=</span> {<span class=\"pl-s\">'n_mels'</span>: <span\
    \ class=\"pl-c1\">31</span>, <span class=\"pl-s\">'sr'</span>: <span class=\"\
    pl-c1\">16000</span>, <span class=\"pl-s\">'hop_length'</span>: <span class=\"\
    pl-c1\">16000</span>, <span class=\"pl-s\">'n_fft'</span>: <span class=\"pl-c1\"\
    >16000</span>, <span class=\"pl-s\">'fmax'</span>: <span class=\"pl-c1\">8000</span>}\n\
    <span class=\"pl-k\">with</span> <span class=\"pl-en\">open</span>(<span class=\"\
    pl-s\">'config.json'</span>, <span class=\"pl-s\">'w+'</span>) <span class=\"\
    pl-k\">as</span> <span class=\"pl-s1\">fl</span>:\n    <span class=\"pl-s1\">json</span>.<span\
    \ class=\"pl-en\">dump</span>(<span class=\"pl-s1\">mel_params</span>, <span class=\"\
    pl-s1\">fl</span>)\n\n!g<span class=\"pl-s1\">it</span> <span class=\"pl-s1\"\
    >clone</span> <span class=\"pl-s1\">https</span>:<span class=\"pl-c1\">//</span><span\
    \ class=\"pl-s1\">github</span>.<span class=\"pl-s1\">com</span><span class=\"\
    pl-c1\">/</span><span class=\"pl-s1\">mjboos</span><span class=\"pl-c1\">/</span><span\
    \ class=\"pl-s1\">audio2bidsstim</span><span class=\"pl-c1\">/</span>\n!p<span\
    \ class=\"pl-s1\">ip</span> <span class=\"pl-s1\">install</span> <span class=\"\
    pl-c1\">-</span><span class=\"pl-s1\">r</span> <span class=\"pl-s1\">audio2bidsstim</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">requirements</span>.<span class=\"\
    pl-s1\">txt</span>\n!p<span class=\"pl-s1\">ython</span> <span class=\"pl-s1\"\
    >audio2bidsstim</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">wav_files_to_bids_tsv</span>.<span\
    \ class=\"pl-s1\">py</span> <span class=\"pl-c1\">/</span><span class=\"pl-s1\"\
    >data</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">ds002322</span><span\
    \ class=\"pl-c1\">-</span><span class=\"pl-s1\">download</span><span class=\"\
    pl-c1\">/</span><span class=\"pl-s1\">stimuli</span><span class=\"pl-c1\">/</span><span\
    \ class=\"pl-v\">DownTheRabbitHoleFinal_mono_exp120_NR16_pad</span>.<span class=\"\
    pl-s1\">wav</span> <span class=\"pl-c1\">-</span><span class=\"pl-s1\">c</span>\
    \ <span class=\"pl-s1\">config</span>.<span class=\"pl-s1\">json</span></pre></div>\n\
    <p>We then need to copy the extracted stimulus representation into the BIDS folder.</p>\n\
    <div class=\"highlight highlight-source-python\"><pre>!c<span class=\"pl-s1\"\
    >p</span> <span class=\"pl-v\">DownTheRabbitHoleFinal_mono_exp120_NR16_pad</span>.<span\
    \ class=\"pl-s1\">tsv</span>.<span class=\"pl-s1\">gz</span> <span class=\"pl-c1\"\
    >/</span><span class=\"pl-s1\">data</span><span class=\"pl-c1\">/</span><span\
    \ class=\"pl-s1\">ds002322</span><span class=\"pl-c1\">-</span><span class=\"\
    pl-s1\">download</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\">derivatives</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">task</span><span class=\"pl-c1\"\
    >-</span><span class=\"pl-s1\">alice_stim</span>.<span class=\"pl-s1\">tsv</span>.<span\
    \ class=\"pl-s1\">gz</span>\n!c<span class=\"pl-s1\">p</span> <span class=\"pl-v\"\
    >DownTheRabbitHoleFinal_mono_exp120_NR16_pad</span>.<span class=\"pl-s1\">json</span>\
    \ <span class=\"pl-c1\">/</span><span class=\"pl-s1\">data</span><span class=\"\
    pl-c1\">/</span><span class=\"pl-s1\">ds002322</span><span class=\"pl-c1\">-</span><span\
    \ class=\"pl-s1\">download</span><span class=\"pl-c1\">/</span><span class=\"\
    pl-s1\">derivatives</span><span class=\"pl-c1\">/</span><span class=\"pl-s1\"\
    >sub</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">22</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">sub</span><span class=\"pl-c1\"\
    >-</span><span class=\"pl-c1\">22_</span><span class=\"pl-s1\">task</span><span\
    \ class=\"pl-c1\">-</span><span class=\"pl-s1\">alice_stim</span>.<span class=\"\
    pl-s1\">json</span></pre></div>\n<p>And, lastly, because for this dataset the\
    \ derivatives folder is missing timing information for the BOLD files - we are\
    \ only interested in the TR - we have to copy that as well.</p>\n<div class=\"\
    highlight highlight-source-python\"><pre>!c<span class=\"pl-s1\">p</span> <span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">data</span><span class=\"pl-c1\"\
    >/</span><span class=\"pl-s1\">ds002322</span><span class=\"pl-c1\">-</span><span\
    \ class=\"pl-s1\">download</span><span class=\"pl-c1\">/</span><span class=\"\
    pl-s1\">sub</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">22</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">sub</span><span class=\"pl-c1\"\
    >-</span><span class=\"pl-c1\">22_</span><span class=\"pl-s1\">task</span><span\
    \ class=\"pl-c1\">-</span><span class=\"pl-s1\">alice_bold</span>.<span class=\"\
    pl-s1\">json</span> <span class=\"pl-c1\">/</span><span class=\"pl-s1\">data</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">ds002322</span><span class=\"\
    pl-c1\">-</span><span class=\"pl-s1\">download</span><span class=\"pl-c1\">/</span><span\
    \ class=\"pl-s1\">derivatives</span><span class=\"pl-c1\">/</span><span class=\"\
    pl-s1\">sub</span><span class=\"pl-c1\">-</span><span class=\"pl-c1\">22</span><span\
    \ class=\"pl-c1\">/</span><span class=\"pl-s1\">sub</span><span class=\"pl-c1\"\
    >-</span><span class=\"pl-c1\">22_</span><span class=\"pl-s1\">task</span><span\
    \ class=\"pl-c1\">-</span><span class=\"pl-s1\">alice_bold</span>.<span class=\"\
    pl-s1\">json</span> </pre></div>\n<p>We are now ready to define some model parameters\
    \ and train the encoding model.</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">voxelwiseencoding</span>.<span\
    \ class=\"pl-s1\">process_bids</span> <span class=\"pl-k\">import</span> <span\
    \ class=\"pl-s1\">run_model_for_subject</span>\n\n<span class=\"pl-c\"># these\
    \ are the parameters used for preprocessing the BOLD fMRI files</span>\n<span\
    \ class=\"pl-s1\">bold_prep_params</span> <span class=\"pl-c1\">=</span> {<span\
    \ class=\"pl-s\">'standardize'</span>: <span class=\"pl-s\">'zscore'</span>, <span\
    \ class=\"pl-s\">'detrend'</span>: <span class=\"pl-c1\">True</span>}\n\n<span\
    \ class=\"pl-c\"># and for lagging the stimulus as well - we want to include 6\
    \ sec stimulus segments to predict fMRI</span>\n<span class=\"pl-s1\">lagging_params</span>\
    \ <span class=\"pl-c1\">=</span> {<span class=\"pl-s\">'lag_time'</span>: <span\
    \ class=\"pl-c1\">6</span>}\n\n<span class=\"pl-c\"># these are the parameters\
    \ for sklearn's Ridge estimator</span>\n<span class=\"pl-s1\">ridge_params</span>\
    \ <span class=\"pl-c1\">=</span> {<span class=\"pl-s\">'alphas'</span>: [<span\
    \ class=\"pl-c1\">1e-1</span>, <span class=\"pl-c1\">1</span>, <span class=\"\
    pl-c1\">100</span>, <span class=\"pl-c1\">1000</span>],\n                <span\
    \ class=\"pl-s\">'cv'</span>: <span class=\"pl-c1\">3</span>, <span class=\"pl-s\"\
    >'normalize'</span>: <span class=\"pl-c1\">True</span>}\n\n\n<span class=\"pl-s1\"\
    >ridges</span>, <span class=\"pl-s1\">scores</span>, <span class=\"pl-s1\">computed_mask</span>\
    \ <span class=\"pl-c1\">=</span> <span class=\"pl-en\">run_model_for_subject</span>(<span\
    \ class=\"pl-s\">'22'</span>, <span class=\"pl-s\">'/data/ds002322-download/derivatives'</span>,\n\
    \                                                      <span class=\"pl-s1\">task</span><span\
    \ class=\"pl-c1\">=</span><span class=\"pl-s\">'alice'</span>, <span class=\"\
    pl-s1\">mask</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'epi'</span>,\
    \ <span class=\"pl-s1\">bold_prep_kwargs</span><span class=\"pl-c1\">=</span><span\
    \ class=\"pl-s1\">bold_prep_params</span>,\n                                 \
    \                     <span class=\"pl-s1\">preprocess_kwargs</span><span class=\"\
    pl-c1\">=</span><span class=\"pl-s1\">lagging_params</span>, <span class=\"pl-s1\"\
    >encoding_kwargs</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">ridge_params</span>)</pre></div>\n\
    <p>We can now assess the quality out-of-sample prediction (in terms of product-moment\
    \ correlations) of our models and visualize where we can predict well.</p>\n<div\
    \ class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span>\
    \ <span class=\"pl-s1\">nilearn</span>.<span class=\"pl-s1\">masking</span> <span\
    \ class=\"pl-k\">import</span> <span class=\"pl-s1\">unmask</span>\n<span class=\"\
    pl-k\">from</span> <span class=\"pl-s1\">nilearn</span>.<span class=\"pl-s1\"\
    >plotting</span> <span class=\"pl-k\">import</span> <span class=\"pl-s1\">plot_stat_map</span>\n\
    <span class=\"pl-en\">plot_stat_map</span>(<span class=\"pl-en\">unmask</span>(<span\
    \ class=\"pl-s1\">scores</span>.<span class=\"pl-en\">mean</span>(<span class=\"\
    pl-s1\">axis</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">-</span><span\
    \ class=\"pl-c1\">1</span>), <span class=\"pl-s1\">computed_mask</span>), <span\
    \ class=\"pl-s1\">threshold</span><span class=\"pl-c1\">=</span><span class=\"\
    pl-c1\">0.1</span>)</pre></div>\n<pre><code>&lt;nilearn.plotting.displays.OrthoSlicer\
    \ at 0x7fbbd11a2320&gt;\n</code></pre>\n<p><a href=\"docs/images/output_11_1.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"docs/images/output_11_1.png\"\
    \ alt=\"png\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-documentation\"\
    \ class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Documentation</h2>\n<p><a href=\"\
    https://mjboos.github.io/voxelwiseencoding\" rel=\"nofollow\">See here</a> for\
    \ further documentation about the Python package and consult the section \"Using\
    \ this module as a BIDS app\" about how to use this library as a Docker image/BIDS\
    \ app.</p>\n<h2>\n<a id=\"user-content-how-to-report-errors\" class=\"anchor\"\
    \ href=\"#how-to-report-errors\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>How to report errors</h2>\n<p>If you\
    \ encounter errors with this code or have any questions about its uage, please\
    \ open an issue on the Github repository <a href=\"https://github.com/mjboos/voxelwiseencoding/\"\
    >here</a>.</p>\n"
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1614124528.0
mjfortier/singularityContainers:
  data_format: 2
  description: container files for singularity-hub.org to build into images.
  filenames:
  - Singularity
  - archived/Singularity_old
  full_name: mjfortier/singularityContainers
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularitycontainers" class="anchor" href="#singularitycontainers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularityContainers</h1>

    <p>container files for singularity-hub.org to build into images.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1572576861.0
mkandes/naked-singularity:
  data_format: 2
  description: A repository of definition files for bootstrapping Singularity containers
    around the software applications, frameworks, and libraries you need to run on
    high-performance computing systems.
  filenames:
  - definition-files/tensorflow/Singularity.tensorflow-2.3.0
  - definition-files/pytorch/Singularity.pytorch-1.5.0
  - definition-files/visit/Singularity.visit-3.1.4-ubuntu-18.04-openmpi-3.1.6
  - definition-files/anaconda/Singularity.anaconda2-py27-2019.10-ubuntu-18.04
  - definition-files/anaconda/Singularity.anaconda3-py38-2020.11-ubuntu-18.04
  - definition-files/mxnet/Singularity.mxnet-1.7.0
  - definition-files/miniconda/Singularity.miniconda3-py37-4.9.2-ubuntu-18.04
  - definition-files/miniconda/Singularity.miniconda2-py27-4.8.3-ubuntu-18.04
  - definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-mvapich-2.3.2
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-mvapich-2.3.2
  - definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-openmpi-4.0.5
  - definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-cuda-11.2-openmpi-4.0.5
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.6
  - definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.4
  - definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-openmpi-3.1.4
  - definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5
  - definition-files/paraview/Singularity.paraview-5.9.0-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5
  - definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.4-osmesa-20.1.5
  - definition-files/xcrysden/Singularity.xcrysden-1.6.2
  - definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.4-openblas-0.3.10
  - definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.6-openblas-0.3.10
  - definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-4.0.5-openblas-0.3.14
  - definition-files/stream/Singularity.stream-5.10-ubuntu-18.04
  - definition-files/spark/Singularity.spark-2.3.1-hadoop-2.7-ubuntu-18.04
  - definition-files/fenics/Singularity.fenics-2019.1.0
  - definition-files/ciml/Singularity.esm-0.3.1
  - definition-files/ciml/Singularity.tape-0.4
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2-openmpi-3.1.6
  - definition-files/ubuntu/Singularity.ubuntu-18.04
  - definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.6
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2-openmpi-4.0.5
  - definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-4.0.5
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2
  - definition-files/ubuntu/Singularity.ubuntu-20.04
  - definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.4
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168
  - definition-files/ubuntu/Singularity.ubuntu-18.04-mvapich-2.3.2
  - definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4
  - definition-files/beast/Singularity.beast-2.6.1
  - definition-files/beast/Singularity.beast-1.10.4
  - definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.6
  - definition-files/ior/Singularity.ior-3.3.0-ubuntu-18.04-openmpi-4.0.5
  - definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.4
  - definition-files/singularity/Singularity.singularity-3.5.3
  - definition-files/centos/Singularity.centos-7.9.2009-openmpi-3.1.4
  - definition-files/centos/Singularity.centos-7.9.2009
  - definition-files/centos/Singularity.centos-7.9.2009-cuda-10.1.168
  - definition-files/centos/Singularity.centos-7.9.2009-mvapich-2.3.2
  - definition-files/gromacs/Singularity.gromacs-2020.3-ubuntu-18.04-cuda-10.1.168-tmpi-avx-512-cuda-70
  full_name: mkandes/naked-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-naked-singularity" class="anchor" href="#naked-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>naked-singularity</h1>

    <p>A repository of definition files for building

    <a href="https://sylabs.io/guides/latest/user-guide" rel="nofollow">Singularity</a>
    containers

    around the software applications, frameworks, and libraries you need to

    run on high-performance computing systems.</p>

    <h2>

    <a id="user-content-install-singularity" class="anchor" href="#install-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Singularity</h2>

    <p>Install Singularity on your Linux desktop, laptop, or virtual machine.</p>

    <div class="highlight highlight-source-shell"><pre>./naked-singularity.sh install</pre></div>

    <h2>

    <a id="user-content-build-a-singularity-container-from-a-definition-file" class="anchor"
    href="#build-a-singularity-container-from-a-definition-file" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build a Singularity
    container from a definition file</h2>

    <p>Build an Ubuntu Singularity container from one of the definition files

    available in this repository.</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build ubuntu.sif
    definition-files/ubuntu/Singularity.ubuntu-18.04</pre></div>

    <h2>

    <a id="user-content-download-an-existing-singularity-container" class="anchor"
    href="#download-an-existing-singularity-container" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Download an existing Singularity container</h2>

    <p>A number of pre-built containers from this repository are also now

    hosted at Singularity Hub.</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull shub://mkandes/naked-singularity:ubuntu-18.04</pre></div>

    <h2>

    <a id="user-content-status" class="anchor" href="#status" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Status</h2>

    <p>A work in progress. But an important note on the immediate future ...</p>

    <p>The future availablity of Singularity Hub is currently unknown and

    may be slated for retirement as early as April 2021. If and when

    Singularity Hub is retired, the current set of naked-sinularity

    definition files will be affected due to the recent move in the past

    year to using a layered dependency chain of multiple containers built

    and hosted on Singularity Hub. Alternative container build and hosting

    options are currently under consideration. This upcoming change will

    likely affect how a given container''s dependency chain is rebuilt and

    supported long-term.</p>

    <h2>

    <a id="user-content-contribute" class="anchor" href="#contribute" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contribute</h2>

    <p>If you would like to contribute one of your own Singularity container

    definition files for a specific application OR request a modification to

    an existing container definition, then please submit a pull request.</p>

    <h2>

    <a id="user-content-author" class="anchor" href="#author" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Author</h2>

    <p>Marty Kandes, Ph.D.<br>

    Computational &amp; Data Science Research Specialist<br>

    High-Performance Computing User Services Group<br>

    San Diego Supercomputer Center<br>

    University of California, San Diego</p>

    <h2>

    <a id="user-content-version" class="anchor" href="#version" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Version</h2>

    <p>1.3.3</p>

    <h2>

    <a id="user-content-last-updated" class="anchor" href="#last-updated" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Last Updated</h2>

    <p>Saturday, March 13th, 2021</p>

    '
  stargazers_count: 30
  subscribers_count: 2
  topics: []
  updated_at: 1620436929.0
montilab/pipeliner:
  data_format: 2
  description: A flexible Nextflow-based framework for the definition of sequencing
    data processing pipelines
  filenames:
  - Singularity
  full_name: montilab/pipeliner
  latest_release: null
  readme: '<h1>

    <a id="user-content-pipeliner" class="anchor" href="#pipeliner" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Pipeliner</h1>

    <p><i>A flexible Nextflow-based framework for the definition of sequencing data
    processing pipelines</i></p>

    <p><a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/e2da22e8e369f5a60ec2e656caa3a42257dad04437afcb1bd670bf395f066899/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c74253230576974682d4e657874666c6f772d627269676874677265656e2e737667"
    alt="Built With" data-canonical-src="https://img.shields.io/badge/Built%20With-Nextflow-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/8a2216c3829e364a4b2d50a3b2b28b85bb2bfb8847e2345f341026872d9a9d11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4672616d65776f726b2d507974686f6e253230332e362d626c75652e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/8a2216c3829e364a4b2d50a3b2b28b85bb2bfb8847e2345f341026872d9a9d11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4672616d65776f726b2d507974686f6e253230332e362d626c75652e737667"
    alt="Python" data-canonical-src="https://img.shields.io/badge/Framework-Python%203.6-blue.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/0ab30b068b2dd5ec6fe37a2b6c2e0d722b12bc61d745bba31407499a217262f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6d7061746962696c6974792d4c696e75782532302532462532304f53582d6f72616e67652e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/0ab30b068b2dd5ec6fe37a2b6c2e0d722b12bc61d745bba31407499a217262f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6d7061746962696c6974792d4c696e75782532302532462532304f53582d6f72616e67652e737667"
    alt="Compatibility" data-canonical-src="https://img.shields.io/badge/Compatibility-Linux%20%2F%20OSX-orange.svg"
    style="max-width:100%;"></a>

    <a href="https://camo.githubusercontent.com/80a1153c429992993a5fc1d8009c2f9ed74f95263366dc21a2daec8fb25077c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646570656e64656e636965732d7570253230746f253230646174652d627269676874677265656e2e737667"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/80a1153c429992993a5fc1d8009c2f9ed74f95263366dc21a2daec8fb25077c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646570656e64656e636965732d7570253230746f253230646174652d627269676874677265656e2e737667"
    alt="Dependencies" data-canonical-src="https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/montilab/pipeliner/issues"><img src="https://camo.githubusercontent.com/aca381c660052e22bf17ac3b2f6d3a0035067b28a4e5746b27e7e2c9306b4cbf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d6f6e74696c61622f706970656c696e65722e737667"
    alt="GitHub Issues" data-canonical-src="https://img.shields.io/github/issues/montilab/pipeliner.svg"
    style="max-width:100%;"></a></p>

    <p align="left"><a href="https://raw.githubusercontent.com/montilab/pipeliner/master/media/framework_schematic.png"
    target="_blank" rel="nofollow"><img src="https://raw.githubusercontent.com/montilab/pipeliner/master/media/framework_schematic.png"
    width="80%" style="max-width:100%;"></a></p><p>

    </p><h2>

    <a id="user-content-features" class="anchor" href="#features" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Features</h2>

    <ul>

    <li>Modular directory structure: It is designed to generate automated result directory
    based on the names of the samples and tools used to process them.</li>

    <li>Platform independent: It is bundled with an anaconda repository which contains
    pre-compiled tools as well as pre-built environments that can be used directly.</li>

    <li>Modular architecture: It allows the expert users to customize, modify processes,
    or add additional tools based on their needs.</li>

    <li>Automated job parallelization, job recovery, and reproducibility.</li>

    </ul>

    <h2>

    <a id="user-content-quickstart" class="anchor" href="#quickstart" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quickstart</h2>

    <p><em>For more information, please refer to the <a href="https://pipeliner.readthedocs.io/en/latest/"
    rel="nofollow">full documentation</a></em></p>

    <h3>

    <a id="user-content-clone-repository" class="anchor" href="#clone-repository"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Clone
    Repository</h3>

    <div class="highlight highlight-source-shell"><pre>$ git clone https://github.com/montilab/pipeliner

    </pre></div>

    <h3>

    <a id="user-content-install-dependencies" class="anchor" href="#install-dependencies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Dependencies</h3>

    <div class="highlight highlight-source-shell"><pre>$ conda env create -f pipeliner/envs/linux_env.yml
    <span class="pl-c"><span class="pl-c">#</span> Linux</span></pre></div>

    <div class="highlight highlight-source-shell"><pre>$ conda env create -f pipeliner/envs/osx_env.yml
    <span class="pl-c"><span class="pl-c">#</span> Mac</span></pre></div>

    <h3>

    <a id="user-content-activate-conda-environment" class="anchor" href="#activate-conda-environment"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Activate
    Conda Environment</h3>

    <div class="highlight highlight-source-shell"><pre>$ <span class="pl-c1">source</span>
    activate pipeliner</pre></div>

    <h3>

    <a id="user-content-update-local-paths" class="anchor" href="#update-local-paths"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Update
    Local Paths</h3>

    <div class="highlight highlight-source-shell"><pre>$ python pipeliner/scripts/paths.py</pre></div>

    <h3>

    <a id="user-content-download-nextflow-executable" class="anchor" href="#download-nextflow-executable"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download
    Nextflow Executable</h3>

    <pre><code>$ cd pipeliner/pipelines

    $ curl -s https://get.nextflow.io | bash

    </code></pre>

    <h3>

    <a id="user-content-locally-run-example-data" class="anchor" href="#locally-run-example-data"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Locally
    Run Example Data</h3>

    <div class="highlight highlight-source-shell"><pre>$ ./nextflow rnaseq.nf -c rnaseq.config</pre></div>

    <h3>

    <a id="user-content-expected-output" class="anchor" href="#expected-output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Expected Output</h3>

    <pre lang="text"><code>N E X T F L O W  ~  version 0.31.1

    Launching `rnaseq.nf` [nasty_pauling] - revision: cd3f572ab2

    [warm up] executor &gt; local

    [31/1b2066] Submitted process &gt; pre_fastqc (ggal_alpha)

    [23/de6d60] Submitted process &gt; pre_fastqc (ggal_theta)

    [7c/28ee53] Submitted process &gt; pre_fastqc (ggal_gamma)

    [97/9ad6c1] Submitted process &gt; check_reads (ggal_alpha)

    [ab/c3eedf] Submitted process &gt; check_reads (ggal_theta)

    [2d/050633] Submitted process &gt; check_reads (ggal_gamma)

    [1d/f3af6d] Submitted process &gt; pre_multiqc

    [32/b1db1d] Submitted process &gt; hisat_indexing (genome_reference.fa)

    [3b/d93c6d] Submitted process &gt; trim_galore (ggal_alpha)

    [9c/3fa50b] Submitted process &gt; trim_galore (ggal_theta)

    [62/25fce0] Submitted process &gt; trim_galore (ggal_gamma)

    [66/ccc9db] Submitted process &gt; hisat_mapping (ggal_alpha)

    [28/69fff5] Submitted process &gt; hisat_mapping (ggal_theta)

    [5c/5ed2b6] Submitted process &gt; hisat_mapping (ggal_gamma)

    [b4/e559ab] Submitted process &gt; gtftobed (genome_annotation.gtf)

    [bc/6f490c] Submitted process &gt; rseqc (ggal_alpha)

    [71/80aa9e] Submitted process &gt; rseqc (ggal_theta)

    [17/ca0d9f] Submitted process &gt; rseqc (ggal_gamma)

    [d7/7d391b] Submitted process &gt; counting (ggal_alpha)

    [df/936854] Submitted process &gt; counting (ggal_theta)

    [11/143c2c] Submitted process &gt; counting (ggal_gamma)

    [31/4c11f9] Submitted process &gt; expression_matrix

    [1f/3af548] Submitted process &gt; multiqc

    Success: Pipeline Completed!

    </code></pre>

    <p><em>Please refer to <a href="https://github.com/montilab/pipeliner/blob/master/docs/reports.md">reports</a>
    for examples</em></p>

    <h2>

    <a id="user-content-cite" class="anchor" href="#cite" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cite</h2>

    <p>Federico A, Karagiannis T, Karri K, Kishore D, Koga Y, Campbell J, Monti S
    (2019) Pipeliner: A Nextflow-based framework for the definition of sequencing
    data processing pipelines. <em>Frontiers in Genetics</em>. <a href="https://doi.org/10.3389/fgene.2019.00614"
    rel="nofollow">https://doi.org/10.3389/fgene.2019.00614</a>.</p>

    '
  stargazers_count: 35
  subscribers_count: 6
  topics:
  - nextflow
  - workflow
  - bioinformatics
  - rna-seq
  - computational-biology
  updated_at: 1615524762.0
mticlla/OmicSingularities:
  data_format: 2
  description: A set of recipies to build Singularity containers for analysis of omic
    data.
  filenames:
  - metaProf/Singularity.MetaProf
  - meta16S/Singularity.meta16S
  - preQC/Singularity.preQC
  - preQC/Singularity.preQC_v0_1
  full_name: mticlla/OmicSingularities
  latest_release: null
  readme: '<h1>

    <a id="user-content-omicsingularities" class="anchor" href="#omicsingularities"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OmicSingularities</h1>

    <p>A set of recipies to build Singularity containers for analysis of omic data.
    In addition, pre-built containers with these recipies can be downloaded from SingularityHub</p>

    <ul>

    <li>

    <a href="https://github.com/mticlla/OmicSingularities/tree/master/preQC"><strong>Pre-processing
    QC (preQC)</strong></a>: a containerized environment with software for quality
    control and preprocessing of sequencing data. Resulting image is built with CentOS
    7 as base OS and Python 3.6. Although this is a companion container of the MetagenomicSnake
    workflow, it can also be used separately.</li>

    <li>

    <a href="https://github.com/mticlla/OmicSingularities/tree/master/meta16S"><strong>meta16S</strong></a>
    : a containerized environment for 16S-rRNA-gene sequencing data analysis with
    QIIME2.</li>

    <li>

    <a href="https://github.com/mticlla/OmicSingularities/blob/master/metaProf/README.md"><strong>MetaProf</strong></a>:
    a containerized/singularized environment for taxonomic and functional profiling
    of metagenomic shotgun data.</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1619998936.0
mvdoc/singularity-def:
  data_format: 2
  description: Singularity definition files for various projects
  filenames:
  - hauntedhouse/Singularity
  - miniconda/Singularity
  - hauntedhouse_freesurfer/Singularity
  full_name: mvdoc/singularity-def
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-recipe-files" class="anchor" href="#singularity-recipe-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    recipe files</h1>

    <p><a href="https://github.com/sylabs/singularity">Singularity</a> containers
    I use the most on HPC clusters.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1495651655.0
mwever/tpami-automlc:
  data_format: 2
  description: null
  filenames:
  - SingularityRecipe
  full_name: mwever/tpami-automlc
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-automl-for-multi-label-classification-overview-and-empirical-evaluation\"\
    \ class=\"anchor\" href=\"#automl-for-multi-label-classification-overview-and-empirical-evaluation\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>AutoML for Multi-Label Classification: Overview and Empirical Evaluation</h1>\n\
    <p>This project provides a platform for benchmarking different optimizers for\
    \ the task of automated machine learning ensuring all optimizers to work on the\
    \ same set of potential solution candidates. The implementation is based on the\
    \ Java open-source library <a href=\"https://github.com/starlibs/AILibs\">AILibs</a>,\
    \ providing the basic technical support for describing search spaces, HTN planning\
    \ and heuristic search algorithms, as well as the infrastructure for synchronizing\
    \ the execution of a benchmarking suite on a distributed system.</p>\n<p>The benchmark\
    \ distinguishes itself from previously published benchmark in the way how the\
    \ optimizers are integrated with the benchmarking system. While all optimizers\
    \ share the same routine for executing candidate solutions, the benchmark works\
    \ in a cross-platform fashion, i.e. although the benchmark and the execution of\
    \ candidate solutions is implemented in Java, optimizers available in Python can\
    \ be benchmarked within the system. The search space and recursive structures\
    \ of the search space are automatically translated into a format understandable\
    \ to the respective optimizers. The inter-platform communication is done via the\
    \ <a href=\"https://developers.google.com/protocol-buffers\" rel=\"nofollow\"\
    >Google ProtoBuf</a> library which offers interfaces for various platforms. Thereby,\
    \ the communication link only transfers the execution request to the benchmarking\
    \ system allowing to share the same evaluation routine for all the optimizers.\
    \ Another advantage is that it also allows for live-tracking the performance of\
    \ the optimizers, logging each evaluated candidate and monitoring the current\
    \ incumbent at any point in time.</p>\n<h2>\n<a id=\"user-content-table-of-contents\"\
    \ class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Table of Contents</h2>\n<ol>\n\
    <li>\n<a href=\"#quickstart---setup\">Quickstart - Setup</a>\n<ol>\n<li><a href=\"\
    #preparing-the-singularity-container\">Preparing the Singularity Container</a></li>\n\
    <li><a href=\"#without-a-singularity-container\">Without a Singularity Container</a></li>\n\
    <li><a href=\"#test-your-setup\">Test your Setup</a></li>\n</ol>\n</li>\n<li>\n\
    <a href=\"#running-the-benchmark\">Running the Benchmark</a>\n<ol>\n<li><a href=\"\
    #hardware-requirements\">Hardware Requirements</a></li>\n<li><a href=\"#initialize-the-database-server\"\
    >Initialize the Database Server</a></li>\n<li><a href=\"#running-a-worker-client\"\
    >Running a Worker Client</a></li>\n<li><a href=\"#post-processing-for-anytime-test-performances\"\
    >Post-Processing for Anytime Test Performances</a></li>\n</ol>\n</li>\n<li>\n\
    <a href=\"#visualizing-benchmark-data\">Visualizing Benchmark Data</a>\n<ol>\n\
    <li>\n<a href=\"#inspecting-the-search-space\">Inspecting the Search Space</a>\n\
    <ol>\n<li><a href=\"#list-algorithms-contained-in-the-search-space\">List Algorithms\
    \ Contained in the Search Space</a></li>\n<li><a href=\"#export-to-gephi-to-visualize-the-search-space-as-a-dag\"\
    >Export to Gephi to visualize the search space as a DAG</a></li>\n<li><a href=\"\
    #generate-html-overview\">Generate HTML Overview</a></li>\n</ol>\n</li>\n<li>\n\
    <a href=\"#evaluation-results\">Evaluation Results</a>\n<ol>\n<li><a href=\"#generate-one-vs-rest-scatter-plots\"\
    >Generate One-VS-Rest Scatter Plots</a></li>\n<li><a href=\"#generate-anytime-average-rank-plots\"\
    >Generate Anytime Average Rank Plots</a></li>\n<li><a href=\"#generate-result-tables\"\
    >Generate Result Tables</a></li>\n<li><a href=\"#generate-incumbent-frequency-statistics\"\
    >Generate Incumbent Frequency Statistics</a></li>\n</ol>\n</li>\n</ol>\n</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-quickstart---setup\" class=\"anchor\" href=\"\
    #quickstart---setup\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Quickstart - Setup</h2>\n<p>Prerequisites: Due\
    \ to certain dependencies requiring a Linux operating system, the execution of\
    \ optimizers in Python is not supported for Windows nor MacOS. However, optimizers\
    \ available in Java can still be executed on Windows or MacOS. Running a Linux\
    \ OS, you may execute Python optimizers as well. To this end, please use the SingularityRecipe\
    \ to build a container in order to ensure all the dependencies necessary for running\
    \ the benchmark are available.</p>\n<h3>\n<a id=\"user-content-preparing-the-singularity-container\"\
    \ class=\"anchor\" href=\"#preparing-the-singularity-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Preparing\
    \ the Singularity Container</h3>\n<p>In order to set up a Singularity Container,\
    \ please ensure that you have administrator permissions and installed the <a href=\"\
    https://sylabs.io/guides/3.6/user-guide/\" rel=\"nofollow\">Singularity Container</a>\
    \ software on your computer. (Hint: On Ubuntu you can simply install it via <code>sudo\
    \ apt install singularity-container</code>). Once you have Singularity Container\
    \ installed on your system, follow these steps in order to create the container:</p>\n\
    <ol>\n<li>Build a singularity container from the provided recipe file: <code>sudo\
    \ singularity build automlc.sif SingularityRecipe</code>.</li>\n<li>Once the container\
    \ is built, you can now proceed to run a shell within the Singularity container\
    \ like this: <code>singularity shell automlc.sif</code> (No sudo this time).</li>\n\
    <li>Please make sure that your current folder is mounted into the Singularity\
    \ container. You may prove this by typing <code>dir</code>. If the files of the\
    \ project's root directory are printed on your command line, everything should\
    \ be fine.</li>\n<li>You are now prepared to run the tasks via the gradle wrapper.\
    \ For further steps please have a look at the subsequent documentation.</li>\n\
    </ol>\n<h3>\n<a id=\"user-content-without-a-singularity-container\" class=\"anchor\"\
    \ href=\"#without-a-singularity-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Without a Singularity Container</h3>\n\
    <p>If you are running Linux you also have the possibility to run the benchmark\
    \ directly on your system, without creating any Singularity container.\nTo this\
    \ end, we have prepared a <code>requirements.txt</code> file so that you can install\
    \ the required dependencies for the Python environment with ease.\nHowever, since\
    \ we mainly work with Singularity containers to have a clearly distinct system\
    \ running, preventing interferences with inappropriate python versions and compatibility\
    \ conflicts in general, we do not officially support the setup variant without\
    \ creating a Singularity container.</p>\n<h3>\n<a id=\"user-content-test-your-setup\"\
    \ class=\"anchor\" href=\"#test-your-setup\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Test your Setup</h3>\n<p>In order\
    \ to test your setup we have prepared a test runner that will work out-of-the-box\
    \ if everything has been setup correctly.\nMore precisely, you can test whether\
    \ each of the optimizers can be run for a specific dataset split with short timeouts\
    \ of 1 minute for the entire optimization run and 45 seconds for a single evaluation.</p>\n\
    <p>You can test to run each optimizer individually via the following commands:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./gradlew testHTNBF\n./gradlew\
    \ testBOHB\n./gradlew testHB\n./gradlew testSMAC\n./gradlew testRandomSearch\n\
    ./gradlew testGGP</pre></div>\n<p>As a shortcut you can also simply test all the\
    \ optimizers as follows:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>./gradlew testAllOptimizers</pre></div>\n<h2>\n<a id=\"user-content-running-the-benchmark\"\
    \ class=\"anchor\" href=\"#running-the-benchmark\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running the Benchmark</h2>\n\
    <p>The benchmark implemented in this repository is meant to be run in a distributed\
    \ way.</p>\n<h3>\n<a id=\"user-content-hardware-requirements\" class=\"anchor\"\
    \ href=\"#hardware-requirements\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Hardware Requirements</h3>\n<p>For\
    \ running a benchmark suite you need the following resources:\nA central database\
    \ server managing the experiments to be executed and worker clients meeting the\
    \ hardware requirements.\nIn the paper we used worker clients each equipped with\
    \ <code>8 CPU cores</code> and <code>32GB RAM</code>.\nThe recommended hardware\
    \ specifications for the database server depends on the degree of parallelization\
    \ and how you set the parameters for the experiments.\nThe latter point is for\
    \ instance depending what evaluation timeouts you choose for assessing the performance\
    \ of single candidates.\nThe smaller the timeout the more intermediate evaluation\
    \ results will be logged in the database.\nAs a consequence there will be a higher\
    \ load on the respective database server.\nIn our study and with the timeout configuration\
    \ proposed in the paper, we found that a configuration of <code>4 CPU cores</code>\
    \ and <code>16GB RAM</code> is sufficient to deal with up to 200 worker clients.\n\
    For the database, we tested only a MySQL database. In principle other drivers\
    \ are usable, but may require the inclusion of additional dependencies for the\
    \ project.\nOfficially, we only support MySQL databases.</p>\n<h3>\n<a id=\"user-content-initialize-the-database-server\"\
    \ class=\"anchor\" href=\"#initialize-the-database-server\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Initialize\
    \ the Database Server</h3>\n<p>In order to initialize the database server, first\
    \ of all, you need to fill in the connection details in the <code>automlc-setup.properties</code>\
    \ file.</p>\n<div class=\"highlight highlight-source-ini\"><pre>...\n<span class=\"\
    pl-k\">db.driver</span> = mysql\n<span class=\"pl-k\">db.host</span> = &lt;YOUR\
    \ DB HOST&gt;\n<span class=\"pl-k\">db.username</span> = &lt;YOUR DB USER&gt;\n\
    <span class=\"pl-k\">db.password</span> = &lt;YOUR DB PASSWORD&gt;\n<span class=\"\
    pl-k\">db.database</span> = &lt;YOUR DATABASE NAME&gt;\n<span class=\"pl-k\">db.table</span>\
    \ = &lt;YOUR JOBS TABLE NAME&gt;\n<span class=\"pl-k\">db.ssl</span> = &lt;FLAG\
    \ WHETHER TO USE SSL&gt;\n<span class=\"pl-k\">candidate_eval_table</span> = &lt;YOUR\
    \ INTERMEDIATE EVALUATIONS TABLE NAME&gt;\n...</pre></div>\n<p>The specifics of\
    \ the benchmark are then given with the following properties in the same files:</p>\n\
    <div class=\"highlight highlight-source-ini\"><pre><span class=\"pl-k\">mem.max</span>\
    \ = 32768 // maximum available memory\n<span class=\"pl-k\">cpu.max</span> = 8\
    \ // number of cores\n\n... // database connection properties and AILibs experimenter\
    \ specific properties \n\n\n<span class=\"pl-k\">algorithm</span> = bf,random,hb,bohb,smac,ggp\
    \ // optimizers to consider\n<span class=\"pl-k\">dataset</span> = arts1,bibtex,birds,bookmarks,business1,computers1,education1,emotions,enron-f,entertainment1,flags,genbase,health1,llog-f,mediamill,medical,recreation1,reference1,scene,science1,social1,society1,tmc2007,yeast\
    \ // datasets to consider\n<span class=\"pl-k\">measure</span> = FMacroAvgD,FMacroAvgL,FMicroAvg\
    \ // performance measures to consider\n<span class=\"pl-k\">split</span> = 0,1,2,3,4,5,6,7,8,9\
    \ // split indices to consider\n<span class=\"pl-k\">seed</span> = 42 // seed\
    \ of the dataset splitter to consider\n<span class=\"pl-k\">globalTimeout</span>=86400\
    \ // timeout for an entire optimization run\n<span class=\"pl-k\">evaluationTimeout</span>=1800\
    \ // timeout for a single candidate evaluation\n\n... // constant properties for\
    \ the experiment runner</pre></div>\n<p>Based on this specification, the benchmark\
    \ will compute all possible combinations of entires given via the fields <code>algorithm</code>,\
    \ <code>dataset</code>, <code>measure</code>, <code>split</code>, <code>seed</code>,\
    \ <code>globalTimeout</code>, <code>evaluationTimeout</code>.\nIn principle it\
    \ is also possible to configure multiple seeds, globalTimeouts or evaluationTimeouts\
    \ in the same style as it is done e.g. for the algorithm field, i.e. simply by\
    \ seperating multiple values by a comma.</p>\n<p>Once the database connection\
    \ is configured and the property values for all the benchmark suite specific parameters\
    \ have been set, you can proceed by initializing the database server centrally\
    \ managing the experiment conduction with the following command:</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>./gradlew initializeExperimentsInDatabase</pre></div>\n\
    <p>This will automatically create a table with the name specified in the <code>db.table</code>\
    \ property which will then specify all the experiments to be executed which have\
    \ been computed via taking the cross-product of all possible combinations of the\
    \ properties describing the benchmark suite.</p>\n<p>For cleaning this table,\
    \ i.e., removing all of its entries, you can run the following command:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./gradlew cleanExperimentsInDatabase</pre></div>\n\
    <p><strong>Caution:</strong> This functionality will also remove all results that\
    \ are stored in this table after running an experiment.</p>\n<h3>\n<a id=\"user-content-preparing-dataset-splits\"\
    \ class=\"anchor\" href=\"#preparing-dataset-splits\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Preparing Dataset\
    \ Splits</h3>\n<p>The original datasets from which the train and test splits have\
    \ been derived are provided via this repository as well.\nThe datasets are located\
    \ in the <code>original_datasets/</code> directory.\nIn order to derive the train\
    \ and test dataset splits via a 10-fold cross-validation, run the following command:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./gradlew generateDatasetSplits</pre></div>\n\
    <p>This will create a directory <code>datasets/</code>, where the generated train\
    \ and test splits are stored in seperated files.\nAs this procedure is done for\
    \ all datasets contained in the <code>original_datasets/</code> directory and\
    \ seed combinations, this probably takes some time and disk space.\nThe generated\
    \ files follow the name schema <code>&lt;DATASET&gt;_&lt;SEED&gt;_&lt;SPLIT INDEX&gt;_{train|test}.arff</code>.\
    \ As the worker client relies on this naming, the schema must not be changed.\n\
    Unfortunately, we cannot provide the dataset splits used in our study directly,\
    \ as this would dramatically increase the size of the repository and lead to unreasonable\
    \ download times for cloning.\nHowever, on request we can of course provide the\
    \ original dataset splits.</p>\n<p><strong>Note</strong>: We assume that all worker\
    \ clients either have all the dataset splits locally available or share a network\
    \ hard drive, centrally providing access to the respective dataset splits. The\
    \ dataset folder can be configured in the <code>automlc-setup.properties</code>\
    \ file via the property <code>datasetFolder</code>.</p>\n<h3>\n<a id=\"user-content-running-a-worker-client\"\
    \ class=\"anchor\" href=\"#running-a-worker-client\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running a Worker\
    \ Client</h3>\n<p>Once everything is set up correctly, you may run a worker client\
    \ via the command</p>\n<div class=\"highlight highlight-source-shell\"><pre>./gradlew\
    \ runExperimentEvaluationWorker</pre></div>\n<p>The execution of the worker will\
    \ also rely on the database connection configured in the <code>automlc-setup.properties</code>.\n\
    However, there is nothing specific you need to configure when deploying the worker\
    \ client in a distributed way.\nIn fact, you can simply run the same command multiple\
    \ times (on different nodes) in order to parallelize the processing of the benchmark.</p>\n\
    <p>The central database server will take care that each of the specified experiments\
    \ will be executed only once at maximum, i.e. it will prevent the same experiment\
    \ from being conducted twice.\nSince one worker client will also only take care\
    \ of running a single experiment, you need to deploy as many worker clients as\
    \ there are rows in the jobs table (named as you configured it in the properties\
    \ file). In addition to the final results, the worker clients will also store\
    \ intermediate evaluation results, i.e., candidates that have been requested for\
    \ evaluation by the respective optimizer together with the measured performance\
    \ value.</p>\n<h3>\n<a id=\"user-content-post-processing-for-anytime-test-performances\"\
    \ class=\"anchor\" href=\"#post-processing-for-anytime-test-performances\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Post-Processing\
    \ for Anytime Test Performances</h3>\n<p>Since assessing the test performances\
    \ on-line, i.e. during an optimizer run, would distort the overall perception\
    \ of the optimization performance, all the test performances for later on compiling\
    \ anytime plots etc. have been estimated via a post-processing step based on the\
    \ log of intermediate candidate evaluations.\nThe post-processing is also implemented\
    \ in a distributed way, analoguous to the already explained setup for running\
    \ the actual benchmark of optimizers. In contrast to the latter, for the post-processing\
    \ the <code>test-eval.properties</code> serves as a configuration.\nObviously,\
    \ the jobs table for the post-processing needs a different name than the one specified\
    \ for the benchmark itself. Furthermore, it will also use the <code>automlc-setup.properties</code>\
    \ file for accessing the logged data and filtering the evaluated candidates.\n\
    Additionally, the post-processing requires access to the datasets directory.</p>\n\
    <p>As before you can configure the corresponding properties, for which optimizers,datasets,\
    \ measures, etc. you want to run the post-processing. Furthermore you can setup\
    \ the jobs table for distributing the workload on a cluster etc. as already described\
    \ before for the benchmark via the following commands:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>./gradlew initializePostProcessingsInDatabase\n\
    ./gradlew cleanPostProcessingsInDatabase\n./gradlew runPostProcessingWorker</pre></div>\n\
    <h2>\n<a id=\"user-content-visualizing-benchmark-data\" class=\"anchor\" href=\"\
    #visualizing-benchmark-data\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Visualizing Benchmark Data</h2>\n\
    <p>Throughout the paper, several visualization of the results have been presented.\
    \ A pre-processed version of the logged data is contained in the directory <code>results/data/</code>.\n\
    From this data, you can further process the data to derive the numbers presented\
    \ in the paper. Plots that have been generated via LaTeX's <code>tikz</code> or\
    \ <code>pgfplots</code>, you can even generate the corresponding LaTeX code in\
    \ the following.</p>\n<h3>\n<a id=\"user-content-inspecting-the-search-space\"\
    \ class=\"anchor\" href=\"#inspecting-the-search-space\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Inspecting\
    \ the Search Space</h3>\n<p>The configuration files containing the specification\
    \ of the search space are contained in the folder <code>searchspace/</code>. For\
    \ this, the root file is <code>searchspace/meka-all.json</code> and (recursively)\
    \ includes the remaining configuration files</p>\n<h4>\n<a id=\"user-content-list-algorithms-contained-in-the-search-space\"\
    \ class=\"anchor\" href=\"#list-algorithms-contained-in-the-search-space\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>List\
    \ Algorithms Contained in the Search Space</h4>\n<p>If you only want to obtain\
    \ a list of algorithms ordered by algorithm type that are contained in the search\
    \ space including their abbreviation as given in the paper, you can use the following\
    \ short cut:</p>\n<div class=\"highlight highlight-source-shell\"><pre>./gradlew\
    \ exportAlgorithmsInSearchSpace</pre></div>\n<p>This will create a txt file in\
    \ the <code>/results</code> directory with the name <code>searchspace-algorithms-in-space.txt</code>\
    \ listing all the algorithm types together with the algorithms belonging to these\
    \ types.</p>\n<h4>\n<a id=\"user-content-export-to-gephi-to-visualize-the-search-space-as-a-dag\"\
    \ class=\"anchor\" href=\"#export-to-gephi-to-visualize-the-search-space-as-a-dag\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Export to Gephi to visualize the search space as a DAG</h4>\n<p><a\
    \ href=\"https://gephi.org/\" rel=\"nofollow\">Gephi</a> is a graph modeling and\
    \ visualization tool. You can export the search space as described in the <code>searchspace/</code>\
    \ folder to the Gephi graph format to be loaded and visualized in Gephi. By running</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./gradlew exportSearchSpacesToGephiFormat</pre></div>\n\
    <p>you will find a folder <code>results/gephi-export</code>, containing three\
    \ files: <code>slc-only.gephi</code>, <code>mlc-only.gephi</code>, and <code>mlc-complete.gephi</code>.\
    \ These three files contain a Gephi specification of a directed acyclic graph\
    \ (DAG) where each node corresponds to one algorithm in the search space and an\
    \ edge a dependency relation from one to another algorithm indicating that the\
    \ algorithm represented by the parent node can be configured with the child node\
    \ as a base learner/kernel. With these graph visualizations one can easily see\
    \ the exponential growth of the search space when combining the single-label classification\
    \ algorithms with the multi-label classification algorithms.</p>\n<h4>\n<a id=\"\
    user-content-generate-html-overview\" class=\"anchor\" href=\"#generate-html-overview\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Generate HTML Overview</h4>\n<p>Generate an HTML document summarizing\
    \ the search space in terms of some statistics, listing all included algorithms\
    \ together with their hyper-parameters (including their types, domains, and defaults)\
    \ and recursive dependencies on other algorithms. The possible choices for each\
    \ dependency are listed and linked within the document accordingly.</p>\n<div\
    \ class=\"highlight highlight-source-shell\"><pre>./gradlew generateMultiLabelSearchSpaceDescription</pre></div>\n\
    <p>This will generate a file <code>results/searchspace-meka.html</code>. If you\
    \ want to generate the same type of description for the single-label classification\
    \ (WEKA) search space use the following.</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>./gradlew generateSingleLabelSearchSpaceDescription</pre></div>\n<p>This\
    \ will generate a file <code>results/searchspace-weka.html</code>.</p>\n<p>The\
    \ bar charts comparing the two search spaces in the paper have been generated\
    \ from the statistical values contained on the top of these HTML documents. However,\
    \ since the plots have been generated using <code>matplotlibs</code> in Python,\
    \ please refer to the Jupyter notebook <code>TPAMI Plots.ipynb</code> for deriving\
    \ the bar charts from these statistics.\nTherewith, you can obtain the comparison\
    \ figure for the different search spaces.</p>\n<h3>\n<a id=\"user-content-evaluation-results\"\
    \ class=\"anchor\" href=\"#evaluation-results\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Evaluation Results</h3>\n<p>The\
    \ data obtained by running the benchmark across various datasets and folds can\
    \ be found in <code>results/data</code>. From this data you can generate several\
    \ statistics, summaries, and plots. How these can be generated is explained in\
    \ the following.</p>\n<h4>\n<a id=\"user-content-generate-one-vs-rest-scatter-plots\"\
    \ class=\"anchor\" href=\"#generate-one-vs-rest-scatter-plots\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Generate\
    \ One-VS-Rest Scatter Plots</h4>\n<p>In the paper we compared the different optimizers\
    \ in a one-vs-rest fashion plotting their performances against each other facilitating\
    \ the analysis which approach performs preferably over the rest. The plots can\
    \ be generated with the following command:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>./gradlew compileScatterPlots</pre></div>\n<p>The command will produce its\
    \ output in the folder <code>results/scatter-plots</code>, where afterwards you\
    \ will find for each optimizer and performance measure (instance-wise F-measure,\
    \ label-wise F-measure, and micro F-measure) you will find a LaTeX file named\
    \ like this: <code>scatter-&lt;optimizer&gt;VSrest-&lt;measure&gt;.tex</code>.\
    \ Furthermore you can find a <code>main.tex</code> which will include all packages\
    \ and scatter plots to compile them into a PDF document.</p>\n<h4>\n<a id=\"user-content-generate-anytime-average-rank-plots\"\
    \ class=\"anchor\" href=\"#generate-anytime-average-rank-plots\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Generate\
    \ Anytime Average Rank Plots</h4>\n<p>Furthermore, we have presented anytime average\
    \ rank plots comparing the different optimizers across the time dimension. You\
    \ can compile the result data into these anytime plots by executing the following\
    \ command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>./gradlew\
    \ compileAnytimeAverageRankPlots</pre></div>\n<p>This will generate several <code>.tex</code>-files\
    \ in the directory <code>results/anytime-plots/</code>.\nIn fact, there are even\
    \ two different types of plots: First following the naming schema <code>avgrank-&lt;MEASURE&gt;.tex</code>\
    \ you will find the average rank plots as presented in the paper.\nHowever, for\
    \ each combination of measure and dataset you can also find the actual average\
    \ anytime performance of the different optimizers contained in the files named\
    \ after the schema <code>&lt;MEASURE&gt;_&lt;DATASET&gt;.tex</code>.\nSince presenting\
    \ those would have required lots of space, these plots have not been included\
    \ in the paper but are made available here as a kind of supplementary material.</p>\n\
    <h4>\n<a id=\"user-content-generate-result-tables\" class=\"anchor\" href=\"#generate-result-tables\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Generate Result Tables</h4>\n<p>For a comparison of the performance\
    \ of eventually returned incumbents, we presented tables for each measure individually.\n\
    You can generate these tables yourself once again by running the following command:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./gradlew compileResultTables</pre></div>\n\
    <p>This will again generate <code>.tex</code>-files containing the LaTeX code\
    \ to represent the corresponding table data. In addition to the average performances\
    \ per dataset and optimizer, a Wilcoxon signed-rank test is conducted with a threshold\
    \ for the p-value of 0.05. The best results, significant improvements and degradations\
    \ are highlighted as described in the paper. Furthermore, in the last row of the\
    \ tables, an average rank for each optimizer across all datasets is given.</p>\n\
    <h4>\n<a id=\"user-content-generate-incumbent-frequency-statistics\" class=\"\
    anchor\" href=\"#generate-incumbent-frequency-statistics\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Generate\
    \ Incumbent Frequency Statistics</h4>\n<p>Another figure in the paper shows which\
    \ algorithms have been chosen with what frequency by which optimizer. Since these\
    \ plots have been generated with the help of matplotlibs, here, we will only compile\
    \ the necessary statistics from the data, necessary to produce the figures.\n\
    In order to compile the statistics from the result data, run the following command:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>./gradlew compileIncumbentFrequencyStatistics</pre></div>\n\
    <p>This will generate a txt file <code>results/incumbent-frequencies.txt</code>\
    \ containing JSON arrays that can be copied and pasted into a Jupyter notebook\
    \ which is also available via this repository. Please refer to the <code>TPAMI\
    \ Plots.ipynb</code> notebook file for further processing of the compiled raw\
    \ data into the nested donut charts.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611333170.0
nasibehm/LBP3d:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: nasibehm/LBP3d
  latest_release: null
  readme: '<h1>

    <a id="user-content-lbp3d" class="anchor" href="#lbp3d" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>LBP3d</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619057271.0
nbarlowATI/shub-test:
  data_format: 2
  description: Test using singularityhub
  filenames:
  - Singularity
  - Singularity.basic
  - Singularity.centostest
  full_name: nbarlowATI/shub-test
  latest_release: null
  readme: '<h1>

    <a id="user-content-shub-test" class="anchor" href="#shub-test" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>shub-test</h1>

    <p>Test using singularityhub</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1617913070.0
netneurolab/markello_ppmisnf:
  data_format: 2
  description: Code supporting the recent preprint Markello et al., 2020
  filenames:
  - container/Singularity
  full_name: netneurolab/markello_ppmisnf
  latest_release: '0.1'
  readme: '<h1>

    <a id="user-content-similarity-network-fusion-in-parkinsons-disease" class="anchor"
    href="#similarity-network-fusion-in-parkinsons-disease" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Similarity network
    fusion in Parkinson''s disease</h1>

    <p><a href="https://zenodo.org/badge/latestdoi/245268776" rel="nofollow"><img
    src="https://camo.githubusercontent.com/b54624e2e59ecddb7b78002cff4e39e47013c88ff0d58f1944cc28f16185036b/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3234353236383737362e737667"
    alt="DOI" data-canonical-src="https://zenodo.org/badge/245268776.svg" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-whats-in-this-repository" class="anchor" href="#whats-in-this-repository"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"What''s
    in this repository?"</h2>

    <p>This repository contains data, code, and results for the manuscript "<a href="https://www.biorxiv.org/content/10.1101/2020.03.05.979526v1"
    rel="nofollow">Integrated morphometric, molecular, and clinical characterization
    of Parkinson''s disease pathology</a>."

    The study examines the application of similarity network fusion to Parkinson''s
    disease using data from the <a href="https://www.ppmi-info.org" rel="nofollow">Parkinson''s
    Progression Markers Initiative</a> (PPMI).</p>

    <p>We''ve tried to document the various aspects of this repository with a whole
    bunch of README files, so feel free to jump around and check things out.</p>

    <h2>

    <a id="user-content-just-let-me-run-the-things" class="anchor" href="#just-let-me-run-the-things"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"Just
    let me run the things!"</h2>

    <p>Itching to just run the analyses?

    You''ll need to make sure you have <a href="./walkthrough/01_accessing_data.md">access
    to the PPMI</a> and have set the appropriate environmental variables (<code>$PPMI_USER</code>)
    and (<code>$PPMI_PASSWORD</code>).

    Once you''ve done that, you can get going with the following:</p>

    <div class="highlight highlight-source-shell"><pre>git clone --recurse-submodules
    https://github.com/netneurolab/markello_ppmisnf

    <span class="pl-c1">cd</span> markello_ppmisnf

    pip install -r requirements.txt

    <span class="pl-k">export</span> PYTHONPATH=<span class="pl-smi">$PYTHONPATH</span>:<span
    class="pl-smi">$PWD</span>/code

    make all</pre></div>

    <p>If you don''t want to deal with the hassle of creating a new Python environment,
    download the Singularity image that we used to run our analyses and run things
    in there:</p>

    <div class="highlight highlight-source-shell"><pre>git clone --recurse-submodules
    https://github.com/netneurolab/markello_ppmisnf

    <span class="pl-c1">cd</span> markello_ppmisnf

    wget -O container/ppmi_snf.simg https://osf.io/h6jwx/download

    bash container/run.sh

    make all</pre></div>

    <h2>

    <a id="user-content-i-want-to-take-things-slow" class="anchor" href="#i-want-to-take-things-slow"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"I
    want to take things slow."</h2>

    <p>If you want a step-by-step through all the methods + analyses, take a look
    at out our <a href="./walkthrough">walkthrough</a>.</p>

    <h2>

    <a id="user-content-i-have-some-questions" class="anchor" href="#i-have-some-questions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"I
    have some questions..."</h2>

    <p><a href="https://github.com/netneurolab/markello_ppmisnf/issues">Open an issue</a>
    on this repository and someone will try and get back to you as soon as possible!</p>

    '
  stargazers_count: 10
  subscribers_count: 5
  topics: []
  updated_at: 1621527226.0
netneurolab/markello_spatialnulls:
  data_format: 2
  description: Code and documentation supporting Markello & Misic, 2021, "Comparing
    spatial null models for brain maps" (NeuroImage)
  filenames:
  - container/Singularity
  full_name: netneurolab/markello_spatialnulls
  latest_release: '0.2'
  readme: '<h1>

    <a id="user-content-spatially-constrained-null-models-in-neuroimaging" class="anchor"
    href="#spatially-constrained-null-models-in-neuroimaging" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Spatially-constrained
    null models in neuroimaging</h1>

    <h2>

    <a id="user-content-whats-in-this-repository" class="anchor" href="#whats-in-this-repository"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"What''s
    in this repository?"</h2>

    <p>This repository contains data, code, and results for the manuscript "<a href="https://doi.org/10.1016/j.neuroimage.2021.118052"
    rel="nofollow">Comparing spatial null models for brain maps</a>" by Ross Markello
    &amp; Bratislav Misic (<em>NeuroImage</em>, 2021).

    We investigated how well different null model implementations account for spatial
    autocorrelation in statistical analyses of whole-brain neuroimaging data.</p>

    <p>We''ve tried to document the various aspects of this repository with a whole
    bunch of README files, so feel free to jump around and check things out.</p>

    <h2>

    <a id="user-content-just-let-me-run-the-things" class="anchor" href="#just-let-me-run-the-things"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"Just
    let me run the things!"</h2>

    <p>Itching to just run the analyses?

    You''ll need to make sure you have installed the appropriate software packages,
    have access to the HCP, and have downloaded the appropriate data files (check
    out our <a href="https://netneurolab.github.io/markello_spatialnulls" rel="nofollow">walkthrough</a>
    for more details!).

    Once you''ve done that, you can get going with the following:</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/netneurolab/markello_spatialnulls

    <span class="pl-c1">cd</span> markello_spatialnulls

    conda env create -f environment.yml

    conda activate markello_spatialnulls

    pip install parspin/

    make all</pre></div>

    <p>If you don''t want to deal with the hassle of creating a new Python environment,
    download the Singularity image that we used to run our analyses and run things
    in there:</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/netneurolab/markello_spatialnulls

    <span class="pl-c1">cd</span> markello_spatialnulls

    wget -O container/markello_spatialnulls.simg https://osf.io/za7fn/download

    singularity run container/markello_spatialnulls.simg make all</pre></div>

    <h2>

    <a id="user-content-id-like-more-information" class="anchor" href="#id-like-more-information"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"I''d
    like more information."</h2>

    <p>If you want a step-by-step through all the methods + analyses, take a look
    at our <a href="https://netneurolab.github.io/markello_spatialnulls" rel="nofollow">walkthrough</a>.</p>

    <h2>

    <a id="user-content-i-have-some-questions" class="anchor" href="#i-have-some-questions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>"I
    have some questions..."</h2>

    <p><a href="https://github.com/netneurolab/markello_spatialnulls/issues">Open
    an issue</a> on this repository and someone will try and get back to you as soon
    as possible!</p>

    '
  stargazers_count: 7
  subscribers_count: 3
  topics: []
  updated_at: 1621535597.0
neurodebian/neurodebian:
  data_format: 2
  description: The ultimate computational platform for Neuroscience
  filenames:
  - singularity/Singularity
  - singularity/Singularity.10.20190801
  - singularity/Singularity.obs
  full_name: neurodebian/neurodebian
  latest_release: debian/0.40.1
  readme: "<pre><code>                              ==  ===============   === =\n\
    \                       =  ================================\n                \
    \  = ======= =        =  ==      ==============\n                ======      \
    \                       = ============ = =\n            ====== =             \
    \                     = ============ =\n        = =======                    \
    \                    = ============\n      ===========                       \
    \                      ==========\n      ===== =                             \
    \                    = ==========\n   ===== =                                \
    \                        ========\n  ======                                  \
    \                          ======\n =====                                    \
    \       =                  =======\n ====                                    \
    \        =                    ======\n ====                        ==   ==  =\
    \  == = ==                      ======\n=====                  = ======== =  ==\
    \   =                            ======\n====              =  =====  ==      \
    \                                   ======\n===               ===== =        \
    \                                       ======\n===               ===        \
    \                                         =======\n  =            = ==       \
    \                                          = ========\n  =  =        =====   \
    \                                          =============\n             =  == \
    \                                       ==  =============\n             == ===\
    \                                 =   ================ =\n               =====\
    \                          == ================== ==\n              ==  ==== =\
    \                 =  ============ = ===  =\n                   = ===== ===   ==\
    \  =============   == = =\n                      = ====================   ==\n\
    </code></pre>\n<p><a href=\"http://neuro.debian.net\" rel=\"nofollow\">NeuroDebian</a>\
    \ is a popular turnkey platform for\nNeuroscience, where software is integrated,\
    \ tested, and delivered\nconveniently and reliably so you could concentrate on\
    \ your research and\nnot on \"system maintenance\".  It provides a large collection\
    \ of popular\nneuroscience research software for the Debian operating system as\
    \ well\nas Ubuntu and other derivatives.  Please visit our\n<a href=\"http://neuro.debian.net\"\
    \ rel=\"nofollow\">main website</a> to discover more.</p>\n<p><a href=\"https://travis-ci.org/neurodebian/neurodebian\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/bde3e9fa0661cc91b1b2ff523416ac95dbdd5365404526a9f08d42f0c33c2f8d/68747470733a2f2f7365637572652e7472617669732d63692e6f72672f6e6575726f64656269616e2f6e6575726f64656269616e2e706e673f6272616e63683d6d6173746572\"\
    \ alt=\"Travis tests status\" data-canonical-src=\"https://secure.travis-ci.org/neurodebian/neurodebian.png?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://hub.docker.com/_/neurodebian/\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/fb1cf50814364d650c82cf7258ea60fae8c313d571b513c7c3ad3e2f729c30bb/687474703a2f2f646f636b6572692e636f2f696d6167652f5f2f6e6575726f64656269616e\"\
    \ alt=\"Docker\" data-canonical-src=\"http://dockeri.co/image/_/neurodebian\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/209\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-related-projects-from-the-neurodebian-authors\"\
    \ class=\"anchor\" href=\"#related-projects-from-the-neurodebian-authors\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Related\
    \ projects from the NeuroDebian authors</h1>\n<ul>\n<li>\n<a href=\"http://open-brain-consent.readthedocs.io\"\
    \ rel=\"nofollow\">Open Brain Consent</a> - samples\nand an ultimate wording for\
    \ experiment participant consent forms to make\nopen data sharing possible</li>\n\
    <li>\n<a href=\"http://datalad.org\" rel=\"nofollow\">DataLad</a> - a data distribution\
    \ and management\nplatform, which addresses shortcomings of solutions of software/code-oriented\n\
    solutions (such as NeuroDebian and pure git), when applied to data</li>\n<li>\n\
    <a href=\"http://reproin.repronim.org\" rel=\"nofollow\">ReproIn</a> - a turnkey\
    \ solution for collecting\nMRI data directly as <a href=\"http://bids.neuroimaging.io\"\
    \ rel=\"nofollow\">BIDS</a> DataLad datasets</li>\n<li>\n<a href=\"duecredit.org\"\
    >DueCredit</a> - Automated collection and reporting of\ncitations for used software/methods/datasets</li>\n\
    <li>\n<a href=\"http://studyforrest.org\" rel=\"nofollow\">Study Forrest</a> -\
    \  a diverse and\never-expanding collection of data and studies on our favorite\
    \ shrimping,\ncross-country running, international ping-pong champion: <em>Forrest\
    \ Gump</em>\n</li>\n<li>\n<a href=\"http://pymvpa.org\" rel=\"nofollow\">PyMVPA</a>\
    \ - a machine learning framework for the analysis\nof (not only) neuroimaging\
    \ data</li>\n<li>Discover more about these and other projects from\n<a href=\"\
    http://centerforopenneuroscience.org\" rel=\"nofollow\">Center for Open Neuroscience</a>\
    \ and\n<a href=\"http://psychoinformatics.de\" rel=\"nofollow\">Psychoinformatics</a>.</li>\n\
    </ul>\n"
  stargazers_count: 49
  subscribers_count: 19
  topics: []
  updated_at: 1617993416.0
nibscbioinformatics/viralevo:
  data_format: 2
  description: null
  filenames:
  - containers/viralevo-reporting/Singularity
  full_name: nibscbioinformatics/viralevo
  latest_release: null
  readme: '<h1>

    <a id="" class="anchor" href="#" aria-hidden="true"><span aria-hidden="true" class="octicon
    octicon-link"></span></a><a href="docs/images/nibscbioinformatics-viralevo_logo.png"
    target="_blank" rel="noopener noreferrer"><img src="docs/images/nibscbioinformatics-viralevo_logo.png"
    alt="nibscbioinformatics/viralevo" style="max-width:100%;"></a>

    </h1>

    <p><strong>Characterisation of viral genomes, intra-host diversity and viral evolution
    across samples</strong>.</p>

    <p><a href="https://github.com/nibscbioinformatics/viralevo/actions"><img src="https://github.com/nibscbioinformatics/viralevo/workflows/nf-core%20CI/badge.svg"
    alt="GitHub Actions CI Status" style="max-width:100%;"></a>

    <a href="https://github.com/nibscbioinformatics/viralevo/actions"><img src="https://github.com/nibscbioinformatics/viralevo/workflows/nf-core%20linting/badge.svg"
    alt="GitHub Actions Linting Status" style="max-width:100%;"></a>

    <a href="https://www.nextflow.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667"
    alt="Nextflow" data-canonical-src="https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg"
    style="max-width:100%;"></a></p>

    <p><a href="http://bioconda.github.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667"
    alt="install with bioconda" data-canonical-src="https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg"
    style="max-width:100%;"></a>

    <a href="https://hub.docker.com/r/nibscbioinformatics/viralevo" rel="nofollow"><img
    src="https://camo.githubusercontent.com/0a860878ad3e1977ef7f5327a20837856d84aafd02647b3b4a591b82a1ca68a9/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e6962736362696f696e666f726d61746963732f766972616c65766f2e737667"
    alt="Docker" data-canonical-src="https://img.shields.io/docker/automated/nibscbioinformatics/viralevo.svg"
    style="max-width:100%;"></a></p>

    <p><a href="https://github.com/nibscbioinformatics/viralevo/workflows/Singularity%20Conversion/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/nibscbioinformatics/viralevo/workflows/Singularity%20Conversion/badge.svg"
    alt="Singularity Conversion" style="max-width:100%;"></a></p>

    <p><a href="https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20&amp;%20Push%20-%20Finishing/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20&amp;%20Push%20-%20Finishing/badge.svg"
    alt="Docker Finishing" style="max-width:100%;"></a></p>

    <p><a href="https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20&amp;%20Push%20-%20Reporting/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20&amp;%20Push%20-%20Reporting/badge.svg"
    alt="Docker Reporting" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

    <p>The ViralEvo pipeline is designed to characterise viral samples, and particularly
    SARS-CoV-2, from short read sequencing data.</p>

    <p>The pipeline is built using <a href="https://www.nextflow.io" rel="nofollow">Nextflow</a>,
    a workflow tool to run tasks across multiple compute infrastructures in a very
    portable manner. It comes with docker containers making installation trivial and
    results highly reproducible.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>i. Install <a href="https://nf-co.re/usage/installation" rel="nofollow"><code>nextflow</code></a></p>

    <p>ii. Install either <a href="https://docs.docker.com/engine/installation/" rel="nofollow"><code>Docker</code></a>
    or <a href="https://www.sylabs.io/guides/3.0/user-guide/" rel="nofollow"><code>Singularity</code></a>
    for full pipeline reproducibility (please only use <a href="https://conda.io/miniconda.html"
    rel="nofollow"><code>Conda</code></a> as a last resort; see <a href="https://nf-co.re/usage/configuration#basic-configuration-profiles"
    rel="nofollow">docs</a>)</p>

    <p>iii. Download the pipeline and test it on a minimal dataset with a single command</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run nibscbioinformatics/viralevo
    -profile test,nibsc --outdir /output/folder</pre></div>

    <p>iv. Start running your own analysis!</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run nibscbioinformatics/viralevo
    -profile nibsc --outdir /output/folder --tools all --genome SARS-CoV-2 --input
    /path/to/sampleinfo.tsv</pre></div>

    <p>See <a href="docs/usage.md">usage docs</a> for all of the available options
    when running the pipeline.</p>

    <h2>

    <a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Documentation</h2>

    <p>The nibscbioinformatics/viralevo pipeline comes with documentation about the
    pipeline, found in the <code>docs/</code> directory:</p>

    <ol>

    <li><a href="https://nf-co.re/usage/installation" rel="nofollow">Installation</a></li>

    <li>Pipeline configuration

    <ul>

    <li><a href="https://nf-co.re/usage/local_installation" rel="nofollow">Local installation</a></li>

    <li><a href="https://nf-co.re/usage/adding_own_config" rel="nofollow">Adding your
    own system config</a></li>

    <li><a href="https://nf-co.re/usage/reference_genomes" rel="nofollow">Reference
    genomes</a></li>

    </ul>

    </li>

    <li><a href="docs/usage.md">Running the pipeline</a></li>

    <li><a href="docs/output.md">Output and how to interpret the results</a></li>

    <li><a href="https://nf-co.re/usage/troubleshooting" rel="nofollow">Troubleshooting</a></li>

    </ol>


    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>nibscbioinformatics/viralevo was originally written by Francesco Lescai and
    Thomas Bleazard.</p>

    <h2>

    <a id="user-content-contributions-and-support" class="anchor" href="#contributions-and-support"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributions
    and Support</h2>

    <p>If you would like to contribute to this pipeline, please see the <a href=".github/CONTRIBUTING.md">contributing
    guidelines</a>.</p>

    <p>For further information or help, don''t hesitate to get in touch on <a href="https://nfcore.slack.com/channels/viralevo"
    rel="nofollow">Slack</a> (you can join with <a href="https://nf-co.re/join/slack"
    rel="nofollow">this invite</a>).</p>

    <h2>

    <a id="user-content-citation" class="anchor" href="#citation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Citation</h2>



    <p>You can cite the <code>nf-core</code> publication as follows:</p>

    <blockquote>

    <p><strong>The nf-core framework for community-curated bioinformatics pipelines.</strong></p>

    <p>Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg,
    Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p>

    <p><em>Nat Biotechnol.</em> 2020 Feb 13. doi: <a href="https://dx.doi.org/10.1038/s41587-020-0439-x"
    rel="nofollow">10.1038/s41587-020-0439-x</a>.<br>

    ReadCube: <a href="https://rdcu.be/b1GjZ" rel="nofollow">Full Access Link</a></p>

    </blockquote>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1606504810.0
nickjer/singularity-r:
  data_format: 2
  description: R in a Singularity container
  filenames:
  - Singularity
  - Singularity.3.6.2
  full_name: nickjer/singularity-r
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-r" class="anchor" href="#singularity-r" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity R</h1>

    <p><a href="https://travis-ci.org/nickjer/singularity-r" rel="nofollow"><img src="https://camo.githubusercontent.com/67e2d6deb1aeb1e18fbd9d72b2bdb73c7ab12099a81313d76bea0546cdfdb1c6/68747470733a2f2f7472617669732d63692e6f72672f6e69636b6a65722f73696e67756c61726974792d722e7376673f6272616e63683d6d6173746572"
    alt="Build Status" data-canonical-src="https://travis-ci.org/nickjer/singularity-r.svg?branch=master"
    style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/462" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="Singularity Hub" data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667"
    alt="GitHub License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for <a href="https://www.r-project.org/" rel="nofollow">R</a>.</p>

    <p>This is still a work in progress.</p>

    <h2>

    <a id="user-content-build" class="anchor" href="#build" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build</h2>

    <p>You can build a local Singularity image named <code>singularity-r.simg</code>
    with:</p>

    <div class="highlight highlight-source-shell"><pre>sudo singularity build singularity-r.simg
    Singularity</pre></div>

    <h2>

    <a id="user-content-deploy" class="anchor" href="#deploy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Deploy</h2>

    <p>Instead of building it yourself you can download the pre-built image from

    <a href="https://www.singularity-hub.org" rel="nofollow">Singularity Hub</a> with:</p>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name singularity-r.simg
    shub://nickjer/singularity-r</pre></div>

    <h2>

    <a id="user-content-run" class="anchor" href="#run" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Run</h2>

    <h3>

    <a id="user-content-r" class="anchor" href="#r" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>R</h3>

    <p>The <code>R</code> command is launched using the default run command:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run singularity-r.simg</pre></div>

    <p>or as an explicit app:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run --app R singularity-r.simg</pre></div>

    <p>Example:</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    run --app R singularity-r.simg --version</span>

    <span class="pl-c1">R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"</span>

    <span class="pl-c1">Copyright (C) 2017 The R Foundation for Statistical Computing</span>

    <span class="pl-c1">Platform: x86_64-pc-linux-gnu (64-bit)</span>


    <span class="pl-c1">R is free software and comes with ABSOLUTELY NO WARRANTY.</span>

    <span class="pl-c1">You are welcome to redistribute it under the terms of the</span>

    <span class="pl-c1">GNU General Public License versions 2 or 3.</span>

    <span class="pl-c1">For more information about these matters see</span>

    <span class="pl-c1">http://www.gnu.org/licenses/.</span></pre></div>

    <h3>

    <a id="user-content-rscript" class="anchor" href="#rscript" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Rscript</h3>

    <p>The <code>Rscript</code> command is launched as an explicit app:</p>

    <div class="highlight highlight-source-shell"><pre>singularity run --app Rscript
    singularity-r.simg</pre></div>

    <p>Example:</p>

    <div class="highlight highlight-text-shell-session"><pre>$ <span class="pl-s1">singularity
    run --app Rscript singularity-r.simg --version</span>

    <span class="pl-c1">R scripting front-end version 3.4.3 (2017-11-30)</span></pre></div>

    <h2>

    <a id="user-content-contributing" class="anchor" href="#contributing" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

    <p>Bug reports and pull requests are welcome on GitHub at

    <a href="https://github.com/nickjer/singularity-r">https://github.com/nickjer/singularity-r</a>.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p>The code is available as open source under the terms of the <a href="http://opensource.org/licenses/MIT"
    rel="nofollow">MIT License</a>.</p>

    '
  stargazers_count: 12
  subscribers_count: 2
  topics:
  - r
  - singularity-image
  updated_at: 1617172672.0
nickjer/singularity-rstudio:
  data_format: 2
  description: RStudio Server in a Singularity container
  filenames:
  - Singularity
  - Singularity.3.6.2
  full_name: nickjer/singularity-rstudio
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-rstudio-server\" class=\"anchor\"\
    \ href=\"#singularity-rstudio-server\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Singularity RStudio Server</h1>\n\
    <p><a href=\"https://travis-ci.org/nickjer/singularity-rstudio\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/291de9d065fa77b739def518b0430f977c5793f78b1b4ce88d235e61c42332ee/68747470733a2f2f7472617669732d63692e6f72672f6e69636b6a65722f73696e67756c61726974792d7273747564696f2e7376673f6272616e63683d6d6173746572\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nickjer/singularity-rstudio.svg?branch=master\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/463\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"Singularity Hub\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a>\n<a href=\"https://opensource.org/licenses/MIT\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\"\
    \ alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity image for <a href=\"https://www.rstudio.com/products/rstudio/\"\
    \ rel=\"nofollow\">RStudio Server</a>. It was built on top of the base\nSingularity\
    \ image <a href=\"https://github.com/nickjer/singularity-r\">nickjer/singularity-r</a>.</p>\n\
    <p>This is still a work in progress.</p>\n<h2>\n<a id=\"user-content-build\" class=\"\
    anchor\" href=\"#build\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Build</h2>\n<p>You can build a local Singularity\
    \ image named <code>singularity-rstudio.simg</code> with:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>sudo singularity build singularity-rstudio.simg\
    \ Singularity</pre></div>\n<h2>\n<a id=\"user-content-deploy\" class=\"anchor\"\
    \ href=\"#deploy\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Deploy</h2>\n<p>Instead of building it yourself you\
    \ can download the pre-built image from\n<a href=\"https://www.singularity-hub.org\"\
    \ rel=\"nofollow\">Singularity Hub</a> with:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity pull --name singularity-rstudio.simg shub://nickjer/singularity-rstudio</pre></div>\n\
    <h2>\n<a id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run</h2>\n\
    <h3>\n<a id=\"user-content-rstudio-server\" class=\"anchor\" href=\"#rstudio-server\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>RStudio Server</h3>\n<p>The <code>rserver</code> command is launched\
    \ using the default run command:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>singularity run singularity-rstudio.simg</pre></div>\n<p>or as an explicit\
    \ app:</p>\n<div class=\"highlight highlight-source-shell\"><pre>singularity run\
    \ --app rserver singularity-rstudio.simg</pre></div>\n<p>Example:</p>\n<div class=\"\
    highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">singularity\
    \ run --app rserver singularity-rstudio.simg --help</span>\n<span class=\"pl-c1\"\
    >command-line options:</span>\n\n<span class=\"pl-c1\">verify:</span>\n<span class=\"\
    pl-c1\">  --verify-installation arg (=0)        verify the current installation</span>\n\
    \n<span class=\"pl-c1\">server:</span>\n<span class=\"pl-c1\">  --server-working-dir\
    \ arg (=/)         program working directory</span>\n<span class=\"pl-c1\">  --server-user\
    \ arg (=rstudio-server)   program user</span>\n<span class=\"pl-c1\">  --server-daemonize\
    \ arg (=0)           run program as daemon</span>\n<span class=\"pl-c1\">  --server-app-armor-enabled\
    \ arg (=1)   is app armor enabled for this session</span>\n<span class=\"pl-c1\"\
    >  --server-set-umask arg (=1)           set the umask to 022 on startup</span>\n\
    \n<span class=\"pl-c1\">...</span></pre></div>\n<h4>\n<a id=\"user-content-simple-password-authentication\"\
    \ class=\"anchor\" href=\"#simple-password-authentication\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Simple Password\
    \ Authentication</h4>\n<p>To secure the RStudio Server you will need to:</p>\n\
    <ol>\n<li>Launch the container with the environment variable <code>RSTUDIO_PASSWORD</code>\
    \ set to\na password of your choosing.</li>\n<li>Launch the <code>rserver</code>\
    \ command with the PAM helper script <code>rstudio_auth</code>.</li>\n</ol>\n\
    <p>An example is given as:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>RSTUDIO_PASSWORD=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>password<span\
    \ class=\"pl-pds\">\"</span></span> singularity run singularity-rstudio.simg \\\
    \n  --auth-none 0 \\\n  --auth-pam-helper rstudio_auth</pre></div>\n<p>Now when\
    \ you attempt to access the RStudio Server you will be presented with a\nlog in\
    \ form. You can log in with your current user name and password you set in\n<code>RSTUDIO_PASSWORD</code>.</p>\n\
    <h4>\n<a id=\"user-content-ldap-authentication\" class=\"anchor\" href=\"#ldap-authentication\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>LDAP Authentication</h4>\n<p>Another option is using an LDAP (or Active\
    \ Directory) server for\nauthentication. Configuration of the LDAP authentication\
    \ script <code>ldap_auth</code> is\nhandled through the following environment\
    \ variables:</p>\n<ul>\n<li>\n<code>LDAP_HOST</code> - the host name of the LDAP\
    \ server</li>\n<li>\n<code>LDAP_USER_DN</code> - the formatted string (where <code>%s</code>\
    \ is replaced with the\nusername supplied during log in) of the bind DN used for\
    \ LDAP authentication</li>\n<li>\n<code>LDAP_CERT_FILE</code> - the file containing\
    \ the CA certificates used by\nthe LDAP server (default: use system CA certificates)</li>\n\
    </ul>\n<p>An example for an LDAP server with signed SSL certificate from a trusted\
    \ CA:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-k\">export</span> LDAP_HOST=ldap.example.com\n<span class=\"pl-k\">export</span>\
    \ LDAP_USER_DN=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cn=%s,dc=example,dc=com<span\
    \ class=\"pl-pds\">'</span></span>\nsingularity run singularity-rstudio.simg \\\
    \n  --auth-none 0 \\\n  --auth-pam-helper-path ldap_auth</pre></div>\n<p>An example\
    \ for an LDAP server with a self-signed SSL certificate:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre><span class=\"pl-k\">export</span> LDAP_HOST=ldap.example.com\n\
    <span class=\"pl-k\">export</span> LDAP_USER_DN=<span class=\"pl-s\"><span class=\"\
    pl-pds\">'</span>cn=%s,dc=example,dc=com<span class=\"pl-pds\">'</span></span>\n\
    <span class=\"pl-k\">export</span> LDAP_CERT_FILE=/ca-certs.pem\nsingularity run\
    \ \\\n  --bind /path/to/ca-certs.pem:/ca-certs.pem \\\n  singularity-rstudio.simg\
    \ \\\n    --auth-none 0 \\\n    --auth-pam-helper-path ldap_auth</pre></div>\n\
    <p>Note that we had to bind mount the CA certificates file from the host machine\n\
    into the container and specify the container's path in <code>LDAP_CERT_FILE</code>\
    \ (not\nthe host's path).</p>\n<h3>\n<a id=\"user-content-r-and-rscript\" class=\"\
    anchor\" href=\"#r-and-rscript\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>R and Rscript</h3>\n<p>See <a href=\"\
    https://github.com/nickjer/singularity-r\">nickjer/singularity-r</a> for more\
    \ information on how to run <code>R</code> and\n<code>Rscript</code> from within\
    \ this Singularity image.</p>\n<h2>\n<a id=\"user-content-contributing\" class=\"\
    anchor\" href=\"#contributing\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Contributing</h2>\n<p>Bug reports\
    \ and pull requests are welcome on GitHub at\n<a href=\"https://github.com/nickjer/singularity-rstudio\"\
    >https://github.com/nickjer/singularity-rstudio</a>.</p>\n<h2>\n<a id=\"user-content-license\"\
    \ class=\"anchor\" href=\"#license\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>License</h2>\n<p>The code is\
    \ available as open source under the terms of the <a href=\"http://opensource.org/licenses/MIT\"\
    \ rel=\"nofollow\">MIT License</a>.</p>\n"
  stargazers_count: 39
  subscribers_count: 5
  topics:
  - rstudio-server
  - singularity-image
  updated_at: 1621773800.0
niwa/lfric_catalyst_adaptor:
  data_format: 2
  description: ParaView Catalyst adaptor example for a Fortran code
  filenames:
  - Singularity
  full_name: niwa/lfric_catalyst_adaptor
  latest_release: null
  readme: '<h1>

    <a id="user-content-lfric-catalyst-adaptor" class="anchor" href="#lfric-catalyst-adaptor"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>LFRic
    Catalyst Adaptor</h1>

    <p>ParaView Catalyst adaptor implementation for the LFRic code</p>

    <p>This package builds a library for visualising simulation data with a simple
    VTK visualisation pipeline. The pipeline can be defined either in C++ or using
    a Python script.</p>

    <h2>

    <a id="user-content-building-the-adaptor" class="anchor" href="#building-the-adaptor"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    the adaptor</h2>

    <p>To build this code, you will need to build and install ParaView with Catalyst
    option enabled. Once this is done, build the code using CMake as follows:</p>

    <pre><code>mkdir build

    cd build

    cmake .. -DParaView_DIR=/path/to/catalyst/install/directory/lib/cmake/paraview-5.4
    -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/path/to/install/dir

    make

    </code></pre>

    <p>If you want to build a debug version of the code, add <code>-DCMAKE_BUILD_TYPE=Debug</code>
    to the CMake configuration, or use the <code>ccmake</code> configuration tool.
    You can add additional compiler flags using the <code>-DCMAKE_CXX_FLAGS=</code>
    option.</p>

    <p>On a Cray XC50 system, the following build setup should work:</p>

    <pre><code>cmake .. -DCMAKE_CXX_COMPILER=CC -DCMAKE_EXE_LINKER_FLAGS=-dynamic
    -DParaView_DIR=/path/to/catalyst/install/directory/lib/cmake/paraview-5.4 -DCMAKE_BUILD_TYPE=Release
    -DCMAKE_INSTALL_PREFIX=/path/to/install/dir

    </code></pre>

    <p>Note that dynamic linking simplifies the linking process of the Fortran application
    significantly.</p>

    <p>Once CMake has finished, run</p>

    <pre><code>make

    make install

    </code></pre>

    <p>to build and install the library.</p>

    <h2>

    <a id="user-content-running-the-test-battery" class="anchor" href="#running-the-test-battery"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    the test battery</h2>

    <p>If you want to test your build, add <code>-DBUILD_TESTING=ON</code> to your
    CMake configuration and run <code>make test</code> or <code>ctest</code> after
    building the code. This will run a number of tests that check basic functionality.</p>

    <h2>

    <a id="user-content-running-a-simulation-with-the-adaptor" class="anchor" href="#running-a-simulation-with-the-adaptor"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    a simulation with the adaptor</h2>

    <p>The Catalyst adaptor and libraries are usually dynamically linked. If the build
    system of your code does not hardcode shared library paths, you will need to set
    (possibly adapting ParaView version)</p>

    <pre><code>export LD_LIBRARY_PATH=/path/to/catalyst/installation/lib/paraview-5.4:$LD_LIBRARY_PATH

    </code></pre>

    <p>If you want to use the Python pipeline, set <code>PYTHONPATH</code> to something
    like</p>

    <pre><code>export PYTHONPATH=/path/to/catalyst/installation/lib/paraview-5.4/site-packages:/path/to/catalyst/installation/lib/paraview-5.4/site-packages/vtk:$PYTHONPATH

    </code></pre>

    <h2>

    <a id="user-content-python-scripts" class="anchor" href="#python-scripts" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Python scripts</h2>

    <p>The repository includes a number of Python scripts which define visualisation
    pipelines or provide some post-processing functionality.</p>

    <h3>

    <a id="user-content-full_outputpy" class="anchor" href="#full_outputpy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>full_output.py</h3>

    <p>Simple Python pipeline for writing the model grid and data field to a VTK file.</p>

    <h3>

    <a id="user-content-spherical_slicepy" class="anchor" href="#spherical_slicepy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>spherical_slice.py</h3>

    <p>Simple Python pipeline for creating spherical slices of model grid with a preset
    radius, which are written into a VTK polydata file. Full output of the model grid
    and data field can also be produced by setting the corresponding flag in the pipeline
    script.</p>

    <h3>

    <a id="user-content-spherical_slice_contourspy" class="anchor" href="#spherical_slice_contourspy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>spherical_slice_contours.py</h3>

    <p>Same as "spherical_slice.py", but includes an additional output file with contours.</p>

    <h3>

    <a id="user-content-spherical_slice_renderedpy" class="anchor" href="#spherical_slice_renderedpy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>spherical_slice_rendered.py</h3>

    <p>Same as "spherical_slice.py", but includes a rendered image of the slice which
    is stored as a png file.</p>

    <h3>

    <a id="user-content-spherical_slice_rendered_coastlinespy" class="anchor" href="#spherical_slice_rendered_coastlinespy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>spherical_slice_rendered_coastlines.py</h3>

    <p>Same as "spherical_slice_rendered.py", but overlays coastlines on the rendered
    image. Requires downloading coastlines data, see source file for instructions.</p>

    <h3>

    <a id="user-content-meridional_slicepy" class="anchor" href="#meridional_slicepy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>meridional_slice.py</h3>

    <p>Creates and stores a meridional slice for a chosen longitude, including a transformation
    from Cartesian to longitude-radius coordinates.</p>

    <h3>

    <a id="user-content-map_projectpy" class="anchor" href="#map_projectpy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>map_project.py</h3>

    <p>This Python program expects a spherical slice (as produced by the <code>spherical_slice.py</code>
    visualisation pipeline) in VTK polydata format as input and produces a VTK polydata
    file with a map projection as output. The program can handle partitioned datasets,
    but computing map projections for multiple timesteps is not supported yet.</p>

    <p>Running</p>

    <pre><code>./map_project.py input.vtp output.vtp

    </code></pre>

    <p>computes a Mollweide map projection. Use flag <code>--list-projections</code>
    to get a list of projections and their short names (projections are provide by
    the PROJ library). Short names can be used to set another projection with the
    <code>--projname</code> flag, e.g., <code>--projname=gall</code>.</p>

    '
  stargazers_count: 2
  subscribers_count: 2
  topics: []
  updated_at: 1619765892.0
okurman/chris_biowulf:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: okurman/chris_biowulf
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-itermae\" class=\"anchor\" href=\"#itermae\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>itermae</h1>\n<p>See the <a href=\"https://darachm.gitlab.io/itermae/concept.html\"\
    \ rel=\"nofollow\">concept here</a> and\n<a href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\"\
    \ rel=\"nofollow\">tutorial here</a>.</p>\n<p><code>itermae</code> is a command-line\
    \ utility to recognize patterns in input sequences\nand generate outputs from\
    \ groups recognized. Basically, it uses fuzzy regular\nexpression operations to\
    \ (primarily) DNA sequence for purposes of DNA\nbarcode/tag/UMI parsing, sequence\
    \ and quality -based filtering,\nand general output re-arrangment.</p>\n<p><a\
    \ href=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\"\
    \ target=\"_blank\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\"\
    \ alt=\"itermae diagram\" data-canonical-src=\"https://darachm.gitlab.io/itermae/_images/parse_diagram_1.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<p><code>itermae</code> reads and makes\
    \ FASTQ, FASTA, text-file, and SAM (tab-delimited)\nfiles using <a href=\"https://pypi.org/project/biopython/\"\
    \ rel=\"nofollow\"><code>Biopython</code></a> sequence records\nto represent slice,\
    \ and read/output formats.\nPattern matching uses the <a href=\"https://pypi.org/project/regex/\"\
    \ rel=\"nofollow\"><code>regex</code></a> library,\nand the tool is designed to\
    \ function in command-line pipes from tools like\n<a href=\"https://www.gnu.org/software/parallel/\"\
    \ rel=\"nofollow\">GNU <code>parallel</code></a>\nto permit light-weight parallelization.</p>\n\
    <p>It's usage might look something like this:</p>\n<pre><code>zcat seq_data.fastqz\
    \ | itermae --config my_config.yml -v &gt; output.sam\n</code></pre>\n<p>or</p>\n\
    <pre><code>zcat seq_data.fastqz \\\n    | parallel --quote --pipe -l 4 --keep-order\
    \ -N 10000 \\\n        itermae --config my_config.yml -v &gt; output.sam\n</code></pre>\n\
    <p>with a <code>my_config.yml</code> file that may look something like this:</p>\n\
    <pre><code>matches:\n    - use: input\n      pattern: NNNNNGTCCTCGAGGTCTCTNNNNNNNNNNNNNNNNNNNNCGTACGCTGCAGGTC\n\
    \      marking: aaaaaBBBBBBBBBBBBBBBccccccccccccccccccccDDDDDDDDDDDDDDD\n    \
    \  marked_groups:\n          a:\n              name: sampleIndex\n           \
    \   repeat: 5\n          B:\n              allowed_errors: 2\n          c:\n \
    \             name: barcode\n              repeat_min: 18\n              repeat_max:\
    \ 22\n          D:\n              allowed_insertions: 1\n              allowed_deletions:\
    \ 2\n              allowed_substititions: 2\noutput_list:\n    -   name: 'barcode'\n\
    \        description: 'description+\" sample=\"+sampleIndex'\n        seq: 'barcode'\n\
    \        filter: 'statistics.median(barcode.quality) &gt;= 35'\n</code></pre>\n\
    <h1>\n<a id=\"user-content-availability-installation-installation\" class=\"anchor\"\
    \ href=\"#availability-installation-installation\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Availability,\
    \ installation, 'installation'</h1>\n<p>Options:</p>\n<ol>\n<li>\n<p>Use pip to\
    \ install <code>itermae</code>, so</p>\n<p>python3 -m pip install itermae</p>\n\
    </li>\n<li>\n<p>You can clone this repo, and install it locally. Dependencies\
    \ are in\n<code>requirements.txt</code>, so\n<code>python3 -m pip install -r requirements.txt</code>\
    \ will install those.</p>\n</li>\n<li>\n<p>You can use <a href=\"https://syslab.org\"\
    \ rel=\"nofollow\">Singularity</a> to pull and run a\n<a href=\"https://singularity-hub.org/collections/4537\"\
    \ rel=\"nofollow\">Singularity image of itermae.py</a>,\nwhere everything is already\
    \ installed.\nThis is the recommended usage.</p>\n<p>This image is built with\
    \ a few other tools,\nlike g/mawk, perl, and parallel, to make command line munging\
    \ easier.</p>\n</li>\n</ol>\n<h1>\n<a id=\"user-content-usage\" class=\"anchor\"\
    \ href=\"#usage\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Usage</h1>\n<p><code>itermae</code> is envisioned\
    \ to be used in a pipe-line where you just got your\nDNA sequencing FASTQ reads\
    \ back, and you want to parse them.\nThe recommended interface is the YAML config\
    \ file, as demonstrated\nin <a href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\"\
    \ rel=\"nofollow\">the tutorial</a>\nand detailed again in the\n<a href=\"https://darachm.gitlab.io/itermae/usage/config.html\"\
    \ rel=\"nofollow\">configuration details</a>.\nYou can also use a command-line\
    \ argument interface as detailed more\n<a href=\"https://darachm.gitlab.io/itermae/usage/examples.html\"\
    \ rel=\"nofollow\">in the examples</a>.</p>\n<p>I recommend you test this on small\
    \ batches of data,\nthen stick it behind GNU <code>parallel</code> and feed the\
    \ whole FASTQ file via\n<code>zcat</code> in on standard input.\nThis parallelizes\
    \ with a small memory footprint, then\nyou write it out to disk (or stream into\
    \ another tool).</p>\n<h1>\n<a id=\"user-content-thanks\" class=\"anchor\" href=\"\
    #thanks\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Thanks</h1>\n<p>Again, the tool is built upon on the excellent work\
    \ of</p>\n<ul>\n<li><a href=\"https://pypi.org/project/regex/\" rel=\"nofollow\"\
    ><code>regex</code></a></li>\n<li><a href=\"https://pypi.org/project/biopython/\"\
    \ rel=\"nofollow\"><code>Biopython</code></a></li>\n<li><a href=\"https://www.gnu.org/software/parallel/\"\
    \ rel=\"nofollow\"><code>parallel</code></a></li>\n</ul>\n<h1>\n<a id=\"user-content-development-helping\"\
    \ class=\"anchor\" href=\"#development-helping\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Development, helping</h1>\n<p>Any\
    \ issues or advice are welcome as an\n<a href=\"https://gitlab.com/darachm/itermae/-/issues\"\
    \ rel=\"nofollow\">issue on the gitlab repo</a>.\nComplaints are especially welcome.</p>\n\
    <p>For development, see the\n<a href=\"https://darachm.gitlab.io/itermae/package.html\"\
    \ rel=\"nofollow\">documentation as rendered from docstrings</a>.</p>\n<p>A set\
    \ of tests is written up with <code>pytest</code> module, and can be run from\
    \ inside\nthe cloned repo with <code>make test</code>.\nSee <code>make help</code>\
    \ for more options, such as building, installing, and uploading.</p>\n<p>There's\
    \ also a bash script with some longer runs in\n<code>profiling_tests</code>, these\
    \ generate longer runs for profiling purposes\nwith <code>cProfile</code> and\
    \ <code>snakeviz</code>.\nBut is out of date. Todo is to re-configure and retest\
    \ that for speed.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1608702922.0
onuryukselen/singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: onuryukselen/singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>singularity</h1>

    <p>Development Branch</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1562279768.0
oogasawa/singularity-latex:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: oogasawa/singularity-latex
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-latex" class="anchor" href="#singularity-latex"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-latex</h1>

    <p>A singularity container of LaTeX typesetting system.</p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <p>Build the Singularity image as follows:</p>

    <pre><code>git clone https://github.com/oogasawa/singularity-latex

    cd singularity-latex

    sudo singularity build singularity-latex.sif Singularity

    </code></pre>

    <p>Compile a LaTeX file. (a DVI file will be generated.)</p>

    <pre><code>singularity exec singularity-latex.sif platex doc.tex

    </code></pre>

    <p>Generate a PDF file from a DVI file.</p>

    <pre><code>singularity exec singularity-latex.sif dvipdfmx doc.dvi

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620240227.0
oogasawa/singularity-ubuntu20:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: oogasawa/singularity-ubuntu20
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-ubuntu20" class="anchor" href="#singularity-ubuntu20"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-ubuntu20</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620420675.0
oxfordmmm/preprocessing:
  data_format: 2
  description: Mycobacterial pre-processing pipeline
  filenames:
  - singularity/Singularity.ppFqtools
  - singularity/Singularity.ppPerljson
  - singularity/Singularity.ppFastqc
  - singularity/Singularity.ppMykrobe
  - singularity/Singularity.ppBowtie2
  - singularity/Singularity.ppBedtools
  - singularity/Singularity.ppBwa
  - singularity/Singularity.ppFastp
  - singularity/Singularity.ppKraken2
  full_name: oxfordmmm/preprocessing
  latest_release: null
  readme: '<h1>

    <a id="user-content-mycobacterial-pre-processing-pipeline" class="anchor" href="#mycobacterial-pre-processing-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mycobacterial
    Pre-processing Pipeline</h1>

    <p>Cleans and QCs reads with fastp and FastQC, classifies with Kraken2 &amp; Mykrobe,
    removes non-bacterial content, and - by alignment to any minority genomes - disambiguates
    mixtures of bacterial reads.</p>

    <p>Takes as input one directory containing pairs of fastq(.gz) or bam files.

    Produces as output one directory per sample, containing the relevant reports &amp;
    a pair of cleaned fastqs.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>The workflow is designed to run with either docker <code>-profile docker</code>
    or singularity <code>-profile singularity</code>. Before running the workflow
    using singularity, the singularity images for the workflow will need to be built
    by running <code>singularity/singularity_pull.sh</code></p>

    <p>E.g. to run the workflow:</p>

    <pre><code>nextflow run main.nf -profile singularity --filetype fastq --input_dir
    fq_dir --pattern "*_R{1,2}.fastq.gz" --unmix_myco yes \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj


    nextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco
    no \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj

    </code></pre>

    <h2>

    <a id="user-content-params" class="anchor" href="#params" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Params</h2>

    <p>The following parameters should be set in <code>nextflow.config</code> or specified
    on the command line:</p>

    <ul>

    <li>

    <strong>input_dir</strong><br>

    Directory containing fastq OR bam files</li>

    <li>

    <strong>filetype</strong><br>

    File type in input_dir. Either "fastq" or "bam"</li>

    <li>

    <strong>pattern</strong><br>

    Regex to match fastq files in input_dir, e.g. "*_R{1,2}.fq.gz"</li>

    <li>

    <strong>output_dir</strong><br>

    Output directory</li>

    <li>

    <strong>unmix_myco</strong><br>

    Do you want to disambiguate mixed-mycobacterial samples by read alignment? Either
    "yes" or "no"</li>

    <li>

    <strong>species</strong><br>

    Principal species in each sample, assuming genus Mycobacterium. Default ''null''.
    If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae,
    chimaera, fortuitum, intracellulare, kansasii, tuberculosis</li>

    <li>

    <strong>kraken_db</strong><br>

    Directory containing <code>*.k2d</code> Kraken2 database files (obtain from <a
    href="https://benlangmead.github.io/aws-indexes/k2" rel="nofollow">https://benlangmead.github.io/aws-indexes/k2</a>)</li>

    <li>

    <strong>bowtie2_index</strong><br>

    Directory containing Bowtie2 index (obtain from <a href="ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip"
    rel="nofollow">ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip</a>).
    The specified path should NOT include the index name</li>

    <li>

    <strong>bowtie_index_name</strong><br>

    Name of the bowtie index, e.g. hg19_1kgmaj<br>

    </li>

    </ul>

    <br>

    <p>For more information on the parameters run <code>nextflow run main.nf --help</code></p>

    <h2>

    <a id="user-content-checkpoints" class="anchor" href="#checkpoints" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoints</h2>

    <p>Checkpoints used throughout this workflow to fail a sample/issue warnings:</p>

    <p>processes preprocessing_checkFqValidity or preprocessing_checkBamValidity</p>

    <ol>

    <li>(Fail) If sample does not pass fqtools ''validate'' or samtools ''quickcheck'',
    as appropriate.</li>

    </ol>

    <p>process preprocessing_countReads<br>

    2. (Fail) If sample contains &lt; 100k pairs of raw reads.</p>

    <p>process preprocessing_fastp<br>

    3. (Fail) If sample contains &lt; 100k pairs of cleaned reads, required to all
    be &gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10
    --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size
    1 --cut_tail_mean_quality 20).</p>

    <p>process preprocessing_kraken2<br>

    4. (Fail) If the top family hit is not Mycobacteriaceae<br>

    5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae <br>

    6. (Warn) If the top family classification is mycobacterial, but this is not consistent
    with top genus and species classifications<br>

    7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications
    meet minimum thresholds of &gt; 5000 reads or &gt; 0.5% of the total reads (this
    is not necessarily a concern as not all mycobacteria have a taxonomic classification
    at this rank) <br>

    8. (Warn) If sample is mixed or contaminated - defined as containing reads &gt;
    the 5000/0.5% thresholds from multiple non-human species<br>

    9. (Warn) If sample contains multiple classifications to mycobacterial species
    complexes, each meeting the &gt; 5000/0.5% thresholds<br>

    10. (Warn) If no species classification meets the 5000/0.5% thresholds<br>

    11. (Warn) If no genus classification meets the 5000/0.5% thresholds<br>

    12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant
    given point 5)</p>

    <p>process preprocessing_identifyBacterialContaminants<br>

    13. (Fail) If the sample is not contaminated and the top species hit is not one
    of the 10 supported Mycobacteria:\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis<br>

    14. (Fail) If the sample is not contaminated and the top species hit is contrary
    to the species expected (e.g. "avium" rather than "tuberculosis" - only tested
    if you provide that expectation)<br>

    15. (Warn) If the top species hit is supported by &lt; 75% coverage<br>

    16. (Warn) If the top species hit has a median coverage depth &lt; 10-fold<br>

    17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant
    species, which means we will not be able to locate its genome, and thereby remove
    it as a contaminant<br>

    18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated
    with a contaminant species'' taxon ID<br>

    19. (Warn) If no complete genome could be found for a contaminant species. The
    workflow will proceed with alignment-based contaminant removal, but you''re warned
    that there''s reduced confidence in detecting reads from this species</p>

    <p>process preprocessing_downloadContamGenomes<br>

    20. (Fail) If a contaminant is detected but we are unable to download a representative
    genome, and thereby remove it</p>

    <p>process preprocessing_summarise<br>

    21. (Fail) If after having taken an alignment-based approach to decontamination,
    Kraken still detects a contaminant species<br>

    22. (Fail) If after having taken an alignment-based approach to decontamination,
    the top species hit is not one of the 10 supported Mycobacteria<br>

    23. (Fail) If, after successfully removing contaminants, the top species hit is
    contrary to the species expected (e.g. "avium" rather than "tuberculosis" - only
    tested if you provide that expectation)</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1620868687.0
panda-planner-dev/pandaPIdriver:
  data_format: 2
  description: null
  filenames:
  - Singularity-Verification
  - Singularity
  full_name: panda-planner-dev/pandaPIdriver
  latest_release: null
  readme: '<h1>

    <a id="user-content-pandapidriver" class="anchor" href="#pandapidriver" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>pandaPIdriver</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619561242.0
parcuri0/singularity-cuda-spack:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: parcuri0/singularity-cuda-spack
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-cuda-spack" class="anchor" href="#singularity-cuda-spack"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-cuda-spack</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1617166672.0
pc2/JHub-HPC-Interface:
  data_format: 2
  description: JupyterHub + High-Performance Computing
  filenames:
  - singularity/Singularity
  - singularity/Singularity_Tensorflow
  full_name: pc2/JHub-HPC-Interface
  latest_release: null
  readme: '<h1>

    <a id="user-content-jupyterhub--high-performance-computing" class="anchor" href="#jupyterhub--high-performance-computing"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>JupyterHub
    + High-Performance Computing</h1>

    <p><strong>High performance Jupyter Notebooks</strong></p>

    <p>The aim of this project is to connect JupyterHub to a high-performance computer
    (HPC). By automatically offloading the computations in a Jupyter notebook to the
    HPC system, even complex calculations are possible. While JupyterHub is deployed
    on a regular server, the notebooks themselves are spawned and run on the remote
    HPC system using a workload manager, such as Slurm.</p>

    <p><strong>Motivation</strong></p>

    <p>The technical core of this project is the transparent integration of digital
    worksheets (Jupyter notebooks), in which learning content and programs can be
    displayed, edited and executed on the students'' own laptops, with current cloud
    and high-performance computing (HPC) technologies. This provides the conditions
    for innovative, digital teaching that encourages independent and interactive development
    of, for example, data science applications, without imposing the complexity of
    using a high-performance computer system on the students. Instead, particularly
    computationally and data-intensive calculations are automatically offloaded to
    a high-performance computer, enabling even sophisticated analyses to be performed
    that would otherwise not be feasible on students'' laptops.</p>

    <p><strong>Features and use cases</strong></p>

    <ul>

    <li>Starting a jupyter notebook server on a remote HPC system in a pre-defined
    singularity container</li>

    <li>Quick config setup when using the Slurm configuration wizard</li>

    <li>Automatically create a singularity overlay so that user changes are persistent</li>

    <li>Great for managing courses with external participants</li>

    <li>Possibility to include files in the notebook directory using WebDAV</li>

    <li>Suitable for HPC users who have their own JupyterHub instance running and
    want to use HPC resources</li>

    </ul>

    <hr>

    <h2>

    <a id="user-content-table-of-contents" class="anchor" href="#table-of-contents"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Table
    of Contents</h2>

    <ul>

    <li>

    <a href="#jupyterhub--high-performance-computing">JupyterHub + High-Performance
    Computing</a>

    <ul>

    <li><a href="#table-of-contents">Table of Contents</a></li>

    <li>

    <a href="#installation-of-jupyterhub-server">Installation of JupyterHub Server</a>

    <ul>

    <li><a href="#jupyterhub-and-batchspawner">JupyterHub and BatchSpawner</a></li>

    <li><a href="#ssh-tunnel-user">SSH tunnel user</a></li>

    <li><a href="#node-mapping">Node mapping</a></li>

    </ul>

    </li>

    <li>

    <a href="#installation-on-hpc-system">Installation on HPC System</a>

    <ul>

    <li><a href="#requirements">Requirements</a></li>

    <li><a href="#install-using-pip">Install using pip</a></li>

    <li>

    <a href="#singularity-container">Singularity Container</a>

    <ul>

    <li>

    <a href="#build-singularity-container">Build Singularity Container</a>

    <ul>

    <li><a href="#compute">Compute</a></li>

    <li><a href="#gpu-tensorflow">GPU (Tensorflow)</a></li>

    </ul>

    </li>

    </ul>

    </li>

    <li><a href="#the-configuration-file">The configuration file</a></li>

    <li><a href="#slurm-configuration-wizard">Slurm configuration wizard</a></li>

    </ul>

    </li>

    <li>

    <a href="#examples">Examples</a>

    <ul>

    <li><a href="#debug-mode">Debug mode</a></li>

    </ul>

    </li>

    <li><a href="#shibboleth-integration">Shibboleth Integration</a></li>

    <li>

    <a href="#nbgrader-integration">NBGrader Integration</a>

    <ul>

    <li><a href="#installation">Installation</a></li>

    <li><a href="#changing-the-student-id-to-the-jupyterhub-logged-in-user-name">Changing
    the Student ID to the JupyterHub logged in user name</a></li>

    <li><a href="#create-nbgrader_configpy">Create nbgrader_config.py</a></li>

    </ul>

    </li>

    <li>

    <a href="#security-precautions">Security Precautions</a>

    <ul>

    <li><a href="#singularity-host-filesystems">Singularity Host Filesystems</a></li>

    <li>

    <a href="#jupyterhub-api-https">JupyterHub API (HTTPS)</a>

    <ul>

    <li><a href="#https">HTTPS</a></li>

    </ul>

    </li>

    <li><a href="#tunnelbot-user">tunnelbot user</a></li>

    </ul>

    </li>

    <li><a href="#troubleshooting">Troubleshooting</a></li>

    </ul>

    </li>

    </ul>

    <hr>

    <h2>

    <a id="user-content-installation-of-jupyterhub-server" class="anchor" href="#installation-of-jupyterhub-server"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation
    of JupyterHub Server</h2>

    <p>This section describes the required installations and configurations on the
    JupyterHub server.</p>

    <h3>

    <a id="user-content-jupyterhub-and-batchspawner" class="anchor" href="#jupyterhub-and-batchspawner"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>JupyterHub
    and BatchSpawner</h3>

    <p>The first thing you should do is install JupyterHub and BatchSpawner. For this
    purpose we provide an Ansible playbook which can be found in <code>/jupyterhub-deployment/</code>.
    See the README for details. Alternatively, you can follow the official installation
    instructions.</p>

    <p>If you decide to do the installations yourself, please proceed as follows:</p>

    <ul>

    <li>install <a href="https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html"
    rel="nofollow">JupyterHub</a>

    </li>

    <li>install <a href="https://github.com/jupyterhub/batchspawner">BatchSpawner</a>

    </li>

    <li>install <a href="https://github.com/jupyterhub/wrapspawner">WrapSpawner</a>
    (make sure to install it in the right environment: <code>/opt/jupyterhub/bin/pip3
    install git+https://github.com/jupyterhub/wrapspawner</code>)</li>

    <li>copy the JupyterHub configuration file <code>/jupyterhub-deployment/config_files/jupyterhub_config.py</code>
    to <code>/opt/jupyterhub/etc/jupyterhub/</code> (you will most likely have to
    edit this file afterwards to make it fit your needs)</li>

    <li>restart the JupyterHub service</li>

    </ul>

    <h3>

    <a id="user-content-ssh-tunnel-user" class="anchor" href="#ssh-tunnel-user" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SSH tunnel user</h3>

    <p>A user called <code>tunnelbot</code> is needed on the JupyterHub server. This
    user is responsible for starting an SSH tunnel between the compute node and the
    JupyterHub server. An SSH key pair for the above mentioned purpose must be generated.
    See <code>/examples/jupyterhub_config.py</code> for more information.</p>

    <h3>

    <a id="user-content-node-mapping" class="anchor" href="#node-mapping" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Node mapping</h3>

    <p>JupyterHub extracts the execution host name of the HPC system (e.g. <code>node01-002</code>).
    When a notebook server is started, an SSH tunnel is established using the notebook
    port.</p>

    <p>In order for JupyterHub to be able to resolve the compute nodes host name,
    the <code>/etc/hosts</code> file must be edited. An example entry might look like
    the following:</p>

    <pre><code>127.0.0.1 node01-001

    127.0.0.1 node01-002

    127.0.0.1 node01-003

    ...

    127.0.0.1 node12-048

    </code></pre>

    <p>The actual node names depend on your HPC system of course.</p>

    <hr>

    <h2>

    <a id="user-content-installation-on-hpc-system" class="anchor" href="#installation-on-hpc-system"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation
    on HPC System</h2>

    <p>This section describes the required installations and configurations of the
    HPC system to enable the interaction with the JuypterHub server.</p>

    <h3>

    <a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Requirements</h3>

    <ul>

    <li>You need a user who is allowed to allocate resources on the HPC system

    <ul>

    <li>With a SSH key pair. The public part must be deposited on the JupyterHub serer
    (<code>tunnelbot</code> user)</li>

    <li>The public key part of the <code>tunnelbot</code>-user created on the JupyterHub
    (-&gt; <em>~/.ssh/authorized_keys</em>)</li>

    </ul>

    </li>

    <li>Singularity (&gt; v.3.7.0)</li>

    <li>mkfs/e2fsprogs with following option:

    <ul>

    <li><a href="https://git.kernel.org/pub/scm/fs/ext2/e2fsprogs.git/commit/?id=217c0bdf17899c0f79b73f76feeadd6d55863180"
    rel="nofollow">https://git.kernel.org/pub/scm/fs/ext2/e2fsprogs.git/commit/?id=217c0bdf17899c0f79b73f76feeadd6d55863180</a></li>

    </ul>

    </li>

    </ul>

    <h3>

    <a id="user-content-install-using-pip" class="anchor" href="#install-using-pip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    using pip</h3>

    <p>You can download and install the required files with pip.</p>

    <p>You may want to build a small Python environment, or install the tools with
    <code>--user</code>.</p>

    <div class="highlight highlight-source-shell"><pre>python3 -m pip install --user
    jh-hpc-interface</pre></div>

    <h3>

    <a id="user-content-singularity-container" class="anchor" href="#singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Container</h3>

    <p>Singularity recipe examples are in the directory singularity/.</p>

    <p>If you do not want to use singularity, then change the value of <code>use_singularity</code>
    in jh_config.ini to false.</p>

    <h4>

    <a id="user-content-build-singularity-container" class="anchor" href="#build-singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build
    Singularity Container</h4>

    <p>To build the container with the recipe files in singularity/ you have to clone
    this repository.</p>

    <p>The following commands replace USER_ID in the recipes to the output of <code>id
    -u</code>, create a new hidden file and build the singularity container with the
    new created file.</p>

    <h5>

    <a id="user-content-compute" class="anchor" href="#compute" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Compute</h5>

    <div class="highlight highlight-source-shell"><pre>USER_ID=<span class="pl-s"><span
    class="pl-pds">$(</span>id -u<span class="pl-pds">)</span></span> <span class="pl-k">&amp;&amp;</span>
    sed <span class="pl-s"><span class="pl-pds">"</span>s/USER_ID/<span class="pl-smi">$USER_ID</span>/<span
    class="pl-pds">"</span></span> <span class="pl-k">&lt;</span> singularity/Singularity
    <span class="pl-k">&gt;</span> singularity/.recipefile_compute <span class="pl-k">&amp;&amp;</span>
    singularity build --remote singularity/compute_jupyter.sif singularity/.recipefile_compute</pre></div>

    <h5>

    <a id="user-content-gpu-tensorflow" class="anchor" href="#gpu-tensorflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>GPU (Tensorflow)</h5>

    <div class="highlight highlight-source-shell"><pre>USER_ID=<span class="pl-s"><span
    class="pl-pds">$(</span>id -u<span class="pl-pds">)</span></span> <span class="pl-k">&amp;&amp;</span>
    sed <span class="pl-s"><span class="pl-pds">"</span>s/USER_ID/<span class="pl-smi">$USER_ID</span>/<span
    class="pl-pds">"</span></span> <span class="pl-k">&lt;</span> singularity/Singularity_Tensorflow
    <span class="pl-k">&gt;</span> singularity/.recipefile_gpu <span class="pl-k">&amp;&amp;</span>
    singularity build --remote singularity/gpu_jupyter.sif singularity/.recipefile_gpu</pre></div>

    <p><em>singularity build help section</em>:</p>

    <blockquote>

    <p><strong>-r, --remote</strong>            build image remotely (does not require
    root)</p>

    </blockquote>

    <p>Please refer to the official docs on how to use the remote build feature: <a
    href="https://sylabs.io/docs/" rel="nofollow">https://sylabs.io/docs/</a></p>

    <h3>

    <a id="user-content-the-configuration-file" class="anchor" href="#the-configuration-file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The
    configuration file</h3>

    <p>In the directory <strong>bin/</strong> is a script, which is deposited after
    the installation on the system.</p>

    <p>With the following call you can display the location of the configuration file:</p>

    <div class="highlight highlight-source-shell"><pre>$ jh_wrapper getconfig</pre></div>

    <p>To learn more about the configuration file, see <a href="docs/jh_config.ini.md">docs/jh_config.ini.md</a></p>

    <h3>

    <a id="user-content-slurm-configuration-wizard" class="anchor" href="#slurm-configuration-wizard"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Slurm
    configuration wizard</h3>

    <p>With the configuration wizard you can prepare your HPC environment.</p>

    <p>The script interactively goes through the configuration file and creates a
    temporary file which can be copied with a simple <code>cp</code>.</p>

    <p>To start the wizard type the following:</p>

    <div class="highlight highlight-source-shell"><pre>$ jh_slurm_wizard</pre></div>

    <hr>

    <h2>

    <a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Examples</h2>

    <p>You will find examples for the configuration files <strong>jh_config.ini</strong>
    and <strong>jupyterhub_config.py</strong> in the directory <em>examples/</em>.</p>

    <hr>

    <h3>

    <a id="user-content-debug-mode" class="anchor" href="#debug-mode" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Debug mode</h3>

    <p>By default the logs contain only information such as warnings or error messages.

    It is also possible to switch on the debug mode, which writes extended information
    into the log files.</p>

    <p>Just set <code>log_level</code> in the configuration file to ''DEBUG''.</p>

    <hr>

    <h2>

    <a id="user-content-shibboleth-integration" class="anchor" href="#shibboleth-integration"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Shibboleth
    Integration</h2>

    <p>Shibboleth authentication was set up for a JupyterHub server in a test environment.
    See <code>./shibboleth/</code> for an example configuration.</p>

    <hr>

    <h2>

    <a id="user-content-nbgrader-integration" class="anchor" href="#nbgrader-integration"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>NBGrader
    Integration</h2>

    <h3>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h3>

    <p>Installation instructions:

    <a href="https://nbgrader.readthedocs.io/en/latest/configuration/jupyterhub_config.html"
    rel="nofollow">https://nbgrader.readthedocs.io/en/latest/configuration/jupyterhub_config.html</a></p>

    <p>To create an exchange directory for every user, just create an empty directory
    in <code>$scratch_dir</code> and mount it into the container with <code>$singularity_bind_extra</code>.</p>

    <h3>

    <a id="user-content-changing-the-student-id-to-the-jupyterhub-logged-in-user-name"
    class="anchor" href="#changing-the-student-id-to-the-jupyterhub-logged-in-user-name"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Changing
    the Student ID to the JupyterHub logged in user name</h3>

    <p>Since the containers run as user <code>jovyan</code>, the value from the <code>$JUPYTERHUB_USER</code>
    variable is automatically used.</p>

    <p>See here for more information:

    <a href="https://jupyter.readthedocs.io/en/latest/community/content-community.html#what-is-a-jovyan"
    rel="nofollow">https://jupyter.readthedocs.io/en/latest/community/content-community.html#what-is-a-jovyan</a></p>

    <h3>

    <a id="user-content-create-nbgrader_configpy" class="anchor" href="#create-nbgrader_configpy"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    nbgrader_config.py</h3>

    <p>See here: <a href="https://nbgrader.readthedocs.io/en/stable/configuration/nbgrader_config.html#use-case-3-nbgrader-and-jupyterhub"
    rel="nofollow">https://nbgrader.readthedocs.io/en/stable/configuration/nbgrader_config.html#use-case-3-nbgrader-and-jupyterhub</a></p>

    <p>To make <em>nbgrader_config.py</em> available in the container, just append
    the file in <code>$singularity_bind_extra</code>.</p>

    <hr>

    <h2>

    <a id="user-content-security-precautions" class="anchor" href="#security-precautions"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Security
    Precautions</h2>

    <h3>

    <a id="user-content-singularity-host-filesystems" class="anchor" href="#singularity-host-filesystems"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Host Filesystems</h3>

    <p>In case you are using Singularity, the host file system may be automatically
    mounted into the container when you start a Singularity Container.</p>

    <p>A possible cause is the option <code>mount hostfs</code> in <em>singularity.conf</em></p>

    <p>See here: <a href="https://sylabs.io/guides/3.5/admin-guide/configfiles.html#singularity-conf"
    rel="nofollow">https://sylabs.io/guides/3.5/admin-guide/configfiles.html#singularity-conf</a></p>

    <h3>

    <a id="user-content-jupyterhub-api-https" class="anchor" href="#jupyterhub-api-https"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>JupyterHub
    API (HTTPS)</h3>

    <h4>

    <a id="user-content-https" class="anchor" href="#https" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>HTTPS</h4>

    <p>See here for more information:

    <a href="https://jupyterhub.readthedocs.io/en/stable/reference/websecurity.html"
    rel="nofollow">https://jupyterhub.readthedocs.io/en/stable/reference/websecurity.html</a></p>

    <h3>

    <a id="user-content-tunnelbot-user" class="anchor" href="#tunnelbot-user" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>tunnelbot user</h3>

    <p>You can increase the security by deactivating shell access for this user.</p>

    <p>Just type:</p>

    <div class="highlight highlight-source-shell"><pre>usermod -s /bin/false tunnelbot</pre></div>

    <hr>

    <h2>

    <a id="user-content-troubleshooting" class="anchor" href="#troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Troubleshooting</h2>

    <p>When problems occur with the JupyterHub, some information can be obtained from
    the logs when debug mode is enabled:</p>

    <p><a href="https://github.com/jupyterhub/jupyterhub/wiki/Debug-Jupyterhub">https://github.com/jupyterhub/jupyterhub/wiki/Debug-Jupyterhub</a></p>

    '
  stargazers_count: 2
  subscribers_count: 7
  topics:
  - jupyter
  - jupyterhub
  - hpc
  - singularity
  updated_at: 1621344647.0
pegasus-isi/darpa_population_modeling:
  data_format: 2
  description: Sample workflow to demonstrate how Pegasus can be used to  manage the
    population modeling tools in the MINT project
  filenames:
  - Singularity
  full_name: pegasus-isi/darpa_population_modeling
  latest_release: null
  readme: '<h1>

    <a id="user-content-darpa_population_modeling" class="anchor" href="#darpa_population_modeling"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>darpa_population_modeling</h1>

    <p>This is a sample workflow to demonstrate how Pegasus can be used to

    manage the population modeling tools in the MINT project.</p>

    <p>The tools have been converted to command line tools.</p>

    <p>A Singularity image, based on Ubuntu Xenial Xerus and with Python3 and

    GIS tools and libraries, are used for the compute environment for the

    jobs.</p>

    <p>The workflow is currently set up to run on the ISI testbed, but can

    be moved to more powerful execution environments if needed.</p>

    <h2>

    <a id="user-content-submitting" class="anchor" href="#submitting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Submitting</h2>

    <p>Check out this repository on <code>workflow.isi.edu</code> and run:</p>

    <pre><code>./workflow/submit.sh

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 6
  topics: []
  updated_at: 1558066303.0
pegasus-isi/fedora-montage:
  data_format: 2
  description: Fedora Singularity image for running Montage workflows
  filenames:
  - Singularity
  full_name: pegasus-isi/fedora-montage
  latest_release: null
  readme: '<p>Fedora Singularity image for running Montage workflows</p>

    '
  stargazers_count: 0
  subscribers_count: 5
  topics: []
  updated_at: 1500493956.0
pegasus-isi/montage-workflow-v3:
  data_format: 2
  description: A Montage workflow for Pegasus 5.0
  filenames:
  - Singularity
  full_name: pegasus-isi/montage-workflow-v3
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-montage-workflow-v3\" class=\"anchor\" href=\"\
    #montage-workflow-v3\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>montage-workflow-v3</h1>\n<p><em>NOTE: This\
    \ is a Montage workflow version which requires Pegasus 5.0. For a version that\
    \ works with\nPegasus 4, please use <code>montage-workflow-v2</code></em></p>\n\
    <p>A new Python DAX generator version of the classic Montage workflow. This workflow\
    \ uses the <a href=\"http://montage.ipac.caltech.edu\" rel=\"nofollow\">Montage\n\
    toolkit</a> to re-project, background correct and add astronomical\nimages into\
    \ custom mosaics.</p>\n<h2>\n<a id=\"user-content-prerequisites\" class=\"anchor\"\
    \ href=\"#prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Prerequisites</h2>\n<ul>\n<li>\n<a href=\"https://pegasus.isi.edu\"\
    \ rel=\"nofollow\">Pegasus</a> - version 5.0 or later</li>\n<li>\n<a href=\"http://montage.ipac.caltech.edu\"\
    \ rel=\"nofollow\">Montage</a> - version 6.0 or later</li>\n<li>\n<a href=\"http://www.astropy.org/\"\
    \ rel=\"nofollow\">AstroPy</a> - version 1.0 or later</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-plan-a-montage-workflow\" class=\"anchor\" href=\"#plan-a-montage-workflow\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Plan a Montage Workflow</h2>\n<p>The <em>./montage-workflow.py</em>\
    \ Python script sets up a <em>data/</em> directory with a Pegasus DAX,\nimage\
    \ tables and region headers. For example:</p>\n<pre><code>./montage-workflow.py\
    \ --center \"56.7 24.0\" --degrees 2.0 \\\n          --band dss:DSS2B:blue --band\
    \ dss:DSS2R:green --band dss:DSS2IR:red\n</code></pre>\n<p>This will create a\
    \ 2x2 degree mosaic centered on 56.7 24.0, with 3 bands making up the\nred, green,\
    \ and blue channels for the final JPEG output. A 2 degree workflow has a lot\n\
    of input images and thus the workflow becomes wide. I simplified version of the\
    \ workflow\nlooks like:</p>\n<p><a href=\"docs/images/dax1.png?raw=true\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"docs/images/dax1.png?raw=true\"\
    \ alt=\"DAX 1\" title=\"DAX 1\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"\
    user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Examples</h2>\n\
    <p>The quickest way to get started is to use the <em>./example-dss.sh</em>\nscript.\
    \ It shows how to use the <em>montage-workflow.py</em> DAX generator to set up\
    \ and plan\n2 degree workflows as described above. Example:</p>\n<pre><code>$\
    \ ./example-dss.sh \n\nAdding band 1 (dss DSS2B -&gt; blue)\nRunning sub command:\
    \ mArchiveList dss DSS2B \"56.7 24.00\" 2.2 2.2 data/1-images.tbl\n[struct stat=\"\
    OK\", count=\"16\"]\nRunning sub command: cd data &amp;&amp; mDAGTbls 1-images.tbl\
    \ region-oversized.hdr 1-raw.tbl 1-projected.tbl 1-corrected.tbl\n[struct stat=\"\
    OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data &amp;&amp; mOverlaps\
    \ 1-raw.tbl 1-diffs.tbl\n[struct stat=\"OK\", count=120]\n\nAdding band 2 (dss\
    \ DSS2R -&gt; green)\nRunning sub command: mArchiveList dss DSS2R \"56.7 24.00\"\
    \ 2.2 2.2 data/2-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command:\
    \ cd data &amp;&amp; mDAGTbls 2-images.tbl region-oversized.hdr 2-raw.tbl 2-projected.tbl\
    \ 2-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub\
    \ command: cd data &amp;&amp; mOverlaps 2-raw.tbl 2-diffs.tbl\n[struct stat=\"\
    OK\", count=120]\n\nAdding band 3 (dss DSS2IR -&gt; red)\nRunning sub command:\
    \ mArchiveList dss DSS2IR \"56.7 24.00\" 2.2 2.2 data/3-images.tbl\n[struct stat=\"\
    OK\", count=\"16\"]\nRunning sub command: cd data &amp;&amp; mDAGTbls 3-images.tbl\
    \ region-oversized.hdr 3-raw.tbl 3-projected.tbl 3-corrected.tbl\n[struct stat=\"\
    OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data &amp;&amp; mOverlaps\
    \ 3-raw.tbl 3-diffs.tbl\n[struct stat=\"OK\", count=120]\n2016.06.02 21:46:32.455\
    \ PDT:    \n2016.06.02 21:46:32.461 PDT:   -----------------------------------------------------------------------\
    \ \n2016.06.02 21:46:32.466 PDT:   File for submitting this DAG to HTCondor  \
    \         : montage-0.dag.condor.sub \n2016.06.02 21:46:32.471 PDT:   Log of DAGMan\
    \ debugging messages                 : montage-0.dag.dagman.out \n2016.06.02 21:46:32.476\
    \ PDT:   Log of HTCondor library output                     : montage-0.dag.lib.out\
    \ \n2016.06.02 21:46:32.481 PDT:   Log of HTCondor library error messages    \
    \         : montage-0.dag.lib.err \n2016.06.02 21:46:32.487 PDT:   Log of the\
    \ life of condor_dagman itself          : montage-0.dag.dagman.log \n2016.06.02\
    \ 21:46:32.492 PDT:    \n2016.06.02 21:46:32.497 PDT:   -no_submit given, not\
    \ submitting DAG to HTCondor.  You can do this with: \n2016.06.02 21:46:32.507\
    \ PDT:   -----------------------------------------------------------------------\
    \ \n2016.06.02 21:46:33.387 PDT:   Your database is compatible with Pegasus version:\
    \ 4.6.1 \n2016.06.02 21:46:33.392 PDT:   \n\nI have concretized your abstract\
    \ workflow. The workflow has been entered \ninto the workflow database with a\
    \ state of \"planned\". The next step is \nto start or execute your workflow.\
    \ The invocation required is\n\npegasus-run  /data/scratch/rynge/montage2/montage-workflow-v2/work/1464929190\n\
    \n2016.06.02 21:46:33.419 PDT:   Time taken to execute is 2.961 seconds \n</code></pre>\n\
    <p>Running the workflow produces fits and jpeg mosaics for each band, as well\
    \ as a combined color one:</p>\n<p><a href=\"docs/images/pleiades.jpg?raw=true\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"docs/images/pleiades.jpg?raw=true\"\
    \ alt=\"Pleiades\" title=\"Pleiades\" style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 1
  subscribers_count: 8
  topics:
  - astronomy
  updated_at: 1616748340.0
pegasus-isi/pegasus:
  data_format: 2
  description: Pegasus Workflow Management System - Automate, recover, and debug scientific
    computations.
  filenames:
  - share/pegasus/init/population/Singularity
  - test/core/044-singularity-nonsharedfs-minimal/image/Singularity
  full_name: pegasus-isi/pegasus
  latest_release: null
  readme: "<p><a href=\"doc/sphinx/images/pegasusfront-black-reduced.png\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"doc/sphinx/images/pegasusfront-black-reduced.png\"\
    \ alt=\"Pegasus\" style=\"max-width:100%;\"></a></p>\n<h2>\n<a id=\"user-content-pegasus-workflow-management-system\"\
    \ class=\"anchor\" href=\"#pegasus-workflow-management-system\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Pegasus\
    \ Workflow Management System</h2>\n<p>Pegasus WMS is a configurable system for\
    \ mapping and executing scientific\nworkflows over a wide range of computational\
    \ infrastructures including laptops,\ncampus clusters, supercomputers, grids,\
    \ and commercial and academic clouds.\nPegasus has been used to run workflows\
    \ with up to 1 million tasks that process\ntens of terabytes of data at a time.</p>\n\
    <p>Pegasus WMS bridges the scientific domain and the execution environment by\n\
    automatically mapping high-level workflow descriptions onto distributed\nresources.\
    \ It automatically locates the necessary input data and computational\nresources\
    \ required by a workflow, and plans out all of the required data\ntransfer and\
    \ job submission operations required to execute the workflow.\nPegasus enables\
    \ scientists to construct workflows in abstract terms without\nworrying about\
    \ the details of the underlying execution environment or the\nparticulars of the\
    \ low-level specifications required by the middleware (Condor,\nGlobus, Amazon\
    \ EC2, etc.). In the process, Pegasus can plan and optimize the\nworkflow to enable\
    \ efficient, high-performance execution of large\nworkflows on complex, distributed\
    \ infrastructures.</p>\n<p>Pegasus has a number of features that contribute to\
    \ its usability and\neffectiveness:</p>\n<ul>\n<li>Portability / Reuse \u2013\
    \ User created workflows can easily be run in different\nenvironments without\
    \ alteration. Pegasus currently runs workflows on top of\nCondor pools, Grid infrastructures\
    \ such as Open Science Grid and XSEDE,\nAmazon EC2, Google Cloud, and HPC clusters.\
    \ The same workflow can run on a\nsingle system or across a heterogeneous set\
    \ of resources.</li>\n<li>Performance \u2013 The Pegasus mapper can reorder, group,\
    \ and prioritize tasks in\norder to increase overall workflow performance.</li>\n\
    <li>Scalability \u2013 Pegasus can easily scale both the size of the workflow,\
    \ and\nthe resources that the workflow is distributed over. Pegasus runs workflows\n\
    ranging from just a few computational tasks up to 1 million. The number of\nresources\
    \ involved in executing a workflow can scale as needed without any\nimpediments\
    \ to performance.</li>\n<li>Provenance \u2013 By default, all jobs in Pegasus\
    \ are launched using the\nKickstart wrapper that captures runtime provenance of\
    \ the job and helps in\ndebugging. Provenance data is collected in a database,\
    \ and the data can be\nqueried with tools such as pegasus-statistics, pegasus-plots,\
    \ or directly\nusing SQL.</li>\n<li>Data Management \u2013 Pegasus handles replica\
    \ selection, data transfers and\noutput registration in data catalogs. These tasks\
    \ are added to a workflow as\nauxilliary jobs by the Pegasus planner.</li>\n<li>Reliability\
    \ \u2013 Jobs and data transfers are automatically retried in case of\nfailures.\
    \ Debugging tools such as pegasus-analyzer help the user to debug the\nworkflow\
    \ in case of non-recoverable failures.</li>\n<li>Error Recovery \u2013 When errors\
    \ occur, Pegasus tries to recover when possible\nby retrying tasks, by retrying\
    \ the entire workflow, by providing workflow-level\ncheckpointing, by re-mapping\
    \ portions of the workflow, by trying alternative\ndata sources for staging data,\
    \ and, when all else fails, by providing a rescue\nworkflow containing a description\
    \ of only the work that remains to be done.\nIt cleans up storage as the workflow\
    \ is executed so that data-intensive\nworkflows have enough space to execute on\
    \ storage-constrained resources.\nPegasus keeps track of what has been done (provenance)\
    \ including the locations\nof data used and produced, and which software was used\
    \ with which parameters.</li>\n</ul>\n<h2>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h2>\n<p>You\
    \ can find more information about Pegasus on the <a href=\"http://pegasus.isi.edu\"\
    \ rel=\"nofollow\">Pegasus Website</a>.</p>\n<p>Pegasus has an extensive <a href=\"\
    http://pegasus.isi.edu/documentation/\" rel=\"nofollow\">User Guide</a>\nthat\
    \ documents how to create, plan, and monitor workflows.</p>\n<p>We recommend you\
    \ start by completing the Pegasus Tutorial from <a href=\"https://pegasus.isi.edu/documentation/user-guide/tutorial.html\"\
    \ rel=\"nofollow\">Chapter 3 of the\nPegasus User Guide</a>.</p>\n<p>The easiest\
    \ way to install Pegasus is to use one of the binary packages\navailable on the\
    \ <a href=\"http://pegasus.isi.edu/downloads\" rel=\"nofollow\">Pegasus downloads\
    \ page</a>.\nConsult <a href=\"https://pegasus.isi.edu/documentation/user-guide/installation.html\"\
    \ rel=\"nofollow\">Chapter 2 of the Pegasus User Guide</a>\nfor more information\
    \ about installing Pegasus from binary packages.</p>\n<p>There is documentation\
    \ on the Pegasus website for the Python, Java and R\n<a href=\"https://pegasus.isi.edu/documentation/reference-guide/api-reference.html\"\
    \ rel=\"nofollow\">Abstract Workflow Generator APIs</a>.\nWe strongly recommend\
    \ using the Python API which is feature complete, and also\nallows you to invoke\
    \ all the pegasus command line tools.</p>\n<p>You can use <em>pegasus-init</em>\
    \ command line tool to run several examples\non your local machine. Consult <a\
    \ href=\"https://pegasus.isi.edu/documentation/user-guide/example-workflows.html\"\
    \ rel=\"nofollow\">Chapter 4 of the Pegasus\nUser Guide</a>\nfor more information.</p>\n\
    <p>There are also examples of how to <a href=\"https://pegasus.isi.edu/documentation/user-guide/execution-environments.html\"\
    \ rel=\"nofollow\">Configure Pegasus for Different Execution\nEnvironments</a>\n\
    in the Pegasus User Guide.</p>\n<p>If you need help using Pegasus, please contact\
    \ us. See the [contact page]\n(<a href=\"http://pegasus.isi.edu/contact\" rel=\"\
    nofollow\">http://pegasus.isi.edu/contact</a>) on the Pegasus website for more\
    \ information.</p>\n<h2>\n<a id=\"user-content-building-from-source\" class=\"\
    anchor\" href=\"#building-from-source\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building from Source</h2>\n<p>Pegasus\
    \ can be compiled on any recent Linux or Mac OS X system.</p>\n<h3>\n<a id=\"\
    user-content-source-dependencies\" class=\"anchor\" href=\"#source-dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Source Dependencies</h3>\n<p>In order to build Pegasus from source,\
    \ make sure you have the following installed:</p>\n<ul>\n<li>Git</li>\n<li>Java\
    \ 8 or higher</li>\n<li>Python 3.5 or higher</li>\n<li>R</li>\n<li>Ant</li>\n\
    <li>gcc</li>\n<li>g++</li>\n<li>make</li>\n<li>tox 3.14.5 or higher</li>\n<li>mysql\
    \ (optional, required to access MySQL databases)</li>\n<li>postgresql (optional,\
    \ required to access PostgreSQL databases)</li>\n<li>Python pyyaml</li>\n<li>Python\
    \ GitPython</li>\n</ul>\n<p>Other packages may be required to run unit tests,\
    \ and build MPI tools.</p>\n<h3>\n<a id=\"user-content-compiling\" class=\"anchor\"\
    \ href=\"#compiling\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Compiling</h3>\n<p>Ant is used to compile Pegasus.</p>\n\
    <p>To get a list of build targets run:</p>\n<pre><code>$ ant -p\n</code></pre>\n\
    <p>The targets that begin with \"dist\" are what you want to use.</p>\n<p>To build\
    \ a basic binary tarball (excluding documentation), run:</p>\n<pre><code>$ ant\
    \ dist\n</code></pre>\n<p>To build the release tarball (including documentation),\
    \ run:</p>\n<pre><code>$ ant dist-release\n</code></pre>\n<p>The resulting packages\
    \ will be created in the <code>dist</code> subdirectory.</p>\n"
  stargazers_count: 114
  subscribers_count: 31
  topics:
  - workflow
  - workflow-management-system
  - bioinformatics
  - distributed-systems
  - hpc
  updated_at: 1621923885.0
perambluate/singularity-definition-files-for-HPC:
  data_format: 2
  description: To build hpc benchmark and mpi with cuda support sif
  filenames:
  - hpl_intel_cuda.def
  - bert.def
  - hpcc_intel.def
  - hpc_mpi_cuda.def
  full_name: perambluate/singularity-definition-files-for-HPC
  latest_release: null
  readme: '<h1>

    <a id="user-content-hpc_mpi_cuda_singu_def_file" class="anchor" href="#hpc_mpi_cuda_singu_def_file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>hpc_mpi_cuda_singu_def_file</h1>

    <p>A collect of definition files to build images for singularity containers, which
    includes hpc benchmarks and mpis with cuda support.</p>

    <p><a href="https://singularity-hub.org/collections/4181" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1589020087.0
peter-jansson/appnuc:
  data_format: 2
  description: Applied nuclear physics relevant software, containerized.
  filenames:
  - Singularity
  full_name: peter-jansson/appnuc
  latest_release: v0.2.0.0
  readme: '<h1>

    <a id="user-content-appnuc" class="anchor" href="#appnuc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>appnuc</h1>

    <p>Applied nuclear physics relevant software.</p>

    <p><a href="https://camo.githubusercontent.com/f400597fcdcb66eeb5702e037732d66d7eecdf94f4f363a2dde0da21c4ba9ec4/68747470733a2f2f7777772e676e752e6f72672f67726170686963732f6c67706c76332d776974682d746578742d3135347836382e706e67"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/f400597fcdcb66eeb5702e037732d66d7eecdf94f4f363a2dde0da21c4ba9ec4/68747470733a2f2f7777772e676e752e6f72672f67726170686963732f6c67706c76332d776974682d746578742d3135347836382e706e67"
    alt="LGPL-3" data-canonical-src="https://www.gnu.org/graphics/lgplv3-with-text-154x68.png"
    style="max-width:100%;"></a></p>

    <p>An Ubuntu based image/container with a bunch of standard programs that are
    useful for scientific work in the field of applied nuclear physics. In addition,
    the following list of relevant software is installed.</p>

    <ul>

    <li>

    <a href="https://geant4.web.cern.ch/" rel="nofollow">Geant4</a> monte carlo framework</li>

    <li>

    <a href="https://root.cern.ch/" rel="nofollow">Root</a> data analysis framework</li>

    <li>

    <a href="https://dx.doi.org/10.18434/T48G6X" rel="nofollow">XCOM</a> program from
    NIST</li>

    </ul>

    <h2>

    <a id="user-content-docker" class="anchor" href="#docker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Docker</h2>

    <p>Docker hub contains the <a href="https://hub.docker.com/r/jansson/appnuc" rel="nofollow">image</a>
    built using the Dockerfile, which can pulled into the local Docker registry by
    the command <code>docker pull jansson/appnuc</code>.</p>

    <p>The image can be started in a container by, e.g., the command <code>docker
    run --rm -it jansson/appnuc bash -l</code>. Significantly more information on
    how to mount a local file system to the container as well as other command line
    options is available in the <a href="https://docs.docker.com/engine/reference/commandline/cli/"
    rel="nofollow">Docker documentation</a>.</p>

    <h2>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h2>

    <p>A <a href="https://sylabs.io/" rel="nofollow">Singularity</a> file containing
    the same containerized Ubuntu and software can be built using the Singularity
    definition file, named <code>Singularity</code>. E.g. using the command <code>sudo
    singularity build appnuc.sif Singularity</code> to build <code>appnuc.sif</code>.</p>

    <p>See the <a href="https://sylabs.io/guides/3.7/user-guide/" rel="nofollow">Singularity
    user guide</a> for more information.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - applied-nuclear-physics
  - dockerfile
  - singularity
  updated_at: 1621853660.0
photocyte/recoll-webui_singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: photocyte/recoll-webui_singularity
  latest_release: null
  readme: "<p>Singularity file for the recoll-webui.</p>\n<p>Image building handled\
    \ by <a href=\"https://singularity-hub.org\" rel=\"nofollow\">singularity-hub.org</a></p>\n\
    <h3>\n<a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Usage</h3>\n\
    <pre><code>singularity pull shub://photocyte/recoll-webui_singularity`\nsingularity\
    \ exec --cleanenv recoll-webui_singularity_latest.sif /recollwebui/webui-standalone.py\n\
    ## Then navigate to http://127.0.0.1:13337\n## The .recoll directory in your home\
    \ directory will need to be, or be symlinked to, a real recoll index directory,\
    \ included a previous indexed xapiandb \n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610240040.0
photocyte/tmsu_singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: photocyte/tmsu_singularity
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/5075" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a>

    Singularity file for <a href="https://tmsu.org" rel="nofollow">tmsu</a>.</p>

    <p>Image building handled by <a href="https://singularity-hub.org" rel="nofollow">singularity-hub.org</a></p>

    <h3>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h3>

    <pre><code>singularity pull shub://photocyte/tmsu_singularity

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1610252258.0
piperwelch/Basic-Empirical-Starter-carlcs361s01w21-6:
  data_format: 2
  description: If you are going to build off of basic Empirical, this is the project
    for you
  filenames:
  - third-party/force-cover/Singularity
  full_name: piperwelch/Basic-Empirical-Starter-carlcs361s01w21-6
  latest_release: null
  readme: '<h1>

    <a id="user-content-evolutionary-algorithm" class="anchor" href="#evolutionary-algorithm"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evolutionary
    Algorithm</h1>

    <p><a href="https://github.com/anyaevostinar/evo-algo/releases"><img src="https://camo.githubusercontent.com/ff63e2cc80517f7b8e8246b33025f569d757ede0ae65c7ea57418d79e5a3709d/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f25324676657273696f6e2d62616467652e6a736f6e"
    alt="version" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fversion-badge.json"
    style="max-width:100%;"></a>

    <a href="https://travis-ci.com/anyaevostinar/evo-algo" rel="nofollow"><img src="https://camo.githubusercontent.com/2bde10efc09d993aad000b3101be56d5410d0ced348c4c7bbedbc7ffce79d630/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f616e796165766f7374696e61722f65766f2d616c676f2e737667"
    alt="" data-canonical-src="https://img.shields.io/travis/anyaevostinar/evo-algo.svg"
    style="max-width:100%;"></a>

    <a href="https://evo-algo.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img
    src="https://camo.githubusercontent.com/2af827df5df15ff6f5c6a9fff5021e631a0049aa2274f98e983135bb66f0ed81/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f65766f2d616c676f2f62616467652f3f76657273696f6e3d6c6174657374"
    alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/evo-algo/badge/?version=latest"
    style="max-width:100%;"></a>

    <a href="https://evo-algo.readthedocs.io/en/latest/" rel="nofollow"><img src="https://camo.githubusercontent.com/0e035446b6d1c7a911bd4b203bd581f2116c7873352a90d4797dc08119abbd0e/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f253246646f63756d656e746174696f6e2d636f7665726167652d62616467652e6a736f6e"
    alt="documentation coverage" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fdocumentation-coverage-badge.json"
    style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/anyaevostinar/evo-algo" rel="nofollow"><img src="https://camo.githubusercontent.com/869814fe0b90d0baadc37140ac31d6d9c0e4e00c2854a51f1030c40678bc13a4/68747470733a2f2f636f6465636f762e696f2f67682f616e796165766f7374696e61722f65766f2d616c676f2f6272616e63682f6d61737465722f67726170682f62616467652e737667"
    alt="code coverage status" data-canonical-src="https://codecov.io/gh/anyaevostinar/evo-algo/branch/master/graph/badge.svg"
    style="max-width:100%;"></a>

    <a href="https://github.com/anyaevostinar/evo-algo/search?q=todo+OR+fixme&amp;type="><img
    src="https://camo.githubusercontent.com/4455f1d1625d643fe17506cb6ad50a9a6612ce62ab4667dc60b75616241c7534/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e636f6d25324665766f2d616c676f253246646f746f2d62616467652e6a736f6e"
    alt="dotos" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.com%2Fevo-algo%2Fdoto-badge.json"
    style="max-width:100%;"></a>

    <a href="https://github.com/anyaevostinar/evo-algo"><img src="https://camo.githubusercontent.com/db51ac0d7785eb561dcfc0cbccfc0cfed9872513120f8f732b1301504c4eb32e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e796165766f7374696e61722f65766f2d616c676f2e7376673f7374796c653d666c61742d737175617265266c6f676f3d676974687562266c6162656c3d5374617273266c6f676f436f6c6f723d7768697465"
    alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/anyaevostinar/evo-algo.svg?style=flat-square&amp;logo=github&amp;label=Stars&amp;logoColor=white"
    style="max-width:100%;"></a></p>

    <p>An evolutionary algorithm</p>

    <p>Check out the live in-browser web app at <a href="https://anyaevostinar.github.io/evo-algo"
    rel="nofollow">https://anyaevostinar.github.io/evo-algo</a>.</p>

    <ul>

    <li>Free software: MIT license</li>

    <li>Documentation: <a href="https://evo-algo.readthedocs.io" rel="nofollow">https://evo-algo.readthedocs.io</a>.</li>

    </ul>

    <h2>

    <a id="user-content-features" class="anchor" href="#features" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Features</h2>

    <ul>

    <li>TODO</li>

    </ul>

    <p><a href="docs/assets/cookie.gif" target="_blank" rel="noopener noreferrer"><img
    src="docs/assets/cookie.gif" alt="cookie monster example" style="max-width:100%;"></a></p>

    <h2>

    <a id="user-content-credits" class="anchor" href="#credits" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credits</h2>

    <p>This package was created with <a href="https://github.com/audreyr/cookiecutter">Cookiecutter</a>
    and the <a href="https://github.com/devosoft/cookiecutter-empirical-project">devosoft/cookiecutter-empirical-project</a>
    project template.</p>

    <p>This package uses <a href="https://github.com/devosoft/Empirical#readme">Empirical</a>,
    a library of tools for scientific software development, with emphasis on also
    being able to build web interfaces using Emscripten.</p>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <p>To install <a href="https://github.com/devosoft/Empirical">Empirical</a>, pull
    down a clone of the Empirical repository.  See <a href="https://empirical.readthedocs.io/en/latest/QuickStartGuides"
    rel="nofollow">Quick Start Guides</a> for directions on cloning and using the
    library.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1615709051.0
pmitev/SSC-singularity-build:
  data_format: 2
  description: null
  filenames:
  - Singularity.lolcow
  full_name: pmitev/SSC-singularity-build
  latest_release: null
  readme: '<h1>

    <a id="user-content-ssc-singularity-build" class="anchor" href="#ssc-singularity-build"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SSC-singularity-build</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621541256.0
pmitev/UPPMAX-Singularity:
  data_format: 2
  description: UPPMAX Singularity builds
  filenames:
  - gapseq/Singularity.gapseq
  - metaWRAP/Singularity.metaWRAP
  - metaWRAP/Singularity.metaWRAP-deps-only
  - metaWRAP/Singularity.metaWRAP-deps-only-ubuntu
  - MitoZ/Singularity.v2.3-pm
  full_name: pmitev/UPPMAX-Singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-uppmax-singularity" class="anchor" href="#uppmax-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>UPPMAX-Singularity</h1>

    <p>UPPMAX Singularity builds</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621189876.0
powerPlant/dvc-srf:
  data_format: 2
  description: Singularity recipe files for dvc (https://github.com/iterative/dvc)
  filenames:
  - Singularity
  - Singularity.2.1.0
  - Singularity.1.6.1
  full_name: powerPlant/dvc-srf
  latest_release: null
  readme: '<p>Singularity recipe files for the DVC tool for Data Version Control</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1621938926.0
pradas/PhysiBoSSa-EMEWS:
  data_format: 2
  description: EMEWS template for PhysiBoSSa
  filenames:
  - data/PhysiBoSSa/addons/PhysiBoSSa/MaBoSS-env-2.0/containers/singularity/Singularity
  full_name: pradas/PhysiBoSSa-EMEWS
  latest_release: null
  readme: "<h2>\n<a id=\"user-content-emews-project-template\" class=\"anchor\" href=\"\
    #emews-project-template\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>EMEWS project template</h2>\n<p>You have just\
    \ created an EMEWS project.</p>\n<p>This project is compatible with swift-t v.\
    \ 1.3+. Earlier\nversions will NOT work.</p>\n<p>The project consists of the following\
    \ directories:</p>\n<pre><code>EMEWS-PhysiBoSSa/\n  data/\n  ext/\n  etc/\n  python/\n\
    \    test/\n  R/\n    test/\n  scripts/\n  swift/\n  README.md\n</code></pre>\n\
    <p>The directories are intended to contain the following:</p>\n<ul>\n<li>\n<code>data</code>\
    \ - model input etc. data</li>\n<li>\n<code>etc</code> - additional code used\
    \ by EMEWS</li>\n<li>\n<code>ext</code> - swift-t extensions such as eqpy, eqr</li>\n\
    <li>\n<code>python</code> - python code (e.g. model exploration algorithms written\
    \ in python)</li>\n<li>\n<code>python/test</code> - tests of the python code</li>\n\
    <li>\n<code>R</code> - R code (e.g. model exploration algorithms written R)</li>\n\
    <li>\n<code>R/test</code> - tests of the R code</li>\n<li>\n<code>scripts</code>\
    \ - any necessary scripts (e.g. scripts to launch a model), excluding\nscripts\
    \ used to run the workflow.</li>\n<li>\n<code>swift</code> - swift code</li>\n\
    </ul>\n<p>Use the subtemplates to customize this structure for particular types\
    \ of\nworkflows. These are: sweep, eqpy, and eqr.</p>\n"
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1591715852.0
pradas/pba_spheroidTNF:
  data_format: 2
  description: null
  filenames:
  - src/addons/PhysiBoSSa/MaBoSS-env-2.0/containers/singularity/Singularity
  full_name: pradas/pba_spheroidTNF
  latest_release: null
  readme: '<h1>

    <a id="user-content-physibossa-nanohub-spheroidtnf" class="anchor" href="#physibossa-nanohub-spheroidtnf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PhysiBoSSa
    nanoHub SpheroidTNF</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1589290842.0
princeton-vl/CoqGym:
  data_format: 2
  description: A Learning Environment for Theorem Proving with the Coq proof assistant
  filenames:
  - Singularity
  full_name: princeton-vl/CoqGym
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-coqgym\" class=\"anchor\" href=\"#coqgym\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>CoqGym</h1>\n\
    <p><a href=\"images/example_proof.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\
    ><img src=\"images/example_proof.jpg\" alt=\"Example proof\" style=\"max-width:100%;\"\
    ></a></p>\n<p>Code for the paper:</p>\n<p><a href=\"https://arxiv.org/abs/1905.09381\"\
    \ rel=\"nofollow\">Learning to Prove Theorems via Interacting with Proof Assistants</a><br>\n\
    <a href=\"https://www.cs.princeton.edu/~kaiyuy/\" rel=\"nofollow\">Kaiyu Yang</a>\
    \ and <a href=\"https://www.cs.princeton.edu/~jiadeng/\" rel=\"nofollow\">Jia\
    \ Deng</a><br>\nInternational Conference on Machine Learning (ICML) 2019</p>\n\
    <pre><code>@inproceedings{yang2019coqgym,\n  title={Learning to Prove Theorems\
    \ via Interacting with Proof Assistants},\n  author={Yang, Kaiyu and Deng, Jia},\n\
    \  booktitle={International Conference on Machine Learning},\n  year={2019},\n\
    }\n</code></pre>\n<p>For potential bugs, please open an issue. For any other questions,\
    \ please ask in <a href=\"https://github.com/princeton-vl/CoqGym/discussions\"\
    >Discussions</a>.</p>\n<h2>\n<a id=\"user-content-table-of-contents\" class=\"\
    anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Table of Contents</h2>\n<ol>\n\
    <li>\n<a href=\"#1-installing-coqgym\">Installing CoqGym</a><br>\n\_ \_  1.1 <a\
    \ href=\"#11-dependencies\">Dependencies</a><br>\n\_ \_  1.2 <a href=\"#12-building-coq-serapi-coqhammer-and-the-coq-projects\"\
    >Building Coq, SerAPI, CoqHammer, and the Coq Projects</a><br>\n\_ \_  1.3 <a\
    \ href=\"#13-extracting-the-proofs-from-coq-code-optional\">Extracting the Proofs\
    \ from Coq Code</a><br>\n\_ \_  1.4 <a href=\"#14-downloading-the-pre-extracted-proofs-recommended\"\
    >Downloading the Pre-extracted Proofs</a>\n</li>\n<li>\n<a href=\"#2-using-coqgym-in-a-container\"\
    >Using CoqGym in a Container</a><br>\n\_ \_  2.1 <a href=\"#21-dependencies\"\
    >Dependencies</a><br>\n\_ \_  2.2 <a href=\"#22-downloading-the-pre-built-container-image\"\
    >Downloading the Pre-built Container Image</a><br>\n\_ \_  2.3 <a href=\"#23-using-the-container\"\
    >Using the Container</a><br>\n\_ \_  2.4 <a href=\"#24-building-the-container-by-yourself\"\
    >Building the Container by Yourself</a>\n</li>\n<li>\n<a href=\"#3-data-format\"\
    >Data Format</a><br>\n\_ \_  3.1 <a href=\"#31-json-files\">JSON Files</a><br>\n\
    \_ \_  3.2 <a href=\"#32-lmdb-file\">LMDB File</a><br>\n\_ \_  3.3 <a href=\"\
    #33-glossary\">Gloassary</a>\n</li>\n<li>\n<a href=\"#4-data-utilities\">Data\
    \ Utilities</a><br>\n\_ \_  4.1 <a href=\"#41-interacting-with-coqgym\">Interacting\
    \ with CoqGym</a><br>\n\_ \_  4.2 <a href=\"#42-parsing-coq-terms\">Parsing Coq\
    \ Terms</a><br>\n\_ \_  4.3 <a href=\"#43-computing-dataset-statistics\">Computing\
    \ Dataset Statistics</a>\n</li>\n<li>\n<a href=\"#5-the-astactic-model\">The ASTactic\
    \ Model</a><br>\n\_ \_  5.1 <a href=\"#51-prerequisites\">Prerequisites</a><br>\n\
    \_ \_  5.2 <a href=\"#52-extracting-proof-steps\">Extracting Proof Steps</a><br>\n\
    \_ \_  5.3 <a href=\"#53-training\">Training</a><br>\n\_ \_  5.4 <a href=\"#54-testing\"\
    >Testing</a>\n</li>\n<li><a href=\"#6-credits\">Credits</a></li>\n<li><a href=\"\
    #7-contributing\">Contributing</a></li>\n</ol>\n<hr>\n<h2>\n<a id=\"user-content-1-installing-coqgym\"\
    \ class=\"anchor\" href=\"#1-installing-coqgym\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>1. Installing CoqGym</h2>\n<p>Follow\
    \ these steps to obtain the CoqGym dataset and build the environment for interacting\
    \ with it.\nAlternatively, you may also use CoqGym in a <a href=\"#2-using-coqgym-in-a-container\"\
    >container</a>.</p>\n<h3>\n<a id=\"user-content-11-dependencies\" class=\"anchor\"\
    \ href=\"#11-dependencies\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>1.1 Dependencies</h3>\n<ul>\n<li><a href=\"\
    https://opam.ocaml.org/\" rel=\"nofollow\">OPAM</a></li>\n<li><a href=\"https://www.anaconda.com/distribution/\"\
    \ rel=\"nofollow\">Anaconda Python 3</a></li>\n<li><a href=\"https://symas.com/lmdb/\"\
    \ rel=\"nofollow\">LMDB</a></li>\n<li><a href=\"https://www.ruby-lang.org/en/\"\
    \ rel=\"nofollow\">Ruby</a></li>\n</ul>\n<h3>\n<a id=\"user-content-12-building-coq-serapi-coqhammer-and-the-coq-projects\"\
    \ class=\"anchor\" href=\"#12-building-coq-serapi-coqhammer-and-the-coq-projects\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1.2 Building Coq, SerAPI, CoqHammer, and the Coq Projects</h3>\n<ol>\n\
    <li>Create an OPAM switch for OCaml 4.07.1+flambda: <code>opam switch create 4.07.1+flambda\
    \ &amp;&amp; eval $(opam env)</code>\n</li>\n<li>Upgrade the installed OPAM packages\
    \ (optional): <code>opam upgrade &amp;&amp; eval $(opam env)</code>\n</li>\n<li>Clone\
    \ the repository: <code>git clone https://github.com/princeton-vl/CoqGym</code>\n\
    </li>\n<li>Install Coq, SerAPI and CoqHammer: <code>cd CoqGym &amp;&amp; source\
    \ install.sh</code>\n</li>\n<li>Build the Coq projects (can take a while): <code>cd\
    \ coq_projects &amp;&amp; make &amp;&amp; cd ..</code>\n</li>\n<li>Create and\
    \ activate the conda environment: <code>conda env create -f coq_gym.yml &amp;&amp;\
    \ conda activate coq_gym</code>\n</li>\n</ol>\n<p><em>Note</em>: <a href=\"https://github.com/coq/coq\"\
    >Coq</a>, <a href=\"https://github.com/ejgallego/coq-serapi\">SerAPI</a>, <a href=\"\
    https://github.com/lukaszcz/coqhammer\">CoqHammer</a>, and the Coq projects in\
    \ <a href=\"./coq_projects\">coq_projects</a> directory are indendent software\
    \ projects with their own code repositories, but please follow the instructions\
    \ above to build the specific versions we need.</p>\n<h3>\n<a id=\"user-content-13-extracting-the-proofs-from-coq-code-optional\"\
    \ class=\"anchor\" href=\"#13-extracting-the-proofs-from-coq-code-optional\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1.3\
    \ Extracting the Proofs from Coq Code (Optional)</h3>\n<p>We include the code\
    \ for extracting CoqGym from Coq source code. However, it is not guaranteed to\
    \ reproduce exactly the same data. The timeout and other miscellaneous errors\
    \ during the data extraction may be machine-dependent. For example, a faster machine\
    \ is likely to have fewer timeout errors and thus can extract more proofs.\nFor\
    \ benchmark purpose, please download and use our pre-extracted version of CoqGym.</p>\n\
    <ol>\n<li>\n<p>Check all Coq files and locate the proofs:<br>\nFor each <code>*.meta</code>\
    \ file in <code>./coq_projects/</code>, run <code>python check_proofs.py --file\
    \ /path/to/*.meta</code><br>\nNow you have generated a <code>*.json</code> file\
    \ in <code>./data/</code> corresponding to each <code>*.meta</code> file. The\
    \ <code>proofs</code> field of the JSON object is a list containing the proof\
    \ names.</p>\n</li>\n<li>\n<p>Extract the proofs:<br>\nFor each <code>*.meta</code>\
    \ file and each proof, run:<br>\n<code>python extract_proof.py --file /path/to/*.meta\
    \ --proof $PROOF_NAME</code><br>\n<code>python extract_synthetic_proofs.py --file\
    \ /path/to/*.meta --proof $PROOF_NAME</code></p>\n</li>\n<li>\n<p>Post-processing:\
    \ <code>python postprocess.py</code></p>\n</li>\n</ol>\n<p><em>Caveat</em>: The\
    \ steps above are computationally expensive. When we say \"For each XXX, run <code>YYY</code>\"\
    , the tasks are embarrassingly parallel, which means you can run them in parallel\
    \ in any order. We do not provide the code for that because it depends on a particular\
    \ HPC infrastructure.</p>\n<h3>\n<a id=\"user-content-14-downloading-the-pre-extracted-proofs-recommended\"\
    \ class=\"anchor\" href=\"#14-downloading-the-pre-extracted-proofs-recommended\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1.4 Downloading the Pre-extracted Proofs (Recommended)</h3>\n<ol>\n\
    <li>\n<p>Download the CoqGym dataset <a href=\"https://drive.google.com/drive/folders/149m_17VkYYkl0kdSB4AI8zodCuTmPaA6?usp=sharing\"\
    \ rel=\"nofollow\">here</a></p>\n</li>\n<li>\n<p>Unzip the data and set the paths:\
    \ <code>python unzip_data.py</code></p>\n</li>\n</ol>\n<p><em>Caveat</em>: The\
    \ second step sets the absolute paths in the data. You have to re-do it whenever\
    \ the absolote path of the <code>data/</code> directory changes (e.g. after moving\
    \ the entire repo to another directory).</p>\n<p>Now you are ready to interact\
    \ with CoqGym! Run <code>python eval_env.py</code> to check if it terminates normally\
    \ without raising an error.</p>\n<hr>\n<h2>\n<a id=\"user-content-2-using-coqgym-in-a-container\"\
    \ class=\"anchor\" href=\"#2-using-coqgym-in-a-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Using\
    \ CoqGym in a Container</h2>\n<p>As a less painful alternative to <a href=\"#1-installing-coqgym\"\
    >installing CoqGym</a> from scratch, we provide a pre-built Singularity container.\n\
    Feel free to skip these steps if you have finished installing CoqGym.\nCurrently\
    \ we do not support GPUs for the container, therefore you have to complete the\
    \ installation steps manually if you want to train models on CoqGym using GPUs.</p>\n\
    <h3>\n<a id=\"user-content-21-dependencies\" class=\"anchor\" href=\"#21-dependencies\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>2.1 Dependencies</h3>\n<ul>\n<li><a href=\"https://singularity.lbl.gov/\"\
    \ rel=\"nofollow\">Singularity</a></li>\n</ul>\n<h3>\n<a id=\"user-content-22-downloading-the-pre-built-container-image\"\
    \ class=\"anchor\" href=\"#22-downloading-the-pre-built-container-image\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2.2\
    \ Downloading the Pre-built Container Image</h3>\n<p>The container image can be\
    \ downloaded <a href=\"https://drive.google.com/drive/folders/13Rwa5no6W4MwSvdjRrENAdDCQqTWhcqy?usp=sharing\"\
    \ rel=\"nofollow\">here</a>.</p>\n<h3>\n<a id=\"user-content-23-using-the-container\"\
    \ class=\"anchor\" href=\"#23-using-the-container\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2.3 Using the\
    \ Container</h3>\n<ol>\n<li>Start a shell session inside the container: <code>singularity\
    \ shell coq_gym.simg</code>\n</li>\n<li>Run <code>source /.bashrc &amp;&amp; cd\
    \ /CoqGym &amp;&amp; eval $(opam env) &amp;&amp; conda activate coq_gym</code>\n\
    </li>\n</ol>\n<p>You are now ready to use CoqGym! Try <code>python eval_env.py</code>\
    \ to see if it terminates normally without raising an error.<br>\nFor further\
    \ instructions about how to use a Singularity container, please consult the documentation\
    \ of Singularity.</p>\n<h3>\n<a id=\"user-content-24-building-the-container-by-yourself\"\
    \ class=\"anchor\" href=\"#24-building-the-container-by-yourself\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2.4\
    \ Building the Container by Yourself</h3>\n<p>We provide a <a href=\"./Singularity\"\
    >Singularity recipe</a> from which you can build the container by yourself.</p>\n\
    <ol>\n<li>You need to be on a Linux machine of which you have sudo privileges.</li>\n\
    <li>Download the dataset <a href=\"https://drive.google.com/drive/folders/149m_17VkYYkl0kdSB4AI8zodCuTmPaA6?usp=sharing\"\
    \ rel=\"nofollow\">here</a> and put the files in your <code>CoqGym/</code> directory.</li>\n\
    <li>Run <code>sudo singularity build coq_gym.simg Singularity</code> to build\
    \ the container image <code>coq_gym.simg</code>\n</li>\n</ol>\n<p><em>Caveat</em>:\
    \ If you run out of disk space when building the container, it may because your\
    \ <code>/tmp</code> directory is not large enough. See <a href=\"https://singularity.lbl.gov/build-environment#temporary-folders\"\
    \ rel=\"nofollow\">https://singularity.lbl.gov/build-environment#temporary-folders</a>\
    \ for a workaround.</p>\n<hr>\n<h2>\n<a id=\"user-content-3-data-format\" class=\"\
    anchor\" href=\"#3-data-format\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>3. Data Format</h2>\n<p>The dataset\
    \ contains three parts:</p>\n<ul>\n<li>\n<p>The <code>data</code> directory: <code>*.json</code>\
    \ files corresponding to the <code>*.v</code> files in Coq source code, whose\
    \ format is explained below. The <code>*.json</code> files contain all important\
    \ information about the proofs: environment, local context, goals, tactics, proof\
    \ trees, etc.</p>\n</li>\n<li>\n<p>The <code>sexp_cache</code> directory: A LMDB\
    \ file that serves as an index for the S-expressions in <code>*.json</code> files.\
    \ The <code>*.json</code> files contain keys for querying <code>sexp_cache</code></p>\n\
    </li>\n<li>\n<p><code>projs_split.json</code>: A JSON object containing the training/validation/testing\
    \ split</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-31-json-files\" class=\"\
    anchor\" href=\"#31-json-files\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>3.1 JSON Files</h3>\n<p>Each <code>*.json</code>\
    \ file in <code>data/</code> corresponds to a Coq source file <code>*.v</code>\
    \ in <code>coq_projects/</code>. For example, <code>data/StructTact/Assoc.json</code>\
    \ corresponds to <a href=\"https://github.com/princeton-vl/CoqGym/blob/master/coq_projects/StructTact/Assoc.v\"\
    >coq_projects/StructTact/Assoc.v</a>.</p>\n<p>The format of the JSON files is\
    \ described below.\nThe hash codes are used as keys to query the LMDB <code>sexp_cache</code>.\n\
    Consult the <a href=\"#33-glossary\">glossary</a> for the terminology.</p>\n<pre><code>{\n\
    \    'filename': 'Assoc.v',            # the path of the Coq source file relative\
    \ to the root directory of the Coq project\n    'coq_project': 'StructTact', \
    \     # the name of the Coq project\n    'vernac_cmds': [                  # a\
    \ list of Coq commands [6] in the source file\n        ['Cd \"$COQGYM_ROOT/coq_projects/StructTact\"\
    .', 'VernacChdir',  '3701e61f37b72b3e61788fce6317466b7bb92b55'],     # [raw command,\
    \ command type, command AST (in hash code)]\n        ['Arguments a_equiv {_} {_}\
    \ _ _ _.', 'VernacArguments', '6777d3c472595dae20427d0892ad03d38f70fde9'],\n \
    \       ...\n        ['Arguments a_equiv {_} {_} _ _ _.', 'VernacArguments', '6777d3c472595dae20427d0892ad03d38f70fde9'],\n\
    \    ],\n   'num_extra_cmds': 107,             # the code in the original Coq\
    \ file starts at vernac_cmds[num_extra_cmds]\n   'proofs': [                 \
    \       # a list of human-written proofs\n       ...\n    ],\n    'synthetic_proofs':\
    \ [             # a list of synthetic proofs\n       ...\n    ],\n}\n</code></pre>\n\
    <p>The format for a proof is as follows, taking the <a href=\"https://github.com/princeton-vl/CoqGym/blob/9267aeb9acd82b7735d0228c4f366f7b843a852b/coq_projects/StructTact/Assoc.v#L47\"\
    ><code>get_set_same</code></a> in <code>coq_projects/StructTact/Assoc.v</code>\
    \ as an example.</p>\n<pre><code>{\n    'name': get_set_same,             # the\
    \ name of the theorem\n    'line_nb': 118,                   # the theorem is\
    \ defined in $file_data['vernac_cmds'][$line_nb]\n    \n    'env_delta': {   \
    \                       # the global environment relative to the previous proof\
    \ in the same file\n        'add' : {                           # entries that\
    \ should be added to the environment\n            'constants' : [\n          \
    \      {\n                    'physical_path': 'coq/theories/Arith/PeanoNat.vo:Nat.mul_wd',\
    \       # the unique identifier\n                    'short_ident': 'PeanoNat.Nat.mul_wd',\
    \                               # the short identifier\n                    'qualid':\
    \ 'Coq.Arith.PeanoNat.Nat.mul_wd',             # the qualified identifier [1]\n\
    \                    'type': '@Morphisms.Proper (forall (_ : nat) (_ : nat), nat)\
    \ (@Morphisms.respectful nat (forall _ : nat, nat) (@eq nat) (@Morphisms.respectful\
    \ nat nat (@eq nat) (@eq nat))) Nat.mul',    # the type [7] of the constant\n\
    \                    'sort': 'Prop',                                        #\
    \ the sort [2] of the constant (the type of its type) [2]\n                  \
    \  'opaque': False,                                       # whether the constant\
    \ is opaque or transparent [3]\n                    'sexp': '333b2895c8e62d21856476bf89fa9681c9058bb9'\
    \     # the S-expression [4] of the constant produced by SerAPI\n            },\
    \   \n            ...\n            ],\n            'inductives' : [\n        \
    \        {\n                    'physical_path' : 'coq/theories/Init/Wf.vo:Acc',\n\
    \                    'blocks': [           # a list of blocks in a mutual inductive\
    \ definition [5]. For regular inductive definitions (most cases), the list has\
    \ length 1\n                        {\n                            'short_ident':\
    \ 'Acc',\n                            'qualid': 'Coq.Init.Wf.Acc',\n         \
    \                   'constructors': [\n                                ['Acc_intro',\
    \ 'forall (A : Type) (R : forall (_ : A) (_ : A), Prop) (x : A) (_ : forall (y\
    \ : A) (_ : R y x), _UNBOUND_REL_6 A R y), _UNBOUND_REL_5 A R x'],      # [constructor\
    \ name, constructor type]\n                                ...\n             \
    \               ]\n                        }\n                    ],\n       \
    \             'is_record': False,\n                    'sexp': '31537cb98179ad7d2de0dd2cc783b4672b34b25b'\n\
    \            }\n            ...\n            \n            ],\n        },\n  \
    \      'subtract' : {                      # entries that should be removed from\
    \ the environment\n            'constants' : [],\n            'inductives' : [],\n\
    \        },\n    },\n    \n    'steps': [                        # a list of proof\
    \ steps\n        {\n            'command': ['induction l; intros; simpl; repeat\
    \ (break_match; simpl); subst; congruence.', 'VernacExtend', 'f6d2cb314d72d23562e5f2ef2657bd2589d44794'],\
    \        # (raw command, command type, command AST (in hash code)) the Coq command\
    \ (usually a tactic but also includes other commands such as +, -, *, etc.) \n\
    \            'goal_ids': {             # the IDs of the goals in the current proof\
    \ step\n                'fg': [27],           # focused goals \n             \
    \   'bg': [] .            # unfocused goals\n            }\n        }\n      \
    \  ...\n    ],      \n    \n    'goals' {                         # the goals\n\
    \        27': {                        # $goal_id -&gt; goal\n               \
    \  'id': 27,            # the goal ID\n                 'type': 'forall (k : K)\
    \ (v : V) (l : list (prod K V)), @eq (option V) (assoc (assoc_set l k v) k) (@Some\
    \ V v)',      # the type (logical statement) of the goal\n                 'hypotheses':\
    \ [      # the local context, a list of local premises\n                     {'idents':\
    \ ['K', 'V'], 'term': [], 'type': 'Type', 'sexp': 'cd1531c49fce6657997962b5375a3ef0a59db34a'}\
    \       # {'idents': [a list of identifiers of the premises (usually of length\
    \ one)], 'term': [a list of Coq terms (usually empty)], 'type': the type (logical\
    \ statement) of the premise}\n                     {'idents': ['K_eq_dec'], 'term':\
    \ [], 'type': \"forall k k' : K, sumbool (@eq K k k') (not (@eq K k k'))\", 'sexp':\
    \ '5f5f5bcf9e10621f8c0c4642c0eba3ff36cbfff8'},\n                 ],\n        \
    \     }\n    },    \n    \n    'proof_tree' : {                  # the proof tree\n\
    \        'goal_id': 27, 'children': []\n    },                     \n}\n</code></pre>\n\
    <h3>\n<a id=\"user-content-32-lmdb-file\" class=\"anchor\" href=\"#32-lmdb-file\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>3.2 LMDB File</h3>\n<p><code>sexp_cache</code> is a LMDB mapping hash\
    \ codes in <code>*.json</code> files to their corresponding S-expressions. Below\
    \ is a code snippet in Python for accessing them.</p>\n<div class=\"highlight\
    \ highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"\
    pl-s1\">utils</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SexpCache</span>\n\
    <span class=\"pl-s1\">sexp_cache</span> <span class=\"pl-c1\">=</span> <span class=\"\
    pl-v\">SexpCache</span>(<span class=\"pl-s\">'sexp_cache'</span>)\n<span class=\"\
    pl-en\">print</span>(<span class=\"pl-s1\">sexp_cache</span>[<span class=\"pl-s\"\
    >'333b2895c8e62d21856476bf89fa9681c9058bb9'</span>])</pre></div>\n<h3>\n<a id=\"\
    user-content-33-glossary\" class=\"anchor\" href=\"#33-glossary\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3.3\
    \ Glossary</h3>\n<ol>\n<li><a href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#qualified-identifiers-and-simple-identifiers\"\
    \ rel=\"nofollow\">qualified identifier</a></li>\n<li><a href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#sorts\"\
    \ rel=\"nofollow\">sort</a></li>\n<li><a href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#sorts\"\
    \ rel=\"nofollow\">opaque, transparent</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/S-expression\"\
    \ rel=\"nofollow\">S-expression</a></li>\n<li><a href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#mutually-defined-inductive-types\"\
    \ rel=\"nofollow\">inductive definition, mutual inductive definition</a></li>\n\
    <li><a href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#mutually-defined-inductive-types\"\
    \ rel=\"nofollow\">Coq (vernac) command</a></li>\n<li><a href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#types\"\
    \ rel=\"nofollow\">type</a></li>\n</ol>\n<hr>\n<h2>\n<a id=\"user-content-4-data-utilities\"\
    \ class=\"anchor\" href=\"#4-data-utilities\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>4. Data Utilities</h2>\n<p>We\
    \ include some tools for interacting with CoqGym, but they are NOT a part of the\
    \ dataset.\nYou may implement your own tools for similar purposes.</p>\n<h3>\n\
    <a id=\"user-content-41-interacting-with-coqgym\" class=\"anchor\" href=\"#41-interacting-with-coqgym\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>4.1 Interacting with CoqGym</h3>\n<p><code>eval_env.py</code> enables\
    \ the interaction with the proofs in CoqGym.\nSee <a href=\"ASTactic/agent.py\"\
    >ASTactic/agent.py</a> for examples.</p>\n<h3>\n<a id=\"user-content-42-parsing-coq-terms\"\
    \ class=\"anchor\" href=\"#42-parsing-coq-terms\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>4.2 Parsing Coq Terms</h3>\n\
    <ul>\n<li>\n<p><code>gallina.py</code>: a parser the S-expressions of Coq terms.\n\
    It may be useful for learning the embeddings of the term ASTs.</p>\n</li>\n<li>\n\
    <p><code>utils.py</code>: functions for iterating through all proofs or Coq files\
    \ in the dataset.</p>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-43-computing-dataset-statistics\"\
    \ class=\"anchor\" href=\"#43-computing-dataset-statistics\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>4.3 Computing\
    \ Dataset Statistics</h3>\n<ul>\n<li>\n<p><code>stats/count_human_proofs.py</code>:\
    \ count the number of human-written proofs in CoqGym.</p>\n</li>\n<li>\n<p><code>stats/count_synthetic_proofs.py</code>:\
    \ count the number of synthetic-written proofs in CoqGym.</p>\n</li>\n<li>\n<p><code>stats/proofs.py</code>:\
    \ compute some statistics of the proofs.</p>\n</li>\n</ul>\n<hr>\n<h2>\n<a id=\"\
    user-content-5-the-astactic-model\" class=\"anchor\" href=\"#5-the-astactic-model\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>5. The ASTactic Model</h2>\n<p><a href=\"/images/astactic.jpg\" target=\"\
    _blank\" rel=\"noopener noreferrer\"><img src=\"/images/astactic.jpg\" alt=\"\
    ASTactic\" style=\"max-width:100%;\"></a></p>\n<p>Here we describe how to train\
    \ and test the ASTactic model on CoqGym.\nThe following content is NOT a part\
    \ of the CoqGym dataset, and therefore you do not need it if you only want to\
    \ access the data.</p>\n<h3>\n<a id=\"user-content-51-prerequisites\" class=\"\
    anchor\" href=\"#51-prerequisites\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>5.1 Prerequisites</h3>\n<ul>\n\
    <li>Make sure CoqGym has been properly installed and configured. The <code>coq_gym</code>\
    \ conda environment is activated, the OPAM switch is on <code>4.07.1+flambda</code>.</li>\n\
    <li>Automated theorem provers: <a href=\"https://vprover.github.io\" rel=\"nofollow\"\
    >Vampire</a>, <a href=\"http://cvc4.cs.stanford.edu/\" rel=\"nofollow\">CVC4</a>,\
    \ <a href=\"http://www.eprover.org\" rel=\"nofollow\">Eprover</a>, and <a href=\"\
    https://github.com/Z3Prover/z3\">Z3</a>. Install all of them and make sure they\
    \ are accessible in PATH, otherwise you may see a performance degradation of the\
    \ hammer baseline.</li>\n<li>\n<a href=\"https://pytorch.org/\" rel=\"nofollow\"\
    >PyTorch</a>: Install the correct version for your hardware in the conda environment\
    \ <code>coq_gym</code>.</li>\n<li>The instructions below assume that you are in\
    \ the <a href=\"./ASTactic/\">ASTactic</a> directory.</li>\n</ul>\n<h3>\n<a id=\"\
    user-content-52-extracting-proof-steps\" class=\"anchor\" href=\"#52-extracting-proof-steps\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>5.2 Extracting Proof Steps</h3>\n<p>The ASTactic model is trained\
    \ on individual proof steps, rather than entire proofs.\nAfter obtaining the CoqGym\
    \ dataset, run <code>python extract_proof_steps.py</code>. This can take a while,\
    \ and you have the option to run it in parallel, please see the <code>--filter</code>\
    \ option in the source code for details.</p>\n<p>The extracted proof steps are\
    \ in <code>proof_steps/</code>. You can double-check the number of proof steps\
    \ to make sure everything works as expected:</p>\n<table>\n<thead>\n<tr>\n<th>Directory</th>\n\
    <th># files</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>proof_steps/train</td>\n\
    <td>121,644</td>\n</tr>\n<tr>\n<td>proof_steps/valid</td>\n<td>68,180</td>\n</tr>\n\
    </tbody>\n</table>\n<h3>\n<a id=\"user-content-53-training\" class=\"anchor\"\
    \ href=\"#53-training\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>5.3 Training</h3>\n<p>To train on the proof\
    \ steps in training + validation set: <code>python main.py --no_validation --exp_id\
    \ astactic</code><br>\nThe \"astactic\" above is an experiment ID, and you may\
    \ change it to other IDs. Model checkpoints will be saved to <code>runs/astactic/checkpoints/</code>.\
    \ See <code>options.py</code> for command line options.</p>\n<p>A pre-trained\
    \ model can be downloaded <a href=\"https://drive.google.com/drive/folders/1AzLaEpoGS3BPMUz9Bl63MHAFRqlF4CtH?usp=sharing\"\
    \ rel=\"nofollow\">here</a>.</p>\n<h3>\n<a id=\"user-content-54-testing\" class=\"\
    anchor\" href=\"#54-testing\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>5.4 Testing</h3>\n<p>Assuming you\
    \ want to test the model checkpoint <code>runs/astactic/checkpoints/model_003.pth</code>\
    \ on the proof <code>get_set_same</code> in <code>../data/StructTact/Assoc.json</code>:</p>\n\
    <ul>\n<li>\n<p>Testing ASTactic:<br>\n<code>python evaluate.py ours ours-TEST\
    \ --path runs/astactic/checkpoints/model_003.pth --file ../data/StructTact/Assoc.json\
    \ --proof \"get_set_same\"</code></p>\n</li>\n<li>\n<p>Testing an automated tactic\
    \ X (may be \"auto\", \"trivial\", \"easy\", \"intuition\", or \"hammer\"):<br>\n\
    <code>python -u evaluate.py X X-TEST --file ../data/StructTact/Assoc.json --proof\
    \ \"get_set_same\"</code></p>\n</li>\n<li>\n<p>Testing ASTactic+X:<br>\n<code>python\
    \ -u evaluate.py ours+X ours+X-TEST --path runs/astactic/checkpoints/model_003.pth\
    \ --file ../data/StructTact/Assoc.json --proof \"get_set_same\"</code></p>\n</li>\n\
    </ul>\n<p><em>Caveat</em>: Testing is computationally expensive, but the workloads\
    \ are embarrassingly parallel, which means you can run them in parallel in any\
    \ order. We do not provide the code for that because it depends on a particular\
    \ HPC infrastructure.</p>\n<h2>\n<a id=\"user-content-6-credits\" class=\"anchor\"\
    \ href=\"#6-credits\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>6. Credits</h2>\n<ul>\n<li>The code is formatted\
    \ using <a href=\"https://github.com/psf/black\"><img src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"\
    \ alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"\
    \ style=\"max-width:100%;\"></a>.</li>\n<li>This repo includes the codebase of\
    \ <a href=\"https://github.com/coq/coq\">Coq</a>, <a href=\"https://github.com/ejgallego/coq-serapi\"\
    >SerAPI</a>, <a href=\"https://github.com/lukaszcz/coqhammer\">CoqHammer</a>,\
    \ and the Coq projects in <a href=\"./coq_projects\">coq_projects</a>.</li>\n\
    </ul>\n<h2>\n<a id=\"user-content-7-contributing\" class=\"anchor\" href=\"#7-contributing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>7. Contributing</h2>\n<p>We welcome and appreciate contributions from\
    \ the community. For bug fixes and relatively minor changes (such as comments,\
    \ typos, etc.), feel free to submit a pull request directly. For anything beyond,\
    \ please first post in <a href=\"https://github.com/princeton-vl/CoqGym/discussions\"\
    >Discussions</a> before implementing.</p>\n"
  stargazers_count: 210
  subscribers_count: 16
  topics:
  - theorem-proving
  - machine-learning
  - icml-2019
  updated_at: 1621517065.0
provarepro/mlperf_inference:
  data_format: 2
  description: MLPerf Inference containers recipes
  filenames:
  - v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_rt_c_tbb-py36-gcc75-ubuntu18
  - v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_src_c_tbb-py36-gcc75-ubuntu18
  - v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_src_c_omp-py36-gcc75-ubuntu18
  - v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_rt_cg_tbb-py36-gcc75-ubuntu18
  full_name: provarepro/mlperf_inference
  latest_release: 0.0.1
  readme: '<h1>

    <a id="user-content-mlperf-inference" class="anchor" href="#mlperf-inference"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MLPerf
    Inference</h1>

    <p>MLPerf Inference containers recipes</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621303140.0
psadil/staHB:
  data_format: 2
  description: Hierarchical Bayesian State Trace Analysis
  filenames:
  - tools/Singularity
  full_name: psadil/staHB
  latest_release: null
  readme: '<h1>

    <a id="user-content-stahb" class="anchor" href="#stahb" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>staHB</h1>

    <p>Hierarchical Bayesian State Trace Analysis</p>

    <p>This repository aims to compare a few different methods of conducting an STA.
    As such, it includes some code from both <a href="https://github.com/michaelkalish/STA">Kalish,
    M. L., Dunn, J. C., Burdakov, O. P., &amp; Sysoev, O. (2016). A statistical test
    of the equality of latent orders. Journal of Mathematical Psychology, 70, 1-11.</a>
    and <a href="http://dx.doi.org/10.1016/j.jmp.2015.08.004" rel="nofollow">Davis-Stober,
    C. P., Morey, R. D., Gretton, M., &amp; Heathcote, A. (2016). Bayes factors for
    state-trace analysis. Journal of Mathematical Psychology, 72, 116-129.</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1538170618.0
pscedu/singularity-bamtools:
  data_format: 2
  description: C++ API & command-line toolkit for working with BAM data
  filenames:
  - 2.5.1/Singularity
  full_name: pscedu/singularity-bamtools
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-bamtools\" class=\"anchor\" href=\"\
    #singularity-bamtools\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-bamtools</h1>\n<p><a href=\"https://www.travis-ci.com/icaoberg/singularity-bamtools\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/188d000bd28e4b53608796e6b21543dfcb90a5074619295b1c037b42ae625ab8/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d62616d746f6f6c732e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-bamtools.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Source code repository can be found <a\
    \ href=\"https://github.com/trinityrnaseq/trinityrnaseq\">here</a>.</p>\n<h2>\n\
    <a id=\"user-content-building-the-container-for-bridges-or-similar\" class=\"\
    anchor\" href=\"#building-the-container-for-bridges-or-similar\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the container for Bridges (or similar)</h2>\n<p>There is no need to build a\
    \ container, because an image is already available from the Galaxy project, hence\
    \ all you need to do is run</p>\n<pre><code>bash ./pull.sh\n</code></pre>\n<p>To\
    \ avoid that pesky warning when building directly from a Docker container, run</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-or-similar\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges (or similar)</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the <code>bamtools</code> script</li>\n</ul>\n<p>to <code>/opt/packages/bamtools/2.5.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/bamtools</code>\
    \ as <code>2.5.1.lua</code>.</p>\n<h2>\n<a id=\"user-content-test\" class=\"anchor\"\
    \ href=\"#test\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Test</h2>\n<p>If <code>test.sh</code> is available,\
    \ then run the command</p>\n<pre><code>bash ./test.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n<p><a\
    \ href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a> at\
    \ the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\
    \ Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  - bamtools
  updated_at: 1621305791.0
pscedu/singularity-gent:
  data_format: 2
  description: GeNT
  filenames:
  - 1.0.0/Singularity
  full_name: pscedu/singularity-gent
  latest_release: 1.0.0
  readme: "<h1>\n<a id=\"user-content-singularity-gent\" class=\"anchor\" href=\"\
    #singularity-gent\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>singularity-gent</h1>\n<p><a href=\"https://www.travis-ci.com/icaoberg/singularity-gent\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/be85592cab6539d1961fcaa7b022c254ec8d8a5d86f4282c9fc6daa968f50e59/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d67656e742e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-gent.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for GeNT.</p>\n<h2>\n\
    <a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"\
    #building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n<p><a\
    \ href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a> at\
    \ the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\n\
    Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  updated_at: 1621306030.0
pscedu/singularity-hmmer:
  data_format: 2
  description: null
  filenames:
  - 3.3.1/Singularity
  full_name: pscedu/singularity-hmmer
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-hmmer\" class=\"anchor\" href=\"\
    #singularity-hmmer\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-hmmer</h1>\n<p>Singularity recipe\
    \ for <a href=\"http://hmmer.org/\" rel=\"nofollow\">HMMER</a>.</p>\n<h2>\n<a\
    \ id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"\
    #building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n\
    <p><a href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a>\
    \ at the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\n\
    Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621305894.0
pscedu/singularity-tiger:
  data_format: 2
  description: Target / Integrative Genetic Element Retriever
  filenames:
  - 5.32.1/Singularity
  full_name: pscedu/singularity-tiger
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-singularity-tiger\" class=\"anchor\" href=\"\
    #singularity-tiger\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-tiger</h1>\n<p><a href=\"https://www.travis-ci.com/icaoberg/singularity-tiger\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8a4d8b01a51057c102939a9fe9eafed97007d23af59106e3b1ae3ff2fa88f649/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d74696765722e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-tiger.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for <a href=\"https://github.com/sandialabs/TIGER\"\
    >TIGER</a>.</p>\n<h2>\n<a id=\"user-content-building-the-image-using-the-recipe\"\
    \ class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Building\
    \ the image using the recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\"\
    \ class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build the\
    \ image locally</h3>\n<p>Run the script <code>build.sh</code> to build image locally.</p>\n\
    <pre><code>bash ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./rbuild.sh\n</code></pre>\n<h2>\n<a id=\"user-content-installing-the-container-on-bridges-or-similar\"\
    \ class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing\
    \ the container on Bridges (or similar)</h2>\n<p>Copy the</p>\n<ul>\n<li>\n<code>SIF</code>\
    \ file</li>\n<li>and the Perl scripts</li>\n</ul>\n<p>to <code>/opt/packages/tiger/5.32.1</code>.</p>\n\
    <p>Copy the file <code>modulefile.lua</code> to <code>/opt/modules/tiger</code>\
    \ as <code>5.32.1.lua</code>.</p>\n<hr>\n<p>Copyright \xA9 2021 Pittsburgh Supercomputing\
    \ Center. All Rights Reserved.</p>\n<p><a href=\"http://www.andrew.cmu.edu/~icaoberg\"\
    \ rel=\"nofollow\">icaoberg</a> at the <a href=\"http://www.psc.edu\" rel=\"nofollow\"\
    >Pittsburgh Supercomputing\nCenter</a> in the <a href=\"https://www.cmu.edu/mcs/\"\
    \ rel=\"nofollow\">Mellon College of Science</a> at <a href=\"http://www.cmu.edu\"\
    \ rel=\"nofollow\">Carnegie Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - tiger
  updated_at: 1621316658.0
pscedu/singularity-trimmomatic:
  data_format: 2
  description: Trimmomatic performs a variety of useful trimming tasks for illumina
    paired-end and single ended data.
  filenames:
  - 0.39/Singularity
  full_name: pscedu/singularity-trimmomatic
  latest_release: '0.39'
  readme: "<h1>\n<a id=\"user-content-singularity-trimmomatic\" class=\"anchor\" href=\"\
    #singularity-trimmomatic\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>singularity-trimmomatic</h1>\n<p><a href=\"\
    https://www.travis-ci.com/icaoberg/singularity-trimmomatic\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/4dde8880c00e91090a3dcb55e0f9d8189179c34c1c845d884436f9760ca2f8e6/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d7472696d6d6f6d617469632e7376673f6272616e63683d6d61696e\"\
    \ alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-trimmomatic.svg?branch=main\"\
    \ style=\"max-width:100%;\"></a></p>\n<p>Singularity recipe for Trimmomatic.</p>\n\
    <h2>\n<a id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\"\
    \ href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Building the image using the\
    \ recipe</h2>\n<h3>\n<a id=\"user-content-to-build-the-image-locally\" class=\"\
    anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>To build the image locally</h3>\n\
    <p>Run the script <code>build.sh</code> to build image locally.</p>\n<pre><code>bash\
    \ ./build.sh\n</code></pre>\n<h3>\n<a id=\"user-content-to-build-the-image-remotely\"\
    \ class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To build\
    \ the image remotely</h3>\n<p>Run the script <code>rbuild.sh</code> to build image\
    \ locally.</p>\n<pre><code>bash ./build.sh\n</code></pre>\n<hr>\n<p>Copyright\
    \ \xA9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.</p>\n<p><a\
    \ href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\">icaoberg</a> at\
    \ the <a href=\"http://www.psc.edu\" rel=\"nofollow\">Pittsburgh Supercomputing\
    \ Center</a> in the <a href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\">Mellon\
    \ College of Science</a> at <a href=\"http://www.cmu.edu\" rel=\"nofollow\">Carnegie\
    \ Mellon University</a>.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - singularity
  - bioinformatics
  - trimmomatic
  updated_at: 1621305830.0
psychoinformatics-de/hirni-toolbox:
  data_format: 2
  description: null
  filenames:
  - data-retrieval/XNAT/Singularity.xnat_crawler
  - analyses/mriqc/Singularity.mriqc
  - converters/heudiconv/Singularity.heudiconv
  - postprocessing/defacing/mridefacer/Singularity.mridefacer
  - postprocessing/fsl/Singularity.fsl
  full_name: psychoinformatics-de/hirni-toolbox
  latest_release: null
  readme: '<p><a href="https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg"
    alt="Build" style="max-width:100%;"></a>

    <a href="https://codecov.io/gh/Clinical-Genomics/fluffy" rel="nofollow"><img src="https://camo.githubusercontent.com/5a8950551f5fd61495950779e32145a28a2346b9809af6e347b18f58dce06213/68747470733a2f2f636f6465636f762e696f2f67682f436c696e6963616c2d47656e6f6d6963732f666c756666792f6272616e63682f6d61737465722f67726170682f62616467652e737667"
    alt="codecov" data-canonical-src="https://codecov.io/gh/Clinical-Genomics/fluffy/branch/master/graph/badge.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-fluffypipe" class="anchor" href="#fluffypipe" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>FluFFyPipe</h1>

    <p>NIPT analysis pipeline, using WisecondorX for detecting aneuplodies and large
    CNVs, AMYCNE for FFY and PREFACE for FF prediction (optional). FluFFYPipe produces
    a variety of output files, as well as a per batch csv summary.</p>

    <p align="center">

    <a href="https://github.com/J35P312/FluFFyPipe/blob/master/logo/IMG_20200320_132001.jpg"
    target="_blank" rel="noopener noreferrer"><img src="https://github.com/J35P312/FluFFyPipe/raw/master/logo/IMG_20200320_132001.jpg"
    width="400" height="400" style="max-width:100%;"></a>

    </p>

    <h1>

    <a id="user-content-run-fluffypipe" class="anchor" href="#run-fluffypipe" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Run FluFFyPipe</h1>

    <p>Run NIPT analysis, using a previously comnputed reference:</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; analyse

    </code></pre>

    <p>Run NIPT analysis, using an internally computed reference (i.e the reference
    is built using all samples listed in samplesheet):</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; analyse --batch-ref

    </code></pre>

    <p>optionally, skip preface:</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; --skip_preface analyse

    </code></pre>

    <p>All output will be written to the output folder, this output includes:</p>

    <pre><code>bam files

    wisecondorX output

    tiddit coverage summary

    Fetal fraction estimation

    </code></pre>

    <p>as well as a summary csv and multiqc html (per batch)</p>

    <p>the input folder is a project folder containing one folder per sample, each
    of these subfolders contain the fastq file(s).

    The samplesheet contains at least a "sampleID" column, the sampleID should match
    the subfolders in the input folder. The samplesheet may contain other columns,
    such as flowcell and index folder: such columns will be printed to the summary
    csv.

    If the samplesheet contains a SampleName column, fluffy will name the output according
    to SampleName</p>

    <p>Create a WisecondorX reference</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; reference

    </code></pre>

    <p>samplesheet should contain atleast a "sampleID" column. All samples in the
    samplesheet will be used to construct the reference, visit the WisecondorX manual
    for more information.</p>

    <h1>

    <a id="user-content-troubleshooting-and-rerun" class="anchor" href="#troubleshooting-and-rerun"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Troubleshooting
    and rerun</h1>

    <p>There are three statuses of the fluffy pipeline:

    running, complete, and failed</p>

    <p>The status of a fluffy run is found in the</p>

    <pre><code>&lt;output_folder&gt;/analysis_status.json

    </code></pre>

    <p>The status of all jobs are listed in</p>

    <pre><code>&lt;output_folder&gt;/sacct/fluffy_&lt;date&gt;.log.status

    </code></pre>

    <p>Where  is the timepoint when the jobs were submitted

    Use grep to find the failed jobs:</p>

    <pre><code>grep -v COMPLETE &lt;output_folder&gt;/sacct/fluffy_&lt;date&gt;.log.status

    </code></pre>

    <p>The output logs are stored in:</p>

    <pre><code> &lt;output_folder&gt;/logs

    </code></pre>

    <p>Before continuing, you may want to generate the summary csv for all completed
    cases:</p>

    <pre><code>bash &lt;output_folder&gt;/scripts/summarizebatch-&lt;hash&gt;

    </code></pre>

    <p>where  is a randomly generated string.</p>

    <p>use the rerun module to rerun failed fluffy analyses:</p>

    <pre><code>fluffy --sample &lt;samplesheet&gt;  --project &lt;input_folder&gt;
    --out &lt;output_folder&gt; --skip_preface rerun

    </code></pre>

    <h1>

    <a id="user-content-install-fluffypipe" class="anchor" href="#install-fluffypipe"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    FluFFyPipe</h1>

    <p>FluFFyPipe requires python 3, slurm, slurmpy, and singularity, python-coloredlogs.</p>

    <p>fluffy may be installed using pip:</p>

    <pre><code>pip install fluffy-cg

    </code></pre>

    <p>alternatively, fluffy is cloned and installed from github:

    git clone <a href="https://github.com/Clinical-Genomics/fluffy">https://github.com/Clinical-Genomics/fluffy</a>

    cd fluffy

    pip install -e .</p>

    <p>Next download the FluFFyPipe singularity container</p>

    <pre><code> singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe

    </code></pre>

    <p>copy the example config (found in example_config), and edit the variables.

    You will need to download/create the following files:</p>

    <pre><code>Reference fasta (indexed using bwa)


    WisecondorX reference files (created using the reference mode)


    PREFACE model file (optional)


    blacklist bed file (used by wisecondorX)


    FluFFyPipe singularity collection (singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe)

    </code></pre>

    '
  stargazers_count: 2
  subscribers_count: 5
  topics: []
  updated_at: 1616079674.0
pvelesko/singularity_files:
  data_format: 2
  description: null
  filenames:
  - Singularity.intelpython3_beer
  full_name: pvelesko/singularity_files
  latest_release: null
  readme: '<p>Singulairy container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1566267553.0
pvrqualitasag/genmon-sidef:
  data_format: 2
  description: Singularity definition and tools for genmon
  filenames:
  - def/Singularity_apache2.recipe
  - def/Singularity_poprep.recipe
  - def/Singularity_genmon.recipe
  full_name: pvrqualitasag/genmon-sidef
  latest_release: null
  readme: '<h1>

    <a id="user-content-genmon-sidef" class="anchor" href="#genmon-sidef" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>genmon-sidef</h1>

    <p>Singularity definition and tools for GenMon. GenMon is tool for monitoring
    genetic resources of animal populations.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621509789.0
pwiszniewski/SingularityTest:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: pwiszniewski/SingularityTest
  latest_release: null
  readme: '<h1>

    <a id="user-content-tvb-pipeline-sc" class="anchor" href="#tvb-pipeline-sc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>tvb-pipeline-sc</h1>

    <p>The dwMRI preprocessing leg of the TVB processing pipeline. Initial version
    cloned from BIDS-Apps/MRtrix3_connectome.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1605105699.0
quadram-institute-bioscience/ncov2019-artic-nf:
  data_format: 2
  description: null
  filenames:
  - environments/illumina/Singularity
  - environments/nanopore/Singularity
  full_name: quadram-institute-bioscience/ncov2019-artic-nf
  latest_release: null
  readme: '<h1>

    <a id="user-content-ncov2019-artic-nf" class="anchor" href="#ncov2019-artic-nf"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ncov2019-artic-nf</h1>

    <p>A Nextflow pipeline for running the ARTIC network''s fieldbioinformatics tools
    (<a href="https://github.com/artic-network/fieldbioinformatics">https://github.com/artic-network/fieldbioinformatics</a>),
    with a focus on ncov2019</p>

    <p>WARNING - THIS REPO IS UNDER ACTIVE DEVELOPMENT AND ITS BEHAVIOUR MAY CHANGE
    AT <strong>ANY</strong> TIME.</p>

    <p>PLEASE ENSURE THAT YOU READ BOTH THE README AND THE CONFIG FILE AND UNDERSTAND
    THE EFFECT OF THE OPTIONS ON YOUR DATA!</p>

    <h4>

    <a id="user-content-qib-settings" class="anchor" href="#qib-settings" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>QIB settings</h4>

    <p>For using the pipeline with QIB setting, i.e. primers V3 and cluster configuration,
    please use <strong>qib</strong> branch</p>

    <div class="highlight highlight-source-shell"><pre>git checkout qib</pre></div>

    <p>Command line to execute the pipeline for a run</p>

    <h4>

    <a id="user-content-illumina-data" class="anchor" href="#illumina-data" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Illumina data</h4>

    <div class="highlight highlight-source-shell"><pre>nextflow run /path/to/ncov2019-artic-nf/folder
    \

    --directory <span class="pl-s"><span class="pl-pds">"</span>/path/to/illumina/fastq/folder<span
    class="pl-pds">"</span></span> \

    --illumina \

    --readTrimming \

    --get_all <span class="pl-cce">\ </span><span class="pl-c"><span class="pl-c">#</span>Put
    all samples (failed and passed qc) into qc_climb_upload folder</span>

    --prefix <span class="pl-s"><span class="pl-pds">"</span>test-v3<span class="pl-pds">"</span></span>
    \

    --outdir <span class="pl-s"><span class="pl-pds">"</span>/path/to/output/folder<span
    class="pl-pds">"</span></span> \

    --bed <span class="pl-s"><span class="pl-pds">"</span>/beegfs/software/ncov2019-artic-nf/primers/scheme/primer_schemes/nCoV-2019/V3/nCoV-2019.bed<span
    class="pl-pds">"</span></span> \

    --ref <span class="pl-s"><span class="pl-pds">"</span>/beegfs/software/ncov2019-artic-nf/primers/scheme/primer_schemes/nCoV-2019/V3/nCoV-2019.reference.fasta<span
    class="pl-pds">"</span></span> \

    --fourLanes <span class="pl-cce">\ </span><span class="pl-c"><span class="pl-c">#</span>Use
    when 4-lane fastqs were not merged</span>

    -profile qib,singularity \

    -with-trace trace-4lanes.txt \

    -with-report report-4lanes.html</pre></div>

    <h4>

    <a id="user-content-nanopore-data-artic-pipeline" class="anchor" href="#nanopore-data-artic-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nanopore
    data (ARTIC pipeline)</h4>

    <pre><code>nextflow run /path/to/ncov2019-artic-nf/folder

    --outdir "/path/to/output" \

    --prefix "NORW-20200418" \

    --useGuppyPlex \

    --min_length 100 \

    --max_length 550 \

    --normalise 500 \

    -profile qib,singularity \

    --medaka \

    --basecalled_fastq "/path/to/basedcall_folder/"

    -with-trace trace.txt \

    -with-report report.html

    </code></pre>

    <h1>

    <a id="user-content-original-readme" class="anchor" href="#original-readme" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Original Readme</h1>

    <h4>

    <a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h4>

    <hr>

    <p>This Nextflow pipeline automates the ARTIC network <a href="https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html"
    title="nCoV-2019 novel coronavirus bioinformatics protocol" rel="nofollow">nCoV-2019
    novel coronavirus bioinformatics protocol</a>. It is being developed to aid the
    harmonisation of the analysis of sequencing data generated by the <a href="https://github.com/COG-UK">COG-UK</a>
    project. It will turn SARS-COV2 sequencing data (Illumina or Nanopore) into consensus
    sequences and provide other helpful outputs to assist the project''s sequencing
    centres with submitting data.</p>

    <h4>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick-start</h4>

    <h5>

    <a id="user-content-illumina" class="anchor" href="#illumina" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Illumina</h5>

    <p><code>nextflow run connor-lab/ncov2019-artic-nf [-profile conda,singularity,docker,slurm,lsf]
    --illumina --prefix "output_file_prefix" --directory /path/to/reads</code></p>

    <p>You can also use cram file input by passing the --cram flag.</p>

    <p>For production use at large scale, where you will run the workflow many times,
    you can avoid cloning the scheme repository, creating an ivar bed file and indexing
    the reference every time by supplying both --ivarBed /path/to/ivar-compatible.bed
    and --alignerRefPrefix /path/to/bwa-indexed/ref.fa.</p>

    <p>Alternatively you can avoid just the cloning of the scheme repository to remain
    on a fixed revision of it over time by passing --schemeRepoURL /path/to/own/clone/of/github.com/artic-network/artic-ncov2019.
    This removes any internet access from the workflow except for the optional upload
    steps.</p>

    <h5>

    <a id="user-content-nanopore" class="anchor" href="#nanopore" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Nanopore</h5>

    <h6>

    <a id="user-content-nanopolish" class="anchor" href="#nanopolish" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Nanopolish</h6>

    <p><code>nextflow run connor-lab/ncov2019-artic-nf [-profile conda,singularity,docker,slurm,lsf]
    --nanopolish --prefix "output_file_prefix" --basecalled_fastq /path/to/directory
    --fast5_pass /path/to/directory --sequencing_summary /path/to/sequencing_summary.txt</code></p>

    <h6>

    <a id="user-content-medaka" class="anchor" href="#medaka" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Medaka</h6>

    <p><code>nextflow run connor-lab/ncov2019-artic-nf [-profile conda,singularity,docker,slurm,lsf]
    --medaka --prefix "output_file_prefix" --basecalled_fastq /path/to/directory --fast5_pass
    /path/to/directory --sequencing_summary /path/to/sequencing_summary.txt</code></p>

    <h4>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h4>

    <p>An up-to-date version of Nextflow is required because the pipeline is written
    in DSL2. Following the instructions at <a href="https://www.nextflow.io/" rel="nofollow">https://www.nextflow.io/</a>
    to download and install Nextflow should get you a recent-enough version.</p>

    <h4>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Containers</h4>

    <p>This repo contains both Singularity and Dockerfiles. You can build the Singularity
    containers locally by running <code>scripts/build_singularity_containers.sh</code>
    and use them with <code>-profile singularity</code> The containers will be available
    from Docker/Singularityhub shortly.</p>

    <h4>

    <a id="user-content-conda" class="anchor" href="#conda" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Conda</h4>

    <p>The repo contains a environment.yml files which automatically build the correct
    conda env if <code>-profile conda</code> is specifed in the command. Although
    you''ll need <code>conda</code> installed, this is probably the easiest way to
    run this pipeline.</p>

    <p>--cache /some/dir can be specified to have a fixed, shared location to store
    the conda build for use by multiple runs of the workflow.</p>

    <h4>

    <a id="user-content-executors" class="anchor" href="#executors" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Executors</h4>

    <p>By default, the pipeline just runs on the local machine. You can specify <code>-profile
    slurm</code> to use a SLURM cluster, or <code>-profile lsf</code> to use an LSF
    cluster. In either case you may need to also use one of the COG-UK institutional
    config profiles (phw or sanger), or provide queue names to use in your own config
    file.</p>

    <h4>

    <a id="user-content-profiles" class="anchor" href="#profiles" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Profiles</h4>

    <p>You can use multiple profiles at once, separating them with a comma. This is
    described in the Nextflow <a href="https://www.nextflow.io/docs/latest/config.html#config-profiles"
    rel="nofollow">documentation</a></p>

    <h4>

    <a id="user-content-config" class="anchor" href="#config" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Config</h4>

    <p>Common configuration options are set in <code>conf/base.config</code>. Workflow
    specific configuration options are set in <code>conf/nanopore.config</code> and
    <code>conf/illumina.config</code> They are described and set to sensible defaults
    (as suggested in the <a href="https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html"
    title="nCoV-2019 novel coronavirus bioinformatics protocol" rel="nofollow">nCoV-2019
    novel coronavirus bioinformatics protocol</a>)</p>

    <h5>

    <a id="user-content-options" class="anchor" href="#options" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Options</h5>

    <ul>

    <li>

    <code>--outdir</code> sets the output directory.</li>

    <li>

    <code>--bwa</code> to swap to bwa for mapping (nanopore only).</li>

    </ul>

    <h5>

    <a id="user-content-workflows" class="anchor" href="#workflows" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Workflows</h5>

    <h6>

    <a id="user-content-nanopore-1" class="anchor" href="#nanopore-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Nanopore</h6>

    <p>Use <code>--nanopolish</code> or <code>--medaka</code> to run these workflows.
    <code>--basecalled_fastq</code> should point to a directory created by <code>guppy_basecaller</code>
    (if you ran with no barcodes), or <code>guppy_barcoder</code> (if you ran with
    barcodes). It is imperative that the following <code>guppy_barcoder</code> command
    be used for demultiplexing:</p>

    <pre><code>guppy_barcoder --require_barcodes_both_ends -i run_name -s output_directory
    --arrangements_files "barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg"

    </code></pre>

    <h6>

    <a id="user-content-illumina-1" class="anchor" href="#illumina-1" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Illumina</h6>

    <p>The Illumina workflow leans heavily on the excellent <a href="https://github.com/andersen-lab/ivar">ivar</a>
    for primer trimming and consensus making. This workflow will be updated to follow
    ivar, as its also in very active development! Use <code>--illumina</code> to run
    the Illumina workflow. Use <code>--directory</code> to point to an Illumina output
    directory usually coded something like: <code>&lt;date&gt;_&lt;machine_id&gt;_&lt;run_no&gt;_&lt;some_zeros&gt;_&lt;flowcell&gt;</code>.
    The workflow will recursively grab all fastq files under this directory, so be
    sure that what you want is in there, and what you don''t, isn''t!</p>

    <p>Important config options are:</p>

    <table>

    <thead>

    <tr>

    <th align="left">Option</th>

    <th align="right">Description</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td align="left">allowNoprimer</td>

    <td align="right">Allow reads that don''t have primer sequence? Ligation prep
    = false, nextera = true</td>

    </tr>

    <tr>

    <td align="left">illuminaKeepLen</td>

    <td align="right">Length of illumina reads to keep after primer trimming</td>

    </tr>

    <tr>

    <td align="left">illuminaQualThreshold</td>

    <td align="right">Sliding window quality threshold for keeping reads after primer
    trimming (illumina)</td>

    </tr>

    <tr>

    <td align="left">mpileupDepth</td>

    <td align="right">Mpileup depth for ivar</td>

    </tr>

    <tr>

    <td align="left">ivarFreqThreshold</td>

    <td align="right">ivar frequency threshold for variant</td>

    </tr>

    <tr>

    <td align="left">ivarMinDepth</td>

    <td align="right">Minimum coverage depth to call variant</td>

    </tr>

    </tbody>

    </table>

    <h4>

    <a id="user-content-qc" class="anchor" href="#qc" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>QC</h4>

    <p>A script to do some basic COG-UK QC is provided in <code>bin/qc.py</code>.
    This currently tests if &gt;50% of reference bases are covered by &gt;10 reads
    (Illumina) or &gt;20 reads (Nanopore), OR if there is a stretch of more than 10
    Kb of sequence without N - setting qc_pass in <code>&lt;outdir&gt;/&lt;prefix&gt;.qc.csv</code>
    to TRUE. <code>bin/qc.py</code> can be extended to incorporate any QC test, as
    long as the script outputs a csv file a "qc_pass" last column, with samples TRUE
    or FALSE.</p>

    <h4>

    <a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Output</h4>

    <p>A subdirectory for each process in the workflow is created in <code>--outdir</code>.
    A <code>qc_pass_climb_upload</code> subdirectory containing files important for
    <a href="https://github.com/COG-UK">COG-UK</a> is created.</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1619829439.0
refitt/refitt:
  data_format: 2
  description: The Recommender Engine for Intelligent Transient Tracking
  filenames:
  - Singularity
  full_name: refitt/refitt
  latest_release: 0.16.4
  readme: '<p>Singularity recipe files for the DVC tool for Data Version Control</p>

    '
  stargazers_count: 4
  subscribers_count: 1
  topics:
  - science
  - astronomy
  - distributed-systems
  - machine-learning
  - citizen-science
  - open-source
  - python
  updated_at: 1621927444.0
reisportela/PythonCourse:
  data_format: 2
  description: null
  filenames:
  - containers/SingularityAnaconda_v3.def
  - containers/Singularity.def
  - containers/SingularityAnaconda_v2.def
  - containers/SingularityAnaconda.def
  full_name: reisportela/PythonCourse
  latest_release: null
  readme: '<h1>

    <a id="user-content-pythoncourse" class="anchor" href="#pythoncourse" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PythonCourse</h1>

    <p>From A to Z. For fun.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620582753.0
richward1/Singularity-Nvidia:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: richward1/Singularity-Nvidia
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-images-for-openvino" class="anchor" href="#singularity-images-for-openvino"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    images for OpenVINO</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621006880.0
richward1/alpine-novnc:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: richward1/alpine-novnc
  latest_release: v2.0
  readme: '<h1>

    <a id="user-content-singularity-images-for-openvino" class="anchor" href="#singularity-images-for-openvino"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    images for OpenVINO</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621006537.0
rohitfarmer/singularity-defs:
  data_format: 2
  description: Definition (recipe) files for singularity containers.
  filenames:
  - comet/Singularity.def
  - comet/chemlab-comet.def
  - cite-seq/Singularity_rocker.def
  - cite-seq/Singularity_publish.def
  - cite-seq/Singularity_3.def
  - cite-seq/Singularity_xenial.def
  - cytof-workflow-v3/Singularity.def
  - H5N1/Singularity_R_3.6.def
  - H5N1/h5n1day100.def
  - H5N1/Singularity.def
  - generic/Singularity.def
  - bittersweet/Singularity.def
  - cytof-deep-cnn/Singularity.def
  - diffnets/diffnets.def
  full_name: rohitfarmer/singularity-defs
  latest_release: null
  readme: '<h1>

    <a id="user-content-definitionrecipe-files-for-singularity-containers" class="anchor"
    href="#definitionrecipe-files-for-singularity-containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Definition/Recipe Files
    for Singularity Containers</h1>

    <p>Some of the containers are available to download from <a href="https://cloud.sylabs.io/library/rohitfarmer"
    rel="nofollow">https://cloud.sylabs.io/library/rohitfarmer</a></p>

    <p>For feedback and collaboration write to me at <a href="mailto:rohit.farmer@gmail.com">rohit.farmer@gmail.com</a></p>

    <h1>

    <a id="user-content-install-singularity-on-linux" class="anchor" href="#install-singularity-on-linux"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install
    Singularity on Linux</h1>

    <h2>

    <a id="user-content-singularity-version-34" class="anchor" href="#singularity-version-34"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Version 3.4</h2>

    <p>Follow the instructions on <a href="https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps"
    rel="nofollow">https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps</a></p>

    <h1>

    <a id="user-content-building-a-singularity-container" class="anchor" href="#building-a-singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    a Singularity Container</h1>

    <h2>

    <a id="user-content-readonly-container" class="anchor" href="#readonly-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Readonly
    Container</h2>

    <p>To build a read-only SquashFS Singularity container on a local machine using
    a recipe/definition file.</p>

    <p><code>sudo singularity build &lt;container-name.sif&gt; &lt;Singularity.def&gt;</code></p>

    <h2>

    <a id="user-content-writable-sandbox" class="anchor" href="#writable-sandbox"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Writable
    Sandbox</h2>

    <p>To build a writable sandbox (essentially a folder) on a local machine using
    a recipe/definition file.</p>

    <p><code>sudo singularity build --sandbox  &lt;sandbox-folder-name/&gt; &lt;Singularity.def&gt;</code></p>

    <p><em>Note: The advantage of building a writable sandbox is that it can be used
    to install and configure packages as you go, and once you are satisfied with the
    requirements, the sandbox can be converted into a read-only SquashFS container.
    To build a sandbox quickly, it''s better to install a minimal set of packages
    via the definition file.</em></p>

    <h3>

    <a id="user-content-installconfigure-packages-in-a-writable-sandbox" class="anchor"
    href="#installconfigure-packages-in-a-writable-sandbox" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Install/Configure Packages
    in a Writable Sandbox</h3>

    <p>Once a writable sandbox is created to execute it to invoke the shell of the
    operating installed in the container in the "writable" mode. If the shell is not
    invoked in the "writable" mode, all the changes will be lost once you exit from
    the container environment.</p>

    <p><code>sudo singularity shell --writable &lt;sandbox-folder-name/&gt;</code></p>

    <p>Install packages as you would, for example, in Ubuntu from the command line.</p>

    <h2>

    <a id="user-content-convert-a-writable-sandbox-to-a-readonly-container" class="anchor"
    href="#convert-a-writable-sandbox-to-a-readonly-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Convert a Writable
    Sandbox to a Readonly Container</h2>

    <p><code>sudo singularity build &lt;container-name.sif&gt; &lt;sandbox-folder-name/&gt;</code></p>

    <h1>

    <a id="user-content-execute-a-container" class="anchor" href="#execute-a-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execute
    a Container</h1>

    <h2>

    <a id="user-content-invoke-a-shell" class="anchor" href="#invoke-a-shell" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Invoke a shell</h2>

    <p>The command below can be used for both read-only/writable containers/sandbox.</p>

    <p><code>singularity shell &lt;container-name.sif&gt;</code></p>

    <p><em>Note: By default, Singularity binds to your current working and home directory.
    Therefore, you do not need to do anything else to execute a script that is in
    your current working directory. It can also pull, for example, Vim settings from
    the .vimrc file in your home directory. Therefore, if Vim installed in the container,
    it can be used with the same settings from inside the container as it would from
    outside.</em></p>

    <h2>

    <a id="user-content-execute-a-command-via-container" class="anchor" href="#execute-a-command-via-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Execute
    a Command via Container</h2>

    <p><code>singularity exec &lt;container-name.sif&gt; &lt;command&gt;</code></p>

    <p>For example: <code>singularity exec &lt;container-name.sif&gt; Rscript --vanilla
    hello.R</code></p>

    <h1>

    <a id="user-content-running-jupyter-notebooks-from-within-a-container" class="anchor"
    href="#running-jupyter-notebooks-from-within-a-container" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running Jupyter Notebooks
    from Within a Container</h1>

    <p>This section is for containers that have Jupyter notebook installed (e.g. cite-seq).</p>

    <p>A generic command that should work on a personal computer. <code>singularity
    exec container-name.sif jupyter notebook --no-browser --ip=127.0.0.1 --port=8888</code><br>

    <em>Note: The IP address and the port number mentioned in the command are the
    jupyter defaults. They can be changed as per need.</em><br>

    Copy the URL generated by jupyter daemon and paste it in your browser; this should
    open Jupyter with the list of the files in your current working directory on the
    host computer.</p>

    <h2>

    <a id="user-content-running-with-r-as-a-kernel" class="anchor" href="#running-with-r-as-a-kernel"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    with R as a Kernel</h2>

    <p>Sometimes if you already have an R kernel installed in your home directory,
    it conflicts with what you have inside the container. Therefore, it would require
    you to re-install the kernel specs in your home directory via the container.</p>

    <pre><code>singularity exec container-name.sif R --quiet --slave -e ''IRkernel::installspec()''


    # Screen log.

    # [InstallKernelSpec] Removing existing kernelspec in /home/user/.local/share/jupyter/kernels/ir

    # [InstallKernelSpec] Installed kernelspec ir in /home/user/.local/share/jupyter/kernels/ir

    </code></pre>

    <h2>

    <a id="user-content-running-on-an-hpc" class="anchor" href="#running-on-an-hpc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running
    on an HPC</h2>

    <ol>

    <li>SSH to the HPC.</li>

    <li>Claim an interactive node.</li>

    <li>Navigate to your project directory. Singularity container should be in your
    project directory.</li>

    <li><code>singularity exec container-name.sif jupyter notebook --no-browser --ip=''0.0.0.0''</code></li>

    <li>Keep the SSH session and Jupyter notebook session running. Copy the URL on
    your local browser.</li>

    </ol>

    <p><em>Note: On some HPCs, you may have to initiate an additional SSH tunnel connecting
    your local machine to the interactive node on the HPC. In that case, follow some
    generic instructions here <a href="https://rohitfarmer.github.io/docs/docs/HPC/jupyter/"
    rel="nofollow">https://rohitfarmer.github.io/docs/docs/HPC/jupyter/</a> or ask
    your system administrator.</em></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1598415038.0
ronjagrosz/manta_singularity:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ronjagrosz/manta_singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-a-singularity-container-for-installing-manta" class="anchor"
    href="#a-singularity-container-for-installing-manta" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>A singularity container
    for installing manta</h1>

    <h2>

    <a id="user-content-build-with" class="anchor" href="#build-with" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build with:</h2>

    <p>sudo singularity build manta.simg Singularity</p>

    <ul>

    <li>Build files are found in ./build</li>

    <li>Install files are found in ./install</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1568290651.0
samapriya/planet-cyverse:
  data_format: 2
  description: Planet CLI in a Docker
  filenames:
  - Singularity
  - osgeo/Singularity
  - basic/Singularity
  - basic/Singularity-geo
  full_name: samapriya/planet-cyverse
  latest_release: null
  readme: "<p><a href=\"https://singularity-hub.org/collections/711\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-remote-sensing-environments-in-scalable-hpc-singularity-images\"\
    \ class=\"anchor\" href=\"#remote-sensing-environments-in-scalable-hpc-singularity-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Remote Sensing Environments in Scalable HPC Singularity Images</h1>\n\
    <p>The purpose of this singularity images and setup is to allow for development\
    \ of an active pipeline between image resources, hosting, ingestion protocol into\
    \ Google Earth Engine and retentation on volume as needed.</p>\n<h2>\n<a id=\"\
    user-content-main-build-components--should-include\" class=\"anchor\" href=\"\
    #main-build-components--should-include\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Main Build components ( Should\
    \ include)</h2>\n<ul>\n<li>Planet API Download Client to download images from\
    \ Planet API <strong>Included</strong>\n</li>\n<li>FTP client to pull images from\
    \ existing sftp or ftp addresses <strong>In progress</strong>\n</li>\n<li>Include\
    \ additional tools (shapely, pdal, rasterio) <strong>In progress</strong>\n</li>\n\
    <li>Singularity container which consists of further tools to manipulate and preprocess\
    \ imagery <strong>In progress</strong>\n</li>\n<li>Singularity container which\
    \ is mounted with large shared volume but individual user folder <strong>In progress</strong>\n\
    </li>\n<li>Earth Engine Upload and processing client including Earth Engine API\
    \ client <strong>Included</strong>\n</li>\n<li>Jupyter notebook to connnect to\
    \ mounted volume for local analysis and export of image or remote analysis and\
    \ export from Google Earth Engine <strong>In progress</strong>\n</li>\n<li>Drive\
    \ to access google drive: Check if all tasks have completed and download from\
    \ google drive, Verify and delete to preserve google drive storage <strong>In\
    \ progress</strong>\n</li>\n</ul>\n<h2>\n<a id=\"user-content-possible-integrations-and-enhancements\"\
    \ class=\"anchor\" href=\"#possible-integrations-and-enhancements\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Possible\
    \ Integrations and Enhancements</h2>\n<ul>\n<li>Leaflet based imagery and vector\
    \ visualization</li>\n<li>Image tiling so user can visualize on the leaflet window\
    \ within the Jupyter notebooks</li>\n<li>End results include QGIS with X11 support\
    \ to allow user to generate maps and export as images.</li>\n</ul>\n<h2>\n<a id=\"\
    user-content-planet-cli-in-a-docker-container\" class=\"anchor\" href=\"#planet-cli-in-a-docker-container\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Planet CLI in a Docker Container</h2>\n<p><em>In Progress</em></p>\n\
    <h2>\n<a id=\"user-content-planet-cli-in-a-singularity-container\" class=\"anchor\"\
    \ href=\"#planet-cli-in-a-singularity-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Planet CLI in a Singularity Container</h2>\n\
    <p>Singularity files are in the <code>/basic</code> and <code>/osgeo</code> folders.\
    \ The <code>/osgeo</code> container is maintained by Tyson Swetnam on <a href=\"\
    \">Singularity Hub</a></p>\n<p>To build a container:</p>\n<pre><code>cd basic/\n\
    sudo singularity build basic.simg Singularity\n</code></pre>\n<pre><code>cd osgeo/\n\
    sudo singularity build osgeo.simg Singularity\n</code></pre>\n<h3>\n<a id=\"user-content-moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\"\
    \ class=\"anchor\" href=\"#moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Moving files from Drive.Google into an Atmosphere or Jetstream VM</h3>\n\
    <p>In the US, academic institutions have increasingly established email accounts\
    \ through Google.\nSome institutions have unlimited storage on <a href=\"https://drive.google.com\"\
    \ rel=\"nofollow\">Drive</a>\nas a service for faculty and students.</p>\n<p>One\
    \ of the issues with uploading / downloading a large number of images to or from\
    \ a Drive\naccount through the web browser (Chrome or Firefox) is the number of\
    \ files allowed,\nthe speed of the uploads, and the size of the downloads.</p>\n\
    <p>Typically a download directly from a Drive account through Chrome is limited\
    \ to &lt;2Gb\nand is zipped by Google before starting.</p>\n<p>While the browser\
    \ can work well for easily uploading a large number sUAS images from a collection,\n\
    downloading the images as .zip files from the Google.drive can become tiresome\
    \ and difficult.</p>\n<p>To get around these problems we can use 3rd party applications\
    \ like FUSE and\n<a href=\"https://github.com/odeke-em/drive\"><code>drive</code></a>.</p>\n\
    <h3>\n<a id=\"user-content-fuse-client-ocamfluse\" class=\"anchor\" href=\"#fuse-client-ocamfluse\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>FUSE client <code>ocamfluse</code>\n</h3>\n<p><a href=\"https://github.com/astrada/google-drive-ocamlfuse\"\
    ><code>google-drive-ocamlfuse</code></a> is a Google Drive Client written in OCaml.\
    \ This is tested for Ubuntu systems.</p>\n<p>It can mount your google drive as\
    \ a folder visible in the file tree. FUSE is slower than other methods like iRODS\
    \ or <code>Drive</code>, but allows for GUI based drag and drop transfers.</p>\n\
    <pre><code>sudo add-apt-repository ppa:alessandro-strada/ppa\n</code></pre>\n\
    <pre><code>sudo apt-get update\nsudo apt-get install google-drive-ocamlfuse\n\
    </code></pre>\n<p>Run the app the first time to get the authentication certificate\
    \ from Google</p>\n<pre><code>google-drive-ocamlfuse\n</code></pre>\n<p>Create\
    \ a folder:</p>\n<pre><code>mkdir ~/googledrive\n</code></pre>\n<p>Mount the new\
    \ googledrive</p>\n<pre><code>google-drive-ocamlfuse ~/googledrive\n</code></pre>\n\
    <p>Open in your favorite file explorer.</p>\n<h3>\n<a id=\"user-content-install-go\"\
    \ class=\"anchor\" href=\"#install-go\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Install <code>Go</code>\n</h3>\n\
    <p>Drive uses the <code>go</code> language. In order to work with it you need\
    \ to <a href=\"https://golang.org/doc/install\" rel=\"nofollow\">install <code>go</code></a>\
    \ onto the VM.</p>\n<p>Remove any other go packages (particularly gccgo-go)</p>\n\
    <pre><code>sudo apt-get -y autoremove gccgo-go\n</code></pre>\n<pre><code>wget\
    \ https://storage.googleapis.com/golang/go1.8.1.linux-amd64.tar.gz\nsudo tar -C\
    \ /usr/local -xzf go1.8.1.linux-amd64.tar.gz\n</code></pre>\n<p>In <code>/etc/profile</code>\
    \ add: <code>export PATH=$PATH:/usr/local/go/bin</code></p>\n<pre><code>cat &lt;&lt;\
    \ ! &gt;&gt; /etc/profile\nexport PATH=$PATH:/usr/local/go/bin\n!\n</code></pre>\n\
    <p>In <code>~/.bashrc</code> - <code>sudo nano ~/.bashrc</code></p>\n<p>Add the\
    \ GOPATH directly from terminal:</p>\n<pre><code>cat &lt;&lt; ! &gt;&gt; ~/.bashrc\n\
    export GOPATH=\\$HOME/go\nexport PATH=\\$GOPATH:\\$GOPATH/bin:\\$PATH\n!\n</code></pre>\n\
    <p>Source the new <code>~/.bashrc</code> close the terminal and reopen</p>\n<pre><code>source\
    \ ~/.bashrc \n</code></pre>\n<p>Follow the <code>go</code>instructions to <a href=\"\
    https://golang.org/doc/install#testing\" rel=\"nofollow\">test the installation</a></p>\n\
    <h3>\n<a id=\"user-content-install-drive-a-drivegoogle-client-for-commandline\"\
    \ class=\"anchor\" href=\"#install-drive-a-drivegoogle-client-for-commandline\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Install <code>drive</code> a drive.google client for commandline</h3>\n\
    <p><a href=\"https://github.com/odeke-em/drive#installing\"><code>drive</code></a>\
    \ is a command line client using Go language</p>\n<p>Install <code>drive</code>\
    \ using <code>go</code></p>\n<pre><code>#install git\nsudo apt-get install git\n\
    </code></pre>\n<pre><code>cd ~/go\ngo get -u github.com/odeke-em/drive/cmd/drive\n\
    </code></pre>\n<h3>\n<a id=\"user-content-initialize-drive-with-your-google-account\"\
    \ class=\"anchor\" href=\"#initialize-drive-with-your-google-account\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Initialize\
    \ <code>drive</code> with your Google account</h3>\n<pre><code>mkdir ~/gdrive\n\
    drive init ~/gdrive\n</code></pre>\n<p>Follow the instructions for copying and\
    \ pasting the html for authentication</p>\n<p>Test your installation</p>\n<pre><code>drive\
    \ ls\n</code></pre>\n<h2>\n<a id=\"user-content-pull-data-from-your-googledrive-account-onto-the-vm\"\
    \ class=\"anchor\" href=\"#pull-data-from-your-googledrive-account-onto-the-vm\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Pull data from your Google.drive account onto the VM</h2>\n<pre><code>drive\
    \ pull your/google/drive/folders/here\n</code></pre>\n<h3>\n<a id=\"user-content-using-drive-on-ua-hpc\"\
    \ class=\"anchor\" href=\"#using-drive-on-ua-hpc\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Using Drive on\
    \ UA HPC</h3>\n<p>Drive is currently available on the ElGato login node</p>\n\
    <p>Load <code>go</code> and <code>drive</code></p>\n<pre><code>module load go\n\
    module load drive\n</code></pre>\n<p>Create a directory where you want to initiate\
    \ <code>drive</code> - preferrably on your <code>/xdisk/</code></p>\n<pre><code>cd\
    \ /xdisk/uid/\nmkdir gdrive\n</code></pre>\n<p>Initiate the drive</p>\n<pre><code>drive\
    \ init /xdisk/uid/gdrive\n</code></pre>\n<p>You will get a request to <code>Visit\
    \ this URL to get an authorization code</code> with a link to a long <code>https://accounts.google.com/o/oauth2/xxxx</code>\
    \ URL - copy paste that link into your preferred browser.</p>\n<p>You will be\
    \ taken to a Google login page, type in your email address (<a href=\"mailto:uid@email.arizona.edu\"\
    >uid@email.arizona.edu</a>) and password.</p>\n<p>Copy/Paste the code provided\
    \ by Google back in your Terminal window where prompted: <code>Paste the authorization\
    \ code:</code></p>\n<p>Now, check to see if your <code>drive.google.com</code>\
    \ acount is active:</p>\n<pre><code>drive ls\n</code></pre>\n<p>A list of the\
    \ directories in your <code>drive.google.com</code> account should be listed,\
    \ e.g.</p>\n<pre><code>/project1\n/project2\n/reports1\n/pictures1\n</code></pre>\n\
    <p>You can <code>pull</code> files or directories from your <code>drive.google.com</code>\
    \ now using commands like:</p>\n<pre><code>drive pull project1/subfolder/\n</code></pre>\n\
    <p>You will see:</p>\n<pre><code>Resolving...\n</code></pre>\n<p>followed by a\
    \ spinning <code>\\</code> <code>|</code> <code>/</code> <code>-</code> set of\
    \ symbols</p>\n<p>The folder contents will be displayed:</p>\n<pre><code> /project1/subfolder/file1.csv\n\
    ...\n+ /project1/subfolder/file955.csv\n+ /project1/subfolder/file966.csv\nAddition\
    \ count 966 src: 5.02GB\nProceed with the changes? [Y/n]:\n</code></pre>\n<p>Select\
    \ <code>y</code> and the download will proceed.</p>\n<p>Typical speeds are between\
    \ 10 and 50 Mb/s.</p>\n<pre><code>Proceed with the changes? [Y/n]:y\n 5392682146\
    \ / 5392682146 [==========================================================================================================================]\
    \ 100.00% 1m55s\n</code></pre>\n<h1>\n<a id=\"user-content-setting-up-cyverse-data-store-and-irods-icommands\"\
    \ class=\"anchor\" href=\"#setting-up-cyverse-data-store-and-irods-icommands\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Setting up CyVerse Data Store and iRods iCommands</h1>\n<p><a href=\"\
    https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\" rel=\"\
    nofollow\">Instructions</a></p>\n<pre><code>$ iinit\n</code></pre>\n<p>You will\
    \ be queried to set up your <code>irods_environment.json</code></p>\n<p>Enter\
    \ the following:</p>\n<table>\n<thead>\n<tr>\n<th>statement</th>\n<th>input</th>\n\
    </tr>\n</thead>\n<tbody>\n<tr>\n<td>DNS</td>\n<td><em>data.cyverse.org</em></td>\n\
    </tr>\n<tr>\n<td>port number</td>\n<td><em>1247</em></td>\n</tr>\n<tr>\n<td>user\
    \ name</td>\n<td><em>your user name</em></td>\n</tr>\n<tr>\n<td>zone</td>\n<td><em>iplant</em></td>\n\
    </tr>\n</tbody>\n</table>\n<p>Set up auto-complete for iCommands\n<a href=\"https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\"\
    \ rel=\"nofollow\">instructions</a></p>\n<p>Download <a href=\"https://pods.iplantcollaborative.org/wiki/download/attachments/6720192/i-commands-auto.bash\"\
    \ rel=\"nofollow\">i-commands-auto.bash</a>.\nIn your home directory, rename i-commands-auto.bash\
    \ to .i-commands-auto.bash\nIn your .bashrc or .bash_profile, enter the following:\n\
    source .i-commands-auto.bash</p>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1520625186.0
sc-eQTLgen-consortium/WG1-pipeline-QC:
  data_format: 2
  description: Part of the sc-eQTLgen consortium pipeline. Step 1, where the QC is
    done.
  filenames:
  - Singularity.WGpipeline
  - Singularity.Imputation
  full_name: sc-eQTLgen-consortium/WG1-pipeline-QC
  latest_release: null
  readme: '<h1>

    <a id="user-content-wg1-pipeline-qc" class="anchor" href="#wg1-pipeline-qc" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>WG1-pipeline-QC</h1>

    <p><a href="https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png"
    target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png"
    width="300" height="140" style="max-width:100%;"></a></p>

    <p>Part of the sceQTL-Gen consortium pipeline. Step 1, where the QC is done.</p>

    <p>Please see the <a href="https://github.com/sc-eQTLgen-consortium/WG1-pipeline-QC/wiki">Wiki</a>
    for information on running the QC pipeline.</p>

    '
  stargazers_count: 2
  subscribers_count: 1
  topics: []
  updated_at: 1621271961.0
seedpcseed/NGS-Assembly:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: seedpcseed/NGS-Assembly
  latest_release: null
  readme: '<h1>

    <a id="user-content-ngs-assembly" class="anchor" href="#ngs-assembly" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NGS-Assembly</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611092680.0
seedpcseed/NGS-filtering:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: seedpcseed/NGS-filtering
  latest_release: null
  readme: '<h1>

    <a id="user-content-ngs-filtering" class="anchor" href="#ngs-filtering" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>NGS-filtering</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611092715.0
seedpcseed/fastp:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: seedpcseed/fastp
  latest_release: null
  readme: '<h1>

    <a id="user-content-fastp" class="anchor" href="#fastp" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>fastp</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611092621.0
seedpcseed/metaprokka:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: seedpcseed/metaprokka
  latest_release: null
  readme: '<h1>

    <a id="user-content-metaprokka" class="anchor" href="#metaprokka" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>metaprokka</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611092582.0
sequana/demultiplex:
  data_format: 2
  description: 'Sequana demultiplexing pipeline '
  filenames:
  - singularity/Singularity
  full_name: sequana/demultiplex
  latest_release: v1.0.4
  readme: '<p>README last updated on: 01/24/2018</p>

    <h1>

    <a id="user-content-railrl" class="anchor" href="#railrl" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>railrl</h1>

    <p>Reinforcement learning framework.

    Some implemented algorithms:</p>

    <ul>

    <li><a href="examples/ddpg.py">Deep Deterministic Policy Gradient (DDPG)</a></li>

    <li><a href="examples/sac.py">Soft Actor Critic</a></li>

    <li><a href="examples/dqn_and_double_dqn.py">(Double) Deep Q-Network (DQN)</a></li>

    <li><a href="examples/her.py">Hindsight Experience Replay (HER)</a></li>

    <li><a href="examples/model_based_dagger.py">MPC with Neural Network Model</a></li>

    <li>

    <a href="examples/naf.py">Normalized Advantage Function (NAF)</a>

    <ul>

    <li>WARNING: I haven''t tested this NAF implementation much, so it may not match
    the paper''s performance. I''m pretty confident about the other two implementations
    though.</li>

    </ul>

    </li>

    </ul>

    <p>To get started, checkout the example scripts, linked above.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <h3>

    <a id="user-content-some-dependancies" class="anchor" href="#some-dependancies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Some
    dependancies</h3>

    <ul>

    <li><code>sudo apt-get install swig</code></li>

    </ul>

    <h3>

    <a id="user-content-create-conda-env" class="anchor" href="#create-conda-env"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    Conda Env</h3>

    <p>Install and use the included ananconda environment</p>

    <pre><code>$ conda env create -f docker/railrl/railrl-env.yml

    $ source activate railrl-env

    (railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py

    </code></pre>

    <p>Or if you want you can use the docker image included.</p>

    <h3>

    <a id="user-content-download-simulation-env-code" class="anchor" href="#download-simulation-env-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download
    Simulation Env Code</h3>

    <ul>

    <li>

    <a href="https://github.com/vitchyr/multiworld">multiworld</a> (contains environments):<code>git
    clone https://github.com/vitchyr/multiworld</code>

    </li>

    </ul>

    <h3>

    <a id="user-content-optional-install-doodad" class="anchor" href="#optional-install-doodad"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>(Optional)
    Install doodad</h3>

    <p>I recommend installing <a href="https://github.com/justinjfu/doodad">doodad</a>
    to

    launch jobs. Some of its nice features include:</p>

    <ul>

    <li>Easily switch between running code locally, on a remote compute with

    Docker, on EC2 with Docker</li>

    <li>Easily add your dependencies that can''t be installed via pip (e.g. you

    borrowed someone''s code)</li>

    </ul>

    <p>If you install doodad, also modify <code>CODE_DIRS_TO_MOUNT</code> in <code>config.py</code>
    to

    include:</p>

    <ul>

    <li>Path to rllab directory</li>

    <li>Path to railrl directory</li>

    <li>Path to other code you want to juse</li>

    </ul>

    <p>You''ll probably also need to update the other variables besides the docker

    images/instance stuff.</p>

    <h3>

    <a id="user-content-setup-config-file" class="anchor" href="#setup-config-file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup
    Config File</h3>

    <p>You must setup the config file for launching experiments, providing paths to
    your code and data directories. Inside <code>railrl/config/launcher_config.py</code>,
    fill in the appropriate paths. You can use <code>railrl/config/launcher_config_template.py</code>
    as an example reference.</p>

    <p><code>cp railrl/launchers/config-template.py railrl/launchers/config.py</code></p>

    <h2>

    <a id="user-content-visualizing-a-policy-and-seeing-results" class="anchor" href="#visualizing-a-policy-and-seeing-results"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visualizing
    a policy and seeing results</h2>

    <p>During training, the results will be saved to a file called under</p>

    <pre><code>LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;

    </code></pre>

    <ul>

    <li>

    <code>LOCAL_LOG_DIR</code> is the directory set by <code>railrl.launchers.config.LOCAL_LOG_DIR</code>

    </li>

    <li>

    <code>&lt;exp_prefix&gt;</code> is given either to <code>setup_logger</code>.</li>

    <li>

    <code>&lt;foldername&gt;</code> is auto-generated and based off of <code>exp_prefix</code>.</li>

    <li>inside this folder, you should see a file called <code>params.pkl</code>.
    To visualize a policy, run</li>

    </ul>

    <pre><code>(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;/params.pkl

    </code></pre>

    <p>If you have rllab installed, you can also visualize the results

    using <code>rllab</code>''s viskit, described at

    the bottom of <a href="http://rllab.readthedocs.io/en/latest/user/cluster.html"
    rel="nofollow">this page</a></p>

    <p>tl;dr run</p>

    <div class="highlight highlight-source-shell"><pre>python rllab/viskit/frontend.py
    LOCAL_LOG_DIR/<span class="pl-k">&lt;</span>exp_prefix<span class="pl-k">&gt;</span>/</pre></div>

    <h3>

    <a id="user-content-add-paths" class="anchor" href="#add-paths" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Add paths</h3>

    <pre><code>export PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo

    </code></pre>

    <h2>

    <a id="user-content-credit" class="anchor" href="#credit" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credit</h2>

    <p>A lot of the coding infrastructure is based on <a href="https://github.com/rll/rllab">rllab</a>.

    Also, the serialization and logger code are basically a carbon copy.</p>

    '
  stargazers_count: 1
  subscribers_count: 4
  topics: []
  updated_at: 1615929334.0
sequana/fastqc:
  data_format: 2
  description: sequana pipeline to perform parallel fastqc and summarize results with
    multiqc plot
  filenames:
  - singularity/Singularity
  full_name: sequana/fastqc
  latest_release: v1.2.0
  readme: '<h1>

    <a id="user-content-docker" class="anchor" href="#docker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Docker</h1>

    <h2>

    <a id="user-content-building-the-docker-container" class="anchor" href="#building-the-docker-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    the docker container</h2>

    <p><em>NOTE: This step is not necessary if you simply want to use an already published
    image to run the example code on the UA HPC.</em></p>

    <pre><code>docker build -f Dockerfile -t uazhlt/pytorch-example .

    </code></pre>

    <h2>

    <a id="user-content-verify-pytorch-version" class="anchor" href="#verify-pytorch-version"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Verify
    PyTorch version</h2>

    <pre><code>docker run --rm -it uazhlt/pytorch-example python -c "import torch;
    print(torch.__version__)"

    </code></pre>

    <h2>

    <a id="user-content-publish-to-dockerhub" class="anchor" href="#publish-to-dockerhub"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publish
    to DockerHub</h2>

    <p><em>NOTE: This step is not necessary if you simply want to use an already published
    image to run the example code on the UA HPC.</em></p>

    <pre><code># login to dockerhub registry

    docker login --username=yourdockerhubusername --email=youremail@domain.com


    docker push org/image-name:taghere

    </code></pre>

    <h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <h2>

    <a id="user-content-building-a-singularity-image" class="anchor" href="#building-a-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    a Singularity image</h2>

    <p>Building a Singularity image from a def file requires sudo on a Linux system.  In
    this tutorial, we avoid discussing details on installing Singularity.  If you''re
    feeling adventurous, take a look at <a href="./Singularity">the example def file
    in this repository</a> and the official documentation:</p>

    <ul>

    <li><a href="https://sylabs.io/guides/3.0/user-guide/installation.html" rel="nofollow">https://sylabs.io/guides/3.0/user-guide/installation.html</a></li>

    </ul>

    <h3>

    <a id="user-content-alternatives" class="anchor" href="#alternatives" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Alternatives</h3>

    <h4>

    <a id="user-content-cloud-builds" class="anchor" href="#cloud-builds" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cloud builds</h4>

    <ul>

    <li>GitHub actions:

    <ul>

    <li><a href="https://github.com/singularityhub/github-ci/blob/master/.github/workflows/go.yml">Example
    GitHub Workflow</a></li>

    <li><a href="https://help.github.com/en/actions/automating-your-workflow-with-github-actions/virtual-environments-for-github-hosted-runners#supported-runners-and-hardware-resources">GitHub-hosted
    runners</a></li>

    </ul>

    </li>

    </ul>

    <h4>

    <a id="user-content-vms" class="anchor" href="#vms" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>VMs</h4>

    <ul>

    <li><a href="https://sylabs.io/guides/3.0/user-guide/installation.html#singularity-vagrant-box"
    rel="nofollow">Vagrant box</a></li>

    </ul>

    <h4>

    <a id="user-content-docker---singularity" class="anchor" href="#docker---singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Docker
    -&gt; Singularity</h4>

    <ul>

    <li><a href="https://github.com/singularityhub/docker2singularity"><code>docker2singularity</code></a></li>

    </ul>

    <h2>

    <a id="user-content-retrieving-a-published-singularity-image" class="anchor" href="#retrieving-a-published-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Retrieving
    a published Singularity image</h2>

    <p>Instead of building from scratch, we''ll focus on a shortcut that simply wraps
    docker images published to DockerHub.</p>

    <pre><code>singularity pull uazhlt-pytorch-example.sif docker://uazhlt/pytorch-example:latest

    </code></pre>

    <h1>

    <a id="user-content-hpc" class="anchor" href="#hpc" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>HPC</h1>

    <p>If you intend to test out <a href="./example">the PyTorch example included
    here</a>, you''ll want to clone this repository:</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/ua-hlt-program/pytorch-example.git</pre></div>

    <h2>

    <a id="user-content-running-singularity-in-an-interactive-pbs-job" class="anchor"
    href="#running-singularity-in-an-interactive-pbs-job" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running Singularity
    in an interactive PBS job</h2>

    <p>Next, we''ll request an interactive job (tested on El Gato):</p>

    <div class="highlight highlight-source-shell"><pre>qsub -I \

    -N interactive-gpu \

    -W group_list=mygroupnamehere \

    -q standard \

    -l select=1:ncpus=2:mem=16gb:ngpus=1 \

    -l cput=3:0:0 \

    -l walltime=1:0:0</pre></div>

    <p>_NOTE: If you''re unfamiliar with <code>qsub</code> and the many options in
    the command above seem puzzling, you can find answers by checking out the manual
    via <code>man qsub</code> _</p>

    <p>If the cluster isn''t too busy, you should soon see a new prompt formatted
    something like <code>[netid@gpu\d\d ~]</code>.</p>

    <p>Now we''ll run the singularity image we grabbed earlier.  Before that, though,
    let''s ensure we''re using the correct version of Singularity and that the correct
    CUDA version is available to Singularity:</p>

    <pre><code>module load singularity/3.2.1

    module load cuda10/10.1

    </code></pre>

    <p>Now we''re finally ready to run the container:</p>

    <pre><code>singularity shell --nv --no-home /path/to/your/uazhlt-pytorch-example.sif

    </code></pre>

    <p>If you ran into an error, check to see if you replaced <code>/path/to/your/</code>
    with the correct path to <code>uazhlt-pytorch-example.sif</code> before executing
    the command.</p>

    <p>We''re now in our Singularity container! If everything went well, we should
    be able to see the gpu:</p>

    <pre><code>nvidia-smi

    </code></pre>

    <p>You should see output like the following:</p>

    <pre><code>+-----------------------------------------------------------------------------+

    | NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |

    |-------------------------------+----------------------+----------------------+

    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC
    |

    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M.
    |

    |===============================+======================+======================|

    |   0  Tesla K20Xm         On   | 00000000:8B:00.0 Off |                    0
    |

    | N/A   17C    P8    18W / 235W |      0MiB /  5700MiB |      0%      Default
    |

    +-------------------------------+----------------------+----------------------+


    +-----------------------------------------------------------------------------+

    | Processes:                                                       GPU Memory
    |

    |  GPU       PID   Type   Process name                             Usage      |

    |=============================================================================|

    |  No running processes found                                                 |

    +-----------------------------------------------------------------------------+

    </code></pre>

    <p>Success (I hope)!  Now let''s try running PyTorch on the GPU with batching...</p>

    <h1>

    <a id="user-content-pytorch-example" class="anchor" href="#pytorch-example" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PyTorch example</h1>

    <p>The Pytorch example code can be found under <a href="./example"><code>example</code></a>.  The
    data used in this example comes from from Delip Rao and Brian MacMahan''s <em>Natural
    Language Processing with PyTorch</em>:</p>

    <ul>

    <li><a href="https://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames">https://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames</a></li>

    </ul>

    <p>The dataset relates surnames to nationalities.  Our version (minor modifications)
    is nested under <a href="./examples/data">examples/data</a>.</p>

    <p><code>train.py</code> houses a command line program for training a classifier.  The
    following invocation will display the tool''s help text:</p>

    <pre><code>python train.py --help

    </code></pre>

    <p>The simple model architecture operates is based on that of deep averaging networks
    (DANs; see <a href="https://aclweb.org/anthology/P15-1162/" rel="nofollow">https://aclweb.org/anthology/P15-1162/</a>).</p>

    <p>Reading through train.py you can quickly see how the code is organized.  Some
    parts (ex. <code>torchtext</code> data loaders) may be unfamiliar to you.</p>

    <h1>

    <a id="user-content-next-steps" class="anchor" href="#next-steps" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Next steps</h1>

    <p>Now that you''ve managed to run some example PyTorch code, there are many paths
    forward:</p>

    <ul>

    <li>Experiment with using pretrained subword embeddings (both fixed and trainable).  Do
    you notice any improvements in performance/faster convergence?</li>

    <li>Try improving or replacing the naive model defined under <code>models.py</code>.</li>

    <li>Add an evaluation script for a trained model that reports macro P, R, and
    F1.  Feel free to use <code>scikit-learn</code>''s classification report.</li>

    <li>Add an inference script to classify new examples.</li>

    <li>Monitor validation loss to and stop training if you begin to overfit.</li>

    <li>Adapt the interactive PBS task outlined above to a PBS script that you can
    submit to the HPC.</li>

    <li>Address the class imbalance in the data through downsampling, class weighting,
    or another technique of your choosing.</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics:
  - fastqc
  - ngs
  - snakemake
  - sequana
  updated_at: 1620673100.0
sequana/sequana_coverage:
  data_format: 2
  description: Snakemake pipeline wrapping sequana_coverage to analyse several samples
    at the same time
  filenames:
  - singularity/Singularity
  full_name: sequana/sequana_coverage
  latest_release: null
  readme: '<p>README last updated on: 01/24/2018</p>

    <h1>

    <a id="user-content-railrl" class="anchor" href="#railrl" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>railrl</h1>

    <p>Reinforcement learning framework.

    Some implemented algorithms:</p>

    <ul>

    <li><a href="examples/ddpg.py">Deep Deterministic Policy Gradient (DDPG)</a></li>

    <li><a href="examples/sac.py">Soft Actor Critic</a></li>

    <li><a href="examples/dqn_and_double_dqn.py">(Double) Deep Q-Network (DQN)</a></li>

    <li><a href="examples/her.py">Hindsight Experience Replay (HER)</a></li>

    <li><a href="examples/model_based_dagger.py">MPC with Neural Network Model</a></li>

    <li>

    <a href="examples/naf.py">Normalized Advantage Function (NAF)</a>

    <ul>

    <li>WARNING: I haven''t tested this NAF implementation much, so it may not match
    the paper''s performance. I''m pretty confident about the other two implementations
    though.</li>

    </ul>

    </li>

    </ul>

    <p>To get started, checkout the example scripts, linked above.</p>

    <h2>

    <a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

    <h3>

    <a id="user-content-some-dependancies" class="anchor" href="#some-dependancies"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Some
    dependancies</h3>

    <ul>

    <li><code>sudo apt-get install swig</code></li>

    </ul>

    <h3>

    <a id="user-content-create-conda-env" class="anchor" href="#create-conda-env"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create
    Conda Env</h3>

    <p>Install and use the included ananconda environment</p>

    <pre><code>$ conda env create -f docker/railrl/railrl-env.yml

    $ source activate railrl-env

    (railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py

    </code></pre>

    <p>Or if you want you can use the docker image included.</p>

    <h3>

    <a id="user-content-download-simulation-env-code" class="anchor" href="#download-simulation-env-code"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Download
    Simulation Env Code</h3>

    <ul>

    <li>

    <a href="https://github.com/vitchyr/multiworld">multiworld</a> (contains environments):<code>git
    clone https://github.com/vitchyr/multiworld</code>

    </li>

    </ul>

    <h3>

    <a id="user-content-optional-install-doodad" class="anchor" href="#optional-install-doodad"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>(Optional)
    Install doodad</h3>

    <p>I recommend installing <a href="https://github.com/justinjfu/doodad">doodad</a>
    to

    launch jobs. Some of its nice features include:</p>

    <ul>

    <li>Easily switch between running code locally, on a remote compute with

    Docker, on EC2 with Docker</li>

    <li>Easily add your dependencies that can''t be installed via pip (e.g. you

    borrowed someone''s code)</li>

    </ul>

    <p>If you install doodad, also modify <code>CODE_DIRS_TO_MOUNT</code> in <code>config.py</code>
    to

    include:</p>

    <ul>

    <li>Path to rllab directory</li>

    <li>Path to railrl directory</li>

    <li>Path to other code you want to juse</li>

    </ul>

    <p>You''ll probably also need to update the other variables besides the docker

    images/instance stuff.</p>

    <h3>

    <a id="user-content-setup-config-file" class="anchor" href="#setup-config-file"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup
    Config File</h3>

    <p>You must setup the config file for launching experiments, providing paths to
    your code and data directories. Inside <code>railrl/config/launcher_config.py</code>,
    fill in the appropriate paths. You can use <code>railrl/config/launcher_config_template.py</code>
    as an example reference.</p>

    <p><code>cp railrl/launchers/config-template.py railrl/launchers/config.py</code></p>

    <h2>

    <a id="user-content-visualizing-a-policy-and-seeing-results" class="anchor" href="#visualizing-a-policy-and-seeing-results"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visualizing
    a policy and seeing results</h2>

    <p>During training, the results will be saved to a file called under</p>

    <pre><code>LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;

    </code></pre>

    <ul>

    <li>

    <code>LOCAL_LOG_DIR</code> is the directory set by <code>railrl.launchers.config.LOCAL_LOG_DIR</code>

    </li>

    <li>

    <code>&lt;exp_prefix&gt;</code> is given either to <code>setup_logger</code>.</li>

    <li>

    <code>&lt;foldername&gt;</code> is auto-generated and based off of <code>exp_prefix</code>.</li>

    <li>inside this folder, you should see a file called <code>params.pkl</code>.
    To visualize a policy, run</li>

    </ul>

    <pre><code>(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/&lt;exp_prefix&gt;/&lt;foldername&gt;/params.pkl

    </code></pre>

    <p>If you have rllab installed, you can also visualize the results

    using <code>rllab</code>''s viskit, described at

    the bottom of <a href="http://rllab.readthedocs.io/en/latest/user/cluster.html"
    rel="nofollow">this page</a></p>

    <p>tl;dr run</p>

    <div class="highlight highlight-source-shell"><pre>python rllab/viskit/frontend.py
    LOCAL_LOG_DIR/<span class="pl-k">&lt;</span>exp_prefix<span class="pl-k">&gt;</span>/</pre></div>

    <h3>

    <a id="user-content-add-paths" class="anchor" href="#add-paths" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Add paths</h3>

    <pre><code>export PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo

    export PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo

    </code></pre>

    <h2>

    <a id="user-content-credit" class="anchor" href="#credit" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Credit</h2>

    <p>A lot of the coding infrastructure is based on <a href="https://github.com/rll/rllab">rllab</a>.

    Also, the serialization and logger code are basically a carbon copy.</p>

    '
  stargazers_count: 1
  subscribers_count: 2
  topics: []
  updated_at: 1595187932.0
sequana/sequana_pipeline_template:
  data_format: 2
  description: a cookiecutter sequana pipeline template
  filenames:
  - '{{cookiecutter.project_slug}}/singularity/Singularity'
  full_name: sequana/sequana_pipeline_template
  latest_release: null
  readme: '<h1>

    <a id="user-content-whatshap" class="anchor" href="#whatshap" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>WhatsHap</h1>

    <p>WhatsHap repository for singularity container</p>

    '
  stargazers_count: 0
  subscribers_count: 3
  topics: []
  updated_at: 1616522945.0
sequana/sequana_quality_control:
  data_format: 2
  description: A quality control pipeline for illumina data set. This pipeline removes
    contaminants (e.g. Phix), performs fastqc, adapter cleaning and trimming and checks
    for contaminants
  filenames:
  - singularity/Singularity
  full_name: sequana/sequana_quality_control
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-pycoqc-nextflow-script\" class=\"anchor\" href=\"\
    #pycoqc-nextflow-script\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>pycoQC nextflow script</h1>\n<p>This is a nextflow\
    \ script that runs pycoQC on the output folder of Guppy.</p>\n<p>The script takes\
    \ the followig command line arguments:</p>\n<pre><code>--guppy_dir  dir \tPath\
    \ to the guppy output directory containing *_sequencing_summary.txt\n--samplename\
    \ name \tSample name\n--resultsDir dir \tPath to the output directory\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1606177348.0
sequana/sequana_revcomp:
  data_format: 2
  description: A simple utility to convert a bunch of input fastq files into their
    reverse complement
  filenames:
  - singularity/Singularity
  full_name: sequana/sequana_revcomp
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-pycoqc-nextflow-script\" class=\"anchor\" href=\"\
    #pycoqc-nextflow-script\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>pycoQC nextflow script</h1>\n<p>This is a nextflow\
    \ script that runs pycoQC on the output folder of Guppy.</p>\n<p>The script takes\
    \ the followig command line arguments:</p>\n<pre><code>--guppy_dir  dir \tPath\
    \ to the guppy output directory containing *_sequencing_summary.txt\n--samplename\
    \ name \tSample name\n--resultsDir dir \tPath to the output directory\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1589402320.0
sguizard/TAMA-singularity:
  data_format: 2
  description: A definition file for building TAMA singularity container
  filenames:
  - Singularity
  full_name: sguizard/TAMA-singularity
  latest_release: v1.0
  readme: '<h1>

    <a id="user-content-tama-singularity" class="anchor" href="#tama-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TAMA-singularity</h1>

    <p>A definition file for building TAMA singularity container</p>

    <h2>

    <a id="user-content-building-container" class="anchor" href="#building-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    container</h2>

    <pre><code>sudo singularity build TAMA.{sif, def}

    </code></pre>

    <h2>

    <a id="user-content-using-tama" class="anchor" href="#using-tama" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using TAMA</h2>

    <p>There are two main Python scripts in TAMA:</p>

    <ul>

    <li>tama_collapse.py</li>

    <li>tama_merge.py</li>

    </ul>

    <p>They can be run as follows:</p>

    <h3>

    <a id="user-content-tama_collapsepy" class="anchor" href="#tama_collapsepy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/GenomeRIK/tama/wiki/Tama-Collapse">tama_collapse.py</a>

    </h3>

    <pre><code>singularity exec TAMA.sif tama_collapse.py -h

    </code></pre>

    <h3>

    <a id="user-content-tama_mergepy" class="anchor" href="#tama_mergepy" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/GenomeRIK/tama/wiki/Tama-Merge">tama_merge.py</a>

    </h3>

    <pre><code>singularity exec TAMA.sif tama_merge.py -h

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619213900.0
shots47s/matlab2017b-Singularity:
  data_format: 2
  description: A repo for my matlab 2017b runtime Singularity container
  filenames:
  - Singularity
  full_name: shots47s/matlab2017b-Singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-matlab2017b-singularity" class="anchor" href="#matlab2017b-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>matlab2017b-Singularity</h1>

    <p>A repo for my matlab 2017b runtime Singularity container</p>

    '
  stargazers_count: 1
  subscribers_count: 0
  topics: []
  updated_at: 1537648868.0
shub-fuzz/afl:
  data_format: 2
  description: Singularity Image for AFL (https://github.com/google/AFL)
  filenames:
  - Singularity.1604
  - Singularity.1804
  - Singularity.i386
  full_name: shub-fuzz/afl
  latest_release: 0.0.1
  readme: '<h1>

    <a id="user-content-afl" class="anchor" href="#afl" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>afl</h1>

    <p><a href="https://github.com/shub-fuzz/afl/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/afl/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3617" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity Image for AFL (<a href="https://github.com/google/AFL">https://github.com/google/AFL</a>)</p>

    <ul>

    <li>usage (x86_64 container):</li>

    </ul>

    <pre><code>singularity pull --name afl.sif https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.1604.sif


    singularity shell afl.sif

    </code></pre>

    <ul>

    <li>pull Ubuntu 18.04 container</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name afl.1804.sif
    https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.1804.sif</pre></div>

    <ul>

    <li>pull Ubuntu 16.04 i386 container</li>

    </ul>

    <pre><code>singularity pull --name afl_i386.sif https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.i386.sif

    singularity pull --name afl_i386.sif shub://shub-fuzz/afl:i386

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619228309.0
shub-fuzz/aflgo:
  data_format: 2
  description: Singularity image for AFLGo (https://github.com/aflgo/aflgo)
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/aflgo
  latest_release: 0.0.1
  readme: '<h1>

    <a id="user-content-aflgo" class="anchor" href="#aflgo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>AFLGo</h1>

    <p><a href="https://github.com/shub-fuzz/aflgo/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/aflgo/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/5085" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for AFLGo (<a href="https://github.com/aflgo/aflgo">https://github.com/aflgo/aflgo</a>)</p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name aflgo.sif https://github.com/shub-fuzz/aflgo/releases/download/0.0.1/shub-fuzz-aflgo.1604.sif


    singularity shell aflgo.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619232831.0
shub-fuzz/aflpp:
  data_format: 2
  description: Singularity image for afl++ (https://github.com/AFLplusplus/AFLplusplus)
  filenames:
  - Singularity.1604
  - Singularity.1804
  - Singularity.i386
  - focal/Singularity.2004
  full_name: shub-fuzz/aflpp
  latest_release: 0.0.1
  readme: '<p>Singularity image for AFL++ (<a href="https://github.com/AFLplusplus/AFLplusplus">https://github.com/AFLplusplus/AFLplusplus</a>)</p>

    <p><a href="https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name aflpp.sif https://github.com/shub-fuzz/aflpp/releases/download/0.0.1/shub-fuzz-aflpp.1604.sif


    singularity shell aflpp.sif

    </code></pre>

    <ul>

    <li>pull Ubuntu 18.04 container</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name aflpp.1804.sif
    https://github.com/shub-fuzz/aflpp/releases/download/0.0.1/shub-fuzz-aflpp.1804.sif</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619233273.0
shub-fuzz/angora:
  data_format: 2
  description: Singularity image for Angora (https://github.com/AngoraFuzzer/Angora)
  filenames:
  - Singularity.1604
  - Singularity.1804
  full_name: shub-fuzz/angora
  latest_release: 0.0.1
  readme: "<p>Singularity image for Angora (<a href=\"https://github.com/AngoraFuzzer/Angora\"\
    >https://github.com/AngoraFuzzer/Angora</a>)</p>\n<p><a href=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml\"\
    ><img src=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml/badge.svg?branch=main\"\
    \ alt=\"singularity-deploy\" style=\"max-width:100%;\"></a>\n<a href=\"https://singularity-hub.org/collections/3645\"\
    \ rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<ul>\n<li>usage:</li>\n</ul>\n<pre><code>singularity\
    \ pull --name angora.sif https://github.com/shub-fuzz/angora/releases/download/0.0.1/shub-fuzz-angora.1604.sif\n\
    \nsingularity shell angora.sif\n</code></pre>\n<ul>\n<li>interactive session:</li>\n\
    </ul>\n<pre><code>singularity shell angora.sif \n</code></pre>\n<ul>\n<li>start\
    \ fuzzing</li>\n</ul>\n<pre><code>singularity exec angora.sif /start_fuzzing [[\
    \ -n &lt;# instances&gt; ]  -t ] &lt;target_path&gt; \n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619232164.0
shub-fuzz/ankou:
  data_format: 2
  description: Singularity image for Ankou (https://github.com/SoftSec-KAIST/Ankou)
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/ankou
  latest_release: 0.0.1
  readme: '<p>Singularity image for Ankou (<a href="https://github.com/SoftSec-KAIST/Ankou">https://github.com/SoftSec-KAIST/Ankou</a>)</p>

    <p><a href="https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/4173" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name ankou.sif https://github.com/shub-fuzz/ankou/releases/download/0.0.1/shub-fuzz-ankou.1604.sif


    singularity shell ankou.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619231819.0
shub-fuzz/build-tools:
  data_format: 2
  description: Ubuntu (rolling) with build tools
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/build-tools
  latest_release: 0.0.1
  readme: '<h1>

    <a id="user-content-build-tools" class="anchor" href="#build-tools" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>build-tools</h1>

    <p>Ubuntu (rolling) with build tools</p>

    <p><a href="https://github.com/shub-fuzz/build-tools/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/build-tools/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619233018.0
shub-fuzz/eclipser:
  data_format: 2
  description: Singularity image for Eclipser (https://github.com/SoftSec-KAIST/Eclipser)
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/eclipser
  latest_release: 0.0.1
  readme: '<p>Singularity image for Eclipser (<a href="https://github.com/SoftSec-KAIST/Eclipser">https://github.com/SoftSec-KAIST/Eclipser</a>)</p>

    <p><a href="https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name eclipser.sif https://github.com/shub-fuzz/eclipser/releases/download/0.0.1/shub-fuzz-eclipser.1604.sif


    singularity shell eclipser.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619231857.0
shub-fuzz/honggfuzz:
  data_format: 2
  description: Singularity image for honggfuzz (https://github.com/google/honggfuzz)
  filenames:
  - Singularity.1604
  - Singularity.1804
  - Singularity.i386
  - v21/Singularity.v21
  full_name: shub-fuzz/honggfuzz
  latest_release: 0.0.1
  readme: '<p><a href="https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3641" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>Singularity image for honggfuzz (<a href="https://github.com/google/honggfuzz">https://github.com/google/honggfuzz</a>)</p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name honggfuzz.sif https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.1/shub-fuzz-honggfuzz.1604.sif


    singularity shell honggfuzz.sif

    </code></pre>

    <ul>

    <li>pull Ubuntu 18.04 container</li>

    </ul>

    <div class="highlight highlight-source-shell"><pre>singularity pull --name honggfuzz.1804.sif
    https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.1/shub-fuzz-honggfuzz.1804.sif</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619230950.0
shub-fuzz/intriguer:
  data_format: 2
  description: Singularity image for Intriguer (https://github.com/seclab-yonsei/intriguer)
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/intriguer
  latest_release: 0.0.1
  readme: '<p>Singularity image for Intriguer (<a href="https://github.com/seclab-yonsei/intriguer">https://github.com/seclab-yonsei/intriguer</a>)</p>

    <p><a href="https://github.com/shub-fuzz/intriguer/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/intriguer/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/5086" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name intriguer.sif https://github.com/shub-fuzz/intriguer/releases/download/0.0.1/shub-fuzz-intriguer.1604.sif


    singularity shell intriguer.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619232642.0
shub-fuzz/llvm:
  data_format: 2
  description: LLVM
  filenames:
  - Singularity.1604
  full_name: shub-fuzz/llvm
  latest_release: 0.0.1
  readme: '<p>Singularity Image for LLVM w/AFL++ (currently v12)</p>

    <p><a href="https://github.com/shub-fuzz/llvm/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/llvm/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name llvm.sif https://github.com/shub-fuzz/llvm/releases/download/0.0.1/shub-fuzz-llvm.1604.sif


    singularity shell llvm.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619231247.0
shub-fuzz/parmesan:
  data_format: 2
  description: Singularity image for ParmeSan (https://github.com/vusec/parmesan)
  filenames:
  - Singularity.2004
  full_name: shub-fuzz/parmesan
  latest_release: 0.0.1
  readme: '<p>Singularity image for ParmeSan (<a href="https://github.com/vusec/parmesan">https://github.com/vusec/parmesan</a>)</p>

    <p><a href="https://github.com/shub-fuzz/afl/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/afl/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/5084" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name afl.sif https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.2004.sif


    singularity shell afl.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619232723.0
shub-fuzz/postgres:
  data_format: 2
  description: Singularity Postgres container
  filenames:
  - Singularity.v12
  full_name: shub-fuzz/postgres
  latest_release: 0.0.1
  readme: '<p><a href="https://github.com/shub-fuzz/postgres/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/postgres/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-singularity-postgres-container" class="anchor" href="#singularity-postgres-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Postgres Container</h1>

    <p>Singularity containter for postgres docker image (<a href="https://hub.docker.com/_/postgres/"
    rel="nofollow">link</a>).

    This singularity recipe modifies from the image to work in service mode.

    It also includes an optional <code>/postgresrc</code> to pass system environment
    variables.</p>

    <h2>

    <a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

    <ul>

    <li>To start the service, <code>singularity instance start -B host_folder:/var/lib/postgresql/data
    postgres.sif pg-database</code>.</li>

    <li>To pass variables for postgres database, use bind a file to <code>/postgresrc</code>.<br>

    For example, <code>-B yourrc:/postgresrc</code>. Some common variables include
    <code>PGPORT</code> and <code>HOSTNAME</code>.

    (Due to some reason, the default postgres starts in <code>0.0.0.0</code>. For
    security reason, this recipe

    uses <code>HOSTNAME</code> which defaults to <code>localhost</code>.)</li>

    <li>Use <code>SINGULARITY_BINDPATH=''host_folder:container_folder,host_file:container_file''</code>
    for easier binding.</li>

    </ul>

    <h2>

    <a id="user-content-building-manually" class="anchor" href="#building-manually"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Manually</h2>

    <p>To build the image, run <code>sudo singularity build &lt;name.sif&gt; Singularity</code>.

    See <a href="https://singularity.lbl.gov/" rel="nofollow">Singularity</a>

    for more info.</p>

    <h2>

    <a id="user-content-acknowledge" class="anchor" href="#acknowledge" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledge</h2>

    <p>The recipes build from many open source projects, including</p>

    <ul>

    <li><a href="https://singularity.lbl.gov/" rel="nofollow">Singularity</a></li>

    <li><a href="https://www.postgresql.org/" rel="nofollow">postgres</a></li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619233219.0
shub-fuzz/qsym:
  data_format: 2
  description: QSYM  - Concolic Execution Engine (https://github.com/sslab-gatech/qsym)
  filenames:
  - Singularity.1604
  - Singularity.1804
  full_name: shub-fuzz/qsym
  latest_release: 0.0.1
  readme: '<p>Singularity Image for QSYM (<a href="https://github.com/sslab-gatech/qsym">https://github.com/sslab-gatech/qsym</a>)</p>

    <p><a href="https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/3625" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <p>QSYM  - Concolic Execution Engine (<a href="https://github.com/sslab-gatech/qsym">https://github.com/sslab-gatech/qsym</a>)</p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name qsym.sif https://github.com/shub-fuzz/qsym/releases/download/0.0.1/shub-fuzz-qsym.1604.sif


    singularity shell qsym.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619231485.0
shub-fuzz/symcc:
  data_format: 2
  description: Singularity image for SymCC (https://github.com/eurecom-s3/symcc)
  filenames:
  - Singularity.1804
  - Singularity.2004
  full_name: shub-fuzz/symcc
  latest_release: 0.0.1
  readme: '<p>Singularity image for <a href="https://github.com/eurecom-s3/symcc">SymCC</a></p>

    <p><a href="https://github.com/shub-fuzz/symcc/actions/workflows/builder.yml"><img
    src="https://github.com/shub-fuzz/symcc/actions/workflows/builder.yml/badge.svg?branch=main"
    alt="singularity-deploy" style="max-width:100%;"></a>

    <a href="https://singularity-hub.org/collections/4732" rel="nofollow"><img src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li>usage:</li>

    </ul>

    <pre><code>singularity pull --name symcc.sif https://github.com/shub-fuzz/symcc/releases/download/0.0.1/shub-fuzz-symcc.1804.sif


    singularity shell symcc.sif

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619232345.0
shwertt/neurovista_evaluation_sw:
  data_format: 2
  description: 'Code for evaluation on full neurovista trial data '
  filenames:
  - Singularity.nv_eval
  full_name: shwertt/neurovista_evaluation_sw
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-neurovista_evaluation_sw\" class=\"anchor\"\
    \ href=\"#neurovista_evaluation_sw\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>neurovista_evaluation_sw</h1>\n\
    <p>Code for evaluation on full neurovista trial data following the <a href=\"\
    https://github.com/epilepsyecosystem/CodeEvaluationDocs\">instructions</a> (commit\
    \ 20e6f0f, dated 16/06/2020).</p>\n<p>This project has been cloned from\n<a href=\"\
    https://github.com/MatthiasEb/neurovista_evaluation\">https://github.com/MatthiasEb/neurovista_evaluation</a>\
    \ (a colleague of mine, both\nworking at Dresden University of Technology, Germany)\
    \ and since adapted to\nfeature networks consisting of 1D-Convolutions on the\
    \ raw time series.</p>\n<h2>\n<a id=\"user-content-settings\" class=\"anchor\"\
    \ href=\"#settings\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Settings</h2>\n<p>Settings can be adjusted in\
    \ SETTINGS.json</p>\n<h2>\n<a id=\"user-content-using-singularity\" class=\"anchor\"\
    \ href=\"#using-singularity\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Using Singularity</h2>\n<p>This project\
    \ has been thoroughly testey with the Singularity recipe\n<code>Singularity.nv_eval</code>\
    \ that is included in the repository.\nThe SingularityHub URI of my image is <code>shwertt/neurovista_evaluation_sw:nv_eval</code>.</p>\n\
    <h3>\n<a id=\"user-content-installing-singularity\" class=\"anchor\" href=\"#installing-singularity\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Installing Singularity</h3>\n<p>If you need to install singularity\
    \ on your work station, it did not suffice\nfor me to just do a <code>sudo apt\
    \ install singularity-container</code> because this\ninstall a singularity version\
    \ 2. But <a href=\"https://singularity-hub.org/\" rel=\"nofollow\">https://singularity-hub.org/</a>\
    \ deploys\nsingularity version 3 images.</p>\n<p>Trying to pull the shub image\
    \ with singularity version 2 generates the\nfollowing error:</p>\n<pre><code>ERROR\
    \  : Unknown image format/type: /mnt/ieecad/s9759051/Downloads/tmp/shwertt-neurovista_evaluation_sw-main-nv_eval\n\
    ABORT  : Retval = 255\n</code></pre>\n<p>In order to install singularity version\
    \ 3 locally, I followed the installation\ninstructions from <a href=\"https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/Container\"\
    \ rel=\"nofollow\">https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/Container</a></p>\n\
    <p>This involved:</p>\n<ol>\n<li>\n<p>Check if go is installed by executing <code>go\
    \ version</code>. If it is not installed, get it with:</p>\n<p><code>wget https://storage.googleapis.com/golang/getgo/installer_linux\
    \ &amp;&amp; chmod +x installer_linux &amp;&amp; ./installer_linux &amp;&amp;\
    \ source $HOME/.bash_profile</code></p>\n</li>\n<li>\n<p>Install Singularity by\
    \ cloning the singularity repo</p>\n<p><code>mkdir -p ${GOPATH}/src/github.com/sylabs\
    \ &amp;&amp; cd ${GOPATH}/src/github.com/sylabs &amp;&amp; git clone https://github.com/sylabs/singularity.git\
    \ &amp;&amp; cd singularity</code></p>\n</li>\n<li>\n<p>Checkout the singularity\
    \ version you want (see the Github Releases page for available releases), e.g.</p>\n\
    <p><code>git checkout v3.6.3</code></p>\n</li>\n<li>\n<p>Check the environment\
    \ variables for <code>go</code> in <code>~/.bash_profile</code></p>\n<p>I had\
    \ to change all references from <code>/home/s9759051</code> to <code>/mnt/ieecad/s9759051</code>\n\
    otherwise the installer could not find the correct environment for the needed\n\
    modules.</p>\n</li>\n<li>\n<p>Build and install</p>\n<p><code>cd ${GOPATH}/src/github.com/sylabs/singularity\
    \ &amp;&amp; ./mconfig &amp;&amp; cd ./builddir &amp;&amp; make &amp;&amp; sudo\
    \ make install</code></p>\n</li>\n</ol>\n<h2>\n<a id=\"user-content-interacting-with-the-singularity-container\"\
    \ class=\"anchor\" href=\"#interacting-with-the-singularity-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Interacting\
    \ with the Singularity container</h2>\n<p>Pull the container from Singularity\
    \ Hub (once)</p>\n<pre><code>singularity pull --name nv_eval_shwertt.sif shub://shwertt/neurovista_evaluation_sw:nv_eval\n\
    </code></pre>\n<p>Set <code>mode=1</code> in <code>SETTINGS.json</code>, then\
    \ start a shell in the container with:</p>\n<pre><code>singularity shell --contain\
    \ -B /PATH/TO/CLONE/OF/THIS/GITHUB/PROJECT/neurovista_evaluation_sw:$HOME -B /scratch:/scratch\
    \ --nv nv_eval_shwertt.sif\n</code></pre>\n<p>This mounts this github project\
    \ to the <code>Home</code> folder inside singularity and\nbinds the local scratch\
    \ folder to the scratch folder inside singularity. If\nthe data files are not\
    \ inside <code>/scratch</code>, please modify this bind statement.\nCUDA support\
    \ and access to the GPU is achieved with the <code>--nv</code> flag.</p>\n<p>Now\
    \ while inside the Singularity container, execute:</p>\n<pre><code>export CUDA_VISIBLE_DEVICES=0\
    \ &amp;&amp; python3 run.py\n</code></pre>\n<p>Please specify your <code>CUDA_VISIBLE_DEVICES</code>\
    \ according to the available resources\nof the supercomputer.</p>\n<p>After training\
    \ is complete, you can test the model by closing the container\nwith <code>exit</code>,\
    \ modify <code>SETTINGS.json</code> to switch to <code>mode=3</code> and start\
    \ the container again with:</p>\n<pre><code>singularity shell --contain -B /PATH/TO/CLONE/OF/THIS/GITHUB/PROJECT/neurovista_evaluation_sw:$HOME\
    \ -B /scratch:/scratch --nv nv_eval_shwertt.sif\n</code></pre>\n<p>then execute\
    \ again:</p>\n<pre><code>export CUDA_VISIBLE_DEVICES=0 &amp;&amp; python3 run.py\n\
    </code></pre>\n<p>Please find the solution file under\n<code>solutions/contest_data_solution_shwertt_mode3.csv</code>.</p>\n\
    <h2>\n<a id=\"user-content-remarks\" class=\"anchor\" href=\"#remarks\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Remarks</h2>\n\
    <p>You should use a GPU for training. I did use an RTX 2080 Ti.\nIf you use a\
    \ GPU with much less RAM, you might have to reduce the batch size, I did not try\
    \ that though.\nWhen I ran the code with <code>run_on_contest_data=1</code>, the\
    \ results seemed to be comparable\nto the results from <a href=\"https://github.com/MatthiasEb/neurovista_evaluation\"\
    >MatthiasEb</a>.\nThe singularity container works just fine, in case you run into\
    \ problems, have any questions or remarks,\nplease do not hesitate to contact\
    \ me.</p>\n<h3>\n<a id=\"user-content-algorithm\" class=\"anchor\" href=\"#algorithm\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Algorithm</h3>\n<p>This approach uses a Residual Network (Resnet)\
    \ based on 1D-Convolutions, applied to the raw time series.\nThe residual block\
    \ consist of this stack:</p>\n<pre><code>def residual_block(X, kernels, size):\n\
    \    out = tf.keras.layers.Conv1D(kernels, size, padding='same')(X)\n    out =\
    \ tf.keras.layers.ReLU()(out)\n    out = tf.keras.layers.Conv1D(kernels, size,\
    \ padding='same')(out)\n    out = tf.keras.layers.add([X, out])\n    out = tf.keras.layers.ReLU()(out)\n\
    \    out = tf.keras.layers.MaxPool1D(pool_size=5, strides=3)(out)\n    return\
    \ out\n</code></pre>\n<p>The deep neural network then consists of six residual\
    \ blocks followed by a\nglobal 1D-MaxPooling layer and a dense layer. The model\
    \ is optimized with\nAdam, which uses a learning rate of 0.001.</p>\n<p>The model\
    \ expects standardized 15 s segments of data, sampled at 200 Hz.\ntensorflow.keras\
    \ (2.0.1) was used as Deep Learning API.\nAfter several testruns with different\
    \ models on the contest data, I chose the\nresnet1d architecture that has been\
    \ described above.</p>\n<p>With it I have achieved the following results:</p>\n\
    <pre><code>{'run_on_contest_data': 1, 'mode': 3, 'pat': '1-3', 'subtract_mean':\
    \ 1, 'model': './trains', 'feat': './features', 'CSV': './CSV', 'solutions': './solutions'}\n\
    ./solutions/contest_data_solution_shwertt_mode3_20201001_resnet1d.csv\nGlobal\
    \ roc auc: 0.6555\nGlobal Public roc auc: 0.6627\nGlobal Private roc auc: 0.6684\n\
    Patient 1 roc auc: 0.2143\nPatient 2 roc auc: 0.6700\nPatient 3 roc auc: 0.7326\n\
    Patient 1 Public roc auc: 0.2111\nPatient 1 Private roc auc: 0.2182\nPatient 2\
    \ Public roc auc: 0.6695\nPatient 2 Private roc auc: 0.6766\nPatient 3 Public\
    \ roc auc: 0.7618\nPatient 3 Private roc auc: 0.7208\n</code></pre>\n<p>However,\
    \ considerable variations are conceivable.</p>\n<h3>\n<a id=\"user-content-implementation\"\
    \ class=\"anchor\" href=\"#implementation\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Implementation</h3>\n<p>Loading\
    \ the original (~ 400 Hz) .mat files, resampling to 200 Hz,\nstandardizing (optionally,\
    \ if <code>subtract_mean==1</code>), splitting them in 15 s\nsegments is done\
    \ asynchronously on the fly by the dataloader in 5 different\nthreads. The 15s\
    \ Segments are enqueued in a buffer with the size of 400\n10-min-sequences, implemented\
    \ as a <code>tf.queue.RandomShuffleQueue</code>. The data is\ntherefore dequeued\
    \ in random order, although not perfectly uniformly\nshuffeled, depending on the\
    \ buffer size and the size of the data set. The\nintention was to ensure a reasonably\
    \ shuffeled training set of 15 s segments\nwhile minimizing IO, working on the\
    \ .mat files and having the possibility for\nstandardization. If the IO-Bandwidth\
    \ of the filesystem is reasonably high,\nthis should not slow down the training\
    \ too much.</p>\n<p>If <code>run_on_contest_data==1</code>, 3 networks (one for\
    \ each patient) are trained and evaluated individually.\nSubsequently, the solution\
    \ file is concatenated.</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1602006304.0
singularityhub/singularity-compose-examples:
  data_format: 2
  description: A simple example of running a MongoDB instance to query a database
  filenames:
  - v1.0/mongodb-build/mongodb/Singularity
  - v1.0/django-nginx-upload/Singularity
  - v1.0/django-nginx-upload/db/Singularity
  - v1.0/django-nginx-upload/nginx/Singularity
  - v1.0/apache-simple/httpd/Singularity
  - v1.0/rstudio-simple/rstudio/Singularity
  - v1.0/rstudio-simple/nginx/Singularity
  - v1.0/jupyter-simple/jupyter/Singularity
  - v1.0/bert-as-compose/server/Singularity
  - v1.0/bert-as-compose/server/Singularity.gpu
  - v1.0/bert-as-compose/client/Singularity
  - v2.0/ping/alp2/Singularity
  - v2.0/ping/alp1/Singularity
  - v2.0/start-args/Singularity
  - v2.0/deephyperx/Singularity
  - v2.0/code-server/Singularity
  full_name: singularityhub/singularity-compose-examples
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-compose-examples" class="anchor" href="#singularity-compose-examples"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    Compose Examples</h1>

    <p>This is a repository of examples for

    <a href="https://singularityhub.github.io/singularity-compose" rel="nofollow">Singularity
    Compose</a>. For the "simple"

    example that is used during testing, see <a href="https://github.com/singularityhub/singularity-compose-simple">singularity-compose-simple</a>.
    Otherwise, all examples are provided here. You can browse based on the spec version
    of Singularity Compose:</p>

    <ul>

    <li>

    <a href="v1.0">v1.0</a> is supported for Singularity compose less than v0.1.0</li>

    <li>

    <a href="v2.0">v2.0</a> is supported for Singularity compose equal to or greater
    than v0.1.0</li>

    </ul>

    '
  stargazers_count: 10
  subscribers_count: 4
  topics:
  - singularity-compose
  - mongodb
  updated_at: 1620468411.0
slhogle/singularity_def_files:
  data_format: 2
  description: Singularity definition files for building various software to run on
    HPC systems
  filenames:
  - torstyverse.def
  - instrain.def
  - checkm.def
  - octopus.def
  full_name: slhogle/singularity_def_files
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-definition-files" class="anchor" href="#singularity-definition-files"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Singularity
    definition files</h1>

    <p>Collection of def files for building some bioinformatics software I commonly
    use.</p>

    <h2>

    <a id="user-content-instrain-v1214" class="anchor" href="#instrain-v1214" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>inStrain v1.2.14</h2>

    <p><a href="https://github.com/MrOlm/inStrain">https://github.com/MrOlm/inStrain</a></p>

    <p>Also contains these functioning binaries:</p>

    <ul>

    <li><a href="https://github.com/samtools/samtools">samtools v1.10</a></li>

    <li><a href="https://github.com/hyattpd/Prodigal">prodigal v2.6.3</a></li>

    <li><a href="https://github.com/lh3/bwa">bwa v0.7.17-r1198-dirty</a></li>

    <li><a href="https://github.com/lh3/minimap2">minimap2 v2.17 (r941)</a></li>

    <li><a href="https://github.com/dnbaker/dashing">Dashing v0.4.8-1-g47e6</a></li>

    <li><a href="https://github.com/ParBLiSS/FastANI">FastANI v1.3</a></li>

    <li><a href="https://github.com/wwood/CoverM">CoverM v0.4.0</a></li>

    </ul>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/instrain" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/instrain</code></p>

    <h2>

    <a id="user-content-octopus-development-branch-version-v070-develop-2bde0433"
    class="anchor" href="#octopus-development-branch-version-v070-develop-2bde0433"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Octopus
    development branch version v0.7.0 (develop 2bde0433)</h2>

    <p><a href="https://github.com/luntergroup/octopus">https://github.com/luntergroup/octopus</a></p>

    <p>Built with:</p>

    <ul>

    <li>patchelf v0.10</li>

    <li>openssl v1.1.1g</li>

    <li>pkg-config v0.29.2</li>

    <li>gpatch v2.7.6</li>

    <li>ncurses v6.2</li>

    <li>cmake v3.17.3</li>

    <li>htslib v1.10</li>

    <li>boost v1.72.0</li>

    <li>GNU C/C++ compiler v9.3.0</li>

    </ul>

    <p>Target: x86_64 Linux 5.3.0-7642-generic<br>

    SIMD extension: AVX2</p>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/octopus" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/octopus</code></p>

    <h2>

    <a id="user-content-torstyverse" class="anchor" href="#torstyverse" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Torstyverse</h2>

    <p>Bundle of useful packages from <a href="https://github.com/tseemann">Torsten
    Seeman</a></p>

    <ul>

    <li><a href="https://github.com/tseemann/samclip">sampclip v0.4.0</a></li>

    <li><a href="https://github.com/tseemann/any2fasta">any2fasta v0.4.2</a></li>

    <li><a href="https://github.com/tseemann/barrnap">barrnap v0.9</a></li>

    <li><a href="https://github.com/tseemann/prokka">prokka v1.14.6</a></li>

    <li><a href="https://github.com/tseemann/shovill">shovill v1.1.0</a></li>

    <li><a href="https://github.com/tseemann/abricate">abricate v1.0.1</a></li>

    <li><a href="https://github.com/tseemann/snippy">snippy v4.6.0</a></li>

    </ul>

    <p><a href="https://cloud.sylabs.io/library/slhogle/base/torstyverse" rel="nofollow">Image
    at Sylabs</a></p>

    <p>Download with:<br>

    <code>singularity pull library://slhogle/base/torstyverse</code></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1614967869.0
smrtin/TEnriAn:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: smrtin/TEnriAn
  latest_release: null
  readme: '<h1>

    <a id="user-content-tenrian" class="anchor" href="#tenrian" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TEnriAn</h1>

    <p>TEnriAn (Target ENrichment ANalysis) pipeline</p>

    <p>recipe file for a singularity container build</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611007887.0
sodalite-hpe/modak:
  data_format: 2
  description: null
  filenames:
  - AI/PyTorch/latest/gpu/src/Singularity.pytorch-latest-gpu-src
  - AI/PyTorch/latest/gpu/pip/Singularity.pytorch-latest-gpu-pip
  - AI/PyTorch/latest/cpu/src/Singularity.pytorch-1.5-cpu-src
  - AI/PyTorch/latest/cpu/src/glow/Singularity
  - AI/PyTorch/latest/cpu/pip/Singularity.pytorch-latest-cpu-pip
  - AI/PyTorch/1.5/gpu/src/Singularity.pytorch-1.5-gpu-src
  - AI/PyTorch/1.5/gpu/pip/Singularity.pytorch-1.5-gpu-pip
  - AI/PyTorch/1.5/cpu/src/Singularity.pytorch-1.5-cpu-src
  - AI/PyTorch/1.5/cpu/src/glow/Singularity.pytorch-1.5-cpu-src-glow
  - AI/PyTorch/1.5/cpu/pip/Singularity.pytorch-1.5-cpu-pip
  - AI/TensorFlow/latest/gpu/Singularity.tensorflow-latest-gpu-src
  - AI/TensorFlow/latest/gpu/src/Singularity.tensorflow-latest-gpu-src
  - AI/TensorFlow/latest/gpu/pip/Singularity.tensorflow-latest-gpu-pip
  - AI/TensorFlow/latest/cpu/src/Singularity.tensorflow-latest-cpu-src
  - AI/TensorFlow/latest/cpu/pip/Singularity.tensorflow-latest-cpu-pip
  - AI/TensorFlow/2.1/gpu/Singularity.tensorflow-2.1-gpu-src
  - AI/TensorFlow/2.1/gpu/src/Singularity.tensorflow-2.1-gpu-src
  - AI/TensorFlow/2.1/gpu/pip/Singularity.tensorflow-2.1-gpu-pip
  - AI/TensorFlow/2.1/cpu/src/Singularity.tensorflow-2.1-cpu-src
  - AI/TensorFlow/2.1/cpu/pip/Singularity.tensorflow-2.1-cpu-pip
  - AI/TensorFlow/ngraph/Singularity.tensorflow-ngraph
  - HPC/code-aster/Singularity.code-aster-serial
  - HPC/cp2k/Singularity.cp2k-mpich
  - HPC/cp2k/Singularity.cp2k-openmpi
  - base/ubuntu/18_04/Singularity.base-ubuntu-18-04
  - base/nvidia/cuda-10.1/Singularity.base-nvidia-cuda-10-1
  - Performance-model/mpich/Singularity.mpich_benchmarks
  - Performance-model/mpich_ubuntu16/Singularity.mpich_ubuntu16_benchmarks
  - test/Singularity.ubuntu-base
  full_name: sodalite-hpe/modak
  latest_release: null
  readme: '<h1>

    <a id="user-content-containers" class="anchor" href="#containers" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>containers</h1>

    <p>contains containers for MODAK</p>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics: []
  updated_at: 1617111147.0
stela2502/singularityImages:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: stela2502/singularityImages
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularityimages" class="anchor" href="#singularityimages"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularityImages</h1>

    <p>This git repo is a skelleton of my work I have done on singularity images.

    These images are used on aurora-ls2 to run analyses on the blades instead of the
    frontend.</p>

    <p>All of that documention is in our Bioinformatics Slack Howto channel.</p>

    <p>The software I install I mainly install from within the singularity image.
    Hence the usage of shell.sh.</p>

    <p>Instaling Python modules is tricky as pip3 always installs in a private path
    and not the global unless told otherwise.

    Hence only I with my username on the computer I build the images could use the
    modules.</p>

    <p>A solution could be to use some conda approach, but as this here will be a
    singularity image we could also try to install globaly:</p>

    <p>Python solution:</p>

    <pre><code>pip3 install --prefix=/usr/local &lt;package name&gt;

    </code></pre>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620320126.0
sylabs/singularity:
  data_format: 2
  description: SingularityCE is the Community Edition of Singularity, an open source
    container platform designed to be simple, fast, and secure.
  filenames:
  - examples/legacy/2.3/contrib/raspbian.def
  - examples/legacy/2.2/centos.def
  - examples/legacy/2.2/arch.def
  - examples/legacy/2.2/ubuntu.def
  - examples/legacy/2.2/docker.def
  - examples/legacy/2.2/debian.def
  - examples/legacy/2.2/busybox.def
  - examples/legacy/2.2/scientific.def
  - examples/legacy/2.2/contrib/r_python_julia.def
  - examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1.def
  - examples/legacy/2.2/contrib/ubuntu-root.def
  - examples/legacy/2.2/contrib/centos7-ompi_master.def
  - examples/legacy/2.2/contrib/fedora.def
  - examples/legacy/2.2/contrib/ubuntu-bio.def
  - examples/legacy/2.2/contrib/ubuntu-openfoam.def
  - examples/legacy/2.2/contrib/centos7-ompi_cuda.def
  - examples/legacy/2.2/contrib/linuxbrew_and_non-root_software_example.def
  - examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1-gpu.def
  - examples/legacy/2.2/contrib/debian85-tensorflow-0.10.def
  - examples/legacy/2.2/contrib/centos-minimal.def
  - examples/build-singularity/build-singularity.def
  - e2e/testdata/Docker_registry.def
  - e2e/testdata/sshfs.def
  - e2e/testdata/inspecter_container.def
  - e2e/testdata/regressions/issue_4583.def
  - e2e/testdata/regressions/issue_5250.def
  - e2e/testdata/regressions/issue_5399.def
  - e2e/testdata/regressions/issue_4203.def
  - e2e/testdata/regressions/issue_5315.def
  - e2e/testdata/regressions/issue_4967.def
  - e2e/testdata/regressions/issue_4969.def
  - e2e/testdata/regressions/issue_4820.def
  full_name: sylabs/singularity
  latest_release: v3.7.3
  readme: '<h1>

    <a id="user-content-singularityce" class="anchor" href="#singularityce" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>SingularityCE</h1>

    <p><a href="https://circleci.com/gh/sylabs/singularity/tree/master" rel="nofollow"><img
    src="https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667"
    alt="CircleCI" data-canonical-src="https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg"
    style="max-width:100%;"></a></p>

    <ul>

    <li><a href="CONTRIBUTING.md">Guidelines for Contributing</a></li>

    <li><a href=".github/PULL_REQUEST_TEMPLATE.md">Pull Request Template</a></li>

    <li><a href="LICENSE.md">Project License</a></li>

    <li><a href="https://www.sylabs.io/docs/" rel="nofollow">Documentation</a></li>

    <li><a href="#support">Support</a></li>

    <li><a href="#citing-singularity">Citation</a></li>

    </ul>

    <p>SingularityCE is the Community Edition of Singularity, an open source container

    platform designed to be simple, fast, and secure. Singularity is optimized

    for compute focused enterprise and HPC workloads, allowing untrusted users

    to run untrusted containers in a trusted way.</p>

    <p>Check out <a href="https://www.sylabs.io/videos" rel="nofollow">talks about
    Singularity</a> and some <a href="https://sylabs.io/case-studies" rel="nofollow">use

    cases of Singularity</a> on our website.</p>

    <h2>

    <a id="user-content-getting-started-with-singularityce" class="anchor" href="#getting-started-with-singularityce"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting
    Started with SingularityCE</h2>

    <p>To install SingularityCE from source, see the <a href="INSTALL.md">installation

    instructions</a>. For other installation options, see <a href="https://www.sylabs.io/guides/latest/admin-guide/"
    rel="nofollow">our

    guide</a>.</p>

    <p>System administrators can learn how to configure SingularityCE, and get an

    overview of its architecture and security features in the <a href="https://www.sylabs.io/guides/latest/admin-guide/"
    rel="nofollow">administrator

    guide</a>.</p>

    <p>For users, see the <a href="https://www.sylabs.io/guides/latest/user-guide/"
    rel="nofollow">user

    guide</a> for details on how to use

    and build Singularity containers.</p>

    <h2>

    <a id="user-content-contributing-to-singularityce" class="anchor" href="#contributing-to-singularityce"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing
    to SingularityCE</h2>

    <p>Community contributions are always greatly appreciated. To start developing

    SingularityCE, check out the <a href="CONTRIBUTING.md">guidelines for contributing</a>.</p>

    <p>We also welcome contributions to our <a href="https://github.com/sylabs/singularity-userdocs">user

    guide</a> and <a href="https://github.com/sylabs/singularity-admindocs">admin

    guide</a>.</p>

    <h2>

    <a id="user-content-support" class="anchor" href="#support" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Support</h2>

    <p>To get help with SingularityCE, check out the community spaces

    detailed at our <a href="https://www.sylabs.io/singularity/community/" rel="nofollow">Community

    Portal</a>.</p>

    <p>See also our <a href="SUPPORT.md">Support Guidelines</a> for further

    information about the best place, and how, to raise different kinds of

    issues and questions.</p>

    <p>For additional support, <a href="https://www.sylabs.io/contact/" rel="nofollow">contact
    us</a> to receive

    more information.</p>

    <h2>

    <a id="user-content-citing-singularity" class="anchor" href="#citing-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Citing
    Singularity</h2>

    <pre><code>Kurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers
    for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459

    </code></pre>

    <p>We also have a Zenodo citation:</p>

    <pre><code>Kurtzer, Gregory M. et. al. Singularity - Linux application and environment

    containers for science. 10.5281/zenodo.1310023

    </code></pre>

    <p><a href="https://doi.org/10.5281/zenodo.1310023" rel="nofollow">https://doi.org/10.5281/zenodo.1310023</a></p>

    <p>This is an ''all versions'' DOI. Follow the link to Zenodo to obtain a DOI
    specific

    to a particular version of Singularity.</p>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p><em>Unless otherwise noted, this project is licensed under a 3-clause BSD license

    found in the <a href="LICENSE.md">license file</a>.</em></p>

    '
  stargazers_count: 23
  subscribers_count: 6
  topics:
  - containers
  - hpc
  - linux
  updated_at: 1621988935.0
sylvainschmitt/singularity-template:
  data_format: 2
  description: template for Singularity container
  filenames:
  - Singularity
  full_name: sylvainschmitt/singularity-template
  latest_release: 0.0.4
  readme: "<h1>\n<a id=\"user-content-template-singularity-container\" class=\"anchor\"\
    \ href=\"#template-singularity-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Template Singularity container</h1>\n\
    <p>Sylvain Schmitt\nApril 28, 2021</p>\n<p><strong>Bionformatics package Template</strong></p>\n\
    <p>Template is a set of utilities that Blah.</p>\n<p>Template Version: X.X.X</p>\n\
    <p>[URL]</p>\n<p>Singularity container based on the recipe: Singularity</p>\n\
    <p>Package installation using Miniconda3 V4.7.12</p>\n<p>Image singularity (V&gt;=3.3)\
    \ is automatically test and built and pushed\non the registry using\n<a href=\"\
    https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\
    >test.yml</a>\n&amp;\n<a href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\
    >builder.yml</a></p>\n<p><strong>build</strong>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity build Singularity img.sif</pre></div>\n<p><strong>pull</strong>:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity pull https://github.com/sylvainschmitt/singularity-template/releases/download/0.0.4/sylvainschmitt-singularity-template.latest.sif</pre></div>\n\
    <p><strong>snakemake</strong>:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre>    <span class=\"pl-s1\">singularity</span>: \n        <span class=\"pl-s\"\
    >\"https://github.com/sylvainschmitt/singularity-template/releases/download/0.0.4/sylvainschmitt-singularity-template.latest.sif\"\
    </span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619647716.0
sylvainschmitt/singularity-tidyverse-Biostrings:
  data_format: 2
  description: 'tidyverse and Biostrings Singularity container '
  filenames:
  - Singularity
  full_name: sylvainschmitt/singularity-tidyverse-Biostrings
  latest_release: 0.0.1
  readme: "<h1>\n<a id=\"user-content-tidyverse-and-biostrings-singularity-container\"\
    \ class=\"anchor\" href=\"#tidyverse-and-biostrings-singularity-container\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>tidyverse\
    \ and Biostrings Singularity container</h1>\n<p>Sylvain Schmitt\nApril 28, 2021</p>\n\
    <p><strong>R packages tidyverse and Biostrings</strong></p>\n<p>The <code>tidyverse</code>\
    \ is an opinionated collection of R packages designed for\ndata science. All packages\
    \ share an underlying design philosophy,\ngrammar, and data structures.</p>\n\
    <p><code>tidyverse</code> Version: X.X.X</p>\n<p>[<a href=\"https://www.tidyverse.org/\"\
    \ rel=\"nofollow\">https://www.tidyverse.org/</a>]</p>\n<p><code>Biostrings</code>\
    \ is a memory efficient string containers, string matching\nalgorithms, and other\
    \ utilities, for fast manipulation of large\nbiological sequences or sets of sequences.</p>\n\
    <p><code>Biostrings</code> Version: X.X.X</p>\n<p>[<a href=\"https://bioconductor.org/packages/release/bioc/html/Biostrings.html\"\
    \ rel=\"nofollow\">https://bioconductor.org/packages/release/bioc/html/Biostrings.html</a>]</p>\n\
    <p>Singularity container based on the recipe:\n<a href=\"https://github.com/sylvainschmitt/singularity-tidyverse-Biostrings/blob/main/Singularity\"\
    ><code>Singularity</code></a></p>\n<p>Image singularity (V&gt;=3.3) is automatically\
    \ test and built and pushed\non the registry using\n<a href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\
    >test.yml</a>\n&amp;\n<a href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\
    >builder.yml</a></p>\n<p><strong>build</strong>:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>sudo singularity build Biostrings.sif Singularity</pre></div>\n<p><strong>pull</strong>:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>singularity pull https://github.com/sylvainschmitt/singularity-tidyverse-Biostrings/releases/download/0.0.1/sylvainschmitt-singularity-tidyverse-Biostrings.latest.sif</pre></div>\n\
    <p><strong>snakemake</strong>:</p>\n<div class=\"highlight highlight-source-python\"\
    ><pre>    <span class=\"pl-s1\">singularity</span>: \n        <span class=\"pl-s\"\
    >\"https://github.com/sylvainschmitt/singularity-tidyverse-Biostrings/releases/download/0.0.1/sylvainschmitt-singularity-tidyverse-Biostrings.latest.sif\"\
    </span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619660633.0
thakk/biobase:
  data_format: 2
  description: null
  filenames:
  - Singularity.bedtools
  - Singularity
  - Singularity.methylkit
  - Singularity.Bowtie2
  - Singularity.FastQC
  - Singularity.samtools
  full_name: thakk/biobase
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-containers-for-bioinformatics-tools" class="anchor"
    href="#singularity-containers-for-bioinformatics-tools" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity containers
    for bioinformatics tools</h1>

    <p>Bioinformatics related singularity container recipies.</p>

    <p>Base is CentOS 8.</p>

    <p>Currently two containers are implemented:</p>

    <ul>

    <li>basic tools:

    <ul>

    <li>Samtools</li>

    <li>BEDTools</li>

    <li>FastQC</li>

    <li>Bowtie2</li>

    <li>MultiQC</li>

    <li>Cutadapt</li>

    <li>STAR</li>

    <li>Hisat2</li>

    <li>Picard</li>

    <li>Trimmomatic</li>

    <li>Samblaster</li>

    <li>VarScan</li>

    <li>Vcfanno</li>

    <li>Plink</li>

    <li>MACS2</li>

    <li>Homer</li>

    <li>NextFlow</li>

    <li>nf-core</li>

    <li>MAGeCK</li>

    <li>TrimGalore</li>

    <li>Bismark</li>

    <li>UCSC tools</li>

    </ul>

    </li>

    <li>methylKit (built from basic):

    <ul>

    <li>R + Bioconductor</li>

    <li>methylkit</li>

    </ul>

    </li>

    <li>samtools (built from Alpine Linux 3.10.3)

    <ul>

    <li>Note, automated Singularity Hub build does not seem to work correctly as this
    recipe uses multistage build to minimize container size</li>

    </ul>

    </li>

    </ul>

    <h2>

    <a id="user-content-availability" class="anchor" href="#availability" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Availability</h2>

    <p>Basic tools container is available at Singularity hub: shub://thakk/biobase</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1589823361.0
theasp/docker-emacs:
  data_format: 2
  description: Emacs in a Docker container
  filenames:
  - Singularity-ubuntu18.04-emacs27-slim
  full_name: theasp/docker-emacs
  latest_release: null
  readme: '<p>Fedora Singularity image for running Montage workflows</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics:
  - docker
  - emacs
  updated_at: 1601662369.0
thomasarsouze/singularity-deploy:
  data_format: 2
  description: Test with singularity-deploy
  filenames:
  - Singularity
  - Singularity.salad
  - Singularity.pokemon
  full_name: thomasarsouze/singularity-deploy
  latest_release: 0.0.12
  readme: "<h1>\n<a id=\"user-content-singularity-deploy\" class=\"anchor\" href=\"\
    #singularity-deploy\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>Singularity Deploy</h1>\n<p><a href=\"img/shpc.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"img/shpc.png\" alt=\"\
    img/shpc.png\" style=\"max-width:100%;\"></a></p>\n<p>Wouldn't it be nice to build\
    \ Singularity images without a registry proper,\nand just keep them alongside\
    \ the GitHub codebase? This is now possible!\nThis small repository provides an\
    \ example to get you started. It will\nbuild one or more images (whatever Singularity.*\
    \ files that are present at\nthe root) and then release them as assets to your\
    \ GitHub repository so\nthat they can be programatically obtained. It is associated\
    \ with\n<a href=\"https://github.com/singularityhub/singularity-hpc\">singularity-hpc</a>\
    \ to allow\nyou to then define LMOD modules for these same containers.</p>\n<blockquote>\n\
    <p>Can I upload the largest of chonkers?</p>\n</blockquote>\n<p>Yes and no. Note\
    \ that assets are limited to 2 GB in size, which is still fairly good. You can\
    \ use\nit as a template for your own recipes as is, or modify it for your custom\n\
    use case. Instructions are below!</p>\n<h2>\n<a id=\"user-content-getting-started\"\
    \ class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Getting Started</h2>\n<h3>\n\
    <a id=\"user-content-1-template-or-fork\" class=\"anchor\" href=\"#1-template-or-fork\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1. Template or Fork</h3>\n<p>If you haven't already, template or fork\
    \ this repository. You can then clone\nyour fork:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ git clone git@github.com:<span class=\"pl-k\"\
    >&lt;</span>username<span class=\"pl-k\">&gt;</span>/singularity-deploy</pre></div>\n\
    <p>You likely want to name the repository by the container. For example, if I\
    \ would\nhave created a container on Docker Hub or similar with the name <code>vsoch/salad</code>,\n\
    here I'd call the repository <code>salad</code>. You obviously are limited to\
    \ your username\nor an organizational namespace.</p>\n<h3>\n<a id=\"user-content-1-write-your-singularity-recipes\"\
    \ class=\"anchor\" href=\"#1-write-your-singularity-recipes\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>1. Write\
    \ your Singularity Recipe(s)</h3>\n<p>First, you should write your container recipe(s)\
    \ in the present working directory.\nFor good practice, when you are updating\
    \ recipes you should checkout a new branch\nand open a pull request, as the repository\
    \ comes with a workflow to trigger on a PR\nto <a href=\".github/workflows/test.yml\"\
    >test your container build</a>. You can add any additional\ntests that that you\
    \ might need. By default, any Singularity.* file will be automatically detected.\n\
    If there is no extension (the name Singularity), the name used will be \"latest.\"\
    \nYou can use these tags across multiple releases of your containers. For example,\n\
    these files would generate packages with sifs named as follows:</p>\n<ul>\n<li>\n\
    <a href=\"Singularity\">Singularity</a> maps to <a href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\
    >https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif</a>\n\
    </li>\n<li>\n<a href=\"Singularity.pokemon\">Singularity.pokemon</a> maps to <a\
    \ href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif\"\
    >https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif</a>\n\
    </li>\n<li>\n<a href=\"Singularity.salad\">Singularity.salad</a> maps to <a href=\"\
    https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\
    >https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif</a>\n\
    </li>\n</ul>\n<p>For each name, you can see the direct download URL contains the\
    \ repository (singularityhub/singularity-deploy),\nYou should not use any <code>:</code>\
    \ characters in either your container tag (the GitHub extension) or\nthe GitHub\
    \ tags (the release tags) as this might interfere with parsing.\nThe GitHub release\
    \ tag (0.0.1 in the example above) is discussed next.</p>\n<h3>\n<a id=\"user-content-2-update-the-version-file\"\
    \ class=\"anchor\" href=\"#2-update-the-version-file\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Update the\
    \ VERSION file</h3>\n<p>Any time that you prepare new container recipes, you should\
    \ update the <a href=\"VERSION\">VERSION</a>\nfile. The way that this repository\
    \ works is to generate a release based on the\nstring in <code>VERSION</code>.\
    \ A version is just a tag, so it could be something like\n<code>0.0.1</code> or\
    \ <code>0.0.1-slim</code>. Keep in mind that GitHub releases cannot have duplicated\n\
    names, so you should not repeat the same tag. Do not use <code>:</code> in your\
    \ tag names.\nIf you do need to re-release a tag (not recommended if a user might\
    \ be using it and then it's changed) you can manually delete\nthe release and\
    \ the tag in the GitHub interface. This is a nice structure because it\nmeans\
    \ you can have containers with different names under the same tag. In the example\n\
    above, we have each of \"deploy,\" \"latest,\" and \"salad\" released under tag\
    \ 0.0.1.\nThis is how it looks on GitHub:</p>\n<p><a href=\"img/releases.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"img/releases.png\"\
    \ alt=\"img/releases.png\" style=\"max-width:100%;\"></a></p>\n<h3>\n<a id=\"\
    user-content-3-how-to-develop\" class=\"anchor\" href=\"#3-how-to-develop\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>3.\
    \ How to Develop</h3>\n<p>As we mentioned previously, the container builds will\
    \ be tested on a pull request,\nand the release will trigger on merge into your\
    \ main branch (main). See the <a href=\".github/workflows/builder.yml\">.github/workflows/builder.yml</a>)\n\
    to edit this. The idea is that you can:</p>\n<ol>\n<li>Develop your container\
    \ via a development branch</li>\n<li>Open a pull request to test the container\
    \ (the <a href=\".github/workflows/test.yml\">.github/workflows/test.yml</a>)</li>\n\
    <li>On merge, your container will be released!</li>\n</ol>\n<h3>\n<a id=\"user-content-4-how-to-pull\"\
    \ class=\"anchor\" href=\"#4-how-to-pull\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>4. How to pull</h3>\n<p>Technically,\
    \ Singularity can pull just knowing the URL. For example:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ singularity pull https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif</pre></div>\n\
    <p>However, the <a href=\"singularity-hpc\">singularity-hpc</a> tool (will be)\
    \ designed to be able to parse and handle\nthese container uris automatically.\
    \ For the containers here, you could do:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:latest\n$ shpc\
    \ pull gh://singularityhub/singularity-deploy/0.0.1:salad\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:pokemon</pre></div>\n\
    <p>or write the container URI into a registry entry:</p>\n<pre><code>gh: singularityhub/singularity-deploy\n\
    latest:\n  latest: \"0.0.1\"\ntags:\n  \"latest\": \"0.0.1\"\n  \"salad\": \"\
    0.0.1\"\n  \"pokemon\": \"0.0.1\"\nmaintainer: \"@vsoch\"\nurl: https://github.com/singularityhub/singularity-deploy\n\
    </code></pre>\n<p>(This part is still under development!)</p>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1617984120.0
tikk3r/lofar-grid-hpccloud:
  data_format: 2
  description: null
  filenames:
  - Singularity.lofar_sksp
  - Singularity.lofar_sksp_base_cuda
  - Singularity.lofar_sksp_base_mkl_cuda
  - Singularity.lofar_sksp_base
  full_name: tikk3r/lofar-grid-hpccloud
  latest_release: v3.1
  readme: '<p><a href="https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572"
    data-canonical-src="https://img.shields.io/github/v/release/tikk3r/lofar-grid-hpccloud?sort=semver"
    style="max-width:100%;"></a></p>

    <p><a href="https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562"
    target="_blank" rel="nofollow"><img src="https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562"
    data-canonical-src="https://img.shields.io/github/license/tikk3r/lofar-grid-hpccloud.svg?logo=github"
    style="max-width:100%;"></a></p>

    [![DOI](<a href="https://zenodo.org/badge/136925861.svg)%5D(https://zenodo.org/badge/latestdoi/136925861"
    rel="nofollow">https://zenodo.org/badge/136925861.svg)](https://zenodo.org/badge/latestdoi/136925861</a>)

    <h1>

    <a id="user-content-lofar-grid-hpccloud" class="anchor" href="#lofar-grid-hpccloud"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>lofar-grid-hpccloud</h1>

    <p>This repository hold resources for deploying the LOFAR software (genericpipeline)
    and related tools through Singularity containers. These containers are general,
    but at the same time somewhat tailored for SKSP use.</p>

    <p>The <code>master</code> branch is empty. Currently the images are based on
    the Fedora 27 Linux distribution, which is available from <a href="https://hub.docker.com/_/fedora"
    rel="nofollow">DockerHub</a>. Recipes to build this container can be found on
    the <code>fedora</code> branch.</p>

    <p>To build a full LOFAR Singularity image, do the following:</p>

    <ol>

    <li>

    <p>Build Singularity.lofarbase</p>

    <p>sudo singularity build lofar_sksp_base.sif Singularity.lofar_sksp_base</p>

    </li>

    <li>

    <p>Build Singularity.lofar (use the <code>From: localimage</code> part instead
    of the Singularity Hub part)</p>

    <p>sudo singularity build lofar_sksp.sif Singularity.lofar_sksp</p>

    </li>

    </ol>

    <p>Pre-built containers are public hosted at <a href="https://lofar-webdav.grid.sara.nl/software/shub_mirror/tikk3r/lofar-grid-hpccloud/"
    rel="nofollow">SURFSara</a>. Sort by date to find the latest container there.</p>

    <p>Visit the  <a href="https://github.com/tikk3r/lofar-grid-hpccloud/wiki">wiki</a>
    for more detailed information and build instructions.</p>

    '
  stargazers_count: 2
  subscribers_count: 3
  topics: []
  updated_at: 1620055907.0
tleonardi/pycoqc_pipeline:
  data_format: 2
  description: Nextflow pipeline that runs pycoQC on Guppy output
  filenames:
  - singularity/Singularity.baseimage
  full_name: tleonardi/pycoqc_pipeline
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-pycoqc-nextflow-script\" class=\"anchor\" href=\"\
    #pycoqc-nextflow-script\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>pycoQC nextflow script</h1>\n<p>This is a nextflow\
    \ script that runs pycoQC on the output folder of Guppy.</p>\n<p>The script takes\
    \ the followig command line arguments:</p>\n<pre><code>--guppy_dir  dir \tPath\
    \ to the guppy output directory containing *_sequencing_summary.txt\n--samplename\
    \ name \tSample name\n--resultsDir dir \tPath to the output directory\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1576530678.0
touala/WhatsHap:
  data_format: 2
  description: WhatsHap repository for singularity container
  filenames:
  - Singularity
  full_name: touala/WhatsHap
  latest_release: null
  readme: '<h1>

    <a id="user-content-whatshap" class="anchor" href="#whatshap" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>WhatsHap</h1>

    <p>WhatsHap repository for singularity container</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1601728652.0
touala/bonito:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: touala/bonito
  latest_release: null
  readme: '<h1>

    <a id="user-content-nextflow-pipeline-for-10x-snatac-seq-data" class="anchor"
    href="#nextflow-pipeline-for-10x-snatac-seq-data" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>NextFlow pipeline for 10X snATAC-seq data</h1>

    <h2>

    <a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

    <p>If you have Singularity installed, you can use the config provided here (''Singularity'')
    to build a container with all the dependencies.</p>

    <p>Otherwise, you''ll need to have the following installed:</p>

    <ol>

    <li>biopython</li>

    <li>bwa</li>

    <li>picardtools</li>

    <li>fastqc</li>

    <li>samtools</li>

    <li>pysam</li>

    <li>ataqv</li>

    <li>cta (the forked version on the porchard GitHub)</li>

    </ol>

    <p>I''ve used this pipeline with NextFlow v. 19.04.1</p>

    <h2>

    <a id="user-content-configuration" class="anchor" href="#configuration" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h2>

    <p>Paths to various generic files (e.g., bwa indices) must be included in the
    nextflow.config file -- check that file and change paths accordingly. These include:</p>

    <ol>

    <li>Blacklist bed files for each genome</li>

    <li>Chrom size files for each genome</li>

    <li>BWA indices</li>

    <li>TSS files (BED6 files denoting TSS positions)</li>

    <li>Gene bed files (BED4 files; included because we get per-gene read counts for
    use with LIGER in downstream processing). You probably want these to represent
    gene bodies + promoters if you plan to use these with LIGER.</li>

    <li>Path to the barcode whitelist (the 10X whitelist is included in this repo)</li>

    </ol>

    <p>You''ll also need to set the params.results variable -- either in the nextflow.config
    file itself, or on the command line when you run the pipeline (''--results /path/to/results'').</p>

    <p>To reduce memory usage of ataqv, we filter out nuclei with low read counts
    before running ataqv. The minimum read threshold is set in the nextflow.config
    file.</p>

    <p>Lastly, you''ll need to include information about each ATAC-seq library, including
    the genome(s) for the species that each library includes, and the paths to the
    fastq files for each readgroup. Organize this information in a JSON file, as in
    library-config.json. Note that for each readgroup, three fastq files are required
    -- the first and second insert reads (''1'' and ''2''), and the read with the
    nuclear barcode (''index'')</p>

    <h2>

    <a id="user-content-running" class="anchor" href="#running" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running</h2>

    <p>Once you have all of the above information, you can run the pipeline as follows
    (in this case, indicating the path to the results on the command line):</p>

    <div class="highlight highlight-source-shell"><pre>nextflow run -with-singularity
    /path/to/Singularity.simg -params-file library-config.json --results /path/to/results
    /path/to/main.nf</pre></div>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1611292894.0
touala/centos8:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: touala/centos8
  latest_release: null
  readme: "<p>After forking this repository, replace <code>orion_username</code> in\
    \ the file <code>2d_unet_CT_W_PET.json</code> with your actual orion username.</p>\n\
    <h1>\n<a id=\"user-content-setup-in-orion-cluster\" class=\"anchor\" href=\"#setup-in-orion-cluster\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Setup in Orion cluster</h1>\n<p>The <code>$HOME</code> directory of\
    \ your login machine (<code>[username@login ~]</code>) should have the following\
    \ structure</p>\n<pre><code>$HOME\n   \u251C\u2500\u2500 cnn_template (the forked\
    \ repository)\n   \u2502   \u251C\u2500\u2500 config\n   \u2502   |   \u251C\u2500\
    \u2500 2d_unet.json\n   \u2502   |   \u2514\u2500\u2500 (other configurations)\n\
    \   |   \u251C\u2500\u2500 outputs\n   \u2502   |   \u2514\u2500\u2500 README.md\n\
    \   \u2502   \u251C\u2500\u2500 customize_obj.py\n   \u2502   \u251C\u2500\u2500\
    \ experiment.py\n   \u2502   \u251C\u2500\u2500 slurm.sh\n   \u2502   \u251C\u2500\
    \u2500 setup.sh\n   \u2502   \u2514\u2500\u2500 (other files)\n   \u251C\u2500\
    \u2500 datasets (Put your datasets in here)\n   \u2502   \u2514\u2500\u2500 headneck\n\
    \   \u2502       \u251C\u2500\u2500 full_dataset_singleclass.h5\n   \u2502   \
    \    \u2514\u2500\u2500 (other datasets)\n   \u251C\u2500\u2500 hnperf (log files\
    \ will be saved in here)\n   \u2502\n</code></pre>\n<p>Start by running <code>setup.sh</code>\
    \ to download the singularity container</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> cnn-template\n./setup.sh</pre></div>\n<p>Alternative\
    \ you can directly download the image file</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c1\">cd</span> cnn-template\nsingularity pull --name deoxys.sif\
    \ shub://huynhngoc/head-neck-analysis</pre></div>\n<h1>\n<a id=\"user-content-run-experiments-on-orion\"\
    \ class=\"anchor\" href=\"#run-experiments-on-orion\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Run experiments\
    \ on Orion</h1>\n<h2>\n<a id=\"user-content-submit-jobs\" class=\"anchor\" href=\"\
    #submit-jobs\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Submit jobs</h2>\n<p>Submit slurm jobs like this:</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>sbatch slurm.sh config/2d_unet.json\
    \ 2d_unet 200</pre></div>\n<p>Which will load the setup from the <code>config/2d_unet.json</code>\
    \ file, train for 200 epochs\nand store the results in the folder <code>$HOME/hnperf/2d_unet/</code>.</p>\n\
    <p>To customize model and prediction checkpoints, add the <code>model_checkpoint_period</code>\
    \ and <code>prediction_checkpoint_period</code> as arguments</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm.sh config/2d_unet_CT_W_PET.json\
    \ 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period\
    \ 5\n</pre></div>\n<p>Which will save the trained model every 5 epochs and predict\
    \ the validation set every 5 epoch</p>\n<h2>\n<a id=\"user-content-continue-experiments-and-run-test\"\
    \ class=\"anchor\" href=\"#continue-experiments-and-run-test\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Continue\
    \ experiments and run test</h2>\n<p>To continue an experiment</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_cont.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period\
    \ 5</pre></div>\n<p>Which will load the saved model and continue training 100\
    \ more epochs</p>\n<p>In the case the job ended unexpectedly before plotting the\
    \ performance:</p>\n<div class=\"highlight highlight-source-shell\"><pre>sbatch\
    \ slurm_vis.sh 2d_unet_CT_W_PET</pre></div>\n<p>To run test</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_test.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET</pre></div>\n<p>To run external validation</p>\n<div class=\"\
    highlight highlight-source-shell\"><pre>sbatch slurm_external.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5\
    \ 2d_unet_CT_W_PET maastro.json</pre></div>\n<h1>\n<a id=\"user-content-misc\"\
    \ class=\"anchor\" href=\"#misc\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Misc</h1>\n<p>Manually build the singularity\
    \ image file</p>\n<pre><code>singularity build --fakeroot Singularity deoxys.sif\n\
    </code></pre>\n<p>Login to a gpu session to use the gpu</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>qlogin --partition=gpu --gres=gpu:1\nsingularity\
    \ <span class=\"pl-c1\">exec</span> --nv deoxys.sif ipython</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1607663517.0
tyson-swetnam/cc-camp:
  data_format: 2
  description: Container Camp test repo
  filenames:
  - tensorflow/Singularity
  - R/Singularity
  - makeflow/Singularity
  - pdal/Singularity
  full_name: tyson-swetnam/cc-camp
  latest_release: null
  readme: '<h1>

    <a id="user-content-cc-camp" class="anchor" href="#cc-camp" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cc-camp</h1>

    <p>Container Camp test repo</p>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1525835054.0
tyson-swetnam/osgeo-singularity:
  data_format: 2
  description: Singularity Container for running Sol
  filenames:
  - Singularity
  full_name: tyson-swetnam/osgeo-singularity
  latest_release: null
  readme: "<p><a href=\"https://singularity-hub.org/collections/567\" rel=\"nofollow\"\
    ><img src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n<h1>\n<a id=\"user-content-singularity-container-for-running-osgeo-software\"\
    \ class=\"anchor\" href=\"#singularity-container-for-running-osgeo-software\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Singularity container for running OSGEO software</h1>\n<p>Singularity\
    \ Container for running OSGEO (GRASS, GDAL, QGIS, SAGA-GIS) on a virtual machine\
    \ or localhost running Singularity.</p>\n<h2>\n<a id=\"user-content-installing-the-container\"\
    \ class=\"anchor\" href=\"#installing-the-container\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Installing the\
    \ container</h2>\n<p>First, <a href=\"https://singularity.lbl.gov/install-linux\"\
    \ rel=\"nofollow\">install Singularity</a> on your localhost or remote system.</p>\n\
    <p>As of early August 2018, Singularity is version <code>2.6.0</code></p>\n<pre><code>VERSION=2.6.0\n\
    wget https://github.com/singularityware/singularity/releases/download/$VERSION/singularity-$VERSION.tar.gz\n\
    tar xvf singularity-$VERSION.tar.gz\ncd singularity-$VERSION\n./configure --prefix=/usr/local\n\
    make\nsudo make install\ncd ..\nsudo rm -rf singularity-$VERSION.tar.gz\n</code></pre>\n\
    <h2>\n<a id=\"user-content-pull-the-container-from-singularity-hub\" class=\"\
    anchor\" href=\"#pull-the-container-from-singularity-hub\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Pull the\
    \ Container from Singularity Hub</h2>\n<p>The <code>latest</code> image is hosted\
    \ on <a href=\"https://www.singularity-hub.org/collections/567\" rel=\"nofollow\"\
    >Singularity Hub</a>.</p>\n<pre><code>singularity pull --name osgeo.simg shub://tyson-swetnam/osgeo-singularity\n\
    </code></pre>\n<h2>\n<a id=\"user-content-build-locally\" class=\"anchor\" href=\"\
    #build-locally\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon\
    \ octicon-link\"></span></a>Build locally</h2>\n<p>To build locally, pull this\
    \ repository:</p>\n<pre><code>git clone https://github.com/tyson-swetnam/osgeo-singularity\n\
    cd osgeo-singularity\n</code></pre>\n<p>Build the container locally:</p>\n<pre><code>sudo\
    \ singularity build osgeo.simg Singularity\n</code></pre>\n<p><strong>NOTE: The\
    \ Singularity file has some options in the <code>%post</code> section for installing\
    \ NVIDIA drivers and OpenGL - these are currently commented out in the Singularity-Hub\
    \ build.</strong></p>\n<h2>\n<a id=\"user-content-starting-the-container\" class=\"\
    anchor\" href=\"#starting-the-container\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Starting the Container</h2>\n\
    <p>To run the container as a shell:</p>\n<pre><code>singularity shell osgeo.simg\n\
    </code></pre>\n<p>To run the container with a GUI interface for GRASS:</p>\n<pre><code>singularity\
    \ exec osgeo.simg grass74\n</code></pre>\n<p>For QGIS GUI:</p>\n<pre><code>singularity\
    \ exec osgeo.simg qgis\n</code></pre>\n<p>For Saga-GIS GUI:</p>\n<pre><code>singularity\
    \ exec osgeo.simg saga_gui\n</code></pre>\n \n<p>If you are accessing the container\
    \ remotely, make sure to use the <code>ssh -X</code> flag</p>\n \n"
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1552184925.0
tyson-swetnam/rstudio-tensorflow-singularity:
  data_format: 2
  description: Singularity recipe file for running RStudio-Server wiht Tensorflow
  filenames:
  - Singularity
  full_name: tyson-swetnam/rstudio-tensorflow-singularity
  latest_release: null
  readme: '<h1>

    <a id="user-content-rstudio-tensorflow-singularity" class="anchor" href="#rstudio-tensorflow-singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>rstudio-tensorflow-singularity</h1>

    <p>Singularity recipe file for running RStudio-Server wiht Tensorflow</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1554261744.0
uazhlt/pytorch-example:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: uazhlt/pytorch-example
  latest_release: null
  readme: '<h1>

    <a id="user-content-docker" class="anchor" href="#docker" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Docker</h1>

    <h2>

    <a id="user-content-building-the-docker-container" class="anchor" href="#building-the-docker-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    the docker container</h2>

    <p><em>NOTE: This step is not necessary if you simply want to use an already published
    image to run the example code on the UA HPC.</em></p>

    <pre><code>docker build -f Dockerfile -t uazhlt/pytorch-example .

    </code></pre>

    <h2>

    <a id="user-content-verify-pytorch-version" class="anchor" href="#verify-pytorch-version"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Verify
    PyTorch version</h2>

    <pre><code>docker run --rm -it uazhlt/pytorch-example python -c "import torch;
    print(torch.__version__)"

    </code></pre>

    <h2>

    <a id="user-content-publish-to-dockerhub" class="anchor" href="#publish-to-dockerhub"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publish
    to DockerHub</h2>

    <p><em>NOTE: This step is not necessary if you simply want to use an already published
    image to run the example code on the UA HPC.</em></p>

    <pre><code># login to dockerhub registry

    docker login --username=yourdockerhubusername --email=youremail@domain.com


    docker push org/image-name:taghere

    </code></pre>

    <h1>

    <a id="user-content-singularity" class="anchor" href="#singularity" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity</h1>

    <h2>

    <a id="user-content-building-a-singularity-image" class="anchor" href="#building-a-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    a Singularity image</h2>

    <p>Building a Singularity image from a def file requires sudo on a Linux system.  In
    this tutorial, we avoid discussing details on installing Singularity.  If you''re
    feeling adventurous, take a look at <a href="./Singularity">the example def file
    in this repository</a> and the official documentation:</p>

    <ul>

    <li><a href="https://sylabs.io/guides/3.0/user-guide/installation.html" rel="nofollow">https://sylabs.io/guides/3.0/user-guide/installation.html</a></li>

    </ul>

    <h3>

    <a id="user-content-alternatives" class="anchor" href="#alternatives" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Alternatives</h3>

    <h4>

    <a id="user-content-cloud-builds" class="anchor" href="#cloud-builds" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Cloud builds</h4>

    <ul>

    <li>GitHub actions:

    <ul>

    <li><a href="https://github.com/singularityhub/github-ci/blob/master/.github/workflows/go.yml">Example
    GitHub Workflow</a></li>

    <li><a href="https://help.github.com/en/actions/automating-your-workflow-with-github-actions/virtual-environments-for-github-hosted-runners#supported-runners-and-hardware-resources">GitHub-hosted
    runners</a></li>

    </ul>

    </li>

    </ul>

    <h4>

    <a id="user-content-vms" class="anchor" href="#vms" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>VMs</h4>

    <ul>

    <li><a href="https://sylabs.io/guides/3.0/user-guide/installation.html#singularity-vagrant-box"
    rel="nofollow">Vagrant box</a></li>

    </ul>

    <h4>

    <a id="user-content-docker---singularity" class="anchor" href="#docker---singularity"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Docker
    -&gt; Singularity</h4>

    <ul>

    <li><a href="https://github.com/singularityhub/docker2singularity"><code>docker2singularity</code></a></li>

    </ul>

    <h2>

    <a id="user-content-retrieving-a-published-singularity-image" class="anchor" href="#retrieving-a-published-singularity-image"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Retrieving
    a published Singularity image</h2>

    <p>Instead of building from scratch, we''ll focus on a shortcut that simply wraps
    docker images published to DockerHub.</p>

    <pre><code>singularity pull uazhlt-pytorch-example.sif docker://uazhlt/pytorch-example:latest

    </code></pre>

    <h1>

    <a id="user-content-hpc" class="anchor" href="#hpc" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>HPC</h1>

    <p>If you intend to test out <a href="./example">the PyTorch example included
    here</a>, you''ll want to clone this repository:</p>

    <div class="highlight highlight-source-shell"><pre>git clone https://github.com/ua-hlt-program/pytorch-example.git</pre></div>

    <h2>

    <a id="user-content-running-singularity-in-an-interactive-pbs-job" class="anchor"
    href="#running-singularity-in-an-interactive-pbs-job" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Running Singularity
    in an interactive PBS job</h2>

    <p>Next, we''ll request an interactive job (tested on El Gato):</p>

    <div class="highlight highlight-source-shell"><pre>qsub -I \

    -N interactive-gpu \

    -W group_list=mygroupnamehere \

    -q standard \

    -l select=1:ncpus=2:mem=16gb:ngpus=1 \

    -l cput=3:0:0 \

    -l walltime=1:0:0</pre></div>

    <p>_NOTE: If you''re unfamiliar with <code>qsub</code> and the many options in
    the command above seem puzzling, you can find answers by checking out the manual
    via <code>man qsub</code> _</p>

    <p>If the cluster isn''t too busy, you should soon see a new prompt formatted
    something like <code>[netid@gpu\d\d ~]</code>.</p>

    <p>Now we''ll run the singularity image we grabbed earlier.  Before that, though,
    let''s ensure we''re using the correct version of Singularity and that the correct
    CUDA version is available to Singularity:</p>

    <pre><code>module load singularity/3.2.1

    module load cuda10/10.1

    </code></pre>

    <p>Now we''re finally ready to run the container:</p>

    <pre><code>singularity shell --nv --no-home /path/to/your/uazhlt-pytorch-example.sif

    </code></pre>

    <p>If you ran into an error, check to see if you replaced <code>/path/to/your/</code>
    with the correct path to <code>uazhlt-pytorch-example.sif</code> before executing
    the command.</p>

    <p>We''re now in our Singularity container! If everything went well, we should
    be able to see the gpu:</p>

    <pre><code>nvidia-smi

    </code></pre>

    <p>You should see output like the following:</p>

    <pre><code>+-----------------------------------------------------------------------------+

    | NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |

    |-------------------------------+----------------------+----------------------+

    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC
    |

    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M.
    |

    |===============================+======================+======================|

    |   0  Tesla K20Xm         On   | 00000000:8B:00.0 Off |                    0
    |

    | N/A   17C    P8    18W / 235W |      0MiB /  5700MiB |      0%      Default
    |

    +-------------------------------+----------------------+----------------------+


    +-----------------------------------------------------------------------------+

    | Processes:                                                       GPU Memory
    |

    |  GPU       PID   Type   Process name                             Usage      |

    |=============================================================================|

    |  No running processes found                                                 |

    +-----------------------------------------------------------------------------+

    </code></pre>

    <p>Success (I hope)!  Now let''s try running PyTorch on the GPU with batching...</p>

    <h1>

    <a id="user-content-pytorch-example" class="anchor" href="#pytorch-example" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>PyTorch example</h1>

    <p>The Pytorch example code can be found under <a href="./example"><code>example</code></a>.  The
    data used in this example comes from from Delip Rao and Brian MacMahan''s <em>Natural
    Language Processing with PyTorch</em>:</p>

    <ul>

    <li><a href="https://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames">https://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames</a></li>

    </ul>

    <p>The dataset relates surnames to nationalities.  Our version (minor modifications)
    is nested under <a href="./examples/data">examples/data</a>.</p>

    <p><code>train.py</code> houses a command line program for training a classifier.  The
    following invocation will display the tool''s help text:</p>

    <pre><code>python train.py --help

    </code></pre>

    <p>The simple model architecture operates is based on that of deep averaging networks
    (DANs; see <a href="https://aclweb.org/anthology/P15-1162/" rel="nofollow">https://aclweb.org/anthology/P15-1162/</a>).</p>

    <p>Reading through train.py you can quickly see how the code is organized.  Some
    parts (ex. <code>torchtext</code> data loaders) may be unfamiliar to you.</p>

    <h1>

    <a id="user-content-next-steps" class="anchor" href="#next-steps" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Next steps</h1>

    <p>Now that you''ve managed to run some example PyTorch code, there are many paths
    forward:</p>

    <ul>

    <li>Experiment with using pretrained subword embeddings (both fixed and trainable).  Do
    you notice any improvements in performance/faster convergence?</li>

    <li>Try improving or replacing the naive model defined under <code>models.py</code>.</li>

    <li>Add an evaluation script for a trained model that reports macro P, R, and
    F1.  Feel free to use <code>scikit-learn</code>''s classification report.</li>

    <li>Add an inference script to classify new examples.</li>

    <li>Monitor validation loss to and stop training if you begin to overfit.</li>

    <li>Adapt the interactive PBS task outlined above to a PBS script that you can
    submit to the HPC.</li>

    <li>Address the class imbalance in the data through downsampling, class weighting,
    or another technique of your choosing.</li>

    </ul>

    '
  stargazers_count: 1
  subscribers_count: 1
  topics: []
  updated_at: 1576591065.0
ucr-singularity/accelerator-project:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ucr-singularity/accelerator-project
  latest_release: null
  readme: '<h1>

    <a id="user-content-accelerator-project" class="anchor" href="#accelerator-project"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>accelerator-project</h1>

    <p>Builds at <a href="https://www.singularity-hub.org/collections/1897" rel="nofollow">https://www.singularity-hub.org/collections/1897</a>.</p>

    <p>Download with</p>

    <p><code>singularity pull shub://ucr-singularity/accelerator-project</code></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1542422144.0
ucr-singularity/asterixdb:
  data_format: 2
  description: AsterixDB container
  filenames:
  - Singularity
  full_name: ucr-singularity/asterixdb
  latest_release: null
  readme: '<h1>

    <a id="user-content-asterixdb" class="anchor" href="#asterixdb" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>asterixdb</h1>

    <p>AsterixDB container</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1535433043.0
ucr-singularity/caffe-gpu:
  data_format: 2
  description: DEPRECATED - Singularity recipe for docker://bvlc/caffe:gpu
  filenames:
  - Singularity
  full_name: ucr-singularity/caffe-gpu
  latest_release: null
  readme: '<h1>

    <a id="user-content-caffe-gpu" class="anchor" href="#caffe-gpu" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>caffe-gpu</h1>

    <p>THIS REPOSITORY IS DEPRECATED. There is no current replacement within ucr-singularity.</p>

    <p>Singularity recipe for docker://bvlc/caffe:gpu</p>

    <p>Adds screen, tmux, vim, xterm</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1557314492.0
ucr-singularity/cs009-p:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ucr-singularity/cs009-p
  latest_release: null
  readme: '<h1>

    <a id="user-content-cs009-p" class="anchor" href="#cs009-p" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cs009-p</h1>

    <p>Singularity recipe for CS 009P.  Singularity Hub collection at <a href="https://www.singularity-hub.org/collections/2064"
    rel="nofollow">https://www.singularity-hub.org/collections/2064</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1546592552.0
ucr-singularity/cs100:
  data_format: 2
  description: Singularity recipe for CS 100
  filenames:
  - Singularity
  full_name: ucr-singularity/cs100
  latest_release: null
  readme: '<h1>

    <a id="user-content-cs100" class="anchor" href="#cs100" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cs100</h1>

    <p>Singularity recipe for CS 100</p>

    <p>Available on Singularity Hub at <a href="https://www.singularity-hub.org/collections/1789"
    rel="nofollow">https://www.singularity-hub.org/collections/1789</a></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1539244991.0
ucr-singularity/cs171:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ucr-singularity/cs171
  latest_release: null
  readme: '<h1>

    <a id="user-content-cs171" class="anchor" href="#cs171" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cs171</h1>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1543654662.0
ucr-singularity/cs172:
  data_format: 2
  description: CS 172
  filenames:
  - Singularity
  full_name: ucr-singularity/cs172
  latest_release: null
  readme: '<h1>

    <a id="user-content-cs172" class="anchor" href="#cs172" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cs172</h1>

    <p>CS 172 Singularity recipe. A SingularityHub collection is available at:</p>

    <p><a href="https://www.singularity-hub.org/collections/1982" rel="nofollow">https://www.singularity-hub.org/collections/1982</a></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1543918836.0
ucr-singularity/cs181:
  data_format: 2
  description: CS 181
  filenames:
  - Singularity
  full_name: ucr-singularity/cs181
  latest_release: null
  readme: '<h1>

    <a id="user-content-cs181" class="anchor" href="#cs181" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cs181</h1>

    <p>CS 181 Singularity recipe.  A SingularityHub collection is available at:</p>

    <p><a href="https://www.singularity-hub.org/collections/1983" rel="nofollow">https://www.singularity-hub.org/collections/1983</a></p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1550211352.0
ucr-singularity/cuda-10.1-base:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ucr-singularity/cuda-10.1-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-101-base" class="anchor" href="#cuda-101-base" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cuda-10.1-base</h1>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1557313283.0
ucr-singularity/cuda-10.1-ml-software:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ucr-singularity/cuda-10.1-ml-software
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-101-ml-software" class="anchor" href="#cuda-101-ml-software"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cuda-10.1-ml-software</h1>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1557451899.0
ucr-singularity/cuda-10.2-cudnn7-base:
  data_format: 2
  description: CUDA 10.2 base image with cuDNN 7
  filenames:
  - Singularity
  full_name: ucr-singularity/cuda-10.2-cudnn7-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-102-cudnn7-base" class="anchor" href="#cuda-102-cudnn7-base"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>cuda-10.2-cudnn7-base</h1>

    <p>CUDA 10.2 base image with cuDNN 7</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1591940186.0
ucr-singularity/cuda-7.0-cudnn4:
  data_format: 2
  description: CUDA 7.0 and CUDNN for Ubuntu 14.04
  filenames:
  - Singularity
  full_name: ucr-singularity/cuda-7.0-cudnn4
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-70-cudnn4" class="anchor" href="#cuda-70-cudnn4" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cuda-7.0-cudnn4</h1>

    <p>CUDA 7.0 and CUDNN for Ubuntu 14.04</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1527065438.0
ucr-singularity/cuda-7.5-cudnn5:
  data_format: 2
  description: Singularty recipe for Ubuntu 14.04 Singularity image with Cuda 7.5
    and Cudnn5
  filenames:
  - Singularity
  full_name: ucr-singularity/cuda-7.5-cudnn5
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-75-cudnn5" class="anchor" href="#cuda-75-cudnn5" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cuda-7.5-cudnn5</h1>

    <p>Singularty recipe for Ubuntu 14.04 Singularity image with Cuda 7.5 and Cudnn5</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1544755927.0
ucr-singularity/cuda-9.0-base:
  data_format: 2
  description: DEPRECATED - CUDA 9, CUDNN 7, Ubuntu 16.04 Singularity recipe with
    dependencies for much ML software installed.
  filenames:
  - Singularity
  - Singularity.test
  full_name: ucr-singularity/cuda-9.0-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-90-base" class="anchor" href="#cuda-90-base" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cuda-9.0-base</h1>

    <p>THIS REPOSITORY IS DEPRECATED. Please use <a href="https://github.com/ucr-singularity/cuda-10.1-base">https://github.com/ucr-singularity/cuda-10.1-base</a>
    instead.</p>

    <p>CUDA 9, CUDNN 7, Ubuntu 16.04 Singularity recipe with dependencies for ML

    software installed. Intended to be used as a base image for other images.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1557314427.0
ucr-singularity/cuda-9.2-base:
  data_format: 2
  description: CUDA 9.2 Singularity recipe for a machine learning environment.
  filenames:
  - Singularity
  full_name: ucr-singularity/cuda-9.2-base
  latest_release: null
  readme: '<h1>

    <a id="user-content-cuda-92-base" class="anchor" href="#cuda-92-base" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>cuda-9.2-base</h1>

    <p>CUDA 9.2 Singularity recipe for a machine learning environment.</p>

    <p>Builds available at <a href="https://www.singularity-hub.org/collections/1971"
    rel="nofollow">https://www.singularity-hub.org/collections/1971</a>.</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1543808072.0
ucr-singularity/faces-project:
  data_format: 2
  description: Singularity recipe for the Faces project.
  filenames:
  - Singularity
  full_name: ucr-singularity/faces-project
  latest_release: null
  readme: '<h1>

    <a id="user-content-faces-project" class="anchor" href="#faces-project" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>faces-project</h1>

    <p>Singularity recipe for the Faces project</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1522033344.0
ucr-singularity/hadoop:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: ucr-singularity/hadoop
  latest_release: null
  readme: '<h1>

    <a id="user-content-hadoop" class="anchor" href="#hadoop" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>hadoop</h1>

    <p>Available on Singularity Hub at <a href="https://www.singularity-hub.org/collections/1636"
    rel="nofollow">https://www.singularity-hub.org/collections/1636</a>.</p>

    <p>A Hadoop, Spark, and Cassandra Singularity recipe. This is not completely self

    contained - the configurations for each of the services are not included

    in the repo. The cluster is designed to have four nodes:</p>

    <ul>

    <li>One node runs a Hadoop primary namenode and resourcemanager. There''s no

    secondary namenode.</li>

    <li>The other 3 nodes run hadoop nodemanager and HDFS.  They also run Cassandra.</li>

    </ul>

    <p>Spark is configured to use Hadoop for running jobs.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1569898140.0
ucr-singularity/ml-anaconda-3:
  data_format: 2
  description: Singularity recipe for ML software using Anaconda 3.
  filenames:
  - Singularity
  full_name: ucr-singularity/ml-anaconda-3
  latest_release: null
  readme: '<h1>

    <a id="user-content-ml-anaconda-3" class="anchor" href="#ml-anaconda-3" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>ml-anaconda-3</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1522996115.0
ucr-singularity/xeus-cling:
  data_format: 2
  description: Singularity recipe for Xeus-Cling, to be run with JupyterHub
  filenames:
  - Singularity
  full_name: ucr-singularity/xeus-cling
  latest_release: null
  readme: '<h1>

    <a id="user-content-xeus-cling" class="anchor" href="#xeus-cling" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>xeus-cling</h1>

    <p>Singularity recipe for Xeus-Cling, to be run with JupyterHub</p>

    '
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1534766438.0
ulorentz/singularity_images:
  data_format: 2
  description: Images to be run on Wilson Cluster at FNAL
  filenames:
  - Singularity.py3_tf114
  full_name: ulorentz/singularity_images
  latest_release: null
  readme: '<h1>

    <a id="user-content-a-singularity-container-for-installing-manta" class="anchor"
    href="#a-singularity-container-for-installing-manta" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>A singularity container
    for installing manta</h1>

    <h2>

    <a id="user-content-build-with" class="anchor" href="#build-with" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Build with:</h2>

    <p>sudo singularity build manta.simg Singularity</p>

    <ul>

    <li>Build files are found in ./build</li>

    <li>Install files are found in ./install</li>

    </ul>

    '
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1586556737.0
ulorentz/wilson_cluster:
  data_format: 2
  description: null
  filenames:
  - Singularity.pytorch
  full_name: ulorentz/wilson_cluster
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4791" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-using-container-based-solutions-on-c3se-clusters" class="anchor"
    href="#using-container-based-solutions-on-c3se-clusters" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Using container-based
    solutions on C3SE clusters</h1>

    <p>Container technology has a number of advantages over the traditional workflow
    of using scientific software. The singularity flavour, in particular, targets
    reproducibility, performance and security with respect to running software in
    an HPC environment. Here, we provide our containerized solutions at C3SE. For
    each of the provided containers, read the specific instructions in the corresponding
    folder to easily get started with using them in your workflow. The actual container
    images are hosted on Singularity Hub. Click on the badge above to quickly get
    access to them!</p>

    <h2>

    <a id="user-content-missing-containers-updates-and-troubleshooting" class="anchor"
    href="#missing-containers-updates-and-troubleshooting" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Missing containers,
    updates, and troubleshooting</h2>

    <p>We continuously add more packages to this repository. If you can''t find a
    relevant container for your needs, or in the case of encountering any errors or
    deprecated features in the material, feel free to contact us: <a href="mailto:support@c3se.chalmers.se">support@c3se.chalmers.se</a>
    or open a pull-request.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1586557138.0
vibaotram/singularity-container:
  data_format: 2
  description: null
  filenames:
  - Singularity.guppy4.5.4gpu-conda-api
  - Singularity.guppy3.6.0gpu-conda-api
  - Singularity.guppy3.6.0cpu-conda-api
  - Singularity.guppy-cpu-conda
  - Singularity.myR_4-0-2_rstudio_1.3
  - Singularity.guppy3.4gpu-conda-api
  - Singularity.cpu-guppy3.4-conda-api
  - Singularity.deepbinner-api
  - Singularity.myR_3-6-3
  - Singularity.guppy4.2.2gpu-conda-api
  - Singularity.guppy4.0.14gpu-conda-api
  full_name: vibaotram/singularity-container
  latest_release: null
  readme: '<p><a href="https://singularity-hub.org/collections/4054" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></p>

    <h1>

    <a id="user-content-singularity-container" class="anchor" href="#singularity-container"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-container</h1>

    <h2>

    <a id="user-content-singularity-images-supporting-basedmux-workflow" class="anchor"
    href="#singularity-images-supporting-basedmux-workflow" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Singularity images
    supporting <a href="https://github.com/vibaotram/baseDmux.git">baseDmux workflow</a>

    </h2>

    <p><strong>Singularity.guppy-cpu-conda</strong></p>

    <p>containing GUPPY version 3.4 CPU, Miniconda3</p>

    <pre><code>shub://vibaotram/singularity-container:guppy-cpu-conda

    </code></pre>

    <p><strong>Singularity.cpu-guppy3.4-conda-api</strong></p>

    <p>containing GUPPY version 3.4 CPU, Miniconda3, ONT_FAST5_API</p>

    <pre><code>shub://vibaotram/singularity-container:cpu-guppy3.4-conda-api

    </code></pre>

    <p><strong>Singularity.guppy3.4gpu-conda-api</strong></p>

    <p>containing GUPPY version 3.4 GPU, Miniconda3, ONT_FAST5_API</p>

    <pre><code>shub://vibaotram/singularity-container:guppy3.4gpu-conda-api

    </code></pre>

    <p><strong>Singularity.deepbinner-api</strong></p>

    <p>containing deepbinner 2.0.0, ONT_FAST5_API, python3</p>

    <pre><code>shub://vibaotram/singularity-container:deepbinner-api

    </code></pre>

    '
  stargazers_count: 1
  subscribers_count: 3
  topics:
  - singularity
  updated_at: 1621400501.0
waldronlab/curatedmetagenomics:
  data_format: 2
  description: null
  filenames:
  - docker/curatedmetagenomics/Singularity
  - docker/sratoolkit/Singularity
  full_name: waldronlab/curatedmetagenomics
  latest_release: null
  readme: '<h1>

    <a id="user-content-project-management" class="anchor" href="#project-management"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project
    management</h1>

    <p>See <a href="https://app.zenhub.com/workspaces/cmd-project-management-5e3d745411e3ced1cfa8fbe9/board?repos=116720695,58228080,95220777,250843441"
    rel="nofollow">zenhub project management</a> across this and related repos.</p>

    <h1>

    <a id="user-content-pipeline-project-overview" class="anchor" href="#pipeline-project-overview"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pipeline
    project overview</h1>

    <p>Here is a high-level <a href="https://www.dropbox.com/s/tawgf4l49190m4o/2020-05-20%20intro%20to%20NCI%201U01%20CA230551%20.pptx?dl=0"
    rel="nofollow">slide deck</a></p>

    <p>See the <a href="https://github.com/waldronlab/curatedmetagenomics/wiki/Environment-variables-and-invocation">wiki</a>
    for full setup and execution instructions.</p>

    <h1>

    <a id="user-content-what-is-here" class="anchor" href="#what-is-here" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>What is here</h1>

    <h2>

    <a id="user-content-metaphlan3--humann3--strainphlan--sratoolkit-dockersingularity-images"
    class="anchor" href="#metaphlan3--humann3--strainphlan--sratoolkit-dockersingularity-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MetaPhlan3
    + HUMAnN3 + StrainPhlAn + sratoolkit Docker+Singularity images</h2>

    <table>

    <thead>

    <tr>

    <th>DockerHub</th>

    <th>SingularityHub</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><a href="https://hub.docker.com/repository/docker/waldronlab/curatedmetagenomics"
    rel="nofollow"><img src="https://camo.githubusercontent.com/acf7b0916598fbca21cbe3bb57b510560a69a4bd2a8446b29408b42807a1e1c6/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f77616c64726f6e6c61622f637572617465646d65746167656e6f6d6963732e737667"
    alt="" data-canonical-src="https://images.microbadger.com/badges/version/waldronlab/curatedmetagenomics.svg"
    style="max-width:100%;"></a></td>

    <td><a href="https://singularity-hub.org/collections/4365" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></td>

    </tr>

    </tbody>

    </table>

    <p>(see <a href="https://github.com/waldronlab/curatedmetagenomics/tree/master/docker/curatedMetagenomics">docker
    directory</a> for Dockerfile and link to Dockerhub).

    - Instructions to run are kept in a <a href="https://github.com/waldronlab/curatedmetagenomics/wiki/Environment-variables-and-invocation">wiki</a>.

    - There is still work to be done turning bash scripts into a Python package with
    improved documentation, arguments, and versioning.</p>

    <h2>

    <a id="user-content-sratoolkit-only-dockersingularity-images" class="anchor" href="#sratoolkit-only-dockersingularity-images"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>sratoolkit-only
    Docker+Singularity images</h2>

    <table>

    <thead>

    <tr>

    <th>DockerHub</th>

    <th>SingularityHub</th>

    </tr>

    </thead>

    <tbody>

    <tr>

    <td><a href="https://hub.docker.com/repository/docker/waldronlab/sratoolkit" rel="nofollow"><img
    src="https://camo.githubusercontent.com/fff4c11c8d2a193b6723a39156009275f1a05d2daefdee785cd6ea8691c0a58f/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f77616c64726f6e6c61622f737261746f6f6c6b69742e737667"
    alt="" data-canonical-src="https://images.microbadger.com/badges/version/waldronlab/sratoolkit.svg"
    style="max-width:100%;"></a></td>

    <td><a href="https://singularity-hub.org/collections/4458" rel="nofollow"><img
    src="https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667"
    alt="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    data-canonical-src="https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg"
    style="max-width:100%;"></a></td>

    </tr>

    </tbody>

    </table>

    <p>(see <a href="https://github.com/waldronlab/curatedmetagenomics/tree/master/docker/sratoolkit">docker
    directory</a>, but this is also included in the above all-in-one image.</p>

    <h2>

    <a id="user-content-a-pypi-project-for-the-curatedmetagenomics-pipeline" class="anchor"
    href="#a-pypi-project-for-the-curatedmetagenomics-pipeline" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>a PyPi project for
    the curatedmetagenomics pipeline</h2>

    <p>See the <a href="https://github.com/waldronlab/curatedmetagenomics/tree/master/python_pipeline">python_pipeline</a>
    directory</p>

    '
  stargazers_count: 1
  subscribers_count: 11
  topics: []
  updated_at: 1593557598.0
wheaton5/souporcell:
  data_format: 2
  description: Clustering scRNAseq by genotypes
  filenames:
  - Singularity
  full_name: wheaton5/souporcell
  latest_release: '2.0'
  readme: "<h1>\n<a id=\"user-content-souporcell\" class=\"anchor\" href=\"#souporcell\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>souporcell</h1>\n<p><a href=\"https://github.com/wheaton5/souporcell/blob/master/souporcell_star.png\"\
    \ target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://github.com/wheaton5/souporcell/raw/master/souporcell_star.png\"\
    \ width=\"100\" style=\"max-width:100%;\"></a></p>\n<p>Preprint manuscript of\
    \ this method available at <a href=\"https://www.biorxiv.org/content/10.1101/699637v1\"\
    \ rel=\"nofollow\">https://www.biorxiv.org/content/10.1101/699637v1</a></p>\n\
    <p>souporcell is a method for clustering mixed-genotype scRNAseq experiments by\
    \ individual.</p>\n<p>The inputs are just the possorted_genome_bam.bam, and barcodes.tsv\
    \ as output from <a href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger\"\
    \ rel=\"nofollow\">cellranger</a>.\nsouporcell is comprised of 6 steps with the\
    \ first 3 using external tools and the final using the code provided here.</p>\n\
    <ol>\n<li>Remapping (<a href=\"https://github.com/lh3/minimap2\">minimap2</a>)</li>\n\
    <li>Calling candidate variants (<a href=\"https://github.com/ekg/freebayes\">freebayes</a>)</li>\n\
    <li>Cell allele counting (<a href=\"https://github.com/10XGenomics/vartrix\">vartrix</a>)</li>\n\
    <li>Clustering cells by genotype (souporcell.py)</li>\n<li>Calling doublets (troublet)</li>\n\
    <li>Calling cluster genotypes and inferring amount of ambient RNA (consensus.py)</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-easy-installation-linux-recommended\" class=\"\
    anchor\" href=\"#easy-installation-linux-recommended\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Easy Installation\
    \ (Linux) (recommended)</h2>\n<p>Download singularity image (1.3gb) (singularity\
    \ is similar to docker but safe for clusters)</p>\n<pre><code>singularity pull\
    \ shub://wheaton5/souporcell\n</code></pre>\n<p>If you are running on a scientific\
    \ cluster, they will likely have singularity, contact your sysadmin for more details.\n\
    If you are running on your own linux box you may need to install <a href=\"https://www.sylabs.io/guides/3.2/user-guide/quick_start.html#quick-installation-steps\"\
    \ rel=\"nofollow\">singularity</a></p>\n<p>requires singularity &gt;= 3.0</p>\n\
    <pre><code>which singularity\nsingularity --version\n</code></pre>\n<p>You should\
    \ now be able to run souporcell_pipeline.py through the singularity container.\
    \ Singularity automatically mounts the current working directory and directories\
    \ downstream from where you run it, otherwise you would need to manually mount\
    \ those directories. Therefor you can run it from a directory that is upstream\
    \ of all of the inputs. Input files are the cellranger bam, cellranger barcodes\
    \ file, and a reference fasta. The cellranger bam is located in the cellranger\
    \ outs directory and is called possorted_genome_bam.bam. The barcodes file is\
    \ located in the cellranger outs/filtered_gene_bc_matrices/&lt;ref_name&gt;/barcodes.tsv.\
    \ The reference fasta should be of the same species but does not necessarily need\
    \ to be the exact cellranger reference.</p>\n<p>The options for using souporcell\
    \ are:</p>\n<pre><code>singularity exec souporcell_latest.sif souporcell_pipeline.py\
    \ -h\nusage: souporcell_pipeline.py [-h] -i BAM -b BARCODES -f FASTA -t THREADS\
    \ -o\n                              OUT_DIR -k CLUSTERS [-p PLOIDY]\n        \
    \                      [--min_alt MIN_ALT] [--min_ref MIN_REF]\n             \
    \                 [--max_loci MAX_LOCI] [--restarts RESTARTS]\n              \
    \                [--common_variants COMMON_VARIANTS]\n                       \
    \       [--known_genotypes KNOWN_GENOTYPES]\n                              [--known_genotypes_sample_names\
    \ KNOWN_GENOTYPES_SAMPLE_NAMES [KNOWN_GENOTYPES_SAMPLE_NAMES ...]]\n         \
    \                     [--skip_remap SKIP_REMAP] [--ignore IGNORE]\n\nsingle cell\
    \ RNAseq mixed genotype clustering using sparse mixture model\nclustering with\
    \ tensorflow.\n\noptional arguments:\n  -h, --help            show this help message\
    \ and exit\n  -i BAM, --bam BAM     cellranger bam\n  -b BARCODES, --barcodes\
    \ BARCODES\n                        barcodes.tsv from cellranger\n  -f FASTA,\
    \ --fasta FASTA\n                        reference fasta file\n  -t THREADS, --threads\
    \ THREADS\n                        max threads to use\n  -o OUT_DIR, --out_dir\
    \ OUT_DIR\n                        name of directory to place souporcell files\n\
    \  -k CLUSTERS, --clusters CLUSTERS\n                        number cluster, tbd\
    \ add easy way to run on a range of\n                        k\n  -p PLOIDY, --ploidy\
    \ PLOIDY\n                        ploidy, must be 1 or 2, default = 2\n  --min_alt\
    \ MIN_ALT     min alt to use locus, default = 10.\n  --min_ref MIN_REF     min\
    \ ref to use locus, default = 10.\n  --max_loci MAX_LOCI   max loci per cell,\
    \ affects speed, default = 2048.\n  --restarts RESTARTS   number of restarts in\
    \ clustering, when there are &gt; 12\n                        clusters we recommend\
    \ increasing this to avoid local\n                        minima\n           \
    \              --common_variants COMMON_VARIANTS\n                        common\
    \ variant loci or known variant loci vcf, must be\n                        vs\
    \ same reference fasta\n  --known_genotypes KNOWN_GENOTYPES\n                \
    \        known variants per clone in population vcf mode, must\n             \
    \           be .vcf right now we dont accept gzip or bcf sorry\n  --known_genotypes_sample_names\
    \ KNOWN_GENOTYPES_SAMPLE_NAMES [KNOWN_GENOTYPES_SAMPLE_NAMES ...]\n          \
    \              which samples in population vcf from known genotypes\n        \
    \                option represent the donors in your sample\n  --skip_remap SKIP_REMAP\n\
    \                        don't remap with minimap2 (not recommended unless in\n\
    \                        conjunction with --common_variants\n  --ignore IGNORE\
    \       set to True to ignore data error assertions\n</code></pre>\n<p>A typical\
    \ command looks like</p>\n<pre><code>singularity exec /path/to/souporcell_latest.sif\
    \ souporcell_pipeline.py -i /path/to/possorted_genome_bam.bam -b /path/to/barcodes.tsv\
    \ -f /path/to/reference.fasta -t num_threads_to_use -o output_dir_name -k num_clusters\n\
    </code></pre>\n<p>The above command will run all six steps of the pipeline and\
    \ it will require up to 24gb of ram for human (minimap2 bam index is high water\
    \ mark for memory). For smaller genomes, fewer clusters, lower --max-loci will\
    \ require less memory. Note that souporcell will require roughly 2x the amount\
    \ of diskspace that the input bam file takes up. This dataset should take several\
    \ hours to run on 8 threads mostly due to read processing, remapping, and variant\
    \ calling.</p>\n<p>If you have a common snps file you may want to use the --common_variants\
    \ option with or without the --skip_remap option. This option will skip conversion\
    \ to fastq, remapping with minimap2, and reattaching barcodes, and the --common_variants\
    \ will remove the freebayes step. Each which will save a significant amount of\
    \ time, but --skip-remap isn't recommended without --common_variants.</p>\n<p>Common\
    \ variant files from 1k genomes filtered to variants &gt;= 2% allele frequency\
    \ in the population and limited to SNPs can be found here for GRCh38</p>\n<pre><code>wget\
    \ --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&amp;confirm=$(wget\
    \ --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate\
    \ 'https://docs.google.com/uc?export=download&amp;id=13aebUpEKrtjliyT9rYzRijtkNJVUk5F_'\
    \ -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&amp;id=13aebUpEKrtjliyT9rYzRijtkNJVUk5F_\"\
    \ -O common_variants_grch38.vcf &amp;&amp; rm -rf /tmp/cookies.txt\n</code></pre>\n\
    <p>or for hg19 here</p>\n<pre><code>wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&amp;confirm=$(wget\
    \ --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate\
    \ 'https://docs.google.com/uc?export=download&amp;id=1lw4T6d7uXsm9dt39ZtEwpuB2VTY3wK1y'\
    \ -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&amp;id=1lw4T6d7uXsm9dt39ZtEwpuB2VTY3wK1y\"\
    \ -O common_variants_hg19.vcf &amp;&amp; rm -rf /tmp/cookies.txt\n</code></pre>\n\
    <h2>\n<a id=\"user-content-practicetesting-data-set-demuxlet-paper-data\" class=\"\
    anchor\" href=\"#practicetesting-data-set-demuxlet-paper-data\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Practice/Testing\
    \ data set: Demuxlet paper data</h2>\n<pre><code>wget https://sra-pub-src-1.s3.amazonaws.com/SRR5398235/A.merged.bam.1\
    \ -O A.merged.bam\nwget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2560nnn/GSM2560245/suppl/GSM2560245_barcodes.tsv.gz\n\
    gunzip GSM2560245_barcodes.tsv.gz\n</code></pre>\n<p>And if you don't have a human\
    \ reference sitting around, grab one here</p>\n<pre><code>wget http://cf.10xgenomics.com/supp/cell-exp/refdata-cellranger-GRCh38-3.0.0.tar.gz\n\
    tar -xzvf refdata-cellranger-GRCh38-3.0.0.tar.gz\n</code></pre>\n<p>Now you should\
    \ be ready to test it out</p>\n<pre><code>singularity exec /path/to/souporcell_latest.sif\
    \ souporcell_pipeline.py -i A.merged.bam -b GSM2560245_barcodes.tsv -f refdata-cellranger-GRCh38-3.0.0/fasta/genome.fa\
    \ -t 8 -o demux_data_test -k 4\n</code></pre>\n<p>This should require about 20gb\
    \ of ram mostly because of the minimap2 indexing step. I might soon host an index\
    \ and reference for human to make this less painful.</p>\n<p>The important files\
    \ are</p>\n<ol>\n<li>clusters.tsv</li>\n<li>cluster_genotypes.vcf</li>\n<li>ambient_rna.txt</li>\n\
    </ol>\n<p>clusters.tsv will look like</p>\n<pre><code>barcode status  assignment\
    \      log_loss_singleton      log_loss_doublet        cluster0        cluster1\n\
    AAACCTGAGATCCGAG-1      singlet 0       -152.7778890920112      -190.5463095948822\
    \      -43.95302689281067      -101.63377524087669\nAAACCTGAGCACCGTC-1      singlet\
    \ 0       -78.56014177554212      -96.66255440088581      -20.949294849836267\
    \     -52.57478083591962\nAAACCTGAGTACGATA-1      singlet 0       -216.0188863327174\
    \      -281.3888392065457      -63.059016939362536     -159.5450834682198\nAAACCTGGTACATGTC-1\
    \      singlet 1       -47.189434469216565     -96.30865717225866      -62.652900832546955\
    \     -15.284168900754413\nAAACCTGTCTACTCAT-1      singlet 0       -129.30104434183454\
    \     -167.87811467946756     -41.09158213888751      -106.3201962010145\nAAACCTGTCTTGTCAT-1\
    \      singlet 0       -85.99781433701455      -110.81701038967158     -24.518165091815554\
    \     -60.05279033826837\nAAACGGGCACTGTTAG-1      singlet 0       -154.26595878718032\
    \     -191.05465308213363     -31.356408693487197     -81.61186496254497\nAAACGGGCATCATCCC-1\
    \      singlet 1       -46.33205678267174      -80.24152434540565      -50.78221280006256\
    \      -14.615983876840312\nAAACGGGGTAGGGTAC-1      singlet 0       -240.5237900569412\
    \      -302.91575436035924     -71.79370547349878      -154.08594135029728\nAAACGGGTCGGCATCG-1\
    \      singlet 0       -166.66827966974532     -226.56795157885028     -51.08790637893961\
    \      -148.04625123166286\n</code></pre>\n<p>With the cell barcode, singlet/doublet\
    \ status, cluster, log_loss_singleton, log_loss_doublet, followed by log loss\
    \ for each cluster.</p>\n<ol start=\"2\">\n<li>cluster_genotypes.vcf is a vcf\
    \ with genotypes for each cluster for each variant in the input vcf from freebayes</li>\n\
    </ol>\n<p>and</p>\n<ol start=\"3\">\n<li>ambient_rna.txt just contains the ambient\
    \ RNA percentage detected</li>\n</ol>\n<h2>\n<a id=\"user-content-hard-install\"\
    \ class=\"anchor\" href=\"#hard-install\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Hard install</h2>\n<p>Instead\
    \ of using singularity you can install everything independently (not recommended,\
    \ but shouldn't be too bad)</p>\n<pre><code>git clone https://github.com/wheaton5/souporcell.git\n\
    </code></pre>\n<p>put souporcell directory on your PATH\nrequires samtools, bcftools,\
    \ htslib, python3, freebayes, vartrix, minimap2 all on your PATH\nI suggest you\
    \ use the conda env I have set up by using the following command if you have conda\
    \ or miniconda</p>\n<pre><code>conda env create -f /path/to/souporcell/souporcell_env.yaml\n\
    conda activate souporcell\n</code></pre>\n<p>You will also need Rust and to compile\
    \ the two rust binaries</p>\n<pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs\
    \ | sh\ncd /path/to/souporcell/souporcell &amp;&amp; cargo build --release\ncd\
    \ /path/to/souporcell/troublet &amp;&amp; cargo build --release\n</code></pre>\n\
    <p>otherwise python packages tensorflow, pyvcf, pystan, pyfaidx, numpy, scipy\
    \ are required, but as the versions change, I do recommend using the presetup\
    \ env.</p>\n<h2>\n<a id=\"user-content-to-run-through-the-pipeline-script\" class=\"\
    anchor\" href=\"#to-run-through-the-pipeline-script\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>To run through\
    \ the pipeline script</h2>\n<pre><code>souporcell_pipeline.py -i /path/to/possorted_genome_bam.bam\
    \ -b /path/to/barcodes.tsv -f /path/to/reference.fasta -t num_threads_to_use -o\
    \ output_dir_name -k num_clusters\n</code></pre>\n<h2>\n<a id=\"user-content-to-run-things-step-by-step-not-through-the-pipeline-script\"\
    \ class=\"anchor\" href=\"#to-run-things-step-by-step-not-through-the-pipeline-script\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>To run things step by step not through the pipeline script</h2>\n\
    <h3>\n<a id=\"user-content-1-remapping\" class=\"anchor\" href=\"#1-remapping\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>1. Remapping</h3>\n<p>We discuss the need for remapping in our manuscript.\
    \ We need to keep track of cell barcodes and and UMIs, so we first create a fastq\
    \ with those items encoded in the readname.\nRequires python 3.0, modules pysam,\
    \ argparse (pip install/conda install depending on environment)\nEasiest to first\
    \ add the souporcell directory to your PATH variable with</p>\n<pre><code>export\
    \ PATH=/path/to/souporcell:$PATH\n</code></pre>\n<p>Then run the renamer.py script\
    \ to put some of the quality information in the read name. For human data this\
    \ step will typically take several hours and the output fq file will be somewhat\
    \ larger than the input bam</p>\n<pre><code>python renamer.py --bam possorted_genome_bam.bam\
    \ --barcodes barcodes.tsv --out fq.fq\n</code></pre>\n<p>Then we must remap these\
    \ reads using minimap2 (similar results have been seen with hisat2)\nRequires\
    \ <a href=\"https://github.com/lh3/minimap2\">minimap2</a>\nand add /path/to/minimap2\
    \ to your PATH. For human data the remapping will typically require more than\
    \ 12 Gb memory and may take a few hours to run.</p>\n<pre><code>minimap2 -ax splice\
    \ -t 8 -G50k -k 21 -w 11 --sr -A2 -B8 -O12,32 -E2,1 -r200 -p.5 -N20 -f1000,5000\
    \ -n2 -m20 -s40 -g2000 -2K50m --secondary=no &lt;reference_fasta_file&gt; &lt;fastq_file&gt;\
    \ &gt; minimap.sam\n</code></pre>\n<p>(note the -t 8 as the number of threads,\
    \ change this as needed)\nNow we must retag the reads with their cell barcodes\
    \ and UMIs</p>\n<pre><code>python retag.py --sam minimap.sam --out minitagged.bam\n\
    </code></pre>\n<p>Then we must sort and index our bam.\nRequires <a href=\"http://www.htslib.org/\"\
    \ rel=\"nofollow\">samtools</a></p>\n<pre><code>samtools sort minitagged.bam minitagged_sorted.bam\n\
    samtools index minitagged_sorted.bam\n</code></pre>\n<h3>\n<a id=\"user-content-2-calling-candidate-variants\"\
    \ class=\"anchor\" href=\"#2-calling-candidate-variants\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Calling\
    \ candidate variants</h3>\n<p>You may wish to break this into multiple jobs such\
    \ as 1 job per chromosome and merge after but the basic command is the following.\n\
    Requires <a href=\"https://github.com/ekg/freebayes\">freebayes</a> and add /path/to/freebayes/bin\
    \ to your PATH</p>\n<pre><code>freebayes -f &lt;reference_fasta&gt; -iXu -C 2\
    \ -q 20 -n 3 -E 1 -m 30 --min-coverage 6 --max-coverage 100000 minitagged_sorted.bam\n\
    </code></pre>\n<h3>\n<a id=\"user-content-3-cell-allele-counting\" class=\"anchor\"\
    \ href=\"#3-cell-allele-counting\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>3. Cell allele counting</h3>\n<p>Requires\
    \ <a href=\"https://github.com/10XGenomics/vartrix\">vartrix</a>\nand add /path/to/vartrix\
    \ to your PATH</p>\n<pre><code>vartrix --umi --mapq 30 -b &lt;bam file&gt; -c\
    \ &lt;barcode tsv&gt; --scoring-method coverage --threads 8 --ref-matrix ref.mtx\
    \ --out-matrix alt.mtx -v &lt;freebayes vcf&gt; --fasta &lt;fasta file used for\
    \ remapping&gt;\n</code></pre>\n<p>note the --threads argument and use an appropriate\
    \ number of threads for your system.</p>\n<h3>\n<a id=\"user-content-4-clustering-cells-by-genotype\"\
    \ class=\"anchor\" href=\"#4-clustering-cells-by-genotype\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>4. Clustering\
    \ cells by genotype</h3>\n<p>Rust required. To install rust:</p>\n<pre><code>curl\
    \ https://sh.rustup.rs -sSf | sh\n</code></pre>\n<p>and to build souporcell clustering</p>\n\
    <pre><code>cd /path/to/souporcell/souporcell\ncargo build --release\n</code></pre>\n\
    <p>And add /path/to/souporcell/souporcell/target/release to your path\nusage</p>\n\
    <pre><code>souporcell -h\nsouporcell 2.4\nHaynes Heaton &lt;whheaton@gmail.com&gt;\n\
    clustering scRNAseq cells by genotype\n\nUSAGE:\n    souporcell [OPTIONS] --alt_matrix\
    \ &lt;alt_matrix&gt; --barcodes &lt;barcodes&gt; --num_clusters &lt;num_clusters&gt;\
    \ --ref_matrix &lt;ref_matrix&gt;\n\nFLAGS:\n    -h, --help       Prints help\
    \ information\n    -V, --version    Prints version information\n\nOPTIONS:\n \
    \   -a, --alt_matrix &lt;alt_matrix&gt;                                      \
    \     alt matrix from vartrix\n    -b, --barcodes &lt;barcodes&gt;           \
    \                                    cell barcodes\n        --initialization_strategy\
    \ &lt;initialization_strategy&gt;\n            cluster initialization strategy,\
    \ defaults to kmeans++, valid values are kmeans++, random_uniform,\n         \
    \   middle_variance, random_cell_assignment\n        --known_cell_assignments\
    \ &lt;known_cell_assignments&gt;\n            tsv with barcodes and their known\
    \ assignments\n\n    -g, --known_genotypes &lt;known_genotypes&gt;\n         \
    \   NOT YET IMPLEMENTED population vcf/bcf of known genotypes if available.\n\
    \            \n        --known_genotypes_sample_names &lt;known_genotypes_sample_names&gt;...\n\
    \            NOT YET IMPLEMENTED sample names, must be samples from the known_genotypes\
    \ vcf\n\n        --min_alt &lt;min_alt&gt;\n            minimum number of cells\
    \ containing the alt allele for the variant to be used for clustering\n\n    \
    \    --min_alt_umis &lt;min_alt_umis&gt;                                     \
    \  min alt umis to use locus for clustering\n        --min_ref &lt;min_ref&gt;\n\
    \            minimum number of cells containing the ref allele for the variant\
    \ to be used for clustering\n\n        --min_ref_umis &lt;min_ref_umis&gt;   \
    \                                    min ref umis to use locus for clustering\n\
    \    -k, --num_clusters &lt;num_clusters&gt;                                 \
    \      number of clusters\n    -r, --ref_matrix &lt;ref_matrix&gt;           \
    \                                ref matrix from vartrix\n    -r, --restarts &lt;restarts&gt;\
    \                                               number of random seedings\n  \
    \      --seed &lt;seed&gt;                                                   \
    \    optional random seed\n    -t, --threads &lt;threads&gt;                 \
    \                                number of threads to use\n</code></pre>\n<p>So\
    \ generally something along the lines of</p>\n<pre><code>souporcell -a alt.mtx\
    \ -r ref.mtx -b barcodes.tsv -k &lt;num_clusters&gt; -t 8 &gt; clusters_tmp.tsv\n\
    </code></pre>\n<p>(note clusters_tmp.tsv output as the doublet caller outputs\
    \ the final clusters file)</p>\n<h3>\n<a id=\"user-content-5-calling-doublets\"\
    \ class=\"anchor\" href=\"#5-calling-doublets\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>5. Calling doublets</h3>\n<p>Rust\
    \ required.\nBuild troublet:</p>\n<pre><code>cd /path/to/souporcell/troublet\n\
    cargo build --release\n</code></pre>\n<p>And add /path/to/souporcell/troublet/target/release\
    \ to your path\nThe usage is</p>\n<pre><code>troublet -h\ntroublet 2.4\nHaynes\
    \ Heaton &lt;whheaton@gmail.com&gt;\nIntergenotypic doublet detection given cluster\
    \ assignments and cell allele counts\n\nUSAGE:\n    troublet [OPTIONS] --alts\
    \ &lt;alts&gt; --clusters &lt;clusters&gt;\n\nFLAGS:\n    -h, --help       Prints\
    \ help information\n    -V, --version    Prints version information\n\nOPTIONS:\n\
    \    -a, --alts &lt;alts&gt;                              alt allele counts per\
    \ cell in sparse matrix format out of vartrix\n    -c, --clusters &lt;clusters&gt;\
    \                      cluster file output from schism\n    -b, --debug &lt;debug&gt;...\
    \                         print debug info for index of cells listed\n    -d,\
    \ --doublet_prior &lt;doublet_prior&gt;            prior on doublets. Defaults\
    \ to 0.5\n        --doublet_threshold &lt;doublet_threshold&gt;    doublet posterior\
    \ threshold, defaults to 0.90\n    -r, --refs &lt;refs&gt;                   \
    \           ref allele counts per cell in sparse matrix format out of vartrix\n\
    \        --singlet_threshold &lt;singlet_threshold&gt;    singlet posterior threshold,\
    \ defaults to 0.90\n</code></pre>\n<p>So generally</p>\n<pre><code>troublet -a\
    \ alt.mtx -r ref.mtx --clusters clusters_tmp.tsv &gt; clusters.tsv\n</code></pre>\n\
    <h3>\n<a id=\"user-content-6-genotype-and-ambient-rna-coinference\" class=\"anchor\"\
    \ href=\"#6-genotype-and-ambient-rna-coinference\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>6. Genotype and\
    \ ambient RNA coinference</h3>\n<p>Python3 required with modules pystan, pyvcf,\
    \ pickle, math, scipy, gzip (pip install should work for each)</p>\n<pre><code>consensus.py\
    \ -h\nusage: consensus.py [-h] -c CLUSTERS -a ALT_MATRIX -r REF_MATRIX [-p PLOIDY]\n\
    \                    --soup_out SOUP_OUT --vcf_out VCF_OUT --output_dir\n    \
    \                OUTPUT_DIR -v VCF\n\nconsensus genotype calling and ambient RNA\
    \ estimation\n\noptional arguments:\n  -h, --help            show this help message\
    \ and exit\n  -c CLUSTERS, --clusters CLUSTERS\n                        tsv cluster\
    \ file from the troublet output\n  -a ALT_MATRIX, --alt_matrix ALT_MATRIX\n  \
    \                      alt matrix file\n  -r REF_MATRIX, --ref_matrix REF_MATRIX\n\
    \                        ref matrix file\n  -p PLOIDY, --ploidy PLOIDY\n     \
    \                   ploidy, must be 1 or 2, defaults to 2\n  --soup_out SOUP_OUT\
    \   soup output\n  --vcf_out VCF_OUT     vcf output\n  --output_dir OUTPUT_DIR\n\
    \                        output directory\n  -v VCF, --vcf VCF     vcf file from\
    \ which alt and ref matrix were created\n</code></pre>\n<p>So generally</p>\n\
    <pre><code>consensus.py -c clusters.tsv -a alt.mtx -r ref.mtx --soup_out soup.txt\
    \ -v &lt;freebayes vcf&gt; --vcf_out cluster_genotypes.vcf --output_dir .\n</code></pre>\n"
  stargazers_count: 62
  subscribers_count: 9
  topics:
  - scrna-seq
  - scrnaseq
  - scrna-seq-analysis
  - bioinformatics
  - computational-biology
  - genomics
  updated_at: 1621879001.0
willgpaik/grass_qgis_aci:
  data_format: 2
  description: Singularity recipe for GRASS GIS and QGIS on Centos 7
  filenames:
  - Singularity
  full_name: willgpaik/grass_qgis_aci
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-grass_qgis_aci\" class=\"anchor\" href=\"#grass_qgis_aci\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>grass_qgis_aci</h1>\n<p>Singularity recipe for GRASS GIS and QGIS\
    \ on Centos 8 For ICS Roar clusters</p>\n<p>2019/1/22<br>\nGRASS 7.4.4 is updated</p>\n\
    <p>2019/1/25<br>\nGRASS GIS g.extension function can be used by:</p>\n<pre><code>$\
    \ grass74 /PATH/to/Mapset/PERMANENT --exec g.extension &lt;ADD-ON&gt;\n</code></pre>\n\
    <p>If Mapset is not installed, download sample Mapset to \"scratch\" directory:</p>\n\
    <pre><code>$ cd ~/scratch\n$ wget https://grass.osgeo.org/sampledata/worldlocation.tar.gz\n\
    $ tar -xf worldlocation.tar.gz\n    \n$ grass74 ~/scratch/worldlocation/PERMANENT\
    \ --exec g.extension &lt;ADD-ON&gt;\n</code></pre>\n<p>To delete sample Mapset:</p>\n\
    <pre><code>$ cd ~/scartch\n$ rm -rf worldlocation\n$ rm worldlocation.tar.gz\n\
    </code></pre>\n<p>2021/03/10<br>\nOS is upgraded to Centos 8 from Centos 7<br>\n\
    GRASS is upgraded to 7.8.5</p>\n<p>2021/03/17<br>\nQGIS is removed (can be added\
    \ again later if requested)</p>\n<p>2021/05/19<br>\nGRASS scripts are enabled<br>\n\
    GRASS GIS Addons can be installed with command:</p>\n<pre><code>$ g.extension\
    \ extension=&lt;Add-on&gt; prefix=&lt;Install_DIR&gt;\n</code></pre>\n<p>Make\
    \ sure to create ~/.grassrc.78 i.e.:</p>\n<pre><code>GISDBASE: &lt;PATH_TO_MAP_DATA&gt;\n\
    LOCATION_NAME: nc_basic_spm_grass7\nMAPSET: user1\n</code></pre>\n<p>then from\
    \ container:</p>\n<pre><code>&gt; export GISRC=$HOME/.grassrc.78\n</code></pre>\n"
  stargazers_count: 0
  subscribers_count: 0
  topics: []
  updated_at: 1621446968.0
willgpaik/horovod_roar:
  data_format: 2
  description: Singularity recipe for Horovod on Centos 8
  filenames:
  - Singularity
  full_name: willgpaik/horovod_roar
  latest_release: null
  readme: '<h1>

    <a id="user-content-horovod_roar" class="anchor" href="#horovod_roar" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>horovod_roar</h1>

    <p>Singularity recipe for Horovod on Centos 8 for Roar</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1619664798.0
wtsi-hgi/nf_cellbender:
  data_format: 2
  description: Single cell Nextflow cellbender pipeline.
  filenames:
  - env/Singularity.preprocessing
  full_name: wtsi-hgi/nf_cellbender
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-description\" class=\"anchor\" href=\"#description\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Description</h1>\n<p>The methods used in this module are described\
    \ in <code>docs/methods.pdf</code>. TODO: <code>docs/methods.pdf</code></p>\n\
    <p>Below is the structure of the results directory. The values that will be listed\
    \ in <code>description_of_params</code> within the directory structure correspond\
    \ to the various parameters one can set. An example of a paramters file is found\
    \ in <code>example_runtime_setup/params.yml</code>.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>nf-qc_cluster\n\u251C\u2500\u2500 normalization_001::description_of_params\n\
    \u2502   \u251C\u2500\u2500 [files: data]\n\u2502   \u251C\u2500\u2500 reduced_dims-pca::description_of_params\n\
    \u2502   \u2502   \u251C\u2500\u2500 [files: data]\n\u2502   \u2502   \u251C\u2500\
    \u2500 [plots: umap]\n\u2502   \u2502   \u251C\u2500\u2500 cluster_001::description_of_params\n\
    \u2502   \u2502   \u2502   \u251C\u2500\u2500 [files: data,clusters]\n\u2502 \
    \  \u2502   \u2502   \u251C\u2500\u2500 [plots: umap]\n\u2502   \u2502   \u2502\
    \   \u251C\u2500\u2500 cluster_markers_001::description_of_params\n\u2502   \u2502\
    \   \u2502   \u2502   \u251C\u2500\u2500 [files: cluster_marker_genes]\n\u2502\
    \   \u2502   \u2502   \u2502   \u2514\u2500\u2500 [plots: marker_genes,marker_genes_dotplot]\n\
    \u2502   \u2502   \u2502   \u251C\u2500\u2500 cluster_markers_002::description_of_params\n\
    \u2502   \u2502   \u2502   ... etc. ...\n\u2502   \u2502   \u251C\u2500\u2500\
    \ cluster_002::description_of_params\n\u2502   \u2502   ... etc. ...\n\u2502 \
    \  \u251C\u2500\u2500 reduced_dims-harmony_001::description_of_params\n\u2502\
    \   \u251C\u2500\u2500 reduced_dims-harmony_002::description_of_params\n\u2502\
    \   ... etc. ...\n\u251C\u2500\u2500 normalization_002::description_of_norm_params\n\
    ... etc. ...\n\u2514\u2500\u2500 adata.h5  <span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> concatenated single cell data with no normalization</span></pre></div>\n\
    <h1>\n<a id=\"user-content-todo-list\" class=\"anchor\" href=\"#todo-list\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>TODO\
    \ list</h1>\n<ul>\n<li>Add <code>docs/methods.pdf</code> file.</li>\n<li>Add brief\
    \ description of module.</li>\n</ul>\n<h1>\n<a id=\"user-content-enhancement-list\"\
    \ class=\"anchor\" href=\"#enhancement-list\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Enhancement list</h1>\n<ul>\n\
    <li>\n<code>scanpy_merge-dev.py</code>: If it were important to have a per sample\
    \ filter, merge could be re-designed to accommodate this.</li>\n<li>\n<code>scanpy_cluster.py</code>:\
    \ Currently for clustering, we can change method (leiden or louvain), resolution,\
    \ and n_pcs. Are there other parameters that need to be scaled over?</li>\n<li>Check\
    \ phenotypes against predicted sex from gene expression.</li>\n<li>Add basic QC\
    \ plots - try to do this in R from anndata frame?</li>\n<li>Scrublet functionality\
    \ + add to metadata + cluster distributions</li>\n<li>Gene scores + add to metadata</li>\n\
    <li>Add marker gene AUC like here <a href=\"http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters\"\
    \ rel=\"nofollow\">http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters</a>\n\
    </li>\n<li>Add summary ARI and LISI metrics computed over a list of many different\
    \ cluster annotations?</li>\n<li>Add tSNE plots - rapid plots with OpenTSNE?</li>\n\
    <li>Calculate marker genes with diffxpy or logreg?</li>\n</ul>\n<h1>\n<a id=\"\
    user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Quickstart</h1>\n\
    <p>Quickstart for deploying this pipeline locally and on a high performance compute\
    \ cluster.</p>\n<h2>\n<a id=\"user-content-1-set-up-the-environment\" class=\"\
    anchor\" href=\"#1-set-up-the-environment\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>1. Set up the environment</h2>\n\
    <p>Install the required packages via conda:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> The repo directory.</span>\n\
    REPO_MODULE=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${HOME}</span>/repo/path/to/this/pipeline<span class=\"pl-pds\">\"</span></span>\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Install environment using\
    \ Conda.</span>\nconda env create --name sc_qc_cluster --file <span class=\"pl-smi\"\
    >${REPO_MODULE}</span>/env/environment.yml\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Activate the new Conda environment.</span>\n<span class=\"pl-c1\"\
    >source</span> activate sc_qc_cluster\n\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> To update environment file:</span>\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span>conda env export --no-builds | grep -v prefix | grep -v name &gt;\
    \ environment.yml</span></pre></div>\n<h2>\n<a id=\"user-content-2-prepare-the-input-files\"\
    \ class=\"anchor\" href=\"#2-prepare-the-input-files\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>2. Prepare the\
    \ input files</h2>\n<p>Generate and/or edit input files for the pipeline.</p>\n\
    <p>The pipeline takes as input:</p>\n<ol>\n<li>\n<strong>--file_paths_10x</strong>:\
    \  Tab-delimited file containing experiment_id and data_path_10x_format columns\
    \ (i.e., list of input samples). Reqired.</li>\n<li>\n<strong>--file_metadata</strong>:\
    \  Tab-delimited file containing sample metadata. This will automatically be subset\
    \ down to the sample list from 1. Reqired.</li>\n<li>\n<strong>--file_sample_qc</strong>:\
    \  YAML file containing sample qc and filtering parameters. Optional. NOTE: in\
    \ the example config file, this is part of the YAML file for <code>-params-file</code>.</li>\n\
    <li>\n<strong>--genes_exclude_hvg</strong>:  Tab-delimited file with genes to\
    \ exclude from\nhighly variable gene list. Must contain ensembl_gene_id column.\
    \ Optional.</li>\n<li>\n<strong>--genes_score</strong>:  Tab-delimited file with\
    \ genes to use to score cells. Must contain ensembl_gene_id and score_idvcolumns.\
    \ If one score_id == \"cell_cycle\", then requires a grouping_id column with \"\
    G2/M\" and \"S\" (see example file in <code>example_runtime_setup</code>). Optional.</li>\n\
    <li>\n<strong>-params-file</strong>:  YAML file containing analysis parameters.\
    \ Optional.</li>\n<li>\n<strong>--run_multiplet</strong>:  Flag to run multiplet\
    \ analysis. Optional.</li>\n<li>\n<strong>--file_cellmetadata</strong>:  Tab-delimited\
    \ file containing experiment_id and data_path_cellmetadata columns. For instance\
    \ this file can be used to pass per cell doublet annotations. Optional.</li>\n\
    </ol>\n<p>Examples of all of these files can be found in <code>example_runtime_setup/</code>.</p>\n\
    <h2>\n<a id=\"user-content-3-set-up-and-run-nextflow\" class=\"anchor\" href=\"\
    #3-set-up-and-run-nextflow\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"\
    octicon octicon-link\"></span></a>3. Set up and run Nextflow</h2>\n<p>Run Nexflow\
    \ locally (NOTE: if running on a virtual machine you may need to set <code>export\
    \ QT_QPA_PLATFORM=\"offscreen\"</code> for scanpy as described <a href=\"https://github.com/ipython/ipython/issues/10627\"\
    >here</a>):</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> Boot up tmux session.</span>\ntmux new -s\
    \ nf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Here we are not going\
    \ to filter any variable genes, so don't pass a file.</span>\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> NOTE: All input file paths should be full paths.</span>\n\
    nextflow run <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"\
    pl-smi\">${REPO_MODULE}</span>/main.nf<span class=\"pl-pds\">\"</span></span>\
    \ \\\n    -profile <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>local<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --file_paths_10x <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/file_paths_10x.tsv<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --file_metadata <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/file_metadata.tsv<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --genes_score <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/genes_score_v001.tsv<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    -params-file <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/params.yml<span\
    \ class=\"pl-pds\">\"</span></span></pre></div>\n<p>Run Nextflow using LSF on\
    \ a compute cluster. More on bgroups <a href=\"https://www.ibm.com/support/knowledgecenter/SSETD4_9.1.3/lsf_config_ref/lsb.params.default_jobgroup.5.html\"\
    \ rel=\"nofollow\">here</a>.:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Set the results directory.</span>\n\
    RESULTS_DIR=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/results/dir<span\
    \ class=\"pl-pds\">\"</span></span>\nmkdir -p <span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${RESULTS_DIR}</span><span class=\"pl-pds\"\
    >\"</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Boot up\
    \ tmux session.</span>\ntmux new -s nf\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Log into an interactive session.</span>\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> NOTE: Here we set the -G parameter due to our institute's\
    \ LSF configuration.</span>\nbgadd <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>/<span class=\"pl-smi\">${USER}</span>/logins<span class=\"pl-pds\"\
    >\"</span></span>\nbsub -q normal -G team152 -g /<span class=\"pl-smi\">${USER}</span>/logins\
    \ -Is -XF -M 8192 -R <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>select[mem&gt;8192]\
    \ rusage[mem=8192]<span class=\"pl-pds\">\"</span></span> /bin/bash\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span> NOTE: If you are running over many cells,\
    \ you may need to start an</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ interactive session on a queue that allows long jobs</span>\n<span class=\"\
    pl-c\"><span class=\"pl-c\">#</span>bsub -q long -G team152 -g /${USER}/logins\
    \ -Is -XF -M 18192 -R \"select[mem&gt;18192] rusage[mem=18192]\" /bin/bash</span>\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Activate the Conda environment\
    \ (inherited by subsequent jobs).</span>\nconda activate sc_qc_cluster\n\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> Set up a group to submit jobs to\
    \ (export a default -g parameter).</span>\nbgadd -L 500 <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>/<span class=\"pl-smi\">${USER}</span>/nf<span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-k\">export</span> LSB_DEFAULT_JOBGROUP=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>/<span class=\"pl-smi\">${USER}</span>/nf<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> Depending on LSF setup, you may want to export a default -G parameter.</span>\n\
    <span class=\"pl-k\">export</span> LSB_DEFAULTGROUP=<span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span>team152<span class=\"pl-pds\">\"</span></span>\n<span\
    \ class=\"pl-c\"><span class=\"pl-c\">#</span> NOTE: By setting the above flags,\
    \ all of the nextflow LSF jobs will have</span>\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> these flags set.</span>\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Settings for scanpy (see note above).</span>\n<span class=\"pl-k\"\
    >export</span> QT_QPA_PLATFORM=<span class=\"pl-s\"><span class=\"pl-pds\">\"\
    </span>offscreen<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> Change to a temporary runtime dir on the node.\
    \ In this demo, we will change</span>\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> to the same execution directory.</span>\n<span class=\"pl-c1\">cd</span>\
    \ <span class=\"pl-smi\">${RESULTS_DIR}</span>\n\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> Remove old logs and nextflow output (if one previously ran nextflow\
    \ in this</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> dir).</span>\n\
    rm -r <span class=\"pl-k\">*</span>html<span class=\"pl-k\">;</span>\nrm .nextflow.log<span\
    \ class=\"pl-k\">*</span><span class=\"pl-k\">;</span>\n\n<span class=\"pl-c\"\
    ><span class=\"pl-c\">#</span> NOTE: If you want to resume a previous workflow,\
    \ add -resume to the flag.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>\
    \ NOTE: If you do not want to filter any variable genes, pass an empty file to</span>\n\
    <span class=\"pl-c\"><span class=\"pl-c\">#</span>       --genes_exclude_hvg.\
    \ See previous local example.</span>\n<span class=\"pl-c\"><span class=\"pl-c\"\
    >#</span> NOTE: --output_dir should be a full path - not relative.</span>\nnextflow\
    \ run <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >${REPO_MODULE}</span>/main.nf<span class=\"pl-pds\">\"</span></span> \\\n   \
    \ -profile <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lsf<span class=\"\
    pl-pds\">\"</span></span> \\\n    --file_paths_10x <span class=\"pl-s\"><span\
    \ class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/file_paths_10x.tsv<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --file_metadata <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/file_metadata.tsv<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --file_sample_qc <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/params.yml<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --genes_exclude_hvg <span class=\"\
    pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/genes_remove_hvg_v001.tsv<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --genes_score <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${REPO_MODULE}</span>/example_runtime_setup/genes_score_v001.tsv<span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --output_dir <span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${RESULTS_DIR}</span><span\
    \ class=\"pl-pds\">\"</span></span> \\\n    --run_multiplet \\\n    -params-file\
    \ <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\"\
    >${REPO_MODULE}</span>/example_runtime_setup/params.yml<span class=\"pl-pds\"\
    >\"</span></span> \\\n    -with-report <span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>nf_report.html<span class=\"pl-pds\">\"</span></span> \\\n    -resume\n\
    \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> NOTE: If you would like to\
    \ see the ongoing processes, look at the log files.</span>\ncat .nextflow.log</pre></div>\n\
    <p>Example of how one may sync results:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>NF_OUT_DIR=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/out_dir<span\
    \ class=\"pl-pds\">\"</span></span>\nrsync -am --include=<span class=\"pl-s\"\
    ><span class=\"pl-pds\">\"</span>*.png<span class=\"pl-pds\">\"</span></span>\
    \ --include=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*/<span class=\"\
    pl-pds\">\"</span></span> --exclude=<span class=\"pl-s\"><span class=\"pl-pds\"\
    >\"</span>*<span class=\"pl-pds\">\"</span></span> my_cluster_ssh:<span class=\"\
    pl-smi\">${NF_OUT_DIR}</span> <span class=\"pl-c1\">.</span>\nrsync -am --include=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.png<span class=\"pl-pds\">\"\
    </span></span> --include=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*/<span\
    \ class=\"pl-pds\">\"</span></span> --exclude=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span>*<span class=\"pl-pds\">\"</span></span> my_cluster_ssh:<span\
    \ class=\"pl-smi\">${NF_OUT_DIR}</span> <span class=\"pl-c1\">.</span></pre></div>\n\
    <h1>\n<a id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Notes</h1>\n\
    <ul>\n<li>On 10 April 2020, we found nextflow was writing some output into the\
    \ <code>${HOME}</code> directory and had used up the alotted ~15Gb on the Sanger\
    \ farm. This resulted in a Java error as soon as a nextflow command was executed.\
    \ Based on file sizes within <code>${HOME}</code>, it seemed like the ouput was\
    \ being written within the conda environment (following <code>du -h | sort -V\
    \ -k 1</code>). By deleting and re-installing the coda environment, the problem\
    \ was solved. The below flags may help prevent this from the future. In addition,\
    \ setting the flag <code>export JAVA_OPTIONS=-Djava.io.tmpdir=/path/with/enough/space/</code>\
    \ may also help.</li>\n</ul>\n<div class=\"highlight highlight-source-shell\"\
    ><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> To be run from the execution\
    \ dir, before the above nextflow command</span>\n<span class=\"pl-c\"><span class=\"\
    pl-c\">#</span> If you are running this on a cluster, make sure you log into an\
    \ interactive</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> session\
    \ with &gt;25Gb of RAM.</span>\n<span class=\"pl-k\">export</span> NXF_OPTS=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span>-Xms25G -Xmx25G<span class=\"\
    pl-pds\">\"</span></span>\n<span class=\"pl-k\">export</span> NXF_HOME=<span class=\"\
    pl-s\"><span class=\"pl-pds\">$(</span>pwd<span class=\"pl-pds\">)</span></span>\n\
    <span class=\"pl-k\">export</span> NXF_WORK=<span class=\"pl-s\"><span class=\"\
    pl-pds\">\"</span><span class=\"pl-smi\">${NXF_HOME}</span>/.nexflow_work<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">export</span> NXF_TEMP=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${NXF_HOME}</span>/.nexflow_temp<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">export</span> NXF_CONDA_CACHEDIR=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${NXF_HOME}</span>/.nexflow_conda<span\
    \ class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">export</span> NXF_SINGULARITY_CACHEDIR=<span\
    \ class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-smi\">${NXF_HOME}</span>/.nexflow_singularity<span\
    \ class=\"pl-pds\">\"</span></span></pre></div>\n"
  stargazers_count: 0
  subscribers_count: 2
  topics: []
  updated_at: 1621547413.0
wyattferguson/trumpbot-rnn:
  data_format: 2
  description: A RNN trained on Donald Trumps tweets
  filenames:
  - Singularity
  full_name: wyattferguson/trumpbot-rnn
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-trumpbot-v10\" class=\"anchor\" href=\"#trumpbot-v10\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Trumpbot v1.0</h1>\n<p>Trumpbot was my attempt at creating a RNN trained\
    \ on Donald Trumps(DT) tweets. I used this as a sort of practice project for learning\
    \ a bit about RNN's and Tensorflow 2. The result was a chaos and a learning experience\
    \ so let's dive in.</p>\n<h2>\n<a id=\"user-content-run-with-containers\" class=\"\
    anchor\" href=\"#run-with-containers\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Run with Containers</h2>\n<h3>\n\
    <a id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Docker</h3>\n\
    <p>If you don't want to install dependencies to your host, you can build a Docker\
    \ container\nwith the included Dockerfile:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ docker build -t trumpbot <span class=\"pl-c1\">.</span></pre></div>\n\
    <p>The entrypoint is the script to generate the tweets:</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ docker run trumpbot\n...\n obamas Top and France\
    \ at 900 PM on FoxNews. Anderson Congratulations to the House vote <span class=\"\
    pl-k\">for</span> MittRomney o\n\n hillary Clinton has been a total disaster.\
    \ I have an idea <span class=\"pl-k\">for</span> <span class=\"pl-smi\">her great\
    \ speech on CNN</span> <span class=\"pl-k\">in</span> the world  a great honor\
    \ <span class=\"pl-k\">for</span> me and his partisan hotel and every spor\n\n\
    \ friends support Trump International Golf Club on the Paris About that Right\
    \ School is started by the DNC and Clinton and the DNC that will be a great show\
    \ with t</pre></div>\n<p>If you want to interact with the container (perhaps training\
    \ first) you can shell inside instead:</p>\n<div class=\"highlight highlight-source-shell\"\
    ><pre>$ docker run -it --entrypoint bash trumpbot\nroot@b53b98f12c34:/code# ls\n\
    Dockerfile  README.md  __init__.py  learn.py  raw_tweets.txt  requirements.txt\t\
    training_checkpoints  trumpbot.py  tweets.txt</pre></div>\n<p>You'll be in the\
    \ <code>/code</code> directory that contains the source code.</p>\n<h3>\n<a id=\"\
    user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"\
    true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Singularity</h3>\n\
    <p>For users that want to perhaps use GPU (or better leverage the host) the recommendation\
    \ is to\nuse a <a href=\"https://www.sylabs.io/guides/3.2/user-guide/\" rel=\"\
    nofollow\">Singularity</a> container, and a recipe file <a href=\"Singularity\"\
    >Singularity</a> is provided\nto build the container.</p>\n<div class=\"highlight\
    \ highlight-source-shell\"><pre>$ sudo singularity build trumpbot.sif Singularity</pre></div>\n\
    <p>And then to run (add the --nv flag if you want to leverage any host libraries).</p>\n\
    <div class=\"highlight highlight-source-shell\"><pre>$ singularity run trumpbot.sif</pre></div>\n\
    <p>If you need to change the way that tensorflow or numpy are installed, you can\
    \ edit the Singularity or Docker recipes.</p>\n<h2>\n<a id=\"user-content-setup\"\
    \ class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Setup</h2>\n<p>Setup is pretty straightforward.\
    \ It only needs numpy and tensorflow 2 alpha just run the start pip install:</p>\n\
    <pre><code>pip3 install -r requirements.txt\n</code></pre>\n<h2>\n<a id=\"user-content-dataset\"\
    \ class=\"anchor\" href=\"#dataset\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Dataset</h2>\n<p>The entire dataset\
    \ was just tweets scraped from the DT twitter account. I used Jefferson Henrique's\
    \ library <a href=\"https://github.com/Jefferson-Henrique/GetOldTweets-python\"\
    >GetOldTweets-python</a> that I modified a little bit. All the raw tweets can\
    \ be found in the raw_tweets.txt file FYI all the links in any tweet have been\
    \ removed.</p>\n<p>The first thing about using Tweets as a dataset for training\
    \ is that they are filled with garbage that wreaks havoc when training. Heres\
    \ what I did:</p>\n<ul>\n<li>Removed any links or urls to photos</li>\n<li>Simplified\
    \ all the puncuation, with Trump this is a big thing, his tweets are a clown fiesta\
    \ of periods and exclemation marks.</li>\n<li>Cleaned out any invisible or non-english\
    \ characters, any foreign characters just casuases trouble.</li>\n<li>Removed\
    \ the '@' symbol, I'll explain why later.</li>\n<li>Removed the first couple of\
    \ months of tweets, they were mostly about the celebrity apprentice and not really\
    \ core to what I was trying to capture.</li>\n<li>Removed any retweets or super\
    \ short @replies</li>\n</ul>\n<p>The final training text is in tweets.txt which\
    \ altogether is about 20,000 tweets.</p>\n<h2>\n<a id=\"user-content-training\"\
    \ class=\"anchor\" href=\"#training\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Training</h2>\n<p>I trained the\
    \ model twice, the first time for 30 epochs which took around 6 hours. The result\
    \ was absolute garbage, at the time I hadn't removed hidden or foreign characters\
    \ so it took 6 hours to spit out complete nonsense. So after I cleaned out the\
    \ tweets again, I ran the training overnight for 50 epochs this time.</p>\n<p>Just\
    \ run the learn.py file to train it again if you want, the model check points\
    \ are stored in the 'training_checkpoints' folder</p>\n<pre><code>python3 learn.py\n\
    </code></pre>\n<h2>\n<a id=\"user-content-generating-tweets\" class=\"anchor\"\
    \ href=\"#generating-tweets\" aria-hidden=\"true\"><span aria-hidden=\"true\"\
    \ class=\"octicon octicon-link\"></span></a>Generating Tweets</h2>\n<p>So now\
    \ the fun part, you can run the command:</p>\n<pre><code>python3 trumpbot.py\n\
    </code></pre>\n<p>This will generate 10 tweets from a random group of topics.\
    \ If you open the trumpbot.py file theres a few things you can play with:</p>\n\
    <pre><code>tweets - Number of messages you want generated\n\ntemperature - This\
    \ controls how predictable the tweet will be, by \n    default its random from\
    \ 0.1 -&gt; 0.4, anything above about 0.7 generates\n    garbage.\n\ntalking_points\
    \ - Is a list of inputs to feed the network, try out \n    differnt words and\
    \ see what works.\n\nnum_generate - This controls the length of the message you\
    \ want to\n     get generated.\n</code></pre>\n<h2>\n<a id=\"user-content-result\"\
    \ class=\"anchor\" href=\"#result\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Result</h2>\n<p>For my first\
    \ crack at text generation Im happy with the results. Here are some sample tweets:</p>\n\
    <pre><code>hillary Clinton has been a total disaster. If you cant admit that \n\
    the U.S. more than has been treated big baster I am a g\n\nDonald Trump is 45%\
    \ Iran\n\nhealthe lobbyist now wants to raise taxes for our country in the \n\
    first place! If only one thing is clea\n\nfriends support Trump Rally Anger Golf\
    \ Club of Caporate legislation \nat the WhiteHouse today! #MakeAmericaGreatAgain\
    \ Thank you for your\n support! #Trump2016 \n\nkoreau like you it was great being\
    \ in the last election then will be\n a great show. I have a fan o\n\nkoreau lies\
    \ and losers and losers will be a great show with the U.S.\n The President has\
    \ a various past c\n</code></pre>\n<h2>\n<a id=\"user-content-what-i-learned\"\
    \ class=\"anchor\" href=\"#what-i-learned\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>What I learned</h2>\n<ul>\n<li>\n\
    <p>Tweets make for a tough training set. Things like @ mentions just pollute the\
    \ hell out of the text so unless you want your bot to be constantly @ing everything\
    \ I need to find a better way to deal with that.</p>\n</li>\n<li>\n<p>Things I\
    \ thought the bot would love talking about stuff like #MAGA, Russia, China, and\
    \ collusion just generate garbage strings.</p>\n</li>\n<li>\n<p>Text generation\
    \ is really hard, and takes a ton of training time.</p>\n</li>\n<li>\n<p>I could\
    \ probably get a bit better results if I let it train a bit longer but for any\
    \ drastic improvements I probably need to try another method or spend alot more\
    \ time tuning the training set.</p>\n</li>\n<li>\n<p>Pick a subject that doesn't\
    \ tweet like hes a dad yelling at a little league game. I think because his tweets\
    \ are short little outbursts its hard to generate a predictable pattern across\
    \ them.</p>\n</li>\n<li>\n<p>The words it groups together for differnt topics\
    \ is probably worth looking at, like whenever you use 'hillary' as a input it\
    \ usually has the words 'liar' or 'disaster' in the sentence. or how it loves\
    \ telling you when its gonna be on @Foxandfriends</p>\n</li>\n<li>\n<p>With the\
    \ method I used spelling its like to add random 'u' infront of words.</p>\n</li>\n\
    </ul>\n<p>I feel like this is good starting point, and with some work we might\
    \ have a digital orange man bot in our future.</p>\n<h2>\n<a id=\"user-content-postbox-credit-contact--support\"\
    \ class=\"anchor\" href=\"#postbox-credit-contact--support\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><g-emoji\
    \ class=\"g-emoji\" alias=\"postbox\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4ee.png\"\
    >\U0001F4EE</g-emoji> Credit, Contact &amp; Support</h2>\n<p>Created by <a href=\"\
    https://twitter.com/programmingsux\" rel=\"nofollow\">Wyatt Ferguson</a></p>\n\
    <p>For any comments or questions your can reach me on Twitter <a href=\"https://twitter.com/programmingsux\"\
    \ rel=\"nofollow\">@programmingsux</a> or visit my little portfolio at <a href=\"\
    https://wyattf.dev\" rel=\"nofollow\">wyattf.dev</a></p>\n<p>If you like my theme\
    \ and want to support me</p>\n<h3>\n<a id=\"user-content-coffee-buy-me-a-coffee\"\
    \ class=\"anchor\" href=\"#coffee-buy-me-a-coffee\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://www.buymeacoffee.com/wyattferguson\"\
    \ rel=\"nofollow\"><g-emoji class=\"g-emoji\" alias=\"coffee\" fallback-src=\"\
    https://github.githubassets.com/images/icons/emoji/unicode/2615.png\">\u2615</g-emoji>\
    \ Buy Me A Coffee</a>\n</h3>\n<h3>\n<a id=\"user-content-zap-follow-me-on-twitter\"\
    \ class=\"anchor\" href=\"#zap-follow-me-on-twitter\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://twitter.com/programmingsux\"\
    \ rel=\"nofollow\"><g-emoji class=\"g-emoji\" alias=\"zap\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/26a1.png\"\
    >\u26A1</g-emoji> Follow me on Twitter</a>\n</h3>\n<h3>\n<a id=\"user-content-bus-follow-me-on-devto\"\
    \ class=\"anchor\" href=\"#bus-follow-me-on-devto\" aria-hidden=\"true\"><span\
    \ aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a><a href=\"https://dev.to/wyattferguson\"\
    \ rel=\"nofollow\"><g-emoji class=\"g-emoji\" alias=\"bus\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f68c.png\"\
    >\U0001F68C</g-emoji> Follow me on DEV.to</a>\n</h3>\n"
  stargazers_count: 16
  subscribers_count: 2
  topics:
  - tensorflow
  - python
  updated_at: 1620777753.0
wzacs1/minION_Metagenomics:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: wzacs1/minION_Metagenomics
  latest_release: null
  readme: '<h1>

    <a id="user-content-minion_metagenomics" class="anchor" href="#minion_metagenomics"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>minION_Metagenomics</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1578393029.0
yh549848/singularity-bowtie2:
  data_format: 2
  description: null
  filenames:
  - 2.4.1/Singularity
  full_name: yh549848/singularity-bowtie2
  latest_release: null
  readme: '<h1>

    <a id="user-content-mycobacterial-pre-processing-pipeline" class="anchor" href="#mycobacterial-pre-processing-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mycobacterial
    Pre-processing Pipeline</h1>

    <p>Cleans and QCs reads with fastp and FastQC, classifies with Kraken2 &amp; Mykrobe,
    removes non-bacterial content, and - by alignment to any minority genomes - disambiguates
    mixtures of bacterial reads.</p>

    <p>Takes as input one directory containing pairs of fastq(.gz) or bam files.

    Produces as output one directory per sample, containing the relevant reports &amp;
    a pair of cleaned fastqs.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>The workflow is designed to run with either docker <code>-profile docker</code>
    or singularity <code>-profile singularity</code>. Before running the workflow
    using singularity, the singularity images for the workflow will need to be built
    by running <code>singularity/singularity_pull.sh</code></p>

    <p>E.g. to run the workflow:</p>

    <pre><code>nextflow run main.nf -profile singularity --filetype fastq --input_dir
    fq_dir --pattern "*_R{1,2}.fastq.gz" --unmix_myco yes \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj


    nextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco
    no \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj

    </code></pre>

    <h2>

    <a id="user-content-params" class="anchor" href="#params" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Params</h2>

    <p>The following parameters should be set in <code>nextflow.config</code> or specified
    on the command line:</p>

    <ul>

    <li>

    <strong>input_dir</strong><br>

    Directory containing fastq OR bam files</li>

    <li>

    <strong>filetype</strong><br>

    File type in input_dir. Either "fastq" or "bam"</li>

    <li>

    <strong>pattern</strong><br>

    Regex to match fastq files in input_dir, e.g. "*_R{1,2}.fq.gz"</li>

    <li>

    <strong>output_dir</strong><br>

    Output directory</li>

    <li>

    <strong>unmix_myco</strong><br>

    Do you want to disambiguate mixed-mycobacterial samples by read alignment? Either
    "yes" or "no"</li>

    <li>

    <strong>species</strong><br>

    Principal species in each sample, assuming genus Mycobacterium. Default ''null''.
    If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae,
    chimaera, fortuitum, intracellulare, kansasii, tuberculosis</li>

    <li>

    <strong>kraken_db</strong><br>

    Directory containing <code>*.k2d</code> Kraken2 database files (obtain from <a
    href="https://benlangmead.github.io/aws-indexes/k2" rel="nofollow">https://benlangmead.github.io/aws-indexes/k2</a>)</li>

    <li>

    <strong>bowtie2_index</strong><br>

    Directory containing Bowtie2 index (obtain from <a href="ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip"
    rel="nofollow">ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip</a>).
    The specified path should NOT include the index name</li>

    <li>

    <strong>bowtie_index_name</strong><br>

    Name of the bowtie index, e.g. hg19_1kgmaj<br>

    </li>

    </ul>

    <br>

    <p>For more information on the parameters run <code>nextflow run main.nf --help</code></p>

    <h2>

    <a id="user-content-checkpoints" class="anchor" href="#checkpoints" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoints</h2>

    <p>Checkpoints used throughout this workflow to fail a sample/issue warnings:</p>

    <p>processes preprocessing_checkFqValidity or preprocessing_checkBamValidity</p>

    <ol>

    <li>(Fail) If sample does not pass fqtools ''validate'' or samtools ''quickcheck'',
    as appropriate.</li>

    </ol>

    <p>process preprocessing_countReads<br>

    2. (Fail) If sample contains &lt; 100k pairs of raw reads.</p>

    <p>process preprocessing_fastp<br>

    3. (Fail) If sample contains &lt; 100k pairs of cleaned reads, required to all
    be &gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10
    --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size
    1 --cut_tail_mean_quality 20).</p>

    <p>process preprocessing_kraken2<br>

    4. (Fail) If the top family hit is not Mycobacteriaceae<br>

    5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae <br>

    6. (Warn) If the top family classification is mycobacterial, but this is not consistent
    with top genus and species classifications<br>

    7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications
    meet minimum thresholds of &gt; 5000 reads or &gt; 0.5% of the total reads (this
    is not necessarily a concern as not all mycobacteria have a taxonomic classification
    at this rank) <br>

    8. (Warn) If sample is mixed or contaminated - defined as containing reads &gt;
    the 5000/0.5% thresholds from multiple non-human species<br>

    9. (Warn) If sample contains multiple classifications to mycobacterial species
    complexes, each meeting the &gt; 5000/0.5% thresholds<br>

    10. (Warn) If no species classification meets the 5000/0.5% thresholds<br>

    11. (Warn) If no genus classification meets the 5000/0.5% thresholds<br>

    12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant
    given point 5)</p>

    <p>process preprocessing_identifyBacterialContaminants<br>

    13. (Fail) If the sample is not contaminated and the top species hit is not one
    of the 10 supported Mycobacteria:\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis<br>

    14. (Fail) If the sample is not contaminated and the top species hit is contrary
    to the species expected (e.g. "avium" rather than "tuberculosis" - only tested
    if you provide that expectation)<br>

    15. (Warn) If the top species hit is supported by &lt; 75% coverage<br>

    16. (Warn) If the top species hit has a median coverage depth &lt; 10-fold<br>

    17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant
    species, which means we will not be able to locate its genome, and thereby remove
    it as a contaminant<br>

    18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated
    with a contaminant species'' taxon ID<br>

    19. (Warn) If no complete genome could be found for a contaminant species. The
    workflow will proceed with alignment-based contaminant removal, but you''re warned
    that there''s reduced confidence in detecting reads from this species</p>

    <p>process preprocessing_downloadContamGenomes<br>

    20. (Fail) If a contaminant is detected but we are unable to download a representative
    genome, and thereby remove it</p>

    <p>process preprocessing_summarise<br>

    21. (Fail) If after having taken an alignment-based approach to decontamination,
    Kraken still detects a contaminant species<br>

    22. (Fail) If after having taken an alignment-based approach to decontamination,
    the top species hit is not one of the 10 supported Mycobacteria<br>

    23. (Fail) If, after successfully removing contaminants, the top species hit is
    contrary to the species expected (e.g. "avium" rather than "tuberculosis" - only
    tested if you provide that expectation)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620946111.0
yh549848/singularity-cufflinks:
  data_format: 2
  description: null
  filenames:
  - 2.2.1/Singularity
  full_name: yh549848/singularity-cufflinks
  latest_release: null
  readme: '<h1>

    <a id="user-content-mycobacterial-pre-processing-pipeline" class="anchor" href="#mycobacterial-pre-processing-pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mycobacterial
    Pre-processing Pipeline</h1>

    <p>Cleans and QCs reads with fastp and FastQC, classifies with Kraken2 &amp; Mykrobe,
    removes non-bacterial content, and - by alignment to any minority genomes - disambiguates
    mixtures of bacterial reads.</p>

    <p>Takes as input one directory containing pairs of fastq(.gz) or bam files.

    Produces as output one directory per sample, containing the relevant reports &amp;
    a pair of cleaned fastqs.</p>

    <h2>

    <a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Quick Start</h2>

    <p>The workflow is designed to run with either docker <code>-profile docker</code>
    or singularity <code>-profile singularity</code>. Before running the workflow
    using singularity, the singularity images for the workflow will need to be built
    by running <code>singularity/singularity_pull.sh</code></p>

    <p>E.g. to run the workflow:</p>

    <pre><code>nextflow run main.nf -profile singularity --filetype fastq --input_dir
    fq_dir --pattern "*_R{1,2}.fastq.gz" --unmix_myco yes \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj


    nextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco
    no \

    --output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name
    hg19_1kgmaj

    </code></pre>

    <h2>

    <a id="user-content-params" class="anchor" href="#params" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Params</h2>

    <p>The following parameters should be set in <code>nextflow.config</code> or specified
    on the command line:</p>

    <ul>

    <li>

    <strong>input_dir</strong><br>

    Directory containing fastq OR bam files</li>

    <li>

    <strong>filetype</strong><br>

    File type in input_dir. Either "fastq" or "bam"</li>

    <li>

    <strong>pattern</strong><br>

    Regex to match fastq files in input_dir, e.g. "*_R{1,2}.fq.gz"</li>

    <li>

    <strong>output_dir</strong><br>

    Output directory</li>

    <li>

    <strong>unmix_myco</strong><br>

    Do you want to disambiguate mixed-mycobacterial samples by read alignment? Either
    "yes" or "no"</li>

    <li>

    <strong>species</strong><br>

    Principal species in each sample, assuming genus Mycobacterium. Default ''null''.
    If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae,
    chimaera, fortuitum, intracellulare, kansasii, tuberculosis</li>

    <li>

    <strong>kraken_db</strong><br>

    Directory containing <code>*.k2d</code> Kraken2 database files (obtain from <a
    href="https://benlangmead.github.io/aws-indexes/k2" rel="nofollow">https://benlangmead.github.io/aws-indexes/k2</a>)</li>

    <li>

    <strong>bowtie2_index</strong><br>

    Directory containing Bowtie2 index (obtain from <a href="ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip"
    rel="nofollow">ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip</a>).
    The specified path should NOT include the index name</li>

    <li>

    <strong>bowtie_index_name</strong><br>

    Name of the bowtie index, e.g. hg19_1kgmaj<br>

    </li>

    </ul>

    <br>

    <p>For more information on the parameters run <code>nextflow run main.nf --help</code></p>

    <h2>

    <a id="user-content-checkpoints" class="anchor" href="#checkpoints" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Checkpoints</h2>

    <p>Checkpoints used throughout this workflow to fail a sample/issue warnings:</p>

    <p>processes preprocessing_checkFqValidity or preprocessing_checkBamValidity</p>

    <ol>

    <li>(Fail) If sample does not pass fqtools ''validate'' or samtools ''quickcheck'',
    as appropriate.</li>

    </ol>

    <p>process preprocessing_countReads<br>

    2. (Fail) If sample contains &lt; 100k pairs of raw reads.</p>

    <p>process preprocessing_fastp<br>

    3. (Fail) If sample contains &lt; 100k pairs of cleaned reads, required to all
    be &gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10
    --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size
    1 --cut_tail_mean_quality 20).</p>

    <p>process preprocessing_kraken2<br>

    4. (Fail) If the top family hit is not Mycobacteriaceae<br>

    5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae <br>

    6. (Warn) If the top family classification is mycobacterial, but this is not consistent
    with top genus and species classifications<br>

    7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications
    meet minimum thresholds of &gt; 5000 reads or &gt; 0.5% of the total reads (this
    is not necessarily a concern as not all mycobacteria have a taxonomic classification
    at this rank) <br>

    8. (Warn) If sample is mixed or contaminated - defined as containing reads &gt;
    the 5000/0.5% thresholds from multiple non-human species<br>

    9. (Warn) If sample contains multiple classifications to mycobacterial species
    complexes, each meeting the &gt; 5000/0.5% thresholds<br>

    10. (Warn) If no species classification meets the 5000/0.5% thresholds<br>

    11. (Warn) If no genus classification meets the 5000/0.5% thresholds<br>

    12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant
    given point 5)</p>

    <p>process preprocessing_identifyBacterialContaminants<br>

    13. (Fail) If the sample is not contaminated and the top species hit is not one
    of the 10 supported Mycobacteria:\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis<br>

    14. (Fail) If the sample is not contaminated and the top species hit is contrary
    to the species expected (e.g. "avium" rather than "tuberculosis" - only tested
    if you provide that expectation)<br>

    15. (Warn) If the top species hit is supported by &lt; 75% coverage<br>

    16. (Warn) If the top species hit has a median coverage depth &lt; 10-fold<br>

    17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant
    species, which means we will not be able to locate its genome, and thereby remove
    it as a contaminant<br>

    18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated
    with a contaminant species'' taxon ID<br>

    19. (Warn) If no complete genome could be found for a contaminant species. The
    workflow will proceed with alignment-based contaminant removal, but you''re warned
    that there''s reduced confidence in detecting reads from this species</p>

    <p>process preprocessing_downloadContamGenomes<br>

    20. (Fail) If a contaminant is detected but we are unable to download a representative
    genome, and thereby remove it</p>

    <p>process preprocessing_summarise<br>

    21. (Fail) If after having taken an alignment-based approach to decontamination,
    Kraken still detects a contaminant species<br>

    22. (Fail) If after having taken an alignment-based approach to decontamination,
    the top species hit is not one of the 10 supported Mycobacteria<br>

    23. (Fail) If, after successfully removing contaminants, the top species hit is
    contrary to the species expected (e.g. "avium" rather than "tuberculosis" - only
    tested if you provide that expectation)</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620946061.0
yh549848/singularity-hisat2:
  data_format: 2
  description: null
  filenames:
  - 2.2.1/Singularity
  - 2.1.0/Singularity
  full_name: yh549848/singularity-hisat2
  latest_release: null
  readme: '<h1>

    <a id="user-content-afnipype" class="anchor" href="#afnipype" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>afnipype</h1>

    <p>Neurodocker Docker/Singularity container build scripts for AFNI+Python 3.6+nipype</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621263935.0
yh549848/singularity-kallisto:
  data_format: 2
  description: null
  filenames:
  - 0.44.0/Singularity
  - 0.46.1/Singularity
  full_name: yh549848/singularity-kallisto
  latest_release: null
  readme: '<h1>

    <a id="user-content-tracula" class="anchor" href="#tracula" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TRACULA</h1>

    <p>TRACULA Pipeline</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621023361.0
yh549848/singularity-rsem:
  data_format: 2
  description: null
  filenames:
  - 1.3.1/Singularity
  - 1.3.3/Singularity
  full_name: yh549848/singularity-rsem
  latest_release: null
  readme: '<h1>

    <a id="user-content-afnipype" class="anchor" href="#afnipype" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>afnipype</h1>

    <p>Neurodocker Docker/Singularity container build scripts for AFNI+Python 3.6+nipype</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621259784.0
yh549848/singularity-rstudio-methylseq:
  data_format: 2
  description: null
  filenames:
  - latest--rstudio1.2.5042r362/Singularity
  full_name: yh549848/singularity-rstudio-methylseq
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-bioconductor-devel-docker-image\" class=\"anchor\"\
    \ href=\"#bioconductor-devel-docker-image\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Bioconductor Devel Docker image</h1>\n\
    <p>Bioconductor Docker image with full set of system dependencies so that\nall\
    \ Bioconductor packages can be installed.</p>\n<p>The Docker images have R and\
    \ Bioconductor with different versions\nunder each \"branch\" in git.</p>\n<p><strong>NOTE</strong>:\
    \ Docker image for bioconductor_full:devel is in the <code>master</code>\nbranch,\
    \ and all the release branches will be under the branch\n<code>RELEASE_X_Y</code>.</p>\n\
    <p>Important Links:</p>\n<p><a href=\"https://hub.docker.com/r/bioconductor/bioconductor_full\"\
    \ rel=\"nofollow\">Docker hub link for bioconductor_full</a></p>\n<p><a href=\"\
    https://github.com/Bioconductor/bioconductor_full\">Github development link for\
    \ bioconductor_full</a></p>\n<h2>\n<a id=\"user-content-advantages-of-the-bioconductor_full-docker-image\"\
    \ class=\"anchor\" href=\"#advantages-of-the-bioconductor_full-docker-image\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Advantages of the <code>bioconductor_full</code> docker image</h2>\n\
    <ol>\n<li>\n<p>The bioconductor_full docker images can be used instead of\ninstalling\
    \ complex dependencies needed for Bioconductor\npackages. The image comes with\
    \ most of the dependencies installed.</p>\n</li>\n<li>\n<p>Quick start up to start\
    \ your analysis with all the Bioconductor\npackages if needed.</p>\n</li>\n<li>\n\
    <p>The image will be regularly updated to reflect the build system on\nBioconductor.\
    \ This is a very useful resource for maintainers who\nare actively developing\
    \ their package to see if it works in tandem\nwith the bioconductor ecosystem.\
    \ It provides a local testing outlet\nfor maintainers and developers.</p>\n</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-installation-and-quick-start\" class=\"anchor\"\
    \ href=\"#installation-and-quick-start\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>Installation and quick start</h2>\n\
    <p>This document assumes you have <a href=\"https://www.docker.com/\" rel=\"nofollow\"\
    >docker</a> installed. Please check\n<a href=\"https://www.docker.com/products/docker-desktop\"\
    \ rel=\"nofollow\">installation</a> if you have more questions regarding this.</p>\n\
    <h3>\n<a id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Quick start</h3>\n<ul>\n<li>\n<p>Start docker on your machine.</p>\n\
    </li>\n<li>\n<p>On the command line, \"pull\" the bioconductor_full docker image\
    \ with\nthe correct tag. These images are hosted on docker hub under the\n<a href=\"\
    https://cloud.docker.com/u/bioconductor/repository/registry-1.docker.io/bioconductor/bioconductor_full\"\
    \ rel=\"nofollow\">Bioconductor organization</a> page.</p>\n<pre><code>  docker\
    \ pull bioconductor/bioconductor_full:devel\n</code></pre>\n<p>or</p>\n<pre><code>\
    \  docker pull bioconductor/bioconductor_full:RELEASE_X_Y\n</code></pre>\n</li>\n\
    <li>\n<p>Once the image is available on your local machine, you can check to\n\
    see if they are available.</p>\n<pre><code>  docker images\n</code></pre>\n</li>\n\
    <li>\n<p>To start using these images with RStudio, this will start the image\n\
    under the 'rstudio' user</p>\n<pre><code>  docker run                        \
    \              \\\n      -e PASSWORD=your_password                   \\\n    \
    \  -p 8787:8787                                \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library\
    \ \\\n      bioconductor/bioconductor_full:devel\n</code></pre>\n</li>\n<li>\n\
    <p>To start the image interactively using the <code>bioc</code> user</p>\n<pre><code>\
    \  docker run                                     \\\n      -it              \
    \                          \\\n      --user bioc                             \
    \   \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library \\\n \
    \     bioconductor/bioconductor_full:devel       \\\n      R\n</code></pre>\n\
    <p>NOTE: The path <code>/usr/local/lib/R/host-site-library</code> is mapped to\n\
    <code>.libPaths()</code> in R. So, when R is started, all the libraries in\nthe\
    \ host directory, <code>host-site-library</code> are available to R. It is\nstored\
    \ on your machine mounted from the volume you fill in place\nof <code>host-site-library</code>.</p>\n\
    <p>These libraries will only work if they are pre-compiled with the\nsame version\
    \ of R that is in the docker image. To explain further,\nyou would need the packages\
    \ built with Bioconductor version '3.9'\nto work with R-3.6. Similarly, you'd\
    \ need Bioconductor version\n'3.9' to work with R-3.6.z.</p>\n</li>\n<li>\n<p>To\
    \ start the docker image in deamon mode, i.e, have the container\nrunning in the\
    \ background use the <code>-d</code> option.</p>\n<pre><code>  sudo docker run\
    \ -it                            \\\n      -d                                \
    \         \\\n      -v host-site-library:/usr/local/lib/R/host-site-library \\\
    \n      --entrypoint /bin/bash                     \\\n      bioconductor/bioconductor_full:devel\n\
    </code></pre>\n<p>This will start the container in the background and keep it\n\
    running. You may check the running processes using <code>docker ps</code>,\nand\
    \ copy the container id.</p>\n<pre><code>  docker ps\n</code></pre>\n<p>To attach\
    \ to a container which is running in the background</p>\n<pre><code>  docker exec\
    \ -it &lt;container_id&gt; bash\n</code></pre>\n<p>NOTE: You can replace <code>bash</code>\
    \ with R to start R directly in the\ncontainer.</p>\n<pre><code>  docker exec\
    \ -it &lt;container_id&gt; R\n</code></pre>\n</li>\n<li>\n<p>To run multiple RStudio\
    \ instances, use a different external port\nmapping (the first port in <code>-p\
    \ XXXX:YYYY</code>) for each instance.\nUse standard shell commands (e.g., adding\
    \ a <code>&amp;</code> at the end of the\nfirst docker command) to place docker\
    \ processes in the\nbackground. The 'devel' instance will be available at\n<a\
    \ href=\"http://localhost:8787\" rel=\"nofollow\">http://localhost:8787</a>, and\
    \ the release image at\n<a href=\"http://localhost:8788\" rel=\"nofollow\">http://localhost:8788</a></p>\n\
    <pre><code>  docker run                                      \\\n      -e PASSWORD=your_password\
    \                   \\\n      -p 8787:8787                                \\\n\
    \      bioconductor/bioconductor_full:devel\n\n  docker run                  \
    \                    \\\n      -e PASSWORD=your_password                   \\\n\
    \      -p 8788:8787                                \\\n      bioconductor/bioconductor_full:RELEASE_3_10\n\
    </code></pre>\n</li>\n</ul>\n<h3>\n<a id=\"user-content-using-latex-inside-the-container\"\
    \ class=\"anchor\" href=\"#using-latex-inside-the-container\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>using LaTeX\
    \ inside the container?</h3>\n<p>Install the <code>tinytex</code> package (<a\
    \ href=\"https://yihui.name/tinytex/\" rel=\"nofollow\">https://yihui.name/tinytex/</a>)\
    \ which has helpers for installing LaTeX functionality.</p>\n<div class=\"highlight\
    \ highlight-source-r\"><pre>install.packages(<span class=\"pl-s\"><span class=\"\
    pl-pds\">'</span>tinytex<span class=\"pl-pds\">'</span></span>)\n<span class=\"\
    pl-e\">tinytex</span><span class=\"pl-k\">::</span>install_tinytex()</pre></div>\n"
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621558877.0
yh549848/singularity-rstudio-rnaseqde:
  data_format: 2
  description: null
  filenames:
  - bc3.10--rstudio125042r362/Singularity
  - bc3.12--rstudio125042r405/Singularity
  full_name: yh549848/singularity-rstudio-rnaseqde
  latest_release: null
  readme: '<h1>

    <a id="user-content-sarscov2_snakemake_pipeline" class="anchor" href="#sarscov2_snakemake_pipeline"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SarsCov2_Snakemake_Pipeline</h1>

    <p>This is a snakemake pipeline used for analyse SarsCov2 sequence data generated
    by illumina machine.

    This pipelien was based on <a href="https://github.com/artic-network/fieldbioinformatics">ARTIC
    network''s fieldbioinformatics tools</a>, <a href="https://github.com/dridk/Sars-CoV-2-NGS-pipeline">Sars-CoV-2-NGS-pipeline</a>
    and <a href="https://github.com/connor-lab/ncov2019-artic-nf">ncov2019-artic-nf</a>
    with some updates:</p>

    <ol>

    <li>

    <code>fastqc</code> and  was used to generate the qc report of input data.</li>

    <li>

    <code>quast</code> was used to generate the sequence assembly report.</li>

    <li>

    <a href="https://github.com/cov-lineages/pangolin">pangolin</a> was used for the
    typing of SarsCov-2</li>

    <li>

    <code>CorGat</code> was used to annotate the sequence, and generate alle frequency
    reports

    You need to clone <a href="https://github.com/matteo14c/CorGAT">CorGat</a> and
    specify the directory in the config files.</li>

    <li>

    <code>multiqc</code> was used to generate the final report.</li>

    </ol>

    <p>The workflow shows like below:</p>

    <p>A test_data file was provided to test the pipeline.

    You may test the pipeline by dry-run

    <code>snakemake -s sars2.smk -n</code>

    then run the pipeline:

    <code>snakemake -s sars2.smk -j 4 --use-conda</code></p>

    <p>WARNING - THIS REPO IS UNDER ACTIVE DEVELOPMENT AND ITS BEHAVIOUR MAY CHANGE
    AT <strong>ANY</strong> TIME.</p>

    <p>PLEASE ENSURE THAT YOU READ BOTH THE README AND THE CONFIG FILE AND UNDERSTAND
    THE EFFECT OF THE OPTIONS ON YOUR DATA!</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621521929.0
yh549848/singularity-salmon:
  data_format: 2
  description: null
  filenames:
  - 1.5.0/Singularity
  full_name: yh549848/singularity-salmon
  latest_release: null
  readme: '<h1>

    <a id="user-content-tracula" class="anchor" href="#tracula" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TRACULA</h1>

    <p>TRACULA Pipeline</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1621037111.0
yh549848/singularity-star:
  data_format: 2
  description: null
  filenames:
  - 2.7.3a/Singularity
  - 2.6.1d/Singularity
  - 2.7.8a/Singularity
  full_name: yh549848/singularity-star
  latest_release: null
  readme: '<h1>

    <a id="user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    class="anchor" href="#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hindsight
    Rationality, and Efficient Deviation Types and Learning Experiments</h1>

    <p>Experiment code for <a href="https://arxiv.org/abs/2012.05874" rel="nofollow">Hindsight
    and Sequential Rationality of Correlated Play</a> and <a href="https://arxiv.org/abs/2102.06973"
    rel="nofollow">Efficient Deviation Types and Learning for Hindsight Rationality
    in Extensive-Form Games</a> conference papers (AAAI-21 and ICML 2021, respectively).</p>

    <p>This repository has a number of different components that work together to
    generate the experimental results.</p>

    <p>The pipeline begins with <code>hr_edl</code>. This is C++ code built over <a
    href="https://github.com/deepmind/open_spiel">OpenSpiel</a> that defines the experiments.
    See <a href="hr_edl/README.md"><code>hr_edl/README.md</code></a> for more information.</p>

    <p>The <code>hr_edl</code> code allows us to run experiments, but experiments
    are run with help from the Python3 library, <code>hr_edl_data</code>. Run <code>pip
    install .</code> to install it.

    With <code>hr_edl_data</code> installed, you can run <code>bin/run_experiment.py</code>
    to run an experiment.</p>

    <p>The experiment configurations used in the papers are defined in <code>Makefile</code>.
    Assuming that <code>hr_edl_data</code> is installed, running <code>make</code>
    should compile <code>hr_edl</code> and run all experiments, updating data files
    in <code>data</code> as necessary, and depositing Numpy data files in <code>results</code>.</p>

    <p>Finally, the Python Jupyter notebooks in <code>notebooks</code> process the
    Numpy data files into the final results, which are also saved in <code>results</code>.</p>

    <h2>

    <a id="user-content-virtual-machines-and-containers" class="anchor" href="#virtual-machines-and-containers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Virtual
    Machines and Containers</h2>

    <p>A <a href="https://www.vagrantup.com/" rel="nofollow">Vagrant</a> virtual machine
    configuration to help run these experiments is defined in <code>Vagrantfile</code>.

    If you already have Python3 installed though, you may not need to use it.

    Typically, the most onerous part of the installation procedure is building <code>hr_edl</code>
    and its dependencies.

    For this, you can use the <a href="https://sylabs.io/" rel="nofollow">Singularity</a>
    container defined by <code>hr_edl/Singularity.def</code>.

    Once you have Singularity installed or you are using the Vagrant virtual machine,
    you can run</p>

    <p><code>sudo singularity build ./hr_edl.sif Singularity.def</code></p>

    <p>to create a Singularity image. Then you can run</p>

    <p><code>singularity exec hr_edl.sif /code/build.optimized/bin/&lt;command&gt;
    [command options]</code></p>

    <p>to run an <code>hr_edl</code> executable inside the container.

    <code>bin/run_experiment.py</code> has a <code>--sif</code> option so you can
    specify a container image in which the experiment should be run.

    You can set the variable <code>SIF</code> and <code>HR_EDL_DIR =/code</code> in
    <code>Makefile</code> (either in the file or in the command like <code>make SIF=my_image.sif
    HR_EDL_DIR=/code</code>) to run all experiments in a given container image.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620951641.0
yh549848/singularity-stringtie:
  data_format: 2
  description: null
  filenames:
  - 1.3.4d/Singularity
  - 2.0.6/Singularity
  - 2.0.0/Singularity
  full_name: yh549848/singularity-stringtie
  latest_release: null
  readme: '<h1>

    <a id="user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    class="anchor" href="#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hindsight
    Rationality, and Efficient Deviation Types and Learning Experiments</h1>

    <p>Experiment code for <a href="https://arxiv.org/abs/2012.05874" rel="nofollow">Hindsight
    and Sequential Rationality of Correlated Play</a> and <a href="https://arxiv.org/abs/2102.06973"
    rel="nofollow">Efficient Deviation Types and Learning for Hindsight Rationality
    in Extensive-Form Games</a> conference papers (AAAI-21 and ICML 2021, respectively).</p>

    <p>This repository has a number of different components that work together to
    generate the experimental results.</p>

    <p>The pipeline begins with <code>hr_edl</code>. This is C++ code built over <a
    href="https://github.com/deepmind/open_spiel">OpenSpiel</a> that defines the experiments.
    See <a href="hr_edl/README.md"><code>hr_edl/README.md</code></a> for more information.</p>

    <p>The <code>hr_edl</code> code allows us to run experiments, but experiments
    are run with help from the Python3 library, <code>hr_edl_data</code>. Run <code>pip
    install .</code> to install it.

    With <code>hr_edl_data</code> installed, you can run <code>bin/run_experiment.py</code>
    to run an experiment.</p>

    <p>The experiment configurations used in the papers are defined in <code>Makefile</code>.
    Assuming that <code>hr_edl_data</code> is installed, running <code>make</code>
    should compile <code>hr_edl</code> and run all experiments, updating data files
    in <code>data</code> as necessary, and depositing Numpy data files in <code>results</code>.</p>

    <p>Finally, the Python Jupyter notebooks in <code>notebooks</code> process the
    Numpy data files into the final results, which are also saved in <code>results</code>.</p>

    <h2>

    <a id="user-content-virtual-machines-and-containers" class="anchor" href="#virtual-machines-and-containers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Virtual
    Machines and Containers</h2>

    <p>A <a href="https://www.vagrantup.com/" rel="nofollow">Vagrant</a> virtual machine
    configuration to help run these experiments is defined in <code>Vagrantfile</code>.

    If you already have Python3 installed though, you may not need to use it.

    Typically, the most onerous part of the installation procedure is building <code>hr_edl</code>
    and its dependencies.

    For this, you can use the <a href="https://sylabs.io/" rel="nofollow">Singularity</a>
    container defined by <code>hr_edl/Singularity.def</code>.

    Once you have Singularity installed or you are using the Vagrant virtual machine,
    you can run</p>

    <p><code>sudo singularity build ./hr_edl.sif Singularity.def</code></p>

    <p>to create a Singularity image. Then you can run</p>

    <p><code>singularity exec hr_edl.sif /code/build.optimized/bin/&lt;command&gt;
    [command options]</code></p>

    <p>to run an <code>hr_edl</code> executable inside the container.

    <code>bin/run_experiment.py</code> has a <code>--sif</code> option so you can
    specify a container image in which the experiment should be run.

    You can set the variable <code>SIF</code> and <code>HR_EDL_DIR =/code</code> in
    <code>Makefile</code> (either in the file or in the command like <code>make SIF=my_image.sif
    HR_EDL_DIR=/code</code>) to run all experiments in a given container image.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620951711.0
yh549848/singularity-tophat2:
  data_format: 2
  description: null
  filenames:
  - 2.1.1/Singularity
  full_name: yh549848/singularity-tophat2
  latest_release: null
  readme: '<h1>

    <a id="user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    class="anchor" href="#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hindsight
    Rationality, and Efficient Deviation Types and Learning Experiments</h1>

    <p>Experiment code for <a href="https://arxiv.org/abs/2012.05874" rel="nofollow">Hindsight
    and Sequential Rationality of Correlated Play</a> and <a href="https://arxiv.org/abs/2102.06973"
    rel="nofollow">Efficient Deviation Types and Learning for Hindsight Rationality
    in Extensive-Form Games</a> conference papers (AAAI-21 and ICML 2021, respectively).</p>

    <p>This repository has a number of different components that work together to
    generate the experimental results.</p>

    <p>The pipeline begins with <code>hr_edl</code>. This is C++ code built over <a
    href="https://github.com/deepmind/open_spiel">OpenSpiel</a> that defines the experiments.
    See <a href="hr_edl/README.md"><code>hr_edl/README.md</code></a> for more information.</p>

    <p>The <code>hr_edl</code> code allows us to run experiments, but experiments
    are run with help from the Python3 library, <code>hr_edl_data</code>. Run <code>pip
    install .</code> to install it.

    With <code>hr_edl_data</code> installed, you can run <code>bin/run_experiment.py</code>
    to run an experiment.</p>

    <p>The experiment configurations used in the papers are defined in <code>Makefile</code>.
    Assuming that <code>hr_edl_data</code> is installed, running <code>make</code>
    should compile <code>hr_edl</code> and run all experiments, updating data files
    in <code>data</code> as necessary, and depositing Numpy data files in <code>results</code>.</p>

    <p>Finally, the Python Jupyter notebooks in <code>notebooks</code> process the
    Numpy data files into the final results, which are also saved in <code>results</code>.</p>

    <h2>

    <a id="user-content-virtual-machines-and-containers" class="anchor" href="#virtual-machines-and-containers"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Virtual
    Machines and Containers</h2>

    <p>A <a href="https://www.vagrantup.com/" rel="nofollow">Vagrant</a> virtual machine
    configuration to help run these experiments is defined in <code>Vagrantfile</code>.

    If you already have Python3 installed though, you may not need to use it.

    Typically, the most onerous part of the installation procedure is building <code>hr_edl</code>
    and its dependencies.

    For this, you can use the <a href="https://sylabs.io/" rel="nofollow">Singularity</a>
    container defined by <code>hr_edl/Singularity.def</code>.

    Once you have Singularity installed or you are using the Vagrant virtual machine,
    you can run</p>

    <p><code>sudo singularity build ./hr_edl.sif Singularity.def</code></p>

    <p>to create a Singularity image. Then you can run</p>

    <p><code>singularity exec hr_edl.sif /code/build.optimized/bin/&lt;command&gt;
    [command options]</code></p>

    <p>to run an <code>hr_edl</code> executable inside the container.

    <code>bin/run_experiment.py</code> has a <code>--sif</code> option so you can
    specify a container image in which the experiment should be run.

    You can set the variable <code>SIF</code> and <code>HR_EDL_DIR =/code</code> in
    <code>Makefile</code> (either in the file or in the command like <code>make SIF=my_image.sif
    HR_EDL_DIR=/code</code>) to run all experiments in a given container image.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1620951820.0
ylugithub/pytorch_gpu_singularity:
  data_format: 2
  description: ' A Pytorch deep learning singularity container'
  filenames:
  - Singularity.1.7.0
  full_name: ylugithub/pytorch_gpu_singularity
  latest_release: null
  readme: "<h1>\n<a id=\"user-content-characterisationvl-software\" class=\"anchor\"\
    \ href=\"#characterisationvl-software\" aria-hidden=\"true\"><span aria-hidden=\"\
    true\" class=\"octicon octicon-link\"></span></a>CharacterisationVL-Software</h1>\n\
    <p>The purpose of this repository is for storing definition files to submit to\
    \ <a href=\"https://singularity-hub.org/\" rel=\"nofollow\">Singularity Hub.</a></p>\n\
    <p>If you are new to Singularity containers, please refer to <a href=\"https://sylabs.io/guides/3.5/user-guide/\"\
    \ rel=\"nofollow\">https://sylabs.io/guides/3.5/user-guide/</a> or a newer version\
    \ of this documentation.</p>\n<p>Each software package is located in its own folder.\
    \ The files are tagged with the software name and version number or date of build.\
    \ Please read below for the naming convention.</p>\n<p>To add software to the\
    \ repository you will need to create a new branch. The new branch is the name\
    \ of the software product. By convention, the new branch will be checked and merged\
    \ into the master branch and then deleted.</p>\n<h2>\n<a id=\"user-content-steps-to-add-a-software-package\"\
    \ class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Steps to\
    \ add a software package</h2>\n<ol>\n<li>Clone this repository</li>\n<li>Create\
    \ a branch</li>\n</ol>\n<pre><code>$ git branch &lt;software name&gt;\n</code></pre>\n\
    <ol start=\"3\">\n<li>Make a subdirectory for the software product.</li>\n</ol>\n\
    <pre><code>$ mkdir &lt;software name&gt;\n</code></pre>\n<ol start=\"4\">\n<li>Add\
    \ all the necessary files.</li>\n</ol>\n<ul>\n<li>Singularity definition file\
    \ or installation script</li>\n<li>Readme file including install and testing notes</li>\n\
    <li>Desktop files for adding to menus with necessary tags</li>\n<li>For full details,\
    \ <a href=\"template/README.md\">please refer to the 'template' folder in this\
    \ repository.</a>\n</li>\n</ul>\n<ol start=\"4\">\n<li>Commit all changes, including\
    \ a helpful message</li>\n</ol>\n<pre><code>$ git commit -m \"&lt;software name&gt;\
    \ added as requested in support ticket\"\n</code></pre>\n<ol start=\"6\">\n<li>Push\
    \ to the remote repository. i.e. this one.</li>\n<li>Submit merge request</li>\n\
    </ol>\n<h2>\n<a id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Naming your Singularity definition file, Singularity Hub and Licensing</h2>\n\
    <p>For all Singularity recipes where the software licensing permits redistribution,\
    \ please use this naming convention:</p>\n<pre><code>   Singularity.applicationName_version\n\
    \   Singularity.applicationName_version-cuda-cudaVersion\n\n</code></pre>\n<p>This\
    \ is where Singularity Hub fits into the equation. There is a webhook between\
    \ this repository and <a href=\"https://singularity-hub.org/\" rel=\"nofollow\"\
    >Singularity Hub</a>. When a commit is merged into the master branch, Singularity\
    \ Hub will build the container.</p>\n<p>If successfully built, the path to the\
    \ container on Singularity Hub is:</p>\n<pre><code>  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n\
    \  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\
    \n</code></pre>\n<p>For software where licensing does not support redistribution,\
    \ the container recipe can still be defined, but the container should not be built\
    \ on Singularity Hub.</p>\n<p>An example on how to handle this situation is the\
    \ recipe for CCP-EM.\nThe <a href=\"ccp-em/README.md\">README.md</a> contains\
    \ a section on Prerequisites. This section lists the required files to build the\
    \ container. The license must be accepted by the end user to obtain them.</p>\n\
    <p>Prerequisite files should not be committed to this repository.</p>\n<p>To prevent\
    \ Singularity Hub from attempting to build the container, we simply use a different\
    \ recipe naming convention as follows:</p>\n<pre><code>   applicationName_version.def\n\
    \   applicationName_version-cuda-cudaVersion.def\n\n</code></pre>\n<h2>\n<a id=\"\
    user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\"\
    \ aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"\
    ></span></a>Ubuntu Base Images</h2>\n<p>The folder 'ubuntu-base-image' contains\
    \ recipes for pre built base containers. These can be used as a starting point\
    \ to aid/speed up the development of your container recipe.</p>\n<p>The current\
    \ versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.</p>\n\
    <p>These are available on Singularity Hub.</p>\n<p>For example: from the Graphviz\
    \ Singularity.graphviz-2.40.1 recipe</p>\n<pre><code>Bootstrap: shub\nFrom:  \
    \    Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n</code></pre>\n\
    <p>These two lines, will tell Singularity to use the 'shub' bootstrap to obtain\
    \ the '1804' ubuntu-base-image container from Singularity Hub.</p>\n<p>From here\
    \ you just need to add the requirements to build a container for your required\
    \ piece of software. Please see <a href=\"graphviz/Singularity.graphviz-2.40.1\"\
    >Singularity.graphviz-2.40.1</a>\nfor the full recipe.</p>\n<p>The current ubuntu-base-images\
    \ include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.</p>\n\
    <h2>\n<a id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"\
    anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\
    ><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>Running\
    \ GUI applications on a non-GPU node</h2>\n<p>The applications in the Singularity\
    \ container should run without the need for a dedicated GPU.</p>\n<p>However,\
    \ an X server needs to be running for this to work. On nodes with GPU, X Server\
    \ is started with NVIDIA driver, and on non-GPU nodes, the X Server is started\
    \ with MESA library.</p>\n<p>X Server can be started during boot (for example,\
    \ using <code>systemctl set-default graphical.target</code>).</p>\n<p>Make sure\
    \ that VirtualGL package is installed in the container. The code below will download\
    \ and install VirtualGL.</p>\n<pre><code>wget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\
    \ndpkg -i virtualgl_2.6.2_amd64.deb\n</code></pre>\n<p>The application startup\
    \ script doesn't need to be modified, however, if the application needs to be\
    \ manually started, then <code>vglrun</code> needs to be appended before running\
    \ the application. For example: <code>singularity exec --nv -B /projects:/projects\
    \ -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX</code></p>\n\
    <p><a href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"><img\
    \ src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\"\
    \ alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"\
    \ style=\"max-width:100%;\"></a></p>\n"
  stargazers_count: 6
  subscribers_count: 1
  topics: []
  updated_at: 1619654142.0
yookuda/singularity-mysql:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: yookuda/singularity-mysql
  latest_release: null
  readme: '<h1>

    <a id="user-content-singularity-mysql" class="anchor" href="#singularity-mysql"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>singularity-mysql</h1>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1593177205.0
zhinoos/TestWebinar:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: zhinoos/TestWebinar
  latest_release: null
  readme: '<h1>

    <a id="user-content-mlcontainer" class="anchor" href="#mlcontainer" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>MLContainer</h1>

    <p>Singularity container and conda environments for ML based analysis @ HEPHY</p>

    <h2>

    <a id="user-content-machine-learning-hats" class="anchor" href="#machine-learning-hats"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>machine-learning-hats</h2>

    <p><a href="https://github.com/FNALLPC/machine-learning-hats">https://github.com/FNALLPC/machine-learning-hats</a></p>

    <p>On HEPGPU01 its easier to use the conda environment</p>

    <pre><code>conda env create --file environment-gpu.yml

    </code></pre>

    <p>On CLIP its better to use the container. It will

    be installed after the shutdown.</p>

    <pre><code>build_ml-hats.sh

    </code></pre>

    <p>Run the shell container</p>

    <pre><code>run_ml-hats.sh

    </code></pre>

    <p>Run a script</p>

    <pre><code>run_ml-hats.sh &lt;scripts&gt;

    </code></pre>

    <h2>

    <a id="user-content-deepjetcore" class="anchor" href="#deepjetcore" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>DeepJetCore</h2>

    <p><a href="https://github.com/DL4Jets/DeepJetCore">https://github.com/DL4Jets/DeepJetCore</a></p>

    <h1>

    <a id="user-content-container-for-deepjetcore" class="anchor" href="#container-for-deepjetcore"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Container
    for DeepJetCore</h1>

    <p>Build the container (only on  HEPGPU01)</p>

    <pre><code>build_deepjet3.sh

    </code></pre>

    <p>Run the container</p>

    <pre><code>run_deepjet3.sh

    </code></pre>

    <h2>

    <a id="user-content-todo" class="anchor" href="#todo" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>TODO</h2>

    <p>Try container on CLIP</p>

    <h2>

    <a id="user-content-open-points" class="anchor" href="#open-points" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Open Points</h2>

    <h3>

    <a id="user-content-conda-environments-on-clip" class="anchor" href="#conda-environments-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conda
    Environments on CLIP</h3>

    <p>CLIP provides already installations of Conda</p>

    <pre><code>ml add Anaconda3/19.10

    </code></pre>

    <p>Nevertheless I could not build reliable environments due to limited

    quota. Also trying to move the correspondig directories to /scratch-cbe/users

    was not successfull.</p>

    <p>At this point it seems better to use the conda environment inside a container.</p>

    <h3>

    <a id="user-content-building-container-on-clip" class="anchor" href="#building-container-on-clip"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on CLIP</h3>

    <p>Building containers on CLIP has two problems</p>

    <ul>

    <li>root rights required to build container (possible solution fakeroot)</li>

    <li>docker container could be transformed in singularity container, but

    the filesystems (BeeGEFS) do not fullfill the singularity requirements</li>

    </ul>

    <p>The best way seems to use CI with Jenkis ans the local singularity hub. This

    has to be understood in the future.</p>

    <h3>

    <a id="user-content-building-container-on-hepgpu01" class="anchor" href="#building-container-on-hepgpu01"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building
    Container on HEPGPU01</h3>

    <p>Root rights are required in case container are build from scratch. Fakeroot

    would be a possible workaround. Has to be tried.</p>

    <p>Another possibility is to use "sudo".</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1612903610.0
zhuyawen/amazon-dsstne:
  data_format: 2
  description: null
  filenames:
  - Singularity
  full_name: zhuyawen/amazon-dsstne
  latest_release: null
  readme: '<h1>

    <a id="user-content-amazon-dsstne-deep-scalable-sparse-tensor-network-engine"
    class="anchor" href="#amazon-dsstne-deep-scalable-sparse-tensor-network-engine"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Amazon
    DSSTNE: Deep Scalable Sparse Tensor Network Engine</h1>

    <p>DSSTNE (pronounced "Destiny") is an open source software library for training
    and deploying recommendation

    models with sparse inputs, fully connected hidden layers, and sparse outputs.
    Models with weight matrices

    that are too large for a single GPU can still be trained on a single host. DSSTNE
    has been used at Amazon

    to generate personalized product recommendations for our customers at Amazon''s
    scale. It is designed for

    production deployment of real-world applications which need to emphasize speed
    and scale over experimental

    flexibility.</p>

    <p>DSSTNE was built with a number of features for production recommendation workloads:</p>

    <ul>

    <li>

    <strong>Multi-GPU Scale</strong>: Training and prediction

    both scale out to use multiple GPUs, spreading out computation

    and storage in a model-parallel fashion for each layer.</li>

    <li>

    <strong>Large Layers</strong>: Model-parallel scaling enables larger networks
    than

    are possible with a single GPU.</li>

    <li>

    <strong>Sparse Data</strong>: DSSTNE is optimized for fast performance on sparse
    datasets, common in recommendation

    problems. Custom GPU kernels perform sparse computation on the GPU, without filling
    in lots of zeroes.</li>

    </ul>

    <h2>

    <a id="user-content-benchmarks" class="anchor" href="#benchmarks" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Benchmarks</h2>

    <ul>

    <li>scottlegrand@ reported [near-linear scaling with multiple GPUs] on the MovieLens
    recommendation problem

    (<a href="https://medium.com/@scottlegrand/first-dsstne-benchmarks-tldr-almost-15x-faster-than-tensorflow-393dbeb80c0f#.ghe74fu1q"
    rel="nofollow">https://medium.com/@scottlegrand/first-dsstne-benchmarks-tldr-almost-15x-faster-than-tensorflow-393dbeb80c0f#.ghe74fu1q</a>)</li>

    <li>Directions on how to run a benchmark can be found in <a href="benchmarks/Benchmark.md">here</a>

    </li>

    </ul>

    <h2>

    <a id="user-content-scaling-up" class="anchor" href="#scaling-up" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Scaling up</h2>

    <ul>

    <li><a href="http://blogs.aws.amazon.com/bigdata/post/TxGEL8IJ0CAXTK/Generating-Recommendations-at-Amazon-Scale-with-Apache-Spark-and-Amazon-DSSTNE"
    rel="nofollow">Using Spark in AWS EMR and Dockers in AWS ECS </a></li>

    </ul>

    <h2>

    <a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

    <p><a href="LICENSE">License</a></p>

    <h2>

    <a id="user-content-setup" class="anchor" href="#setup" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h2>

    <ul>

    <li>Follow <a href="docs/getting_started/setup.md">Setup</a> for step by step
    instructions on installing and setting up DSSTNE</li>

    </ul>

    <h2>

    <a id="user-content-user-guide" class="anchor" href="#user-guide" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>User Guide</h2>

    <ul>

    <li>Check <a href="docs/getting_started/userguide.md">User Guide</a> for detailed
    information about the features in DSSTNE</li>

    </ul>

    <h2>

    <a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Examples</h2>

    <ul>

    <li>Check <a href="docs/getting_started/examples.md">Examples</a> to start trying
    your first Neural Network Modeling using DSSTNE</li>

    </ul>

    <h2>

    <a id="user-content-qa" class="anchor" href="#qa" aria-hidden="true"><span aria-hidden="true"
    class="octicon octicon-link"></span></a>Q&amp;A</h2>

    <p><a href="FAQ.md">FAQ</a></p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1517840829.0
zihangs/plan_generators:
  data_format: 2
  description: null
  filenames:
  - forbiditerative/Singularity
  - fd-red-black-postipc2018/Singularity
  full_name: zihangs/plan_generators
  latest_release: null
  readme: '<h1>

    <a id="user-content-generate-plans-using-forbid-iterative-planners-top-k-diverse-etc"
    class="anchor" href="#generate-plans-using-forbid-iterative-planners-top-k-diverse-etc"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Generate
    plans using forbid iterative planners (top-k, diverse, etc)</h1>

    <p>This is a tool for creating synthetic plans, the generated plans will be converted
    to event logs (<code>.xes</code> files). Basically, this is a script which wrapped
    up current exisiting planners and the output data are in a suitable structure
    and format for our goal recognition experiments.</p>

    <h3>

    <a id="user-content-things-need-to-be-prepared" class="anchor" href="#things-need-to-be-prepared"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Things
    need to be prepared</h3>

    <ol>

    <li>

    <p>For running diverse planner, CPLEX and an recommended fast-downward planner
    is required.</p>

    </li>

    <li>

    <p>Required python 3 runtime environment, I recommend to build a python 3 virtual
    environment if you both have python 2 and python 3 installed.</p>

    </li>

    <li>

    <p>Need to complie C++ source codes, by following instuctions (CPLEX need to be
    pre-installed).</p>

    </li>

    <li>

    <p>To configure the command line environment variables in absolute path</p>

    <p>export DIVERSE_SCORE_COMPUTATION_PATH=/home/ubuntu/plan_generators/diversescore
    (VM)</p>

    <p>export DIVERSE_FAST_DOWNWARD_PLANNER_PATH=/home/ubuntu/plan_generators/fd-red-black-postipc2018
    (VM)</p>

    <p>export DIVERSE_SCORE_COMPUTATION_PATH=/Users/zihangs/plan_generators/diversescore
    (Mac)</p>

    </li>

    <li>

    <p>Download the dataset and put the downloaded dataset in this directory.</p>

    </li>

    </ol>

    <h3>

    <a id="user-content-commands-for-running-the-script" class="anchor" href="#commands-for-running-the-script"
    aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Commands
    for running the script</h3>

    <p>Before run the commands, you have to check the parameter configrations, the
    parameters at the top of <code>run.py</code>. Then just run the following commands
    to starts.</p>

    <div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span>
    activate the python 3 venv (if you don''t using venv, ignore this step)</span>

    <span class="pl-c1">source</span> <span class="pl-k">&lt;</span>venv<span class="pl-k">&gt;</span>/bin/activate


    <span class="pl-c"><span class="pl-c">#</span> run script</span>

    python run.py</pre></div>

    <p>It will take a long time to run.</p>

    <h3>

    <a id="user-content-outputs" class="anchor" href="#outputs" aria-hidden="true"><span
    aria-hidden="true" class="octicon octicon-link"></span></a>Outputs</h3>

    <p>Once the process is completed, check this directory, there will be a sub-directory
    <code>gene_data/</code>. All the domains, problems, tests and generated plans
    will be there. Then this directory will be used for our next steps for mining
    process models.</p>

    '
  stargazers_count: 0
  subscribers_count: 1
  topics: []
  updated_at: 1608999691.0
