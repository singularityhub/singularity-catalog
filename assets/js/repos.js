var data =
[
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "MontrealSergiy/deformation",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-deformation-field\" class=\"anchor\" href=\"#deformation-field\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeformation field\u003c/h1\u003e\n\u003cp\u003eThis PERL script is a wrapper that is calling sequence of commands for generating deformation fields scrips\n\u003ca href=\"https://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields\" rel=\"nofollow\"\u003ehttps://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields\u003c/a\u003e\nSource code for deformation pipeline and dependencies (MINC):\n\u003ca href=\"https://github.com/Mouse-Imaging-Centre/generate_deformation_fields\"\u003ehttps://github.com/Mouse-Imaging-Centre/generate_deformation_fields\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsage\u003c/p\u003e\n\u003cp\u003e./deformation.pl -input ICBM_00100_t1_final.mnc \u0026lt;\u0026lt;this could be any anatomical minc file, for a collection of minc files\u0026gt;\u0026gt; -output dummy_hoho -deformation_ratio 0.6 -coordinate 70 100 70 10 10 10 -tolerance_space 4 \u0026lt;\u0026gt; -blur_determinant 0.25 \u0026lt;\u0026gt; -error 0.00001 \u0026lt;\u0026gt; -iteration 100\u003c/p\u003e\n\u003cp\u003eThe output of running this command looks like this:\nICBM_00100_t1_final_deformed_by_0.4atROIx70-y100-z70dimx10.dimy10.dimz10.mnc. \u003c/p\u003e\n\u003cp\u003eWe will also have a directory dummy_hoho/TMP that will contain the in-between-files.\u003c/p\u003e\n\u003cp\u003e$:/dummy_hoho/TMP$ ls\u003c/p\u003e\n\u003cp\u003eblock.mnc\u003c/p\u003e\n\u003cp\u003eblurred0.25determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc\u003c/p\u003e\n\u003cp\u003eDDDDdilated.mnc\u003c/p\u003e\n\u003cp\u003eDDDDring.mnc\u003c/p\u003e\n\u003cp\u003edeterminant_r_0.4_grid.mnc\u003c/p\u003e\n\u003cp\u003edeterminant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc\u003c/p\u003e\n\u003cp\u003edeterminant_r_0.4.xfm\u003c/p\u003e\n\u003cp\u003emask.mnc\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621971777.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for dvc (https://github.com/iterative/dvc)",
    "filenames": [
      "Singularity",
      "Singularity.2.1.0",
      "Singularity.1.6.1"
    ],
    "full_name": "powerPlant/dvc-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the DVC tool for Data Version Control\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1621938926.0
  },
  {
    "data_format": 2,
    "description": "Bayesian Inference and Optimisation for the Monash Electrochemical Simulator",
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "lukegun/BIOMEC",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4983\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-biomec\" class=\"anchor\" href=\"#biomec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBIOMEC\u003c/h1\u003e\n\u003cp\u003eBayesian Inference and Optimisation for the Monash electrochemical Simulator (MECSim) is the application developed by the\nmonash electrochemistry group with the assistance of Assosiate Professor Jie Zhang, Emeratious Professor Alan Bond and technical assistance from Gareth Kennedy And Martin Robinson.\u003c/p\u003e\n\u003cp\u003eIt is an automatic plaform for parameterisation that uses mathmatical optimisation and Bayesian Inference to calculate parameters involved in the electrochemical simulation.\nBuilt around \u003ca href=\"http://www.garethkennedy.net/MECSim.html\" rel=\"nofollow\"\u003eMECSim\u003c/a\u003e and first applied in the PAPER. BIOMEC allows for automated parameterisation of DC and FTAC voltammetery, allowing highly dimensional fits of the posteriour distrabution.\u003c/p\u003e\n\u003cp\u003eBIOMEC uses \u003ca href=\"https://github.com/pints-team/pints\"\u003ePINTS\u003c/a\u003e for univariant Bayesian inference.\u003c/p\u003e\n\u003cp\u003eFor information of current uses see the original \u003ca href=\"https://chemistry-europe.onlinelibrary.wiley.com/doi/abs/10.1002/celc.202100391\" rel=\"nofollow\"\u003eBIOMEC paper\u003c/a\u003e, the \u003ca href=\"https://doi.org/10.1002/celc.201700678\" rel=\"nofollow\"\u003eoriginal Bayesian inference paper\u003c/a\u003e for AC voltammetry or our most recent \u003ca href=\"https://doi.org/10.1039/D0CC07549C\" rel=\"nofollow\"\u003efeatured article\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-biomec-image\" class=\"anchor\" href=\"#installing-biomec-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling BIOMEC image\u003c/h2\u003e\n\u003cp\u003eThe code is run in a singularity container which works for Ubuntu/UNIX and MAC (untested) OS systems.\nSingularity will need to be installed to use the image. Where the guide is seen in the following \u003ca href=\"https://sylabs.io/guides/3.6/user-guide/quick_start.html\" rel=\"nofollow\"\u003ewebsite\u003c/a\u003e or downloaded from connected singularity hub.\u003c/p\u003e\n\u003cp\u003eOnce singularity has been installed, download the BIOMEC file and run the code to create the BIOMEC container (which should be around 580MB).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo singularity build BIOMEC.simg Singularity.def\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the image is built the imput file (input.txt) can be passed to the image by using the following command.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ./BIOMEC.simg input.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will generate and ouput file with plots and results once completed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-generating-input-files\" class=\"anchor\" href=\"#generating-input-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerating input files\u003c/h2\u003e\n\u003cp\u003einputwritter.py can guide users unfamilaur with generating input files to create an input file for the BIOMEC container, this program is contained in the BIOMEC_inputwritter.\nSimply run the file using the following command and follow the prompts and an input file will be generated.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ python3 inputwritter.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe output of this file will then be of the form \u0026lt;input.txt\u0026gt; though other names will work.\nIt is important that a copy of the MECSim Master.inp file is present in the folder you run inputwritter.py as the MECSim input file is required for BIOMEC to run.\u003c/p\u003e\n\u003cp\u003eOnce comfortable with writting the input file it is recommended to use any text editor.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-biomec\" class=\"anchor\" href=\"#running-biomec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning BIOMEC\u003c/h2\u003e\n\u003cp\u003ePDF tutorial or youtube videos to come.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-supporting-code\" class=\"anchor\" href=\"#supporting-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupporting Code\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBIOMEC_inputwritter: Basic terminal/ .exe code for guiding uses in writting the input files for BIOMEC\u003c/li\u003e\n\u003cli\u003eMCMC PLOTTER: code to plot the mcmc output chains from the Iter_log.txt to images\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-known-issues\" class=\"anchor\" href=\"#known-issues\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown Issues\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCustom waveforms have not been tested and Estart and End cannot equal zero.\u003c/li\u003e\n\u003cli\u003eNumber of data poins in experimental data must be a multiple of two.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing\" class=\"anchor\" href=\"#citing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting\u003c/h2\u003e\n\u003cp\u003ePlease, cite the original \u003ca href=\"https://chemistry-europe.onlinelibrary.wiley.com/doi/abs/10.1002/celc.202100391\" rel=\"nofollow\"\u003eBIOMEC paper\u003c/a\u003e if you have used this package in a publication.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eBIOMEC analysis/python code is open source under the GPL-3.0 License, with MECSim developed by Gareth Kennedy and contaned in the mecsim.cpython-37m-x86_64-linux-gnu.so shared object is under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-get-in-touch\" class=\"anchor\" href=\"#get-in-touch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGet in touch\u003c/h2\u003e\n\u003cp\u003eFor Questions/Bugs Email me at \u003ca href=\"mailto:luke.gundry1@monash.edu\"\u003eluke.gundry1@monash.edu\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "baysian-inference",
      "optimization",
      "voltammetry",
      "electrochemistry"
    ],
    "updated_at": 1621949924.0
  },
  {
    "data_format": 2,
    "description": "Custom implementation of neurodocker (https://github.com/ReproNim/neurodocker)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "achennings/neurodocker",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-neurodocker\" class=\"anchor\" href=\"#neurodocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eneurodocker\u003c/h1\u003e\n\u003cp\u003eCustom implementation of neurodocker (\u003ca href=\"https://github.com/ReproNim/neurodocker\"\u003ehttps://github.com/ReproNim/neurodocker\u003c/a\u003e)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621991816.0
  },
  {
    "data_format": 2,
    "description": "Applied nuclear physics relevant software, containerized.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "peter-jansson/appnuc",
    "latest_release": "v0.2.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-appnuc\" class=\"anchor\" href=\"#appnuc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eappnuc\u003c/h1\u003e\n\u003cp\u003eApplied nuclear physics relevant software.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/f400597fcdcb66eeb5702e037732d66d7eecdf94f4f363a2dde0da21c4ba9ec4/68747470733a2f2f7777772e676e752e6f72672f67726170686963732f6c67706c76332d776974682d746578742d3135347836382e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f400597fcdcb66eeb5702e037732d66d7eecdf94f4f363a2dde0da21c4ba9ec4/68747470733a2f2f7777772e676e752e6f72672f67726170686963732f6c67706c76332d776974682d746578742d3135347836382e706e67\" alt=\"LGPL-3\" data-canonical-src=\"https://www.gnu.org/graphics/lgplv3-with-text-154x68.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn Ubuntu based image/container with a bunch of standard programs that are useful for scientific work in the field of applied nuclear physics. In addition, the following list of relevant software is installed.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://geant4.web.cern.ch/\" rel=\"nofollow\"\u003eGeant4\u003c/a\u003e monte carlo framework\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://root.cern.ch/\" rel=\"nofollow\"\u003eRoot\u003c/a\u003e data analysis framework\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://dx.doi.org/10.18434/T48G6X\" rel=\"nofollow\"\u003eXCOM\u003c/a\u003e program from NIST\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h2\u003e\n\u003cp\u003eDocker hub contains the \u003ca href=\"https://hub.docker.com/r/jansson/appnuc\" rel=\"nofollow\"\u003eimage\u003c/a\u003e built using the Dockerfile, which can pulled into the local Docker registry by the command \u003ccode\u003edocker pull jansson/appnuc\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe image can be started in a container by, e.g., the command \u003ccode\u003edocker run --rm -it jansson/appnuc bash -l\u003c/code\u003e. Significantly more information on how to mount a local file system to the container as well as other command line options is available in the \u003ca href=\"https://docs.docker.com/engine/reference/commandline/cli/\" rel=\"nofollow\"\u003eDocker documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h2\u003e\n\u003cp\u003eA \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e file containing the same containerized Ubuntu and software can be built using the Singularity definition file, named \u003ccode\u003eSingularity\u003c/code\u003e. E.g. using the command \u003ccode\u003esudo singularity build appnuc.sif Singularity\u003c/code\u003e to build \u003ccode\u003eappnuc.sif\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href=\"https://sylabs.io/guides/3.7/user-guide/\" rel=\"nofollow\"\u003eSingularity user guide\u003c/a\u003e for more information.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "applied-nuclear-physics",
      "dockerfile",
      "singularity"
    ],
    "updated_at": 1621853660.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/bedpost-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003eRuns FSL\u0027s bedpostx on the input DWI data set, and creates a PDF report of the results.\nQuite simple - see /opt/src/pipeline.sh for the main script.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621826090.0
  },
  {
    "data_format": 2,
    "description": "An annotation pipeline using snakemake",
    "filenames": [
      "Singularity/Singularity.def"
    ],
    "full_name": "jtevns/AnnotationPipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-annotationpipeline\" class=\"anchor\" href=\"#annotationpipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnnotationPipeline\u003c/h1\u003e\n\u003cp\u003eAn annotation pipeline using snakemake that curently supports pfam, eggnog, tigrfam annotations.\u003c/p\u003e\n\u003cp\u003eTo run:\u003cbr\u003e\n-Create a directory for annotation\u003cbr\u003e\n-Create a folder and put your original fasta files in\n- module load python3.7-anaconda\n-Run filtering script on a directory of bins\u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0-This is required to filter out bins that do not meet the length requirement for running prodigal (20000 bases and prep the directory for the pipeline)\u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0- Run: \u003ccode\u003e/nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/scripts/prep_annotation.py folder_of_fastas\u003c/code\u003e\u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0- the script results in the creation of a directory called Passing_Bins in the directory the script is run from with all\n\u00a0\u00a0\u00a0\u00a0bins linked in the directory that meet the length threshold.\u003cbr\u003e\n- copy the config template from /nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/config_template.yaml and name it config.yaml.\u003cbr\u003e\n- use the file extension in the Passing_Bins dir (fa or fna or fasta etc).\u003cbr\u003e\n- module load singularity.\u003cbr\u003e\n-Run Annotation.smk  (I recommend putting this in a slurm script with 36 cores and 180gb mem for fastest run time).\u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0 \u003ccode\u003esingularity exec /nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/Singularity/annotation_tools.sif snakemake -s /nfs/turbo/lsa-dudelabs/pipelines/AnnotationPipeline/Annotation.smk --cores\u003c/code\u003e\u003cbr\u003e\nNote on headers:\nfasta headers cannot contain \":\"\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621642145.0
  },
  {
    "data_format": 2,
    "description": "Files to support building and maintenance of Singularity containers for Hall D",
    "filenames": [
      "recipes/Singularity.ubuntu.bionic-20210222",
      "recipes/Singularity.ubuntu.focal-20200925",
      "recipes/Singularity.centos-6.10",
      "recipes/Singularity.centos-8.2.2004",
      "recipes/Singularity.markito3.gluex_docker_devel",
      "recipes/Singularity.fedora-33",
      "recipes/Singularity.centos-7.7.1908",
      "recipes/Singularity.ubuntu.xenial-20210114",
      "recipes/Singularity.markito3-gluex_docker_prod",
      "recipes/Singularity.fedora-32"
    ],
    "full_name": "JeffersonLab/hd_singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hd_singularity\" class=\"anchor\" href=\"#hd_singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehd_singularity\u003c/h1\u003e\n\u003cp\u003eFiles to support building and maintenance of Singularity containers for Hall D.\u003c/p\u003e\n\u003cp\u003eContains scripts and recipes for creating Singularity containers from scratch.\u003c/p\u003e\n\u003cp\u003eThe main script is scripts/create_gluex_container.sh. Its usage message is as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUsage: create_gluex_container.sh [-h] -r \u0026lt;recipe-file\u0026gt; -p \u0026lt;prereqs-script\u0026gt; \\\n       [-d DIRECTORY] [-t STRING]\n\nNote: must be run as root\n\nOptions:\n  -h print this usage message\n  -r Singularity recipe file\n  -p script that installs gluex software\n  -d output directory for containers (default: current working directory)\n  -t token to be used to name containers (default = extension in \"Singularity.ext\")\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 8,
    "topics": [],
    "updated_at": 1621622436.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity",
      "container/Singularity_madeline2"
    ],
    "full_name": "Clinical-Genomics-Lund/nextflow_wgs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hd_singularity\" class=\"anchor\" href=\"#hd_singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehd_singularity\u003c/h1\u003e\n\u003cp\u003eFiles to support building and maintenance of Singularity containers for Hall D.\u003c/p\u003e\n\u003cp\u003eContains scripts and recipes for creating Singularity containers from scratch.\u003c/p\u003e\n\u003cp\u003eThe main script is scripts/create_gluex_container.sh. Its usage message is as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUsage: create_gluex_container.sh [-h] -r \u0026lt;recipe-file\u0026gt; -p \u0026lt;prereqs-script\u0026gt; \\\n       [-d DIRECTORY] [-t STRING]\n\nNote: must be run as root\n\nOptions:\n  -h print this usage message\n  -r Singularity recipe file\n  -p script that installs gluex software\n  -d output directory for containers (default: current working directory)\n  -t token to be used to name containers (default = extension in \"Singularity.ext\")\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1621964896.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.snowflake"
    ],
    "full_name": "longgangfan/ubuntu2004uwgeo-sig",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ubuntu2004uwgeo-sig\" class=\"anchor\" href=\"#ubuntu2004uwgeo-sig\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eubuntu2004uwgeo-sig\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621608269.0
  },
  {
    "data_format": 2,
    "description": "Global Alignment using Deep Learning",
    "filenames": [
      "Singularity"
    ],
    "full_name": "InigoMoreno/deep_ga",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-deep-ga\" class=\"anchor\" href=\"#deep-ga\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edeep-ga\u003c/h1\u003e\n\u003cp\u003eGlobal Alignment using Deep Learning\u003c/p\u003e\n\u003cp\u003eFor the moment this is just a python package with some helper functions\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h1\u003e\n\u003cp\u003eThis will try to install tensorflow, for this to work, you need python 3.6-3.8 (see \u003ca href=\"https://www.tensorflow.org/install\" rel=\"nofollow\"\u003ehttps://www.tensorflow.org/install\u003c/a\u003e)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m pip install --upgrade pip\npython -m pip install git+https://github.com/InigoMoreno/deep_ga\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621892077.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "bioconductor_full/Singularity",
      "bioconductor_full/Singularity.RELEASE_3_8",
      "bioconductor_full/Singularity.RELEASE_3_10",
      "bioconductor_full/Singularity.RELEASE_3_9",
      "bioconductor_docker/Singularity"
    ],
    "full_name": "ahalfpen727/Docker-Resources",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-bioconductor-devel-docker-image\" class=\"anchor\" href=\"#bioconductor-devel-docker-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBioconductor Devel Docker image\u003c/h1\u003e\n\u003cp\u003eBioconductor Docker image with full set of system dependencies so that\nall Bioconductor packages can be installed.\u003c/p\u003e\n\u003cp\u003eThe Docker images have R and Bioconductor with different versions\nunder each \"branch\" in git.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e: Docker image for bioconductor_full:devel is in the \u003ccode\u003emaster\u003c/code\u003e\nbranch, and all the release branches will be under the branch\n\u003ccode\u003eRELEASE_X_Y\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eImportant Links:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/bioconductor/bioconductor_full\" rel=\"nofollow\"\u003eDocker hub link for bioconductor_full\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Bioconductor/bioconductor_full\"\u003eGithub development link for bioconductor_full\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-advantages-of-the-bioconductor_full-docker-image\" class=\"anchor\" href=\"#advantages-of-the-bioconductor_full-docker-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvantages of the \u003ccode\u003ebioconductor_full\u003c/code\u003e docker image\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThe bioconductor_full docker images can be used instead of\ninstalling complex dependencies needed for Bioconductor\npackages. The image comes with most of the dependencies installed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eQuick start up to start your analysis with all the Bioconductor\npackages if needed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe image will be regularly updated to reflect the build system on\nBioconductor. This is a very useful resource for maintainers who\nare actively developing their package to see if it works in tandem\nwith the bioconductor ecosystem. It provides a local testing outlet\nfor maintainers and developers.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-and-quick-start\" class=\"anchor\" href=\"#installation-and-quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation and quick start\u003c/h2\u003e\n\u003cp\u003eThis document assumes you have \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003edocker\u003c/a\u003e installed. Please check\n\u003ca href=\"https://www.docker.com/products/docker-desktop\" rel=\"nofollow\"\u003einstallation\u003c/a\u003e if you have more questions regarding this.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eStart docker on your machine.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOn the command line, \"pull\" the bioconductor_full docker image with\nthe correct tag. These images are hosted on docker hub under the\n\u003ca href=\"https://cloud.docker.com/u/bioconductor/repository/registry-1.docker.io/bioconductor/bioconductor_full\" rel=\"nofollow\"\u003eBioconductor organization\u003c/a\u003e page.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker pull bioconductor/bioconductor_full:devel\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker pull bioconductor/bioconductor_full:RELEASE_X_Y\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOnce the image is available on your local machine, you can check to\nsee if they are available.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker images\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo start using these images with RStudio, this will start the image\nunder the \u0027rstudio\u0027 user\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run                                      \\\n      -e PASSWORD=your_password                   \\\n      -p 8787:8787                                \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library \\\n      bioconductor/bioconductor_full:devel\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo start the image interactively using the \u003ccode\u003ebioc\u003c/code\u003e user\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run                                     \\\n      -it                                        \\\n      --user bioc                                \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library \\\n      bioconductor/bioconductor_full:devel       \\\n      R\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE: The path \u003ccode\u003e/usr/local/lib/R/host-site-library\u003c/code\u003e is mapped to\n\u003ccode\u003e.libPaths()\u003c/code\u003e in R. So, when R is started, all the libraries in\nthe host directory, \u003ccode\u003ehost-site-library\u003c/code\u003e are available to R. It is\nstored on your machine mounted from the volume you fill in place\nof \u003ccode\u003ehost-site-library\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThese libraries will only work if they are pre-compiled with the\nsame version of R that is in the docker image. To explain further,\nyou would need the packages built with Bioconductor version \u00273.9\u0027\nto work with R-3.6. Similarly, you\u0027d need Bioconductor version\n\u00273.9\u0027 to work with R-3.6.z.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo start the docker image in deamon mode, i.e, have the container\nrunning in the background use the \u003ccode\u003e-d\u003c/code\u003e option.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  sudo docker run -it                            \\\n      -d                                         \\\n      -v host-site-library:/usr/local/lib/R/host-site-library \\\n      --entrypoint /bin/bash                     \\\n      bioconductor/bioconductor_full:devel\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will start the container in the background and keep it\nrunning. You may check the running processes using \u003ccode\u003edocker ps\u003c/code\u003e,\nand copy the container id.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker ps\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo attach to a container which is running in the background\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker exec -it \u0026lt;container_id\u0026gt; bash\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE: You can replace \u003ccode\u003ebash\u003c/code\u003e with R to start R directly in the\ncontainer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker exec -it \u0026lt;container_id\u0026gt; R\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo run multiple RStudio instances, use a different external port\nmapping (the first port in \u003ccode\u003e-p XXXX:YYYY\u003c/code\u003e) for each instance.\nUse standard shell commands (e.g., adding a \u003ccode\u003e\u0026amp;\u003c/code\u003e at the end of the\nfirst docker command) to place docker processes in the\nbackground. The \u0027devel\u0027 instance will be available at\n\u003ca href=\"http://localhost:8787\" rel=\"nofollow\"\u003ehttp://localhost:8787\u003c/a\u003e, and the release image at\n\u003ca href=\"http://localhost:8788\" rel=\"nofollow\"\u003ehttp://localhost:8788\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run                                      \\\n      -e PASSWORD=your_password                   \\\n      -p 8787:8787                                \\\n      bioconductor/bioconductor_full:devel\n\n  docker run                                      \\\n      -e PASSWORD=your_password                   \\\n      -p 8788:8787                                \\\n      bioconductor/bioconductor_full:RELEASE_3_10\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-latex-inside-the-container\" class=\"anchor\" href=\"#using-latex-inside-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eusing LaTeX inside the container?\u003c/h3\u003e\n\u003cp\u003eInstall the \u003ccode\u003etinytex\u003c/code\u003e package (\u003ca href=\"https://yihui.name/tinytex/\" rel=\"nofollow\"\u003ehttps://yihui.name/tinytex/\u003c/a\u003e) which has helpers for installing LaTeX functionality.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003einstall.packages(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003etinytex\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-e\"\u003etinytex\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_tinytex()\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621559866.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "latest--rstudio1.2.5042r362/Singularity"
    ],
    "full_name": "yh549848/singularity-rstudio-methylseq",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-bioconductor-devel-docker-image\" class=\"anchor\" href=\"#bioconductor-devel-docker-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBioconductor Devel Docker image\u003c/h1\u003e\n\u003cp\u003eBioconductor Docker image with full set of system dependencies so that\nall Bioconductor packages can be installed.\u003c/p\u003e\n\u003cp\u003eThe Docker images have R and Bioconductor with different versions\nunder each \"branch\" in git.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e: Docker image for bioconductor_full:devel is in the \u003ccode\u003emaster\u003c/code\u003e\nbranch, and all the release branches will be under the branch\n\u003ccode\u003eRELEASE_X_Y\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eImportant Links:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/bioconductor/bioconductor_full\" rel=\"nofollow\"\u003eDocker hub link for bioconductor_full\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Bioconductor/bioconductor_full\"\u003eGithub development link for bioconductor_full\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-advantages-of-the-bioconductor_full-docker-image\" class=\"anchor\" href=\"#advantages-of-the-bioconductor_full-docker-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvantages of the \u003ccode\u003ebioconductor_full\u003c/code\u003e docker image\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThe bioconductor_full docker images can be used instead of\ninstalling complex dependencies needed for Bioconductor\npackages. The image comes with most of the dependencies installed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eQuick start up to start your analysis with all the Bioconductor\npackages if needed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe image will be regularly updated to reflect the build system on\nBioconductor. This is a very useful resource for maintainers who\nare actively developing their package to see if it works in tandem\nwith the bioconductor ecosystem. It provides a local testing outlet\nfor maintainers and developers.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-and-quick-start\" class=\"anchor\" href=\"#installation-and-quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation and quick start\u003c/h2\u003e\n\u003cp\u003eThis document assumes you have \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003edocker\u003c/a\u003e installed. Please check\n\u003ca href=\"https://www.docker.com/products/docker-desktop\" rel=\"nofollow\"\u003einstallation\u003c/a\u003e if you have more questions regarding this.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eStart docker on your machine.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOn the command line, \"pull\" the bioconductor_full docker image with\nthe correct tag. These images are hosted on docker hub under the\n\u003ca href=\"https://cloud.docker.com/u/bioconductor/repository/registry-1.docker.io/bioconductor/bioconductor_full\" rel=\"nofollow\"\u003eBioconductor organization\u003c/a\u003e page.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker pull bioconductor/bioconductor_full:devel\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker pull bioconductor/bioconductor_full:RELEASE_X_Y\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOnce the image is available on your local machine, you can check to\nsee if they are available.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker images\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo start using these images with RStudio, this will start the image\nunder the \u0027rstudio\u0027 user\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run                                      \\\n      -e PASSWORD=your_password                   \\\n      -p 8787:8787                                \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library \\\n      bioconductor/bioconductor_full:devel\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo start the image interactively using the \u003ccode\u003ebioc\u003c/code\u003e user\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run                                     \\\n      -it                                        \\\n      --user bioc                                \\\n      -v ~/host-site-library:/usr/local/lib/R/host-site-library \\\n      bioconductor/bioconductor_full:devel       \\\n      R\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE: The path \u003ccode\u003e/usr/local/lib/R/host-site-library\u003c/code\u003e is mapped to\n\u003ccode\u003e.libPaths()\u003c/code\u003e in R. So, when R is started, all the libraries in\nthe host directory, \u003ccode\u003ehost-site-library\u003c/code\u003e are available to R. It is\nstored on your machine mounted from the volume you fill in place\nof \u003ccode\u003ehost-site-library\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThese libraries will only work if they are pre-compiled with the\nsame version of R that is in the docker image. To explain further,\nyou would need the packages built with Bioconductor version \u00273.9\u0027\nto work with R-3.6. Similarly, you\u0027d need Bioconductor version\n\u00273.9\u0027 to work with R-3.6.z.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo start the docker image in deamon mode, i.e, have the container\nrunning in the background use the \u003ccode\u003e-d\u003c/code\u003e option.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  sudo docker run -it                            \\\n      -d                                         \\\n      -v host-site-library:/usr/local/lib/R/host-site-library \\\n      --entrypoint /bin/bash                     \\\n      bioconductor/bioconductor_full:devel\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will start the container in the background and keep it\nrunning. You may check the running processes using \u003ccode\u003edocker ps\u003c/code\u003e,\nand copy the container id.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker ps\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo attach to a container which is running in the background\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker exec -it \u0026lt;container_id\u0026gt; bash\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE: You can replace \u003ccode\u003ebash\u003c/code\u003e with R to start R directly in the\ncontainer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker exec -it \u0026lt;container_id\u0026gt; R\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo run multiple RStudio instances, use a different external port\nmapping (the first port in \u003ccode\u003e-p XXXX:YYYY\u003c/code\u003e) for each instance.\nUse standard shell commands (e.g., adding a \u003ccode\u003e\u0026amp;\u003c/code\u003e at the end of the\nfirst docker command) to place docker processes in the\nbackground. The \u0027devel\u0027 instance will be available at\n\u003ca href=\"http://localhost:8787\" rel=\"nofollow\"\u003ehttp://localhost:8787\u003c/a\u003e, and the release image at\n\u003ca href=\"http://localhost:8788\" rel=\"nofollow\"\u003ehttp://localhost:8788\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run                                      \\\n      -e PASSWORD=your_password                   \\\n      -p 8787:8787                                \\\n      bioconductor/bioconductor_full:devel\n\n  docker run                                      \\\n      -e PASSWORD=your_password                   \\\n      -p 8788:8787                                \\\n      bioconductor/bioconductor_full:RELEASE_3_10\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-latex-inside-the-container\" class=\"anchor\" href=\"#using-latex-inside-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eusing LaTeX inside the container?\u003c/h3\u003e\n\u003cp\u003eInstall the \u003ccode\u003etinytex\u003c/code\u003e package (\u003ca href=\"https://yihui.name/tinytex/\" rel=\"nofollow\"\u003ehttps://yihui.name/tinytex/\u003c/a\u003e) which has helpers for installing LaTeX functionality.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003einstall.packages(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003etinytex\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-e\"\u003etinytex\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_tinytex()\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621558877.0
  },
  {
    "data_format": 2,
    "description": "Single cell Nextflow cellbender pipeline.",
    "filenames": [
      "env/Singularity.preprocessing"
    ],
    "full_name": "wtsi-hgi/nf_cellbender",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h1\u003e\n\u003cp\u003eThe methods used in this module are described in \u003ccode\u003edocs/methods.pdf\u003c/code\u003e. TODO: \u003ccode\u003edocs/methods.pdf\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eBelow is the structure of the results directory. The values that will be listed in \u003ccode\u003edescription_of_params\u003c/code\u003e within the directory structure correspond to the various parameters one can set. An example of a paramters file is found in \u003ccode\u003eexample_runtime_setup/params.yml\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enf-qc_cluster\n\u251c\u2500\u2500 normalization_001::description_of_params\n\u2502   \u251c\u2500\u2500 [files: data]\n\u2502   \u251c\u2500\u2500 reduced_dims-pca::description_of_params\n\u2502   \u2502   \u251c\u2500\u2500 [files: data]\n\u2502   \u2502   \u251c\u2500\u2500 [plots: umap]\n\u2502   \u2502   \u251c\u2500\u2500 cluster_001::description_of_params\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 [files: data,clusters]\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 [plots: umap]\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cluster_markers_001::description_of_params\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 [files: cluster_marker_genes]\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 [plots: marker_genes,marker_genes_dotplot]\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cluster_markers_002::description_of_params\n\u2502   \u2502   \u2502   ... etc. ...\n\u2502   \u2502   \u251c\u2500\u2500 cluster_002::description_of_params\n\u2502   \u2502   ... etc. ...\n\u2502   \u251c\u2500\u2500 reduced_dims-harmony_001::description_of_params\n\u2502   \u251c\u2500\u2500 reduced_dims-harmony_002::description_of_params\n\u2502   ... etc. ...\n\u251c\u2500\u2500 normalization_002::description_of_norm_params\n... etc. ...\n\u2514\u2500\u2500 adata.h5  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e concatenated single cell data with no normalization\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-todo-list\" class=\"anchor\" href=\"#todo-list\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTODO list\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eAdd \u003ccode\u003edocs/methods.pdf\u003c/code\u003e file.\u003c/li\u003e\n\u003cli\u003eAdd brief description of module.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-enhancement-list\" class=\"anchor\" href=\"#enhancement-list\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnhancement list\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003escanpy_merge-dev.py\u003c/code\u003e: If it were important to have a per sample filter, merge could be re-designed to accommodate this.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003escanpy_cluster.py\u003c/code\u003e: Currently for clustering, we can change method (leiden or louvain), resolution, and n_pcs. Are there other parameters that need to be scaled over?\u003c/li\u003e\n\u003cli\u003eCheck phenotypes against predicted sex from gene expression.\u003c/li\u003e\n\u003cli\u003eAdd basic QC plots - try to do this in R from anndata frame?\u003c/li\u003e\n\u003cli\u003eScrublet functionality + add to metadata + cluster distributions\u003c/li\u003e\n\u003cli\u003eGene scores + add to metadata\u003c/li\u003e\n\u003cli\u003eAdd marker gene AUC like here \u003ca href=\"http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters\" rel=\"nofollow\"\u003ehttp://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eAdd summary ARI and LISI metrics computed over a list of many different cluster annotations?\u003c/li\u003e\n\u003cli\u003eAdd tSNE plots - rapid plots with OpenTSNE?\u003c/li\u003e\n\u003cli\u003eCalculate marker genes with diffxpy or logreg?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h1\u003e\n\u003cp\u003eQuickstart for deploying this pipeline locally and on a high performance compute cluster.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-1-set-up-the-environment\" class=\"anchor\" href=\"#1-set-up-the-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Set up the environment\u003c/h2\u003e\n\u003cp\u003eInstall the required packages via conda:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e The repo directory.\u003c/span\u003e\nREPO_MODULE=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${HOME}\u003c/span\u003e/repo/path/to/this/pipeline\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Install environment using Conda.\u003c/span\u003e\nconda env create --name sc_qc_cluster --file \u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/env/environment.yml\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Activate the new Conda environment.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e activate sc_qc_cluster\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e To update environment file:\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003econda env export --no-builds | grep -v prefix | grep -v name \u0026gt; environment.yml\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-prepare-the-input-files\" class=\"anchor\" href=\"#2-prepare-the-input-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Prepare the input files\u003c/h2\u003e\n\u003cp\u003eGenerate and/or edit input files for the pipeline.\u003c/p\u003e\n\u003cp\u003eThe pipeline takes as input:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cstrong\u003e--file_paths_10x\u003c/strong\u003e:  Tab-delimited file containing experiment_id and data_path_10x_format columns (i.e., list of input samples). Reqired.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--file_metadata\u003c/strong\u003e:  Tab-delimited file containing sample metadata. This will automatically be subset down to the sample list from 1. Reqired.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--file_sample_qc\u003c/strong\u003e:  YAML file containing sample qc and filtering parameters. Optional. NOTE: in the example config file, this is part of the YAML file for \u003ccode\u003e-params-file\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--genes_exclude_hvg\u003c/strong\u003e:  Tab-delimited file with genes to exclude from\nhighly variable gene list. Must contain ensembl_gene_id column. Optional.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--genes_score\u003c/strong\u003e:  Tab-delimited file with genes to use to score cells. Must contain ensembl_gene_id and score_idvcolumns. If one score_id == \"cell_cycle\", then requires a grouping_id column with \"G2/M\" and \"S\" (see example file in \u003ccode\u003eexample_runtime_setup\u003c/code\u003e). Optional.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e-params-file\u003c/strong\u003e:  YAML file containing analysis parameters. Optional.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--run_multiplet\u003c/strong\u003e:  Flag to run multiplet analysis. Optional.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--file_cellmetadata\u003c/strong\u003e:  Tab-delimited file containing experiment_id and data_path_cellmetadata columns. For instance this file can be used to pass per cell doublet annotations. Optional.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eExamples of all of these files can be found in \u003ccode\u003eexample_runtime_setup/\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-3-set-up-and-run-nextflow\" class=\"anchor\" href=\"#3-set-up-and-run-nextflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Set up and run Nextflow\u003c/h2\u003e\n\u003cp\u003eRun Nexflow locally (NOTE: if running on a virtual machine you may need to set \u003ccode\u003eexport QT_QPA_PLATFORM=\"offscreen\"\u003c/code\u003e for scanpy as described \u003ca href=\"https://github.com/ipython/ipython/issues/10627\"\u003ehere\u003c/a\u003e):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Boot up tmux session.\u003c/span\u003e\ntmux new -s nf\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Here we are not going to filter any variable genes, so don\u0027t pass a file.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: All input file paths should be full paths.\u003c/span\u003e\nnextflow run \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/main.nf\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    -profile \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003elocal\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --file_paths_10x \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/file_paths_10x.tsv\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --file_metadata \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/file_metadata.tsv\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --genes_score \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/genes_score_v001.tsv\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    -params-file \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/params.yml\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eRun Nextflow using LSF on a compute cluster. More on bgroups \u003ca href=\"https://www.ibm.com/support/knowledgecenter/SSETD4_9.1.3/lsf_config_ref/lsb.params.default_jobgroup.5.html\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Set the results directory.\u003c/span\u003e\nRESULTS_DIR=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/path/to/results/dir\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nmkdir -p \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${RESULTS_DIR}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Boot up tmux session.\u003c/span\u003e\ntmux new -s nf\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Log into an interactive session.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: Here we set the -G parameter due to our institute\u0027s LSF configuration.\u003c/span\u003e\nbgadd \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/logins\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nbsub -q normal -G team152 -g /\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/logins -Is -XF -M 8192 -R \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eselect[mem\u0026gt;8192] rusage[mem=8192]\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e /bin/bash\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: If you are running over many cells, you may need to start an\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e interactive session on a queue that allows long jobs\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003ebsub -q long -G team152 -g /${USER}/logins -Is -XF -M 18192 -R \"select[mem\u0026gt;18192] rusage[mem=18192]\" /bin/bash\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Activate the Conda environment (inherited by subsequent jobs).\u003c/span\u003e\nconda activate sc_qc_cluster\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Set up a group to submit jobs to (export a default -g parameter).\u003c/span\u003e\nbgadd -L 500 \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/nf\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LSB_DEFAULT_JOBGROUP=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/nf\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Depending on LSF setup, you may want to export a default -G parameter.\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LSB_DEFAULTGROUP=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eteam152\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: By setting the above flags, all of the nextflow LSF jobs will have\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e these flags set.\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Settings for scanpy (see note above).\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e QT_QPA_PLATFORM=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eoffscreen\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Change to a temporary runtime dir on the node. In this demo, we will change\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e to the same execution directory.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e${RESULTS_DIR}\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Remove old logs and nextflow output (if one previously ran nextflow in this\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e dir).\u003c/span\u003e\nrm -r \u003cspan class=\"pl-k\"\u003e*\u003c/span\u003ehtml\u003cspan class=\"pl-k\"\u003e;\u003c/span\u003e\nrm .nextflow.log\u003cspan class=\"pl-k\"\u003e*\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: If you want to resume a previous workflow, add -resume to the flag.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: If you do not want to filter any variable genes, pass an empty file to\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e       --genes_exclude_hvg. See previous local example.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: --output_dir should be a full path - not relative.\u003c/span\u003e\nnextflow run \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/main.nf\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    -profile \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003elsf\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --file_paths_10x \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/file_paths_10x.tsv\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --file_metadata \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/file_metadata.tsv\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --file_sample_qc \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/params.yml\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --genes_exclude_hvg \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/genes_remove_hvg_v001.tsv\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --genes_score \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/genes_score_v001.tsv\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --output_dir \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${RESULTS_DIR}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --run_multiplet \\\n    -params-file \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${REPO_MODULE}\u003c/span\u003e/example_runtime_setup/params.yml\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    -with-report \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enf_report.html\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    -resume\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e NOTE: If you would like to see the ongoing processes, look at the log files.\u003c/span\u003e\ncat .nextflow.log\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample of how one may sync results:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eNF_OUT_DIR=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/path/to/out_dir\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nrsync -am --include=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e*.png\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --include=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e*/\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --exclude=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e*\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e my_cluster_ssh:\u003cspan class=\"pl-smi\"\u003e${NF_OUT_DIR}\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\nrsync -am --include=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e*.png\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --include=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e*/\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --exclude=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e*\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e my_cluster_ssh:\u003cspan class=\"pl-smi\"\u003e${NF_OUT_DIR}\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eOn 10 April 2020, we found nextflow was writing some output into the \u003ccode\u003e${HOME}\u003c/code\u003e directory and had used up the alotted ~15Gb on the Sanger farm. This resulted in a Java error as soon as a nextflow command was executed. Based on file sizes within \u003ccode\u003e${HOME}\u003c/code\u003e, it seemed like the ouput was being written within the conda environment (following \u003ccode\u003edu -h | sort -V -k 1\u003c/code\u003e). By deleting and re-installing the coda environment, the problem was solved. The below flags may help prevent this from the future. In addition, setting the flag \u003ccode\u003eexport JAVA_OPTIONS=-Djava.io.tmpdir=/path/with/enough/space/\u003c/code\u003e may also help.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e To be run from the execution dir, before the above nextflow command\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e If you are running this on a cluster, make sure you log into an interactive\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e session with \u0026gt;25Gb of RAM.\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e NXF_OPTS=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e-Xms25G -Xmx25G\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e NXF_HOME=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e NXF_WORK=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${NXF_HOME}\u003c/span\u003e/.nexflow_work\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e NXF_TEMP=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${NXF_HOME}\u003c/span\u003e/.nexflow_temp\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e NXF_CONDA_CACHEDIR=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${NXF_HOME}\u003c/span\u003e/.nexflow_conda\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e NXF_SINGULARITY_CACHEDIR=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${NXF_HOME}\u003c/span\u003e/.nexflow_singularity\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1621547413.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.lolcow"
    ],
    "full_name": "pmitev/SSC-singularity-build",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ssc-singularity-build\" class=\"anchor\" href=\"#ssc-singularity-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSSC-singularity-build\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621541256.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "arxiv/Singularity.ub16.04-tf1.10.1-torch0.4.1-root6.14.04",
      "arxiv/Singularity.ubuntu16.04-larcv_develop",
      "arxiv/Singularity.ub16.04-tf1.11.0-torch0.4.1-root6.14.04",
      "arxiv/Singularity.ub16.04-tf1.11.0-torch0.4.1",
      "arxiv/Singularity.ubuntu16.04-gpu",
      "arxiv/Singularity.ub16.04-tf1.10.1-torch0.4.1",
      "arxiv/Singularity.ubuntu16.04-gpu-larcv_develop",
      "arxiv/Singularity.ubuntu16.04-basic",
      "arxiv/Singularity.ubuntu16.04-gpu-py3",
      "arxiv/Singularity.ub16.04-tf1.7-torch0.4",
      "recipes/Singularity.ub16.04-cuda100-pytorch1.0.0-scn",
      "recipes/Singularity.ub16.04-cuda90-pytorch0.4.1",
      "recipes/Singularity",
      "recipes/Singularity.ub18.04-gpu-ana0-ml-larcv2",
      "recipes/Singularity.ub18.04-gpu-ana0-ml",
      "recipes/Singularity.ub16.04-cuda90-pytorch1.0.0-scn",
      "recipes/Singularity.ub16.04-cuda90-tf1.12.0",
      "recipes/Singularity.ub18.04-cuda10.2-extra",
      "recipes/Singularity.ub16.04-cuda100-pytorchdev20181215",
      "recipes/Singularity.HKMLWorkshop",
      "recipes/Singularity.ub18.04-gpu-ana0",
      "recipes/Singularity.ub18.04-gpu-ana0-mn",
      "recipes/Singularity.ub18.04-cpu-ana0",
      "recipes/Singularity.ub16.04-cuda90-py3-pytorch1.0.1-scn",
      "recipes/Singularity.ub18.04-cuda10.1-ml-larcv2",
      "recipes/Singularity.ub16.04-cuda90-pytorchdev20181015",
      "recipes/Singularity.ub18.04-cuda100-py3-pytorch1.1.0-scn-docker",
      "recipes/Singularity.ub18.04-cpu",
      "recipes/Singularity.ub18.04-gpu",
      "recipes/Singularity.ub18.04-cuda100-py3-pytorch1.1.0-scn",
      "recipes/Singularity.ub18.04-cpu-ana0-larcv2",
      "recipes/Singularity.ub18.04-cuda100-py3-pytorch1.0.1-scn"
    ],
    "full_name": "DeepLearnPhysics/larcv2-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://raw.githubusercontent.com/DeepLearnPhysics/larcv2-singularity/master/LICENSE\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2ff6a06f2f6e08b17783133ca7ebc23ce1f8ac4415eee8e835647b57048a8f0d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e737667\" alt=\"license\" data-canonical-src=\"https://img.shields.io/github/license/mashape/apistatus.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/459\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-larcv2-singularity\" class=\"anchor\" href=\"#larcv2-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elarcv2-singularity\u003c/h1\u003e\n\u003cp\u003eSingularity build scripts for \u003ca href=\"https://www.singularity-hub.org/collections/459\" rel=\"nofollow\"\u003esingularity hub\u003c/a\u003e. You can learn about Singularity in \u003ca href=\"https://github.com/DeepLearnPhysics/playground-singularity/wiki\"\u003eour wiki\u003c/a\u003e or \u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eofficial doc\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo \u003ccode\u003epull\u003c/code\u003e the container, simply try\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTAG=latest\nsingularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:$TAG\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more fun things to do, you can read \u003ca href=\"https://github.com/DeepLearnPhysics/playground-singularity/wiki\"\u003eour wiki\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-whats-in-the-build\" class=\"anchor\" href=\"#whats-in-the-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat\u0027s in the build?\u003c/h2\u003e\n\u003cp\u003eAll builds are based on \u003cstrong\u003eUbuntu16.04 LTS\u003c/strong\u003e with some highlighted packages below\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython packages: \u003ccode\u003epip\u003c/code\u003e \u003ccode\u003enumpy\u003c/code\u003e \u003ccode\u003escipy\u003c/code\u003e \u003ccode\u003escikit\u003c/code\u003e \u003ccode\u003eopencv-python\u003c/code\u003e \u003ccode\u003eh5py\u003c/code\u003e \u003ccode\u003etables\u003c/code\u003e \u003ccode\u003epandas\u003c/code\u003e \u003ccode\u003ematplotlib\u003c/code\u003e \u003ccode\u003eipython\u003c/code\u003e \u003ccode\u003ejupyter notebook\u003c/code\u003e \u003ccode\u003epyyaml\u003c/code\u003e \u003ccode\u003ezmq\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eDevelopment kit: \u003ccode\u003eg++\u003c/code\u003e/\u003ccode\u003egcc\u003c/code\u003e \u003ccode\u003elibqt4-dev\u003c/code\u003e \u003ccode\u003epython-dev\u003c/code\u003e \u003ccode\u003ecuda-9.0\u003c/code\u003e \u003ccode\u003ecudnn-7\u003c/code\u003e \u003ccode\u003ecython\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eUtility kit    : \u003ccode\u003egit\u003c/code\u003e \u003ccode\u003ewget\u003c/code\u003e \u003ccode\u003eemacs\u003c/code\u003e \u003ccode\u003evim\u003c/code\u003e \u003ccode\u003easciinema\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe build 3 types of images.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003eBase\u003c/em\u003e image\n\u003cul\u003e\n\u003cli\u003eLatest tag: \u003cstrong\u003eub16.04-tf1.10.1-torch0.4.1\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etensorflow-gpu\u003c/code\u003e 1.10.1, \u003ccode\u003epytorch\u003c/code\u003e 0.4.1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:ub16.04-tf1.10.1-torch0.4.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003eROOT\u003c/em\u003e image (include \u003cem\u003eBase\u003c/em\u003e)\n\u003cul\u003e\n\u003cli\u003eLatest tag: \u003cstrong\u003eub16.04-tf1.10-torch0.4.1-root6.14.04\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eROOT\u003c/code\u003e 6.14.04, additional python package \u003ccode\u003eroot_numpy\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:ub16.04-tf1.10.1-torch0.4.1-root6.14.04\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003eLArCV\u003c/em\u003e image (include \u003cem\u003eROOT\u003c/em\u003e)\n\u003cul\u003e\n\u003cli\u003eTag: \u003cstrong\u003elatest\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eAdditional python package \u003ccode\u003elarcv\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull -n local.img shub://DeepLearnPhysics/larcv2-singularity:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-docker-images\" class=\"anchor\" href=\"#docker-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker images?\u003c/h1\u003e\n\u003cp\u003eCheckout built images on our \u003ca href=\"https://hub.docker.com/u/deeplearnphysics/dashboard/\" rel=\"nofollow\"\u003edocker hub\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "larcv",
      "singularity",
      "singularity-hub"
    ],
    "updated_at": 1590751288.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "envs/illumina/Singularity"
    ],
    "full_name": "here0009/SarsCov2_Snakemake_Pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-sarscov2_snakemake_pipeline\" class=\"anchor\" href=\"#sarscov2_snakemake_pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSarsCov2_Snakemake_Pipeline\u003c/h1\u003e\n\u003cp\u003eThis is a snakemake pipeline used for analyse SarsCov2 sequence data generated by illumina machine.\nThis pipelien was based on \u003ca href=\"https://github.com/artic-network/fieldbioinformatics\"\u003eARTIC network\u0027s fieldbioinformatics tools\u003c/a\u003e, \u003ca href=\"https://github.com/dridk/Sars-CoV-2-NGS-pipeline\"\u003eSars-CoV-2-NGS-pipeline\u003c/a\u003e and \u003ca href=\"https://github.com/connor-lab/ncov2019-artic-nf\"\u003encov2019-artic-nf\u003c/a\u003e with some updates:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003efastqc\u003c/code\u003e and  was used to generate the qc report of input data.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003equast\u003c/code\u003e was used to generate the sequence assembly report.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/cov-lineages/pangolin\"\u003epangolin\u003c/a\u003e was used for the typing of SarsCov-2\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCorGat\u003c/code\u003e was used to annotate the sequence, and generate alle frequency reports\nYou need to clone \u003ca href=\"https://github.com/matteo14c/CorGAT\"\u003eCorGat\u003c/a\u003e and specify the directory in the config files.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emultiqc\u003c/code\u003e was used to generate the final report.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe workflow shows like below:\u003c/p\u003e\n\u003cp\u003eA test_data file was provided to test the pipeline.\nYou may test the pipeline by dry-run\n\u003ccode\u003esnakemake -s sars2.smk -n\u003c/code\u003e\nthen run the pipeline:\n\u003ccode\u003esnakemake -s sars2.smk -j 4 --use-conda\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eWARNING - THIS REPO IS UNDER ACTIVE DEVELOPMENT AND ITS BEHAVIOUR MAY CHANGE AT \u003cstrong\u003eANY\u003c/strong\u003e TIME.\u003c/p\u003e\n\u003cp\u003ePLEASE ENSURE THAT YOU READ BOTH THE README AND THE CONFIG FILE AND UNDERSTAND THE EFFECT OF THE OPTIONS ON YOUR DATA!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621527734.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "bc3.10--rstudio125042r362/Singularity",
      "bc3.12--rstudio125042r405/Singularity"
    ],
    "full_name": "yh549848/singularity-rstudio-rnaseqde",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-sarscov2_snakemake_pipeline\" class=\"anchor\" href=\"#sarscov2_snakemake_pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSarsCov2_Snakemake_Pipeline\u003c/h1\u003e\n\u003cp\u003eThis is a snakemake pipeline used for analyse SarsCov2 sequence data generated by illumina machine.\nThis pipelien was based on \u003ca href=\"https://github.com/artic-network/fieldbioinformatics\"\u003eARTIC network\u0027s fieldbioinformatics tools\u003c/a\u003e, \u003ca href=\"https://github.com/dridk/Sars-CoV-2-NGS-pipeline\"\u003eSars-CoV-2-NGS-pipeline\u003c/a\u003e and \u003ca href=\"https://github.com/connor-lab/ncov2019-artic-nf\"\u003encov2019-artic-nf\u003c/a\u003e with some updates:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003efastqc\u003c/code\u003e and  was used to generate the qc report of input data.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003equast\u003c/code\u003e was used to generate the sequence assembly report.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/cov-lineages/pangolin\"\u003epangolin\u003c/a\u003e was used for the typing of SarsCov-2\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCorGat\u003c/code\u003e was used to annotate the sequence, and generate alle frequency reports\nYou need to clone \u003ca href=\"https://github.com/matteo14c/CorGAT\"\u003eCorGat\u003c/a\u003e and specify the directory in the config files.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emultiqc\u003c/code\u003e was used to generate the final report.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe workflow shows like below:\u003c/p\u003e\n\u003cp\u003eA test_data file was provided to test the pipeline.\nYou may test the pipeline by dry-run\n\u003ccode\u003esnakemake -s sars2.smk -n\u003c/code\u003e\nthen run the pipeline:\n\u003ccode\u003esnakemake -s sars2.smk -j 4 --use-conda\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eWARNING - THIS REPO IS UNDER ACTIVE DEVELOPMENT AND ITS BEHAVIOUR MAY CHANGE AT \u003cstrong\u003eANY\u003c/strong\u003e TIME.\u003c/p\u003e\n\u003cp\u003ePLEASE ENSURE THAT YOU READ BOTH THE README AND THE CONFIG FILE AND UNDERSTAND THE EFFECT OF THE OPTIONS ON YOUR DATA!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621521929.0
  },
  {
    "data_format": 2,
    "description": "Workflow for prOteome and tRanScriptome functiOnal aNnotation. (Mirror from Ifremer\u0027s Gitlab).",
    "filenames": [
      "containers/Singularity.interproscan-5.51-85.0"
    ],
    "full_name": "ifremer-bioinformatics/orson",
    "latest_release": "v1.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-orson-workflow-for-proteome-and-transcriptome-functional-annotation\" class=\"anchor\" href=\"#orson-workflow-for-proteome-and-transcriptome-functional-annotation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eORSON: workflow for prOteome and tRanScriptome functiOnal aNnotation\u003c/strong\u003e.\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ifremer-bioinformatics/orson\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/68d7de18944473ac497abf10c6474d22ea7670f99506880609ed17327999e012/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f52534f4e25323076657273696f6e2d312e302e302d7265643f6c6162656c436f6c6f723d303030303030\" alt=\"ORSON version\" data-canonical-src=\"https://img.shields.io/badge/ORSON%20version-1.0.0-red?labelColor=000000\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a1e0f23194e457dfe0e30e723e75e3d231e038aa553636a5de68b76c877496ff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413532302e31302e302d3233616136322e7376673f6c6162656c436f6c6f723d303030303030\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A520.10.0-23aa62.svg?labelColor=000000\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0b0568e8f684f1ea04320511f0635c70c144cad7fb7daec19a8e605f02933b01/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72756e253230776974682d73696e67756c61726974792d3164333535632e7376673f6c6162656c436f6c6f723d303030303030\" alt=\"Run with with singularity\" data-canonical-src=\"https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://ifremer-bioinformatics.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/31a937a688093f0712fada8741813c83b48db9380ba74f25f296c9dc503c9730/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446576656c6f706572732d536542694d45522d79656c6c6f773f6c6162656c436f6c6f723d303030303030\" alt=\"Developers\" data-canonical-src=\"https://img.shields.io/badge/Developers-SeBiMER-yellow?labelColor=000000\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe ORSON pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003ei. Install \u003ca href=\"https://www.nextflow.io/docs/latest/getstarted.html#installation\" rel=\"nofollow\"\u003e\u003ccode\u003enextflow\u003c/code\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eii. Install \u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e for full pipeline reproducibility\u003c/p\u003e\n\u003cp\u003eiii. Download the pipeline and test it on a minimal dataset with a single command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile test,singularity\u003c/pre\u003e\u003c/div\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTo use this workflow on a computing cluster, it is necessary to provide a configuration file for your system. For some institutes, this one already exists and is referenced on \u003ca href=\"https://github.com/nf-core/configs#documentation\"\u003enf-core/configs\u003c/a\u003e. If so, you can simply download your institute custom config file and simply use \u003ccode\u003e-c \u0026lt;institute_config_file\u0026gt;\u003c/code\u003e in your command. This will enable either \u003ccode\u003edocker\u003c/code\u003e or \u003ccode\u003esingularity\u003c/code\u003e and set the appropriate execution settings for your local compute environment.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eiv. Start running your own analysis!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile custom,singularity [-c \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003einstitute_config_file\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee \u003ca href=\"docs/usage.md\"\u003eusage docs\u003c/a\u003e for a complete description of all of the options available when running the pipeline.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eThis workflow comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"docs/usage.md#install-the-pipeline\"\u003ePipeline installation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md#local-installation\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md#your-own-config\"\u003eAdding your own system config\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md#running-the-pipeline\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHere is an overview of the many steps available in orson pipeline:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"docs/images/ORSON_workflow.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/ORSON_workflow.png\" alt=\"ORSON\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cp\u003eTo use ORSON, all tools are automatically installed via pre-built singularity images available at SeBiMER ftp; these images are built from recipes available \u003ca href=\"/containers\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eDatabases are also automatically download according to user\u0027s choice (default: Enzyme, UniProt SwissProt). See the \u003ca href=\"/docs/usage.md#installing-annotated-sequence-banks\"\u003eInstalling annotated sequence banks\u003c/a\u003e section of the usage documentation.\u003c/p\u003e\n\u003cp\u003eHowever, you must have local access to the BUSCO lineage databases. To download them, please refer to the tool\u0027s documentation to \u003ca href=\"https://busco.ezlab.org/busco_userguide.html#download-and-automated-update\" rel=\"nofollow\"\u003edownload the lineage databases\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eORSON is written by \u003ca href=\"https://ifremer-bioinformatics.github.io/\" rel=\"nofollow\"\u003eSeBiMER\u003c/a\u003e, the Bioinformatics Core Facility of \u003ca href=\"https://wwz.ifremer.fr/en/\" rel=\"nofollow\"\u003eIFREMER\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions\" class=\"anchor\" href=\"#contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions\u003c/h2\u003e\n\u003cp\u003eWe welcome contributions to the pipeline. If such case you can do one of the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse issues to submit your questions\u003c/li\u003e\n\u003cli\u003eFork the project, do your developments and submit a pull request\u003c/li\u003e\n\u003cli\u003eContact us (see email below)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupport\u003c/h2\u003e\n\u003cp\u003eFor further information or help, don\u0027t hesitate to get in touch with the orson developpers:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"assets/sebimer-email.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"assets/sebimer-email.png\" alt=\"sebimer email\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "workflow",
      "annotation",
      "transcriptome",
      "proteome",
      "nextflow-pipelines"
    ],
    "updated_at": 1621527121.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for GRASS GIS and QGIS on Centos 7",
    "filenames": [
      "Singularity"
    ],
    "full_name": "willgpaik/grass_qgis_aci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-grass_qgis_aci\" class=\"anchor\" href=\"#grass_qgis_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egrass_qgis_aci\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for GRASS GIS and QGIS on Centos 8 For ICS Roar clusters\u003c/p\u003e\n\u003cp\u003e2019/1/22\u003cbr\u003e\nGRASS 7.4.4 is updated\u003c/p\u003e\n\u003cp\u003e2019/1/25\u003cbr\u003e\nGRASS GIS g.extension function can be used by:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ grass74 /PATH/to/Mapset/PERMANENT --exec g.extension \u0026lt;ADD-ON\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf Mapset is not installed, download sample Mapset to \"scratch\" directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd ~/scratch\n$ wget https://grass.osgeo.org/sampledata/worldlocation.tar.gz\n$ tar -xf worldlocation.tar.gz\n    \n$ grass74 ~/scratch/worldlocation/PERMANENT --exec g.extension \u0026lt;ADD-ON\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo delete sample Mapset:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd ~/scartch\n$ rm -rf worldlocation\n$ rm worldlocation.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2021/03/10\u003cbr\u003e\nOS is upgraded to Centos 8 from Centos 7\u003cbr\u003e\nGRASS is upgraded to 7.8.5\u003c/p\u003e\n\u003cp\u003e2021/03/17\u003cbr\u003e\nQGIS is removed (can be added again later if requested)\u003c/p\u003e\n\u003cp\u003e2021/05/19\u003cbr\u003e\nGRASS scripts are enabled\u003cbr\u003e\nGRASS GIS Addons can be installed with command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ g.extension extension=\u0026lt;Add-on\u0026gt; prefix=\u0026lt;Install_DIR\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMake sure to create ~/.grassrc.78 i.e.:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eGISDBASE: \u0026lt;PATH_TO_MAP_DATA\u0026gt;\nLOCATION_NAME: nc_basic_spm_grass7\nMAPSET: user1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethen from container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; export GISRC=$HOME/.grassrc.78\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1621446968.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for fastANI",
    "filenames": [
      "1.33/Singularity"
    ],
    "full_name": "icaoberg/singularity-fastani",
    "latest_release": "v1.3.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-fastani\" class=\"anchor\" href=\"#singularity-fastani\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-fastani\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for fastANI.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1621557973.0
  },
  {
    "data_format": 2,
    "description": "GeNT",
    "filenames": [
      "1.0.0/Singularity"
    ],
    "full_name": "pscedu/singularity-gent",
    "latest_release": "1.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-gent\" class=\"anchor\" href=\"#singularity-gent\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-gent\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.travis-ci.com/icaoberg/singularity-gent\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/be85592cab6539d1961fcaa7b022c254ec8d8a5d86f4282c9fc6daa968f50e59/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d67656e742e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-gent.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for GeNT.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1621306030.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "3.3.1/Singularity"
    ],
    "full_name": "pscedu/singularity-hmmer",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-hmmer\" class=\"anchor\" href=\"#singularity-hmmer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-hmmer\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"http://hmmer.org/\" rel=\"nofollow\"\u003eHMMER\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621305894.0
  },
  {
    "data_format": 2,
    "description": "C++ API \u0026 command-line toolkit for working with BAM data",
    "filenames": [
      "2.5.1/Singularity"
    ],
    "full_name": "pscedu/singularity-bamtools",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bamtools\" class=\"anchor\" href=\"#singularity-bamtools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bamtools\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.travis-ci.com/icaoberg/singularity-bamtools\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/188d000bd28e4b53608796e6b21543dfcb90a5074619295b1c037b42ae625ab8/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d62616d746f6f6c732e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-bamtools.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSource code repository can be found \u003ca href=\"https://github.com/trinityrnaseq/trinityrnaseq\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-container-for-bridges-or-similar\" class=\"anchor\" href=\"#building-the-container-for-bridges-or-similar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the container for Bridges (or similar)\u003c/h2\u003e\n\u003cp\u003eThere is no need to build a container, because an image is already available from the Galaxy project, hence all you need to do is run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./pull.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo avoid that pesky warning when building directly from a Docker container, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-or-similar\" class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges (or similar)\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ebamtools\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bamtools/2.5.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modules/bamtools\u003c/code\u003e as \u003ccode\u003e2.5.1.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-test\" class=\"anchor\" href=\"#test\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest\u003c/h2\u003e\n\u003cp\u003eIf \u003ccode\u003etest.sh\u003c/code\u003e is available, then run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics",
      "bamtools"
    ],
    "updated_at": 1621305791.0
  },
  {
    "data_format": 2,
    "description": "Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data.",
    "filenames": [
      "0.39/Singularity"
    ],
    "full_name": "pscedu/singularity-trimmomatic",
    "latest_release": "0.39",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-trimmomatic\" class=\"anchor\" href=\"#singularity-trimmomatic\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-trimmomatic\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.travis-ci.com/icaoberg/singularity-trimmomatic\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4dde8880c00e91090a3dcb55e0f9d8189179c34c1c845d884436f9760ca2f8e6/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d7472696d6d6f6d617469632e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-trimmomatic.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for Trimmomatic.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics",
      "trimmomatic"
    ],
    "updated_at": 1621305830.0
  },
  {
    "data_format": 2,
    "description": "Target / Integrative Genetic Element Retriever",
    "filenames": [
      "5.32.1/Singularity"
    ],
    "full_name": "pscedu/singularity-tiger",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-tiger\" class=\"anchor\" href=\"#singularity-tiger\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-tiger\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.travis-ci.com/icaoberg/singularity-tiger\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8a4d8b01a51057c102939a9fe9eafed97007d23af59106e3b1ae3ff2fa88f649/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d74696765722e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-tiger.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/TIGER\"\u003eTIGER\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-or-similar\" class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges (or similar)\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the Perl scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/tiger/5.32.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modules/tiger\u003c/code\u003e as \u003ccode\u003e5.32.1.lua\u003c/code\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "tiger"
    ],
    "updated_at": 1621316658.0
  },
  {
    "data_format": 2,
    "description": "MLPerf Inference containers recipes",
    "filenames": [
      "v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_rt_c_tbb-py36-gcc75-ubuntu18",
      "v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_src_c_tbb-py36-gcc75-ubuntu18",
      "v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_src_c_omp-py36-gcc75-ubuntu18",
      "v0.5/cpp/OpenVINO/singularity/Singularity.v0.5-OpenVINO-2019_R3.1_rt_cg_tbb-py36-gcc75-ubuntu18"
    ],
    "full_name": "provarepro/mlperf_inference",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mlperf-inference\" class=\"anchor\" href=\"#mlperf-inference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMLPerf Inference\u003c/h1\u003e\n\u003cp\u003eMLPerf Inference containers recipes\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621303140.0
  },
  {
    "data_format": 2,
    "description": "Neurodocker Docker/Singularity container build scripts for AFNI+Python 3.6+nipype",
    "filenames": [
      "Singularity"
    ],
    "full_name": "DavidEWarrenPhD/afnipype",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-afnipype\" class=\"anchor\" href=\"#afnipype\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eafnipype\u003c/h1\u003e\n\u003cp\u003eNeurodocker Docker/Singularity container build scripts for AFNI+Python 3.6+nipype\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621289479.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.2.1/Singularity",
      "2.1.0/Singularity"
    ],
    "full_name": "yh549848/singularity-hisat2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-afnipype\" class=\"anchor\" href=\"#afnipype\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eafnipype\u003c/h1\u003e\n\u003cp\u003eNeurodocker Docker/Singularity container build scripts for AFNI+Python 3.6+nipype\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621263935.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "1.3.1/Singularity",
      "1.3.3/Singularity"
    ],
    "full_name": "yh549848/singularity-rsem",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-afnipype\" class=\"anchor\" href=\"#afnipype\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eafnipype\u003c/h1\u003e\n\u003cp\u003eNeurodocker Docker/Singularity container build scripts for AFNI+Python 3.6+nipype\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621259784.0
  },
  {
    "data_format": 2,
    "description": "UPPMAX Singularity builds",
    "filenames": [
      "gapseq/Singularity.gapseq",
      "metaWRAP/Singularity.metaWRAP",
      "metaWRAP/Singularity.metaWRAP-deps-only",
      "metaWRAP/Singularity.metaWRAP-deps-only-ubuntu",
      "MitoZ/Singularity.v2.3-pm"
    ],
    "full_name": "pmitev/UPPMAX-Singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-uppmax-singularity\" class=\"anchor\" href=\"#uppmax-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUPPMAX-Singularity\u003c/h1\u003e\n\u003cp\u003eUPPMAX Singularity builds\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621189876.0
  },
  {
    "data_format": 2,
    "description": "Container recipes for OpenVINO",
    "filenames": [
      "ubuntu18/2019/singularity/Singularity.2019_R3.1-tbb-cg-py36-gcc7.5-ubuntu18",
      "ubuntu18/2019/singularity/Singularity.2019_R3.1-tbb-c-py36-gcc7.5-ubuntu18",
      "ubuntu18/2019/singularity/Singularity.2019_R3.1-omp-c-py36-gcc7.5-ubuntu18",
      "ubuntu18/2019/singularity/Singularity.2019_R3.1-omp-cg-py36-gcc7.5-ubuntu18"
    ],
    "full_name": "fenz-org/OpenVino",
    "latest_release": "0.0.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-openvino\" class=\"anchor\" href=\"#openvino\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpenVino\u003c/h1\u003e\n\u003cp\u003eContainer recipes for OpenVINO\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621303507.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/petreg",
    "latest_release": "v1.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pet-to-mr-registration-via-ct\" class=\"anchor\" href=\"#pet-to-mr-registration-via-ct\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePET to MR registration via CT\u003c/h1\u003e\n\u003cp\u003ePreprocessing and extraction of data from regions in a PET image, using FSL. Regions are derived\nfrom a MultiAtlas segmentation (\u003ca href=\"https://github.com/VUIIS/Multi-Atlas-v3.0.0\"\u003ehttps://github.com/VUIIS/Multi-Atlas-v3.0.0\u003c/a\u003e,\n\u003ca href=\"https://github.com/MASILab/SLANTbrainSeg\"\u003ehttps://github.com/MASILab/SLANTbrainSeg\u003c/a\u003e).\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMotion correction applied to PET time series (\u003ccode\u003emcflirt\u003c/code\u003e, 6 dof)\u003c/li\u003e\n\u003cli\u003eMean PET registered to CT (\u003ccode\u003eflirt\u003c/code\u003e, 6 dof)\u003c/li\u003e\n\u003cli\u003eCT registered to MR (\u003ccode\u003eflirt\u003c/code\u003e, default 6 dof)\u003c/li\u003e\n\u003cli\u003eSUVR calculated from summed PET image, referenced to cerebellum\u003c/li\u003e\n\u003cli\u003ePET regional values extracted with nilearn\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h2\u003e\n\u003cp\u003eThe command line is\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epetreg.sh \u0026lt;arguments\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd \u003ccode\u003e\u0026lt;arguments\u0026gt;\u003c/code\u003e are below. The first four are required:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--pet_niigz       PET time series\n--ct_niigz        CT\n--mr_niigz        MR (typically T1W)\n--seg_niigz       Segmentation of MR from multi-atlas / slant\n\n--ctmr_dof        DOF for CT/MR registration (default 6)\n\n--project         Information from XNAT, if used (optional). These\n--subject           are only used to annotate the PDF QA report.\n--session\n--scan\n\n--out_dir         Where outputs will be stored (default /OUTPUTS)\n\n--labels_csv      Labels corresponding to seg_niigz. Optional argument\n                    only needed in special circumstances.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cp\u003eAll output images have been resampled to the field of view, voxel size,\nand geometry of the MR.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCT_REG                   CT registered to MR\nCT_REG_MAT               Transform from CT to MR (FSL format)\nMEANPET_REG              Mean PET registered to MR\nMOT_PARAMS               PET time series motion parameters (FSL format)\nPDF                      QA report\nPET_REG                  PET time series registered to MR\nPET_REG_MAT              Transform from PET to MR (FSL format)\nPET_ROI_MEANS            ROI means of PET time series\nPET_ROI_SDEVS            ROI std devs of PET time series\nROIS                     ROI image\nROI_LABELS               ROI label names\nSUM_REG                  Sum of PET images over time\nSUM_ROI_MEANS            ROI means of summed image\nSUM_ROI_SDEVS            ROI std devs of summed image\nSUVR_REG                 SUVR image calculated from summed image\nSUVR_ROI_MEANS           ROI means of SUVR image\nSUVR_ROI_SDEVS           ROI std devs of SUVR image\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eFSL: \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki\" rel=\"nofollow\"\u003ehttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMCFLIRT: \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT\" rel=\"nofollow\"\u003ehttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFLIRT: \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT#References\" rel=\"nofollow\"\u003ehttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT#References\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003enilearn: \u003ca href=\"https://nilearn.github.io/authors.html#citing\" rel=\"nofollow\"\u003ehttps://nilearn.github.io/authors.html#citing\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMultiAtlas: \u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/23265798/\" rel=\"nofollow\"\u003ehttps://pubmed.ncbi.nlm.nih.gov/23265798/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSLANT: \u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/30910724/\" rel=\"nofollow\"\u003ehttps://pubmed.ncbi.nlm.nih.gov/30910724/\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621746836.0
  },
  {
    "data_format": 2,
    "description": "Collection of singularity recipes",
    "filenames": [
      "PCAngsd/Singularity.PCAngsd_v0.99",
      "clumpak/Singularity.clumpak_v1.1",
      "angsd/Singularity.angsd_v0.933",
      "ngsLD/Singularity.ngsLD_v1.1.1",
      "ngsRelate/Singularity.ngsRelate_v2.0"
    ],
    "full_name": "James-S-Santangelo/singularity-recipes",
    "latest_release": null,
    "readme": "\u003cp\u003eThis repository contains Singularity recipes for genomics tools that I have not found available through other means (e.g., Conda, Docker).\u003c/p\u003e\n\u003cp\u003eSingularity images are available on \u003ca href=\"https://cloud.sylabs.io/library/james-s-santangelo\" rel=\"nofollow\"\u003eSylab\u0027s Cloud Library\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621059542.0
  },
  {
    "data_format": 2,
    "description": "Singularity base container with Nix to be used in XSEDE compute environment (currently in development)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "XSEDE/singularity-nix-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-nix-base\" class=\"anchor\" href=\"#singularity-nix-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-nix-base\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4462\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity base container with Nix to be used in XSEDE compute environment (currently in development)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 13,
    "topics": [
      "nix",
      "singularity",
      "singularity-nix"
    ],
    "updated_at": 1621060868.0
  },
  {
    "data_format": 2,
    "description": "FreeSurfer 6 Pipeline",
    "filenames": [
      "Singularity",
      "Singularity.v1.2.1",
      "Singularity.v1.1.0",
      "Singularity.v1.2.3",
      "Singularity.v1.0.0",
      "Singularity.v1.2.0"
    ],
    "full_name": "ccmvumc/FS6",
    "latest_release": "v1.2.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fs6\" class=\"anchor\" href=\"#fs6\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFS6\u003c/h1\u003e\n\u003cp\u003eFreeSurfer 6 Pipeline\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1120\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621039187.0
  },
  {
    "data_format": 2,
    "description": "TRACULA Pipeline",
    "filenames": [
      "Singularity",
      "Singularity.v2.1.1",
      "Singularity.v2.0.0"
    ],
    "full_name": "ccmvumc/TRACULA",
    "latest_release": "v2.1.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tracula\" class=\"anchor\" href=\"#tracula\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTRACULA\u003c/h1\u003e\n\u003cp\u003eTRACULA Pipeline\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621037592.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "1.5.0/Singularity"
    ],
    "full_name": "yh549848/singularity-salmon",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tracula\" class=\"anchor\" href=\"#tracula\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTRACULA\u003c/h1\u003e\n\u003cp\u003eTRACULA Pipeline\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621037111.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "0.44.0/Singularity",
      "0.46.1/Singularity"
    ],
    "full_name": "yh549848/singularity-kallisto",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tracula\" class=\"anchor\" href=\"#tracula\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTRACULA\u003c/h1\u003e\n\u003cp\u003eTRACULA Pipeline\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621023361.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "ubuntu18/2019/Singularity.2019_R3.1-c-py36-gcc7.5-ubuntu18",
      "ubuntu18/2019/Singularity.2019_R3.1-TBB-c-py36-gcc7.5-ubuntu18"
    ],
    "full_name": "fenz/OpenVINO",
    "latest_release": "0.0.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-images-for-openvino\" class=\"anchor\" href=\"#singularity-images-for-openvino\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity images for OpenVINO\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621008787.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "richward1/Singularity-Nvidia",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-images-for-openvino\" class=\"anchor\" href=\"#singularity-images-for-openvino\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity images for OpenVINO\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621006880.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "richward1/alpine-novnc",
    "latest_release": "v2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-images-for-openvino\" class=\"anchor\" href=\"#singularity-images-for-openvino\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity images for OpenVINO\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621006537.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.latest"
    ],
    "full_name": "bioexcel/biobb_cmip",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://biobb-cmip.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8bd09664a4dca78a8f246d76f3af7fc6da719393b3f9c6cbc6a8b291b19f3d80/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f62696f62622d636d69702f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"\" data-canonical-src=\"https://readthedocs.org/projects/biobb-cmip/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/bioconda/biobb_cmip\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a477fdb1fd9bc9eb7ffa6cae6a019c6d4c3902fd468b3126f1b78e56c7dcff83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e7376673f7374796c653d666c6174\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://quay.io/repository/biocontainers/biobb_cmip\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ca418e4db0b3de91a09a5df4a59446da015b6164598a8bc255918e911484f84f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d517561792e696f2d626c7565\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/docker-Quay.io-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.singularity-hub.org/collections/2735/usage\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/Apache-2.0\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-biobb_cmip\" class=\"anchor\" href=\"#biobb_cmip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebiobb_cmip\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eBiobb_cmip is the Biobb module collection to compute classical molecular interaction potentials.\nBiobb (BioExcel building blocks) packages are Python building blocks that\ncreate new layer of compatibility and interoperability over popular\nbioinformatics tools.\nThe latest documentation of this package can be found in our readthedocs site:\n\u003ca href=\"http://biobb-cmip.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003elatest API documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-version\" class=\"anchor\" href=\"#version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion\u003c/h3\u003e\n\u003cp\u003ev3.7.0 2021.2\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eUsing PIP:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eImportant:\u003c/strong\u003e PIP only installs the package. All the dependencies must be installed separately. To perform a complete installation, please use ANACONDA, DOCKER or SINGULARITY.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  pip install \"biobb_cmip\u0026gt;=3.7.0\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage: \u003ca href=\"https://biobb-cmip.readthedocs.io/en/latest/modules.html\" rel=\"nofollow\"\u003ePython API documentation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing ANACONDA:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  conda install -c bioconda \"biobb_cmip\u0026gt;=3.7.0\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage: With conda installation BioBBs can be used with the \u003ca href=\"https://biobb-cmip.readthedocs.io/en/latest/modules.html\" rel=\"nofollow\"\u003ePython API documentation\u003c/a\u003e and the \u003ca href=\"https://biobb-cmip.readthedocs.io/en/latest/command_line.html\" rel=\"nofollow\"\u003eCommand Line documentation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing DOCKER:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker pull quay.io/biocontainers/biobb_cmip:3.7.0--py_0\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run quay.io/biocontainers/biobb_cmip:3.7.0--py_0 \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing SINGULARITY:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMacOS users\u003c/strong\u003e: it\u0027s strongly recommended to avoid Singularity and use \u003cstrong\u003eDocker\u003c/strong\u003e as containerization system.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity pull --name biobb_cmip.sif shub://bioexcel/biobb_cmip\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity exec biobb_cmip.sif \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe command list and specification can be found at the \u003ca href=\"https://biobb-cmip.readthedocs.io/en/latest/command_line.html\" rel=\"nofollow\"\u003eCommand Line documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-copyright--licensing\" class=\"anchor\" href=\"#copyright--licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCopyright \u0026amp; Licensing\u003c/h3\u003e\n\u003cp\u003eThis software has been developed in the \u003ca href=\"http://mmb.irbbarcelona.org\" rel=\"nofollow\"\u003eMMB group\u003c/a\u003e at the \u003ca href=\"http://www.bsc.es/\" rel=\"nofollow\"\u003eBSC\u003c/a\u003e \u0026amp; \u003ca href=\"https://www.irbbarcelona.org/\" rel=\"nofollow\"\u003eIRB\u003c/a\u003e for the \u003ca href=\"http://bioexcel.eu/\" rel=\"nofollow\"\u003eEuropean BioExcel\u003c/a\u003e, funded by the European Commission (EU H2020 \u003ca href=\"http://cordis.europa.eu/projects/823830\" rel=\"nofollow\"\u003e823830\u003c/a\u003e, EU H2020 \u003ca href=\"http://cordis.europa.eu/projects/675728\" rel=\"nofollow\"\u003e675728\u003c/a\u003e).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(c) 2015-2021 \u003ca href=\"https://www.bsc.es/\" rel=\"nofollow\"\u003eBarcelona Supercomputing Center\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e(c) 2015-2021 \u003ca href=\"https://www.irbbarcelona.org/\" rel=\"nofollow\"\u003eInstitute for Research in Biomedicine\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLicensed under the\n\u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\"\u003eApache License 2.0\u003c/a\u003e, see the file LICENSE for details.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67\" alt=\"\" title=\"Bioexcel\" data-canonical-src=\"https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1621526007.0
  },
  {
    "data_format": 2,
    "description": "Experiments, data, and results for MAL and general sum game experiments with CFR.",
    "filenames": [
      "hr_edl/Singularity.def"
    ],
    "full_name": "dmorrill10/hr_edl_experiments",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" class=\"anchor\" href=\"#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHindsight Rationality, and Efficient Deviation Types and Learning Experiments\u003c/h1\u003e\n\u003cp\u003eExperiment code for \u003ca href=\"https://arxiv.org/abs/2012.05874\" rel=\"nofollow\"\u003eHindsight and Sequential Rationality of Correlated Play\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2102.06973\" rel=\"nofollow\"\u003eEfficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games\u003c/a\u003e conference papers (AAAI-21 and ICML 2021, respectively).\u003c/p\u003e\n\u003cp\u003eThis repository has a number of different components that work together to generate the experimental results.\u003c/p\u003e\n\u003cp\u003eThe pipeline begins with \u003ccode\u003ehr_edl\u003c/code\u003e. This is C++ code built over \u003ca href=\"https://github.com/deepmind/open_spiel\"\u003eOpenSpiel\u003c/a\u003e that defines the experiments. See \u003ca href=\"hr_edl/README.md\"\u003e\u003ccode\u003ehr_edl/README.md\u003c/code\u003e\u003c/a\u003e for more information.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ehr_edl\u003c/code\u003e code allows us to run experiments, but experiments are run with help from the Python3 library, \u003ccode\u003ehr_edl_data\u003c/code\u003e. Run \u003ccode\u003epip install .\u003c/code\u003e to install it.\nWith \u003ccode\u003ehr_edl_data\u003c/code\u003e installed, you can run \u003ccode\u003ebin/run_experiment.py\u003c/code\u003e to run an experiment.\u003c/p\u003e\n\u003cp\u003eThe experiment configurations used in the papers are defined in \u003ccode\u003eMakefile\u003c/code\u003e. Assuming that \u003ccode\u003ehr_edl_data\u003c/code\u003e is installed, running \u003ccode\u003emake\u003c/code\u003e should compile \u003ccode\u003ehr_edl\u003c/code\u003e and run all experiments, updating data files in \u003ccode\u003edata\u003c/code\u003e as necessary, and depositing Numpy data files in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFinally, the Python Jupyter notebooks in \u003ccode\u003enotebooks\u003c/code\u003e process the Numpy data files into the final results, which are also saved in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-virtual-machines-and-containers\" class=\"anchor\" href=\"#virtual-machines-and-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVirtual Machines and Containers\u003c/h2\u003e\n\u003cp\u003eA \u003ca href=\"https://www.vagrantup.com/\" rel=\"nofollow\"\u003eVagrant\u003c/a\u003e virtual machine configuration to help run these experiments is defined in \u003ccode\u003eVagrantfile\u003c/code\u003e.\nIf you already have Python3 installed though, you may not need to use it.\nTypically, the most onerous part of the installation procedure is building \u003ccode\u003ehr_edl\u003c/code\u003e and its dependencies.\nFor this, you can use the \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container defined by \u003ccode\u003ehr_edl/Singularity.def\u003c/code\u003e.\nOnce you have Singularity installed or you are using the Vagrant virtual machine, you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build ./hr_edl.sif Singularity.def\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto create a Singularity image. Then you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec hr_edl.sif /code/build.optimized/bin/\u0026lt;command\u0026gt; [command options]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto run an \u003ccode\u003ehr_edl\u003c/code\u003e executable inside the container.\n\u003ccode\u003ebin/run_experiment.py\u003c/code\u003e has a \u003ccode\u003e--sif\u003c/code\u003e option so you can specify a container image in which the experiment should be run.\nYou can set the variable \u003ccode\u003eSIF\u003c/code\u003e and \u003ccode\u003eHR_EDL_DIR =/code\u003c/code\u003e in \u003ccode\u003eMakefile\u003c/code\u003e (either in the file or in the command like \u003ccode\u003emake SIF=my_image.sif HR_EDL_DIR=/code\u003c/code\u003e) to run all experiments in a given container image.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621058061.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.1.1/Singularity"
    ],
    "full_name": "yh549848/singularity-tophat2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" class=\"anchor\" href=\"#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHindsight Rationality, and Efficient Deviation Types and Learning Experiments\u003c/h1\u003e\n\u003cp\u003eExperiment code for \u003ca href=\"https://arxiv.org/abs/2012.05874\" rel=\"nofollow\"\u003eHindsight and Sequential Rationality of Correlated Play\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2102.06973\" rel=\"nofollow\"\u003eEfficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games\u003c/a\u003e conference papers (AAAI-21 and ICML 2021, respectively).\u003c/p\u003e\n\u003cp\u003eThis repository has a number of different components that work together to generate the experimental results.\u003c/p\u003e\n\u003cp\u003eThe pipeline begins with \u003ccode\u003ehr_edl\u003c/code\u003e. This is C++ code built over \u003ca href=\"https://github.com/deepmind/open_spiel\"\u003eOpenSpiel\u003c/a\u003e that defines the experiments. See \u003ca href=\"hr_edl/README.md\"\u003e\u003ccode\u003ehr_edl/README.md\u003c/code\u003e\u003c/a\u003e for more information.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ehr_edl\u003c/code\u003e code allows us to run experiments, but experiments are run with help from the Python3 library, \u003ccode\u003ehr_edl_data\u003c/code\u003e. Run \u003ccode\u003epip install .\u003c/code\u003e to install it.\nWith \u003ccode\u003ehr_edl_data\u003c/code\u003e installed, you can run \u003ccode\u003ebin/run_experiment.py\u003c/code\u003e to run an experiment.\u003c/p\u003e\n\u003cp\u003eThe experiment configurations used in the papers are defined in \u003ccode\u003eMakefile\u003c/code\u003e. Assuming that \u003ccode\u003ehr_edl_data\u003c/code\u003e is installed, running \u003ccode\u003emake\u003c/code\u003e should compile \u003ccode\u003ehr_edl\u003c/code\u003e and run all experiments, updating data files in \u003ccode\u003edata\u003c/code\u003e as necessary, and depositing Numpy data files in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFinally, the Python Jupyter notebooks in \u003ccode\u003enotebooks\u003c/code\u003e process the Numpy data files into the final results, which are also saved in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-virtual-machines-and-containers\" class=\"anchor\" href=\"#virtual-machines-and-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVirtual Machines and Containers\u003c/h2\u003e\n\u003cp\u003eA \u003ca href=\"https://www.vagrantup.com/\" rel=\"nofollow\"\u003eVagrant\u003c/a\u003e virtual machine configuration to help run these experiments is defined in \u003ccode\u003eVagrantfile\u003c/code\u003e.\nIf you already have Python3 installed though, you may not need to use it.\nTypically, the most onerous part of the installation procedure is building \u003ccode\u003ehr_edl\u003c/code\u003e and its dependencies.\nFor this, you can use the \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container defined by \u003ccode\u003ehr_edl/Singularity.def\u003c/code\u003e.\nOnce you have Singularity installed or you are using the Vagrant virtual machine, you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build ./hr_edl.sif Singularity.def\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto create a Singularity image. Then you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec hr_edl.sif /code/build.optimized/bin/\u0026lt;command\u0026gt; [command options]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto run an \u003ccode\u003ehr_edl\u003c/code\u003e executable inside the container.\n\u003ccode\u003ebin/run_experiment.py\u003c/code\u003e has a \u003ccode\u003e--sif\u003c/code\u003e option so you can specify a container image in which the experiment should be run.\nYou can set the variable \u003ccode\u003eSIF\u003c/code\u003e and \u003ccode\u003eHR_EDL_DIR =/code\u003c/code\u003e in \u003ccode\u003eMakefile\u003c/code\u003e (either in the file or in the command like \u003ccode\u003emake SIF=my_image.sif HR_EDL_DIR=/code\u003c/code\u003e) to run all experiments in a given container image.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620951820.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "1.3.4d/Singularity",
      "2.0.6/Singularity",
      "2.0.0/Singularity"
    ],
    "full_name": "yh549848/singularity-stringtie",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" class=\"anchor\" href=\"#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHindsight Rationality, and Efficient Deviation Types and Learning Experiments\u003c/h1\u003e\n\u003cp\u003eExperiment code for \u003ca href=\"https://arxiv.org/abs/2012.05874\" rel=\"nofollow\"\u003eHindsight and Sequential Rationality of Correlated Play\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2102.06973\" rel=\"nofollow\"\u003eEfficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games\u003c/a\u003e conference papers (AAAI-21 and ICML 2021, respectively).\u003c/p\u003e\n\u003cp\u003eThis repository has a number of different components that work together to generate the experimental results.\u003c/p\u003e\n\u003cp\u003eThe pipeline begins with \u003ccode\u003ehr_edl\u003c/code\u003e. This is C++ code built over \u003ca href=\"https://github.com/deepmind/open_spiel\"\u003eOpenSpiel\u003c/a\u003e that defines the experiments. See \u003ca href=\"hr_edl/README.md\"\u003e\u003ccode\u003ehr_edl/README.md\u003c/code\u003e\u003c/a\u003e for more information.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ehr_edl\u003c/code\u003e code allows us to run experiments, but experiments are run with help from the Python3 library, \u003ccode\u003ehr_edl_data\u003c/code\u003e. Run \u003ccode\u003epip install .\u003c/code\u003e to install it.\nWith \u003ccode\u003ehr_edl_data\u003c/code\u003e installed, you can run \u003ccode\u003ebin/run_experiment.py\u003c/code\u003e to run an experiment.\u003c/p\u003e\n\u003cp\u003eThe experiment configurations used in the papers are defined in \u003ccode\u003eMakefile\u003c/code\u003e. Assuming that \u003ccode\u003ehr_edl_data\u003c/code\u003e is installed, running \u003ccode\u003emake\u003c/code\u003e should compile \u003ccode\u003ehr_edl\u003c/code\u003e and run all experiments, updating data files in \u003ccode\u003edata\u003c/code\u003e as necessary, and depositing Numpy data files in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFinally, the Python Jupyter notebooks in \u003ccode\u003enotebooks\u003c/code\u003e process the Numpy data files into the final results, which are also saved in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-virtual-machines-and-containers\" class=\"anchor\" href=\"#virtual-machines-and-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVirtual Machines and Containers\u003c/h2\u003e\n\u003cp\u003eA \u003ca href=\"https://www.vagrantup.com/\" rel=\"nofollow\"\u003eVagrant\u003c/a\u003e virtual machine configuration to help run these experiments is defined in \u003ccode\u003eVagrantfile\u003c/code\u003e.\nIf you already have Python3 installed though, you may not need to use it.\nTypically, the most onerous part of the installation procedure is building \u003ccode\u003ehr_edl\u003c/code\u003e and its dependencies.\nFor this, you can use the \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container defined by \u003ccode\u003ehr_edl/Singularity.def\u003c/code\u003e.\nOnce you have Singularity installed or you are using the Vagrant virtual machine, you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build ./hr_edl.sif Singularity.def\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto create a Singularity image. Then you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec hr_edl.sif /code/build.optimized/bin/\u0026lt;command\u0026gt; [command options]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto run an \u003ccode\u003ehr_edl\u003c/code\u003e executable inside the container.\n\u003ccode\u003ebin/run_experiment.py\u003c/code\u003e has a \u003ccode\u003e--sif\u003c/code\u003e option so you can specify a container image in which the experiment should be run.\nYou can set the variable \u003ccode\u003eSIF\u003c/code\u003e and \u003ccode\u003eHR_EDL_DIR =/code\u003c/code\u003e in \u003ccode\u003eMakefile\u003c/code\u003e (either in the file or in the command like \u003ccode\u003emake SIF=my_image.sif HR_EDL_DIR=/code\u003c/code\u003e) to run all experiments in a given container image.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620951711.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.7.3a/Singularity",
      "2.6.1d/Singularity",
      "2.7.8a/Singularity"
    ],
    "full_name": "yh549848/singularity-star",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" class=\"anchor\" href=\"#hindsight-rationality-and-efficient-deviation-types-and-learning-experiments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHindsight Rationality, and Efficient Deviation Types and Learning Experiments\u003c/h1\u003e\n\u003cp\u003eExperiment code for \u003ca href=\"https://arxiv.org/abs/2012.05874\" rel=\"nofollow\"\u003eHindsight and Sequential Rationality of Correlated Play\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2102.06973\" rel=\"nofollow\"\u003eEfficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games\u003c/a\u003e conference papers (AAAI-21 and ICML 2021, respectively).\u003c/p\u003e\n\u003cp\u003eThis repository has a number of different components that work together to generate the experimental results.\u003c/p\u003e\n\u003cp\u003eThe pipeline begins with \u003ccode\u003ehr_edl\u003c/code\u003e. This is C++ code built over \u003ca href=\"https://github.com/deepmind/open_spiel\"\u003eOpenSpiel\u003c/a\u003e that defines the experiments. See \u003ca href=\"hr_edl/README.md\"\u003e\u003ccode\u003ehr_edl/README.md\u003c/code\u003e\u003c/a\u003e for more information.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ehr_edl\u003c/code\u003e code allows us to run experiments, but experiments are run with help from the Python3 library, \u003ccode\u003ehr_edl_data\u003c/code\u003e. Run \u003ccode\u003epip install .\u003c/code\u003e to install it.\nWith \u003ccode\u003ehr_edl_data\u003c/code\u003e installed, you can run \u003ccode\u003ebin/run_experiment.py\u003c/code\u003e to run an experiment.\u003c/p\u003e\n\u003cp\u003eThe experiment configurations used in the papers are defined in \u003ccode\u003eMakefile\u003c/code\u003e. Assuming that \u003ccode\u003ehr_edl_data\u003c/code\u003e is installed, running \u003ccode\u003emake\u003c/code\u003e should compile \u003ccode\u003ehr_edl\u003c/code\u003e and run all experiments, updating data files in \u003ccode\u003edata\u003c/code\u003e as necessary, and depositing Numpy data files in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFinally, the Python Jupyter notebooks in \u003ccode\u003enotebooks\u003c/code\u003e process the Numpy data files into the final results, which are also saved in \u003ccode\u003eresults\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-virtual-machines-and-containers\" class=\"anchor\" href=\"#virtual-machines-and-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVirtual Machines and Containers\u003c/h2\u003e\n\u003cp\u003eA \u003ca href=\"https://www.vagrantup.com/\" rel=\"nofollow\"\u003eVagrant\u003c/a\u003e virtual machine configuration to help run these experiments is defined in \u003ccode\u003eVagrantfile\u003c/code\u003e.\nIf you already have Python3 installed though, you may not need to use it.\nTypically, the most onerous part of the installation procedure is building \u003ccode\u003ehr_edl\u003c/code\u003e and its dependencies.\nFor this, you can use the \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container defined by \u003ccode\u003ehr_edl/Singularity.def\u003c/code\u003e.\nOnce you have Singularity installed or you are using the Vagrant virtual machine, you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build ./hr_edl.sif Singularity.def\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto create a Singularity image. Then you can run\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec hr_edl.sif /code/build.optimized/bin/\u0026lt;command\u0026gt; [command options]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eto run an \u003ccode\u003ehr_edl\u003c/code\u003e executable inside the container.\n\u003ccode\u003ebin/run_experiment.py\u003c/code\u003e has a \u003ccode\u003e--sif\u003c/code\u003e option so you can specify a container image in which the experiment should be run.\nYou can set the variable \u003ccode\u003eSIF\u003c/code\u003e and \u003ccode\u003eHR_EDL_DIR =/code\u003c/code\u003e in \u003ccode\u003eMakefile\u003c/code\u003e (either in the file or in the command like \u003ccode\u003emake SIF=my_image.sif HR_EDL_DIR=/code\u003c/code\u003e) to run all experiments in a given container image.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620951641.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "deps/DPMC/DMC/Singularity",
      "deps/DPMC/HTB/Singularity",
      "deps/DPMC/tensor/Singularity",
      "deps/DPMC/lg/Singularity"
    ],
    "full_name": "dilkas/wmc-without-parameters",
    "latest_release": null,
    "readme": "\u003cp\u003eThis repository holds all source code (that I am legally allowed to distribute) related to the following publications:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDilkas P., Belle V. \u003cstrong\u003eWeighted Model Counting with Conditional Weights for Bayesian Networks\u003c/strong\u003e. UAI 2021.\u003c/li\u003e\n\u003cli\u003eDilkas P., Belle V. \u003cstrong\u003eWeighted Model Counting Without Parameter Variables\u003c/strong\u003e. SAT 2021.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621730220.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity.ppFqtools",
      "singularity/Singularity.ppPerljson",
      "singularity/Singularity.ppFastqc",
      "singularity/Singularity.ppMykrobe",
      "singularity/Singularity.ppBowtie2",
      "singularity/Singularity.ppBedtools",
      "singularity/Singularity.ppBwa",
      "singularity/Singularity.ppFastp",
      "singularity/Singularity.ppKraken2"
    ],
    "full_name": "Pathogen-Genomics-Cymru/tb-pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mycobacterial-pre-processing-pipeline\" class=\"anchor\" href=\"#mycobacterial-pre-processing-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMycobacterial Pre-processing Pipeline\u003c/h1\u003e\n\u003cp\u003eCleans and QCs reads with fastp and FastQC, classifies with Kraken2 \u0026amp; Mykrobe, removes non-bacterial content, and - by alignment to any minority genomes - disambiguates mixtures of bacterial reads.\u003c/p\u003e\n\u003cp\u003eTakes as input one directory containing pairs of fastq(.gz) or bam files.\nProduces as output one directory per sample, containing the relevant reports \u0026amp; a pair of cleaned fastqs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eThe workflow is designed to run with either docker \u003ccode\u003e-profile docker\u003c/code\u003e or singularity \u003ccode\u003e-profile singularity\u003c/code\u003e. Before running the workflow using singularity, the singularity images for the workflow will need to be built by running \u003ccode\u003esingularity/singularity_pull.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eE.g. to run the workflow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main.nf -profile singularity --filetype fastq --input_dir fq_dir --pattern \"*_R{1,2}.fastq.gz\" --unmix_myco yes \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\nnextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco no \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-params\" class=\"anchor\" href=\"#params\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParams\u003c/h2\u003e\n\u003cp\u003eThe following parameters should be set in \u003ccode\u003enextflow.config\u003c/code\u003e or specified on the command line:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003einput_dir\u003c/strong\u003e\u003cbr\u003e\nDirectory containing fastq OR bam files\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efiletype\u003c/strong\u003e\u003cbr\u003e\nFile type in input_dir. Either \"fastq\" or \"bam\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003epattern\u003c/strong\u003e\u003cbr\u003e\nRegex to match fastq files in input_dir, e.g. \"*_R{1,2}.fq.gz\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eoutput_dir\u003c/strong\u003e\u003cbr\u003e\nOutput directory\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eunmix_myco\u003c/strong\u003e\u003cbr\u003e\nDo you want to disambiguate mixed-mycobacterial samples by read alignment? Either \"yes\" or \"no\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003especies\u003c/strong\u003e\u003cbr\u003e\nPrincipal species in each sample, assuming genus Mycobacterium. Default \u0027null\u0027. If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare, kansasii, tuberculosis\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ekraken_db\u003c/strong\u003e\u003cbr\u003e\nDirectory containing \u003ccode\u003e*.k2d\u003c/code\u003e Kraken2 database files (obtain from \u003ca href=\"https://benlangmead.github.io/aws-indexes/k2\" rel=\"nofollow\"\u003ehttps://benlangmead.github.io/aws-indexes/k2\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie2_index\u003c/strong\u003e\u003cbr\u003e\nDirectory containing Bowtie2 index (obtain from \u003ca href=\"ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\" rel=\"nofollow\"\u003eftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\u003c/a\u003e). The specified path should NOT include the index name\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie_index_name\u003c/strong\u003e\u003cbr\u003e\nName of the bowtie index, e.g. hg19_1kgmaj\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003cp\u003eFor more information on the parameters run \u003ccode\u003enextflow run main.nf --help\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-checkpoints\" class=\"anchor\" href=\"#checkpoints\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheckpoints\u003c/h2\u003e\n\u003cp\u003eCheckpoints used throughout this workflow to fail a sample/issue warnings:\u003c/p\u003e\n\u003cp\u003eprocesses preprocessing_checkFqValidity or preprocessing_checkBamValidity\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e(Fail) If sample does not pass fqtools \u0027validate\u0027 or samtools \u0027quickcheck\u0027, as appropriate.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eprocess preprocessing_countReads\u003cbr\u003e\n2. (Fail) If sample contains \u0026lt; 100k pairs of raw reads.\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_fastp\u003cbr\u003e\n3. (Fail) If sample contains \u0026lt; 100k pairs of cleaned reads, required to all be \u0026gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10 --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size 1 --cut_tail_mean_quality 20).\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_kraken2\u003cbr\u003e\n4. (Fail) If the top family hit is not Mycobacteriaceae\u003cbr\u003e\n5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae \u003cbr\u003e\n6. (Warn) If the top family classification is mycobacterial, but this is not consistent with top genus and species classifications\u003cbr\u003e\n7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications meet minimum thresholds of \u0026gt; 5000 reads or \u0026gt; 0.5% of the total reads (this is not necessarily a concern as not all mycobacteria have a taxonomic classification at this rank) \u003cbr\u003e\n8. (Warn) If sample is mixed or contaminated - defined as containing reads \u0026gt; the 5000/0.5% thresholds from multiple non-human species\u003cbr\u003e\n9. (Warn) If sample contains multiple classifications to mycobacterial species complexes, each meeting the \u0026gt; 5000/0.5% thresholds\u003cbr\u003e\n10. (Warn) If no species classification meets the 5000/0.5% thresholds\u003cbr\u003e\n11. (Warn) If no genus classification meets the 5000/0.5% thresholds\u003cbr\u003e\n12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant given point 5)\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_identifyBacterialContaminants\u003cbr\u003e\n13. (Fail) If the sample is not contaminated and the top species hit is not one of the 10 supported Mycobacteria:\\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis\u003cbr\u003e\n14. (Fail) If the sample is not contaminated and the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003cbr\u003e\n15. (Warn) If the top species hit is supported by \u0026lt; 75% coverage\u003cbr\u003e\n16. (Warn) If the top species hit has a median coverage depth \u0026lt; 10-fold\u003cbr\u003e\n17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant species, which means we will not be able to locate its genome, and thereby remove it as a contaminant\u003cbr\u003e\n18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated with a contaminant species\u0027 taxon ID\u003cbr\u003e\n19. (Warn) If no complete genome could be found for a contaminant species. The workflow will proceed with alignment-based contaminant removal, but you\u0027re warned that there\u0027s reduced confidence in detecting reads from this species\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_downloadContamGenomes\u003cbr\u003e\n20. (Fail) If a contaminant is detected but we are unable to download a representative genome, and thereby remove it\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_summarise\u003cbr\u003e\n21. (Fail) If after having taken an alignment-based approach to decontamination, Kraken still detects a contaminant species\u003cbr\u003e\n22. (Fail) If after having taken an alignment-based approach to decontamination, the top species hit is not one of the 10 supported Mycobacteria\u003cbr\u003e\n23. (Fail) If, after successfully removing contaminants, the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1620947690.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.4.1/Singularity"
    ],
    "full_name": "yh549848/singularity-bowtie2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mycobacterial-pre-processing-pipeline\" class=\"anchor\" href=\"#mycobacterial-pre-processing-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMycobacterial Pre-processing Pipeline\u003c/h1\u003e\n\u003cp\u003eCleans and QCs reads with fastp and FastQC, classifies with Kraken2 \u0026amp; Mykrobe, removes non-bacterial content, and - by alignment to any minority genomes - disambiguates mixtures of bacterial reads.\u003c/p\u003e\n\u003cp\u003eTakes as input one directory containing pairs of fastq(.gz) or bam files.\nProduces as output one directory per sample, containing the relevant reports \u0026amp; a pair of cleaned fastqs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eThe workflow is designed to run with either docker \u003ccode\u003e-profile docker\u003c/code\u003e or singularity \u003ccode\u003e-profile singularity\u003c/code\u003e. Before running the workflow using singularity, the singularity images for the workflow will need to be built by running \u003ccode\u003esingularity/singularity_pull.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eE.g. to run the workflow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main.nf -profile singularity --filetype fastq --input_dir fq_dir --pattern \"*_R{1,2}.fastq.gz\" --unmix_myco yes \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\nnextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco no \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-params\" class=\"anchor\" href=\"#params\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParams\u003c/h2\u003e\n\u003cp\u003eThe following parameters should be set in \u003ccode\u003enextflow.config\u003c/code\u003e or specified on the command line:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003einput_dir\u003c/strong\u003e\u003cbr\u003e\nDirectory containing fastq OR bam files\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efiletype\u003c/strong\u003e\u003cbr\u003e\nFile type in input_dir. Either \"fastq\" or \"bam\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003epattern\u003c/strong\u003e\u003cbr\u003e\nRegex to match fastq files in input_dir, e.g. \"*_R{1,2}.fq.gz\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eoutput_dir\u003c/strong\u003e\u003cbr\u003e\nOutput directory\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eunmix_myco\u003c/strong\u003e\u003cbr\u003e\nDo you want to disambiguate mixed-mycobacterial samples by read alignment? Either \"yes\" or \"no\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003especies\u003c/strong\u003e\u003cbr\u003e\nPrincipal species in each sample, assuming genus Mycobacterium. Default \u0027null\u0027. If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare, kansasii, tuberculosis\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ekraken_db\u003c/strong\u003e\u003cbr\u003e\nDirectory containing \u003ccode\u003e*.k2d\u003c/code\u003e Kraken2 database files (obtain from \u003ca href=\"https://benlangmead.github.io/aws-indexes/k2\" rel=\"nofollow\"\u003ehttps://benlangmead.github.io/aws-indexes/k2\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie2_index\u003c/strong\u003e\u003cbr\u003e\nDirectory containing Bowtie2 index (obtain from \u003ca href=\"ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\" rel=\"nofollow\"\u003eftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\u003c/a\u003e). The specified path should NOT include the index name\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie_index_name\u003c/strong\u003e\u003cbr\u003e\nName of the bowtie index, e.g. hg19_1kgmaj\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003cp\u003eFor more information on the parameters run \u003ccode\u003enextflow run main.nf --help\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-checkpoints\" class=\"anchor\" href=\"#checkpoints\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheckpoints\u003c/h2\u003e\n\u003cp\u003eCheckpoints used throughout this workflow to fail a sample/issue warnings:\u003c/p\u003e\n\u003cp\u003eprocesses preprocessing_checkFqValidity or preprocessing_checkBamValidity\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e(Fail) If sample does not pass fqtools \u0027validate\u0027 or samtools \u0027quickcheck\u0027, as appropriate.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eprocess preprocessing_countReads\u003cbr\u003e\n2. (Fail) If sample contains \u0026lt; 100k pairs of raw reads.\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_fastp\u003cbr\u003e\n3. (Fail) If sample contains \u0026lt; 100k pairs of cleaned reads, required to all be \u0026gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10 --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size 1 --cut_tail_mean_quality 20).\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_kraken2\u003cbr\u003e\n4. (Fail) If the top family hit is not Mycobacteriaceae\u003cbr\u003e\n5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae \u003cbr\u003e\n6. (Warn) If the top family classification is mycobacterial, but this is not consistent with top genus and species classifications\u003cbr\u003e\n7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications meet minimum thresholds of \u0026gt; 5000 reads or \u0026gt; 0.5% of the total reads (this is not necessarily a concern as not all mycobacteria have a taxonomic classification at this rank) \u003cbr\u003e\n8. (Warn) If sample is mixed or contaminated - defined as containing reads \u0026gt; the 5000/0.5% thresholds from multiple non-human species\u003cbr\u003e\n9. (Warn) If sample contains multiple classifications to mycobacterial species complexes, each meeting the \u0026gt; 5000/0.5% thresholds\u003cbr\u003e\n10. (Warn) If no species classification meets the 5000/0.5% thresholds\u003cbr\u003e\n11. (Warn) If no genus classification meets the 5000/0.5% thresholds\u003cbr\u003e\n12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant given point 5)\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_identifyBacterialContaminants\u003cbr\u003e\n13. (Fail) If the sample is not contaminated and the top species hit is not one of the 10 supported Mycobacteria:\\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis\u003cbr\u003e\n14. (Fail) If the sample is not contaminated and the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003cbr\u003e\n15. (Warn) If the top species hit is supported by \u0026lt; 75% coverage\u003cbr\u003e\n16. (Warn) If the top species hit has a median coverage depth \u0026lt; 10-fold\u003cbr\u003e\n17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant species, which means we will not be able to locate its genome, and thereby remove it as a contaminant\u003cbr\u003e\n18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated with a contaminant species\u0027 taxon ID\u003cbr\u003e\n19. (Warn) If no complete genome could be found for a contaminant species. The workflow will proceed with alignment-based contaminant removal, but you\u0027re warned that there\u0027s reduced confidence in detecting reads from this species\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_downloadContamGenomes\u003cbr\u003e\n20. (Fail) If a contaminant is detected but we are unable to download a representative genome, and thereby remove it\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_summarise\u003cbr\u003e\n21. (Fail) If after having taken an alignment-based approach to decontamination, Kraken still detects a contaminant species\u003cbr\u003e\n22. (Fail) If after having taken an alignment-based approach to decontamination, the top species hit is not one of the 10 supported Mycobacteria\u003cbr\u003e\n23. (Fail) If, after successfully removing contaminants, the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620946111.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.2.1/Singularity"
    ],
    "full_name": "yh549848/singularity-cufflinks",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mycobacterial-pre-processing-pipeline\" class=\"anchor\" href=\"#mycobacterial-pre-processing-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMycobacterial Pre-processing Pipeline\u003c/h1\u003e\n\u003cp\u003eCleans and QCs reads with fastp and FastQC, classifies with Kraken2 \u0026amp; Mykrobe, removes non-bacterial content, and - by alignment to any minority genomes - disambiguates mixtures of bacterial reads.\u003c/p\u003e\n\u003cp\u003eTakes as input one directory containing pairs of fastq(.gz) or bam files.\nProduces as output one directory per sample, containing the relevant reports \u0026amp; a pair of cleaned fastqs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eThe workflow is designed to run with either docker \u003ccode\u003e-profile docker\u003c/code\u003e or singularity \u003ccode\u003e-profile singularity\u003c/code\u003e. Before running the workflow using singularity, the singularity images for the workflow will need to be built by running \u003ccode\u003esingularity/singularity_pull.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eE.g. to run the workflow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main.nf -profile singularity --filetype fastq --input_dir fq_dir --pattern \"*_R{1,2}.fastq.gz\" --unmix_myco yes \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\nnextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco no \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-params\" class=\"anchor\" href=\"#params\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParams\u003c/h2\u003e\n\u003cp\u003eThe following parameters should be set in \u003ccode\u003enextflow.config\u003c/code\u003e or specified on the command line:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003einput_dir\u003c/strong\u003e\u003cbr\u003e\nDirectory containing fastq OR bam files\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efiletype\u003c/strong\u003e\u003cbr\u003e\nFile type in input_dir. Either \"fastq\" or \"bam\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003epattern\u003c/strong\u003e\u003cbr\u003e\nRegex to match fastq files in input_dir, e.g. \"*_R{1,2}.fq.gz\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eoutput_dir\u003c/strong\u003e\u003cbr\u003e\nOutput directory\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eunmix_myco\u003c/strong\u003e\u003cbr\u003e\nDo you want to disambiguate mixed-mycobacterial samples by read alignment? Either \"yes\" or \"no\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003especies\u003c/strong\u003e\u003cbr\u003e\nPrincipal species in each sample, assuming genus Mycobacterium. Default \u0027null\u0027. If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare, kansasii, tuberculosis\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ekraken_db\u003c/strong\u003e\u003cbr\u003e\nDirectory containing \u003ccode\u003e*.k2d\u003c/code\u003e Kraken2 database files (obtain from \u003ca href=\"https://benlangmead.github.io/aws-indexes/k2\" rel=\"nofollow\"\u003ehttps://benlangmead.github.io/aws-indexes/k2\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie2_index\u003c/strong\u003e\u003cbr\u003e\nDirectory containing Bowtie2 index (obtain from \u003ca href=\"ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\" rel=\"nofollow\"\u003eftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\u003c/a\u003e). The specified path should NOT include the index name\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie_index_name\u003c/strong\u003e\u003cbr\u003e\nName of the bowtie index, e.g. hg19_1kgmaj\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003cp\u003eFor more information on the parameters run \u003ccode\u003enextflow run main.nf --help\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-checkpoints\" class=\"anchor\" href=\"#checkpoints\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheckpoints\u003c/h2\u003e\n\u003cp\u003eCheckpoints used throughout this workflow to fail a sample/issue warnings:\u003c/p\u003e\n\u003cp\u003eprocesses preprocessing_checkFqValidity or preprocessing_checkBamValidity\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e(Fail) If sample does not pass fqtools \u0027validate\u0027 or samtools \u0027quickcheck\u0027, as appropriate.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eprocess preprocessing_countReads\u003cbr\u003e\n2. (Fail) If sample contains \u0026lt; 100k pairs of raw reads.\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_fastp\u003cbr\u003e\n3. (Fail) If sample contains \u0026lt; 100k pairs of cleaned reads, required to all be \u0026gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10 --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size 1 --cut_tail_mean_quality 20).\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_kraken2\u003cbr\u003e\n4. (Fail) If the top family hit is not Mycobacteriaceae\u003cbr\u003e\n5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae \u003cbr\u003e\n6. (Warn) If the top family classification is mycobacterial, but this is not consistent with top genus and species classifications\u003cbr\u003e\n7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications meet minimum thresholds of \u0026gt; 5000 reads or \u0026gt; 0.5% of the total reads (this is not necessarily a concern as not all mycobacteria have a taxonomic classification at this rank) \u003cbr\u003e\n8. (Warn) If sample is mixed or contaminated - defined as containing reads \u0026gt; the 5000/0.5% thresholds from multiple non-human species\u003cbr\u003e\n9. (Warn) If sample contains multiple classifications to mycobacterial species complexes, each meeting the \u0026gt; 5000/0.5% thresholds\u003cbr\u003e\n10. (Warn) If no species classification meets the 5000/0.5% thresholds\u003cbr\u003e\n11. (Warn) If no genus classification meets the 5000/0.5% thresholds\u003cbr\u003e\n12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant given point 5)\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_identifyBacterialContaminants\u003cbr\u003e\n13. (Fail) If the sample is not contaminated and the top species hit is not one of the 10 supported Mycobacteria:\\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis\u003cbr\u003e\n14. (Fail) If the sample is not contaminated and the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003cbr\u003e\n15. (Warn) If the top species hit is supported by \u0026lt; 75% coverage\u003cbr\u003e\n16. (Warn) If the top species hit has a median coverage depth \u0026lt; 10-fold\u003cbr\u003e\n17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant species, which means we will not be able to locate its genome, and thereby remove it as a contaminant\u003cbr\u003e\n18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated with a contaminant species\u0027 taxon ID\u003cbr\u003e\n19. (Warn) If no complete genome could be found for a contaminant species. The workflow will proceed with alignment-based contaminant removal, but you\u0027re warned that there\u0027s reduced confidence in detecting reads from this species\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_downloadContamGenomes\u003cbr\u003e\n20. (Fail) If a contaminant is detected but we are unable to download a representative genome, and thereby remove it\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_summarise\u003cbr\u003e\n21. (Fail) If after having taken an alignment-based approach to decontamination, Kraken still detects a contaminant species\u003cbr\u003e\n22. (Fail) If after having taken an alignment-based approach to decontamination, the top species hit is not one of the 10 supported Mycobacteria\u003cbr\u003e\n23. (Fail) If, after successfully removing contaminants, the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620946061.0
  },
  {
    "data_format": 2,
    "description": "The files to build singularity images for GOMAP pipeline",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "Dill-PICL/GOMAP-img-build",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mycobacterial-pre-processing-pipeline\" class=\"anchor\" href=\"#mycobacterial-pre-processing-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMycobacterial Pre-processing Pipeline\u003c/h1\u003e\n\u003cp\u003eCleans and QCs reads with fastp and FastQC, classifies with Kraken2 \u0026amp; Mykrobe, removes non-bacterial content, and - by alignment to any minority genomes - disambiguates mixtures of bacterial reads.\u003c/p\u003e\n\u003cp\u003eTakes as input one directory containing pairs of fastq(.gz) or bam files.\nProduces as output one directory per sample, containing the relevant reports \u0026amp; a pair of cleaned fastqs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eThe workflow is designed to run with either docker \u003ccode\u003e-profile docker\u003c/code\u003e or singularity \u003ccode\u003e-profile singularity\u003c/code\u003e. Before running the workflow using singularity, the singularity images for the workflow will need to be built by running \u003ccode\u003esingularity/singularity_pull.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eE.g. to run the workflow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main.nf -profile singularity --filetype fastq --input_dir fq_dir --pattern \"*_R{1,2}.fastq.gz\" --unmix_myco yes \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\nnextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco no \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-params\" class=\"anchor\" href=\"#params\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParams\u003c/h2\u003e\n\u003cp\u003eThe following parameters should be set in \u003ccode\u003enextflow.config\u003c/code\u003e or specified on the command line:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003einput_dir\u003c/strong\u003e\u003cbr\u003e\nDirectory containing fastq OR bam files\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efiletype\u003c/strong\u003e\u003cbr\u003e\nFile type in input_dir. Either \"fastq\" or \"bam\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003epattern\u003c/strong\u003e\u003cbr\u003e\nRegex to match fastq files in input_dir, e.g. \"*_R{1,2}.fq.gz\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eoutput_dir\u003c/strong\u003e\u003cbr\u003e\nOutput directory\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eunmix_myco\u003c/strong\u003e\u003cbr\u003e\nDo you want to disambiguate mixed-mycobacterial samples by read alignment? Either \"yes\" or \"no\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003especies\u003c/strong\u003e\u003cbr\u003e\nPrincipal species in each sample, assuming genus Mycobacterium. Default \u0027null\u0027. If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare, kansasii, tuberculosis\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ekraken_db\u003c/strong\u003e\u003cbr\u003e\nDirectory containing \u003ccode\u003e*.k2d\u003c/code\u003e Kraken2 database files (obtain from \u003ca href=\"https://benlangmead.github.io/aws-indexes/k2\" rel=\"nofollow\"\u003ehttps://benlangmead.github.io/aws-indexes/k2\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie2_index\u003c/strong\u003e\u003cbr\u003e\nDirectory containing Bowtie2 index (obtain from \u003ca href=\"ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\" rel=\"nofollow\"\u003eftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\u003c/a\u003e). The specified path should NOT include the index name\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie_index_name\u003c/strong\u003e\u003cbr\u003e\nName of the bowtie index, e.g. hg19_1kgmaj\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003cp\u003eFor more information on the parameters run \u003ccode\u003enextflow run main.nf --help\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-checkpoints\" class=\"anchor\" href=\"#checkpoints\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheckpoints\u003c/h2\u003e\n\u003cp\u003eCheckpoints used throughout this workflow to fail a sample/issue warnings:\u003c/p\u003e\n\u003cp\u003eprocesses preprocessing_checkFqValidity or preprocessing_checkBamValidity\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e(Fail) If sample does not pass fqtools \u0027validate\u0027 or samtools \u0027quickcheck\u0027, as appropriate.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eprocess preprocessing_countReads\u003cbr\u003e\n2. (Fail) If sample contains \u0026lt; 100k pairs of raw reads.\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_fastp\u003cbr\u003e\n3. (Fail) If sample contains \u0026lt; 100k pairs of cleaned reads, required to all be \u0026gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10 --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size 1 --cut_tail_mean_quality 20).\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_kraken2\u003cbr\u003e\n4. (Fail) If the top family hit is not Mycobacteriaceae\u003cbr\u003e\n5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae \u003cbr\u003e\n6. (Warn) If the top family classification is mycobacterial, but this is not consistent with top genus and species classifications\u003cbr\u003e\n7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications meet minimum thresholds of \u0026gt; 5000 reads or \u0026gt; 0.5% of the total reads (this is not necessarily a concern as not all mycobacteria have a taxonomic classification at this rank) \u003cbr\u003e\n8. (Warn) If sample is mixed or contaminated - defined as containing reads \u0026gt; the 5000/0.5% thresholds from multiple non-human species\u003cbr\u003e\n9. (Warn) If sample contains multiple classifications to mycobacterial species complexes, each meeting the \u0026gt; 5000/0.5% thresholds\u003cbr\u003e\n10. (Warn) If no species classification meets the 5000/0.5% thresholds\u003cbr\u003e\n11. (Warn) If no genus classification meets the 5000/0.5% thresholds\u003cbr\u003e\n12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant given point 5)\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_identifyBacterialContaminants\u003cbr\u003e\n13. (Fail) If the sample is not contaminated and the top species hit is not one of the 10 supported Mycobacteria:\\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis\u003cbr\u003e\n14. (Fail) If the sample is not contaminated and the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003cbr\u003e\n15. (Warn) If the top species hit is supported by \u0026lt; 75% coverage\u003cbr\u003e\n16. (Warn) If the top species hit has a median coverage depth \u0026lt; 10-fold\u003cbr\u003e\n17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant species, which means we will not be able to locate its genome, and thereby remove it as a contaminant\u003cbr\u003e\n18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated with a contaminant species\u0027 taxon ID\u003cbr\u003e\n19. (Warn) If no complete genome could be found for a contaminant species. The workflow will proceed with alignment-based contaminant removal, but you\u0027re warned that there\u0027s reduced confidence in detecting reads from this species\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_downloadContamGenomes\u003cbr\u003e\n20. (Fail) If a contaminant is detected but we are unable to download a representative genome, and thereby remove it\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_summarise\u003cbr\u003e\n21. (Fail) If after having taken an alignment-based approach to decontamination, Kraken still detects a contaminant species\u003cbr\u003e\n22. (Fail) If after having taken an alignment-based approach to decontamination, the top species hit is not one of the 10 supported Mycobacteria\u003cbr\u003e\n23. (Fail) If, after successfully removing contaminants, the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1620903521.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "Dill-PICL/GOMAP-base",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1184\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-gomap-base\" class=\"anchor\" href=\"#gomap-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGOMAP-base\u003c/h1\u003e\n\u003cp\u003eThis is the base image for the GOMAP-singularity container. This base image has all the requirements installed for running GOMAP\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620899212.0
  },
  {
    "data_format": 2,
    "description": "Mycobacterial pre-processing pipeline",
    "filenames": [
      "singularity/Singularity.ppFqtools",
      "singularity/Singularity.ppPerljson",
      "singularity/Singularity.ppFastqc",
      "singularity/Singularity.ppMykrobe",
      "singularity/Singularity.ppBowtie2",
      "singularity/Singularity.ppBedtools",
      "singularity/Singularity.ppBwa",
      "singularity/Singularity.ppFastp",
      "singularity/Singularity.ppKraken2"
    ],
    "full_name": "oxfordmmm/preprocessing",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mycobacterial-pre-processing-pipeline\" class=\"anchor\" href=\"#mycobacterial-pre-processing-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMycobacterial Pre-processing Pipeline\u003c/h1\u003e\n\u003cp\u003eCleans and QCs reads with fastp and FastQC, classifies with Kraken2 \u0026amp; Mykrobe, removes non-bacterial content, and - by alignment to any minority genomes - disambiguates mixtures of bacterial reads.\u003c/p\u003e\n\u003cp\u003eTakes as input one directory containing pairs of fastq(.gz) or bam files.\nProduces as output one directory per sample, containing the relevant reports \u0026amp; a pair of cleaned fastqs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eThe workflow is designed to run with either docker \u003ccode\u003e-profile docker\u003c/code\u003e or singularity \u003ccode\u003e-profile singularity\u003c/code\u003e. Before running the workflow using singularity, the singularity images for the workflow will need to be built by running \u003ccode\u003esingularity/singularity_pull.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eE.g. to run the workflow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main.nf -profile singularity --filetype fastq --input_dir fq_dir --pattern \"*_R{1,2}.fastq.gz\" --unmix_myco yes \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\nnextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco no \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-params\" class=\"anchor\" href=\"#params\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParams\u003c/h2\u003e\n\u003cp\u003eThe following parameters should be set in \u003ccode\u003enextflow.config\u003c/code\u003e or specified on the command line:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003einput_dir\u003c/strong\u003e\u003cbr\u003e\nDirectory containing fastq OR bam files\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efiletype\u003c/strong\u003e\u003cbr\u003e\nFile type in input_dir. Either \"fastq\" or \"bam\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003epattern\u003c/strong\u003e\u003cbr\u003e\nRegex to match fastq files in input_dir, e.g. \"*_R{1,2}.fq.gz\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eoutput_dir\u003c/strong\u003e\u003cbr\u003e\nOutput directory\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eunmix_myco\u003c/strong\u003e\u003cbr\u003e\nDo you want to disambiguate mixed-mycobacterial samples by read alignment? Either \"yes\" or \"no\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003especies\u003c/strong\u003e\u003cbr\u003e\nPrincipal species in each sample, assuming genus Mycobacterium. Default \u0027null\u0027. If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare, kansasii, tuberculosis\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ekraken_db\u003c/strong\u003e\u003cbr\u003e\nDirectory containing \u003ccode\u003e*.k2d\u003c/code\u003e Kraken2 database files (obtain from \u003ca href=\"https://benlangmead.github.io/aws-indexes/k2\" rel=\"nofollow\"\u003ehttps://benlangmead.github.io/aws-indexes/k2\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie2_index\u003c/strong\u003e\u003cbr\u003e\nDirectory containing Bowtie2 index (obtain from \u003ca href=\"ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\" rel=\"nofollow\"\u003eftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\u003c/a\u003e). The specified path should NOT include the index name\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie_index_name\u003c/strong\u003e\u003cbr\u003e\nName of the bowtie index, e.g. hg19_1kgmaj\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003cp\u003eFor more information on the parameters run \u003ccode\u003enextflow run main.nf --help\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-checkpoints\" class=\"anchor\" href=\"#checkpoints\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheckpoints\u003c/h2\u003e\n\u003cp\u003eCheckpoints used throughout this workflow to fail a sample/issue warnings:\u003c/p\u003e\n\u003cp\u003eprocesses preprocessing_checkFqValidity or preprocessing_checkBamValidity\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e(Fail) If sample does not pass fqtools \u0027validate\u0027 or samtools \u0027quickcheck\u0027, as appropriate.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eprocess preprocessing_countReads\u003cbr\u003e\n2. (Fail) If sample contains \u0026lt; 100k pairs of raw reads.\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_fastp\u003cbr\u003e\n3. (Fail) If sample contains \u0026lt; 100k pairs of cleaned reads, required to all be \u0026gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10 --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size 1 --cut_tail_mean_quality 20).\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_kraken2\u003cbr\u003e\n4. (Fail) If the top family hit is not Mycobacteriaceae\u003cbr\u003e\n5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae \u003cbr\u003e\n6. (Warn) If the top family classification is mycobacterial, but this is not consistent with top genus and species classifications\u003cbr\u003e\n7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications meet minimum thresholds of \u0026gt; 5000 reads or \u0026gt; 0.5% of the total reads (this is not necessarily a concern as not all mycobacteria have a taxonomic classification at this rank) \u003cbr\u003e\n8. (Warn) If sample is mixed or contaminated - defined as containing reads \u0026gt; the 5000/0.5% thresholds from multiple non-human species\u003cbr\u003e\n9. (Warn) If sample contains multiple classifications to mycobacterial species complexes, each meeting the \u0026gt; 5000/0.5% thresholds\u003cbr\u003e\n10. (Warn) If no species classification meets the 5000/0.5% thresholds\u003cbr\u003e\n11. (Warn) If no genus classification meets the 5000/0.5% thresholds\u003cbr\u003e\n12. (Fail) If no family classification meets the 5000/0.5% thresholds (redundant given point 5)\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_identifyBacterialContaminants\u003cbr\u003e\n13. (Fail) If the sample is not contaminated and the top species hit is not one of the 10 supported Mycobacteria:\\ abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis\u003cbr\u003e\n14. (Fail) If the sample is not contaminated and the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003cbr\u003e\n15. (Warn) If the top species hit is supported by \u0026lt; 75% coverage\u003cbr\u003e\n16. (Warn) If the top species hit has a median coverage depth \u0026lt; 10-fold\u003cbr\u003e\n17. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant species, which means we will not be able to locate its genome, and thereby remove it as a contaminant\u003cbr\u003e\n18. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated with a contaminant species\u0027 taxon ID\u003cbr\u003e\n19. (Warn) If no complete genome could be found for a contaminant species. The workflow will proceed with alignment-based contaminant removal, but you\u0027re warned that there\u0027s reduced confidence in detecting reads from this species\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_downloadContamGenomes\u003cbr\u003e\n20. (Fail) If a contaminant is detected but we are unable to download a representative genome, and thereby remove it\u003c/p\u003e\n\u003cp\u003eprocess preprocessing_summarise\u003cbr\u003e\n21. (Fail) If after having taken an alignment-based approach to decontamination, Kraken still detects a contaminant species\u003cbr\u003e\n22. (Fail) If after having taken an alignment-based approach to decontamination, the top species hit is not one of the 10 supported Mycobacteria\u003cbr\u003e\n23. (Fail) If, after successfully removing contaminants, the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1620868687.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "yookuda/singularity-mysql",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-mysql\" class=\"anchor\" href=\"#singularity-mysql\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-mysql\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1593177205.0
  },
  {
    "data_format": 2,
    "description": "nf-ncov-voc",
    "filenames": [
      "environments/Singularity"
    ],
    "full_name": "anwarMZ/nf-ncov-voc",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-ncov-voc\" class=\"anchor\" href=\"#nf-ncov-voc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-ncov-voc\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620791202.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for PacBio applications.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "mcw-rcc/pacbioapps",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pacbio-apps-container\" class=\"anchor\" href=\"#pacbio-apps-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePacBio Apps Container\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1263\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA container for installing and running PacBio applications using Conda.\u003c/p\u003e\n\u003cp\u003eTo use the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ module load pacbioapps/latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUse the included wrapper script \u003ccode\u003erunpbapps\u003c/code\u003e to run commands (e.g., blasr):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ runpbapps blasr --version\nblasr\t5.3.2\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity-container"
    ],
    "updated_at": 1586494536.0
  },
  {
    "data_format": 2,
    "description": "Analytical MK approach and ABC inference based on Julia",
    "filenames": [
      "scripts/singularity/Singularity"
    ],
    "full_name": "jmurga/Analytical.jl",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-abc-mk\" class=\"anchor\" href=\"#abc-mk\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eABC-MK\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://jmurga.github.io/Analytical.jl/dev\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/docs-latest-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eExtended ABC-MK calculations accounting for background selection and weak adaptation based on Julia language.\u003c/p\u003e\n\u003cp\u003eIncluded in this repository are the scripts used to simulate and infer parameters from ABC software.\u003c/p\u003e\n\u003cp\u003eTo install the module we highly recommend to use \u003ca href=\"https://julialang.org/downloads/\" rel=\"nofollow\"\u003eLTS official Julia binaries\u003c/a\u003e. You can easily export the Julia bin through \u003ccode\u003eexport PATH=\"/path/to/directory/julia-1.0.5/bin:$PATH\"\u003c/code\u003e. In addition, since the package use \u003cem\u003escipy\u003c/em\u003e functions, we recommend executing Julia activating the following \u003ca href=\"https://github.com/jmurga/Analytical.jl/tree/master/scripts/abc-mk.yml\"\u003econda enviroment\u003c/a\u003e (or you can just just install \u003cem\u003escipy\u003c/em\u003e in your default python). Once Julia is installed just run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ejulia -e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003eusing Pkg;Pkg.add(PackageSpec(path=\"https://github.com/jmurga/Analytical.jl\"))\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe provide a Docker image based on Debian including Julia and all packages needed to run the estimations. It also includes jupyterlab and several data science packages. You can run the whole Debian image or just the jupyterlab instance pulling the image from dockerhub:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Pull the image\u003c/span\u003e\ndocker pull jmurga/mktest\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Run docker bash interactive session linking to some local volume to export data\u003c/span\u003e\ndocker run -i -t -v \u003cspan class=\"pl-smi\"\u003e${HOME}\u003c/span\u003e/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eanyData\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e:/data/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eanyData\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e jmurga/abcmk julia -J /root/mktest.so\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Run only jupyter notebook from docker image. Change the port if 8888 is already used\u003c/span\u003e\ndocker run -i -t -p 8888:8888 jmurga/abcmk /bin/bash -c \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ejupyter-lab --ip=\u0027*\u0027 --port=8888 --no-browser --allow-root\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621523421.0
  },
  {
    "data_format": 2,
    "description": "GTN ToolFactory flavour of https://github.com/bgruening/docker-galaxy-stable ",
    "filenames": [
      "compose/toolfactory-configurator/files/Singularity.def"
    ],
    "full_name": "fubar2/toolfactory-galaxy-server",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-galaxy-toolfactory-appliance\" class=\"anchor\" href=\"#galaxy-toolfactory-appliance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGalaxy ToolFactory Appliance\u003c/h1\u003e\n\u003cp\u003eThis is a ToolFactory flavoured developer appliance in Docker extending the basic \u003ccode\u003edocker-galaxy-stable\u003c/code\u003e composition.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe ToolFactory - a form driven code generator to make new Galaxy tools from scripts - is installed with a testing tool.\u003c/li\u003e\n\u003cli\u003eA container for post-install configuration is added to create the ToolFactory flavour. The new container then runs a planemo server\noutside the Galaxy tool execution environment to test tools, returning the planemo test\nreports, log and updated archive to the user\u0027s current history.\u003c/li\u003e\n\u003cli\u003eA history containing 15 demonstration tool generation jobs to rerun and build on. Samples use bash, python, perl (yes, even perl. Galaxy is a very welcoming community..),\nRscript and for more discerning users, prolog and lisp. Any scriptable language in Conda should work.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-built-on-docker-galaxy-stable\" class=\"anchor\" href=\"#built-on-docker-galaxy-stable\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilt on docker-galaxy-stable\u003c/h2\u003e\n\u003cp\u003eThis flavour of the docker-compose infrastructure is based on \u003ca href=\"https://github.com/bgruening/docker-galaxy-stable\"\u003ehttps://github.com/bgruening/docker-galaxy-stable\u003c/a\u003e - there is excellent documentation at\nthat site. Respect. A few minor pointers only are provided below - please refer to the original documentation for details about this extensive infrastructure for Galaxy flavours including\n(untested) cluster and other deployment options.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-a-standalone-pop-up-desktop-galaxy-appliance\" class=\"anchor\" href=\"#a-standalone-pop-up-desktop-galaxy-appliance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA standalone, pop-up desktop Galaxy appliance\u003c/h2\u003e\n\u003cp\u003eThe Appliance supporting the ToolFactory is a fully featured \u003ccode\u003edocker-galaxy-stable\u003c/code\u003e Galaxy server, ideal for scientists and developers\nwho can use a private pop-up desktop server for learning how Galaxy works, building new tools, using interactive environments and any available toolshed tools to do real work,\npotentially at scale. The Appliance adds only one specific container. The others are all pulled from Bj\u00f6rn\u0027s docker-galaxy-stable quay.io containers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-private-desktop-use-only-is-recommended\" class=\"anchor\" href=\"#private-desktop-use-only-is-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrivate desktop use only is recommended.\u003c/h2\u003e\n\u003cp\u003eThis Appliance has been configured to weaken some of Galaxy\u0027s strict job-runner isolation features so it can install and test tools conveniently.\nIt is safe to run on a private Linux laptop or workstation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRunning it on any server accessible from the public internet exposes it to potential miscreants. This is strongly discouraged\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAlthough Galaxy\u0027s job execution security is very tight and safe, allowing potentially hostile users to build and then immediately run their own tools exposes any production server\nto unwelcome security risk.\u003c/p\u003e\n\u003cp\u003eThe Appliance runs as a set of Docker containers, so it is secured to that extent from the host system. ToolFactory and other related source code is\nincluded in this repository for the curious or the dubious. Specific security disclosure details are discussed in the compose documentation. The mechanisms described offer a\nconvenient way for tools to remotely execute tasks outside the Galaxy job execution environment in suitably private environments such as this Appliance.\nThis may open up interesting uses for Galaxy on the desktop with specialised tools accessing GPU or other local resources.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tutorial-and-documentation\" class=\"anchor\" href=\"#tutorial-and-documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTutorial and documentation\u003c/h2\u003e\n\u003cp\u003eThere is an accompanying GTN ToolFactory developer tutorial PR in beta test.\n\u003ca href=\"https://training.galaxy.lazarus.name/training-material/topics/dev/tutorials/tool-generators/tutorial.html\" rel=\"nofollow\"\u003eIt is temporarily available here on a private server\u003c/a\u003e\nto help explain how this Appliance can be used to generate Galaxy tools from working command line scripts. There is a linked advanced tutorial.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-and-startup\" class=\"anchor\" href=\"#installation-and-startup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation and startup\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/fubar2/toolfactory-galaxy-server\ncd toolfactory-galaxy-server/compose\ndocker-compose pull\ndocker-compose up\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYour appliance should be running with a local Galaxy on localhost:8080 after a fair bit of activity and about 5-10 minutes. Wait until all is done before logging in.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWatch the logs as they scroll by on the terminal. They are very instructive and informative for those who need to understand how Galaxy actually works.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eKeep an eye out for Conda processes on your machine.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWait until they \u003cstrong\u003eall\u003c/strong\u003e stop.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRestarting is much faster.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOut of the box login is \u003ccode\u003eadmin@galaxy.org\u003c/code\u003e and the password is \u003ccode\u003epassword\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis is obviously insecure but convenient and easily changed at first login.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eChange it more permanently in docker-compose.yml if you intend to use this Appliance for your own work.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAt present, the admin_key default (\u003ccode\u003efakekey\u003c/code\u003e) is hard-wired into the ToolFactory boot process, but should be changed at first login to something less well known if\nthe appliance is exposed at all.\u003c/li\u003e\n\u003cli\u003eThe API key is the administrative key for the appliance Galaxy so if the Appliance is accessible on a network, it is\nexposed to easy API based remote mischief until a new API key is generated. Another good reason not to expose the Appliance anywhere.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe container \u003ccode\u003e/export\u003c/code\u003e directory is mounted locally at \u003ccode\u003e...compose/export\u003c/code\u003e .\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-demonstration-tools-are-the-functional-documentation\" class=\"anchor\" href=\"#demonstration-tools-are-the-functional-documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemonstration tools are the functional documentation\u003c/h2\u003e\n\u003cp\u003eSee how they were built, by rerunning the generating job to recreate the ToolFactory form for the run.\nLook at the form settings for each one to see what can be done.\u003c/p\u003e\n\u003cp\u003eTo view the form that generated each job, open the toolshed archive or the XML by clicking on it, and select the \u003ccode\u003ererun\u003c/code\u003e button.\nEdit the form and rerun to create an updated tool. The history has previous versions so work is not entirely lost.\nChange the tool ID to change the tool name and avoid overwriting previous versions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-generating-your-own-tools\" class=\"anchor\" href=\"#generating-your-own-tools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerating your own tools\u003c/h2\u003e\n\u003cp\u003eGenerated tools are installed on build. The whole process takes a few seconds normally. Refreshing the Galaxy panel will be needed for the tool menu to be updated after the\nnewly generated tool is installed. It will be found in the \u003ccode\u003eToolFactory Generated Tools\u003c/code\u003e section.\u003c/p\u003e\n\u003cp\u003eA new tool requiring Conda dependencies will take time as those must be installed the first time it is run. After that first run or if the dependency is already installed,\nthe tool will run without delay.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIf two or more new tools that need new dependencies on first run try to install them at the same time, Conda will fail in interesting ways. It is not designed for multiple simultaneous users\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun one new tool at a time to avoid this causing problems. Once the dependency is installed, the tool will not need to install it again unless the dependency is altered.\u003c/p\u003e\n\u003cp\u003eChoose the names thoughtfully and be warned: there are no checks on tool names - any existing installed tool with the same name will be overwritten permanently. The history\nwill retain all the generating jobs if you accidentally overwrite a tool.\u003c/p\u003e\n\u003cp\u003eRefresh the page (by clicking the home icon or the \"analysis\" tab) to see them in the \u003ccode\u003eToolFactory Generated Tools\u003c/code\u003e section and try them out.\u003c/p\u003e\n\u003cp\u003eRerun the job and adjust the form. Rinse and repeat until ready.\u003c/p\u003e\n\u003cp\u003eThe generated tool has not been run to generate test outputs, so the archive is not complete although the installed tool may work fine.\u003c/p\u003e\n\u003cp\u003eTo generate a real, tested toolshed archive, use the companion \u003ccode\u003eplanemo_test\u003c/code\u003e tool. Planemo will be run in a separate\ncontainer. The generated test outputs and the newly updated tested toolshed archive will appear when the job is done. The very first test in a\nfresh Appliance takes a few minutes as Conda installs some dependencies - only needed once.\nSubsequently more like a minute or two, depending on Conda time to install all new dependencies needed\nfor the tool to run.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-safely-shut-the-appliance-down\" class=\"anchor\" href=\"#to-safely-shut-the-appliance-down\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo safely shut the appliance down\u003c/h2\u003e\n\u003cp\u003eIf you used \u003ccode\u003e-d\u003c/code\u003e to detach,\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003edocker-compose down\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003efrom the same place you started should shut it down nicely\u003c/p\u003e\n\u003cp\u003eOtherwise, CtrlC from the attached console will stop the services.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-if-things-go-wrong-or-if-the-appliance-is-no-longer-needed\" class=\"anchor\" href=\"#if-things-go-wrong-or-if-the-appliance-is-no-longer-needed\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIf things go wrong or if the Appliance is no longer needed\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDelete the \u003ccode\u003e...compose/export\u003c/code\u003e directory - you will need \u003ccode\u003esudo rm -rf export/*\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThen you can delete the parent git or wget \u003ccode\u003etoolfactory-galaxy-server\u003c/code\u003e directory\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse \u003ccode\u003edocker system prune\u003c/code\u003e and respond \u003ccode\u003ey\u003c/code\u003e to the prompt to clean up any damaged containers.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRemove all the docker images in the usual way.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-security---why-this-appliance-is-not-suitable-for-exposing-on-the-public-internet\" class=\"anchor\" href=\"#security---why-this-appliance-is-not-suitable-for-exposing-on-the-public-internet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSecurity - why this Appliance is not suitable for exposing on the public internet\u003c/h2\u003e\n\u003cp\u003eSee \u003ca href=\"https://github.com/fubar2/toolfactory-galaxy-server/tree/main/compose#readme\"\u003ethe notes on Appliance security considerations.\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621871144.0
  },
  {
    "data_format": 2,
    "description": "Python code to identify/analyze cells from microscopic stack images.",
    "filenames": [
      "Singularity.cellprof2"
    ],
    "full_name": "ArjitM/CellMorphology",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cellmorphology\" class=\"anchor\" href=\"#cellmorphology\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCellMorphology\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eUnder development\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePython code to binarize noisy input images of microscopic images for further cell morphology analysis.\u003c/p\u003e\n\u003cp\u003eAttempted localized edge detection, noise elimination, and edge propagation.\nClustering functional\nsegmentation needs touch-ups, constrictions identified OK, bends pending\nStack image collation needs review\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "python",
      "image-processing",
      "cell-morphology-analysis",
      "matlab"
    ],
    "updated_at": 1620661441.0
  },
  {
    "data_format": 2,
    "description": "Implements GA-DQN tuner which consists of a genetic algorithm that uses two deep Q-network agents.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "lhutton1/ga-dqn-tuner",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-generating-high-performance-code-for-deep-learning-workloads-a-reinforcement-learning-based-approach\" class=\"anchor\" href=\"#generating-high-performance-code-for-deep-learning-workloads-a-reinforcement-learning-based-approach\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerating high-performance code for deep learning workloads: a reinforcement learning based approach.\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eImplemented as part of a final year dissertation. Should not be considered for production use.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis project aims to apply reinforcement learning to auto-tuning in AutoTVM (part of the TVM machine learning compiler),\nin order to improve the experience of the end user. Currently, reinforcement learning is applied to the GATuner - a genetic algorithm\nthat repeatedly applies elitism, 2-point crossover and mutation to a population. Named \u003cstrong\u003eGA-DQN\u003c/strong\u003e, the new tuner uses two independent\ndeep Q-network (DQN)\u0027s that are applied to crossover and mutation. Crossover is completed by allowing DQN to suggest the point at\nwhich to crossover a gene, while, mutation is completed by allowing DQN to select which detail to randomly mutate. In addition, an evaluation\nframework is provided to assess the performance of GA-DQN.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/assets/ga-dqn-pipeline.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/ga-dqn-pipeline.png\" alt=\"GA-DQN tuning pipeline\" title=\"GA-DQN tuning pipeline\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eTo use the tuner, TVM must be installed and visible within your python environment. Due to needing additional features not available in a released\nversion of TVM, a forked version of TVM is used which applies a small amount debugging code and a fix to the PyTorch front-end parser. A pinned\nversion is also used as TVM is mostly in a development stage and the API\u0027s used are unstable. Consequently, the GA-DQN tuner has only been tested\nwith this specific commit, along with small modifications ontop. The required version can be pulled from git like so:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone --recursive https://github.com/lhutton1/tvm.git tvm\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e tvm\ngit checkout autotvm-measure-remote-time\ngit checkout d2452502b9486a7993d9dec3d04e449efdd81cf7\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTVM also requires a number of dependencies such as: Cuda, Python3.6, LLVM, XGBoost (for the XGBTuner) and PyTorch (for the GA-DQN tuner). As such, we recommend using a containerised environment powered by Singularity. Similar to docker, an image must be built from which containers can be run based on the image. First install Singularity, then build the image using a simple script provided:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Install Singularity\u003c/span\u003e\nsudo wget -O- http://neuro.debian.net/lists/xenial.us-ca.full \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e sudo tee /etc/apt/sources.list.d/neurodebian.sources.list \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \\\n    sudo apt-key adv --recv-keys --keyserver hkp://pool.sks-keyservers.net:80 0xA5D32F012649A5A9 \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \\\n    sudo apt-get update\n    \nsudo apt-get install -y singularity-container\n    \n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Build image\u003c/span\u003e\n./create_image.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFrom this a container can be created and GA-DQN can be run from within this container using the presented shell:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./create_container.sh rl-tuner.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow in the shell, test your container works correctly by attempting to run the evaluation framework help prompt:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython driver.py --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003eNote: This has been tested on a Ubuntu 18.04 setup and is not guaranteed to work with other operating systems. These scripts have also been tested on the University of Leeds HPC cluster, ARC.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote: it is possible to build TVM and install its dependencies from scratch, although this is not recommended due to the number of packages required. The process required should be similar to that provided in \u003ccode\u003ecreate_image.sh\u003c/code\u003e script. However, it is recommended you create a new virtual environment for python in this process.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-rl-tuner\" class=\"anchor\" href=\"#rl-tuner\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRL Tuner\u003c/h2\u003e\n\u003cp\u003eGA-DQN is a tuner that combines advancements in reinforcement learning and the genetic algorithm tuner that currently exists in TVM. Two independent deep Q-network (DQN)\u0027s are used to suggest where to crossover genes and which detail of a gene to mutate.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ga-tuner\" class=\"anchor\" href=\"#ga-tuner\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGA Tuner\u003c/h2\u003e\n\u003cp\u003eThe GA tuner is code obtained from the open source TVM compiler. It is here for convenience and to allow a small amount of debug code to be added so that it can be evaluated. This work is not my own.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-evaluation-framework-tools\" class=\"anchor\" href=\"#evaluation-framework-tools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEvaluation framework (tools)\u003c/h2\u003e\n\u003cp\u003eProvides a series of tools and experiments to quickly test various tuning algorithms in AutoTVM. Use tune and benchmark commands on a series of pre-trained models to evaluate random, genetic algorithm, extreme gradient boost and GA-DQN algorithms. Use the experiment framework to evaluate various aspects of GA-DQN, with graphical monitoring.\u003c/p\u003e\n\u003cp\u003eA command line driver is provided for this framework:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython driver.py -m=tune -c=../config-example.json\npython driver.py -m=benchmark -c=../config-example.json\npython driver.py -m=experiment -c=../config-example.json\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ga-dqn-pipeline-example\" class=\"anchor\" href=\"#ga-dqn-pipeline-example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGA-DQN pipeline example\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/assets/ga-dqn-pipeline-example.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/assets/ga-dqn-pipeline-example.png\" alt=\"GA-DQN pipeline example\" title=\"GA-DQN pipeline example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1620668898.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/SingularityAnaconda_v3.def",
      "containers/Singularity.def",
      "containers/SingularityAnaconda_v2.def",
      "containers/SingularityAnaconda.def"
    ],
    "full_name": "reisportela/PythonCourse",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pythoncourse\" class=\"anchor\" href=\"#pythoncourse\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePythonCourse\u003c/h1\u003e\n\u003cp\u003eFrom A to Z. For fun.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620582753.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.breakseq",
      "Singularity.pophuman",
      "Singularity.abcmk"
    ],
    "full_name": "jmurga/bgd-pic",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pythoncourse\" class=\"anchor\" href=\"#pythoncourse\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePythonCourse\u003c/h1\u003e\n\u003cp\u003eFrom A to Z. For fun.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620435632.0
  },
  {
    "data_format": 2,
    "description": "generic fmri analysis environment",
    "filenames": [
      "Singularity.bak",
      "Singularity",
      "Singularity_fsl"
    ],
    "full_name": "dchaimow/gfae",
    "latest_release": null,
    "readme": "\u003cp\u003egeneric fmri analysis environment\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620511929.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "oogasawa/singularity-ubuntu20",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-ubuntu20\" class=\"anchor\" href=\"#singularity-ubuntu20\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-ubuntu20\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620420675.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for HERA software",
    "filenames": [
      "Singularity.casa6_modular",
      "Singularity.hera1",
      "Singularity.rtp",
      "Singularity.casa_imaging"
    ],
    "full_name": "HERA-Team/hera-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hera-singularity\" class=\"anchor\" href=\"#hera-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehera-singularity\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4892\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notice\" class=\"anchor\" href=\"#notice\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotice\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eApril 27, 2021\u003c/strong\u003e:\n\u003ca href=\"https://singularityhub.github.io/singularityhub-docs/2021/going-read-only/\" rel=\"nofollow\"\u003eSingularity Hub remote built service is no longer available.\u003c/a\u003e We are considering other alternative. The old Singularity Hub builds can still be accessed via the badge link above, which now redirects to DataLad. The \u003ccode\u003esingularity pull\u003c/code\u003e can also do the pull from datalad URL. We will manually build and upload to Ilifu for the time being.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eApril 28, 2021\u003c/strong\u003e:\nThe containers have been built and are available at \u003ccode\u003e/ilifu/astro/projects/hera/containers\u003c/code\u003e, with a few additional new containers that will be documented soon. We will keep rebuilding and replacing these files weekly, to keep the software stack up to date with the development, until we have an automated solution.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eThis repository contains recipe files for building singularity containers for the HERA software suits. The containers are remotely built on Singularity Hub when the recipes are pushed to the \u003ccode\u003emain\u003c/code\u003e branch. Container images can be directly download from the the badge link above or by using the \u003ccode\u003esingularity pull\u003c/code\u003e command line (see \u003ca href=\"##-Singularity-Commands\"\u003ebelow\u003c/a\u003e). Ilifu users, make sure to read \u003ca href=\"###-Specific-Usages-for-Ilifu\"\u003eSpecific Usages for Ilifu\u003c/a\u003e section and check the relevant page on the HERA wiki.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-container-and-singularity\" class=\"anchor\" href=\"#about-container-and-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout Container and Singularity\u003c/h2\u003e\n\u003cp\u003eContainers are encapsulated software environments and abstract the software and applications from the underlying operating system. This allows users to run workflows in customized environments, switch between environments, and to share these environments with colleagues and research teams.\u003c/p\u003e\n\u003cp\u003eSingularity is a free, cross-platform and open-source computer program that performs operating-system-level virtualization also known as containerization (another widely used one being Docker).\u003c/p\u003e\n\u003cp\u003eA singularity container is required for computing on the Ilifu cloud-computing cluster, which HERA has access (see the HERA wiki page on this).\u003c/p\u003e\n\u003cp\u003eSuggestion for other container recipes and implementations are welcome!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-container-content\" class=\"anchor\" href=\"#container-content\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer Content\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python-packages\" class=\"anchor\" href=\"#python-packages\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython Packages\u003c/h3\u003e\n\u003cp\u003eAll containers are built with \u003ccode\u003eUbuntu 20.04\u003c/code\u003e and \u003ccode\u003eminiconda\u003c/code\u003e with \u003ccode\u003epython=3.8\u003c/code\u003e unless otherwise specify \u003ca href=\"###-Different-Between-Containers:\"\u003ebelow\u003c/a\u003e, and come standard with the following packages:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eData Analysis\u003c/th\u003e\n\u003cth\u003eAstronomical\u003c/th\u003e\n\u003cth\u003eHERA\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003edask\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eaipy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003elinsolve\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ejupyterlab\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eastropy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003euvtools\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ematplotlib\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eastropy-healpix\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_qm\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003enumpy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eastroquery\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_cal\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003epandas\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ecartopy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_sim\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003escipy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehealpy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_psepc\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003escikit-learn\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003ccode\u003epyuvdata\u003c/code\u003e\u003csup\u003e\u003ca href=\"#myfootnote1\"\u003e1\u003c/a\u003e\u003c/sup\u003e\n\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003exarray\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003ccode\u003epyuvsim\u003c/code\u003e\u003csup\u003e\u003ca href=\"#myfootnote2\"\u003e2\u003c/a\u003e\u003c/sup\u003e\n\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003ca name=\"user-content-myfootnote2\"\u003e1\u003c/a\u003e: with CASA measurement sets, HEALPix beam, and CST beam functionalities\u003cbr\u003e\n\u003ca name=\"user-content-myfootnote1\"\u003e2\u003c/a\u003e: with profiling and full simulator\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-different-between-containers\" class=\"anchor\" href=\"#different-between-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDifferent Between Containers:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ehera1\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eIntended for general-purpose computing with most of the commonly used data analysis, astronomical, and HERA software packages\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ertp\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eFor running the RTP pipeline and analysis with \u003ccode\u003emakeflow\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eEquivalent to \u003ccode\u003ehera1\u003c/code\u003e with an addition of \u003ccode\u003ehera_opm\u003c/code\u003e, \u003ccode\u003ehera_mc\u003c/code\u003e, and  \u003ccode\u003ehera_notebook_templates\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ehera_pipelines\u003c/code\u003e is cloned to \u003ccode\u003e/usr/local\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecasa_imaging\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eEquivalent to \u003ccode\u003ehera1\u003c/code\u003e with a full installation of \u003ccode\u003ecasa-6\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecasa6_modular\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eEquivalent to \u003ccode\u003ehera1\u003c/code\u003e with a pip-wheel installation of \u003ccode\u003ecasa-6\u003c/code\u003e, making \u003ccode\u003ecasatasks\u003c/code\u003e, \u003ccode\u003ecasatools\u003c/code\u003e, and \u003ccode\u003ecasampi\u003c/code\u003e packages available (see \u003ca href=\"https://casa-pip.nrao.edu/\" rel=\"nofollow\"\u003ehttps://casa-pip.nrao.edu/\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003eBased on \u003ccode\u003ePython 3.6\u003c/code\u003e and \u003ccode\u003eUbuntu 18.04\u003c/code\u003e for casa-pip compatibility.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-environment-variables\" class=\"anchor\" href=\"#environment-variables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnvironment Variables\u003c/h3\u003e\n\u003cp\u003eThe following environment variables are also exported in all containers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCONDA_INSTALL_PATH=\"/usr/local/miniconda3\"\nCONDA_INIT_SCRIPT=\"$CONDA_INSTALL_PATH/etc/profile.d/conda.sh\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003ertp\u003c/code\u003e container has an additional environment variable that point to \u003ccode\u003ehera_pipelines\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHERA_PIPELINES_PATH=\"/usr/local/hera_pipelines\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-commands\" class=\"anchor\" href=\"#singularity-commands\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Commands\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-pull\" class=\"anchor\" href=\"#pull\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003epull\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eUse \u003ccode\u003esingularity pull\u003c/code\u003e to download the container from Singularity Hub\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity pull [name_to_save_the_image_(optional)] shub://HERA-Team/hera-singularity:\u0026lt;recipe\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor example,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity pull rtp.sif shub://HERA-Team/hera-singularity:rtp\nINFO:    Downloading shub image\n 1.98 GiB / 1.98 GiB [=======================================================] 100.00% 13.12 MiB/s 2m34s\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-shell\" class=\"anchor\" href=\"#shell\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003eshell\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eThe \u003ccode\u003esingularity shell\u003c/code\u003e command allows you to spawn a new shell within your container and interact with it as though it were a small virtual machine.\u003c/p\u003e\n\u003cp\u003eBy default, \u003ccode\u003eshell\u003c/code\u003e invokes \u003ccode\u003e/bin/sh --norc\u003c/code\u003e, which means that \u003ccode\u003e.bashrc\u003c/code\u003e will not be executed (more on this \u003ca href=\"https://github.com/hpcng/singularity/issues/643\"\u003ehere\u003c/a\u003e) and thus \u003ccode\u003econda\u003c/code\u003e will not be initialized. To have \u003ccode\u003econda\u003c/code\u003e working, you can do one of the following:\u003c/p\u003e\n\u003cp\u003ea) Run \u003ccode\u003eexec $SHELL\u003c/code\u003e inside the singularity shell. If \u003ccode\u003e$SHELL\u003c/code\u003e is \u003ccode\u003e\\bin\\bash\u003c/code\u003e (as in our Ubuntu build), \u003ccode\u003e.bashrc\u003c/code\u003e will be read.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity shell rtp.sif\nSingularity\u0026gt; exec $SHELL\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eb) Manually execute the conda initialization script inside singularity shell. A \u003ccode\u003eCONDA_INIT_SCRIPT\u003c/code\u003e environment variable pointing to the absolute path of the script (\u003ccode\u003e/usr/local/miniconda3/etc/profile.d/conda.sh\u003c/code\u003e), is made available for this purpose. Note that \u003ccode\u003e.\u003c/code\u003e must be used as \u003ccode\u003esource\u003c/code\u003e won\u0027t work under \u003ccode\u003esh\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity shell rtp.sif\nSingularity\u0026gt; . $CONDA_INIT_SCRIPT\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eb) Specify \u003ccode\u003e\\bin\\bash\u003c/code\u003e as a shell to use when executing the \u003ccode\u003eshell\u003c/code\u003e command, either by using the \u003ccode\u003eSINGULARITY_SHELL\u003c/code\u003e environment variable,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ SINGULARITY_SHELL=/bin/bash singularity shell hera-rtp.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor \u003ccode\u003e-s\u003c/code\u003e option,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity shell -s /bin/bash hera-rtp.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-exec\" class=\"anchor\" href=\"#exec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003eexec\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eThe \u003ccode\u003esingularity exec\u003c/code\u003e command allows you to execute a custom command within a container by specifying the image file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec rtp.sif echo \"Hello World!\"\nHello World!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e$ cat myscript.sh\nHello World!\n$ singularity exec rtp.sif bash myscript.sh\nHello World!\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-file-permission-and-bind-path\" class=\"anchor\" href=\"#file-permission-and-bind-path\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFile Permission and Bind Path\u003c/h3\u003e\n\u003cp\u003eSingularity containers run as the user and share host services. When Singularity \u2018switch\u2019 from the host operating system to the containerized operating system, the OS-level system files on the host becomes inaccessible. (the root user on the host system is also different from the root in the container!)\u003c/p\u003e\n\u003cp\u003eBy default, the user home directory on the host system will be mapped to the user home directory in the container, preserving all file permission. On Ilifu, the shared data paths on the host are also mapped.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-specific-usages-for-ilifu\" class=\"anchor\" href=\"#specific-usages-for-ilifu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpecific Usages for Ilifu\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-container-file-locations\" class=\"anchor\" href=\"#container-file-locations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer File Locations\u003c/h4\u003e\n\u003cp\u003eRecent builds are available at \u003ccode\u003e/ilifu/astro/projects/hera/containers\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-using-a-hera-container-as-a-jupyter-kernel\" class=\"anchor\" href=\"#using-a-hera-container-as-a-jupyter-kernel\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing a HERA container as a Jupyter kernel\u003c/h4\u003e\n\u003cp\u003eSee \u003ca href=\"https://docs.ilifu.ac.za/#/tech_docs/software_environments?id=using-a-custom-container-as-a-jupyter-kernel\" rel=\"nofollow\"\u003ethis page\u003c/a\u003e on Ilifu documentation. We may try to semi-automate this process for users with a shell script in the future.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 13,
    "topics": [],
    "updated_at": 1620417376.0
  },
  {
    "data_format": 2,
    "description": "A not working Github actions docker example",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jkwmoore/singularity-gha-docker",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-builder---github-actions\" class=\"anchor\" href=\"#singularity-builder---github-actions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Builder - Github Actions\u003c/h1\u003e\n\u003cp\u003eThis has been converted from the Gitlab Project here:  \u003ca href=\"https://gitlab.com/singularityhub/gitlab-ci\" rel=\"nofollow\"\u003ehttps://gitlab.com/singularityhub/gitlab-ci\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is a simple example of how you can achieve:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eversion control of your recipes\u003c/li\u003e\n\u003cli\u003eversioning to include image hash \u003cem\u003eand\u003c/em\u003e commit id\u003c/li\u003e\n\u003cli\u003ebuild of associated container and\u003c/li\u003e\n\u003cli\u003epush to a storage endpoint (or \u003cdel\u003eGitLab\u003c/del\u003e Github artifact)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efor a reproducible build workflow.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should this be managed via \u003cdel\u003eGitLab\u003c/del\u003e Github?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cdel\u003eGitLab\u003c/del\u003e Github, by way of easy integration with continuous integration, is an easy way\nto have a workflow set up where multiple people can collaborate on a container recipe,\nthe recipe can be tested (with whatever testing you need), discussed in pull requests,\nand then finally pushed to be a \u003cdel\u003eGitLab\u003c/del\u003e Github artifact, to your storage of choice\nor to Singularity Registry.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should I use this instead of a service?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou could use a remote builder, but if you do the build in a continuous integration\nservice you get complete control over it. This means everything from the version of\nSingularity to use, to the tests that you run for your container. You have a lot more\nfreedom in the rate of building, and organization of your repository, because it\u0027s you\nthat writes the configuration. Although the default would work for most, you can\nedit the build, setup, and circle configuration file in the\n\u003ca href=\".gitlabci\"\u003e.gitlabci\u003c/a\u003e folder to fit your needs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eAdd your Singularity recipes to this repository, and edit the build commands in\nthe \u003ca href=\".gitlabci/build.sh\"\u003ebuild.sh\u003c/a\u003e file. This is where you can specify endpoints\n(Singularity Registry, Dropbox, Google Storage, AWS) along with container names\n(the uri) and tag. You can build as many recipes as you like, just add another line!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e                               \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e recipe relative to repository base\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --tag tacos --cli google-storage Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --cli google-drive Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --cli globus Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --cli registry Singularity\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor each client that you use, required environment variables (e.g., credentials to push,\nor interact with the API) must be defined in the (encrypted) Travis environment. To\nknow what variables to define, along with usage for the various clients, see\nthe \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eclient specific pages\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-detailed-started\" class=\"anchor\" href=\"#detailed-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetailed Started\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-0-fork-this-repository\" class=\"anchor\" href=\"#0-fork-this-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e0. Fork this repository\u003c/h3\u003e\n\u003cp\u003eYou can clone and tweak, but it\u0027s easiest likely to get started with our example\nfiles and edit them as you need.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-get-to-know-gitlab-github-actions\" class=\"anchor\" href=\"#1-get-to-know-gitlab-github-actions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Get to Know \u003cdel\u003eGitLab\u003c/del\u003e Github Actions\u003c/h3\u003e\n\u003cp\u003eGithub has a built-in Continuous Integration service called Github Actions you should be able to use for free. You can get started here \u003ca href=\"https://github.com/features/actions\"\u003ehttps://github.com/features/actions\u003c/a\u003e and take a look at the \u003ccode\u003e.github\\workflows\\build.yml\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eArtifacts will be found in the \u003ccode\u003e/home/runner/work/REPO-NAME/REPO-NAME/\u003c/code\u003e directory.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-add-your-recipes\" class=\"anchor\" href=\"#2-add-your-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Add your Recipe(s)\u003c/h3\u003e\n\u003cp\u003eFor the example here, we have a single recipe named \"Singularity\" that is provided\nas an input argument to the \u003ca href=\".gitlabci/build.sh\"\u003ebuild script\u003c/a\u003e. You could add another\nrecipe, and then of course call the build to happen more than once.\nThe build script will name the image based on the recipe, and you of course\ncan change this. Just write the path to it (relative to the repository base) in\nyour \u003ca href=\".github/workflows/build.yml\"\u003e.github/workflows/build.yml\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-configure-singularity\" class=\"anchor\" href=\"#3-configure-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Configure Singularity\u003c/h3\u003e\n\u003cp\u003eWe previously used \u003ca href=\".gitlabci/setup.sh\"\u003esetup\u003c/a\u003e to setup the build, but now use a base image instead.\nThe previous instructions are provided for posterity.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cdel\u003eInstall Singularity, we use the release 2.6 branch as it was the last to not be written in GoLang. You could of course change the lines in \u003ca href=\".gitlabci/setup.sh\"\u003esetup.sh\u003c/a\u003e to use a specific tagged release, an older version, or development version.\u003c/del\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe current Github Action is using the \u003ccode\u003equay.io/singularity/singularity:v3.7.3\u003c/code\u003e image - at present the slim images are missing /bin/bash - the version can be changed at will as per your production environment versioning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall the sregistry client, if needed. The \u003ca href=\"https://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003esregistry client\u003c/a\u003e allows you to issue a command like \"sregistry push ...\" to upload a finished image to one of your cloud / storage endpoints. By default, the push won\u0027t happen, and you will just build an image using the CI.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-configure-the-build\" class=\"anchor\" href=\"#4-configure-the-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. Configure the Build\u003c/h3\u003e\n\u003cp\u003eThe basic steps for the \u003ca href=\".gitlabci/build.sh\"\u003ebuild\u003c/a\u003e are the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRunning build.sh with no inputs will default to a recipe called \"Singularity\" in the base of the repository. You can provide an argument to point to a different recipe path, always relative to the base of your repository.\u003c/li\u003e\n\u003cli\u003eIf you want to define a particular unique resource identifier for a finished container (to be uploaded to your storage endpoint) you can do that with \u003ccode\u003e--uri collection/container\u003c/code\u003e. If you don\u0027t define one, a robot name will be generated.\u003c/li\u003e\n\u003cli\u003eYou can add \u003ccode\u003e--uri\u003c/code\u003e to specify a custom name, and this can include the tag, OR you can specify \u003ccode\u003e--tag\u003c/code\u003e to go along with a name without one. It depends on which is easier for you.\u003c/li\u003e\n\u003cli\u003eIf you add \u003ccode\u003e--cli\u003c/code\u003e then this is telling the build script that you have defined the \u003ca href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"nofollow\"\u003eneeded environment variables\u003c/a\u003e for your \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eclient of choice\u003c/a\u003e and you want successful builds to be pushed to your storage endpoint. See \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003ehere\u003c/a\u003e for a list of current client endpoints, or roll your own!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee the \u003ca href=\".gitlab-ci.yml\"\u003e.gitlab-ci.yml\u003c/a\u003e for examples of this build.sh command (commented out). If there is some cloud service that you\u0027d like that is not provided, please \u003ca href=\"https://www.github.com/singularityhub/sregistry-cli/issues\"\u003eopen an issue\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-5-pull--download-your-container\" class=\"anchor\" href=\"#5-pull--download-your-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5. Pull / Download your Container\u003c/h3\u003e\n\u003cp\u003eYou can access the artifacts from the Actions tab, under the build action for any given run. You may wish to edit the \u003ccode\u003ebuild.yml\u003c/code\u003e to export individual .sif images rather than a zip file of artifacts.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620413122.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "davidhin/singularity-example",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-builder---github-actions\" class=\"anchor\" href=\"#singularity-builder---github-actions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Builder - Github Actions\u003c/h1\u003e\n\u003cp\u003eThis has been converted from the Gitlab Project here:  \u003ca href=\"https://gitlab.com/singularityhub/gitlab-ci\" rel=\"nofollow\"\u003ehttps://gitlab.com/singularityhub/gitlab-ci\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is a simple example of how you can achieve:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eversion control of your recipes\u003c/li\u003e\n\u003cli\u003eversioning to include image hash \u003cem\u003eand\u003c/em\u003e commit id\u003c/li\u003e\n\u003cli\u003ebuild of associated container and\u003c/li\u003e\n\u003cli\u003epush to a storage endpoint (or \u003cdel\u003eGitLab\u003c/del\u003e Github artifact)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efor a reproducible build workflow.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should this be managed via \u003cdel\u003eGitLab\u003c/del\u003e Github?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cdel\u003eGitLab\u003c/del\u003e Github, by way of easy integration with continuous integration, is an easy way\nto have a workflow set up where multiple people can collaborate on a container recipe,\nthe recipe can be tested (with whatever testing you need), discussed in pull requests,\nand then finally pushed to be a \u003cdel\u003eGitLab\u003c/del\u003e Github artifact, to your storage of choice\nor to Singularity Registry.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should I use this instead of a service?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou could use a remote builder, but if you do the build in a continuous integration\nservice you get complete control over it. This means everything from the version of\nSingularity to use, to the tests that you run for your container. You have a lot more\nfreedom in the rate of building, and organization of your repository, because it\u0027s you\nthat writes the configuration. Although the default would work for most, you can\nedit the build, setup, and circle configuration file in the\n\u003ca href=\".gitlabci\"\u003e.gitlabci\u003c/a\u003e folder to fit your needs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eAdd your Singularity recipes to this repository, and edit the build commands in\nthe \u003ca href=\".gitlabci/build.sh\"\u003ebuild.sh\u003c/a\u003e file. This is where you can specify endpoints\n(Singularity Registry, Dropbox, Google Storage, AWS) along with container names\n(the uri) and tag. You can build as many recipes as you like, just add another line!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e                               \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e recipe relative to repository base\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --tag tacos --cli google-storage Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --cli google-drive Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --cli globus Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .gitlabci/build.sh --uri collection/container --cli registry Singularity\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor each client that you use, required environment variables (e.g., credentials to push,\nor interact with the API) must be defined in the (encrypted) Travis environment. To\nknow what variables to define, along with usage for the various clients, see\nthe \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eclient specific pages\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-detailed-started\" class=\"anchor\" href=\"#detailed-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetailed Started\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-0-fork-this-repository\" class=\"anchor\" href=\"#0-fork-this-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e0. Fork this repository\u003c/h3\u003e\n\u003cp\u003eYou can clone and tweak, but it\u0027s easiest likely to get started with our example\nfiles and edit them as you need.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-get-to-know-gitlab-github-actions\" class=\"anchor\" href=\"#1-get-to-know-gitlab-github-actions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Get to Know \u003cdel\u003eGitLab\u003c/del\u003e Github Actions\u003c/h3\u003e\n\u003cp\u003eGithub has a built-in Continuous Integration service called Github Actions you should be able to use for free. You can get started here \u003ca href=\"https://github.com/features/actions\"\u003ehttps://github.com/features/actions\u003c/a\u003e and take a look at the \u003ccode\u003e.github\\workflows\\build.yml\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eArtifacts will be found in the \u003ccode\u003e/home/runner/work/REPO-NAME/REPO-NAME/\u003c/code\u003e directory.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-add-your-recipes\" class=\"anchor\" href=\"#2-add-your-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Add your Recipe(s)\u003c/h3\u003e\n\u003cp\u003eFor the example here, we have a single recipe named \"Singularity\" that is provided\nas an input argument to the \u003ca href=\".gitlabci/build.sh\"\u003ebuild script\u003c/a\u003e. You could add another\nrecipe, and then of course call the build to happen more than once.\nThe build script will name the image based on the recipe, and you of course\ncan change this. Just write the path to it (relative to the repository base) in\nyour \u003ca href=\".github/workflows/build.yml\"\u003e.github/workflows/build.yml\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-configure-singularity\" class=\"anchor\" href=\"#3-configure-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Configure Singularity\u003c/h3\u003e\n\u003cp\u003eWe previously used \u003ca href=\".gitlabci/setup.sh\"\u003esetup\u003c/a\u003e to setup the build, but now use a base image instead.\nThe previous instructions are provided for posterity.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cdel\u003eInstall Singularity, we use the release 2.6 branch as it was the last to not be written in GoLang. You could of course change the lines in \u003ca href=\".gitlabci/setup.sh\"\u003esetup.sh\u003c/a\u003e to use a specific tagged release, an older version, or development version.\u003c/del\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe current Github Action is using the \u003ccode\u003equay.io/singularity/singularity:v3.7.3\u003c/code\u003e image - at present the slim images are missing /bin/bash - the version can be changed at will as per your production environment versioning.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall the sregistry client, if needed. The \u003ca href=\"https://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003esregistry client\u003c/a\u003e allows you to issue a command like \"sregistry push ...\" to upload a finished image to one of your cloud / storage endpoints. By default, the push won\u0027t happen, and you will just build an image using the CI.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-configure-the-build\" class=\"anchor\" href=\"#4-configure-the-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. Configure the Build\u003c/h3\u003e\n\u003cp\u003eThe basic steps for the \u003ca href=\".gitlabci/build.sh\"\u003ebuild\u003c/a\u003e are the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRunning build.sh with no inputs will default to a recipe called \"Singularity\" in the base of the repository. You can provide an argument to point to a different recipe path, always relative to the base of your repository.\u003c/li\u003e\n\u003cli\u003eIf you want to define a particular unique resource identifier for a finished container (to be uploaded to your storage endpoint) you can do that with \u003ccode\u003e--uri collection/container\u003c/code\u003e. If you don\u0027t define one, a robot name will be generated.\u003c/li\u003e\n\u003cli\u003eYou can add \u003ccode\u003e--uri\u003c/code\u003e to specify a custom name, and this can include the tag, OR you can specify \u003ccode\u003e--tag\u003c/code\u003e to go along with a name without one. It depends on which is easier for you.\u003c/li\u003e\n\u003cli\u003eIf you add \u003ccode\u003e--cli\u003c/code\u003e then this is telling the build script that you have defined the \u003ca href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"nofollow\"\u003eneeded environment variables\u003c/a\u003e for your \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eclient of choice\u003c/a\u003e and you want successful builds to be pushed to your storage endpoint. See \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003ehere\u003c/a\u003e for a list of current client endpoints, or roll your own!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee the \u003ca href=\".gitlab-ci.yml\"\u003e.gitlab-ci.yml\u003c/a\u003e for examples of this build.sh command (commented out). If there is some cloud service that you\u0027d like that is not provided, please \u003ca href=\"https://www.github.com/singularityhub/sregistry-cli/issues\"\u003eopen an issue\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-5-pull--download-your-container\" class=\"anchor\" href=\"#5-pull--download-your-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5. Pull / Download your Container\u003c/h3\u003e\n\u003cp\u003eYou can access the artifacts from the Actions tab, under the build action for any given run. You may wish to edit the \u003ccode\u003ebuild.yml\u003c/code\u003e to export individual .sif images rather than a zip file of artifacts.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620391274.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.0.2.2",
      "Singularity.0.1-alpha",
      "Singularity.0.2.1",
      "Singularity.0.2.0"
    ],
    "full_name": "dcgc-bfx/dcgc-single-cell",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/dcgc-bfx/dcgc-single-cell/workflows/Build/badge.svg?branch=main\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/dcgc-bfx/dcgc-single-cell/workflows/Build/badge.svg?branch=main\" alt=\"Build\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5095\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-dcgc-single-cell\" class=\"anchor\" href=\"#dcgc-single-cell\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edcgc-single-cell\u003c/h1\u003e\n\u003cp\u003eDCGC singularity recipe for single cell analysis\u003c/p\u003e\n\u003cp\u003ePull it from the singularity hub: \u003ca href=\"https://singularity-hub.org/collections/5095\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/5095\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStart jupyter lab:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --writable-tmpfs --app jupyter shub://dcgc-bfx/dcgc-single-cell\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eStart rstudio server listening on port 8787:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --writable-tmpfs --app rserver shub://dcgc-bfx/dcgc-single-cell 8787\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620324842.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "stela2502/singularityImages",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularityimages\" class=\"anchor\" href=\"#singularityimages\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularityImages\u003c/h1\u003e\n\u003cp\u003eThis git repo is a skelleton of my work I have done on singularity images.\nThese images are used on aurora-ls2 to run analyses on the blades instead of the frontend.\u003c/p\u003e\n\u003cp\u003eAll of that documention is in our Bioinformatics Slack Howto channel.\u003c/p\u003e\n\u003cp\u003eThe software I install I mainly install from within the singularity image. Hence the usage of shell.sh.\u003c/p\u003e\n\u003cp\u003eInstaling Python modules is tricky as pip3 always installs in a private path and not the global unless told otherwise.\nHence only I with my username on the computer I build the images could use the modules.\u003c/p\u003e\n\u003cp\u003eA solution could be to use some conda approach, but as this here will be a singularity image we could also try to install globaly:\u003c/p\u003e\n\u003cp\u003ePython solution:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install --prefix=/usr/local \u0026lt;package name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620320126.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "compute_environment/singularity/Singularity"
    ],
    "full_name": "graneklab/rna_enrichment",
    "latest_release": null,
    "readme": "\u003cp\u003eThis repository contains the code for \u003ca href=\"https://doi.org/10.1101/2021.03.01.433483\" rel=\"nofollow\"\u003eComparative analysis of RNA enrichment methods for preparation of Cryptococcus neoformans RNA sequencing libraries\u003c/a\u003e. Instructions for reproducing the analysis described in the manuscript are below.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-full-analysis-except-lncrna-discovery\" class=\"anchor\" href=\"#full-analysis-except-lncrna-discovery\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFull Analysis, except lncRNA discovery\u003c/h1\u003e\n\u003cp\u003eTo run the full analysis in the manuscript (except lncRNA discovery):\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003cli\u003eRun the script \u003ccode\u003erun_full_analysis.sh\u003c/code\u003e, which is found in top level of this repository\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe \u003ccode\u003erun_full_analysis.sh\u003c/code\u003e script will download the singularity image from Sylabs Cloud, download the raw sequence data from SRA, and run all the components of the analysis.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"https://sylabs.io/guides/3.7/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e. It is known to work with Singularity version 3.6.4, but probably works fine with version 3.0 and above\u003c/li\u003e\n\u003cli\u003eBash\u003c/li\u003e\n\u003cli\u003eCreate the directories specified by \u003ccode\u003eDATA_BASE_DIR\u003c/code\u003e, \u003ccode\u003eWORKSPACE_BASE_DIR\u003c/code\u003e and \u003ccode\u003eSPACE_DIR\u003c/code\u003e (see below)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-recommended-configuration\" class=\"anchor\" href=\"#recommended-configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRecommended Configuration\u003c/h2\u003e\n\u003cp\u003eIt is recommended that you make the configuration adjustments\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCheck the values in the \u003ccode\u003ecommon_config.R\u003c/code\u003e for these variables and adjust appropriately for the machine you will be running on:\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003etotal_threads\u003c/code\u003e should be less than the total number of cores you have available\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emax_jobs\u003c/code\u003e can be 1 or higher, ideally should be a factor of total_threads\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emax_memory_mb\u003c/code\u003e should be less than the total memory you have available\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSet values for the following environment variables in your .bashrc, when running \u003ccode\u003erun_full_analysis.sh\u003c/code\u003e, or by editing \u003ccode\u003esetup.sh\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eDATA_BASE_DIR\u003c/code\u003e: where raw data will be downloaded\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eWORKSPACE_BASE_DIR\u003c/code\u003e: where intermediate results and final results will be output\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSPACE_DIR\u003c/code\u003e : directory used by lncRNA\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSINGULARITY_CACHEDIR\u003c/code\u003e described in the \u003ca href=\"https://sylabs.io/guides/2.6/user-guide/build_environment.html#cache\" rel=\"nofollow\"\u003eSingularity User Manual\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSINGULARITY_PULLFOLDER\u003c/code\u003e described in the \u003ca href=\"https://sylabs.io/guides/2.6/user-guide/build_environment.html#cache\" rel=\"nofollow\"\u003eSingularity User Manual\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-a-cluster\" class=\"anchor\" href=\"#running-on-a-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on a cluster\u003c/h2\u003e\n\u003cp\u003eThis script can be run on a SLURM cluster with the following command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esrun ./run_full_analysis.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHowever a slightly more complex command might be desireable, which specifies the partition, account, number of CPUs and amount of memory to allocate, such as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esrun -A chsi -p chsi --cpus-per-task=60 --mem=300G ./run_full_analysis.sh\n\u003c/code\u003e\u003c/pre\u003e\n \n \n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621656631.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "lkirk/toil-demo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-demo-repo-for-messing-around-with-toil-workflows\" class=\"anchor\" href=\"#demo-repo-for-messing-around-with-toil-workflows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo repo for messing around with toil workflows\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-the-workflow\" class=\"anchor\" href=\"#running-the-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the workflow\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run library://lkirk/default/toil-demo:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e./scripts/build-sandbox library://lkirk/default/toil-demo:latest toil-sandbox\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-for-dev-purposes\" class=\"anchor\" href=\"#install-for-dev-purposes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall (for dev purposes)\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e./scripts/run-sandbox toil-sandbox pip install -e .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-play\" class=\"anchor\" href=\"#play\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlay\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e./scripts/run-sandbox toil-sandbox ipython\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-test\" class=\"anchor\" href=\"#test\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e./scripts/run-sandbox toil-sandbox pytest demo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-useful-links\" class=\"anchor\" href=\"#useful-links\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful links\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://toil.readthedocs.io/en/latest/developingWorkflows/developing.html#workflows-with-multiple-jobs\" rel=\"nofollow\"\u003emultiple jobs, child, follow on\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620259690.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "cfe-lab/proviral",
    "latest_release": "v2.3.4",
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-readme\" class=\"anchor\" href=\"#readme\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReadme\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eminimap2 (\u003ca href=\"https://github.com/lh3/minimap2\"\u003ehttps://github.com/lh3/minimap2\u003c/a\u003e) (must be available via commandline)\u003c/li\u003e\n\u003cli\u003eblast tools (\u003ca href=\"ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/\" rel=\"nofollow\"\u003eftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eR and RSCRIPT (\u003ca href=\"https://www.r-project.org/\" rel=\"nofollow\"\u003ehttps://www.r-project.org/\u003c/a\u003e)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-builds\" class=\"anchor\" href=\"#singularity-builds\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity builds\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBuild all singularity images inside of the \u003ccode\u003esimages\u003c/code\u003e folder\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-filtering\" class=\"anchor\" href=\"#filtering\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFiltering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAt the core of the proviral pipeline, data is read from \u003ccode\u003econtigs.csv\u003c/code\u003e and \u003ccode\u003econseqs.csv\u003c/code\u003e files produced by MiCall\u003c/li\u003e\n\u003cli\u003eFirst the pipeline reads through all of the contigs, then the contigs\u003c/li\u003e\n\u003cli\u003eWhen it does this (see the \u003ccode\u003efind_primers()\u003c/code\u003e function) it applies the following logic in this order for filtering/tagging:\n\u003col\u003e\n\u003cli\u003eIf a sample is not proviral, skip it. Do not attempt to find primers or anything, just log a message saying \u003ccode\u003esample X was skipped because it was non-proviral\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eIf a sample has 0 in the remap column of the \u003ccode\u003ecascade.csv\u003c/code\u003e file, tag that sequence with an error: \u003ccode\u003eNo contig/conseq constructed\u003c/code\u003e, do not analyze it or try to find primers, and write it to the \u003ccode\u003e*primer_analysis.csv\u003c/code\u003e file (which records all failures)\u003c/li\u003e\n\u003cli\u003eIf the \u003ccode\u003econsensus-percent-cutoff\u003c/code\u003e is NOT \u003ccode\u003eMAX\u003c/code\u003e, tag it with an error: \u003ccode\u003econtig not MAX\u003c/code\u003e and skip the sequence (do not try to find primers)\u003c/li\u003e\n\u003cli\u003eIf the reference of the sample is \u003ccode\u003eHIV1-CON-XX-Consensus-seed\u003c/code\u003e tag that sequence with an error: \u003ccode\u003eis V3 sequence\u003c/code\u003e, skip the sequence (do not try to find primers), and write it to the \u003ccode\u003e*primer_analysis.csv\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eIf there is an \u003ccode\u003eX\u003c/code\u003e in the middle of the sequence, tag that sequence with an error: \u003ccode\u003elow internal read coverage\u003c/code\u003e, skip the sequence (do not try to find primers), and write it to the \u003ccode\u003e*primer_analysis.csv\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eIf there are ANY non-TCGA characters in the sequence, tag that sequence with an error: \u003ccode\u003econtig sequence contained non-TCGA/gap\u003c/code\u003e, skip the sequence (do not try to find primers), and write it to the \u003ccode\u003e*primer_analysis.csv\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eFor each end (5\u0027 (fwd), 3\u0027 (rev)) of the sequence:\n\u003col\u003e\n\u003cli\u003eIf there are \u003ccode\u003eX\u003c/code\u003e characters found, try to remove them (if they are clustered) and if not possible to remove tag the fwd/rev end with a fwd/rev primer error: \u003ccode\u003elow read coverage in primer region\u003c/code\u003e, skip the fwd/rev end (do not try to find primers)\u003c/li\u003e\n\u003cli\u003eIf fwd/rev end has zero nucleotides found for primer, tag the fwd/rev end with a fwd/rev primer error: \u003ccode\u003eprimer was not found\u003c/code\u003e, skip to the next end if any\u003c/li\u003e\n\u003cli\u003eIf the fwd/rev primer is deemed not valid, tag the fwd/rev end with a fwd/rev primer error: \u003ccode\u003eprimer failed secondary validation\u003c/code\u003e, skip to the next end if any\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eWrite the sequence to the \u003ccode\u003e*primer_analysis.csv\u003c/code\u003e file regardless of tagged errors in any error column\u003c/li\u003e\n\u003cli\u003eLoad the \u003ccode\u003e*primer_analysis.csv\u003c/code\u003e files for both contigs and conseqs and for both of them apply the following filters in order:\n\u003col\u003e\n\u003cli\u003eRemove all rows where either the \u003ccode\u003eerror\u003c/code\u003e, \u003ccode\u003efwd_error\u003c/code\u003e, or \u003ccode\u003erev_error\u003c/code\u003e is tagged\u003c/li\u003e\n\u003cli\u003eRemove the primers from the sequences (for hivseqinr)\u003c/li\u003e\n\u003cli\u003eRemove rows where sample name appears twice (duplicates)\u003c/li\u003e\n\u003cli\u003eRemove rows where the reference contains \u003ccode\u003eunknown\u003c/code\u003e or \u003ccode\u003ereverse\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eFinally merge the filtered contigs and conseqs and write the final \u003ccode\u003e*filtered.csv\u003c/code\u003e file with conseqs taking precedence over contigs\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1621296936.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "oogasawa/singularity-latex",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-latex\" class=\"anchor\" href=\"#singularity-latex\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-latex\u003c/h1\u003e\n\u003cp\u003eA singularity container of LaTeX typesetting system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eBuild the Singularity image as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/oogasawa/singularity-latex\ncd singularity-latex\nsudo singularity build singularity-latex.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCompile a LaTeX file. (a DVI file will be generated.)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec singularity-latex.sif platex doc.tex\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGenerate a PDF file from a DVI file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec singularity-latex.sif dvipdfmx doc.dvi\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620240227.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "eliasarnold/image-generator-for-BScThesis",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-parametric-face-image-generatorextension-by-arneli00\" class=\"anchor\" href=\"#parametric-face-image-generatorextension-by-arneli00\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eparametric-face-image-generator(extension by arneli00)\u003c/h1\u003e\n\u003cp\u003eThis software is based on the parametric-face-image-generator of the gravis group. The \u003ca href=\"https://github.com/unibas-gravis/parametric-face-image-generator\"\u003eoriginal Software\u003c/a\u003e enables you to generate fully parametric face images from the Basel Face Model 2017 as proposed in:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e[1] Adam Kortylewski, Andreas Schneider, Thomas Gerig, Bernhard Egger, Andreas Morel-Forster and Thomas Vetter\n\u003ca href=\"https://arxiv.org/abs/1802.05891\" rel=\"nofollow\"\u003e\"Training Deep Face Recognition Systems with Synthetic Data\"\u003c/a\u003e,\nIN: arXiv preprint (2018)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e[2] Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster and Thomas Vetter\n\u003ca href=\"https://arxiv.org/abs/1712.01619\" rel=\"nofollow\"\u003e\"Empirically Analyzing the Effect of Dataset Biases on Deep Face Recognition Systems\"\u003c/a\u003e,\nIN: arXiv preprint (2017)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can control the variation of parameters such as pose, shape, color, camera and illumination based on your demand and application.\nThis dataset can be used for training and comparing machine learning techniques such as CNNs on a common ground as proposed in [1] by generating fully controlled training and test data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-whats-new\" class=\"anchor\" href=\"#whats-new\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWHAT\u0027S NEW\u003c/h2\u003e\n\u003cp\u003eThis version produces two images of the same face at a time. For the first image, it uses the Basel Face Model \"face12\" and for the other image the \"bfm\" version. This version only works for three occlusionModes. These are:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-random\" class=\"anchor\" href=\"#random\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erandom\u003c/h3\u003e\n\u003cp\u003eThis mode renders an arbitrary image of a hand or a microphone over the output image\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-random-1\" class=\"anchor\" href=\"#random-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erandom-1\u003c/h3\u003e\n\u003cp\u003eThis mode renders an image of a hand over the image\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-random-2\" class=\"anchor\" href=\"#random-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erandom-2\u003c/h3\u003e\n\u003cp\u003eThis mode renders an image of a microphone over the image\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-box\" class=\"anchor\" href=\"#box\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebox\u003c/h3\u003e\n\u003cp\u003eThis mode occludes the image with a rectangle at a random position, with random shape and random color\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-none\" class=\"anchor\" href=\"#none\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enone\u003c/h3\u003e\n\u003cp\u003eIf you don\u0027t want an occlusion to be added to the image, specify this option\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOUTPUT\u003c/h2\u003e\n\u003cp\u003eThis modified version provides two new folders in the output (in this version, the output is split in two: output/bfm and output/face12):\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-img_masks\" class=\"anchor\" href=\"#img_masks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eimg_masks\u003c/h3\u003e\n\u003cp\u003eThese images provide the ground truth segmentation of the image. It distinguishes between face-region, occlusion-region, and background.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-img_occlusion\" class=\"anchor\" href=\"#img_occlusion\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eimg_occlusion\u003c/h3\u003e\n\u003cp\u003eThe original images overlaid with an occlusion of one of the above modes.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBernhard Egger\u003c/li\u003e\n\u003cli\u003eAdam Kortylewski\u003c/li\u003e\n\u003cli\u003eAndreas Morel-Forster\u003c/li\u003e\n\u003cli\u003eAndreas Schneider\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-maintainers\" class=\"anchor\" href=\"#maintainers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMaintainers\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUniversity of Basel, Graphics and Vision research: \u003ca href=\"https://github.com/unibas-gravis\"\u003e@unibas-gravis\u003c/a\u003e, \u003ca href=\"http://gravis.cs.unibas.ch\" rel=\"nofollow\"\u003ehomepage\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\"\u003eApache License, Version 2.0\u003c/a\u003e, details see LICENSE\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCopyright 2017, University of Basel, Graphics and Vision Research\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620184864.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "challenge-engine/test-starting-kit",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"#test-starting-kit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etest-starting-kit\u003c/h1\u003e\n\u003cp\u003e\u003cg-emoji class=\"g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\u003e\ud83e\udd13\u003c/g-emoji\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1620159134.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "Clinical-Genomics-Linkoping/Lund_nextflow_wgs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"#test-starting-kit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etest-starting-kit\u003c/h1\u003e\n\u003cp\u003e\u003cg-emoji class=\"g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\u003e\ud83e\udd13\u003c/g-emoji\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620156274.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.spades_3.14.1",
      "Singularity.flye_2.8",
      "Singularity.masurca_4.0.3",
      "Singularity.trinity_2.11.0",
      "Singularity.canu_1.9",
      "Singularity.meraculous_2.2.6",
      "Singularity.miniasm_0.3r179"
    ],
    "full_name": "TomHarrop/assemblers",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"#test-starting-kit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etest-starting-kit\u003c/h1\u003e\n\u003cp\u003e\u003cg-emoji class=\"g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\u003e\ud83e\udd13\u003c/g-emoji\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1620120967.0
  },
  {
    "data_format": 2,
    "description": "singularity environment manager for NGS pipelines ",
    "filenames": [
      "damona/recipes/fitter/Singularity.fitter_1.3.0",
      "damona/recipes/phantompeakqualtools/Singularity.phantompeakqualtools_1.2.2",
      "damona/recipes/salmon/Singularity.salmon_1.3.0",
      "damona/recipes/minimap2/Singularity.minimap2_2.17.0",
      "damona/recipes/R/Singularity.R_4.0.2",
      "damona/recipes/R/Singularity.R_3.6.3",
      "damona/recipes/sequana_perl_tools/Singularity.sequana_perl_tools_0.1.0",
      "damona/recipes/rnaseqc/Singularity.rnaseqc_2.35.0",
      "damona/recipes/ucsc/Singularity.ucsc_0.1.0",
      "damona/recipes/damona/Singularity.damona_0.4.2",
      "damona/recipes/damona/Singularity.damona_0.3.0",
      "damona/recipes/rtools/Singularity.Rtools_1.1.0",
      "damona/recipes/rtools/Singularity.Rtools_1.0.0",
      "damona/recipes/kraken/Singularity.kraken_1.1",
      "damona/recipes/kraken/Singularity.kraken_2.0.9",
      "damona/recipes/gffread/Singularity.gffread_0.12.1",
      "damona/recipes/canu/Singularity.canu_1.8.0",
      "damona/recipes/canu/Singularity.canu_1.6.0",
      "damona/recipes/fastqc/Singularity.fastqc_0.11.8",
      "damona/recipes/fastqc/Singularity.fastqc_0.11.9",
      "damona/recipes/rnadiff/Singularity.rnadiff_1.7.1",
      "damona/recipes/prokka/Singularity.prokka_1.14.5",
      "damona/recipes/art/Singularity.art_3.11.14",
      "damona/recipes/trf/Singularity.trf_4.09",
      "damona/recipes/trf/Singularity.trf_4.10.0",
      "damona/recipes/falco/Singularity.falco_0.2.1",
      "damona/recipes/conda/Singularity.conda_4.7.12",
      "damona/recipes/conda/Singularity.conda_4.9.2",
      "damona/recipes/bcl2fastq/Singularity.bcl2fastq_2.20.0",
      "damona/recipes/graphviz/Singularity.graphviz_2.43.0",
      "damona/recipes/sequana_tools/Singularity.sequana_tools_0.9.0",
      "damona/recipes/sequana_tools/Singularity.sequana_tools_0.11.0",
      "damona/recipes/sequana_tools/Singularity.sequana_tools_0.10.0"
    ],
    "full_name": "cokelaer/damona",
    "latest_release": "v0.4.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-test-starting-kit\" class=\"anchor\" href=\"#test-starting-kit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etest-starting-kit\u003c/h1\u003e\n\u003cp\u003e\u003cg-emoji class=\"g-emoji\" alias=\"nerd_face\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f913.png\"\u003e\ud83e\udd13\u003c/g-emoji\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1620073798.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.intelpython3_beer"
    ],
    "full_name": "pvelesko/singularity_files",
    "latest_release": null,
    "readme": "\u003cp\u003eSingulairy container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1566267553.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.centos_tf",
      "Singularity.pytorch",
      "Singularity.ExplainAI",
      "Singularity.centos_torch3",
      "Singularity.centos_tf2",
      "Singularity.ubuntu_tf",
      "Singularity.torch",
      "Singularity.torch_mmf",
      "Singularity.jax",
      "Singularity.physio",
      "Singularity.mac_local",
      "Singularity.Spektral",
      "Singularity.centos_torch2",
      "Singularity.ubuntu_pre",
      "Singularity.centos_torch",
      "Singularity.ubuntu_torch",
      "Singularity.ExplainAI2"
    ],
    "full_name": "cyang31/containers",
    "latest_release": null,
    "readme": "\u003cp\u003eSingulairy container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620037627.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "environments/illumina/Singularity",
      "environments/nanopore/Singularity"
    ],
    "full_name": "quadram-institute-bioscience/ncov2019-artic-nf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ncov2019-artic-nf\" class=\"anchor\" href=\"#ncov2019-artic-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003encov2019-artic-nf\u003c/h1\u003e\n\u003cp\u003eA Nextflow pipeline for running the ARTIC network\u0027s fieldbioinformatics tools (\u003ca href=\"https://github.com/artic-network/fieldbioinformatics\"\u003ehttps://github.com/artic-network/fieldbioinformatics\u003c/a\u003e), with a focus on ncov2019\u003c/p\u003e\n\u003cp\u003eWARNING - THIS REPO IS UNDER ACTIVE DEVELOPMENT AND ITS BEHAVIOUR MAY CHANGE AT \u003cstrong\u003eANY\u003c/strong\u003e TIME.\u003c/p\u003e\n\u003cp\u003ePLEASE ENSURE THAT YOU READ BOTH THE README AND THE CONFIG FILE AND UNDERSTAND THE EFFECT OF THE OPTIONS ON YOUR DATA!\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-qib-settings\" class=\"anchor\" href=\"#qib-settings\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQIB settings\u003c/h4\u003e\n\u003cp\u003eFor using the pipeline with QIB setting, i.e. primers V3 and cluster configuration, please use \u003cstrong\u003eqib\u003c/strong\u003e branch\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit checkout qib\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eCommand line to execute the pipeline for a run\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-illumina-data\" class=\"anchor\" href=\"#illumina-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIllumina data\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run /path/to/ncov2019-artic-nf/folder \\\n--directory \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/path/to/illumina/fastq/folder\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n--illumina \\\n--readTrimming \\\n--get_all \u003cspan class=\"pl-cce\"\u003e\\ \u003c/span\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003ePut all samples (failed and passed qc) into qc_climb_upload folder\u003c/span\u003e\n--prefix \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etest-v3\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n--outdir \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/path/to/output/folder\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n--bed \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/beegfs/software/ncov2019-artic-nf/primers/scheme/primer_schemes/nCoV-2019/V3/nCoV-2019.bed\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n--ref \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/beegfs/software/ncov2019-artic-nf/primers/scheme/primer_schemes/nCoV-2019/V3/nCoV-2019.reference.fasta\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n--fourLanes \u003cspan class=\"pl-cce\"\u003e\\ \u003c/span\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003eUse when 4-lane fastqs were not merged\u003c/span\u003e\n-profile qib,singularity \\\n-with-trace trace-4lanes.txt \\\n-with-report report-4lanes.html\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-nanopore-data-artic-pipeline\" class=\"anchor\" href=\"#nanopore-data-artic-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNanopore data (ARTIC pipeline)\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run /path/to/ncov2019-artic-nf/folder\n--outdir \"/path/to/output\" \\\n--prefix \"NORW-20200418\" \\\n--useGuppyPlex \\\n--min_length 100 \\\n--max_length 550 \\\n--normalise 500 \\\n-profile qib,singularity \\\n--medaka \\\n--basecalled_fastq \"/path/to/basedcall_folder/\"\n-with-trace trace.txt \\\n-with-report report.html\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-original-readme\" class=\"anchor\" href=\"#original-readme\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOriginal Readme\u003c/h1\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h4\u003e\n\u003chr\u003e\n\u003cp\u003eThis Nextflow pipeline automates the ARTIC network \u003ca href=\"https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html\" title=\"nCoV-2019 novel coronavirus bioinformatics protocol\" rel=\"nofollow\"\u003enCoV-2019 novel coronavirus bioinformatics protocol\u003c/a\u003e. It is being developed to aid the harmonisation of the analysis of sequencing data generated by the \u003ca href=\"https://github.com/COG-UK\"\u003eCOG-UK\u003c/a\u003e project. It will turn SARS-COV2 sequencing data (Illumina or Nanopore) into consensus sequences and provide other helpful outputs to assist the project\u0027s sequencing centres with submitting data.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick-start\u003c/h4\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-illumina\" class=\"anchor\" href=\"#illumina\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIllumina\u003c/h5\u003e\n\u003cp\u003e\u003ccode\u003enextflow run connor-lab/ncov2019-artic-nf [-profile conda,singularity,docker,slurm,lsf] --illumina --prefix \"output_file_prefix\" --directory /path/to/reads\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eYou can also use cram file input by passing the --cram flag.\u003c/p\u003e\n\u003cp\u003eFor production use at large scale, where you will run the workflow many times, you can avoid cloning the scheme repository, creating an ivar bed file and indexing the reference every time by supplying both --ivarBed /path/to/ivar-compatible.bed and --alignerRefPrefix /path/to/bwa-indexed/ref.fa.\u003c/p\u003e\n\u003cp\u003eAlternatively you can avoid just the cloning of the scheme repository to remain on a fixed revision of it over time by passing --schemeRepoURL /path/to/own/clone/of/github.com/artic-network/artic-ncov2019. This removes any internet access from the workflow except for the optional upload steps.\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-nanopore\" class=\"anchor\" href=\"#nanopore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNanopore\u003c/h5\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-nanopolish\" class=\"anchor\" href=\"#nanopolish\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNanopolish\u003c/h6\u003e\n\u003cp\u003e\u003ccode\u003enextflow run connor-lab/ncov2019-artic-nf [-profile conda,singularity,docker,slurm,lsf] --nanopolish --prefix \"output_file_prefix\" --basecalled_fastq /path/to/directory --fast5_pass /path/to/directory --sequencing_summary /path/to/sequencing_summary.txt\u003c/code\u003e\u003c/p\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-medaka\" class=\"anchor\" href=\"#medaka\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMedaka\u003c/h6\u003e\n\u003cp\u003e\u003ccode\u003enextflow run connor-lab/ncov2019-artic-nf [-profile conda,singularity,docker,slurm,lsf] --medaka --prefix \"output_file_prefix\" --basecalled_fastq /path/to/directory --fast5_pass /path/to/directory --sequencing_summary /path/to/sequencing_summary.txt\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h4\u003e\n\u003cp\u003eAn up-to-date version of Nextflow is required because the pipeline is written in DSL2. Following the instructions at \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003ehttps://www.nextflow.io/\u003c/a\u003e to download and install Nextflow should get you a recent-enough version.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers\u003c/h4\u003e\n\u003cp\u003eThis repo contains both Singularity and Dockerfiles. You can build the Singularity containers locally by running \u003ccode\u003escripts/build_singularity_containers.sh\u003c/code\u003e and use them with \u003ccode\u003e-profile singularity\u003c/code\u003e The containers will be available from Docker/Singularityhub shortly.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-conda\" class=\"anchor\" href=\"#conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConda\u003c/h4\u003e\n\u003cp\u003eThe repo contains a environment.yml files which automatically build the correct conda env if \u003ccode\u003e-profile conda\u003c/code\u003e is specifed in the command. Although you\u0027ll need \u003ccode\u003econda\u003c/code\u003e installed, this is probably the easiest way to run this pipeline.\u003c/p\u003e\n\u003cp\u003e--cache /some/dir can be specified to have a fixed, shared location to store the conda build for use by multiple runs of the workflow.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-executors\" class=\"anchor\" href=\"#executors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecutors\u003c/h4\u003e\n\u003cp\u003eBy default, the pipeline just runs on the local machine. You can specify \u003ccode\u003e-profile slurm\u003c/code\u003e to use a SLURM cluster, or \u003ccode\u003e-profile lsf\u003c/code\u003e to use an LSF cluster. In either case you may need to also use one of the COG-UK institutional config profiles (phw or sanger), or provide queue names to use in your own config file.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-profiles\" class=\"anchor\" href=\"#profiles\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eProfiles\u003c/h4\u003e\n\u003cp\u003eYou can use multiple profiles at once, separating them with a comma. This is described in the Nextflow \u003ca href=\"https://www.nextflow.io/docs/latest/config.html#config-profiles\" rel=\"nofollow\"\u003edocumentation\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-config\" class=\"anchor\" href=\"#config\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfig\u003c/h4\u003e\n\u003cp\u003eCommon configuration options are set in \u003ccode\u003econf/base.config\u003c/code\u003e. Workflow specific configuration options are set in \u003ccode\u003econf/nanopore.config\u003c/code\u003e and \u003ccode\u003econf/illumina.config\u003c/code\u003e They are described and set to sensible defaults (as suggested in the \u003ca href=\"https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html\" title=\"nCoV-2019 novel coronavirus bioinformatics protocol\" rel=\"nofollow\"\u003enCoV-2019 novel coronavirus bioinformatics protocol\u003c/a\u003e)\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptions\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e--outdir\u003c/code\u003e sets the output directory.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--bwa\u003c/code\u003e to swap to bwa for mapping (nanopore only).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-workflows\" class=\"anchor\" href=\"#workflows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflows\u003c/h5\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-nanopore-1\" class=\"anchor\" href=\"#nanopore-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNanopore\u003c/h6\u003e\n\u003cp\u003eUse \u003ccode\u003e--nanopolish\u003c/code\u003e or \u003ccode\u003e--medaka\u003c/code\u003e to run these workflows. \u003ccode\u003e--basecalled_fastq\u003c/code\u003e should point to a directory created by \u003ccode\u003eguppy_basecaller\u003c/code\u003e (if you ran with no barcodes), or \u003ccode\u003eguppy_barcoder\u003c/code\u003e (if you ran with barcodes). It is imperative that the following \u003ccode\u003eguppy_barcoder\u003c/code\u003e command be used for demultiplexing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eguppy_barcoder --require_barcodes_both_ends -i run_name -s output_directory --arrangements_files \"barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-illumina-1\" class=\"anchor\" href=\"#illumina-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIllumina\u003c/h6\u003e\n\u003cp\u003eThe Illumina workflow leans heavily on the excellent \u003ca href=\"https://github.com/andersen-lab/ivar\"\u003eivar\u003c/a\u003e for primer trimming and consensus making. This workflow will be updated to follow ivar, as its also in very active development! Use \u003ccode\u003e--illumina\u003c/code\u003e to run the Illumina workflow. Use \u003ccode\u003e--directory\u003c/code\u003e to point to an Illumina output directory usually coded something like: \u003ccode\u003e\u0026lt;date\u0026gt;_\u0026lt;machine_id\u0026gt;_\u0026lt;run_no\u0026gt;_\u0026lt;some_zeros\u0026gt;_\u0026lt;flowcell\u0026gt;\u003c/code\u003e. The workflow will recursively grab all fastq files under this directory, so be sure that what you want is in there, and what you don\u0027t, isn\u0027t!\u003c/p\u003e\n\u003cp\u003eImportant config options are:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"left\"\u003eOption\u003c/th\u003e\n\u003cth align=\"right\"\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eallowNoprimer\u003c/td\u003e\n\u003ctd align=\"right\"\u003eAllow reads that don\u0027t have primer sequence? Ligation prep = false, nextera = true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eilluminaKeepLen\u003c/td\u003e\n\u003ctd align=\"right\"\u003eLength of illumina reads to keep after primer trimming\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eilluminaQualThreshold\u003c/td\u003e\n\u003ctd align=\"right\"\u003eSliding window quality threshold for keeping reads after primer trimming (illumina)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003empileupDepth\u003c/td\u003e\n\u003ctd align=\"right\"\u003eMpileup depth for ivar\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eivarFreqThreshold\u003c/td\u003e\n\u003ctd align=\"right\"\u003eivar frequency threshold for variant\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eivarMinDepth\u003c/td\u003e\n\u003ctd align=\"right\"\u003eMinimum coverage depth to call variant\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-qc\" class=\"anchor\" href=\"#qc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQC\u003c/h4\u003e\n\u003cp\u003eA script to do some basic COG-UK QC is provided in \u003ccode\u003ebin/qc.py\u003c/code\u003e. This currently tests if \u0026gt;50% of reference bases are covered by \u0026gt;10 reads (Illumina) or \u0026gt;20 reads (Nanopore), OR if there is a stretch of more than 10 Kb of sequence without N - setting qc_pass in \u003ccode\u003e\u0026lt;outdir\u0026gt;/\u0026lt;prefix\u0026gt;.qc.csv\u003c/code\u003e to TRUE. \u003ccode\u003ebin/qc.py\u003c/code\u003e can be extended to incorporate any QC test, as long as the script outputs a csv file a \"qc_pass\" last column, with samples TRUE or FALSE.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h4\u003e\n\u003cp\u003eA subdirectory for each process in the workflow is created in \u003ccode\u003e--outdir\u003c/code\u003e. A \u003ccode\u003eqc_pass_climb_upload\u003c/code\u003e subdirectory containing files important for \u003ca href=\"https://github.com/COG-UK\"\u003eCOG-UK\u003c/a\u003e is created.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1619829439.0
  },
  {
    "data_format": 2,
    "description": "Evaluation and annotation of assembled transcripts",
    "filenames": [
      "Singularity"
    ],
    "full_name": "BrendelGroup/mRNAmarkup",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mrnamarkup---a-workflow-for-annotating-transcript-sequences\" class=\"anchor\" href=\"#mrnamarkup---a-workflow-for-annotating-transcript-sequences\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emRNAmarkup - a workflow for annotating transcript sequences\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003emRNAmarkup is implemented as a bash script which should work on any Linux or\nUNIX system.  Please see \u003ca href=\"./INSTALL\"\u003eINSTALL\u003c/a\u003e for requirements of other software\nand specific set-up instructions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eIf you want to avoid any installation hussles, the following will do nicely,\nusing our mRNAmarkup \u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container that\nencapsulates all scripts, programs, and system packages.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/BrendelGroup/mRNAmarkup\ncd mRNAmarkup/\nsingularity pull http://BrendelGroup.org/SingularityHub/mRNAmarkup.sif\n./xsetup\ncd db\nsingularity exec -e -B ${PWD}/.. ../mRNAmarkup.sif bash 0README\ncd ..\ncd data\nsingularity exec -e -B ${PWD}/.. ../mRNAmarkup.sif ./xtest\nxdiff\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reference\" class=\"anchor\" href=\"#reference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference\u003c/h2\u003e\n\u003cp\u003emanuscript to be submitted\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003ePlease direct all comments and suggestions to\n\u003ca href=\"mailto:vbrendel@indiana.edu\"\u003eVolker Brendel\u003c/a\u003e\nat \u003ca href=\"http://brendelgroup.org/\" rel=\"nofollow\"\u003eIndiana University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620409883.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "wzacs1/minION_Metagenomics",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-minion_metagenomics\" class=\"anchor\" href=\"#minion_metagenomics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eminION_Metagenomics\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1578393029.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ctpelok77/fdss",
    "latest_release": null,
    "readme": "\u003cp\u003eFast Downward is a domain-independent planning system.\u003c/p\u003e\n\u003cp\u003eFor documentation and contact information see \u003ca href=\"http://www.fast-downward.org/\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe following directories are not part of Fast Downward as covered by this\nlicense:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify it under\nthe terms of the GNU General Public License as published by the Free Software\nFoundation, either version 3 of the License, or (at your option) any later\nversion.\n\nFast Downward is distributed in the hope that it will be useful, but WITHOUT ANY\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\nPARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with\nthis program. If not, see \u0026lt;http://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619778901.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity/Singularity-GCC-VisTools-MINT"
    ],
    "full_name": "MetOffice/LFRic-Containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containerisation-of-lfric\" class=\"anchor\" href=\"#containerisation-of-lfric\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainerisation of LFRic\u003c/h1\u003e\n\u003cp\u003eThis repository hosts LFRic container recipes and links to similar external\nrepositories.\u003c/p\u003e\n\u003cp\u003eMore detailed information about \u003ccode\u003eLFRic\u003c/code\u003e and further references can be found in\n\u003ca href=\"https://github.com/MetOffice/LFRic-Containers/blob/master/LFRicIntro.md\"\u003e\u003cem\u003eIntroduction to LFRic\u003c/em\u003e\u003c/a\u003e\nsection.\u003c/p\u003e\n\u003cp\u003eInstructions on building and runing \u003ccode\u003eLFRic\u003c/code\u003e in two container platforms,\n\u003ca href=\"https://docs.docker.com/install/\" rel=\"nofollow\"\u003eDocker CE\u003c/a\u003e and\n\u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e, are stored in two subdirectories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/MetOffice/LFRic-Containers/blob/master/Docker/README.md\"\u003eDocker\u003c/a\u003e;\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/MetOffice/LFRic-Containers/blob/master/Singularity/README.md\"\u003eSingularity\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1620312607.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity/Singularity.v1.0"
    ],
    "full_name": "IARCbioinfo/svaba-nf",
    "latest_release": "v1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-svaba-nf\" class=\"anchor\" href=\"#svaba-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esvaba-nf\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-structural-variant-calling\" class=\"anchor\" href=\"#structural-variant-calling\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStructural variant calling\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/IARCbioinfo/svaba-nf\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f5e9b3e470d2c780fc3cc37f9f44d338736561a859f1c8677933503b83c46108/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f73766162612d6e662e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/IARCbioinfo/svaba-nf.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/iarcbioinfo/svaba-nf/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c5d5383ee3d6248c7d31b5fcaa21fc7d689b53ecb330dbfe628cd1fae38c853/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d72656164792d626c75652e737667\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/badge/docker-ready-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4720\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/IARCbioinfo/svaba-nf/blob/master/svaba.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/IARCbioinfo/svaba-nf/raw/master/svaba.png\" alt=\"Image SvABA\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003ePerform structural variant calling with SvABA.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThis pipeline is based on \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003enextflow\u003c/a\u003e. As we have several nextflow pipelines, we have centralized the common information in the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository. Please read it carefully as it contains essential information for the installation, basic usage and configuration of nextflow and our pipelines.\u003c/li\u003e\n\u003cli\u003eSvABA: see official installation \u003ca href=\"https://github.com/walaj/svaba\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--input_folder\u003c/td\u003e\n\u003ctd\u003eFolder containing normal (.normal.bam) and tumor (.tumor.bam) BAM files\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--correspondance\u003c/td\u003e\n\u003ctd\u003eA correspondance file, with columns \u003ccode\u003eID\u003c/code\u003e, \u003ccode\u003etumor\u003c/code\u003e, and \u003ccode\u003enormal\u003c/code\u003e specifying the name of the sample and the tumor/normal file names in the input folder\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-mandatory\" class=\"anchor\" href=\"#mandatory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMandatory\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eExample value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--ref\u003c/td\u003e\n\u003ctd\u003eref.fa\u003c/td\u003e\n\u003ctd\u003ePath to reference fasta file. It should be indexed\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptional\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDefault value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--output_folder\u003c/td\u003e\n\u003ctd\u003e\".\"\u003c/td\u003e\n\u003ctd\u003ePath to output folder\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--dbsnp_file\u003c/td\u003e\n\u003ctd\u003edbsnp_indel.vcf\u003c/td\u003e\n\u003ctd\u003eDbSNP file, e.g. available \u003ca href=\"https://data.broadinstitute.org/snowman/dbsnp_indel.vcf\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--cpu\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003eNumber of cpu to use\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--mem\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003eSize of memory used in GB\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--targets\u003c/td\u003e\n\u003ctd\u003eNULL\u003c/td\u003e\n\u003ctd\u003ebed file with target positions\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--options\u003c/td\u003e\n\u003ctd\u003eNULL\u003c/td\u003e\n\u003ctd\u003eList of options to pass to svaba\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003cstrong\u003eName\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eDescription\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--help\u003c/td\u003e\n\u003ctd\u003eDisplay help\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download-test-data-set\" class=\"anchor\" href=\"#download-test-data-set\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload test data set\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003egit clone https://github.com/iarcbioinfo/data_test\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003enextflow run IARCbioinfo/svaba-nf -r v1.0 -profile singularity--input_folder  path/to/input/ --svaba path/to/svaba/ --ref_file path/to/ref/ --dbsnp_file path/to/dbsnp_indel.vcf --output_folder /path/to/output\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTo run the pipeline without singularity just remove \"-profile singularity\". Alternatively, one can run the pipeline using a docker container (-profile docker).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tumor-only-mode\" class=\"anchor\" href=\"#tumor-only-mode\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTumor-only mode\u003c/h3\u003e\n\u003cp\u003eTo trigger the Tumor-only mode in some samples, put \"None\" (with capital N) in the normal column of the corresponding sample.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003cstrong\u003eName\u003c/strong\u003e\u003c/th\u003e\n\u003cth\u003e\u003cstrong\u003eDescription\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003etxts (.bps.txt.gz)\u003c/td\u003e\n\u003ctd\u003eRaw, unfiltered variants\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBAMs (.contigs.bam)\u003c/td\u003e\n\u003ctd\u003eUnsorted assembly contigs as aligned to the reference with BWA-MEM\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLogs (.log)\u003c/td\u003e\n\u003ctd\u003eRun-time information\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etxts (.discordants.txt.gz)\u003c/td\u003e\n\u003ctd\u003eDiscordant reads identified with 2+ reads\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eVCFs (.vcf )\u003c/td\u003e\n\u003ctd\u003eVCF of rearrangements and indels\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directed-acyclic-graph\" class=\"anchor\" href=\"#directed-acyclic-graph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirected Acyclic Graph\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://htmlpreview.github.io/?https://github.com/IARCbioinfo/svaba-nf/blob/master/dag.html\" rel=\"nofollow\"\u003e\u003cimg src=\"dag.png\" alt=\"DAG\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions\" class=\"anchor\" href=\"#contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eEmail\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eNicolas Alcala*\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:AlcalaN@iarc.fr\"\u003eAlcalaN@iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper to contact for support\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTiffany Delhomme\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:DelhommeT@students.iarc.fr\"\u003eDelhommeT@students.iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 11,
    "topics": [],
    "updated_at": 1598988848.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity_jlabsolidbase_devel",
      "Singularity.1.0.2"
    ],
    "full_name": "jlabsolid/container",
    "latest_release": null,
    "readme": "\u003cp\u003eContainer\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1521257911.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.dRep",
      "Singularity.eukcc_vanilla",
      "Singularity.cmseq_conda",
      "Singularity.metabat2",
      "Singularity.nQuire",
      "Singularity.megahit",
      "Singularity.R",
      "Singularity.kraken2",
      "Singularity.sourmash",
      "Singularity.seqtk",
      "Singularity.metawap_docker",
      "Singularity.puntseq",
      "Singularity.mummer",
      "Singularity.euk_decide",
      "Singularity.CAT_update",
      "Singularity.BUSCO4",
      "Singularity.cmseq",
      "Singularity.art",
      "Singularity.snakemake",
      "Singularity.EukRep",
      "Singularity.qiime2",
      "Singularity.biopython",
      "Singularity.repeatmasker",
      "Singularity.deeptools",
      "Singularity.antismash_standalone",
      "Singularity.mashmap",
      "Singularity.nanofilt",
      "Singularity.METAMVGL",
      "Singularity.VAMB_10.1",
      "Singularity.bamm",
      "Singularity.BUSCO5",
      "Singularity.bioconvert",
      "Singularity.mafft",
      "Singularity.metawrap",
      "Singularity.raxml-ng",
      "Singularity.CAT",
      "Singularity.krona",
      "Singularity.minimap2",
      "Singularity.pysam",
      "Singularity.ete3",
      "Singularity.dbcan",
      "Singularity.spades_3.15",
      "Singularity.sepp",
      "Singularity.spades",
      "Singularity.ncbi-downloader",
      "Singularity.metaeuk",
      "Singularity.BUSCO414",
      "Singularity.dRep3",
      "Singularity.pasta",
      "Singularity.VAMB",
      "Singularity.mash",
      "Singularity.ploidyNGS",
      "Singularity.bbmap",
      "Singularity.VAMP",
      "Singularity.bioinfo",
      "Singularity.tree",
      "Singularity.spades_3.13",
      "Singularity.mmseq2",
      "Singularity.bwa",
      "Singularity.iqtree",
      "Singularity.famsa",
      "Singularity.fastani",
      "Singularity.trimal",
      "Singularity.comparem"
    ],
    "full_name": "hexmek/container",
    "latest_release": null,
    "readme": "\u003cp\u003eContainer\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1619722202.0
  },
  {
    "data_format": 2,
    "description": "ElikoPy is Python library aiming at easing the processing of diffusion imaging for microstructural analysis.",
    "filenames": [
      "Singularity_elikopy"
    ],
    "full_name": "Hyedryn/elikopy",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-elikopy\" class=\"anchor\" href=\"#elikopy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eElikoPy\u003c/h1\u003e\n\u003cp\u003eElikoPy is Python library aiming at easing the processing of diffusion imaging for microstructural analysis.\nThis Python library is based on\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDIPY, a python library for the analysis of MR diffusion imaging.\u003c/li\u003e\n\u003cli\u003eMicrostructure fingerprinting, a python library doing estimation of white matter microstructural properties from a dictionary of Monte Carlo diffusion MRI fingerprints.\u003c/li\u003e\n\u003cli\u003eFSL, a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.\u003c/li\u003e\n\u003cli\u003eDIAMOND, a c software that is characterizing brain tissue by assessment of the distribution of anisotropic microstructural environments in diffusion\u2010compartment imaging.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eElikoPy requires \u003ca href=\"https://www.python.org/\" rel=\"nofollow\"\u003ePython\u003c/a\u003e v3.7+ to run.\u003c/p\u003e\n\u003cp\u003eAfter cloning the repo, you can either firstly install all the python dependencies including optionnal dependency used to speed up the code:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ pip install -r requirements.txt --user\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOr you can install directly the library with only the mandatory dependencies (if you performed the previous step, you still need to perform this step):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ python3 setup.py install --user\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eMicrostructure Fingerprinting is currently not avaible in the standard python repo, you can clone and install this library manually.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git clone git@github.com:rensonnetg/microstructure_fingerprinting.git\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e microstructure_fingerprinting\n$ python setup.py install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFSL also needs to be installed and availabe in our path if you want to perform mouvement correction or tbss.\u003c/p\u003e\n\u003cp\u003eUnfortunatly, the DIAMOND code is not publically available. If you do not have it in your possesion, you will not be able to use this algorithm. If you have it, simply add the executable to your path.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cp\u003eTodo\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h3\u003e\n\u003cp\u003eWant to contribute? Great!\u003c/p\u003e\n\u003cp\u003eDo not hesitate to open issue or pull request!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTodos\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRelease a complete and accurate documentation for the library\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eFree Software, Hell Yeah!\u003c/strong\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "microstructure-fingerprinting",
      "fsl",
      "tbss",
      "python-library",
      "diffusion-imaging",
      "preprocessing",
      "dmri",
      "diamond",
      "noddi",
      "dti"
    ],
    "updated_at": 1621866034.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "bjhall/sarscov2-nf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-elikopy\" class=\"anchor\" href=\"#elikopy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eElikoPy\u003c/h1\u003e\n\u003cp\u003eElikoPy is Python library aiming at easing the processing of diffusion imaging for microstructural analysis.\nThis Python library is based on\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDIPY, a python library for the analysis of MR diffusion imaging.\u003c/li\u003e\n\u003cli\u003eMicrostructure fingerprinting, a python library doing estimation of white matter microstructural properties from a dictionary of Monte Carlo diffusion MRI fingerprints.\u003c/li\u003e\n\u003cli\u003eFSL, a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.\u003c/li\u003e\n\u003cli\u003eDIAMOND, a c software that is characterizing brain tissue by assessment of the distribution of anisotropic microstructural environments in diffusion\u2010compartment imaging.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eElikoPy requires \u003ca href=\"https://www.python.org/\" rel=\"nofollow\"\u003ePython\u003c/a\u003e v3.7+ to run.\u003c/p\u003e\n\u003cp\u003eAfter cloning the repo, you can either firstly install all the python dependencies including optionnal dependency used to speed up the code:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ pip install -r requirements.txt --user\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOr you can install directly the library with only the mandatory dependencies (if you performed the previous step, you still need to perform this step):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ python3 setup.py install --user\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eMicrostructure Fingerprinting is currently not avaible in the standard python repo, you can clone and install this library manually.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git clone git@github.com:rensonnetg/microstructure_fingerprinting.git\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e microstructure_fingerprinting\n$ python setup.py install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFSL also needs to be installed and availabe in our path if you want to perform mouvement correction or tbss.\u003c/p\u003e\n\u003cp\u003eUnfortunatly, the DIAMOND code is not publically available. If you do not have it in your possesion, you will not be able to use this algorithm. If you have it, simply add the executable to your path.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cp\u003eTodo\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h3\u003e\n\u003cp\u003eWant to contribute? Great!\u003c/p\u003e\n\u003cp\u003eDo not hesitate to open issue or pull request!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTodos\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRelease a complete and accurate documentation for the library\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eFree Software, Hell Yeah!\u003c/strong\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619701441.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for Angora (https://github.com/AngoraFuzzer/Angora)",
    "filenames": [
      "Singularity.1604",
      "Singularity.1804"
    ],
    "full_name": "shub-fuzz/angora",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity image for Angora (\u003ca href=\"https://github.com/AngoraFuzzer/Angora\"\u003ehttps://github.com/AngoraFuzzer/Angora\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3645\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name angora.sif https://github.com/shub-fuzz/angora/releases/download/0.0.1/shub-fuzz-angora.1604.sif\n\nsingularity shell angora.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003einteractive session:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell angora.sif \n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003estart fuzzing\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec angora.sif /start_fuzzing [[ -n \u0026lt;# instances\u0026gt; ]  -t ] \u0026lt;target_path\u0026gt; \n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619232164.0
  },
  {
    "data_format": 2,
    "description": "Ubuntu (rolling) with build tools",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/build-tools",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-build-tools\" class=\"anchor\" href=\"#build-tools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebuild-tools\u003c/h1\u003e\n\u003cp\u003eUbuntu (rolling) with build tools\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/build-tools/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/build-tools/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619233018.0
  },
  {
    "data_format": 2,
    "description": "Singularity Postgres container",
    "filenames": [
      "Singularity.v12"
    ],
    "full_name": "shub-fuzz/postgres",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/postgres/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/postgres/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-postgres-container\" class=\"anchor\" href=\"#singularity-postgres-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Postgres Container\u003c/h1\u003e\n\u003cp\u003eSingularity containter for postgres docker image (\u003ca href=\"https://hub.docker.com/_/postgres/\" rel=\"nofollow\"\u003elink\u003c/a\u003e).\nThis singularity recipe modifies from the image to work in service mode.\nIt also includes an optional \u003ccode\u003e/postgresrc\u003c/code\u003e to pass system environment variables.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTo start the service, \u003ccode\u003esingularity instance start -B host_folder:/var/lib/postgresql/data postgres.sif pg-database\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eTo pass variables for postgres database, use bind a file to \u003ccode\u003e/postgresrc\u003c/code\u003e.\u003cbr\u003e\nFor example, \u003ccode\u003e-B yourrc:/postgresrc\u003c/code\u003e. Some common variables include \u003ccode\u003ePGPORT\u003c/code\u003e and \u003ccode\u003eHOSTNAME\u003c/code\u003e.\n(Due to some reason, the default postgres starts in \u003ccode\u003e0.0.0.0\u003c/code\u003e. For security reason, this recipe\nuses \u003ccode\u003eHOSTNAME\u003c/code\u003e which defaults to \u003ccode\u003elocalhost\u003c/code\u003e.)\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003eSINGULARITY_BINDPATH=\u0027host_folder:container_folder,host_file:container_file\u0027\u003c/code\u003e for easier binding.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-manually\" class=\"anchor\" href=\"#building-manually\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Manually\u003c/h2\u003e\n\u003cp\u003eTo build the image, run \u003ccode\u003esudo singularity build \u0026lt;name.sif\u0026gt; Singularity\u003c/code\u003e.\nSee \u003ca href=\"https://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\nfor more info.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledge\" class=\"anchor\" href=\"#acknowledge\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledge\u003c/h2\u003e\n\u003cp\u003eThe recipes build from many open source projects, including\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.postgresql.org/\" rel=\"nofollow\"\u003epostgres\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619233219.0
  },
  {
    "data_format": 2,
    "description": "NodeJS container used for PROVA! webserver",
    "filenames": [
      "Singularity.v14.16-buster-slim"
    ],
    "full_name": "fenz/node4prova",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-node4prova\" class=\"anchor\" href=\"#node4prova\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enode4prova\u003c/h1\u003e\n\u003cp\u003eNodeJS container used for PROVA! webserver\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619154525.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Horovod on Centos 8",
    "filenames": [
      "Singularity"
    ],
    "full_name": "willgpaik/horovod_roar",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-horovod_roar\" class=\"anchor\" href=\"#horovod_roar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehorovod_roar\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for Horovod on Centos 8 for Roar\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619664798.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for asciinema",
    "filenames": [
      "2.0.2/Singularity"
    ],
    "full_name": "icaoberg/singularity-asciinema",
    "latest_release": "v2.0.2-r3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-asciinema\" class=\"anchor\" href=\"#singularity-asciinema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-asciinema\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.travis-ci.com/icaoberg/singularity-asciinema\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7af8220113fb0c457eda3ba87d518d1a80a3bfc8eab5f60a14fac181d20e05c3/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d61736369696e656d612e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-asciinema.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for asciinema.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-or-similar\" class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges (or similar)\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003easciinema\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/asciinema/2.0.2\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modules/asciinema\u003c/code\u003e as \u003ccode\u003e2.0.2.lua\u003c/code\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "asciinema",
      "singularity-recipe"
    ],
    "updated_at": 1619661412.0
  },
  {
    "data_format": 2,
    "description": "tidyverse and Biostrings Singularity container ",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sylvainschmitt/singularity-tidyverse-Biostrings",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tidyverse-and-biostrings-singularity-container\" class=\"anchor\" href=\"#tidyverse-and-biostrings-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etidyverse and Biostrings Singularity container\u003c/h1\u003e\n\u003cp\u003eSylvain Schmitt\nApril 28, 2021\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eR packages tidyverse and Biostrings\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003etidyverse\u003c/code\u003e is an opinionated collection of R packages designed for\ndata science. All packages share an underlying design philosophy,\ngrammar, and data structures.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003etidyverse\u003c/code\u003e Version: X.X.X\u003c/p\u003e\n\u003cp\u003e[\u003ca href=\"https://www.tidyverse.org/\" rel=\"nofollow\"\u003ehttps://www.tidyverse.org/\u003c/a\u003e]\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eBiostrings\u003c/code\u003e is a memory efficient string containers, string matching\nalgorithms, and other utilities, for fast manipulation of large\nbiological sequences or sets of sequences.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eBiostrings\u003c/code\u003e Version: X.X.X\u003c/p\u003e\n\u003cp\u003e[\u003ca href=\"https://bioconductor.org/packages/release/bioc/html/Biostrings.html\" rel=\"nofollow\"\u003ehttps://bioconductor.org/packages/release/bioc/html/Biostrings.html\u003c/a\u003e]\u003c/p\u003e\n\u003cp\u003eSingularity container based on the recipe:\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-tidyverse-Biostrings/blob/main/Singularity\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eImage singularity (V\u0026gt;=3.3) is automatically test and built and pushed\non the registry using\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\u003etest.yml\u003c/a\u003e\n\u0026amp;\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\u003ebuilder.yml\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ebuild\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build Biostrings.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003epull\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull https://github.com/sylvainschmitt/singularity-tidyverse-Biostrings/releases/download/0.0.1/sylvainschmitt-singularity-tidyverse-Biostrings.latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003esnakemake\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003esingularity\u003c/span\u003e: \n        \u003cspan class=\"pl-s\"\u003e\"https://github.com/sylvainschmitt/singularity-tidyverse-Biostrings/releases/download/0.0.1/sylvainschmitt-singularity-tidyverse-Biostrings.latest.sif\"\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619660633.0
  },
  {
    "data_format": 2,
    "description": "Docker image",
    "filenames": [
      "Singularity.latest"
    ],
    "full_name": "AdamWilsonLab/docker_geospatial_plus",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-geospatial-plus\" class=\"anchor\" href=\"#geospatial-plus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeospatial Plus\u003c/h1\u003e\n\u003cp\u003eBuilding on the versioned geospatial Rocker image.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-github-actions\" class=\"anchor\" href=\"#github-actions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGithub Actions\u003c/h1\u003e\n\u003cp\u003eThis repository uses GitHub Actions to test the docker image prior to making it available as a GitHub package.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620074754.0
  },
  {
    "data_format": 2,
    "description": "template for Singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sylvainschmitt/singularity-template",
    "latest_release": "0.0.4",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-template-singularity-container\" class=\"anchor\" href=\"#template-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTemplate Singularity container\u003c/h1\u003e\n\u003cp\u003eSylvain Schmitt\nApril 28, 2021\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBionformatics package Template\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTemplate is a set of utilities that Blah.\u003c/p\u003e\n\u003cp\u003eTemplate Version: X.X.X\u003c/p\u003e\n\u003cp\u003e[URL]\u003c/p\u003e\n\u003cp\u003eSingularity container based on the recipe: Singularity\u003c/p\u003e\n\u003cp\u003ePackage installation using Miniconda3 V4.7.12\u003c/p\u003e\n\u003cp\u003eImage singularity (V\u0026gt;=3.3) is automatically test and built and pushed\non the registry using\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\u003etest.yml\u003c/a\u003e\n\u0026amp;\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\u003ebuilder.yml\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ebuild\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build Singularity img.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003epull\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull https://github.com/sylvainschmitt/singularity-template/releases/download/0.0.4/sylvainschmitt-singularity-template.latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003esnakemake\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003esingularity\u003c/span\u003e: \n        \u003cspan class=\"pl-s\"\u003e\"https://github.com/sylvainschmitt/singularity-template/releases/download/0.0.4/sylvainschmitt-singularity-template.latest.sif\"\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619647716.0
  },
  {
    "data_format": 2,
    "description": "singularity recipes for naturalistic imaging / neuroscout project",
    "filenames": [
      "Singularity.pliersgoogle",
      "Singularity.room2reverb",
      "Singularity.cifti",
      "Singularity.googlespeech",
      "Singularity.nat_img",
      "Singularity.debian",
      "Singularity_gpu",
      "Singularity.nat_img_gpu",
      "Singularity.centosslim",
      "Singularity.centospliers",
      "Singularity.centosdyneusr",
      "Singularity.feature_extraction",
      "Singularity.centos",
      "Singularity.r2r"
    ],
    "full_name": "jsmentch/analyze-fmri",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-analyze-fmri\" class=\"anchor\" href=\"#analyze-fmri\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eanalyze-fmri\u003c/h1\u003e\n\u003cp\u003esingularity recipes for naturalistic imaging / neuroscout project\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619637484.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "mahwisharif/singularity-cinnamon",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-cinnamon\" class=\"anchor\" href=\"#singularity-cinnamon\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-cinnamon\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1619586956.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity-Verification",
      "Singularity"
    ],
    "full_name": "panda-planner-dev/pandaPIdriver",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pandapidriver\" class=\"anchor\" href=\"#pandapidriver\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epandaPIdriver\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619561242.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for ML software using Anaconda 3.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/ml-anaconda-3",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ml-anaconda-3\" class=\"anchor\" href=\"#ml-anaconda-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eml-anaconda-3\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1522996115.0
  },
  {
    "data_format": 2,
    "description": "CUDA 7.0 and CUDNN for Ubuntu 14.04",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cuda-7.0-cudnn4",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-70-cudnn4\" class=\"anchor\" href=\"#cuda-70-cudnn4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-7.0-cudnn4\u003c/h1\u003e\n\u003cp\u003eCUDA 7.0 and CUDNN for Ubuntu 14.04\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1527065438.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for the Faces project.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/faces-project",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-faces-project\" class=\"anchor\" href=\"#faces-project\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efaces-project\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for the Faces project\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1522033344.0
  },
  {
    "data_format": 2,
    "description": "Singularty recipe for Ubuntu 14.04 Singularity image with Cuda 7.5 and Cudnn5",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cuda-7.5-cudnn5",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-75-cudnn5\" class=\"anchor\" href=\"#cuda-75-cudnn5\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-7.5-cudnn5\u003c/h1\u003e\n\u003cp\u003eSingularty recipe for Ubuntu 14.04 Singularity image with Cuda 7.5 and Cudnn5\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1544755927.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cs171",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cs171\" class=\"anchor\" href=\"#cs171\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecs171\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1543654662.0
  },
  {
    "data_format": 2,
    "description": "AsterixDB container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/asterixdb",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-asterixdb\" class=\"anchor\" href=\"#asterixdb\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003easterixdb\u003c/h1\u003e\n\u003cp\u003eAsterixDB container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1535433043.0
  },
  {
    "data_format": 2,
    "description": "DEPRECATED - Singularity recipe for docker://bvlc/caffe:gpu",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/caffe-gpu",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-caffe-gpu\" class=\"anchor\" href=\"#caffe-gpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecaffe-gpu\u003c/h1\u003e\n\u003cp\u003eTHIS REPOSITORY IS DEPRECATED. There is no current replacement within ucr-singularity.\u003c/p\u003e\n\u003cp\u003eSingularity recipe for docker://bvlc/caffe:gpu\u003c/p\u003e\n\u003cp\u003eAdds screen, tmux, vim, xterm\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1557314492.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for CS 100",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cs100",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cs100\" class=\"anchor\" href=\"#cs100\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecs100\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for CS 100\u003c/p\u003e\n\u003cp\u003eAvailable on Singularity Hub at \u003ca href=\"https://www.singularity-hub.org/collections/1789\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/1789\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1539244991.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cs009-p",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cs009-p\" class=\"anchor\" href=\"#cs009-p\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecs009-p\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for CS 009P.  Singularity Hub collection at \u003ca href=\"https://www.singularity-hub.org/collections/2064\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/2064\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1546592552.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cuda-10.1-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-101-base\" class=\"anchor\" href=\"#cuda-101-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-10.1-base\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1557313283.0
  },
  {
    "data_format": 2,
    "description": "CUDA 9.2 Singularity recipe for a machine learning environment.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cuda-9.2-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-92-base\" class=\"anchor\" href=\"#cuda-92-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-9.2-base\u003c/h1\u003e\n\u003cp\u003eCUDA 9.2 Singularity recipe for a machine learning environment.\u003c/p\u003e\n\u003cp\u003eBuilds available at \u003ca href=\"https://www.singularity-hub.org/collections/1971\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/1971\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1543808072.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/accelerator-project",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-accelerator-project\" class=\"anchor\" href=\"#accelerator-project\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eaccelerator-project\u003c/h1\u003e\n\u003cp\u003eBuilds at \u003ca href=\"https://www.singularity-hub.org/collections/1897\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/1897\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eDownload with\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity pull shub://ucr-singularity/accelerator-project\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1542422144.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cuda-10.1-ml-software",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-101-ml-software\" class=\"anchor\" href=\"#cuda-101-ml-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-10.1-ml-software\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1557451899.0
  },
  {
    "data_format": 2,
    "description": "DEPRECATED - CUDA 9, CUDNN 7, Ubuntu 16.04 Singularity recipe with dependencies for much ML software installed.",
    "filenames": [
      "Singularity",
      "Singularity.test"
    ],
    "full_name": "ucr-singularity/cuda-9.0-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-90-base\" class=\"anchor\" href=\"#cuda-90-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-9.0-base\u003c/h1\u003e\n\u003cp\u003eTHIS REPOSITORY IS DEPRECATED. Please use \u003ca href=\"https://github.com/ucr-singularity/cuda-10.1-base\"\u003ehttps://github.com/ucr-singularity/cuda-10.1-base\u003c/a\u003e instead.\u003c/p\u003e\n\u003cp\u003eCUDA 9, CUDNN 7, Ubuntu 16.04 Singularity recipe with dependencies for ML\nsoftware installed. Intended to be used as a base image for other images.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1557314427.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Xeus-Cling, to be run with JupyterHub",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/xeus-cling",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-xeus-cling\" class=\"anchor\" href=\"#xeus-cling\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003exeus-cling\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for Xeus-Cling, to be run with JupyterHub\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1534766438.0
  },
  {
    "data_format": 2,
    "description": "CS 181",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cs181",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cs181\" class=\"anchor\" href=\"#cs181\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecs181\u003c/h1\u003e\n\u003cp\u003eCS 181 Singularity recipe.  A SingularityHub collection is available at:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.singularity-hub.org/collections/1983\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/1983\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1550211352.0
  },
  {
    "data_format": 2,
    "description": "CS 172",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cs172",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cs172\" class=\"anchor\" href=\"#cs172\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecs172\u003c/h1\u003e\n\u003cp\u003eCS 172 Singularity recipe. A SingularityHub collection is available at:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.singularity-hub.org/collections/1982\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/1982\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1543918836.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/hadoop",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hadoop\" class=\"anchor\" href=\"#hadoop\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehadoop\u003c/h1\u003e\n\u003cp\u003eAvailable on Singularity Hub at \u003ca href=\"https://www.singularity-hub.org/collections/1636\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/1636\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA Hadoop, Spark, and Cassandra Singularity recipe. This is not completely self\ncontained - the configurations for each of the services are not included\nin the repo. The cluster is designed to have four nodes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOne node runs a Hadoop primary namenode and resourcemanager. There\u0027s no\nsecondary namenode.\u003c/li\u003e\n\u003cli\u003eThe other 3 nodes run hadoop nodemanager and HDFS.  They also run Cassandra.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSpark is configured to use Hadoop for running jobs.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1569898140.0
  },
  {
    "data_format": 2,
    "description": "CUDA 10.2 base image with cuDNN 7",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ucr-singularity/cuda-10.2-cudnn7-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-102-cudnn7-base\" class=\"anchor\" href=\"#cuda-102-cudnn7-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-10.2-cudnn7-base\u003c/h1\u003e\n\u003cp\u003eCUDA 10.2 base image with cuDNN 7\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1591940186.0
  },
  {
    "data_format": 2,
    "description": "Singularity definition and tools for genmon",
    "filenames": [
      "def/Singularity_apache2.recipe",
      "def/Singularity_poprep.recipe",
      "def/Singularity_genmon.recipe"
    ],
    "full_name": "pvrqualitasag/genmon-sidef",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-genmon-sidef\" class=\"anchor\" href=\"#genmon-sidef\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egenmon-sidef\u003c/h1\u003e\n\u003cp\u003eSingularity definition and tools for GenMon. GenMon is tool for monitoring genetic resources of animal populations.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621509789.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.d"
    ],
    "full_name": "bii-dpi/sing",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-genmon-sidef\" class=\"anchor\" href=\"#genmon-sidef\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egenmon-sidef\u003c/h1\u003e\n\u003cp\u003eSingularity definition and tools for GenMon. GenMon is tool for monitoring genetic resources of animal populations.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618901689.0
  },
  {
    "data_format": 2,
    "description": "Nextflow based WGS workflow for Vp (Will be updated later)",
    "filenames": [
      "environments/Singularity"
    ],
    "full_name": "anwarMZ/nf-upcoast-v",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-upcoast-v\" class=\"anchor\" href=\"#nf-upcoast-v\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-upcoast-v\u003c/h1\u003e\n\u003cp\u003eNextflow based WGS workflow for Vp (Will be updated later)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619136571.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "3.697/Singularity"
    ],
    "full_name": "icaoberg/singularity-phylip-suite",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-upcoast-v\" class=\"anchor\" href=\"#nf-upcoast-v\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-upcoast-v\u003c/h1\u003e\n\u003cp\u003eNextflow based WGS workflow for Vp (Will be updated later)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619486665.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "src/downward/misc/releases/latest/Singularity",
      "src/downward/misc/releases/20.06/Singularity.20.06",
      "src/downward/misc/releases/19.06/Singularity.19.06",
      "src/downward/misc/releases/19.12/Singularity.19.12"
    ],
    "full_name": "ScarfZapdos/conan-bge-questgen",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-a-quest-generator-based-on-conan-code\" class=\"anchor\" href=\"#a-quest-generator-based-on-conan-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA Quest generator based on CONAN code\u003c/h1\u003e\n\u003cp\u003eReference : Let CONAN tell you a story: Procedural quest generation, or multi-agent planning for interactive emergent narrative systems\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003ecd src/downward\u003cbr\u003e\n./build.py\u003cbr\u003e\ncd ../\u003cbr\u003e\npython3 adventuresawaitbge.py\u003c/p\u003e\n\u003cp\u003eIt will generate a quest for each implemented characters of BGE.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1621449910.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "SingularityRecipe"
    ],
    "full_name": "mwever/tpami-automlc",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-automl-for-multi-label-classification-overview-and-empirical-evaluation\" class=\"anchor\" href=\"#automl-for-multi-label-classification-overview-and-empirical-evaluation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAutoML for Multi-Label Classification: Overview and Empirical Evaluation\u003c/h1\u003e\n\u003cp\u003eThis project provides a platform for benchmarking different optimizers for the task of automated machine learning ensuring all optimizers to work on the same set of potential solution candidates. The implementation is based on the Java open-source library \u003ca href=\"https://github.com/starlibs/AILibs\"\u003eAILibs\u003c/a\u003e, providing the basic technical support for describing search spaces, HTN planning and heuristic search algorithms, as well as the infrastructure for synchronizing the execution of a benchmarking suite on a distributed system.\u003c/p\u003e\n\u003cp\u003eThe benchmark distinguishes itself from previously published benchmark in the way how the optimizers are integrated with the benchmarking system. While all optimizers share the same routine for executing candidate solutions, the benchmark works in a cross-platform fashion, i.e. although the benchmark and the execution of candidate solutions is implemented in Java, optimizers available in Python can be benchmarked within the system. The search space and recursive structures of the search space are automatically translated into a format understandable to the respective optimizers. The inter-platform communication is done via the \u003ca href=\"https://developers.google.com/protocol-buffers\" rel=\"nofollow\"\u003eGoogle ProtoBuf\u003c/a\u003e library which offers interfaces for various platforms. Thereby, the communication link only transfers the execution request to the benchmarking system allowing to share the same evaluation routine for all the optimizers. Another advantage is that it also allows for live-tracking the performance of the optimizers, logging each evaluated candidate and monitoring the current incumbent at any point in time.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"#quickstart---setup\"\u003eQuickstart - Setup\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#preparing-the-singularity-container\"\u003ePreparing the Singularity Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#without-a-singularity-container\"\u003eWithout a Singularity Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#test-your-setup\"\u003eTest your Setup\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#running-the-benchmark\"\u003eRunning the Benchmark\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#hardware-requirements\"\u003eHardware Requirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#initialize-the-database-server\"\u003eInitialize the Database Server\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-a-worker-client\"\u003eRunning a Worker Client\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#post-processing-for-anytime-test-performances\"\u003ePost-Processing for Anytime Test Performances\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#visualizing-benchmark-data\"\u003eVisualizing Benchmark Data\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"#inspecting-the-search-space\"\u003eInspecting the Search Space\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#list-algorithms-contained-in-the-search-space\"\u003eList Algorithms Contained in the Search Space\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#export-to-gephi-to-visualize-the-search-space-as-a-dag\"\u003eExport to Gephi to visualize the search space as a DAG\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generate-html-overview\"\u003eGenerate HTML Overview\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#evaluation-results\"\u003eEvaluation Results\u003c/a\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#generate-one-vs-rest-scatter-plots\"\u003eGenerate One-VS-Rest Scatter Plots\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generate-anytime-average-rank-plots\"\u003eGenerate Anytime Average Rank Plots\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generate-result-tables\"\u003eGenerate Result Tables\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#generate-incumbent-frequency-statistics\"\u003eGenerate Incumbent Frequency Statistics\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quickstart---setup\" class=\"anchor\" href=\"#quickstart---setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart - Setup\u003c/h2\u003e\n\u003cp\u003ePrerequisites: Due to certain dependencies requiring a Linux operating system, the execution of optimizers in Python is not supported for Windows nor MacOS. However, optimizers available in Java can still be executed on Windows or MacOS. Running a Linux OS, you may execute Python optimizers as well. To this end, please use the SingularityRecipe to build a container in order to ensure all the dependencies necessary for running the benchmark are available.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-preparing-the-singularity-container\" class=\"anchor\" href=\"#preparing-the-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreparing the Singularity Container\u003c/h3\u003e\n\u003cp\u003eIn order to set up a Singularity Container, please ensure that you have administrator permissions and installed the \u003ca href=\"https://sylabs.io/guides/3.6/user-guide/\" rel=\"nofollow\"\u003eSingularity Container\u003c/a\u003e software on your computer. (Hint: On Ubuntu you can simply install it via \u003ccode\u003esudo apt install singularity-container\u003c/code\u003e). Once you have Singularity Container installed on your system, follow these steps in order to create the container:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBuild a singularity container from the provided recipe file: \u003ccode\u003esudo singularity build automlc.sif SingularityRecipe\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eOnce the container is built, you can now proceed to run a shell within the Singularity container like this: \u003ccode\u003esingularity shell automlc.sif\u003c/code\u003e (No sudo this time).\u003c/li\u003e\n\u003cli\u003ePlease make sure that your current folder is mounted into the Singularity container. You may prove this by typing \u003ccode\u003edir\u003c/code\u003e. If the files of the project\u0027s root directory are printed on your command line, everything should be fine.\u003c/li\u003e\n\u003cli\u003eYou are now prepared to run the tasks via the gradle wrapper. For further steps please have a look at the subsequent documentation.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-without-a-singularity-container\" class=\"anchor\" href=\"#without-a-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWithout a Singularity Container\u003c/h3\u003e\n\u003cp\u003eIf you are running Linux you also have the possibility to run the benchmark directly on your system, without creating any Singularity container.\nTo this end, we have prepared a \u003ccode\u003erequirements.txt\u003c/code\u003e file so that you can install the required dependencies for the Python environment with ease.\nHowever, since we mainly work with Singularity containers to have a clearly distinct system running, preventing interferences with inappropriate python versions and compatibility conflicts in general, we do not officially support the setup variant without creating a Singularity container.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-test-your-setup\" class=\"anchor\" href=\"#test-your-setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest your Setup\u003c/h3\u003e\n\u003cp\u003eIn order to test your setup we have prepared a test runner that will work out-of-the-box if everything has been setup correctly.\nMore precisely, you can test whether each of the optimizers can be run for a specific dataset split with short timeouts of 1 minute for the entire optimization run and 45 seconds for a single evaluation.\u003c/p\u003e\n\u003cp\u003eYou can test to run each optimizer individually via the following commands:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew testHTNBF\n./gradlew testBOHB\n./gradlew testHB\n./gradlew testSMAC\n./gradlew testRandomSearch\n./gradlew testGGP\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAs a shortcut you can also simply test all the optimizers as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew testAllOptimizers\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-benchmark\" class=\"anchor\" href=\"#running-the-benchmark\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the Benchmark\u003c/h2\u003e\n\u003cp\u003eThe benchmark implemented in this repository is meant to be run in a distributed way.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hardware-requirements\" class=\"anchor\" href=\"#hardware-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHardware Requirements\u003c/h3\u003e\n\u003cp\u003eFor running a benchmark suite you need the following resources:\nA central database server managing the experiments to be executed and worker clients meeting the hardware requirements.\nIn the paper we used worker clients each equipped with \u003ccode\u003e8 CPU cores\u003c/code\u003e and \u003ccode\u003e32GB RAM\u003c/code\u003e.\nThe recommended hardware specifications for the database server depends on the degree of parallelization and how you set the parameters for the experiments.\nThe latter point is for instance depending what evaluation timeouts you choose for assessing the performance of single candidates.\nThe smaller the timeout the more intermediate evaluation results will be logged in the database.\nAs a consequence there will be a higher load on the respective database server.\nIn our study and with the timeout configuration proposed in the paper, we found that a configuration of \u003ccode\u003e4 CPU cores\u003c/code\u003e and \u003ccode\u003e16GB RAM\u003c/code\u003e is sufficient to deal with up to 200 worker clients.\nFor the database, we tested only a MySQL database. In principle other drivers are usable, but may require the inclusion of additional dependencies for the project.\nOfficially, we only support MySQL databases.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-initialize-the-database-server\" class=\"anchor\" href=\"#initialize-the-database-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInitialize the Database Server\u003c/h3\u003e\n\u003cp\u003eIn order to initialize the database server, first of all, you need to fill in the connection details in the \u003ccode\u003eautomlc-setup.properties\u003c/code\u003e file.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-ini\"\u003e\u003cpre\u003e...\n\u003cspan class=\"pl-k\"\u003edb.driver\u003c/span\u003e = mysql\n\u003cspan class=\"pl-k\"\u003edb.host\u003c/span\u003e = \u0026lt;YOUR DB HOST\u0026gt;\n\u003cspan class=\"pl-k\"\u003edb.username\u003c/span\u003e = \u0026lt;YOUR DB USER\u0026gt;\n\u003cspan class=\"pl-k\"\u003edb.password\u003c/span\u003e = \u0026lt;YOUR DB PASSWORD\u0026gt;\n\u003cspan class=\"pl-k\"\u003edb.database\u003c/span\u003e = \u0026lt;YOUR DATABASE NAME\u0026gt;\n\u003cspan class=\"pl-k\"\u003edb.table\u003c/span\u003e = \u0026lt;YOUR JOBS TABLE NAME\u0026gt;\n\u003cspan class=\"pl-k\"\u003edb.ssl\u003c/span\u003e = \u0026lt;FLAG WHETHER TO USE SSL\u0026gt;\n\u003cspan class=\"pl-k\"\u003ecandidate_eval_table\u003c/span\u003e = \u0026lt;YOUR INTERMEDIATE EVALUATIONS TABLE NAME\u0026gt;\n...\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe specifics of the benchmark are then given with the following properties in the same files:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-ini\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003emem.max\u003c/span\u003e = 32768 // maximum available memory\n\u003cspan class=\"pl-k\"\u003ecpu.max\u003c/span\u003e = 8 // number of cores\n\n... // database connection properties and AILibs experimenter specific properties \n\n\n\u003cspan class=\"pl-k\"\u003ealgorithm\u003c/span\u003e = bf,random,hb,bohb,smac,ggp // optimizers to consider\n\u003cspan class=\"pl-k\"\u003edataset\u003c/span\u003e = arts1,bibtex,birds,bookmarks,business1,computers1,education1,emotions,enron-f,entertainment1,flags,genbase,health1,llog-f,mediamill,medical,recreation1,reference1,scene,science1,social1,society1,tmc2007,yeast // datasets to consider\n\u003cspan class=\"pl-k\"\u003emeasure\u003c/span\u003e = FMacroAvgD,FMacroAvgL,FMicroAvg // performance measures to consider\n\u003cspan class=\"pl-k\"\u003esplit\u003c/span\u003e = 0,1,2,3,4,5,6,7,8,9 // split indices to consider\n\u003cspan class=\"pl-k\"\u003eseed\u003c/span\u003e = 42 // seed of the dataset splitter to consider\n\u003cspan class=\"pl-k\"\u003eglobalTimeout\u003c/span\u003e=86400 // timeout for an entire optimization run\n\u003cspan class=\"pl-k\"\u003eevaluationTimeout\u003c/span\u003e=1800 // timeout for a single candidate evaluation\n\n... // constant properties for the experiment runner\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBased on this specification, the benchmark will compute all possible combinations of entires given via the fields \u003ccode\u003ealgorithm\u003c/code\u003e, \u003ccode\u003edataset\u003c/code\u003e, \u003ccode\u003emeasure\u003c/code\u003e, \u003ccode\u003esplit\u003c/code\u003e, \u003ccode\u003eseed\u003c/code\u003e, \u003ccode\u003eglobalTimeout\u003c/code\u003e, \u003ccode\u003eevaluationTimeout\u003c/code\u003e.\nIn principle it is also possible to configure multiple seeds, globalTimeouts or evaluationTimeouts in the same style as it is done e.g. for the algorithm field, i.e. simply by seperating multiple values by a comma.\u003c/p\u003e\n\u003cp\u003eOnce the database connection is configured and the property values for all the benchmark suite specific parameters have been set, you can proceed by initializing the database server centrally managing the experiment conduction with the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew initializeExperimentsInDatabase\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will automatically create a table with the name specified in the \u003ccode\u003edb.table\u003c/code\u003e property which will then specify all the experiments to be executed which have been computed via taking the cross-product of all possible combinations of the properties describing the benchmark suite.\u003c/p\u003e\n\u003cp\u003eFor cleaning this table, i.e., removing all of its entries, you can run the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew cleanExperimentsInDatabase\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eCaution:\u003c/strong\u003e This functionality will also remove all results that are stored in this table after running an experiment.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-preparing-dataset-splits\" class=\"anchor\" href=\"#preparing-dataset-splits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreparing Dataset Splits\u003c/h3\u003e\n\u003cp\u003eThe original datasets from which the train and test splits have been derived are provided via this repository as well.\nThe datasets are located in the \u003ccode\u003eoriginal_datasets/\u003c/code\u003e directory.\nIn order to derive the train and test dataset splits via a 10-fold cross-validation, run the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew generateDatasetSplits\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will create a directory \u003ccode\u003edatasets/\u003c/code\u003e, where the generated train and test splits are stored in seperated files.\nAs this procedure is done for all datasets contained in the \u003ccode\u003eoriginal_datasets/\u003c/code\u003e directory and seed combinations, this probably takes some time and disk space.\nThe generated files follow the name schema \u003ccode\u003e\u0026lt;DATASET\u0026gt;_\u0026lt;SEED\u0026gt;_\u0026lt;SPLIT INDEX\u0026gt;_{train|test}.arff\u003c/code\u003e. As the worker client relies on this naming, the schema must not be changed.\nUnfortunately, we cannot provide the dataset splits used in our study directly, as this would dramatically increase the size of the repository and lead to unreasonable download times for cloning.\nHowever, on request we can of course provide the original dataset splits.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: We assume that all worker clients either have all the dataset splits locally available or share a network hard drive, centrally providing access to the respective dataset splits. The dataset folder can be configured in the \u003ccode\u003eautomlc-setup.properties\u003c/code\u003e file via the property \u003ccode\u003edatasetFolder\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-a-worker-client\" class=\"anchor\" href=\"#running-a-worker-client\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning a Worker Client\u003c/h3\u003e\n\u003cp\u003eOnce everything is set up correctly, you may run a worker client via the command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew runExperimentEvaluationWorker\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe execution of the worker will also rely on the database connection configured in the \u003ccode\u003eautomlc-setup.properties\u003c/code\u003e.\nHowever, there is nothing specific you need to configure when deploying the worker client in a distributed way.\nIn fact, you can simply run the same command multiple times (on different nodes) in order to parallelize the processing of the benchmark.\u003c/p\u003e\n\u003cp\u003eThe central database server will take care that each of the specified experiments will be executed only once at maximum, i.e. it will prevent the same experiment from being conducted twice.\nSince one worker client will also only take care of running a single experiment, you need to deploy as many worker clients as there are rows in the jobs table (named as you configured it in the properties file). In addition to the final results, the worker clients will also store intermediate evaluation results, i.e., candidates that have been requested for evaluation by the respective optimizer together with the measured performance value.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-post-processing-for-anytime-test-performances\" class=\"anchor\" href=\"#post-processing-for-anytime-test-performances\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePost-Processing for Anytime Test Performances\u003c/h3\u003e\n\u003cp\u003eSince assessing the test performances on-line, i.e. during an optimizer run, would distort the overall perception of the optimization performance, all the test performances for later on compiling anytime plots etc. have been estimated via a post-processing step based on the log of intermediate candidate evaluations.\nThe post-processing is also implemented in a distributed way, analoguous to the already explained setup for running the actual benchmark of optimizers. In contrast to the latter, for the post-processing the \u003ccode\u003etest-eval.properties\u003c/code\u003e serves as a configuration.\nObviously, the jobs table for the post-processing needs a different name than the one specified for the benchmark itself. Furthermore, it will also use the \u003ccode\u003eautomlc-setup.properties\u003c/code\u003e file for accessing the logged data and filtering the evaluated candidates.\nAdditionally, the post-processing requires access to the datasets directory.\u003c/p\u003e\n\u003cp\u003eAs before you can configure the corresponding properties, for which optimizers,datasets, measures, etc. you want to run the post-processing. Furthermore you can setup the jobs table for distributing the workload on a cluster etc. as already described before for the benchmark via the following commands:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew initializePostProcessingsInDatabase\n./gradlew cleanPostProcessingsInDatabase\n./gradlew runPostProcessingWorker\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-visualizing-benchmark-data\" class=\"anchor\" href=\"#visualizing-benchmark-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVisualizing Benchmark Data\u003c/h2\u003e\n\u003cp\u003eThroughout the paper, several visualization of the results have been presented. A pre-processed version of the logged data is contained in the directory \u003ccode\u003eresults/data/\u003c/code\u003e.\nFrom this data, you can further process the data to derive the numbers presented in the paper. Plots that have been generated via LaTeX\u0027s \u003ccode\u003etikz\u003c/code\u003e or \u003ccode\u003epgfplots\u003c/code\u003e, you can even generate the corresponding LaTeX code in the following.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-inspecting-the-search-space\" class=\"anchor\" href=\"#inspecting-the-search-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInspecting the Search Space\u003c/h3\u003e\n\u003cp\u003eThe configuration files containing the specification of the search space are contained in the folder \u003ccode\u003esearchspace/\u003c/code\u003e. For this, the root file is \u003ccode\u003esearchspace/meka-all.json\u003c/code\u003e and (recursively) includes the remaining configuration files\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-list-algorithms-contained-in-the-search-space\" class=\"anchor\" href=\"#list-algorithms-contained-in-the-search-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eList Algorithms Contained in the Search Space\u003c/h4\u003e\n\u003cp\u003eIf you only want to obtain a list of algorithms ordered by algorithm type that are contained in the search space including their abbreviation as given in the paper, you can use the following short cut:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew exportAlgorithmsInSearchSpace\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will create a txt file in the \u003ccode\u003e/results\u003c/code\u003e directory with the name \u003ccode\u003esearchspace-algorithms-in-space.txt\u003c/code\u003e listing all the algorithm types together with the algorithms belonging to these types.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-export-to-gephi-to-visualize-the-search-space-as-a-dag\" class=\"anchor\" href=\"#export-to-gephi-to-visualize-the-search-space-as-a-dag\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExport to Gephi to visualize the search space as a DAG\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://gephi.org/\" rel=\"nofollow\"\u003eGephi\u003c/a\u003e is a graph modeling and visualization tool. You can export the search space as described in the \u003ccode\u003esearchspace/\u003c/code\u003e folder to the Gephi graph format to be loaded and visualized in Gephi. By running\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew exportSearchSpacesToGephiFormat\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eyou will find a folder \u003ccode\u003eresults/gephi-export\u003c/code\u003e, containing three files: \u003ccode\u003eslc-only.gephi\u003c/code\u003e, \u003ccode\u003emlc-only.gephi\u003c/code\u003e, and \u003ccode\u003emlc-complete.gephi\u003c/code\u003e. These three files contain a Gephi specification of a directed acyclic graph (DAG) where each node corresponds to one algorithm in the search space and an edge a dependency relation from one to another algorithm indicating that the algorithm represented by the parent node can be configured with the child node as a base learner/kernel. With these graph visualizations one can easily see the exponential growth of the search space when combining the single-label classification algorithms with the multi-label classification algorithms.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-generate-html-overview\" class=\"anchor\" href=\"#generate-html-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate HTML Overview\u003c/h4\u003e\n\u003cp\u003eGenerate an HTML document summarizing the search space in terms of some statistics, listing all included algorithms together with their hyper-parameters (including their types, domains, and defaults) and recursive dependencies on other algorithms. The possible choices for each dependency are listed and linked within the document accordingly.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew generateMultiLabelSearchSpaceDescription\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will generate a file \u003ccode\u003eresults/searchspace-meka.html\u003c/code\u003e. If you want to generate the same type of description for the single-label classification (WEKA) search space use the following.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew generateSingleLabelSearchSpaceDescription\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will generate a file \u003ccode\u003eresults/searchspace-weka.html\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe bar charts comparing the two search spaces in the paper have been generated from the statistical values contained on the top of these HTML documents. However, since the plots have been generated using \u003ccode\u003ematplotlibs\u003c/code\u003e in Python, please refer to the Jupyter notebook \u003ccode\u003eTPAMI Plots.ipynb\u003c/code\u003e for deriving the bar charts from these statistics.\nTherewith, you can obtain the comparison figure for the different search spaces.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-evaluation-results\" class=\"anchor\" href=\"#evaluation-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEvaluation Results\u003c/h3\u003e\n\u003cp\u003eThe data obtained by running the benchmark across various datasets and folds can be found in \u003ccode\u003eresults/data\u003c/code\u003e. From this data you can generate several statistics, summaries, and plots. How these can be generated is explained in the following.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-generate-one-vs-rest-scatter-plots\" class=\"anchor\" href=\"#generate-one-vs-rest-scatter-plots\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate One-VS-Rest Scatter Plots\u003c/h4\u003e\n\u003cp\u003eIn the paper we compared the different optimizers in a one-vs-rest fashion plotting their performances against each other facilitating the analysis which approach performs preferably over the rest. The plots can be generated with the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew compileScatterPlots\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe command will produce its output in the folder \u003ccode\u003eresults/scatter-plots\u003c/code\u003e, where afterwards you will find for each optimizer and performance measure (instance-wise F-measure, label-wise F-measure, and micro F-measure) you will find a LaTeX file named like this: \u003ccode\u003escatter-\u0026lt;optimizer\u0026gt;VSrest-\u0026lt;measure\u0026gt;.tex\u003c/code\u003e. Furthermore you can find a \u003ccode\u003emain.tex\u003c/code\u003e which will include all packages and scatter plots to compile them into a PDF document.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-generate-anytime-average-rank-plots\" class=\"anchor\" href=\"#generate-anytime-average-rank-plots\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate Anytime Average Rank Plots\u003c/h4\u003e\n\u003cp\u003eFurthermore, we have presented anytime average rank plots comparing the different optimizers across the time dimension. You can compile the result data into these anytime plots by executing the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew compileAnytimeAverageRankPlots\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will generate several \u003ccode\u003e.tex\u003c/code\u003e-files in the directory \u003ccode\u003eresults/anytime-plots/\u003c/code\u003e.\nIn fact, there are even two different types of plots: First following the naming schema \u003ccode\u003eavgrank-\u0026lt;MEASURE\u0026gt;.tex\u003c/code\u003e you will find the average rank plots as presented in the paper.\nHowever, for each combination of measure and dataset you can also find the actual average anytime performance of the different optimizers contained in the files named after the schema \u003ccode\u003e\u0026lt;MEASURE\u0026gt;_\u0026lt;DATASET\u0026gt;.tex\u003c/code\u003e.\nSince presenting those would have required lots of space, these plots have not been included in the paper but are made available here as a kind of supplementary material.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-generate-result-tables\" class=\"anchor\" href=\"#generate-result-tables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate Result Tables\u003c/h4\u003e\n\u003cp\u003eFor a comparison of the performance of eventually returned incumbents, we presented tables for each measure individually.\nYou can generate these tables yourself once again by running the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew compileResultTables\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will again generate \u003ccode\u003e.tex\u003c/code\u003e-files containing the LaTeX code to represent the corresponding table data. In addition to the average performances per dataset and optimizer, a Wilcoxon signed-rank test is conducted with a threshold for the p-value of 0.05. The best results, significant improvements and degradations are highlighted as described in the paper. Furthermore, in the last row of the tables, an average rank for each optimizer across all datasets is given.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-generate-incumbent-frequency-statistics\" class=\"anchor\" href=\"#generate-incumbent-frequency-statistics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate Incumbent Frequency Statistics\u003c/h4\u003e\n\u003cp\u003eAnother figure in the paper shows which algorithms have been chosen with what frequency by which optimizer. Since these plots have been generated with the help of matplotlibs, here, we will only compile the necessary statistics from the data, necessary to produce the figures.\nIn order to compile the statistics from the result data, run the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./gradlew compileIncumbentFrequencyStatistics\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will generate a txt file \u003ccode\u003eresults/incumbent-frequencies.txt\u003c/code\u003e containing JSON arrays that can be copied and pasted into a Jupyter notebook which is also available via this repository. Please refer to the \u003ccode\u003eTPAMI Plots.ipynb\u003c/code\u003e notebook file for further processing of the compiled raw data into the nested donut charts.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611333170.0
  },
  {
    "data_format": 2,
    "description": "Practice codes for high-performance computing",
    "filenames": [
      "OpenACC_Bootcamp/nways/Singularity",
      "OpenACC_Bootcamp/nways_cfd/Singularity"
    ],
    "full_name": "20akshay00/HPC_practice",
    "latest_release": null,
    "readme": "\u003ch4\u003e\n\u003ca id=\"user-content-open-acc-gpu-bootcamp\" class=\"anchor\" href=\"#open-acc-gpu-bootcamp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpen ACC; GPU bootcamp:\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://www.gpuhackathons.org/events-overview\" rel=\"nofollow\"\u003ehttps://www.gpuhackathons.org/events-overview\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-mpi-workshop\" class=\"anchor\" href=\"#mpi-workshop\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMPI Workshop:\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://sites.google.com/view/mpi-workshop\" rel=\"nofollow\"\u003ehttps://sites.google.com/view/mpi-workshop\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619722913.0
  },
  {
    "data_format": 2,
    "description": "WIP: CookieCutter Docker/Singularity container project template",
    "filenames": [
      "{{ cookiecutter.repo_name }}/Singularity"
    ],
    "full_name": "dl-container-registry/container-cookiecutter",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc-container-cookiecutter\" class=\"anchor\" href=\"#hpc-container-cookiecutter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPC Container CookieCutter\u003c/h1\u003e\n\u003cp\u003eContainer template repo for building joint docker and singularity images. The\ntemplate assumes the docker image contains the container, and the singularity\nfile simply pulls down the docker image and converts it to a singularity\ncontainer.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstantiate template using \u003ccode\u003ecookiecutter\u003c/code\u003e:\n\u003ccode\u003ecookiecutter https://github.com/dl-container-registry/container-cookiecutter\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ca href=\"https://github.com/new\"\u003enew github repository\u003c/a\u003e with the same name\nthat you entered for \u003ccode\u003erepo_name\u003c/code\u003e in the template.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSet up GitHub Auto-Deployment integration in settings.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGithub Token: Your GitHub token (obtained from \u003ca href=\"https://github.com/settings/tokens\"\u003edeveloper\nsettings\u003c/a\u003e with \u003ccode\u003erepo_deployment\u003c/code\u003e privileges)\u003c/li\u003e\n\u003cli\u003eEnvironment: \u003ccode\u003esingularity\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eDeploy on status: tick\u003c/li\u003e\n\u003cli\u003eGitHub api url: empty\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSet up Singularity hub\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreate a \u003ca href=\"https://www.singularity-hub.org/collections/new\" rel=\"nofollow\"\u003enew container collection\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eChange settings:\n\u003cul\u003e\n\u003cli\u003eBuild Trigger: Deployment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSave.\u003c/li\u003e\n\u003cli\u003eNote Singularity Hub ID, replace \u003ccode\u003eSH_ID\u003c/code\u003e with your ID in the README.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSet up \u003ca href=\"https://travis-ci.org/\" rel=\"nofollow\"\u003eTravis\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://travis-ci.org/profile/\" rel=\"nofollow\"\u003eEnable travis on repository\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eClick the cog next to the repository to go to the settings.\u003c/li\u003e\n\u003cli\u003eSet \u003ccode\u003eDOCKER_USERNAME\u003c/code\u003e to your docker username.\u003c/li\u003e\n\u003cli\u003eSet \u003ccode\u003eDOCKER_PASSWORD\u003c/code\u003e to your docker password (make sure you to quote the full\npassword in case your password has special characters).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [
      "cookiecutter",
      "template",
      "docker",
      "singularity",
      "container",
      "hpc",
      "travis"
    ],
    "updated_at": 1517774874.0
  },
  {
    "data_format": 2,
    "description": "Simulating, Reconstructing and Analysing Data for FEL IDI Experiments",
    "filenames": [
      "Singularity.py38",
      "Singularity",
      "Singularity.simple"
    ],
    "full_name": "fzimmermann89/idi",
    "latest_release": "210514",
    "readme": "\u003cp\u003eCAVE: Hic sunt dracones\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThe code is a mess, undocumented and only certain code paths are tested.\u003c/em\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-idi---incoherent-diffraction-imaging\" class=\"anchor\" href=\"#idi---incoherent-diffraction-imaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIDI - INCOHERENT DIFFRACTION IMAGING\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4824\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg\" alt=\"tests\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/fzimmermann89/idi.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity Image now at \u003ca href=\"https://cloud.sylabs.io/library/_container/607b669a4ad4aa1fdea0c43c\" rel=\"nofollow\"\u003elibrary://fzimmermann89/idi/idi\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eConda Pacakges at \u003ca href=\"https://anaconda.org/zimmf/idi\" rel=\"nofollow\"\u003ezimmf/idi\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePIP Source at \u003ca href=\"https://pypi.org/project/idi/\" rel=\"nofollow\"\u003eidi\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWheels at \u003ca href=\"https://github.com/fzimmermann89/idi/releases/latest\"\u003eReleases\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-content-of-the-repo\" class=\"anchor\" href=\"#content-of-the-repo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtent of the repo\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eipynb: example notebooks\u003c/li\u003e\n\u003cli\u003esimulation: simulation of incoherent images\u003c/li\u003e\n\u003cli\u003ereconstruction: direct and ft based reconstruction\u003c/li\u003e\n\u003cli\u003eutil: some small utilities for data analysis, geometry and random distributions, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-preparation-for-slac-sdf\" class=\"anchor\" href=\"#preparation-for-slac-sdf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epreparation for slac sdf:\u003c/h2\u003e\n\u003cp\u003eUse Singulariy, if using OOD launcher, use the following to start a jupyterhub\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    function jupyter() { singularity run --app jupyter --nv -B /sdf,/gpfs,/scratch,/lscratch library://fzimmermann89/idi/idi $@; }\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-preparation-for-sacla\" class=\"anchor\" href=\"#preparation-for-sacla\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epreparation for sacla:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDownload and install miniconda, setup ssh tunnel for web access.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n local3 python=3.7 numpy mkl mkl-dev ipython ipykernel cython jinja2 numba numexpr matplotlib six scipy jupyterlab\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate local3\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install https://github.com/fzimmermann89/idi/\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epython -m ipykernel install --user --name local-simulation-env3 --display-name \"local simulation(py37)\"\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(C) Felix Zimmermann\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "idi",
      "reconstruction",
      "simulation",
      "xray",
      "incoherent-images",
      "fel"
    ],
    "updated_at": 1621013018.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for chalk-cli.",
    "filenames": [
      "4.1.0/Singularity"
    ],
    "full_name": "icaoberg/singularity-chalk-cli",
    "latest_release": "v4.1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-chalk-cli\" class=\"anchor\" href=\"#singularity-chalk-cli\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-chalk-cli\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.travis-ci.com/icaoberg/singularity-chalk-cli\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/edef36a43b2f6e93623b669084ecd8c07f14abb4e6a940915e33cbf76670c50c/68747470733a2f2f7777772e7472617669732d63692e636f6d2f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c692e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/icaoberg/singularity-chalk-cli.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/chalk/chalk-cli\"\u003echalk-cli\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-or-similar\" class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges (or similar)\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003echalk-cli\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/chalk-cli/4.1.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modules/chalk-cli\u003c/code\u003e as \u003ccode\u003e4.1.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec singularity-chalk-cli-4.1.0.sif chalk -t \u0027{red.bold Dungeons and Dragons {~bold.blue (with added fairies)}}\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"images/screenshot.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/screenshot.png\" alt=\"Screenshot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-alternative-installation\" class=\"anchor\" href=\"#alternative-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlternative Installation\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003espack install npm\nspack load npm\nnpm install -g chalk-cli\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "cli-utilities",
      "utilities"
    ],
    "updated_at": 1619482220.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for afl++ (https://github.com/AFLplusplus/AFLplusplus)",
    "filenames": [
      "Singularity.1604",
      "Singularity.1804",
      "Singularity.i386",
      "focal/Singularity.2004"
    ],
    "full_name": "shub-fuzz/aflpp",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity image for AFL++ (\u003ca href=\"https://github.com/AFLplusplus/AFLplusplus\"\u003ehttps://github.com/AFLplusplus/AFLplusplus\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name aflpp.sif https://github.com/shub-fuzz/aflpp/releases/download/0.0.1/shub-fuzz-aflpp.1604.sif\n\nsingularity shell aflpp.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 18.04 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name aflpp.1804.sif https://github.com/shub-fuzz/aflpp/releases/download/0.0.1/shub-fuzz-aflpp.1804.sif\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619233273.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for SymCC (https://github.com/eurecom-s3/symcc)",
    "filenames": [
      "Singularity.1804",
      "Singularity.2004"
    ],
    "full_name": "shub-fuzz/symcc",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity image for \u003ca href=\"https://github.com/eurecom-s3/symcc\"\u003eSymCC\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/symcc/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/symcc/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4732\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name symcc.sif https://github.com/shub-fuzz/symcc/releases/download/0.0.1/shub-fuzz-symcc.1804.sif\n\nsingularity shell symcc.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619232345.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for Intriguer (https://github.com/seclab-yonsei/intriguer)",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/intriguer",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity image for Intriguer (\u003ca href=\"https://github.com/seclab-yonsei/intriguer\"\u003ehttps://github.com/seclab-yonsei/intriguer\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/intriguer/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/intriguer/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5086\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name intriguer.sif https://github.com/shub-fuzz/intriguer/releases/download/0.0.1/shub-fuzz-intriguer.1604.sif\n\nsingularity shell intriguer.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619232642.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for AFLGo (https://github.com/aflgo/aflgo)",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/aflgo",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-aflgo\" class=\"anchor\" href=\"#aflgo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAFLGo\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/aflgo/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/aflgo/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5085\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for AFLGo (\u003ca href=\"https://github.com/aflgo/aflgo\"\u003ehttps://github.com/aflgo/aflgo\u003c/a\u003e)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name aflgo.sif https://github.com/shub-fuzz/aflgo/releases/download/0.0.1/shub-fuzz-aflgo.1604.sif\n\nsingularity shell aflgo.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619232831.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for ParmeSan (https://github.com/vusec/parmesan)",
    "filenames": [
      "Singularity.2004"
    ],
    "full_name": "shub-fuzz/parmesan",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity image for ParmeSan (\u003ca href=\"https://github.com/vusec/parmesan\"\u003ehttps://github.com/vusec/parmesan\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/afl/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/afl/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5084\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name afl.sif https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.2004.sif\n\nsingularity shell afl.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619232723.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for Ankou (https://github.com/SoftSec-KAIST/Ankou)",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/ankou",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity image for Ankou (\u003ca href=\"https://github.com/SoftSec-KAIST/Ankou\"\u003ehttps://github.com/SoftSec-KAIST/Ankou\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4173\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name ankou.sif https://github.com/shub-fuzz/ankou/releases/download/0.0.1/shub-fuzz-ankou.1604.sif\n\nsingularity shell ankou.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619231819.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for Eclipser (https://github.com/SoftSec-KAIST/Eclipser)",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/eclipser",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity image for Eclipser (\u003ca href=\"https://github.com/SoftSec-KAIST/Eclipser\"\u003ehttps://github.com/SoftSec-KAIST/Eclipser\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name eclipser.sif https://github.com/shub-fuzz/eclipser/releases/download/0.0.1/shub-fuzz-eclipser.1604.sif\n\nsingularity shell eclipser.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619231857.0
  },
  {
    "data_format": 2,
    "description": "QSYM  - Concolic Execution Engine (https://github.com/sslab-gatech/qsym)",
    "filenames": [
      "Singularity.1604",
      "Singularity.1804"
    ],
    "full_name": "shub-fuzz/qsym",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity Image for QSYM (\u003ca href=\"https://github.com/sslab-gatech/qsym\"\u003ehttps://github.com/sslab-gatech/qsym\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3625\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eQSYM  - Concolic Execution Engine (\u003ca href=\"https://github.com/sslab-gatech/qsym\"\u003ehttps://github.com/sslab-gatech/qsym\u003c/a\u003e)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name qsym.sif https://github.com/shub-fuzz/qsym/releases/download/0.0.1/shub-fuzz-qsym.1604.sif\n\nsingularity shell qsym.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619231485.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for honggfuzz (https://github.com/google/honggfuzz)",
    "filenames": [
      "Singularity.1604",
      "Singularity.1804",
      "Singularity.i386",
      "v21/Singularity.v21"
    ],
    "full_name": "shub-fuzz/honggfuzz",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3641\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for honggfuzz (\u003ca href=\"https://github.com/google/honggfuzz\"\u003ehttps://github.com/google/honggfuzz\u003c/a\u003e)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name honggfuzz.sif https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.1/shub-fuzz-honggfuzz.1604.sif\n\nsingularity shell honggfuzz.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 18.04 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name honggfuzz.1804.sif https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.1/shub-fuzz-honggfuzz.1804.sif\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619230950.0
  },
  {
    "data_format": 2,
    "description": "LLVM",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/llvm",
    "latest_release": "0.0.1",
    "readme": "\u003cp\u003eSingularity Image for LLVM w/AFL++ (currently v12)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/llvm/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/llvm/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name llvm.sif https://github.com/shub-fuzz/llvm/releases/download/0.0.1/shub-fuzz-llvm.1604.sif\n\nsingularity shell llvm.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619231247.0
  },
  {
    "data_format": 2,
    "description": "Singularity Image for AFL (https://github.com/google/AFL)",
    "filenames": [
      "Singularity.1604",
      "Singularity.1804",
      "Singularity.i386"
    ],
    "full_name": "shub-fuzz/afl",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-afl\" class=\"anchor\" href=\"#afl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eafl\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/afl/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/afl/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3617\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity Image for AFL (\u003ca href=\"https://github.com/google/AFL\"\u003ehttps://github.com/google/AFL\u003c/a\u003e)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage (x86_64 container):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name afl.sif https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.1604.sif\n\nsingularity shell afl.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 18.04 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name afl.1804.sif https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.1804.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 16.04 i386 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name afl_i386.sif https://github.com/shub-fuzz/afl/releases/download/0.0.1/shub-fuzz-afl.i386.sif\nsingularity pull --name afl_i386.sif shub://shub-fuzz/afl:i386\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619228309.0
  },
  {
    "data_format": 2,
    "description": "Singularity container definition for the upcoming Pogona simulator",
    "filenames": [
      "Singularity"
    ],
    "full_name": "lumpiluk/pogona-container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pogona-container\" class=\"anchor\" href=\"#pogona-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epogona-container\u003c/h1\u003e\n\u003cp\u003eSingularity container definition for the upcoming Pogona simulator\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4128\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eTo use a pre-built image, first pull it from Singularity Hub:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull shub://lumpiluk/pogona-container\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAfterwards, you can open a subshell using the image by running\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./singularity_shell.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFrom there, all prerequisites for running OpenFOAM simulations or setting up a Python Pipenv/Virtualenv for the Pogona simulator should be available.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619194569.0
  },
  {
    "data_format": 2,
    "description": "A definition file for building TAMA singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sguizard/TAMA-singularity",
    "latest_release": "v1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tama-singularity\" class=\"anchor\" href=\"#tama-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTAMA-singularity\u003c/h1\u003e\n\u003cp\u003eA definition file for building TAMA singularity container\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-container\" class=\"anchor\" href=\"#building-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding container\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build TAMA.{sif, def}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-tama\" class=\"anchor\" href=\"#using-tama\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing TAMA\u003c/h2\u003e\n\u003cp\u003eThere are two main Python scripts in TAMA:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etama_collapse.py\u003c/li\u003e\n\u003cli\u003etama_merge.py\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThey can be run as follows:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tama_collapsepy\" class=\"anchor\" href=\"#tama_collapsepy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/GenomeRIK/tama/wiki/Tama-Collapse\"\u003etama_collapse.py\u003c/a\u003e\n\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec TAMA.sif tama_collapse.py -h\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tama_mergepy\" class=\"anchor\" href=\"#tama_mergepy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/GenomeRIK/tama/wiki/Tama-Merge\"\u003etama_merge.py\u003c/a\u003e\n\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec TAMA.sif tama_merge.py -h\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619213900.0
  },
  {
    "data_format": 2,
    "description": "Container for AnnotSV software. ",
    "filenames": [
      "Singularity",
      "Singularity_2.2"
    ],
    "full_name": "Clinical-Genomics-Lund/annotsv_container",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-annotsv-container-for-wgs-pipeline\" class=\"anchor\" href=\"#annotsv-container-for-wgs-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnnotSV container for WGS pipeline\u003c/h2\u003e\n\u003cp\u003eThis is a singularity recipe for AnnotSV 2.3 and 2.2.\u003c/p\u003e\n\u003cp\u003esudo singularity build annotsv2.3.sif Singularity\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1618855170.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "jchrist27/singularity-test",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-annotsv-container-for-wgs-pipeline\" class=\"anchor\" href=\"#annotsv-container-for-wgs-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnnotSV container for WGS pipeline\u003c/h2\u003e\n\u003cp\u003eThis is a singularity recipe for AnnotSV 2.3 and 2.2.\u003c/p\u003e\n\u003cp\u003esudo singularity build annotsv2.3.sif Singularity\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618801208.0
  },
  {
    "data_format": 2,
    "description": "Builds a singularity container around the forced alignment tool Gentle",
    "filenames": [
      "Singularity.recipe"
    ],
    "full_name": "AudiovisualMetadataPlatform/gentle-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-gentle-singularity\" class=\"anchor\" href=\"#gentle-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egentle-singularity\u003c/h1\u003e\n\u003cp\u003eBuilds a singularity container around the forced alignment tool Gentle\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1618627863.0
  },
  {
    "data_format": 2,
    "description": "A high-performance computing implementation of DMC",
    "filenames": [
      "RynLib/setup/build/Singularity/Singularity.def",
      "RynLib/setup/build/Singularity/SingularityUpdate.def",
      "RynLib/setup/build/Singularity/Singularity.ubuntu",
      "RynLib/setup/build/Singularity/SingularityCore.def"
    ],
    "full_name": "McCoyGroup/PyHPCDMC",
    "latest_release": "v1.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pyhpdcdmc\" class=\"anchor\" href=\"#pyhpdcdmc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePyHPDCDMC\u003c/h1\u003e\n\u003cp\u003eThis started out as a quick layer between python and entos for running DMC\u003c/p\u003e\n\u003cp\u003eIt\u0027s grown a bit...\u003c/p\u003e\n\u003cp\u003eYou can find some documentation \u003ca href=\"https//:mccoygroup.github.io/Documentation/RynLib\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618010480.0
  },
  {
    "data_format": 2,
    "description": "Test with singularity-deploy",
    "filenames": [
      "Singularity",
      "Singularity.salad",
      "Singularity.pokemon"
    ],
    "full_name": "thomasarsouze/singularity-deploy",
    "latest_release": "0.0.12",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-deploy\" class=\"anchor\" href=\"#singularity-deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Deploy\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"img/shpc.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"img/shpc.png\" alt=\"img/shpc.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWouldn\u0027t it be nice to build Singularity images without a registry proper,\nand just keep them alongside the GitHub codebase? This is now possible!\nThis small repository provides an example to get you started. It will\nbuild one or more images (whatever Singularity.* files that are present at\nthe root) and then release them as assets to your GitHub repository so\nthat they can be programatically obtained. It is associated with\n\u003ca href=\"https://github.com/singularityhub/singularity-hpc\"\u003esingularity-hpc\u003c/a\u003e to allow\nyou to then define LMOD modules for these same containers.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCan I upload the largest of chonkers?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYes and no. Note that assets are limited to 2 GB in size, which is still fairly good. You can use\nit as a template for your own recipes as is, or modify it for your custom\nuse case. Instructions are below!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-template-or-fork\" class=\"anchor\" href=\"#1-template-or-fork\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Template or Fork\u003c/h3\u003e\n\u003cp\u003eIf you haven\u0027t already, template or fork this repository. You can then clone\nyour fork:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git clone git@github.com:\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/singularity-deploy\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou likely want to name the repository by the container. For example, if I would\nhave created a container on Docker Hub or similar with the name \u003ccode\u003evsoch/salad\u003c/code\u003e,\nhere I\u0027d call the repository \u003ccode\u003esalad\u003c/code\u003e. You obviously are limited to your username\nor an organizational namespace.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-write-your-singularity-recipes\" class=\"anchor\" href=\"#1-write-your-singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Write your Singularity Recipe(s)\u003c/h3\u003e\n\u003cp\u003eFirst, you should write your container recipe(s) in the present working directory.\nFor good practice, when you are updating recipes you should checkout a new branch\nand open a pull request, as the repository comes with a workflow to trigger on a PR\nto \u003ca href=\".github/workflows/test.yml\"\u003etest your container build\u003c/a\u003e. You can add any additional\ntests that that you might need. By default, any Singularity.* file will be automatically detected.\nIf there is no extension (the name Singularity), the name used will be \"latest.\"\nYou can use these tags across multiple releases of your containers. For example,\nthese files would generate packages with sifs named as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"Singularity\"\u003eSingularity\u003c/a\u003e maps to \u003ca href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\u003ehttps://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"Singularity.pokemon\"\u003eSingularity.pokemon\u003c/a\u003e maps to \u003ca href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif\"\u003ehttps://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"Singularity.salad\"\u003eSingularity.salad\u003c/a\u003e maps to \u003ca href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\u003ehttps://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor each name, you can see the direct download URL contains the repository (singularityhub/singularity-deploy),\nYou should not use any \u003ccode\u003e:\u003c/code\u003e characters in either your container tag (the GitHub extension) or\nthe GitHub tags (the release tags) as this might interfere with parsing.\nThe GitHub release tag (0.0.1 in the example above) is discussed next.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-update-the-version-file\" class=\"anchor\" href=\"#2-update-the-version-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Update the VERSION file\u003c/h3\u003e\n\u003cp\u003eAny time that you prepare new container recipes, you should update the \u003ca href=\"VERSION\"\u003eVERSION\u003c/a\u003e\nfile. The way that this repository works is to generate a release based on the\nstring in \u003ccode\u003eVERSION\u003c/code\u003e. A version is just a tag, so it could be something like\n\u003ccode\u003e0.0.1\u003c/code\u003e or \u003ccode\u003e0.0.1-slim\u003c/code\u003e. Keep in mind that GitHub releases cannot have duplicated\nnames, so you should not repeat the same tag. Do not use \u003ccode\u003e:\u003c/code\u003e in your tag names.\nIf you do need to re-release a tag (not recommended if a user might be using it and then it\u0027s changed) you can manually delete\nthe release and the tag in the GitHub interface. This is a nice structure because it\nmeans you can have containers with different names under the same tag. In the example\nabove, we have each of \"deploy,\" \"latest,\" and \"salad\" released under tag 0.0.1.\nThis is how it looks on GitHub:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"img/releases.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"img/releases.png\" alt=\"img/releases.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-how-to-develop\" class=\"anchor\" href=\"#3-how-to-develop\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. How to Develop\u003c/h3\u003e\n\u003cp\u003eAs we mentioned previously, the container builds will be tested on a pull request,\nand the release will trigger on merge into your main branch (main). See the \u003ca href=\".github/workflows/builder.yml\"\u003e.github/workflows/builder.yml\u003c/a\u003e)\nto edit this. The idea is that you can:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDevelop your container via a development branch\u003c/li\u003e\n\u003cli\u003eOpen a pull request to test the container (the \u003ca href=\".github/workflows/test.yml\"\u003e.github/workflows/test.yml\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eOn merge, your container will be released!\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-how-to-pull\" class=\"anchor\" href=\"#4-how-to-pull\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. How to pull\u003c/h3\u003e\n\u003cp\u003eTechnically, Singularity can pull just knowing the URL. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity pull https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHowever, the \u003ca href=\"singularity-hpc\"\u003esingularity-hpc\u003c/a\u003e tool (will be) designed to be able to parse and handle\nthese container uris automatically. For the containers here, you could do:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:latest\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:salad\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:pokemon\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor write the container URI into a registry entry:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egh: singularityhub/singularity-deploy\nlatest:\n  latest: \"0.0.1\"\ntags:\n  \"latest\": \"0.0.1\"\n  \"salad\": \"0.0.1\"\n  \"pokemon\": \"0.0.1\"\nmaintainer: \"@vsoch\"\nurl: https://github.com/singularityhub/singularity-deploy\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(This part is still under development!)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617984120.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ddcap/singularity-halvade",
    "latest_release": "hadoop_conf_halvade",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-use-singularity-image\" class=\"anchor\" href=\"#use-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse singularity image\u003c/h1\u003e\n\u003cp\u003eSingularity image:\n\u003ca href=\"https://singularity-hub.org/collections/5392\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity instance start \\\n  --bind \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003emktemp -d run/hostname_XXXX\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:/run \\\n  --bind dropbear/:/etc/dropbear \\\n  --bind log/:/usr/local/spark/logs \\\n  --bind work/:/usr/local/spark/work \\\n  halvade.sif halvade-single\n\n  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e start spark inside the started image\u003c/span\u003e\n  singularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e instance://halvade-single /usr/local/spark/sbin/start-all.sh\n\n  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e if pwless ssh doesn\u0027t work, you can specify the used key in the `bashrc` file in the same directory as your halvade.sif file by adding this line:\u003c/span\u003e\n  SPARK_SSH_OPTS=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$SPARK_SSH_OPTS\u003c/span\u003e -i /home/username/.ssh/pwless_rsa\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-extra-variables-to-use-in-script\" class=\"anchor\" href=\"#extra-variables-to-use-in-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eextra variables to use in script:\u003c/h1\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eHALVADE_HOME\nHALVADE_OPTS=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e-option0 value --option1 value -option2\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nRESOURCES \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e should have all these options: --driver-memory ${DRIVER_MEM} --executor-memory ${EXECUTOR_MEMORY} --executor-cores ${EXECUTOR_CORES} --conf spark.task.cpus=${TASK_CPUS} --conf spark.executor.memoryOverhead=${OVERHEAD_MEMORY}\u003c/span\u003e\n\nSPARK_SSH_OPTS \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e can add -i here if pwless_rsa is not found\u003c/span\u003e\nHADOOP_SSH_OPTS\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install-locally-without-the-singularity-image\" class=\"anchor\" href=\"#install-locally-without-the-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall locally without the singularity image:\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-start-spark-without-yarn\" class=\"anchor\" href=\"#start-spark-without-yarn\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003estart Spark without YARN\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e use default settings -\u0026gt; all cpus/memory in spark\u003c/span\u003e\nstart-all.sh\nstop-all.sh\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003eTo set memory/CPU per node:\u003c/span\u003e\nstart-master.sh\nstart-slave.sh spark://node001:7077 -c 6 -m 50 \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e on every worker node\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e stop\u003c/span\u003e\nstop-slave.sh \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e on every worker node\u003c/span\u003e\nstop-master.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003espark-submit\u003c/code\u003e will need \u003ccode\u003e--master\u003c/code\u003e option to be set to \u003ccode\u003espark://master:7077\u003c/code\u003e to use the standalone Spark.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619643326.0
  },
  {
    "data_format": 2,
    "description": "Test using singularityhub",
    "filenames": [
      "Singularity",
      "Singularity.basic",
      "Singularity.centostest"
    ],
    "full_name": "nbarlowATI/shub-test",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-shub-test\" class=\"anchor\" href=\"#shub-test\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eshub-test\u003c/h1\u003e\n\u003cp\u003eTest using singularityhub\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617913070.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "M30819-2020/setapDocker",
    "latest_release": "latest",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-setapdocker\" class=\"anchor\" href=\"#setapdocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esetapDocker\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621993189.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "mdellalma/cellrangerv6.0.0",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cellranger--v600\" class=\"anchor\" href=\"#cellranger--v600\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCellRanger  (v6.0.0)\u003c/h1\u003e\n\u003cp\u003eContainer CellRanger 10X v6.0.0\u003c/p\u003e\n\u003cp\u003eThe download files for the last vesion of CellRanger need to be updated but the recipe stays the same.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617230630.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "parcuri0/singularity-cuda-spack",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-cuda-spack\" class=\"anchor\" href=\"#singularity-cuda-spack\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-cuda-spack\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617166672.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "csorianot/snATACseq-NextFlow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nextflow-pipeline-for-10x-snatac-seq-data\" class=\"anchor\" href=\"#nextflow-pipeline-for-10x-snatac-seq-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextFlow pipeline for 10X snATAC-seq data\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eIf you have Singularity installed, you can use the config provided here (\u0027Singularity\u0027) to build a container with all the dependencies.\u003c/p\u003e\n\u003cp\u003eOtherwise, you\u0027ll need to have the following installed:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ebiopython\u003c/li\u003e\n\u003cli\u003ebwa\u003c/li\u003e\n\u003cli\u003epicardtools\u003c/li\u003e\n\u003cli\u003efastqc\u003c/li\u003e\n\u003cli\u003esamtools\u003c/li\u003e\n\u003cli\u003epysam\u003c/li\u003e\n\u003cli\u003eataqv\u003c/li\u003e\n\u003cli\u003ecta (the forked version on the porchard GitHub)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI\u0027ve used this pipeline with NextFlow v. 19.04.1\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003ePaths to various generic files (e.g., bwa indices) must be included in the nextflow.config file -- check that file and change paths accordingly. These include:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBlacklist bed files for each genome\u003c/li\u003e\n\u003cli\u003eChrom size files for each genome\u003c/li\u003e\n\u003cli\u003eBWA indices\u003c/li\u003e\n\u003cli\u003eTSS files (BED6 files denoting TSS positions)\u003c/li\u003e\n\u003cli\u003eGene bed files (BED4 files; included because we get per-gene read counts for use with LIGER in downstream processing). You probably want these to represent gene bodies + promoters if you plan to use these with LIGER.\u003c/li\u003e\n\u003cli\u003ePath to the barcode whitelist (the 10X whitelist is included in this repo)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou\u0027ll also need to set the params.results variable -- either in the nextflow.config file itself, or on the command line when you run the pipeline (\u0027--results /path/to/results\u0027).\u003c/p\u003e\n\u003cp\u003eTo reduce memory usage of ataqv, we filter out nuclei with low read counts before running ataqv. The minimum read threshold is set in the nextflow.config file.\u003c/p\u003e\n\u003cp\u003eLastly, you\u0027ll need to include information about each ATAC-seq library, including the genome(s) for the species that each library includes, and the paths to the fastq files for each readgroup. Organize this information in a JSON file, as in library-config.json. Note that for each readgroup, three fastq files are required -- the first and second insert reads (\u00271\u0027 and \u00272\u0027), and the read with the nuclear barcode (\u0027index\u0027)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running\" class=\"anchor\" href=\"#running\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning\u003c/h2\u003e\n\u003cp\u003eOnce you have all of the above information, you can run the pipeline as follows (in this case, indicating the path to the results on the command line):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run -with-singularity /path/to/Singularity.simg -params-file library-config.json --results /path/to/results /path/to/main.nf\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619189918.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "touala/bonito",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nextflow-pipeline-for-10x-snatac-seq-data\" class=\"anchor\" href=\"#nextflow-pipeline-for-10x-snatac-seq-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextFlow pipeline for 10X snATAC-seq data\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eIf you have Singularity installed, you can use the config provided here (\u0027Singularity\u0027) to build a container with all the dependencies.\u003c/p\u003e\n\u003cp\u003eOtherwise, you\u0027ll need to have the following installed:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ebiopython\u003c/li\u003e\n\u003cli\u003ebwa\u003c/li\u003e\n\u003cli\u003epicardtools\u003c/li\u003e\n\u003cli\u003efastqc\u003c/li\u003e\n\u003cli\u003esamtools\u003c/li\u003e\n\u003cli\u003epysam\u003c/li\u003e\n\u003cli\u003eataqv\u003c/li\u003e\n\u003cli\u003ecta (the forked version on the porchard GitHub)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI\u0027ve used this pipeline with NextFlow v. 19.04.1\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003ePaths to various generic files (e.g., bwa indices) must be included in the nextflow.config file -- check that file and change paths accordingly. These include:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBlacklist bed files for each genome\u003c/li\u003e\n\u003cli\u003eChrom size files for each genome\u003c/li\u003e\n\u003cli\u003eBWA indices\u003c/li\u003e\n\u003cli\u003eTSS files (BED6 files denoting TSS positions)\u003c/li\u003e\n\u003cli\u003eGene bed files (BED4 files; included because we get per-gene read counts for use with LIGER in downstream processing). You probably want these to represent gene bodies + promoters if you plan to use these with LIGER.\u003c/li\u003e\n\u003cli\u003ePath to the barcode whitelist (the 10X whitelist is included in this repo)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou\u0027ll also need to set the params.results variable -- either in the nextflow.config file itself, or on the command line when you run the pipeline (\u0027--results /path/to/results\u0027).\u003c/p\u003e\n\u003cp\u003eTo reduce memory usage of ataqv, we filter out nuclei with low read counts before running ataqv. The minimum read threshold is set in the nextflow.config file.\u003c/p\u003e\n\u003cp\u003eLastly, you\u0027ll need to include information about each ATAC-seq library, including the genome(s) for the species that each library includes, and the paths to the fastq files for each readgroup. Organize this information in a JSON file, as in library-config.json. Note that for each readgroup, three fastq files are required -- the first and second insert reads (\u00271\u0027 and \u00272\u0027), and the read with the nuclear barcode (\u0027index\u0027)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running\" class=\"anchor\" href=\"#running\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning\u003c/h2\u003e\n\u003cp\u003eOnce you have all of the above information, you can run the pipeline as follows (in this case, indicating the path to the results on the command line):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run -with-singularity /path/to/Singularity.simg -params-file library-config.json --results /path/to/results /path/to/main.nf\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611292894.0
  },
  {
    "data_format": 2,
    "description": "Scripts supporting pagoo\u0027s publication",
    "filenames": [
      "Singularity"
    ],
    "full_name": "iferres/pagoo_publication_scripts",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/5123\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pagoo---publication-scripts\" class=\"anchor\" href=\"#pagoo---publication-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePagoo - Publication Scripts\u003c/h1\u003e\n\u003cp\u003eScripts supporting pagoo\u0027s publication.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis repo contains 2 scripts to reproduce the analyses described in the pagoo manuscript. \u003ccode\u003etiming_benchmark.R\u003c/code\u003e runs roary and evaluates timings over a set of pagoo\u0027s operations. \u003ccode\u003eCfetus_pangenome_example.R\u003c/code\u003e downloads a Campylobacter fetus dataset and uses pagoo along with other R packages to perform a series of analyses.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-data\" class=\"anchor\" href=\"#data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData\u003c/h2\u003e\n\u003cp\u003eData required by \u003ccode\u003etiming_benchmark.R\u003c/code\u003e is hosted at: \u003ca href=\"https://zenodo.org/record/3341535#.YA7_8ZqvHJE\" rel=\"nofollow\"\u003ehttps://zenodo.org/record/3341535#.YA7_8ZqvHJE\u003c/a\u003e (Publication: \u003ca href=\"https://www.nature.com/articles/s41598-019-54004-5\" rel=\"nofollow\"\u003eDecano \u0026amp; Downing, 2019\u003c/a\u003e, dataset doi:10.5281/zenodo.3341534). The script automatically downloads and decompress it in the working directory.\u003c/p\u003e\n\u003cp\u003eData required by \u003ccode\u003eCfetus_pangenome_example.R\u003c/code\u003e is hosted at: \u003ca href=\"https://figshare.com/articles/dataset/Campylobacter_fetus_genomes_and_pangenome_for_pagoo_demo/13622354\" rel=\"nofollow\"\u003ehttps://figshare.com/articles/dataset/Campylobacter_fetus_genomes_and_pangenome_for_pagoo_demo/13622354\u003c/a\u003e . The script automatically downloads and decompress it in the working directory.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container\u003c/h2\u003e\n\u003cp\u003eThis repo also contains a Singularity file to build a Singularity image with all dependencies needed to run the scripts.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h3\u003e\n\u003cp\u003eTo manually build the container:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/iferres/pagoo_publication_scripts\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e ./pagoo_publication_scripts\nsudo singularity build pagoo_publicaction_container.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pull\" class=\"anchor\" href=\"#pull\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePull\u003c/h3\u003e\n\u003cp\u003eTo pull the prebuilt container hosted at singularity-hub:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull pagoo_publication_container.sif shub://iferres/pagoo_publication_scripts\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h3\u003e\n\u003cp\u003eTo run the C. fetus script, all at once:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e The following expects the script (.R) and the container (.sif) \u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e in the current working directory.\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e pagoo_publication_container.sif Rscript --vanilla Cfetus_pangenome_example.R\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you want to play more freely with the package, you can shell into the container, start an interactive R session, and load pagoo:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Start container\u003c/span\u003e\nsingularity shell pagoo_publication_container.sif\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Start R\u003c/span\u003e\nR\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Inside R, load pagoo\u003c/span\u003e\nlibrary(\u003cspan class=\"pl-smi\"\u003epagoo\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-known-limitations\" class=\"anchor\" href=\"#known-limitations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown limitations\u003c/h4\u003e\n\u003cp\u003eShiny app doesn\u0027t work from within the container, although it is not needed by the scripts.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620083275.0
  },
  {
    "data_format": 2,
    "description": "Predict blood\u2013brain barrier (BBB) permeability of a compound",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "martinobertoni/BBBpredictor",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/5286\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-blood-brain-barrier-bbb-predictor\" class=\"anchor\" href=\"#blood-brain-barrier-bbb-predictor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBlood-brain barrier (BBB) predictor:\u003c/h1\u003e\n\u003cp\u003eClassifier that will predict if a chemical coumpound will pass the blood-brain barrier.\nSBNB lab (IRB Barcelona) - Nov 2020.\u003c/p\u003e\n\u003cp\u003eUSAGE: BBBpredictor myfile.tsv [SMILES or INCHI]\u003c/p\u003e\n\u003cp\u003eWhere myfile.tsv is an input text file that contains two columns separated by TABS.\nCOLUMN 1: compound unique id (ex:1 or a molecule name without tab characters inside).\nCOLUMN 2: compound SMILES string.\u003c/p\u003e\n\u003cp\u003eNOTE: The default format is SMILES and can be set to InChI if the second argument INCHI is provided.\nNOTE: Lines starting with \u0027#\u0027 as well as empty lines are ignored.\u003c/p\u003e\n\u003cp\u003eOUTPUT: tsv file containing the prediction for each compound.\u003c/p\u003e\n\u003cp\u003ePREDICTION LEGEND: the molecule pass the BBB?\u003c/p\u003e\n\u003cp\u003e0: no\u003c/p\u003e\n\u003cp\u003e1: yes\u003c/p\u003e\n\u003cp\u003e-1: the molecular signature could not be calculated for this coumpound\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1616100804.0
  },
  {
    "data_format": 2,
    "description": "Recipes for singularity images supported on Mistral.",
    "filenames": [
      "jupyterhub/Singularity.jupyterhub",
      "jupyter/Singularity.jupyter",
      "ml/Singularity.ml"
    ],
    "full_name": "SofianeB/singularity-recipes",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4819\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to use?\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003emodule load singularity \n\nsingularity pull shub://statiksof/singularity-recipes:TAG\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen, you can \u003ccode\u003eshell\u003c/code\u003e or \u003ccode\u003erun\u003c/code\u003e the container.\u003c/p\u003e\n\u003cp\u003esee \u003ca href=\"https://singularity-hub.org/collections/4819\" rel=\"nofollow\"\u003ehere\u003c/a\u003e for all available tags.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity-image",
      "hpc",
      "containers"
    ],
    "updated_at": 1619185671.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "J35P312/test_singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-test_singularity\" class=\"anchor\" href=\"#test_singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etest_singularity\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611094520.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "seedpcseed/metaprokka",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-metaprokka\" class=\"anchor\" href=\"#metaprokka\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emetaprokka\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611092582.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.hpc"
    ],
    "full_name": "hqhv/oneapi",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-metaprokka\" class=\"anchor\" href=\"#metaprokka\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emetaprokka\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611091491.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "seedpcseed/fastp",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fastp\" class=\"anchor\" href=\"#fastp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efastp\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611092621.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "seedpcseed/NGS-filtering",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ngs-filtering\" class=\"anchor\" href=\"#ngs-filtering\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNGS-filtering\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611092715.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "seedpcseed/NGS-Assembly",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ngs-assembly\" class=\"anchor\" href=\"#ngs-assembly\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNGS-Assembly\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611092680.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "fabianrost84/singularity-action-test",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ngs-assembly\" class=\"anchor\" href=\"#ngs-assembly\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNGS-Assembly\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611018483.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "smrtin/TEnriAn",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tenrian\" class=\"anchor\" href=\"#tenrian\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTEnriAn\u003c/h1\u003e\n\u003cp\u003eTEnriAn (Target ENrichment ANalysis) pipeline\u003c/p\u003e\n\u003cp\u003erecipe file for a singularity container build\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611007887.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "jesus-gorronogoitia/easy",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-easy\" class=\"anchor\" href=\"#easy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eeasy\u003c/h1\u003e\n\u003cp\u003eExample of Singularity container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1615937293.0
  },
  {
    "data_format": 2,
    "description": "Singularity Recipe for HPC",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jintonic/sing",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/5087\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-sing\" class=\"anchor\" href=\"#sing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esing\u003c/h1\u003e\n\u003cp\u003eSingularity Recipe for HPC\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1610801957.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity",
      "container/Singularity_cnvkit95"
    ],
    "full_name": "Clinical-Genomics-Lund/SomaticPanelPipeline",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/5087\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-sing\" class=\"anchor\" href=\"#sing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esing\u003c/h1\u003e\n\u003cp\u003eSingularity Recipe for HPC\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1618430747.0
  },
  {
    "data_format": 2,
    "description": "This repository should contain all the works related to HPC",
    "filenames": [
      "singularity/lolcow/Singularity"
    ],
    "full_name": "dagonzalezfo/HPCPlayground",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpcplayground\" class=\"anchor\" href=\"#hpcplayground\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPCPlayground\u003c/h1\u003e\n\u003cp\u003eThis repository should contain all the works related to HPC\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1610561146.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/Singularity.dev",
      "containers/Singularity.0.0.3",
      "containers/Singularity.0.0.4",
      "containers/Singularity.0.0.2",
      "containers/Singularity.0.0.1"
    ],
    "full_name": "Samanwaya1301/ET_CE_bilby_pipe",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpcplayground\" class=\"anchor\" href=\"#hpcplayground\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPCPlayground\u003c/h1\u003e\n\u003cp\u003eThis repository should contain all the works related to HPC\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1610562729.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "photocyte/tmsu_singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/5075\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\nSingularity file for \u003ca href=\"https://tmsu.org\" rel=\"nofollow\"\u003etmsu\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eImage building handled by \u003ca href=\"https://singularity-hub.org\" rel=\"nofollow\"\u003esingularity-hub.org\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://photocyte/tmsu_singularity\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1610252258.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ariclenesGBSN/data-labeling-tool-reactjs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"#universal-data-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUniversal Data Tool\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\" alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\" alt=\"Master Branch\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://badge.fury.io/js/universal-data-tool\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\" alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\" alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\" alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://twitter.com/UniversalDataTl\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\" alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTry it out at \u003ca href=\"https://udt.dev\" rel=\"nofollow\"\u003eudt.dev\u003c/a\u003e, \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003edownload the desktop app\u003c/a\u003e or \u003ca href=\"https://docs.universaldatatool.com/running-on-premise\" rel=\"nofollow\"\u003erun on-premise\u003c/a\u003e.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cb\u003e\n  \u003ca href=\"https://docs.universaldatatool.com\" rel=\"nofollow\"\u003eDocs\u003c/a\u003e \u2022 \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003eWebsite\u003c/a\u003e \u2022 \u003ca href=\"https://udt.dev\" rel=\"nofollow\"\u003ePlayground\u003c/a\u003e \u2022 \u003ca href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\" rel=\"nofollow\"\u003eLibrary Usage\u003c/a\u003e \u2022 \u003ca href=\"https://docs.universaldatatool.com/running-on-premise\" rel=\"nofollow\"\u003eOn-Premise\u003c/a\u003e\n  \u003c/b\u003e\n\u003c/p\u003e\n\u003cp\u003eThe Universal Data Tool is a web/desktop app for editing and annotating images, text, audio, documents and to view and edit any data defined in the extensible \u003ca href=\"https://github.com/UniversalDataTool/udt-format\"\u003e.udt.json and .udt.csv standard\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-supported-data\" class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupported Data\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\" rel=\"nofollow\"\u003eImage Segmentation\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\" rel=\"nofollow\"\u003eImage Classification\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\" rel=\"nofollow\"\u003eText Classification\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\" rel=\"nofollow\"\u003eNamed Entity Recognition\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\" rel=\"nofollow\"\u003eNamed Entity Relations / Part of Speech Tagging\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\" rel=\"nofollow\"\u003eAudio Transcription\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\" rel=\"nofollow\"\u003eData Entry\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\" rel=\"nofollow\"\u003eVideo Segmentation\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\" rel=\"nofollow\"\u003eLandmark / Pose Annotation\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-recent-updates\" class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRecent Updates\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\"\u003eFollow our development on Youtube!\u003c/a\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/q20WrCRcG4k\" rel=\"nofollow\"\u003eCommunity Update Video 9\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\" rel=\"nofollow\"\u003eCommunity Update Video 8\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://youtu.be/glPPFgXibdw\" rel=\"nofollow\"\u003eCommunity Update Video 7\u003c/a\u003e \u003ca href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\" rel=\"nofollow\"\u003e(blog version)\u003c/a\u003e\n\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCollaborate with others in real time, no sign up!\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eUsable on \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003eweb\u003c/a\u003e or as \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\u003eWindows,Mac or Linux desktop application\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eConfigure your project with an easy-to-use GUI\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://universaldatatool.com/courses\" rel=\"nofollow\"\u003eEasily create courses to train your labelers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDownload/upload as easy-to-use CSV (\u003ca href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\u003esample.udt.csv\u003c/a\u003e) or JSON (\u003ca href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\u003esample.udt.json\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSupport for Images, Videos, PDFs, Text, Audio Transcription and many other formats\u003c/li\u003e\n\u003cli\u003eCan be \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\u003eeasily integrated into a React application\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eAnnotate images or videos with classifications, tags, bounding boxes, polygons and points\u003c/li\u003e\n\u003cli\u003eFast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly\u003c/li\u003e\n\u003cli\u003eImport data from Google Drive, Youtube, CSV, Clipboard and more\u003c/li\u003e\n\u003cli\u003eAnnotate NLP datasets with Named Entity Recognition (NER), classification and Part of Speech (PoS) tagging.\u003c/li\u003e\n\u003cli\u003eEasily \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\u003eload into pandas\u003c/a\u003e or \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\u003euse with fast.ai\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRuns \u003ca href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\" rel=\"nofollow\"\u003ewith docker\u003c/a\u003e \u003ccode\u003edocker run -p 3000:3000 universaldatatool/universaldatatool\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRuns \u003ca href=\"https://singularity-hub.org/collections/4792\" rel=\"nofollow\"\u003ewith singularity\u003c/a\u003e \u003ccode\u003esingularity run universaldatatool/universaldatatool\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sponsors\" class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSponsors\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://wao.ai\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\" alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://momentum-tech.ca/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\" alt=\"momentum image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.enabledintelligence.net/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\" alt=\"enabled intelligence image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWeb App\u003c/h3\u003e\n\u003cp\u003eJust visit \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003euniversaldatatool.com\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eTrying to run the web app locally? Run \u003ccode\u003enpm install\u003c/code\u003e then \u003ccode\u003enpm run start\u003c/code\u003e after cloning this repository to start the web server.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-desktop-application\" class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDesktop Application\u003c/h3\u003e\n\u003cp\u003eDownload the latest release from the \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003ereleases page\u003c/a\u003e and run the executable you downloaded.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e(Optional) Say hi in the \u003ca href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\" rel=\"nofollow\"\u003eSlack channel\u003c/a\u003e!\u003c/li\u003e\n\u003cli\u003eRead \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\u003ethis guide to get started with development\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors-\" class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors \u003cg-emoji class=\"g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\u003e\u2728\u003c/g-emoji\u003e\n\u003c/h2\u003e\n\u003cp\u003eThanks goes to these wonderful people (\u003ca href=\"https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\"\u003eemoji key\u003c/a\u003e):\u003c/p\u003e\n\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://twitter.com/seveibar\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eSeverin Ibarluzea\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\" title=\"Reviewed Pull Requests\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\"\u003e\ud83d\udc40\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"http://puskuruk.github.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ePuskuruk\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\" title=\"Reviewed Pull Requests\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\"\u003e\ud83d\udc40\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/CedricJean\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eCedricJean\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"http://berupon.hatenablog.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eberu\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/Ownmarc\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMarc\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/Wafaa-arbash\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eWafaa-arbash\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/pgrimaud\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ePierre Grimaud\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/sreevardhanreddi\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003esreevardhanreddi\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/mrdadah\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMohammed Eldadah\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://x8795278.blogspot.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ex213212\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/hysios\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ehysios \u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://congdv.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eCong Dao\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eRenato Junior\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-MrJunato\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://gitlab.com/rickstaa\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eRick\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-rickstaa\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/anaplian\"\u003e\u003cimg src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eanaplian\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://www.behance.net/MiguelCarvalho13\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMiguel Carvalho\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-miguelcarvalho13\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://kyleo.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eKyle OBrien\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/hakkiyagiz\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eHakk\u0131 Ya\u011f\u0131z ERD\u0130N\u00c7\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/jvdavim\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eJo\u00e3o Victor Davim\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\n\n\u003cp\u003eThis project follows the \u003ca href=\"https://github.com/all-contributors/all-contributors\"\u003eall-contributors\u003c/a\u003e specification. Contributions of any kind welcome!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1615266457.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/Singularity.0.3.6",
      "containers/Singularity.0.4.1",
      "containers/Singularity.0.3.5",
      "containers/Singularity.0.3.3",
      "containers/Singularity.0.4.0"
    ],
    "full_name": "Samanwaya1301/bilby-BHNS",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"#universal-data-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUniversal Data Tool\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\" alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\" alt=\"Master Branch\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://badge.fury.io/js/universal-data-tool\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\" alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\" alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\" alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://twitter.com/UniversalDataTl\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\" alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTry it out at \u003ca href=\"https://udt.dev\" rel=\"nofollow\"\u003eudt.dev\u003c/a\u003e, \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003edownload the desktop app\u003c/a\u003e or \u003ca href=\"https://docs.universaldatatool.com/running-on-premise\" rel=\"nofollow\"\u003erun on-premise\u003c/a\u003e.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cb\u003e\n  \u003ca href=\"https://docs.universaldatatool.com\" rel=\"nofollow\"\u003eDocs\u003c/a\u003e \u2022 \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003eWebsite\u003c/a\u003e \u2022 \u003ca href=\"https://udt.dev\" rel=\"nofollow\"\u003ePlayground\u003c/a\u003e \u2022 \u003ca href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\" rel=\"nofollow\"\u003eLibrary Usage\u003c/a\u003e \u2022 \u003ca href=\"https://docs.universaldatatool.com/running-on-premise\" rel=\"nofollow\"\u003eOn-Premise\u003c/a\u003e\n  \u003c/b\u003e\n\u003c/p\u003e\n\u003cp\u003eThe Universal Data Tool is a web/desktop app for editing and annotating images, text, audio, documents and to view and edit any data defined in the extensible \u003ca href=\"https://github.com/UniversalDataTool/udt-format\"\u003e.udt.json and .udt.csv standard\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-supported-data\" class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupported Data\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\" rel=\"nofollow\"\u003eImage Segmentation\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\" rel=\"nofollow\"\u003eImage Classification\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\" rel=\"nofollow\"\u003eText Classification\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\" rel=\"nofollow\"\u003eNamed Entity Recognition\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\" rel=\"nofollow\"\u003eNamed Entity Relations / Part of Speech Tagging\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\" rel=\"nofollow\"\u003eAudio Transcription\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\" rel=\"nofollow\"\u003eData Entry\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\" rel=\"nofollow\"\u003eVideo Segmentation\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\" rel=\"nofollow\"\u003eLandmark / Pose Annotation\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-recent-updates\" class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRecent Updates\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\"\u003eFollow our development on Youtube!\u003c/a\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/q20WrCRcG4k\" rel=\"nofollow\"\u003eCommunity Update Video 9\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\" rel=\"nofollow\"\u003eCommunity Update Video 8\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://youtu.be/glPPFgXibdw\" rel=\"nofollow\"\u003eCommunity Update Video 7\u003c/a\u003e \u003ca href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\" rel=\"nofollow\"\u003e(blog version)\u003c/a\u003e\n\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCollaborate with others in real time, no sign up!\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eUsable on \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003eweb\u003c/a\u003e or as \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\u003eWindows,Mac or Linux desktop application\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eConfigure your project with an easy-to-use GUI\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://universaldatatool.com/courses\" rel=\"nofollow\"\u003eEasily create courses to train your labelers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDownload/upload as easy-to-use CSV (\u003ca href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\u003esample.udt.csv\u003c/a\u003e) or JSON (\u003ca href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\u003esample.udt.json\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSupport for Images, Videos, PDFs, Text, Audio Transcription and many other formats\u003c/li\u003e\n\u003cli\u003eCan be \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\u003eeasily integrated into a React application\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eAnnotate images or videos with classifications, tags, bounding boxes, polygons and points\u003c/li\u003e\n\u003cli\u003eFast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly\u003c/li\u003e\n\u003cli\u003eImport data from Google Drive, Youtube, CSV, Clipboard and more\u003c/li\u003e\n\u003cli\u003eAnnotate NLP datasets with Named Entity Recognition (NER), classification and Part of Speech (PoS) tagging.\u003c/li\u003e\n\u003cli\u003eEasily \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\u003eload into pandas\u003c/a\u003e or \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\u003euse with fast.ai\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRuns \u003ca href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\" rel=\"nofollow\"\u003ewith docker\u003c/a\u003e \u003ccode\u003edocker run -p 3000:3000 universaldatatool/universaldatatool\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRuns \u003ca href=\"https://singularity-hub.org/collections/4792\" rel=\"nofollow\"\u003ewith singularity\u003c/a\u003e \u003ccode\u003esingularity run universaldatatool/universaldatatool\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sponsors\" class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSponsors\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://wao.ai\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\" alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://momentum-tech.ca/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\" alt=\"momentum image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.enabledintelligence.net/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\" alt=\"enabled intelligence image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWeb App\u003c/h3\u003e\n\u003cp\u003eJust visit \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003euniversaldatatool.com\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eTrying to run the web app locally? Run \u003ccode\u003enpm install\u003c/code\u003e then \u003ccode\u003enpm run start\u003c/code\u003e after cloning this repository to start the web server.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-desktop-application\" class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDesktop Application\u003c/h3\u003e\n\u003cp\u003eDownload the latest release from the \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003ereleases page\u003c/a\u003e and run the executable you downloaded.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e(Optional) Say hi in the \u003ca href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\" rel=\"nofollow\"\u003eSlack channel\u003c/a\u003e!\u003c/li\u003e\n\u003cli\u003eRead \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\u003ethis guide to get started with development\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors-\" class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors \u003cg-emoji class=\"g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\u003e\u2728\u003c/g-emoji\u003e\n\u003c/h2\u003e\n\u003cp\u003eThanks goes to these wonderful people (\u003ca href=\"https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\"\u003eemoji key\u003c/a\u003e):\u003c/p\u003e\n\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://twitter.com/seveibar\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eSeverin Ibarluzea\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\" title=\"Reviewed Pull Requests\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\"\u003e\ud83d\udc40\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"http://puskuruk.github.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ePuskuruk\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\" title=\"Reviewed Pull Requests\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\"\u003e\ud83d\udc40\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/CedricJean\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eCedricJean\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"http://berupon.hatenablog.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eberu\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/Ownmarc\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMarc\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/Wafaa-arbash\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eWafaa-arbash\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/pgrimaud\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ePierre Grimaud\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/sreevardhanreddi\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003esreevardhanreddi\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/mrdadah\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMohammed Eldadah\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://x8795278.blogspot.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ex213212\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/hysios\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ehysios \u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://congdv.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eCong Dao\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eRenato Junior\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-MrJunato\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://gitlab.com/rickstaa\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eRick\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-rickstaa\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/anaplian\"\u003e\u003cimg src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eanaplian\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://www.behance.net/MiguelCarvalho13\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMiguel Carvalho\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-miguelcarvalho13\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://kyleo.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eKyle OBrien\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/hakkiyagiz\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eHakk\u0131 Ya\u011f\u0131z ERD\u0130N\u00c7\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/jvdavim\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eJo\u00e3o Victor Davim\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\n\n\u003cp\u003eThis project follows the \u003ca href=\"https://github.com/all-contributors/all-contributors\"\u003eall-contributors\u003c/a\u003e specification. Contributions of any kind welcome!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1615492416.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.itermae",
      "Singularity.latest",
      "Singularity.test_base",
      "Singularity.test",
      "Singularity.itermae-plus"
    ],
    "full_name": "darachm/itermae",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-itermae\" class=\"anchor\" href=\"#itermae\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eitermae\u003c/h1\u003e\n\u003cp\u003eSee the \u003ca href=\"https://darachm.gitlab.io/itermae/concept.html\" rel=\"nofollow\"\u003econcept here\u003c/a\u003e and\n\u003ca href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\" rel=\"nofollow\"\u003etutorial here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eitermae\u003c/code\u003e is a command-line utility to recognize patterns in input sequences\nand generate outputs from groups recognized. Basically, it uses fuzzy regular\nexpression operations to (primarily) DNA sequence for purposes of DNA\nbarcode/tag/UMI parsing, sequence and quality -based filtering,\nand general output re-arrangment.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\" alt=\"itermae diagram\" data-canonical-src=\"https://darachm.gitlab.io/itermae/_images/parse_diagram_1.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eitermae\u003c/code\u003e reads and makes FASTQ, FASTA, text-file, and SAM (tab-delimited)\nfiles using \u003ca href=\"https://pypi.org/project/biopython/\" rel=\"nofollow\"\u003e\u003ccode\u003eBiopython\u003c/code\u003e\u003c/a\u003e sequence records\nto represent slice, and read/output formats.\nPattern matching uses the \u003ca href=\"https://pypi.org/project/regex/\" rel=\"nofollow\"\u003e\u003ccode\u003eregex\u003c/code\u003e\u003c/a\u003e library,\nand the tool is designed to function in command-line pipes from tools like\n\u003ca href=\"https://www.gnu.org/software/parallel/\" rel=\"nofollow\"\u003eGNU \u003ccode\u003eparallel\u003c/code\u003e\u003c/a\u003e\nto permit light-weight parallelization.\u003c/p\u003e\n\u003cp\u003eIt\u0027s usage might look something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ezcat seq_data.fastqz | itermae --config my_config.yml -v \u0026gt; output.sam\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ezcat seq_data.fastqz \\\n    | parallel --quote --pipe -l 4 --keep-order -N 10000 \\\n        itermae --config my_config.yml -v \u0026gt; output.sam\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewith a \u003ccode\u003emy_config.yml\u003c/code\u003e file that may look something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ematches:\n    - use: input\n      pattern: NNNNNGTCCTCGAGGTCTCTNNNNNNNNNNNNNNNNNNNNCGTACGCTGCAGGTC\n      marking: aaaaaBBBBBBBBBBBBBBBccccccccccccccccccccDDDDDDDDDDDDDDD\n      marked_groups:\n          a:\n              name: sampleIndex\n              repeat: 5\n          B:\n              allowed_errors: 2\n          c:\n              name: barcode\n              repeat_min: 18\n              repeat_max: 22\n          D:\n              allowed_insertions: 1\n              allowed_deletions: 2\n              allowed_substititions: 2\noutput_list:\n    -   name: \u0027barcode\u0027\n        description: \u0027description+\" sample=\"+sampleIndex\u0027\n        seq: \u0027barcode\u0027\n        filter: \u0027statistics.median(barcode.quality) \u0026gt;= 35\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-availability-installation-installation\" class=\"anchor\" href=\"#availability-installation-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailability, installation, \u0027installation\u0027\u003c/h1\u003e\n\u003cp\u003eOptions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUse pip to install \u003ccode\u003eitermae\u003c/code\u003e, so\u003c/p\u003e\n\u003cp\u003epython3 -m pip install itermae\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can clone this repo, and install it locally. Dependencies are in\n\u003ccode\u003erequirements.txt\u003c/code\u003e, so\n\u003ccode\u003epython3 -m pip install -r requirements.txt\u003c/code\u003e will install those.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can use \u003ca href=\"https://syslab.org\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e to pull and run a\n\u003ca href=\"https://singularity-hub.org/collections/4537\" rel=\"nofollow\"\u003eSingularity image of itermae.py\u003c/a\u003e,\nwhere everything is already installed.\nThis is the recommended usage.\u003c/p\u003e\n\u003cp\u003eThis image is built with a few other tools,\nlike g/mawk, perl, and parallel, to make command line munging easier.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003eitermae\u003c/code\u003e is envisioned to be used in a pipe-line where you just got your\nDNA sequencing FASTQ reads back, and you want to parse them.\nThe recommended interface is the YAML config file, as demonstrated\nin \u003ca href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\" rel=\"nofollow\"\u003ethe tutorial\u003c/a\u003e\nand detailed again in the\n\u003ca href=\"https://darachm.gitlab.io/itermae/usage/config.html\" rel=\"nofollow\"\u003econfiguration details\u003c/a\u003e.\nYou can also use a command-line argument interface as detailed more\n\u003ca href=\"https://darachm.gitlab.io/itermae/usage/examples.html\" rel=\"nofollow\"\u003ein the examples\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI recommend you test this on small batches of data,\nthen stick it behind GNU \u003ccode\u003eparallel\u003c/code\u003e and feed the whole FASTQ file via\n\u003ccode\u003ezcat\u003c/code\u003e in on standard input.\nThis parallelizes with a small memory footprint, then\nyou write it out to disk (or stream into another tool).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks\u003c/h1\u003e\n\u003cp\u003eAgain, the tool is built upon on the excellent work of\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://pypi.org/project/regex/\" rel=\"nofollow\"\u003e\u003ccode\u003eregex\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pypi.org/project/biopython/\" rel=\"nofollow\"\u003e\u003ccode\u003eBiopython\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.gnu.org/software/parallel/\" rel=\"nofollow\"\u003e\u003ccode\u003eparallel\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-development-helping\" class=\"anchor\" href=\"#development-helping\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment, helping\u003c/h1\u003e\n\u003cp\u003eAny issues or advice are welcome as an\n\u003ca href=\"https://gitlab.com/darachm/itermae/-/issues\" rel=\"nofollow\"\u003eissue on the gitlab repo\u003c/a\u003e.\nComplaints are especially welcome.\u003c/p\u003e\n\u003cp\u003eFor development, see the\n\u003ca href=\"https://darachm.gitlab.io/itermae/package.html\" rel=\"nofollow\"\u003edocumentation as rendered from docstrings\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA set of tests is written up with \u003ccode\u003epytest\u003c/code\u003e module, and can be run from inside\nthe cloned repo with \u003ccode\u003emake test\u003c/code\u003e.\nSee \u003ccode\u003emake help\u003c/code\u003e for more options, such as building, installing, and uploading.\u003c/p\u003e\n\u003cp\u003eThere\u0027s also a bash script with some longer runs in\n\u003ccode\u003eprofiling_tests\u003c/code\u003e, these generate longer runs for profiling purposes\nwith \u003ccode\u003ecProfile\u003c/code\u003e and \u003ccode\u003esnakeviz\u003c/code\u003e.\nBut is out of date. Todo is to re-configure and retest that for speed.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619159484.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "okurman/chris_biowulf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-itermae\" class=\"anchor\" href=\"#itermae\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eitermae\u003c/h1\u003e\n\u003cp\u003eSee the \u003ca href=\"https://darachm.gitlab.io/itermae/concept.html\" rel=\"nofollow\"\u003econcept here\u003c/a\u003e and\n\u003ca href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\" rel=\"nofollow\"\u003etutorial here\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eitermae\u003c/code\u003e is a command-line utility to recognize patterns in input sequences\nand generate outputs from groups recognized. Basically, it uses fuzzy regular\nexpression operations to (primarily) DNA sequence for purposes of DNA\nbarcode/tag/UMI parsing, sequence and quality -based filtering,\nand general output re-arrangment.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2544a3064392c608c6654ebf968cd6bd4c57854711bb7f57b6de4092f4f81dde/68747470733a2f2f6461726163686d2e6769746c61622e696f2f697465726d61652f5f696d616765732f70617273655f6469616772616d5f312e737667\" alt=\"itermae diagram\" data-canonical-src=\"https://darachm.gitlab.io/itermae/_images/parse_diagram_1.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eitermae\u003c/code\u003e reads and makes FASTQ, FASTA, text-file, and SAM (tab-delimited)\nfiles using \u003ca href=\"https://pypi.org/project/biopython/\" rel=\"nofollow\"\u003e\u003ccode\u003eBiopython\u003c/code\u003e\u003c/a\u003e sequence records\nto represent slice, and read/output formats.\nPattern matching uses the \u003ca href=\"https://pypi.org/project/regex/\" rel=\"nofollow\"\u003e\u003ccode\u003eregex\u003c/code\u003e\u003c/a\u003e library,\nand the tool is designed to function in command-line pipes from tools like\n\u003ca href=\"https://www.gnu.org/software/parallel/\" rel=\"nofollow\"\u003eGNU \u003ccode\u003eparallel\u003c/code\u003e\u003c/a\u003e\nto permit light-weight parallelization.\u003c/p\u003e\n\u003cp\u003eIt\u0027s usage might look something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ezcat seq_data.fastqz | itermae --config my_config.yml -v \u0026gt; output.sam\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ezcat seq_data.fastqz \\\n    | parallel --quote --pipe -l 4 --keep-order -N 10000 \\\n        itermae --config my_config.yml -v \u0026gt; output.sam\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewith a \u003ccode\u003emy_config.yml\u003c/code\u003e file that may look something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ematches:\n    - use: input\n      pattern: NNNNNGTCCTCGAGGTCTCTNNNNNNNNNNNNNNNNNNNNCGTACGCTGCAGGTC\n      marking: aaaaaBBBBBBBBBBBBBBBccccccccccccccccccccDDDDDDDDDDDDDDD\n      marked_groups:\n          a:\n              name: sampleIndex\n              repeat: 5\n          B:\n              allowed_errors: 2\n          c:\n              name: barcode\n              repeat_min: 18\n              repeat_max: 22\n          D:\n              allowed_insertions: 1\n              allowed_deletions: 2\n              allowed_substititions: 2\noutput_list:\n    -   name: \u0027barcode\u0027\n        description: \u0027description+\" sample=\"+sampleIndex\u0027\n        seq: \u0027barcode\u0027\n        filter: \u0027statistics.median(barcode.quality) \u0026gt;= 35\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-availability-installation-installation\" class=\"anchor\" href=\"#availability-installation-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailability, installation, \u0027installation\u0027\u003c/h1\u003e\n\u003cp\u003eOptions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUse pip to install \u003ccode\u003eitermae\u003c/code\u003e, so\u003c/p\u003e\n\u003cp\u003epython3 -m pip install itermae\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can clone this repo, and install it locally. Dependencies are in\n\u003ccode\u003erequirements.txt\u003c/code\u003e, so\n\u003ccode\u003epython3 -m pip install -r requirements.txt\u003c/code\u003e will install those.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can use \u003ca href=\"https://syslab.org\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e to pull and run a\n\u003ca href=\"https://singularity-hub.org/collections/4537\" rel=\"nofollow\"\u003eSingularity image of itermae.py\u003c/a\u003e,\nwhere everything is already installed.\nThis is the recommended usage.\u003c/p\u003e\n\u003cp\u003eThis image is built with a few other tools,\nlike g/mawk, perl, and parallel, to make command line munging easier.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003eitermae\u003c/code\u003e is envisioned to be used in a pipe-line where you just got your\nDNA sequencing FASTQ reads back, and you want to parse them.\nThe recommended interface is the YAML config file, as demonstrated\nin \u003ca href=\"https://darachm.gitlab.io/itermae/usage/tutorial.html\" rel=\"nofollow\"\u003ethe tutorial\u003c/a\u003e\nand detailed again in the\n\u003ca href=\"https://darachm.gitlab.io/itermae/usage/config.html\" rel=\"nofollow\"\u003econfiguration details\u003c/a\u003e.\nYou can also use a command-line argument interface as detailed more\n\u003ca href=\"https://darachm.gitlab.io/itermae/usage/examples.html\" rel=\"nofollow\"\u003ein the examples\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI recommend you test this on small batches of data,\nthen stick it behind GNU \u003ccode\u003eparallel\u003c/code\u003e and feed the whole FASTQ file via\n\u003ccode\u003ezcat\u003c/code\u003e in on standard input.\nThis parallelizes with a small memory footprint, then\nyou write it out to disk (or stream into another tool).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks\u003c/h1\u003e\n\u003cp\u003eAgain, the tool is built upon on the excellent work of\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://pypi.org/project/regex/\" rel=\"nofollow\"\u003e\u003ccode\u003eregex\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pypi.org/project/biopython/\" rel=\"nofollow\"\u003e\u003ccode\u003eBiopython\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.gnu.org/software/parallel/\" rel=\"nofollow\"\u003e\u003ccode\u003eparallel\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-development-helping\" class=\"anchor\" href=\"#development-helping\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment, helping\u003c/h1\u003e\n\u003cp\u003eAny issues or advice are welcome as an\n\u003ca href=\"https://gitlab.com/darachm/itermae/-/issues\" rel=\"nofollow\"\u003eissue on the gitlab repo\u003c/a\u003e.\nComplaints are especially welcome.\u003c/p\u003e\n\u003cp\u003eFor development, see the\n\u003ca href=\"https://darachm.gitlab.io/itermae/package.html\" rel=\"nofollow\"\u003edocumentation as rendered from docstrings\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA set of tests is written up with \u003ccode\u003epytest\u003c/code\u003e module, and can be run from inside\nthe cloned repo with \u003ccode\u003emake test\u003c/code\u003e.\nSee \u003ccode\u003emake help\u003c/code\u003e for more options, such as building, installing, and uploading.\u003c/p\u003e\n\u003cp\u003eThere\u0027s also a bash script with some longer runs in\n\u003ccode\u003eprofiling_tests\u003c/code\u003e, these generate longer runs for profiling purposes\nwith \u003ccode\u003ecProfile\u003c/code\u003e and \u003ccode\u003esnakeviz\u003c/code\u003e.\nBut is out of date. Todo is to re-configure and retest that for speed.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1608702922.0
  },
  {
    "data_format": 2,
    "description": "metarepo for tidying up container recipes, currently Singularity",
    "filenames": [
      "tensorflow/Singularity.tensorflow-v2.4.0-rc4-compiled",
      "tensorflow/Singularity.tensorflow-v2.0.3-compiled",
      "tensorflow/Singularity.tensorflow-v1.15.4-compiled-partial",
      "tensorflow/Singularity.tensorflow-v2.2.0-compiled",
      "bioconda/Singularity.bioconda",
      "bioinfmunger/Singularity.bioinfmunger",
      "lh3-aligners/Singularity.lh3-aligners",
      "base/Singularity.base",
      "pacbio/Singularity.pacbio",
      "starcode/Singularity.starcode-v0.1.1",
      "jupyter/Singularity.jupyter-plus-tensorflow-v2.2.0-compiled",
      "jupyter/Singularity.jupyter-plus-alignparse",
      "jupyter/Singularity.jupyter",
      "jupyter/Singularity.jupyter-plus-bioconda",
      "jupyter/Singularity.jupyter-plus-tensorflow-v2.4.0-rc4-compiled",
      "jupyter/Singularity.jupyter-plus",
      "r/Singularity.r",
      "r/Singularity.r-plus",
      "ubuntu/Singularity.ubuntu2004",
      "shell/Singularity.shell-plus"
    ],
    "full_name": "darachm/containers",
    "latest_release": null,
    "readme": "\u003cp\u003eThis is for tracking, hosting recipes for Singularity containers, such that\nit can get mirrored on Github and singularity-hub can get it.\u003c/p\u003e\n\u003cp\u003eOrganzation copied from \u003ca href=\"https://github.com/jlboat/BioinfoContainers\"\u003ejlboat\u003c/a\u003e.\n(Of course, makes total sense to just use tags to organize things!)\u003c/p\u003e\n\u003cp\u003eSome recipes are for individual tools, some are for workflows and so are\ncombos.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619081056.0
  },
  {
    "data_format": 2,
    "description": "Dashboard for project tracking, inspired by https://github.com/harvard-nrg/dpdash",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "PREDICT-DPACC/dpdash-devel",
    "latest_release": null,
    "readme": "\u003cp\u003eDashboard for project tracking, adapted from \u003ca href=\"https://github.com/harvard-nrg/dpdash\"\u003ehttps://github.com/harvard-nrg/dpdash\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMaintained by Tashrif Billah, PNL, BWH (HMS)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1611883890.0
  },
  {
    "data_format": 2,
    "description": "Container used to run IMI spikeScreen",
    "filenames": [
      "Singularity"
    ],
    "full_name": "IMIMF-UNILJSI/spikeScreenContainer",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-spikescreencontainer\" class=\"anchor\" href=\"#spikescreencontainer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003espikeScreenContainer\u003c/h1\u003e\n\u003cp\u003eContainer used to run IMI spikeScreen\nThis repo is meant to increase portability through automatic automatic container builds on shub.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1614360734.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "huynhngoc/cnn-template",
    "latest_release": null,
    "readme": "\u003cp\u003eAfter forking this repository, replace \u003ccode\u003eorion_username\u003c/code\u003e in the file \u003ccode\u003e2d_unet_CT_W_PET.json\u003c/code\u003e with your actual orion username.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-in-orion-cluster\" class=\"anchor\" href=\"#setup-in-orion-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup in Orion cluster\u003c/h1\u003e\n\u003cp\u003eThe \u003ccode\u003e$HOME\u003c/code\u003e directory of your login machine (\u003ccode\u003e[username@login ~]\u003c/code\u003e) should have the following structure\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$HOME\n   \u251c\u2500\u2500 cnn_template (the forked repository)\n   \u2502   \u251c\u2500\u2500 config\n   \u2502   |   \u251c\u2500\u2500 2d_unet.json\n   \u2502   |   \u2514\u2500\u2500 (other configurations)\n   |   \u251c\u2500\u2500 outputs\n   \u2502   |   \u2514\u2500\u2500 README.md\n   \u2502   \u251c\u2500\u2500 customize_obj.py\n   \u2502   \u251c\u2500\u2500 experiment.py\n   \u2502   \u251c\u2500\u2500 slurm.sh\n   \u2502   \u251c\u2500\u2500 setup.sh\n   \u2502   \u2514\u2500\u2500 (other files)\n   \u251c\u2500\u2500 datasets (Put your datasets in here)\n   \u2502   \u2514\u2500\u2500 headneck\n   \u2502       \u251c\u2500\u2500 full_dataset_singleclass.h5\n   \u2502       \u2514\u2500\u2500 (other datasets)\n   \u251c\u2500\u2500 hnperf (log files will be saved in here)\n   \u2502\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart by running \u003ccode\u003esetup.sh\u003c/code\u003e to download the singularity container\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e cnn-template\n./setup.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAlternative you can directly download the image file\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e cnn-template\nsingularity pull --name deoxys.sif shub://huynhngoc/head-neck-analysis\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-experiments-on-orion\" class=\"anchor\" href=\"#run-experiments-on-orion\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun experiments on Orion\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-submit-jobs\" class=\"anchor\" href=\"#submit-jobs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubmit jobs\u003c/h2\u003e\n\u003cp\u003eSubmit slurm jobs like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet.json 2d_unet 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the setup from the \u003ccode\u003econfig/2d_unet.json\u003c/code\u003e file, train for 200 epochs\nand store the results in the folder \u003ccode\u003e$HOME/hnperf/2d_unet/\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo customize model and prediction checkpoints, add the \u003ccode\u003emodel_checkpoint_period\u003c/code\u003e and \u003ccode\u003eprediction_checkpoint_period\u003c/code\u003e as arguments\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet_CT_W_PET.json 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\n\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will save the trained model every 5 epochs and predict the validation set every 5 epoch\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-continue-experiments-and-run-test\" class=\"anchor\" href=\"#continue-experiments-and-run-test\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContinue experiments and run test\u003c/h2\u003e\n\u003cp\u003eTo continue an experiment\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_cont.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the saved model and continue training 100 more epochs\u003c/p\u003e\n\u003cp\u003eIn the case the job ended unexpectedly before plotting the performance:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_vis.sh 2d_unet_CT_W_PET\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run test\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_test.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run external validation\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_external.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET maastro.json\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-misc\" class=\"anchor\" href=\"#misc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMisc\u003c/h1\u003e\n\u003cp\u003eManually build the singularity image file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build --fakeroot Singularity deoxys.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLogin to a gpu session to use the gpu\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eqlogin --partition=gpu --gres=gpu:1\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --nv deoxys.sif ipython\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618886215.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "boldqc2-master/Singularity"
    ],
    "full_name": "bragalab/boldqc",
    "latest_release": null,
    "readme": "\u003cp\u003eAfter forking this repository, replace \u003ccode\u003eorion_username\u003c/code\u003e in the file \u003ccode\u003e2d_unet_CT_W_PET.json\u003c/code\u003e with your actual orion username.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-in-orion-cluster\" class=\"anchor\" href=\"#setup-in-orion-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup in Orion cluster\u003c/h1\u003e\n\u003cp\u003eThe \u003ccode\u003e$HOME\u003c/code\u003e directory of your login machine (\u003ccode\u003e[username@login ~]\u003c/code\u003e) should have the following structure\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$HOME\n   \u251c\u2500\u2500 cnn_template (the forked repository)\n   \u2502   \u251c\u2500\u2500 config\n   \u2502   |   \u251c\u2500\u2500 2d_unet.json\n   \u2502   |   \u2514\u2500\u2500 (other configurations)\n   |   \u251c\u2500\u2500 outputs\n   \u2502   |   \u2514\u2500\u2500 README.md\n   \u2502   \u251c\u2500\u2500 customize_obj.py\n   \u2502   \u251c\u2500\u2500 experiment.py\n   \u2502   \u251c\u2500\u2500 slurm.sh\n   \u2502   \u251c\u2500\u2500 setup.sh\n   \u2502   \u2514\u2500\u2500 (other files)\n   \u251c\u2500\u2500 datasets (Put your datasets in here)\n   \u2502   \u2514\u2500\u2500 headneck\n   \u2502       \u251c\u2500\u2500 full_dataset_singleclass.h5\n   \u2502       \u2514\u2500\u2500 (other datasets)\n   \u251c\u2500\u2500 hnperf (log files will be saved in here)\n   \u2502\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart by running \u003ccode\u003esetup.sh\u003c/code\u003e to download the singularity container\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e cnn-template\n./setup.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAlternative you can directly download the image file\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e cnn-template\nsingularity pull --name deoxys.sif shub://huynhngoc/head-neck-analysis\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-experiments-on-orion\" class=\"anchor\" href=\"#run-experiments-on-orion\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun experiments on Orion\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-submit-jobs\" class=\"anchor\" href=\"#submit-jobs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubmit jobs\u003c/h2\u003e\n\u003cp\u003eSubmit slurm jobs like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet.json 2d_unet 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the setup from the \u003ccode\u003econfig/2d_unet.json\u003c/code\u003e file, train for 200 epochs\nand store the results in the folder \u003ccode\u003e$HOME/hnperf/2d_unet/\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo customize model and prediction checkpoints, add the \u003ccode\u003emodel_checkpoint_period\u003c/code\u003e and \u003ccode\u003eprediction_checkpoint_period\u003c/code\u003e as arguments\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet_CT_W_PET.json 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\n\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will save the trained model every 5 epochs and predict the validation set every 5 epoch\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-continue-experiments-and-run-test\" class=\"anchor\" href=\"#continue-experiments-and-run-test\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContinue experiments and run test\u003c/h2\u003e\n\u003cp\u003eTo continue an experiment\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_cont.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the saved model and continue training 100 more epochs\u003c/p\u003e\n\u003cp\u003eIn the case the job ended unexpectedly before plotting the performance:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_vis.sh 2d_unet_CT_W_PET\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run test\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_test.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run external validation\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_external.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET maastro.json\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-misc\" class=\"anchor\" href=\"#misc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMisc\u003c/h1\u003e\n\u003cp\u003eManually build the singularity image file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build --fakeroot Singularity deoxys.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLogin to a gpu session to use the gpu\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eqlogin --partition=gpu --gres=gpu:1\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --nv deoxys.sif ipython\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1620161860.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "touala/centos8",
    "latest_release": null,
    "readme": "\u003cp\u003eAfter forking this repository, replace \u003ccode\u003eorion_username\u003c/code\u003e in the file \u003ccode\u003e2d_unet_CT_W_PET.json\u003c/code\u003e with your actual orion username.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-in-orion-cluster\" class=\"anchor\" href=\"#setup-in-orion-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup in Orion cluster\u003c/h1\u003e\n\u003cp\u003eThe \u003ccode\u003e$HOME\u003c/code\u003e directory of your login machine (\u003ccode\u003e[username@login ~]\u003c/code\u003e) should have the following structure\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$HOME\n   \u251c\u2500\u2500 cnn_template (the forked repository)\n   \u2502   \u251c\u2500\u2500 config\n   \u2502   |   \u251c\u2500\u2500 2d_unet.json\n   \u2502   |   \u2514\u2500\u2500 (other configurations)\n   |   \u251c\u2500\u2500 outputs\n   \u2502   |   \u2514\u2500\u2500 README.md\n   \u2502   \u251c\u2500\u2500 customize_obj.py\n   \u2502   \u251c\u2500\u2500 experiment.py\n   \u2502   \u251c\u2500\u2500 slurm.sh\n   \u2502   \u251c\u2500\u2500 setup.sh\n   \u2502   \u2514\u2500\u2500 (other files)\n   \u251c\u2500\u2500 datasets (Put your datasets in here)\n   \u2502   \u2514\u2500\u2500 headneck\n   \u2502       \u251c\u2500\u2500 full_dataset_singleclass.h5\n   \u2502       \u2514\u2500\u2500 (other datasets)\n   \u251c\u2500\u2500 hnperf (log files will be saved in here)\n   \u2502\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart by running \u003ccode\u003esetup.sh\u003c/code\u003e to download the singularity container\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e cnn-template\n./setup.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAlternative you can directly download the image file\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e cnn-template\nsingularity pull --name deoxys.sif shub://huynhngoc/head-neck-analysis\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-experiments-on-orion\" class=\"anchor\" href=\"#run-experiments-on-orion\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun experiments on Orion\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-submit-jobs\" class=\"anchor\" href=\"#submit-jobs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubmit jobs\u003c/h2\u003e\n\u003cp\u003eSubmit slurm jobs like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet.json 2d_unet 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the setup from the \u003ccode\u003econfig/2d_unet.json\u003c/code\u003e file, train for 200 epochs\nand store the results in the folder \u003ccode\u003e$HOME/hnperf/2d_unet/\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo customize model and prediction checkpoints, add the \u003ccode\u003emodel_checkpoint_period\u003c/code\u003e and \u003ccode\u003eprediction_checkpoint_period\u003c/code\u003e as arguments\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet_CT_W_PET.json 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\n\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will save the trained model every 5 epochs and predict the validation set every 5 epoch\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-continue-experiments-and-run-test\" class=\"anchor\" href=\"#continue-experiments-and-run-test\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContinue experiments and run test\u003c/h2\u003e\n\u003cp\u003eTo continue an experiment\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_cont.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the saved model and continue training 100 more epochs\u003c/p\u003e\n\u003cp\u003eIn the case the job ended unexpectedly before plotting the performance:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_vis.sh 2d_unet_CT_W_PET\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run test\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_test.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run external validation\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm_external.sh ../hnperf/2d_unet_CT_W_PET/model/model.030.h5 2d_unet_CT_W_PET maastro.json\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-misc\" class=\"anchor\" href=\"#misc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMisc\u003c/h1\u003e\n\u003cp\u003eManually build the singularity image file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build --fakeroot Singularity deoxys.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLogin to a gpu session to use the gpu\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eqlogin --partition=gpu --gres=gpu:1\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --nv deoxys.sif ipython\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607663517.0
  },
  {
    "data_format": 2,
    "description": "If you are going to build off of basic Empirical, this is the project for you",
    "filenames": [
      "third-party/force-cover/Singularity"
    ],
    "full_name": "piperwelch/Basic-Empirical-Starter-carlcs361s01w21-6",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-evolutionary-algorithm\" class=\"anchor\" href=\"#evolutionary-algorithm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEvolutionary Algorithm\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/anyaevostinar/evo-algo/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ff63e2cc80517f7b8e8246b33025f569d757ede0ae65c7ea57418d79e5a3709d/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f25324676657273696f6e2d62616467652e6a736f6e\" alt=\"version\" data-canonical-src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fversion-badge.json\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.com/anyaevostinar/evo-algo\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2bde10efc09d993aad000b3101be56d5410d0ced348c4c7bbedbc7ffce79d630/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f616e796165766f7374696e61722f65766f2d616c676f2e737667\" alt=\"\" data-canonical-src=\"https://img.shields.io/travis/anyaevostinar/evo-algo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://evo-algo.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2af827df5df15ff6f5c6a9fff5021e631a0049aa2274f98e983135bb66f0ed81/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f65766f2d616c676f2f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/evo-algo/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://evo-algo.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0e035446b6d1c7a911bd4b203bd581f2116c7873352a90d4797dc08119abbd0e/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f253246646f63756d656e746174696f6e2d636f7665726167652d62616467652e6a736f6e\" alt=\"documentation coverage\" data-canonical-src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fdocumentation-coverage-badge.json\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/anyaevostinar/evo-algo\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/869814fe0b90d0baadc37140ac31d6d9c0e4e00c2854a51f1030c40678bc13a4/68747470733a2f2f636f6465636f762e696f2f67682f616e796165766f7374696e61722f65766f2d616c676f2f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"code coverage status\" data-canonical-src=\"https://codecov.io/gh/anyaevostinar/evo-algo/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/anyaevostinar/evo-algo/search?q=todo+OR+fixme\u0026amp;type=\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4455f1d1625d643fe17506cb6ad50a9a6612ce62ab4667dc60b75616241c7534/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e636f6d25324665766f2d616c676f253246646f746f2d62616467652e6a736f6e\" alt=\"dotos\" data-canonical-src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.com%2Fevo-algo%2Fdoto-badge.json\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/anyaevostinar/evo-algo\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/db51ac0d7785eb561dcfc0cbccfc0cfed9872513120f8f732b1301504c4eb32e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e796165766f7374696e61722f65766f2d616c676f2e7376673f7374796c653d666c61742d737175617265266c6f676f3d676974687562266c6162656c3d5374617273266c6f676f436f6c6f723d7768697465\" alt=\"GitHub stars\" data-canonical-src=\"https://img.shields.io/github/stars/anyaevostinar/evo-algo.svg?style=flat-square\u0026amp;logo=github\u0026amp;label=Stars\u0026amp;logoColor=white\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn evolutionary algorithm\u003c/p\u003e\n\u003cp\u003eCheck out the live in-browser web app at \u003ca href=\"https://anyaevostinar.github.io/evo-algo\" rel=\"nofollow\"\u003ehttps://anyaevostinar.github.io/evo-algo\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFree software: MIT license\u003c/li\u003e\n\u003cli\u003eDocumentation: \u003ca href=\"https://evo-algo.readthedocs.io\" rel=\"nofollow\"\u003ehttps://evo-algo.readthedocs.io\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"docs/assets/cookie.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/assets/cookie.gif\" alt=\"cookie monster example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eThis package was created with \u003ca href=\"https://github.com/audreyr/cookiecutter\"\u003eCookiecutter\u003c/a\u003e and the \u003ca href=\"https://github.com/devosoft/cookiecutter-empirical-project\"\u003edevosoft/cookiecutter-empirical-project\u003c/a\u003e project template.\u003c/p\u003e\n\u003cp\u003eThis package uses \u003ca href=\"https://github.com/devosoft/Empirical#readme\"\u003eEmpirical\u003c/a\u003e, a library of tools for scientific software development, with emphasis on also being able to build web interfaces using Emscripten.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eTo install \u003ca href=\"https://github.com/devosoft/Empirical\"\u003eEmpirical\u003c/a\u003e, pull down a clone of the Empirical repository.  See \u003ca href=\"https://empirical.readthedocs.io/en/latest/QuickStartGuides\" rel=\"nofollow\"\u003eQuick Start Guides\u003c/a\u003e for directions on cloning and using the library.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1615709051.0
  },
  {
    "data_format": 2,
    "description": "Container for captioning project using pytorch and java",
    "filenames": [
      "torch_java/Singularity.simple_v2",
      "torch_java/Singularity.simple_v3",
      "torch_java/Singularity.torch_java11_w_py",
      "torch_java/Singularity.simple_torch:0.4",
      "torch_java/Singularity.torch_java11",
      "torch_java/Singularity.simple"
    ],
    "full_name": "jackjamend/CaptionContainer",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-evolutionary-algorithm\" class=\"anchor\" href=\"#evolutionary-algorithm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEvolutionary Algorithm\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/anyaevostinar/evo-algo/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ff63e2cc80517f7b8e8246b33025f569d757ede0ae65c7ea57418d79e5a3709d/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f25324676657273696f6e2d62616467652e6a736f6e\" alt=\"version\" data-canonical-src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fversion-badge.json\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.com/anyaevostinar/evo-algo\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2bde10efc09d993aad000b3101be56d5410d0ced348c4c7bbedbc7ffce79d630/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f616e796165766f7374696e61722f65766f2d616c676f2e737667\" alt=\"\" data-canonical-src=\"https://img.shields.io/travis/anyaevostinar/evo-algo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://evo-algo.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2af827df5df15ff6f5c6a9fff5021e631a0049aa2274f98e983135bb66f0ed81/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f65766f2d616c676f2f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/evo-algo/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://evo-algo.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0e035446b6d1c7a911bd4b203bd581f2116c7873352a90d4797dc08119abbd0e/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e6769746875622e696f25324665766f2d616c676f253246646f63756d656e746174696f6e2d636f7665726167652d62616467652e6a736f6e\" alt=\"documentation coverage\" data-canonical-src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.github.io%2Fevo-algo%2Fdocumentation-coverage-badge.json\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/anyaevostinar/evo-algo\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/869814fe0b90d0baadc37140ac31d6d9c0e4e00c2854a51f1030c40678bc13a4/68747470733a2f2f636f6465636f762e696f2f67682f616e796165766f7374696e61722f65766f2d616c676f2f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"code coverage status\" data-canonical-src=\"https://codecov.io/gh/anyaevostinar/evo-algo/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/anyaevostinar/evo-algo/search?q=todo+OR+fixme\u0026amp;type=\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4455f1d1625d643fe17506cb6ad50a9a6612ce62ab4667dc60b75616241c7534/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d6874747073253341253246253246616e796165766f7374696e61722e636f6d25324665766f2d616c676f253246646f746f2d62616467652e6a736f6e\" alt=\"dotos\" data-canonical-src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fanyaevostinar.com%2Fevo-algo%2Fdoto-badge.json\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/anyaevostinar/evo-algo\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/db51ac0d7785eb561dcfc0cbccfc0cfed9872513120f8f732b1301504c4eb32e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e796165766f7374696e61722f65766f2d616c676f2e7376673f7374796c653d666c61742d737175617265266c6f676f3d676974687562266c6162656c3d5374617273266c6f676f436f6c6f723d7768697465\" alt=\"GitHub stars\" data-canonical-src=\"https://img.shields.io/github/stars/anyaevostinar/evo-algo.svg?style=flat-square\u0026amp;logo=github\u0026amp;label=Stars\u0026amp;logoColor=white\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn evolutionary algorithm\u003c/p\u003e\n\u003cp\u003eCheck out the live in-browser web app at \u003ca href=\"https://anyaevostinar.github.io/evo-algo\" rel=\"nofollow\"\u003ehttps://anyaevostinar.github.io/evo-algo\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFree software: MIT license\u003c/li\u003e\n\u003cli\u003eDocumentation: \u003ca href=\"https://evo-algo.readthedocs.io\" rel=\"nofollow\"\u003ehttps://evo-algo.readthedocs.io\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"docs/assets/cookie.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/assets/cookie.gif\" alt=\"cookie monster example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eThis package was created with \u003ca href=\"https://github.com/audreyr/cookiecutter\"\u003eCookiecutter\u003c/a\u003e and the \u003ca href=\"https://github.com/devosoft/cookiecutter-empirical-project\"\u003edevosoft/cookiecutter-empirical-project\u003c/a\u003e project template.\u003c/p\u003e\n\u003cp\u003eThis package uses \u003ca href=\"https://github.com/devosoft/Empirical#readme\"\u003eEmpirical\u003c/a\u003e, a library of tools for scientific software development, with emphasis on also being able to build web interfaces using Emscripten.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eTo install \u003ca href=\"https://github.com/devosoft/Empirical\"\u003eEmpirical\u003c/a\u003e, pull down a clone of the Empirical repository.  See \u003ca href=\"https://empirical.readthedocs.io/en/latest/QuickStartGuides\" rel=\"nofollow\"\u003eQuick Start Guides\u003c/a\u003e for directions on cloning and using the library.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1616709684.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "CNRResistanceAntibiotic/mutAnalysis",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mutanalysis\" class=\"anchor\" href=\"#mutanalysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emutAnalysis\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605218495.0
  },
  {
    "data_format": 2,
    "description": "Container for ML based analysis @ HEPHY",
    "filenames": [
      "machine-learning-hats/Singularity.cpu",
      "machine-learning-hats/Singularity.gpu"
    ],
    "full_name": "HephyAnalysisSW/MLContainer",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mlcontainer\" class=\"anchor\" href=\"#mlcontainer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMLContainer\u003c/h1\u003e\n\u003cp\u003eSingularity container and conda environments for ML based analysis @ HEPHY\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-machine-learning-hats\" class=\"anchor\" href=\"#machine-learning-hats\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emachine-learning-hats\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/FNALLPC/machine-learning-hats\"\u003ehttps://github.com/FNALLPC/machine-learning-hats\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOn HEPGPU01 its easier to use the conda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create --file environment-gpu.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn CLIP its better to use the container. It will\nbe installed after the shutdown.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the shell container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun a script\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh \u0026lt;scripts\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deepjetcore\" class=\"anchor\" href=\"#deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepJetCore\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/DL4Jets/DeepJetCore\"\u003ehttps://github.com/DL4Jets/DeepJetCore\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-container-for-deepjetcore\" class=\"anchor\" href=\"#container-for-deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer for DeepJetCore\u003c/h1\u003e\n\u003cp\u003eBuild the container (only on  HEPGPU01)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-todo\" class=\"anchor\" href=\"#todo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTODO\u003c/h2\u003e\n\u003cp\u003eTry container on CLIP\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-open-points\" class=\"anchor\" href=\"#open-points\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpen Points\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-conda-environments-on-clip\" class=\"anchor\" href=\"#conda-environments-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConda Environments on CLIP\u003c/h3\u003e\n\u003cp\u003eCLIP provides already installations of Conda\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eml add Anaconda3/19.10\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNevertheless I could not build reliable environments due to limited\nquota. Also trying to move the correspondig directories to /scratch-cbe/users\nwas not successfull.\u003c/p\u003e\n\u003cp\u003eAt this point it seems better to use the conda environment inside a container.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-clip\" class=\"anchor\" href=\"#building-container-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on CLIP\u003c/h3\u003e\n\u003cp\u003eBuilding containers on CLIP has two problems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eroot rights required to build container (possible solution fakeroot)\u003c/li\u003e\n\u003cli\u003edocker container could be transformed in singularity container, but\nthe filesystems (BeeGEFS) do not fullfill the singularity requirements\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe best way seems to use CI with Jenkis ans the local singularity hub. This\nhas to be understood in the future.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-hepgpu01\" class=\"anchor\" href=\"#building-container-on-hepgpu01\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on HEPGPU01\u003c/h3\u003e\n\u003cp\u003eRoot rights are required in case container are build from scratch. Fakeroot\nwould be a possible workaround. Has to be tried.\u003c/p\u003e\n\u003cp\u003eAnother possibility is to use \"sudo\".\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1603481294.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "zhinoos/TestWebinar",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mlcontainer\" class=\"anchor\" href=\"#mlcontainer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMLContainer\u003c/h1\u003e\n\u003cp\u003eSingularity container and conda environments for ML based analysis @ HEPHY\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-machine-learning-hats\" class=\"anchor\" href=\"#machine-learning-hats\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emachine-learning-hats\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/FNALLPC/machine-learning-hats\"\u003ehttps://github.com/FNALLPC/machine-learning-hats\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOn HEPGPU01 its easier to use the conda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create --file environment-gpu.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn CLIP its better to use the container. It will\nbe installed after the shutdown.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the shell container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun a script\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh \u0026lt;scripts\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deepjetcore\" class=\"anchor\" href=\"#deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepJetCore\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/DL4Jets/DeepJetCore\"\u003ehttps://github.com/DL4Jets/DeepJetCore\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-container-for-deepjetcore\" class=\"anchor\" href=\"#container-for-deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer for DeepJetCore\u003c/h1\u003e\n\u003cp\u003eBuild the container (only on  HEPGPU01)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-todo\" class=\"anchor\" href=\"#todo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTODO\u003c/h2\u003e\n\u003cp\u003eTry container on CLIP\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-open-points\" class=\"anchor\" href=\"#open-points\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpen Points\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-conda-environments-on-clip\" class=\"anchor\" href=\"#conda-environments-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConda Environments on CLIP\u003c/h3\u003e\n\u003cp\u003eCLIP provides already installations of Conda\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eml add Anaconda3/19.10\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNevertheless I could not build reliable environments due to limited\nquota. Also trying to move the correspondig directories to /scratch-cbe/users\nwas not successfull.\u003c/p\u003e\n\u003cp\u003eAt this point it seems better to use the conda environment inside a container.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-clip\" class=\"anchor\" href=\"#building-container-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on CLIP\u003c/h3\u003e\n\u003cp\u003eBuilding containers on CLIP has two problems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eroot rights required to build container (possible solution fakeroot)\u003c/li\u003e\n\u003cli\u003edocker container could be transformed in singularity container, but\nthe filesystems (BeeGEFS) do not fullfill the singularity requirements\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe best way seems to use CI with Jenkis ans the local singularity hub. This\nhas to be understood in the future.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-hepgpu01\" class=\"anchor\" href=\"#building-container-on-hepgpu01\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on HEPGPU01\u003c/h3\u003e\n\u003cp\u003eRoot rights are required in case container are build from scratch. Fakeroot\nwould be a possible workaround. Has to be tried.\u003c/p\u003e\n\u003cp\u003eAnother possibility is to use \"sudo\".\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1612903610.0
  },
  {
    "data_format": 2,
    "description": "This is the repo from Khun Sang\u0027s gitlab",
    "filenames": [
      "containers/Singularity.dev",
      "containers/Singularity.0.0.3",
      "containers/Singularity.0.0.4",
      "containers/Singularity.0.0.2",
      "containers/Singularity.0.0.1"
    ],
    "full_name": "Samanwaya1301/tidal-heating-bilby-pipe",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mlcontainer\" class=\"anchor\" href=\"#mlcontainer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMLContainer\u003c/h1\u003e\n\u003cp\u003eSingularity container and conda environments for ML based analysis @ HEPHY\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-machine-learning-hats\" class=\"anchor\" href=\"#machine-learning-hats\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emachine-learning-hats\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/FNALLPC/machine-learning-hats\"\u003ehttps://github.com/FNALLPC/machine-learning-hats\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOn HEPGPU01 its easier to use the conda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create --file environment-gpu.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn CLIP its better to use the container. It will\nbe installed after the shutdown.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the shell container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun a script\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh \u0026lt;scripts\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deepjetcore\" class=\"anchor\" href=\"#deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepJetCore\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/DL4Jets/DeepJetCore\"\u003ehttps://github.com/DL4Jets/DeepJetCore\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-container-for-deepjetcore\" class=\"anchor\" href=\"#container-for-deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer for DeepJetCore\u003c/h1\u003e\n\u003cp\u003eBuild the container (only on  HEPGPU01)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-todo\" class=\"anchor\" href=\"#todo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTODO\u003c/h2\u003e\n\u003cp\u003eTry container on CLIP\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-open-points\" class=\"anchor\" href=\"#open-points\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpen Points\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-conda-environments-on-clip\" class=\"anchor\" href=\"#conda-environments-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConda Environments on CLIP\u003c/h3\u003e\n\u003cp\u003eCLIP provides already installations of Conda\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eml add Anaconda3/19.10\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNevertheless I could not build reliable environments due to limited\nquota. Also trying to move the correspondig directories to /scratch-cbe/users\nwas not successfull.\u003c/p\u003e\n\u003cp\u003eAt this point it seems better to use the conda environment inside a container.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-clip\" class=\"anchor\" href=\"#building-container-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on CLIP\u003c/h3\u003e\n\u003cp\u003eBuilding containers on CLIP has two problems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eroot rights required to build container (possible solution fakeroot)\u003c/li\u003e\n\u003cli\u003edocker container could be transformed in singularity container, but\nthe filesystems (BeeGEFS) do not fullfill the singularity requirements\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe best way seems to use CI with Jenkis ans the local singularity hub. This\nhas to be understood in the future.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-hepgpu01\" class=\"anchor\" href=\"#building-container-on-hepgpu01\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on HEPGPU01\u003c/h3\u003e\n\u003cp\u003eRoot rights are required in case container are build from scratch. Fakeroot\nwould be a possible workaround. Has to be tried.\u003c/p\u003e\n\u003cp\u003eAnother possibility is to use \"sudo\".\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605113430.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Metage2Metabo metacom.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ArnaudBelcour/metage2metabo-metacom_singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mlcontainer\" class=\"anchor\" href=\"#mlcontainer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMLContainer\u003c/h1\u003e\n\u003cp\u003eSingularity container and conda environments for ML based analysis @ HEPHY\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-machine-learning-hats\" class=\"anchor\" href=\"#machine-learning-hats\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emachine-learning-hats\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/FNALLPC/machine-learning-hats\"\u003ehttps://github.com/FNALLPC/machine-learning-hats\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOn HEPGPU01 its easier to use the conda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create --file environment-gpu.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn CLIP its better to use the container. It will\nbe installed after the shutdown.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the shell container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun a script\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_ml-hats.sh \u0026lt;scripts\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deepjetcore\" class=\"anchor\" href=\"#deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepJetCore\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/DL4Jets/DeepJetCore\"\u003ehttps://github.com/DL4Jets/DeepJetCore\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-container-for-deepjetcore\" class=\"anchor\" href=\"#container-for-deepjetcore\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer for DeepJetCore\u003c/h1\u003e\n\u003cp\u003eBuild the container (only on  HEPGPU01)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebuild_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_deepjet3.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-todo\" class=\"anchor\" href=\"#todo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTODO\u003c/h2\u003e\n\u003cp\u003eTry container on CLIP\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-open-points\" class=\"anchor\" href=\"#open-points\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpen Points\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-conda-environments-on-clip\" class=\"anchor\" href=\"#conda-environments-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConda Environments on CLIP\u003c/h3\u003e\n\u003cp\u003eCLIP provides already installations of Conda\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eml add Anaconda3/19.10\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNevertheless I could not build reliable environments due to limited\nquota. Also trying to move the correspondig directories to /scratch-cbe/users\nwas not successfull.\u003c/p\u003e\n\u003cp\u003eAt this point it seems better to use the conda environment inside a container.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-clip\" class=\"anchor\" href=\"#building-container-on-clip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on CLIP\u003c/h3\u003e\n\u003cp\u003eBuilding containers on CLIP has two problems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eroot rights required to build container (possible solution fakeroot)\u003c/li\u003e\n\u003cli\u003edocker container could be transformed in singularity container, but\nthe filesystems (BeeGEFS) do not fullfill the singularity requirements\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe best way seems to use CI with Jenkis ans the local singularity hub. This\nhas to be understood in the future.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-container-on-hepgpu01\" class=\"anchor\" href=\"#building-container-on-hepgpu01\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Container on HEPGPU01\u003c/h3\u003e\n\u003cp\u003eRoot rights are required in case container are build from scratch. Fakeroot\nwould be a possible workaround. Has to be tried.\u003c/p\u003e\n\u003cp\u003eAnother possibility is to use \"sudo\".\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1616028008.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.beta"
    ],
    "full_name": "huynhngoc/head-neck-analysis",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-head-and-neck-cancer-analysis\" class=\"anchor\" href=\"#head-and-neck-cancer-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHead and Neck cancer analysis\u003c/h1\u003e\n\u003cp\u003eStart by running \u003ccode\u003esetup.sh\u003c/code\u003e to download the singularity container\nThen, submit slurm jobs like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet.json 2d_unet 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the setup from the \u003ccode\u003econfig/2d_unet.json\u003c/code\u003e file, train for 200 epochs\nand store the results in the folder \u003ccode\u003e$HOME/logs/hn_perf/2d_unet/\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo customize model and prediction checkpoints\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm.sh config/3d_vnet_32_normalize.json 3d_vnet_32_normalize 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo continue an experiment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm_cont.sh config/3d_vnet_32_normalize/model/model.030.h5 3d_vnet_32_normalize 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo plot performance\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm_vis.sh 3d_vnet_32_normalize\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run test\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm_test.sh 3d_vnet_32/model/model.030.h5 3d_vnet_32\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively, if your cluster does not have slurm installed, simply omit the \u003ccode\u003esbatch\u003c/code\u003e\npart of the call above, thus running\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./slurm.sh config/2d_unet.json 2d_unet 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eManually build\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build --fakeroot Singularity deoxys.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRemember to login to a gpu session to use the gpu\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eqlogin --partition=gpu --gres=gpu:1\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620348200.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "forbiditerative/Singularity",
      "fd-red-black-postipc2018/Singularity"
    ],
    "full_name": "zihangs/plan_generators",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-generate-plans-using-forbid-iterative-planners-top-k-diverse-etc\" class=\"anchor\" href=\"#generate-plans-using-forbid-iterative-planners-top-k-diverse-etc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate plans using forbid iterative planners (top-k, diverse, etc)\u003c/h1\u003e\n\u003cp\u003eThis is a tool for creating synthetic plans, the generated plans will be converted to event logs (\u003ccode\u003e.xes\u003c/code\u003e files). Basically, this is a script which wrapped up current exisiting planners and the output data are in a suitable structure and format for our goal recognition experiments.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-things-need-to-be-prepared\" class=\"anchor\" href=\"#things-need-to-be-prepared\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThings need to be prepared\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFor running diverse planner, CPLEX and an recommended fast-downward planner is required.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRequired python 3 runtime environment, I recommend to build a python 3 virtual environment if you both have python 2 and python 3 installed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNeed to complie C++ source codes, by following instuctions (CPLEX need to be pre-installed).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo configure the command line environment variables in absolute path\u003c/p\u003e\n\u003cp\u003eexport DIVERSE_SCORE_COMPUTATION_PATH=/home/ubuntu/plan_generators/diversescore (VM)\u003c/p\u003e\n\u003cp\u003eexport DIVERSE_FAST_DOWNWARD_PLANNER_PATH=/home/ubuntu/plan_generators/fd-red-black-postipc2018 (VM)\u003c/p\u003e\n\u003cp\u003eexport DIVERSE_SCORE_COMPUTATION_PATH=/Users/zihangs/plan_generators/diversescore (Mac)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDownload the dataset and put the downloaded dataset in this directory.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-commands-for-running-the-script\" class=\"anchor\" href=\"#commands-for-running-the-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommands for running the script\u003c/h3\u003e\n\u003cp\u003eBefore run the commands, you have to check the parameter configrations, the parameters at the top of \u003ccode\u003erun.py\u003c/code\u003e. Then just run the following commands to starts.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e activate the python 3 venv (if you don\u0027t using venv, ignore this step)\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003evenv\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/bin/activate\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e run script\u003c/span\u003e\npython run.py\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIt will take a long time to run.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h3\u003e\n\u003cp\u003eOnce the process is completed, check this directory, there will be a sub-directory \u003ccode\u003egene_data/\u003c/code\u003e. All the domains, problems, tests and generated plans will be there. Then this directory will be used for our next steps for mining process models.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1608999691.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v1.0.0"
    ],
    "full_name": "darachm/ubuntu-2004",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-generate-plans-using-forbid-iterative-planners-top-k-diverse-etc\" class=\"anchor\" href=\"#generate-plans-using-forbid-iterative-planners-top-k-diverse-etc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate plans using forbid iterative planners (top-k, diverse, etc)\u003c/h1\u003e\n\u003cp\u003eThis is a tool for creating synthetic plans, the generated plans will be converted to event logs (\u003ccode\u003e.xes\u003c/code\u003e files). Basically, this is a script which wrapped up current exisiting planners and the output data are in a suitable structure and format for our goal recognition experiments.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-things-need-to-be-prepared\" class=\"anchor\" href=\"#things-need-to-be-prepared\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThings need to be prepared\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFor running diverse planner, CPLEX and an recommended fast-downward planner is required.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRequired python 3 runtime environment, I recommend to build a python 3 virtual environment if you both have python 2 and python 3 installed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNeed to complie C++ source codes, by following instuctions (CPLEX need to be pre-installed).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo configure the command line environment variables in absolute path\u003c/p\u003e\n\u003cp\u003eexport DIVERSE_SCORE_COMPUTATION_PATH=/home/ubuntu/plan_generators/diversescore (VM)\u003c/p\u003e\n\u003cp\u003eexport DIVERSE_FAST_DOWNWARD_PLANNER_PATH=/home/ubuntu/plan_generators/fd-red-black-postipc2018 (VM)\u003c/p\u003e\n\u003cp\u003eexport DIVERSE_SCORE_COMPUTATION_PATH=/Users/zihangs/plan_generators/diversescore (Mac)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDownload the dataset and put the downloaded dataset in this directory.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-commands-for-running-the-script\" class=\"anchor\" href=\"#commands-for-running-the-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommands for running the script\u003c/h3\u003e\n\u003cp\u003eBefore run the commands, you have to check the parameter configrations, the parameters at the top of \u003ccode\u003erun.py\u003c/code\u003e. Then just run the following commands to starts.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e activate the python 3 venv (if you don\u0027t using venv, ignore this step)\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003evenv\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/bin/activate\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e run script\u003c/span\u003e\npython run.py\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIt will take a long time to run.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h3\u003e\n\u003cp\u003eOnce the process is completed, check this directory, there will be a sub-directory \u003ccode\u003egene_data/\u003c/code\u003e. All the domains, problems, tests and generated plans will be there. Then this directory will be used for our next steps for mining process models.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1597826109.0
  },
  {
    "data_format": 2,
    "description": "Test SingularityHub cloud builders",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ftabaro/singularity-test",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-test-singularityhub-builder\" class=\"anchor\" href=\"#test-singularityhub-builder\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest SingularityHub builder\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1597165176.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for nf-upcoast-v (https://github.com/anwarMZ/nf-upcoast-v)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "anwarMZ/nf-upcoastv-singularity",
    "latest_release": "1.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-deploy\" class=\"anchor\" href=\"#singularity-deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Deploy\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"img/shpc.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"img/shpc.png\" alt=\"img/shpc.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWouldn\u0027t it be nice to build Singularity images without a registry proper,\nand just keep them alongside the GitHub codebase? This is now possible!\nThis small repository provides an example to get you started. It will\nbuild one or more images (whatever Singularity.* files that are present at\nthe root) and then release them as assets to your GitHub repository so\nthat they can be programatically obtained. It is associated with\n\u003ca href=\"https://github.com/singularityhub/singularity-hpc\"\u003esingularity-hpc\u003c/a\u003e to allow\nyou to then define LMOD modules for these same containers.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCan I upload the largest of chonkers?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYes and no. Note that assets are limited to 2 GB in size, which is still fairly good. You can use\nit as a template for your own recipes as is, or modify it for your custom\nuse case. Instructions are below!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-template-or-fork\" class=\"anchor\" href=\"#1-template-or-fork\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Template or Fork\u003c/h3\u003e\n\u003cp\u003eIf you haven\u0027t already, template or fork this repository. You can then clone\nyour fork:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git clone git@github.com:\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/singularity-deploy\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou likely want to name the repository by the container. For example, if I would\nhave created a container on Docker Hub or similar with the name \u003ccode\u003evsoch/salad\u003c/code\u003e,\nhere I\u0027d call the repository \u003ccode\u003esalad\u003c/code\u003e. You obviously are limited to your username\nor an organizational namespace.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-write-your-singularity-recipes\" class=\"anchor\" href=\"#1-write-your-singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Write your Singularity Recipe(s)\u003c/h3\u003e\n\u003cp\u003eFirst, you should write your container recipe(s) in the present working directory.\nFor good practice, when you are updating recipes you should checkout a new branch\nand open a pull request, as the repository comes with a workflow to trigger on a PR\nto \u003ca href=\".github/workflows/test.yml\"\u003etest your container build\u003c/a\u003e. You can add any additional\ntests that that you might need. By default, any Singularity.* file will be automatically detected.\nIf there is no extension (the name Singularity), the name used will be \"latest.\"\nYou can use these tags across multiple releases of your containers. For example,\nthese files would generate packages with sifs named as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"Singularity\"\u003eSingularity\u003c/a\u003e maps to \u003ca href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\u003ehttps://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"Singularity.pokemon\"\u003eSingularity.pokemon\u003c/a\u003e maps to \u003ca href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif\"\u003ehttps://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.pokemon.sif\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"Singularity.salad\"\u003eSingularity.salad\u003c/a\u003e maps to \u003ca href=\"https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\"\u003ehttps://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor each name, you can see the direct download URL contains the repository (singularityhub/singularity-deploy),\nYou should not use any \u003ccode\u003e:\u003c/code\u003e characters in either your container tag (the GitHub extension) or\nthe GitHub tags (the release tags) as this might interfere with parsing.\nThe GitHub release tag (0.0.1 in the example above) is discussed next.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-update-the-version-file\" class=\"anchor\" href=\"#2-update-the-version-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Update the VERSION file\u003c/h3\u003e\n\u003cp\u003eAny time that you prepare new container recipes, you should update the \u003ca href=\"VERSION\"\u003eVERSION\u003c/a\u003e\nfile. The way that this repository works is to generate a release based on the\nstring in \u003ccode\u003eVERSION\u003c/code\u003e. A version is just a tag, so it could be something like\n\u003ccode\u003e0.0.1\u003c/code\u003e or \u003ccode\u003e0.0.1-slim\u003c/code\u003e. Keep in mind that GitHub releases cannot have duplicated\nnames, so you should not repeat the same tag. Do not use \u003ccode\u003e:\u003c/code\u003e in your tag names.\nIf you do need to re-release a tag (not recommended if a user might be using it and then it\u0027s changed) you can manually delete\nthe release and the tag in the GitHub interface. This is a nice structure because it\nmeans you can have containers with different names under the same tag. In the example\nabove, we have each of \"deploy,\" \"latest,\" and \"salad\" released under tag 0.0.1.\nThis is how it looks on GitHub:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"img/releases.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"img/releases.png\" alt=\"img/releases.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-how-to-develop\" class=\"anchor\" href=\"#3-how-to-develop\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. How to Develop\u003c/h3\u003e\n\u003cp\u003eAs we mentioned previously, the container builds will be tested on a pull request,\nand the release will trigger on merge into your main branch (main). See the \u003ca href=\".github/workflows/builder.yml\"\u003e.github/workflows/builder.yml\u003c/a\u003e)\nto edit this. The idea is that you can:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDevelop your container via a development branch\u003c/li\u003e\n\u003cli\u003eOpen a pull request to test the container (the \u003ca href=\".github/workflows/test.yml\"\u003e.github/workflows/test.yml\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eOn merge, your container will be released!\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-how-to-pull\" class=\"anchor\" href=\"#4-how-to-pull\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. How to pull\u003c/h3\u003e\n\u003cp\u003eTechnically, Singularity can pull just knowing the URL. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity pull https://github.com/singularityhub/singularity-deploy/releases/download/0.0.1/singularityhub-singularity-deploy.latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHowever, the \u003ca href=\"singularity-hpc\"\u003esingularity-hpc\u003c/a\u003e tool (will be) designed to be able to parse and handle\nthese container uris automatically. For the containers here, you could do:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:latest\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:salad\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:pokemon\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor write the container URI into a registry entry:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egh: singularityhub/singularity-deploy\nlatest:\n  latest: \"0.0.1\"\ntags:\n  \"latest\": \"0.0.1\"\n  \"salad\": \"0.0.1\"\n  \"pokemon\": \"0.0.1\"\nmaintainer: \"@vsoch\"\nurl: https://github.com/singularityhub/singularity-deploy\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(This part is still under development!)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "container-recipes",
      "singularity-hpc"
    ],
    "updated_at": 1619679157.0
  },
  {
    "data_format": 2,
    "description": "Repo for the singularitx container used in the assembly and annotation exercise of the msc course on bioinformatics",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ikmb/teaching-med-assembly",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-teaching-med-assembly\" class=\"anchor\" href=\"#teaching-med-assembly\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eteaching-med-assembly\u003c/h1\u003e\n\u003cp\u003eRepo for the singularity container used in the assembly and annotation exercise of the msc course on bioinformatics\u003c/p\u003e\n\u003cp\u003eThis needs to be re-build/updated each year since Prokka stops working after some time.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1606743227.0
  },
  {
    "data_format": 2,
    "description": "Testing automated Docker Hub and Singularity Hub builds",
    "filenames": [
      "kraken2/latest/Singularity.kraken2-latest",
      "kraken2/2.0.8/Singularity.kraken2-2.0.8",
      "abricate/latest/Singularity.abricate-latest",
      "abricate/0.9.9/Singularity.abricate-0.9.9",
      "quast/latest/Singularity.quast-latest",
      "quast/5.0.2/Singularity.quast-5.0.2",
      "mykrobe/latest/Singularity.mykrobe-latest",
      "mykrobe/0.8.1/Singularity.mykrobe-0.8.1",
      "minimap2/latest/Singularity.minimap2-latest",
      "minimap2/2.17/Singularity.minimap2-2.17",
      "multiqc/latest/Singularity.multiqc-latest",
      "multiqc/1.9/Singularity.multiqc-1.9",
      "centrifuge/latest/Singularity.centrifuge-latest",
      "centrifuge/1.0.4/Singularity.centrifuge-1.0.4",
      "trimgalore/latest/Singularity.trimgalore-latest",
      "trimgalore/0.6.5/Singularity.trimgalore-0.6.5",
      "krona/2.7.1/Singularity.krona-2.7.1",
      "krona/latest/Singularity.krona-latest",
      "prokka/latest/Singularity.prokka-latest",
      "prokka/1.14.5/Singularity.prokka-1.14.5",
      "shovill/1.1.0/Singularity.shovill-1.1.0",
      "shovill/latest/Singularity.shovill-latest",
      "shovill/1.0.0/Singularity.shovill-1.0.0"
    ],
    "full_name": "annacprice/containerCI-test",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containerci-test\" class=\"anchor\" href=\"#containerci-test\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainerCI-test\u003c/h1\u003e\n\u003cp\u003eAutomated Docker Hub and Singularity Hub builds\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-current-workflow\" class=\"anchor\" href=\"#current-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCurrent Workflow\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eCreate/edit Dockerfile\u003c/li\u003e\n\u003cli\u003eUse spython to translate Dockerfile to Singularity recipe\u003c/li\u003e\n\u003cli\u003eSet tag and push changes to GitHub, which triggers Docker Hub and Singularity Hub builds\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-downloading-images\" class=\"anchor\" href=\"#downloading-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownloading Images\u003c/h2\u003e\n\u003cp\u003eTo download all the latest docker images associated with this repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://raw.githubusercontent.com/annacprice/containerCI-test/master/scripts/docker_download.sh\n./docker_download.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo download all the latest singularity images associated with this repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://raw.githubusercontent.com/annacprice/containerCI-test/master/scripts/singularity_download.sh\n./singularity_download.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-image-repositories\" class=\"anchor\" href=\"#image-repositories\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImage Repositories\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/u/annacprice\" rel=\"nofollow\"\u003ehttps://hub.docker.com/u/annacprice\u003c/a\u003e \u003cbr\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4576\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/4576\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1603930984.0
  },
  {
    "data_format": 2,
    "description": "nextflow training course at https://seqera.io/training/",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "bunop/nextflow-training",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nextflow-training\" class=\"anchor\" href=\"#nextflow-training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enextflow-training\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-some-examples\" class=\"anchor\" href=\"#run-some-examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun some examples\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run combineInput.nf -resume -process.echo\n$ nextflow run inputRepeaters.nf -resume -process.echo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use-containers-with-nextflow\" class=\"anchor\" href=\"#how-to-use-containers-with-nextflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to use containers with nextflow\u003c/h2\u003e\n\u003cp\u003eCreate a file \u003ccode\u003enextflow.config\u003c/code\u003e (mind \u003ccode\u003e\u0027\u0027\u003c/code\u003e for delimiting the paramters):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eprocess.container=\u0027nextflow/rnaseq-nf\u0027\ndocker.runOptions=\u0027-u $(id -u):$(id -g)\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePull a docker image:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ docker pull nextflow/rnaseq-nf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen call nextflow with \u003ccode\u003ewith-docker\u003c/code\u003e parameter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run script2.nf -with-docker\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAdding:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker.enabled=true\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto \u003ccode\u003enextflow.config\u003c/code\u003e let to avoid specifing \u003ccode\u003e-with-docker\u003c/code\u003e parameter when calling nextflow\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-override-parameters-using-cmd\" class=\"anchor\" href=\"#override-parameters-using-cmd\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverride parameters using cmd\u003c/h2\u003e\n\u003cp\u003eOverride the default \u003ccode\u003ereads\u003c/code\u003e parameter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run script3.nf --reads \u0027data/ggal/*_{1,2}.fq\u0027 -resume\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCalling:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run script4.nf\n$ nextflow run script4.nf -resume --reads \u0027data/ggal/*_{1,2}.fq\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewill execute only the new calculations and will re-use the old computed results\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-calling-a-nextflow-project-using-git\" class=\"anchor\" href=\"#calling-a-nextflow-project-using-git\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCalling a nextflow project using git\u003c/h2\u003e\n\u003cp\u003ecalling nextflow on this git repository: \u003ca href=\"https://github.com/nextflow-io/rnaseq-nf.git\"\u003ehttps://github.com/nextflow-io/rnaseq-nf.git\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run nextflow-io/rnaseq-nf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI can call a specific revison of a nextflow git pipeline. Or by calling:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run rnaseq-nf -with-docker -with-report -with-trace -with-timeline -with-dag dag.png -resume\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI can generate some useful reports\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1606151572.0
  },
  {
    "data_format": 2,
    "description": "Files and resources to build Simvascular\u0027s svpre, svpost, svsolver on a supercomputing cluster.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "aksa2832/Simvascular-on-cluster",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-simvascular-on-cluster\" class=\"anchor\" href=\"#simvascular-on-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimvascular-on-cluster\u003c/h1\u003e\n\u003cp\u003eFiles and resources to :\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBuild Simvascular\u0027s svpre, svpost, svsolver on a supercomputing cluster.\u003c/li\u003e\n\u003cli\u003eSubmit a simulation job using svsolver.\u003c/li\u003e\n\u003cli\u003eCheck the progress of your simulation\u003c/li\u003e\n\u003cli\u003eDownload the generated files after the simulation is completed\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote : the following instructions have been written for use on University of Colorado Boulder\u0027s research computing account.\nPlease consult your supercomputing facility if you are affiliated with a different cluster, as some steps might be unique for your cluster account system.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-1-to-build-a-container-for-use-on-the-cluster\" class=\"anchor\" href=\"#1-to-build-a-container-for-use-on-the-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. To build a container for use on the cluster\u003c/h2\u003e\n\u003cp\u003eYou can directly pull this \u003ca href=\"https://singularity-hub.org/collections/4568\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e ready built container onto your supercomputing cluster account\u0027s project directory (the directory intended to store software builds and smaller data sets) by:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLogin to your cluster account\u003c/li\u003e\n\u003cli\u003ego to projects directory\u003c/li\u003e\n\u003cli\u003essh into compile node\u003c/li\u003e\n\u003cli\u003eload singularity module\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  $ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e projects/\u003cspan class=\"pl-smi\"\u003e$USER\u003c/span\u003e/\n  $ ssh scompile\n  $ module load singularity/\n  $ mkdir svsolver-container-build\n  $ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e svsolver-container-build\n  $ singularity pull simvascular-svsolver.sif shub://aksa2832/Simvascular-on-cluster\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eOR\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreate a public github repository and copy the 3 build here files to your github repo.\nBuild files for CU Boulder\u0027s supercomputing cluster: quick-build-linux.sh, quick-build-centos7-open-mpi.sh, Singularity (text file containing recipe for the build)\u003c/li\u003e\n\u003cli\u003eThen, login to \u003ca href=\"https://singularity-hub.org\" rel=\"nofollow\"\u003ehttps://singularity-hub.org\u003c/a\u003e with your github credentials.\u003c/li\u003e\n\u003cli\u003eGo to \u003cstrong\u003eMy collections\u003c/strong\u003e -\u0026gt; \u003cstrong\u003eAdd collection\u003c/strong\u003e -\u0026gt; choose your new github repository with the build files and enter the recipe file name as \u003cstrong\u003eSingularity\u003c/strong\u003e.\nThis will automatically build the container for you and place it on singularity hub. Here is how it should be after build: \u003ca href=\"https://singularity-hub.org/collections/4568\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eDownload the container to your supercomputing cluster account\u0027s project directory (the directory intended to store software builds and smaller data sets) by:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e projects/\u003cspan class=\"pl-smi\"\u003e$USER\u003c/span\u003e/\n$ ssh scompile\n$ module load singularity/\n$ mkdir svsolver-container-build\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e svsolver-container-build\n$ singularity pull simvascular-svsolver.sif shub://\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eyourgithubusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esvsolver_github_reponame\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-to-submit-a-job-prepare-data-folder-and-job-script\" class=\"anchor\" href=\"#2-to-submit-a-job-prepare-data-folder-and-job-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. To submit a job: Prepare data-folder and job-script\u003c/h2\u003e\n\u003cp\u003eStep 1: Create the data files for the job with \u003cstrong\u003eSimvascular application\u003c/strong\u003e on your local computer.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen SimVascular\u003c/li\u003e\n\u003cli\u003eRight click the job node (eg. \"test-simvascular-project\") under \u003cstrong\u003eSimulations\u003c/strong\u003e in Data Manager\u003c/li\u003e\n\u003cli\u003eClick \"Export Data Files\"\u003c/li\u003e\n\u003cli\u003eSelect a folder for exporting(eg. \"test-simvascular-project-data\").\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 2: Write and load a solver script.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWrite the script: Follow instructions given in the template file \"sample_jobscript\" and modify the parameters according to your requirements.\u003c/li\u003e\n\u003cli\u003eSave the script as your-job-script.sh to the folder containing the exported data files from SimVascular after step 1(\"test-simvascular-project-data\") .\nThis folder should then contain all the Data Files and the job script required for the simulation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 3: Upload this data-folder on your supercomputing cluster account\u0027s working directory(usually /scratch/$USER).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen terminal(or equivalent command prompt).\u003c/li\u003e\n\u003cli\u003eSecure copy the data-folder(\"test-simvascular-project-data\") onto the cluster\u0027s working directory. Here @login.rc.colorado.edu is the login id you use for cluster account.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ scp -rv \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e/path/to/test-simvascular-project-data/on/your/computer\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e@login.rc.colorado.edu:\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e/target/path/to/working/directory/on/cluster/\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-submit-the-job\" class=\"anchor\" href=\"#2-submit-the-job\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Submit the job:\u003c/h2\u003e\n\u003cp\u003eStep 4: Submit the simulation job.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLogin to your cluster account\u003c/li\u003e\n\u003cli\u003enavigate to the working directory(eg. /scratch/$USER/test-simvascular-project-data) from where the simulation will be launched.\u003c/li\u003e\n\u003cli\u003eEnter the following commands to submit the job:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ module load slurm/summit\n$ sbatch your-job-script.sh\u003c/pre\u003e\u003c/div\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-3-check-the-progress-of-your-simulation\" class=\"anchor\" href=\"#3-check-the-progress-of-your-simulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Check the progress of your simulation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eIf you have enabled email notification in you job script, you will be notified as soon as the simulation starts and the moment it ends via. email.\u003c/li\u003e\n\u003cli\u003eFind information on your job\u2019s start time using the squeue command:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ squeue --user=your_rc-username --start\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eTo check the status/progress of the submitted job\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sacct --starttime=YYYY-MM-DD --format=User,JobName,JobId,partition,state,time,start,end,elapsed,MaxRss,MaxVMSize,nnodes,ncpus,nodelist\n\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eFinally check the output file(.out file) generated in \u0026lt;/scratch/$USER/test-simvascular-project-data\u0026gt; for simulation performance details.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-4-download-generated-files\" class=\"anchor\" href=\"#4-download-generated-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. Download generated files\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCompress the folder containing generated files(will be named n-procs_case for n cores used):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ tar -cvzf \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-k\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e\u003cspan class=\"pl-k\"\u003en-cores\u0026gt;-procs_case\u0026gt;.tar.gz\u003c/span\u003e \u0026lt;/scratch/summit/$USER/test-simvascular-project-data/\u0026lt;n-cores\u0026gt;-procs_case\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eUse globus file transfer or secure copy the zipped folder on your local computer:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003escp -rv \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e@login.rc.colorado.edu:\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e/scratch/summit/\u003cspan class=\"pl-smi\"\u003e$USER\u003c/span\u003e/test-simvascular-project-data/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003en-cores\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e-procs_case.tar.gz\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etarget/storage/path/on/your/computer\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1606621737.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for a deep learning (pytorch) environment + GPU support",
    "filenames": [
      "Singularity.1.0.0"
    ],
    "full_name": "manuel-munoz-aguirre/singularity-pytorch-gpu",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-pytorch-gpu\" class=\"anchor\" href=\"#singularity-pytorch-gpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-pytorch-gpu\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4969\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for a deep learning (pytorch) environment + GPU support (cuda-10.2). Contains libraries to perform common ML tasks. \u003ccode\u003eOpenslide\u003c/code\u003e is included to manipulate whole-slide histology images, \u003ccode\u003eimagemagick\u003c/code\u003e for general image manipulation. \u003ccode\u003eJupyterLab\u003c/code\u003e and \u003ccode\u003ecode-server\u003c/code\u003e (VS Code) are also included in the image. This image has been tested in an HPC (SGE) with distributed pytorch applications.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-singularity\" class=\"anchor\" href=\"#installing-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling singularity\u003c/h2\u003e\n\u003cp\u003eTo install singularity, see the \u003ca href=\"https://sylabs.io/guides/3.6/admin-guide/installation.html#installation-on-linux\" rel=\"nofollow\"\u003eofficial docs\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-buildingdownloading-the-image\" class=\"anchor\" href=\"#buildingdownloading-the-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding/downloading the image\u003c/h2\u003e\n\u003cp\u003eTo build an image called \u003ccode\u003etorchenv.sif\u003c/code\u003e based on the definition file \u003ccode\u003eSingularity.1.0.0\u003c/code\u003e, an NVIDIA GPU and \u003ccode\u003ecuda-10.2\u003c/code\u003e drivers must be available on the host system. Clone this repository, move into it and run the singularity build command.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/manuel-munoz-aguirre/singularity-pytorch-gpu.git \u0026amp;\u0026amp; \\\ncd singularity-pytorch-gpu \u0026amp;\u0026amp; \\\nsudo singularity build torchenv.sif Singularity.1.0.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOtherwise, the image can be pulled directly from singularity hub:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull torchenv.sif shub://manuel-munoz-aguirre/singularity-pytorch-gpu:1.0.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-the-container\" class=\"anchor\" href=\"#using-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the container\u003c/h2\u003e\n\u003cp\u003eTo spawn an interactive shell within the container, use the command below. The \u003ccode\u003e--nv\u003c/code\u003e flag setups the container to use NVIDIA GPUs (read more \u003ca href=\"https://sylabs.io/guides/3.6/user-guide/gpu.html\" rel=\"nofollow\"\u003ehere\u003c/a\u003e).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell --nv torchenv.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run a script (for example, \u003ccode\u003escript.py\u003c/code\u003e) using the container without starting an interactive shell:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec --nv torchenv.sif python3 script.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe container can also be launched and used on a system without a GPU, but upon startup it will display a warning about missing NVIDIA binaries on the host.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "pytorch",
      "deep-learning",
      "machine-learning",
      "environment"
    ],
    "updated_at": 1612808313.0
  },
  {
    "data_format": 2,
    "description": "The dwMRI preprocessing leg of the TVB processing pipeline. Initial version cloned from BIDS-Apps/MRtrix3_connectome.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "BrainModes/tvb-pipeline-sc",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tvb-pipeline-sc\" class=\"anchor\" href=\"#tvb-pipeline-sc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etvb-pipeline-sc\u003c/h1\u003e\n\u003cp\u003eThe dwMRI preprocessing leg of the TVB processing pipeline. Initial version cloned from BIDS-Apps/MRtrix3_connectome.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1591895471.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "pwiszniewski/SingularityTest",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tvb-pipeline-sc\" class=\"anchor\" href=\"#tvb-pipeline-sc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etvb-pipeline-sc\u003c/h1\u003e\n\u003cp\u003eThe dwMRI preprocessing leg of the TVB processing pipeline. Initial version cloned from BIDS-Apps/MRtrix3_connectome.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605105699.0
  },
  {
    "data_format": 2,
    "description": "UNDER CONSTRUCTION - Scripts for the workshop on OpenFOAM containers",
    "filenames": [
      "04_buildingAnOpenFOAMContainer/openfoam-2.4.x/02_PortingToSingularity/Singularity.def"
    ],
    "full_name": "PawseySC/containers-openfoam-workshop-scripts",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-workshop-on-the-usage-of-openfoam-containers-at-pawsey\" class=\"anchor\" href=\"#workshop-on-the-usage-of-openfoam-containers-at-pawsey\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkshop on the usage of OpenFOAM containers at Pawsey\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eOrganisers\u003c/strong\u003e: Alexis Espinosa (PawseySC) and Marco De La Pierre (PawseySC)\u003c/p\u003e\n\u003cp\u003eThe use of containers has become an attractive framework for several areas of research supported by Pawsey (including bioinformatics and machine learning, among others).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNow, Pawsey supports the usage of OpenFOAM containers.\u003c/strong\u003e For the most recent versions of OpenFOAM (and some others), Pawsey have prebuilt and tested Singularity containers.\u003c/p\u003e\n\u003cp\u003eThis repository contains material for the exercises of the workshop.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep-by-step guide\u003c/strong\u003e: \u003ca href=\"https://pawseysc.github.io/containers-openfoam-workshop\" rel=\"nofollow\"\u003ehttps://pawseysc.github.io/containers-openfoam-workshop\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1619177799.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "src/addons/PhysiBoSSa/MaBoSS-env-2.0/containers/singularity/Singularity"
    ],
    "full_name": "pradas/pba_spheroidTNF",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-physibossa-nanohub-spheroidtnf\" class=\"anchor\" href=\"#physibossa-nanohub-spheroidtnf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePhysiBoSSa nanoHub SpheroidTNF\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1589290842.0
  },
  {
    "data_format": 2,
    "description": "The main platform for sharing all the information about AI/HPC-related containers at C3SE. See also the included tutorial to get started. ",
    "filenames": [
      "Julia/Singularity.Julia-v1.4.0",
      "ambertools/Singularity.ambertools-v20",
      "FEniCS/Singularity.FEniCS",
      "PyTorch/Singularity.PyTorch-v1.5.0-py3",
      "PyTorch/Singularity.PyTorch-v1.6.0-py3",
      "PyTorch/Singularity.PyTorch-v1.7.0-py3",
      "TensorFlow/Singularity.TensorFlow-v2.2.0-tf2-py3-NGC-R20.08",
      "TensorFlow/Singularity.TensorFlow-v2.3.1-tf2-py3-GPU-Jupyter",
      "TensorFlow/Singularity.TensorFlow-v2.1.0-tf2-py3-NGC-R20.03"
    ],
    "full_name": "c3se/containers",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4791\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-container-based-solutions-on-c3se-clusters\" class=\"anchor\" href=\"#using-container-based-solutions-on-c3se-clusters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing container-based solutions on C3SE clusters\u003c/h1\u003e\n\u003cp\u003eContainer technology has a number of advantages over the traditional workflow of using scientific software. The singularity flavour, in particular, targets reproducibility, performance and security with respect to running software in an HPC environment. Here, we provide our containerized solutions at C3SE. For each of the provided containers, read the specific instructions in the corresponding folder to easily get started with using them in your workflow. The actual container images are hosted on Singularity Hub. Click on the badge above to quickly get access to them!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-missing-containers-updates-and-troubleshooting\" class=\"anchor\" href=\"#missing-containers-updates-and-troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMissing containers, updates, and troubleshooting\u003c/h2\u003e\n\u003cp\u003eWe continuously add more packages to this repository. If you can\u0027t find a relevant container for your needs, or in the case of encountering any errors or deprecated features in the material, feel free to contact us: \u003ca href=\"mailto:support@c3se.chalmers.se\"\u003esupport@c3se.chalmers.se\u003c/a\u003e or open a pull-request.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "singularity-containers",
      "docker",
      "hpc"
    ],
    "updated_at": 1605120969.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "basicInstallations/polar-bear/02_PortingToSingularity/Singularity.def",
      "basicInstallations/pangolin/02_PortingToSingularity/Singularity.def"
    ],
    "full_name": "alexisespinosa-uptake/condaContainers",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4791\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-container-based-solutions-on-c3se-clusters\" class=\"anchor\" href=\"#using-container-based-solutions-on-c3se-clusters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing container-based solutions on C3SE clusters\u003c/h1\u003e\n\u003cp\u003eContainer technology has a number of advantages over the traditional workflow of using scientific software. The singularity flavour, in particular, targets reproducibility, performance and security with respect to running software in an HPC environment. Here, we provide our containerized solutions at C3SE. For each of the provided containers, read the specific instructions in the corresponding folder to easily get started with using them in your workflow. The actual container images are hosted on Singularity Hub. Click on the badge above to quickly get access to them!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-missing-containers-updates-and-troubleshooting\" class=\"anchor\" href=\"#missing-containers-updates-and-troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMissing containers, updates, and troubleshooting\u003c/h2\u003e\n\u003cp\u003eWe continuously add more packages to this repository. If you can\u0027t find a relevant container for your needs, or in the case of encountering any errors or deprecated features in the material, feel free to contact us: \u003ca href=\"mailto:support@c3se.chalmers.se\"\u003esupport@c3se.chalmers.se\u003c/a\u003e or open a pull-request.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1592319832.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "MariusCausemann/brain-inversion",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4791\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-container-based-solutions-on-c3se-clusters\" class=\"anchor\" href=\"#using-container-based-solutions-on-c3se-clusters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing container-based solutions on C3SE clusters\u003c/h1\u003e\n\u003cp\u003eContainer technology has a number of advantages over the traditional workflow of using scientific software. The singularity flavour, in particular, targets reproducibility, performance and security with respect to running software in an HPC environment. Here, we provide our containerized solutions at C3SE. For each of the provided containers, read the specific instructions in the corresponding folder to easily get started with using them in your workflow. The actual container images are hosted on Singularity Hub. Click on the badge above to quickly get access to them!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-missing-containers-updates-and-troubleshooting\" class=\"anchor\" href=\"#missing-containers-updates-and-troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMissing containers, updates, and troubleshooting\u003c/h2\u003e\n\u003cp\u003eWe continuously add more packages to this repository. If you can\u0027t find a relevant container for your needs, or in the case of encountering any errors or deprecated features in the material, feel free to contact us: \u003ca href=\"mailto:support@c3se.chalmers.se\"\u003esupport@c3se.chalmers.se\u003c/a\u003e or open a pull-request.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1609864119.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.pytorch"
    ],
    "full_name": "ulorentz/wilson_cluster",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4791\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-container-based-solutions-on-c3se-clusters\" class=\"anchor\" href=\"#using-container-based-solutions-on-c3se-clusters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing container-based solutions on C3SE clusters\u003c/h1\u003e\n\u003cp\u003eContainer technology has a number of advantages over the traditional workflow of using scientific software. The singularity flavour, in particular, targets reproducibility, performance and security with respect to running software in an HPC environment. Here, we provide our containerized solutions at C3SE. For each of the provided containers, read the specific instructions in the corresponding folder to easily get started with using them in your workflow. The actual container images are hosted on Singularity Hub. Click on the badge above to quickly get access to them!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-missing-containers-updates-and-troubleshooting\" class=\"anchor\" href=\"#missing-containers-updates-and-troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMissing containers, updates, and troubleshooting\u003c/h2\u003e\n\u003cp\u003eWe continuously add more packages to this repository. If you can\u0027t find a relevant container for your needs, or in the case of encountering any errors or deprecated features in the material, feel free to contact us: \u003ca href=\"mailto:support@c3se.chalmers.se\"\u003esupport@c3se.chalmers.se\u003c/a\u003e or open a pull-request.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1586557138.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "auze7347/Singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4791\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-container-based-solutions-on-c3se-clusters\" class=\"anchor\" href=\"#using-container-based-solutions-on-c3se-clusters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing container-based solutions on C3SE clusters\u003c/h1\u003e\n\u003cp\u003eContainer technology has a number of advantages over the traditional workflow of using scientific software. The singularity flavour, in particular, targets reproducibility, performance and security with respect to running software in an HPC environment. Here, we provide our containerized solutions at C3SE. For each of the provided containers, read the specific instructions in the corresponding folder to easily get started with using them in your workflow. The actual container images are hosted on Singularity Hub. Click on the badge above to quickly get access to them!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-missing-containers-updates-and-troubleshooting\" class=\"anchor\" href=\"#missing-containers-updates-and-troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMissing containers, updates, and troubleshooting\u003c/h2\u003e\n\u003cp\u003eWe continuously add more packages to this repository. If you can\u0027t find a relevant container for your needs, or in the case of encountering any errors or deprecated features in the material, feel free to contact us: \u003ca href=\"mailto:support@c3se.chalmers.se\"\u003esupport@c3se.chalmers.se\u003c/a\u003e or open a pull-request.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1604388527.0
  },
  {
    "data_format": 2,
    "description": "Imputation pipeline",
    "filenames": [
      "Singularity/Singularity.v1.0"
    ],
    "full_name": "IARCbioinfo/Imputation-nf",
    "latest_release": "v1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-genotyping-imputation---pipeline-v10\" class=\"anchor\" href=\"#genotyping-imputation---pipeline-v10\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenotyping imputation  : Pipeline V1.0\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-a-nextflow-pipeline-to-realise-a-datasets-genotyping-imputation\" class=\"anchor\" href=\"#a-nextflow-pipeline-to-realise-a-datasets-genotyping-imputation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA nextflow pipeline to realise a dataset\u0027s genotyping imputation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/IARCbioinfo/Imputation-nf\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e3a0e94b24410397271294a485f985c85941b292ba2c7cbf3fefb26bf1e0c76b/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f74656d706c6174652d6e662e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/IARCbioinfo/template-nf.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/iarcbioinfo/imputation-nf/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c5d5383ee3d6248c7d31b5fcaa21fc7d689b53ecb330dbfe628cd1fae38c853/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d72656164792d626c75652e737667\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/badge/docker-ready-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4533\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://zenodo.org/badge/latestdoi/94193130\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5decad826d7116b1c950b6b48c06052496f141e803525b088ce3cdcaaa4d7b88/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f39343139333133302e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/94193130.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"template-nf.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"template-nf.png\" alt=\"Workflow representation\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe pipeline used to perform the imputation of several targets datasets processed with standard input.\u003c/p\u003e\n\u003cp\u003eHere is a summary of the method :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePreprocessing of data : by using the nextflow script Preparation.nf with create a directory \"file/\" with all the dependencies.\u003c/li\u003e\n\u003cli\u003eFirst step : Origin estimation of sample from the target dataset by using admixture tools and the hapmap dataset as reference.\u003c/li\u003e\n\u003cli\u003eSecond step : Series of SNPs filters and quality checking from the target dataset before the imputation step.\u003c/li\u003e\n\u003cli\u003eThird step : VCF production\u003c/li\u003e\n\u003cli\u003eLast step : Phasing and imputation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee the Usage section to test the full pipeline with your target dataset.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eThe pipeline works under Linux distributions.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThis pipeline is based on \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003enextflow\u003c/a\u003e. As we have several nextflow pipelines, we have centralized the common information in the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository. Please read it carefully as it contains essential information for the installation, basic usage and configuration of nextflow and our pipelines.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExternal software:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eLiftOver : conda install ucsc-liftover\u003c/li\u003e\n\u003cli\u003ePlink (PLINK v1.90b6.12 64-bit (28 Oct 2019)) : conda install plink\u003c/li\u003e\n\u003cli\u003eAdmixture (ADMIXTURE Version 1.3.0) : conda install admixture\u003c/li\u003e\n\u003cli\u003ePerl : conda install perl\u003c/li\u003e\n\u003cli\u003eTerm::ReadKey module : conda install perl-termreadkey\u003c/li\u003e\n\u003cli\u003eBcfTools : conda install bcftools\u003c/li\u003e\n\u003cli\u003eeagle 2.4.1 : \u003ca href=\"https://data.broadinstitute.org/alkesgroup/Eagle/#x1-50002.2\" rel=\"nofollow\"\u003eSee instructions\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eminimac4 : conda install cmake ; pip install cget ; git clone \u003ca href=\"https://github.com/statgen/Minimac4.git\"\u003ehttps://github.com/statgen/Minimac4.git\u003c/a\u003e ; cd Minimac4 ; bash install.sh\u003c/li\u003e\n\u003cli\u003eSamtools : conda install samtools\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eFile to download :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"zzz.bwh.harvard.edu/plink/dist/hapmap_r23a.zip\"\u003eHapmap Dataset\u003c/a\u003e : as reference\u0027s dataset for admixture\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.hagsc.org/hgdp/data/hgdp.zip\" rel=\"nofollow\"\u003eHGDP Dataset\u003c/a\u003e : for the dataset\u0027s test, you have to use the toMap.py \u0026amp; toPed.py in the \u0027converstion\u0027 directory to convert files in the .map/.ped plink format. Next you have to convert this last output in the .bed/.bam/.fam plink format by using plink line command and run the imputation\u0027s pipeline.\u003c/li\u003e\n\u003cli\u003ePerl tool : \u003ca href=\"https://www.well.ox.ac.uk/~wrayner/tools/\" rel=\"nofollow\"\u003eHRC-1000G-check-bim-NoReadKey.pl\u003c/a\u003e \u0026amp; \u003ca href=\"https://www.well.ox.ac.uk/~wrayner/tools/1000GP_Phase3_combined.legend.gz\" rel=\"nofollow\"\u003e1000GP_Phase3_combined.legend\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eLiftOver tool : \u003ca href=\"http://hgdownload.cse.ucsc.edu/goldenpath/hg19/liftOver/hg19ToHg38.over.chain.gz\" rel=\"nofollow\"\u003ehg19ToHg38.over.chain\u003c/a\u003e \u0026amp; \u003ca href=\"http://hgdownload.cse.ucsc.edu/goldenpath/hg18/liftOver/hg18ToHg38.over.chain.gz\" rel=\"nofollow\"\u003ehg18ToHg38.over.chain\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ePeparation dataset tool : \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2432498/bin/pone.0002551.s003.xls\" rel=\"nofollow\"\u003epone.0002551.s003.xls\u003c/a\u003e (Convert it in .csv format)\u003c/li\u003e\n\u003cli\u003eAdmixture tool : relationships_w_pops_121708.txt\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/zhanxw/checkVCF/raw/master/checkVCF.py\"\u003eCheckVCF\u003c/a\u003e, \u003ca href=\"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz\" rel=\"nofollow\"\u003eFasta file in V37\u003c/a\u003e \u0026amp; \u003ca href=\"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/\" rel=\"nofollow\"\u003eFasta file in V38\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/\" rel=\"nofollow\"\u003e1000G Reference in Hg38\u003c/a\u003e with the \u003ca href=\"https://data.broadinstitute.org/alkesgroup/Eagle/#x1-320005.3.2\" rel=\"nofollow\"\u003edoc\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCreate \u003ca href=\"https://imputationserver.readthedocs.io/en/latest/create-reference-panels/#create-legend-files\" rel=\"nofollow\"\u003elegend\u003c/a\u003e, \u003ca href=\"https://data.broadinstitute.org/alkesgroup/Eagle/#x1-320005.3.2\" rel=\"nofollow\"\u003ebcf\u003c/a\u003e \u0026amp; \u003ca href=\"https://imputationserver.readthedocs.io/en/latest/create-reference-panels/#create-m3vcf-files\" rel=\"nofollow\"\u003em3vcf\u003c/a\u003e files for the reference\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eOther to know :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSee the Usage part to create the environment to run the pipeline. All the necessary dependencies are download with the using of the script Preparation.nf. To run it, you\u0027ll need to install the next software : in2csv(1.0.5), liftOver, plink, Minimac3(2.0.1) \u0026amp; bcftools\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can avoid installing all the external software of the main scritp by only installing Docker. See the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository for more information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ePlink datasets\u003c/td\u003e\n\u003ctd\u003eCorresponds to the target dataset to be analysed. Composed by the following files : bed, bim \u0026amp; fam\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eInput environment\u003c/td\u003e\n\u003ctd\u003ePath to your input directory\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-mandatory\" class=\"anchor\" href=\"#mandatory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMandatory\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eExample value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--target\u003c/td\u003e\n\u003ctd\u003emy_target\u003c/td\u003e\n\u003ctd\u003ePattern of the target dataset which do the link with the file .bed/.bim./fam for plink\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--input\u003c/td\u003e\n\u003ctd\u003euser/main_data/\u003c/td\u003e\n\u003ctd\u003eThe path of the main directory where we can find 2 directory : my_target/ + files/\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--output\u003c/td\u003e\n\u003ctd\u003euser/my_result/\u003c/td\u003e\n\u003ctd\u003eThe path of the main directory where you want to place your results\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptional\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDefault value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--script\u003c/td\u003e\n\u003ctd\u003emy/directory/script/bin\u003c/td\u003e\n\u003ctd\u003eThe path of the bin script directory, to be able to run the annexe programme grom the pipeline\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--geno1\u003c/td\u003e\n\u003ctd\u003e0.03\u003c/td\u003e\n\u003ctd\u003eFirst genotyping call rate plink threshold, apply in the full target dataset\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--geno2\u003c/td\u003e\n\u003ctd\u003e0.03\u003c/td\u003e\n\u003ctd\u003eSecond genotyping call rate plink threshold, apply in the target dataset divide by population\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--maf\u003c/td\u003e\n\u003ctd\u003e0.01\u003c/td\u003e\n\u003ctd\u003eMinor allele frequencies plink threshold, apply in the full target dataset\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--pihat\u003c/td\u003e\n\u003ctd\u003e0.185\u003c/td\u003e\n\u003ctd\u003eMinimum pi_hat value use for the relatedness test, 0.185 is halfway between the expected IBD for third- and second-degree relatives\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--hwe\u003c/td\u003e\n\u003ctd\u003e1e-8\u003c/td\u003e\n\u003ctd\u003eHardy-Weinberg Equilibrium plink p-value threshold\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--legend\u003c/td\u003e\n\u003ctd\u003eALL.chr_GRCh38.genotypes.20170504.legend\u003c/td\u003e\n\u003ctd\u003eFile to use as .legend\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--fasta\u003c/td\u003e\n\u003ctd\u003eGRCh38_full_analysis_set_plus_decoy_hla.fa\u003c/td\u003e\n\u003ctd\u003eFile to use as fasta reference\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--chain\u003c/td\u003e\n\u003ctd\u003ehg18ToHg38.over.chain\u003c/td\u003e\n\u003ctd\u003eFile to use as liftover conversion\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--VCFref\u003c/td\u003e\n\u003ctd\u003emy/directory/ref/vcf/\u003c/td\u003e\n\u003ctd\u003eDirectory to use as VCF reference\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--BCFref\u003c/td\u003e\n\u003ctd\u003emy/directory/ref/bcf/\u003c/td\u003e\n\u003ctd\u003eDirectory to use as BCF reference\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--M3VCFref\u003c/td\u003e\n\u003ctd\u003emy/directory/ref/m3vcf/\u003c/td\u003e\n\u003ctd\u003eDirectory to use as M3VCF reference\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--conversion\u003c/td\u003e\n\u003ctd\u003ehg38/hg18/hg19\u003c/td\u003e\n\u003ctd\u003eOption to convert data from hg18 to HG38 version of the genome. Standard value is hg38\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--cloud\u003c/td\u003e\n\u003ctd\u003ehg38/hg18/hg19\u003c/td\u003e\n\u003ctd\u003eOption to convert data from hg18 to HG38 version of the genome. Standard value is hg38\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--token_Michighan\u003c/td\u003e\n\u003ctd\u003epath/to/my_token.txt\u003c/td\u003e\n\u003ctd\u003eOption to convert data from hg18 to HG38 version of the genome. Standard value is hg38\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--token_TOPMed\u003c/td\u003e\n\u003ctd\u003epath/to/my_token.txt\u003c/td\u003e\n\u003ctd\u003eOption to convert data from hg18 to HG38 version of the genome. Standard value is hg38\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--QC_cloud\u003c/td\u003e\n\u003ctd\u003emy/directory/donwload_imputation_server\u003c/td\u003e\n\u003ctd\u003eOption to convert data from hg18 to HG38 version of the genome. Standard value is hg38\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-flags\" class=\"anchor\" href=\"#flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFlags\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlags are special parameters without value.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--help\u003c/td\u003e\n\u003ctd\u003eDisplay help\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003ePrepare the environment to run the imputation pipeline.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir data\ncd data\nnextflow run IARCbioinfo/Imputation-nf/bin/Preparation.nf --out /data/\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003ePaste the bim/bed/fam plink target files in a directory, and the directory in your \"data/\" directory. You have to call the plink files and your directory with the same pattern, as the following exemple : data/target/target{.bed,.bim,.fam}. So now you have 2 directories in your \"data/\" repertory :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e_ data/my_target/ : with the plink target files (my_target.bed, my_target.bim, my_target.fam).\u003c/p\u003e\n\u003cp\u003e_ data/files/ : with all the dependencies.\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eRun the imputation pipeline.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run IARCbioinfo/Imputation.nf --target my_target --input /data/ --output /results/ -r v1.0 -profile singularity \n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eIf you want to run the imputation in one of the server (Michigan and/or TOPMed Imputation), you need you write your token acces in a file and to give it in argument. For example :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run IARCbioinfo/Imputation.nf --target my_target --input /data/ --output /results/ --cloud on --token_Michighan /folder/my_token_Michighan.txt --token_TOPMed /folder/my_token_TOPMed.txt -r v1.0 -profile singularity \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce your imputation data is downloaded, you can run the end of the QC analysis :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run IARCbioinfo/Imputation.nf --target my_target --input /data/ --output /results/ --QC_cloud /downloaded_imputation_server_file/ -r v1.0 -profile singularity \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eoutput1\u003c/td\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eoutput2\u003c/td\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-detailed-description-optional-section\" class=\"anchor\" href=\"#detailed-description-optional-section\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetailed description (optional section)\u003c/h2\u003e\n\u003cp\u003e...\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directed-acyclic-graph\" class=\"anchor\" href=\"#directed-acyclic-graph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirected Acyclic Graph\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://htmlpreview.github.io/?https://github.com/IARCbioinfo/Imputation-nf/blob/master/dag.html\" rel=\"nofollow\"\u003e\u003cimg src=\"dag.png\" alt=\"DAG\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions\" class=\"anchor\" href=\"#contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eEmail\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eGabriel Aur\u00e9lie\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:gabriela@students.iarc.fr\"\u003egabriela@students.iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper to contact for support\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLipinski Boris\u003c/td\u003e\n\u003ctd\u003e\n\u003ca href=\"mailto:LipinskiB@students.iarc.fr\"\u003eLipinskiB@students.iarc.fr\u003c/a\u003e / \u003ca href=\"mailto:boris.lipinski@etu.univ-lyon1.fr\"\u003eboris.lipinski@etu.univ-lyon1.fr\u003c/a\u003e\n\u003c/td\u003e\n\u003ctd\u003eDeveloper to contact for support\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references-optional\" class=\"anchor\" href=\"#references-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences (optional)\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-faq-optional\" class=\"anchor\" href=\"#faq-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFAQ (optional)\u003c/h2\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-test-pipeline\" class=\"anchor\" href=\"#test-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etest-pipeline\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1614573597.0
  },
  {
    "data_format": 2,
    "description": "The pipeline for analyzing the UKBiobank data, with variant annotation tool VEP and some in-house tools",
    "filenames": [
      "container/Singularity.vep-96.0"
    ],
    "full_name": "HealthML/Nextflow-pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nextflow-pipeline\" class=\"anchor\" href=\"#nextflow-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextflow-pipeline\u003c/h1\u003e\n\u003cp\u003eThe current pipeline analyze the exon-seq data generated from either of Regeneron\u2019s own pipeline \u003ccode\u003e(SPB)\u003c/code\u003e or Functionally\nEquivalent \u003ccode\u003e(FE)\u003c/code\u003e piplines from UKbiobank. Initially, the raw input data (\u003ccode\u003e.bed\u003c/code\u003e,\u003ccode\u003e.fam\u003c/code\u003e, \u003ccode\u003e.bai\u003c/code\u003e) are filtered and converted to vcf files using two \u003ccode\u003eplink2\u003c/code\u003e processes. Then, the variants (\u003ccode\u003evcf\u003c/code\u003e file) are annotated in the third process, using the ensembleVariant Effect Predictor (\u003ccode\u003evep\u003c/code\u003e) tool. The corresponding result is then processed using an in-house tool called SEAK. The pipeline has a total of four processes. The tools used for all the four processes are containerized in the \u003ca href=\"https://github.com/HealthML/Nextflow-pipeline/blob/master/container/Dockerfile\"\u003edocker image \u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/HealthML/Nextflow-pipeline.git\ncd Nextflow-pipeline\ngit checkout dev_nf\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-nextflow\" class=\"anchor\" href=\"#nextflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextflow\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003emake install\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-docker-image-installion\" class=\"anchor\" href=\"#docker-image-installion\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker image installion\u003c/h1\u003e\n\u003cp\u003eTo install the docker image for all the process tools using Docker, run the Makefile command in the container directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd container\nmake docker-build\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-how-to-run-the-pipeline\" class=\"anchor\" href=\"#how-to-run-the-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to run the pipeline\u003c/h1\u003e\n\u003cp\u003eIn order to run the pipeline for the data generated from Regeneron\u2019s own pipeline \u003ccode\u003e(SPB)\u003c/code\u003e or Functionally\nEquivalent \u003ccode\u003e(FE)\u003c/code\u003e pipleine from UKbiobank using the VEP\u0027s cache references, please use the following command. For example, if you wanna run the samples from FE pipeline try the follwoing command on the terminal.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e./nextflow run main.nf -resume --samples ukb_FE_50k_exome_seq\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe pipeline downloads automatically hg38 fasta file. However, for the current pipeline I am using reference genome (\u003ccode\u003e.fa\u003c/code\u003e), the annoation file (\u003ccode\u003e.gtf\u003c/code\u003e) and their corresponding indexed files (\u003ccode\u003e.fai\u003c/code\u003e \u0026amp; \u003ccode\u003e.tbi\u003c/code\u003e files). For runing the pipeline using these references, please run the following command on the terminal.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./nextflow run main.nf --ref_fa /home/Alva.Rani/UKbiobank/derived/projects/kernels_VEP/Homo_sapiens.GRCh38.dna.primary_assembly.fa --gtf /home/Alva.Rani/data/reference/Homo_sapiens.GRCh38.97.gtf.gz --gtf_tbi /home/Alva.Rani/data/reference/Homo_sapiens.GRCh38.97.gtf.gz.tbi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you can access the VM server and the above mentioned folder, there is index for the reference genome.\u003c/p\u003e\n\u003cp\u003eOtherwise, you can also run the whole pipeline by using the following one liner,\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e./nextflow run main.nf\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1613765139.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v0.0.0"
    ],
    "full_name": "darachm/singularity_ncbi-entrez",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_ncbi-entrez\" class=\"anchor\" href=\"#singularity_ncbi-entrez\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_ncbi-entrez\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603628264.0
  },
  {
    "data_format": 2,
    "description": "Docker image of virus related tools",
    "filenames": [
      "Viromescan/Singularity"
    ],
    "full_name": "inextvir/Docker",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_ncbi-entrez\" class=\"anchor\" href=\"#singularity_ncbi-entrez\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_ncbi-entrez\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1609869497.0
  },
  {
    "data_format": 2,
    "description": "An agent for Azure Pipelines using a Singularity image",
    "filenames": [
      "Singularity"
    ],
    "full_name": "basnijholt/azure-singularity-agent",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wip-azure-singularity-agent\" class=\"anchor\" href=\"#wip-azure-singularity-agent\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWIP: azure-singularity-agent\u003c/h1\u003e\n\u003cp\u003eAn agent for Azure Pipelines using a Singularity image\u003c/p\u003e\n\u003cp\u003eBuild with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build azure-singularity-agent.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eenv AZP_URL=https://dev.azure.com/\u0026lt;organization\u0026gt; AZP_TOKEN=\u0026lt;PAT token\u0026gt; AZP_AGENT_NAME=mydockeragent singularity run azure-singularity-agent.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSeems to not work because the resulting \u003ccode\u003esif\u003c/code\u003e is read-only.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1583436019.0
  },
  {
    "data_format": 2,
    "description": "ZMQ-based analysis pipeline for processing Tomography data from FXI-18 at NSLS-II",
    "filenames": [
      "fxi_zmq/Singularity",
      "fxi_analysis/Singularity"
    ],
    "full_name": "HarinarayanKrishnan/fxi_analysis_pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wip-azure-singularity-agent\" class=\"anchor\" href=\"#wip-azure-singularity-agent\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWIP: azure-singularity-agent\u003c/h1\u003e\n\u003cp\u003eAn agent for Azure Pipelines using a Singularity image\u003c/p\u003e\n\u003cp\u003eBuild with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build azure-singularity-agent.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eenv AZP_URL=https://dev.azure.com/\u0026lt;organization\u0026gt; AZP_TOKEN=\u0026lt;PAT token\u0026gt; AZP_AGENT_NAME=mydockeragent singularity run azure-singularity-agent.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSeems to not work because the resulting \u003ccode\u003esif\u003c/code\u003e is read-only.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1582365848.0
  },
  {
    "data_format": 2,
    "description": "TensorFlow",
    "filenames": [
      "2.2.0-c_intelpython-Python-3.7/Singularity.2.2.0-c_intelpython-Python-3.7",
      "2.1.0-p-Python-3.7-gpu/Singularity.2.1.0-p-Python-3.7-gpu",
      "2.1.0-c_anaconda-Python-3.7/Singularity.2.1.0-c_anaconda-Python-3.7",
      "2.1.0-p-Python-3.7/Singularity.2.1.0-p-Python-3.7",
      "2.1.0-c_intel-Python-3.7/Singularity.2.1.0-c_intel-Python-3.7",
      "2.1.0-c_anaconda-Python-3.7-gpu/Singularity.2.1.0-c_anaconda-Python-3.7-gpu"
    ],
    "full_name": "fenz-org/tensorflow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-container-recipes-for-tensorflow\" class=\"anchor\" href=\"#container-recipes-for-tensorflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer recipes for TensorFlow\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1604175891.0
  },
  {
    "data_format": 2,
    "description": "EMEWS template for PhysiBoSSa",
    "filenames": [
      "data/PhysiBoSSa/addons/PhysiBoSSa/MaBoSS-env-2.0/containers/singularity/Singularity"
    ],
    "full_name": "pradas/PhysiBoSSa-EMEWS",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-emews-project-template\" class=\"anchor\" href=\"#emews-project-template\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEMEWS project template\u003c/h2\u003e\n\u003cp\u003eYou have just created an EMEWS project.\u003c/p\u003e\n\u003cp\u003eThis project is compatible with swift-t v. 1.3+. Earlier\nversions will NOT work.\u003c/p\u003e\n\u003cp\u003eThe project consists of the following directories:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eEMEWS-PhysiBoSSa/\n  data/\n  ext/\n  etc/\n  python/\n    test/\n  R/\n    test/\n  scripts/\n  swift/\n  README.md\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe directories are intended to contain the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003edata\u003c/code\u003e - model input etc. data\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eetc\u003c/code\u003e - additional code used by EMEWS\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eext\u003c/code\u003e - swift-t extensions such as eqpy, eqr\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003epython\u003c/code\u003e - python code (e.g. model exploration algorithms written in python)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003epython/test\u003c/code\u003e - tests of the python code\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eR\u003c/code\u003e - R code (e.g. model exploration algorithms written R)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eR/test\u003c/code\u003e - tests of the R code\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003escripts\u003c/code\u003e - any necessary scripts (e.g. scripts to launch a model), excluding\nscripts used to run the workflow.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eswift\u003c/code\u003e - swift code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUse the subtemplates to customize this structure for particular types of\nworkflows. These are: sweep, eqpy, and eqr.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1591715852.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v0.0.0"
    ],
    "full_name": "darachm/singularity_miniconda3",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-emews-project-template\" class=\"anchor\" href=\"#emews-project-template\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEMEWS project template\u003c/h2\u003e\n\u003cp\u003eYou have just created an EMEWS project.\u003c/p\u003e\n\u003cp\u003eThis project is compatible with swift-t v. 1.3+. Earlier\nversions will NOT work.\u003c/p\u003e\n\u003cp\u003eThe project consists of the following directories:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eEMEWS-PhysiBoSSa/\n  data/\n  ext/\n  etc/\n  python/\n    test/\n  R/\n    test/\n  scripts/\n  swift/\n  README.md\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe directories are intended to contain the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003edata\u003c/code\u003e - model input etc. data\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eetc\u003c/code\u003e - additional code used by EMEWS\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eext\u003c/code\u003e - swift-t extensions such as eqpy, eqr\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003epython\u003c/code\u003e - python code (e.g. model exploration algorithms written in python)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003epython/test\u003c/code\u003e - tests of the python code\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eR\u003c/code\u003e - R code (e.g. model exploration algorithms written R)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eR/test\u003c/code\u003e - tests of the R code\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003escripts\u003c/code\u003e - any necessary scripts (e.g. scripts to launch a model), excluding\nscripts used to run the workflow.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eswift\u003c/code\u003e - swift code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUse the subtemplates to customize this structure for particular types of\nworkflows. These are: sweep, eqpy, and eqr.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1581321093.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v1.0.0"
    ],
    "full_name": "darachm/singularity_ubuntu",
    "latest_release": null,
    "readme": "\u003cp\u003eThis is a base updated singularity image of ubuntu 1804, updated on 200208,\nto make it faster to debug Singularity builds.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1581237176.0
  },
  {
    "data_format": 2,
    "description": "Singularity definition files for various bioinformatics tasks",
    "filenames": [
      "general/Singularity",
      "snATAC/Singularity",
      "snRNA/Singularity"
    ],
    "full_name": "ParkerLab/containers",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eRepository containers Singularity container defintion files and necessary commands to\nbuild those containers.\u003c/p\u003e\n\u003cp\u003eIdeally, you\u0027d want to keep these definition files updated so that lab members can\nre-build containers as needed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-locate-singularity-images\" class=\"anchor\" href=\"#locate-singularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocate Singularity images\u003c/h2\u003e\n\u003cp\u003ePre-built Singularity images are located at \u003ccode\u003e/lab/data/sw/containers/\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThanks to Peter Orchard (@porchard).\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1580658783.0
  },
  {
    "data_format": 2,
    "description": "Review how to write a singularity image",
    "filenames": [
      "Singularity"
    ],
    "full_name": "j23414/singularity_event",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_event\" class=\"anchor\" href=\"#singularity_event\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_event\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4858\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eReview how to write a singularity image\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602550496.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v0.0.0"
    ],
    "full_name": "darachm/singularity_ncbi-blast",
    "latest_release": null,
    "readme": "\u003cp\u003eThis is just a singularity file for yoinking in the Docker file for NCBI BLAST as a simg\u003c/p\u003e\n\u003cp\u003eThis is an example of poor documentation\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603609657.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v0.0.1",
      "Singularity.v1.0.0",
      "Singularity.v2.0.0"
    ],
    "full_name": "darachm/singularity_jupyter",
    "latest_release": null,
    "readme": "\u003cp\u003eThis is just a singularity file for yoinking in the Docker file for NCBI BLAST as a simg\u003c/p\u003e\n\u003cp\u003eThis is an example of poor documentation\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1584671066.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for some phylogeny tools.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ArnaudBelcour/phylogeny-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-phylogeny\" class=\"anchor\" href=\"#singularity-recipe-for-phylogeny\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe for phylogeny\u003c/h1\u003e\n\u003cp\u003eThis singularity recipe contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBEAGLE library v3.2.0 (PRE-RELEASE)\u003c/li\u003e\n\u003cli\u003eBEAST v1.10.4\u003c/li\u003e\n\u003cli\u003eFastME 2.1.6.1\u003c/li\u003e\n\u003cli\u003eFastTree 2.1\u003c/li\u003e\n\u003cli\u003eIQ-TREE version 2.0-rc1 (November 21, 2019)\u003c/li\u003e\n\u003cli\u003eMrBayes 3.2.7\u003c/li\u003e\n\u003cli\u003ePhyML v3.3.20190909\u003c/li\u003e\n\u003cli\u003eRAxML-NG v0.9.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing the Singularity recipe, you can create a Singularity container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build phylogeny.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese tools can be called using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec phylogeny.sif tool_exec\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003cstrong\u003etool_exec\u003c/strong\u003e to use:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ebeast\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003efastme\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eFastTree\u003c/strong\u003e, \u003cstrong\u003eFastTreeDbl\u003c/strong\u003e and \u003cstrong\u003eFastTreeMP\u003c/strong\u003e for FastTree\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eiqtree\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003emb\u003c/strong\u003e for MrBayes\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ephyml\u003c/strong\u003e and \u003cstrong\u003ephyml-mpi\u003c/strong\u003e for PhyML\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eraxml-ng\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1578956805.0
  },
  {
    "data_format": 2,
    "description": "Code for evaluation on full neurovista trial data ",
    "filenames": [
      "Singularity.nv_eval"
    ],
    "full_name": "shwertt/neurovista_evaluation_sw",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-neurovista_evaluation_sw\" class=\"anchor\" href=\"#neurovista_evaluation_sw\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eneurovista_evaluation_sw\u003c/h1\u003e\n\u003cp\u003eCode for evaluation on full neurovista trial data following the \u003ca href=\"https://github.com/epilepsyecosystem/CodeEvaluationDocs\"\u003einstructions\u003c/a\u003e (commit 20e6f0f, dated 16/06/2020).\u003c/p\u003e\n\u003cp\u003eThis project has been cloned from\n\u003ca href=\"https://github.com/MatthiasEb/neurovista_evaluation\"\u003ehttps://github.com/MatthiasEb/neurovista_evaluation\u003c/a\u003e (a colleague of mine, both\nworking at Dresden University of Technology, Germany) and since adapted to\nfeature networks consisting of 1D-Convolutions on the raw time series.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-settings\" class=\"anchor\" href=\"#settings\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSettings\u003c/h2\u003e\n\u003cp\u003eSettings can be adjusted in SETTINGS.json\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Singularity\u003c/h2\u003e\n\u003cp\u003eThis project has been thoroughly testey with the Singularity recipe\n\u003ccode\u003eSingularity.nv_eval\u003c/code\u003e that is included in the repository.\nThe SingularityHub URI of my image is \u003ccode\u003eshwertt/neurovista_evaluation_sw:nv_eval\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installing-singularity\" class=\"anchor\" href=\"#installing-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling Singularity\u003c/h3\u003e\n\u003cp\u003eIf you need to install singularity on your work station, it did not suffice\nfor me to just do a \u003ccode\u003esudo apt install singularity-container\u003c/code\u003e because this\ninstall a singularity version 2. But \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/\u003c/a\u003e deploys\nsingularity version 3 images.\u003c/p\u003e\n\u003cp\u003eTrying to pull the shub image with singularity version 2 generates the\nfollowing error:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eERROR  : Unknown image format/type: /mnt/ieecad/s9759051/Downloads/tmp/shwertt-neurovista_evaluation_sw-main-nv_eval\nABORT  : Retval = 255\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn order to install singularity version 3 locally, I followed the installation\ninstructions from \u003ca href=\"https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/Container\" rel=\"nofollow\"\u003ehttps://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/Container\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis involved:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCheck if go is installed by executing \u003ccode\u003ego version\u003c/code\u003e. If it is not installed, get it with:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ewget https://storage.googleapis.com/golang/getgo/installer_linux \u0026amp;\u0026amp; chmod +x installer_linux \u0026amp;\u0026amp; ./installer_linux \u0026amp;\u0026amp; source $HOME/.bash_profile\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall Singularity by cloning the singularity repo\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003emkdir -p ${GOPATH}/src/github.com/sylabs \u0026amp;\u0026amp; cd ${GOPATH}/src/github.com/sylabs \u0026amp;\u0026amp; git clone https://github.com/sylabs/singularity.git \u0026amp;\u0026amp; cd singularity\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheckout the singularity version you want (see the Github Releases page for available releases), e.g.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egit checkout v3.6.3\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the environment variables for \u003ccode\u003ego\u003c/code\u003e in \u003ccode\u003e~/.bash_profile\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eI had to change all references from \u003ccode\u003e/home/s9759051\u003c/code\u003e to \u003ccode\u003e/mnt/ieecad/s9759051\u003c/code\u003e\notherwise the installer could not find the correct environment for the needed\nmodules.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild and install\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecd ${GOPATH}/src/github.com/sylabs/singularity \u0026amp;\u0026amp; ./mconfig \u0026amp;\u0026amp; cd ./builddir \u0026amp;\u0026amp; make \u0026amp;\u0026amp; sudo make install\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-interacting-with-the-singularity-container\" class=\"anchor\" href=\"#interacting-with-the-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInteracting with the Singularity container\u003c/h2\u003e\n\u003cp\u003ePull the container from Singularity Hub (once)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name nv_eval_shwertt.sif shub://shwertt/neurovista_evaluation_sw:nv_eval\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSet \u003ccode\u003emode=1\u003c/code\u003e in \u003ccode\u003eSETTINGS.json\u003c/code\u003e, then start a shell in the container with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell --contain -B /PATH/TO/CLONE/OF/THIS/GITHUB/PROJECT/neurovista_evaluation_sw:$HOME -B /scratch:/scratch --nv nv_eval_shwertt.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis mounts this github project to the \u003ccode\u003eHome\u003c/code\u003e folder inside singularity and\nbinds the local scratch folder to the scratch folder inside singularity. If\nthe data files are not inside \u003ccode\u003e/scratch\u003c/code\u003e, please modify this bind statement.\nCUDA support and access to the GPU is achieved with the \u003ccode\u003e--nv\u003c/code\u003e flag.\u003c/p\u003e\n\u003cp\u003eNow while inside the Singularity container, execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport CUDA_VISIBLE_DEVICES=0 \u0026amp;\u0026amp; python3 run.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePlease specify your \u003ccode\u003eCUDA_VISIBLE_DEVICES\u003c/code\u003e according to the available resources\nof the supercomputer.\u003c/p\u003e\n\u003cp\u003eAfter training is complete, you can test the model by closing the container\nwith \u003ccode\u003eexit\u003c/code\u003e, modify \u003ccode\u003eSETTINGS.json\u003c/code\u003e to switch to \u003ccode\u003emode=3\u003c/code\u003e and start the container again with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell --contain -B /PATH/TO/CLONE/OF/THIS/GITHUB/PROJECT/neurovista_evaluation_sw:$HOME -B /scratch:/scratch --nv nv_eval_shwertt.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethen execute again:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport CUDA_VISIBLE_DEVICES=0 \u0026amp;\u0026amp; python3 run.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePlease find the solution file under\n\u003ccode\u003esolutions/contest_data_solution_shwertt_mode3.csv\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-remarks\" class=\"anchor\" href=\"#remarks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemarks\u003c/h2\u003e\n\u003cp\u003eYou should use a GPU for training. I did use an RTX 2080 Ti.\nIf you use a GPU with much less RAM, you might have to reduce the batch size, I did not try that though.\nWhen I ran the code with \u003ccode\u003erun_on_contest_data=1\u003c/code\u003e, the results seemed to be comparable\nto the results from \u003ca href=\"https://github.com/MatthiasEb/neurovista_evaluation\"\u003eMatthiasEb\u003c/a\u003e.\nThe singularity container works just fine, in case you run into problems, have any questions or remarks,\nplease do not hesitate to contact me.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-algorithm\" class=\"anchor\" href=\"#algorithm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlgorithm\u003c/h3\u003e\n\u003cp\u003eThis approach uses a Residual Network (Resnet) based on 1D-Convolutions, applied to the raw time series.\nThe residual block consist of this stack:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef residual_block(X, kernels, size):\n    out = tf.keras.layers.Conv1D(kernels, size, padding=\u0027same\u0027)(X)\n    out = tf.keras.layers.ReLU()(out)\n    out = tf.keras.layers.Conv1D(kernels, size, padding=\u0027same\u0027)(out)\n    out = tf.keras.layers.add([X, out])\n    out = tf.keras.layers.ReLU()(out)\n    out = tf.keras.layers.MaxPool1D(pool_size=5, strides=3)(out)\n    return out\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe deep neural network then consists of six residual blocks followed by a\nglobal 1D-MaxPooling layer and a dense layer. The model is optimized with\nAdam, which uses a learning rate of 0.001.\u003c/p\u003e\n\u003cp\u003eThe model expects standardized 15 s segments of data, sampled at 200 Hz.\ntensorflow.keras (2.0.1) was used as Deep Learning API.\nAfter several testruns with different models on the contest data, I chose the\nresnet1d architecture that has been described above.\u003c/p\u003e\n\u003cp\u003eWith it I have achieved the following results:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\u0027run_on_contest_data\u0027: 1, \u0027mode\u0027: 3, \u0027pat\u0027: \u00271-3\u0027, \u0027subtract_mean\u0027: 1, \u0027model\u0027: \u0027./trains\u0027, \u0027feat\u0027: \u0027./features\u0027, \u0027CSV\u0027: \u0027./CSV\u0027, \u0027solutions\u0027: \u0027./solutions\u0027}\n./solutions/contest_data_solution_shwertt_mode3_20201001_resnet1d.csv\nGlobal roc auc: 0.6555\nGlobal Public roc auc: 0.6627\nGlobal Private roc auc: 0.6684\nPatient 1 roc auc: 0.2143\nPatient 2 roc auc: 0.6700\nPatient 3 roc auc: 0.7326\nPatient 1 Public roc auc: 0.2111\nPatient 1 Private roc auc: 0.2182\nPatient 2 Public roc auc: 0.6695\nPatient 2 Private roc auc: 0.6766\nPatient 3 Public roc auc: 0.7618\nPatient 3 Private roc auc: 0.7208\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHowever, considerable variations are conceivable.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-implementation\" class=\"anchor\" href=\"#implementation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImplementation\u003c/h3\u003e\n\u003cp\u003eLoading the original (~ 400 Hz) .mat files, resampling to 200 Hz,\nstandardizing (optionally, if \u003ccode\u003esubtract_mean==1\u003c/code\u003e), splitting them in 15 s\nsegments is done asynchronously on the fly by the dataloader in 5 different\nthreads. The 15s Segments are enqueued in a buffer with the size of 400\n10-min-sequences, implemented as a \u003ccode\u003etf.queue.RandomShuffleQueue\u003c/code\u003e. The data is\ntherefore dequeued in random order, although not perfectly uniformly\nshuffeled, depending on the buffer size and the size of the data set. The\nintention was to ensure a reasonably shuffeled training set of 15 s segments\nwhile minimizing IO, working on the .mat files and having the possibility for\nstandardization. If the IO-Bandwidth of the filesystem is reasonably high,\nthis should not slow down the training too much.\u003c/p\u003e\n\u003cp\u003eIf \u003ccode\u003erun_on_contest_data==1\u003c/code\u003e, 3 networks (one for each patient) are trained and evaluated individually.\nSubsequently, the solution file is concatenated.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602006304.0
  },
  {
    "data_format": 2,
    "description": "Build Singularity containers to run SpaDES simulations on HPC clusters.",
    "filenames": [
      "Singularity.spades_base",
      "Singularity.spades_github-development",
      "Singularity.spades_github-master"
    ],
    "full_name": "gparadis/spades-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-spades-singularity\" class=\"anchor\" href=\"#spades-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003espades-singularity\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-this-project\" class=\"anchor\" href=\"#about-this-project\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout this project\u003c/h2\u003e\n\u003cp\u003eThis project implements a scripted framework for automating the process of building Singularity containers for running SpaDES simulations on HPC clusters.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-i-am-super-impatient-and-refuse-to-take-the-time-to-understand-what-i-am-doing-before-running-any-commands-just-tell-me-how-to-do-the-thing-right-now\" class=\"anchor\" href=\"#i-am-super-impatient-and-refuse-to-take-the-time-to-understand-what-i-am-doing-before-running-any-commands-just-tell-me-how-to-do-the-thing-right-now\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eI am super impatient, and refuse to take the time to understand what I am doing before running any commands. Just tell me how to do the thing right now!\u003c/h2\u003e\n\u003cp\u003eTo build, sign, and push the base container flavour to the cloud image repository, simply run \u003ccode\u003emake all flavour=FLAVOUR\u003c/code\u003e, where \u003ccode\u003eFLAVOUR\u003c/code\u003e is one of \u003ccode\u003ebase\u003c/code\u003e, \u003ccode\u003egithub-master\u003c/code\u003e, or \u003ccode\u003egithub-development\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eNot sure which flavour to use? Read on!\u003c/p\u003e\n\u003cp\u003eNote that, if you do not have Singularity installed yet, you will need to run \u003ccode\u003emake install-singularity\u003c/code\u003e first.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-container-definition-files\" class=\"anchor\" href=\"#singularity-container-definition-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container definition files\u003c/h2\u003e\n\u003cp\u003eThis Singularity container definition files follow standard Singularity definition file naming conventions (i.e., they are prefixed with \u003ccode\u003eSingularity.\u003c/code\u003e followed by a \u003cem\u003etag\u003c/em\u003e string). There are three flavours (tags) defined in this project: \u003ccode\u003ebase\u003c/code\u003e, \u003ccode\u003egithub-master\u003c/code\u003e, and \u003ccode\u003egithub-development\u003c/code\u003e. Note that the R code that installs SpaDES packages for each flavour is contained in a script named \u003ccode\u003espades-setup_flavour.R\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eYou can also create new custom flavours by copying and modifying some files from an existing flavour. New flavours should be compatible with automated make targets (as long as you did not break the filename patterns).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-base-flavour\" class=\"anchor\" href=\"#base-flavour\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBase flavour\u003c/h3\u003e\n\u003cp\u003eThe base container flavour includes the latest stable CRAN versions of core SpaDES R packages. This base can be used to run SpaDES models directly (for simpler projects, where the CRAN packages are all you need). The base image also serves as a \u003cem\u003ebootstrap\u003c/em\u003e image for other flavours. The base container flavour is implemented in \u003ccode\u003eSingularity.spades_base\u003c/code\u003e and \u003ccode\u003espades-setup_base.R\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-github-flavours\" class=\"anchor\" href=\"#github-flavours\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGitHub flavours\u003c/h3\u003e\n\u003cp\u003eThere are two GitHub container flavours (\u003ccode\u003egithub-master\u003c/code\u003e, \u003ccode\u003egithub-development\u003c/code\u003e). These install core SpaDES R packages from the latest code pushed to GitHub repositories for \u003ccode\u003emaster\u003c/code\u003e and \u003ccode\u003edevelopment\u003c/code\u003e branches, respectively. The GitHub container flavours are implemented in the \u003ccode\u003eSingularity.spades-github_BRANCH\u003c/code\u003e and \u003ccode\u003espades-setup_github-BRANCH\u003c/code\u003e (where \u003ccode\u003eBRANCH\u003c/code\u003e is one of \u003ccode\u003emaster\u003c/code\u003e or \u003ccode\u003edevelopment\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eThe GitHub container flavours are \u003cem\u003ebootstrapped\u003c/em\u003e from the base container flavour. Defintion file implementation assumes that a local base container image is available in path \u003ccode\u003ebuild/spades.sif\u003c/code\u003e, so the base container must be built first (the base container will automatically get built if not present if you run \u003ccode\u003emake build flavour=FLAVOUR\u003c/code\u003e, where \u003ccode\u003eFLAVOUR\u003c/code\u003e is any value except for \u003ccode\u003ebase\u003c/code\u003e).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-custom-flavours\" class=\"anchor\" href=\"#custom-flavours\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCustom flavours\u003c/h3\u003e\n\u003cp\u003eYou can create a custom container flavour but copying \u003ccode\u003eSingularity.spades_github-master\u003c/code\u003e and \u003ccode\u003espades-setup_github-master.R\u003c/code\u003e---rename these to \u003ccode\u003eSingularity.spades_foo\u003c/code\u003e and \u003ccode\u003espades-setup_foo.R\u003c/code\u003e (where \u003ccode\u003efoo\u003c/code\u003e is whatever unique flavour name you want) and modify as required. Minimally, you just need to edit one line of code in the Singularity definition file to point to \u003ccode\u003espades-setup_foo.R\u003c/code\u003e, and edit the code in \u003ccode\u003espades-setup_foo.R\u003c/code\u003e to install whatever versions of SpaDES R packages you need.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-makefile-details\" class=\"anchor\" href=\"#makefile-details\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMakefile details\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003eMakefile\u003c/code\u003e implements a number of make targets.\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003emake build flavour=FLAVOUR sandbox=true\u003c/code\u003e to build a sandbox container (in \u003ccode\u003ebuild/spades_FLAVOUR_sandbox\u003c/code\u003e). See Singularity documentation for details on sandbox containers.\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003emake build flavour=FLAVOUR\u003c/code\u003e to build a container as a single \u003cem\u003esingularity image file\u003c/em\u003e (in \u003ccode\u003ebuild/spades_FLAVOUR.sif\u003c/code\u003e). See Singularity documentation for details on SIF containers.\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003emake push flavour=FLAVOUR\u003c/code\u003e to sign your SIF image and push it to your Sylabs cloud image library account. See the \u003ca href=\"https:%5Ccloud.sylabs.io\"\u003eSylabs Container Library\u003c/a\u003e to create and configure your account.\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003emake all flavour=FLAVOUR\u003c/code\u003e to build and push your image in one step.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602060129.0
  },
  {
    "data_format": 2,
    "description": "Nextflow pipeline that runs pycoQC on Guppy output",
    "filenames": [
      "singularity/Singularity.baseimage"
    ],
    "full_name": "tleonardi/pycoqc_pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pycoqc-nextflow-script\" class=\"anchor\" href=\"#pycoqc-nextflow-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epycoQC nextflow script\u003c/h1\u003e\n\u003cp\u003eThis is a nextflow script that runs pycoQC on the output folder of Guppy.\u003c/p\u003e\n\u003cp\u003eThe script takes the followig command line arguments:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--guppy_dir  dir \tPath to the guppy output directory containing *_sequencing_summary.txt\n--samplename name \tSample name\n--resultsDir dir \tPath to the output directory\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1576530678.0
  },
  {
    "data_format": 2,
    "description": "A quality control pipeline for illumina data set. This pipeline removes contaminants (e.g. Phix), performs fastqc, adapter cleaning and trimming and checks for contaminants",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "sequana/sequana_quality_control",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pycoqc-nextflow-script\" class=\"anchor\" href=\"#pycoqc-nextflow-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epycoQC nextflow script\u003c/h1\u003e\n\u003cp\u003eThis is a nextflow script that runs pycoQC on the output folder of Guppy.\u003c/p\u003e\n\u003cp\u003eThe script takes the followig command line arguments:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--guppy_dir  dir \tPath to the guppy output directory containing *_sequencing_summary.txt\n--samplename name \tSample name\n--resultsDir dir \tPath to the output directory\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1606177348.0
  },
  {
    "data_format": 2,
    "description": "A simple utility to convert a bunch of input fastq files into their reverse complement",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "sequana/sequana_revcomp",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pycoqc-nextflow-script\" class=\"anchor\" href=\"#pycoqc-nextflow-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epycoQC nextflow script\u003c/h1\u003e\n\u003cp\u003eThis is a nextflow script that runs pycoQC on the output folder of Guppy.\u003c/p\u003e\n\u003cp\u003eThe script takes the followig command line arguments:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--guppy_dir  dir \tPath to the guppy output directory containing *_sequencing_summary.txt\n--samplename name \tSample name\n--resultsDir dir \tPath to the output directory\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1589402320.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.bedtools",
      "Singularity",
      "Singularity.methylkit",
      "Singularity.Bowtie2",
      "Singularity.FastQC",
      "Singularity.samtools"
    ],
    "full_name": "thakk/biobase",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-containers-for-bioinformatics-tools\" class=\"anchor\" href=\"#singularity-containers-for-bioinformatics-tools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity containers for bioinformatics tools\u003c/h1\u003e\n\u003cp\u003eBioinformatics related singularity container recipies.\u003c/p\u003e\n\u003cp\u003eBase is CentOS 8.\u003c/p\u003e\n\u003cp\u003eCurrently two containers are implemented:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebasic tools:\n\u003cul\u003e\n\u003cli\u003eSamtools\u003c/li\u003e\n\u003cli\u003eBEDTools\u003c/li\u003e\n\u003cli\u003eFastQC\u003c/li\u003e\n\u003cli\u003eBowtie2\u003c/li\u003e\n\u003cli\u003eMultiQC\u003c/li\u003e\n\u003cli\u003eCutadapt\u003c/li\u003e\n\u003cli\u003eSTAR\u003c/li\u003e\n\u003cli\u003eHisat2\u003c/li\u003e\n\u003cli\u003ePicard\u003c/li\u003e\n\u003cli\u003eTrimmomatic\u003c/li\u003e\n\u003cli\u003eSamblaster\u003c/li\u003e\n\u003cli\u003eVarScan\u003c/li\u003e\n\u003cli\u003eVcfanno\u003c/li\u003e\n\u003cli\u003ePlink\u003c/li\u003e\n\u003cli\u003eMACS2\u003c/li\u003e\n\u003cli\u003eHomer\u003c/li\u003e\n\u003cli\u003eNextFlow\u003c/li\u003e\n\u003cli\u003enf-core\u003c/li\u003e\n\u003cli\u003eMAGeCK\u003c/li\u003e\n\u003cli\u003eTrimGalore\u003c/li\u003e\n\u003cli\u003eBismark\u003c/li\u003e\n\u003cli\u003eUCSC tools\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003emethylKit (built from basic):\n\u003cul\u003e\n\u003cli\u003eR + Bioconductor\u003c/li\u003e\n\u003cli\u003emethylkit\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003esamtools (built from Alpine Linux 3.10.3)\n\u003cul\u003e\n\u003cli\u003eNote, automated Singularity Hub build does not seem to work correctly as this recipe uses multistage build to minimize container size\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-availability\" class=\"anchor\" href=\"#availability\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailability\u003c/h2\u003e\n\u003cp\u003eBasic tools container is available at Singularity hub: shub://thakk/biobase\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1589823361.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for DeepEC.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ArnaudBelcour/deepec-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-deepec-singularity-recipe\" class=\"anchor\" href=\"#deepec-singularity-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepEC Singularity recipe\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://bitbucket.org/kaistsystemsbiology/deepec/src/master/\" rel=\"nofollow\"\u003eDeepEC\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFirst download the recipe, for example with a git clone of this repository.\u003c/p\u003e\n\u003cp\u003eSingularity container can be created with the command (this command needs admin right):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build deepec.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe deepec.sif file size is around 1.3 GB.\u003c/p\u003e\n\u003cp\u003eThen DeepEC can be called with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec deepec.sif python /programs/deepec/deepec.py -i fasta_file -o output_folder\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1575494893.0
  },
  {
    "data_format": 2,
    "description": "WhatsHap repository for singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "touala/WhatsHap",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-whatshap\" class=\"anchor\" href=\"#whatshap\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhatsHap\u003c/h1\u003e\n\u003cp\u003eWhatsHap repository for singularity container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1601728652.0
  },
  {
    "data_format": 2,
    "description": "a cookiecutter sequana pipeline template",
    "filenames": [
      "{{cookiecutter.project_slug}}/singularity/Singularity"
    ],
    "full_name": "sequana/sequana_pipeline_template",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-whatshap\" class=\"anchor\" href=\"#whatshap\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhatsHap\u003c/h1\u003e\n\u003cp\u003eWhatsHap repository for singularity container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1616522945.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Microstructure Diffusion Toolbox (http://mdt-toolbox.readthedocs.io/en/latest/index.html)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "akhanf/mdt-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mdt-singularity\" class=\"anchor\" href=\"#mdt-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emdt-singularity\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for Microstructure Diffusion Toolbox (MDT), a fast and flexible python toolbox for microstructural modelling (including NODDI etc..). \u003cem\u003eNote:\u003c/em\u003e  I am not a developer/contributor of MDT, for more info on MDT see here:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCode: \u003ca href=\"https://github.com/cbclab/MDT\"\u003ehttps://github.com/cbclab/MDT\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eDocs: \u003ca href=\"http://mdt-toolbox.readthedocs.io/en/latest/index.html\" rel=\"nofollow\"\u003ehttp://mdt-toolbox.readthedocs.io/en/latest/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI made this Singularity container for utilizing the CPUs for OpenCL on Linux systems that either have an incompatible GPU (Quadro), or no GPU (HPC systems). Installs Intel OpenCL drivers on Ubuntu 14.04 (newer versions not supported), and then installs MDT (pip3) and all dependencies (since PPA not supported on older Ubuntu version that Intel OpenCL supports). Big props to the MDT developers for creating this great toolbox -- this should hopefully make it easier for anyone to run it on an Intel CPU-based compute cluster.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote:\u003c/em\u003e This requires Singularity to be installed (\u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/\u003c/a\u003e), requires sudo privilege to build the container (local machine), but can be used without sudo privilege by copying the container to another system.\u003c/p\u003e\n\u003cp\u003eBuild:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone http://github.com/akhanf/mdt-singularity\ncd mdt-singularity\nsudo singularity build mdt.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTest:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec mdt.simg mdt-list-devices\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePreliminary testing: CPU - Intel(R) Xeon(R) CPU E5-2687W v3 @ 3.10GHz (Intel(R) OpenCL), with 20 cores takes ~2.5 hours to run NODDI (Cascade) on a HCP dataset (very slow compared to what you could do with a proper GPU, but amazingly fast compared to NODDI toolbox!! (would take several days..)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1522544846.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "onuryukselen/singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity\u003c/h1\u003e\n\u003cp\u003eDevelopment Branch\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1562279768.0
  },
  {
    "data_format": 2,
    "description": "Visual Studio Code base container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jekriske/vscode",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity\u003c/h1\u003e\n\u003cp\u003eDevelopment Branch\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1535667885.0
  },
  {
    "data_format": 2,
    "description": "Documentation, code examples, and tutorials for the SemWES platform",
    "filenames": [
      "code_examples/Singularity/abortable_demo_job/Singularity",
      "code_examples/Singularity/hellocuda/Singularity",
      "code_examples/Singularity/waiter/Singularity",
      "code_examples/Singularity/abortable_waiter/Singularity"
    ],
    "full_name": "SemWES/docs-and-training",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-service-and-workflow-development-for-the-semwes-platform-stack-in-cloudifacturing\" class=\"anchor\" href=\"#service-and-workflow-development-for-the-semwes-platform-stack-in-cloudifacturing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eService and workflow development for the SemWES platform stack in CloudiFacturing\u003c/h1\u003e\n\u003cp\u003eWelcome to the SemWES/CloudiFacturing service-development resources!\u003c/p\u003e\n\u003cp\u003eThis repository is meant to be a center for documentation relevant for everyone\nwho develops services and workflows on the SemWES platform stack in\nthe CloudiFacturing project. Here, you will find everything from high-level\ndescriptions of the concepts behind the platform to step-by-step\ntutorials for advanced topics in service development.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contents-\" class=\"anchor\" href=\"#contents-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents \u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#documentation-version\"\u003eDocumentation version\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#contributing\"\u003eContributing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#platform-overview\"\u003ePlatform overview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#service-implementation-concepts-examples-tutorials\"\u003eService implementation: concepts, examples, tutorials\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#general-concepts\"\u003eGeneral concepts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#semwes-synchronous-services\"\u003eSemWES Synchronous services\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#semwes-aynchronous-services\"\u003eSemWES Aynchronous services\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#semwes-applications\"\u003eSemWES Applications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#service-deployment\"\u003eService deployment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#soap-services\"\u003eSOAP services\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#file-access\"\u003eFile access\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#using-hpc-resources\"\u003eUsing HPC resources\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#advanced-topics\"\u003eAdvanced topics\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#workflow-creation-execution-and-monitoring\"\u003eWorkflow creation, execution, and monitoring\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#basic-workflow-editing\"\u003eBasic workflow editing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#using-the-hpc-service\"\u003eUsing the HPC service\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#available-utility-services\"\u003eAvailable utility services\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#workflow-debugging\"\u003eWorkflow debugging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#advanced-topics\"\u003eAdvanced topics\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#stream-data-integration\"\u003eStream Data Integration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#emgora-integration\"\u003eemGORA Integration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#reference-documentation-of-platform-services\"\u003eReference documentation of platform services\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation-version\" class=\"anchor\" href=\"#documentation-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation version\u003c/h2\u003e\n\u003cp\u003eCurrent documentation version: \u003ccode\u003e3.20.0\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href=\"CHANGELOG.md\"\u003eChangelog\u003c/a\u003e for versioning details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eEveryone is \u003cem\u003ewelcome\u003c/em\u003e to contribute to this documentation center. Typos,\nclarifications, interesting code examples, or also just questions \u2013 anything is\nvaluable. To contribute, choose one of the following options:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFork the repository, add your changes and create a pull request.\u003c/li\u003e\n\u003cli\u003eCreate an issue in the GitHub issue tracker.\u003c/li\u003e\n\u003cli\u003eWrite an email with your ideas to \u003ca href=\"mailto:robert.schittny@sintef.no\"\u003eRobert\u003c/a\u003e\n(discouraged, rather use one of the options above).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-platform-overview\" class=\"anchor\" href=\"#platform-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlatform overview\u003c/h2\u003e\n\u003cp\u003eNew to the SemWES platform? Read all about its concepts and background here.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"infrastructure_overview/getting_access.md\"\u003eGetting access\u003c/a\u003e: Need access to\nthe SemWES platform in CloudiFacturing? Look here.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"infrastructure_overview/demos.md\"\u003eDemos\u003c/a\u003e: If you want to get your hands\ndirty immediately and try out some demo workflows on the platform, look here.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"./infrastructure_overview/authentication.md\"\u003eUsers, projects, and authentication\u003c/a\u003e:\nGives a short overview of how user authentication and data-access restriction\nworks on the SemWES platform.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"infrastructure_overview/workflows_and_services.md\"\u003eWorkflows and services in the SemWES cloud \u2013 an overview\u003c/a\u003e:\nGives a compact overview over the concepts behind SemWES and what services\nand workflows are and how they are executed. Also explains the nomenclature\nused in this repository.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"infrastructure_overview/service_types.md\"\u003eThe SemWES service types\u003c/a\u003e:\nDescription and requirements of synchronous services, asynchronous services,\nand applications. Read this if you\u0027re wondering what kind of service you\nneed to develop for a certain use case.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"infrastructure_overview/storage.md\"\u003eAccessing cloud storage: Generic Storage Services (GSS)\u003c/a\u003e:\nOn the SemWES platform, different cloud storages can be accessed in a\nsimple, unified way. Learn about the basic concepts of the SemWES\nGeneric Storage Services and the available storage solutions in\nCloudiFacturing.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-service-implementation-concepts-examples-tutorials\" class=\"anchor\" href=\"#service-implementation-concepts-examples-tutorials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eService implementation: concepts, examples, tutorials\u003c/h2\u003e\n\u003cp\u003eAll workflows in the SemWES platform are basically a series of calls to\nindividual web services. This section provides information on how to develop\nsuch web services in a way that they can be registered in the SemWES\nplatform.\u003c/p\u003e\n\u003cp\u003eAll documentation here deals with things done \"in code\" (as opposed to via\nthe graphical tools provided on the portal).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-general-concepts\" class=\"anchor\" href=\"#general-concepts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeneral concepts\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"./service_implementation/available_parameters.md\"\u003eAvailable parameters\u003c/a\u003e:\nThe workflow manager offers a set of \"global\" parameters which are available\nto every service. Here, we take a closer look at these parameters and their\nmain use cases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-semwes-synchronous-services\" class=\"anchor\" href=\"#semwes-synchronous-services\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSemWES Synchronous services\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCheck out the \u003ca href=\"infrastructure_overview/service_types.md\"\u003eSemWES service types\u003c/a\u003e\nfor a high-level description of synchronous services and their required\ninterface.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"code_examples/Python/sync_calculator\"\u003eCode example (Python): synchronous service\u003c/a\u003e:\nA very simple synchronous calculator service implemented in Python\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"code_examples/Java/skeleton_syncservice\"\u003eCode example (Java): synchronous-service skeleton\u003c/a\u003e:\nBare-bone skeleton of a synchronous service\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"tutorials/services/python_sync_calculator.md\"\u003eTutorial: Deploy and modify synchronous calculator webservice\u003c/a\u003e:\nThis tutorial teaches you how to deploy and modify one of the code examples,\nnamely the synchronous Calculator webservice implemented in Python. Good as a\nstarting point in the service development.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-semwes-aynchronous-services\" class=\"anchor\" href=\"#semwes-aynchronous-services\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSemWES Aynchronous services\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCheck out the \u003ca href=\"infrastructure_overview/service_types.md\"\u003eSemWES service types\u003c/a\u003e\nfor a high-level description of asynchronous services and their required\ninterface.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"code_examples/Python/async_waiter\"\u003eCode example (Python): Waiter\u003c/a\u003e:\nAn asynchronous service which does nothing but waiting\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"tutorials/services/python_async_waiter.md\"\u003eTutorial: Create a simple asynchronous service\u003c/a\u003e:\nThis tutorial guides you through the deployment steps of a simple\nasynchronous service. Starting from a simply Python script representing a\nlong-running computation, we will wrap an asynchronous web service around it\nwhich can be deployed in the SemWES infrastructure stack.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-semwes-applications\" class=\"anchor\" href=\"#semwes-applications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSemWES Applications\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCheck out the \u003ca href=\"infrastructure_overview/service_types.md\"\u003eSemWES service types\u003c/a\u003e\nfor a high-level description of applications and their required interface.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"code_examples/Python/app_simple\"\u003eCode example (Python): Dialog\u003c/a\u003e:\nSimplest possible example application containing a one-button dialog.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"code_examples/Python/app_debugger\"\u003eCode example (Python): Debugger\u003c/a\u003e:\nDebug application for workflows; pauses a workflow and displays parameter\ncontents.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-service-deployment\" class=\"anchor\" href=\"#service-deployment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eService deployment\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/deployment_strategy.md\"\u003eService-deployment concept\u003c/a\u003e:\nLearn about the SemWES deployment strategy here.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/deployment_automated.md\"\u003eService-deployment manual\u003c/a\u003e:\nDescribes how services can be deployed in SemWES\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-soap-services\" class=\"anchor\" href=\"#soap-services\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSOAP services\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"service_implementation/basics_testing.md\"\u003eTesting SOAP services\u003c/a\u003e:\nExplains how deployed SOAP services can be tested without having to go\nthrough the workflow manager.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-file-access\" class=\"anchor\" href=\"#file-access\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFile access\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIf you haven\u0027t done so already, it is recommended to read the general\nprinciples of \u003ca href=\"infrastructure_overview/storage.md\"\u003eaccessing cloud storage\u003c/a\u003e\non the SemWES platform.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"service_implementation/basics_gss_libraries.md\"\u003eHigh-level file access using GSS libraries\u003c/a\u003e:\nWhile it is perfectly possible to directly interact with the GSS API, it is\nhighly recommended to use the provided GSS client libraries for file access\nin SemWES. This article gives an overview over existing libraries and\ntheir usage.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-hpc-resources\" class=\"anchor\" href=\"#using-hpc-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing HPC resources\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/basics_hpc.md\"\u003eHPC access through the SemWES platform\u003c/a\u003e:\nThe SemWES platform abstracts access of specific HPC resources with a\ngeneric API, making it possible to run computations on different HPC\nresources without any change to the computation code. This article explains\nthe concepts and technical background of this solution.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/basics_singularity.md\"\u003ePackaging software in Singularity images\u003c/a\u003e:\nAll software that should be run on the HPC resources accessible through the\nSemWES platform must be wrapped into Singularity images which are then\nexecuted as isolated containers on an HPC cluster. Learn how to create,\nupload, and register such images in this article.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/advanced_hpc_notifications.md\"\u003eCommunicating with a running HPC job\u003c/a\u003e:\nSometimes, communication with a running HPC job is important, for example to\nbe able to control or abort a simulation if required. This article explains\nhow to set up a Singularity image for this kind of communication.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/basics_hpc_logs.md\"\u003eDebugging HPC applications\u003c/a\u003e:\nDebugging HPC applications in SemWES can be difficult due to many\nlayers of abstraction between the running application and the user. This\narticle gives some hints on debugging and loggin.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/advanced_hpc_mpi.md\"\u003eSingularity and MPI applications\u003c/a\u003e:\nLearn how to prepare your Singularity image for parallel\nexecution using MPI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/advanced_hpc_gpu.md\"\u003eGPU support for Singularity images\u003c/a\u003e:\nLearn how to prepare your Singularity image for access of underlying GPU cores.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-advanced-topics\" class=\"anchor\" href=\"#advanced-topics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvanced topics\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/advanced_error_handling.md\"\u003eError handling in SOAP services\u003c/a\u003e:\nExplains how to gracefully handle errors inside SOAP services such that the\noutside world can process them.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"service_implementation/advanced_authentication.md\"\u003eUsing authentication services inside a service\u003c/a\u003e:\nWhen a service is executed inside a workflow, the portal takes care that\nproper user authentication. If necessary, however, the authentication\nmanager can also be used inside services.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-workflow-creation-execution-and-monitoring\" class=\"anchor\" href=\"#workflow-creation-execution-and-monitoring\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow creation, execution, and monitoring\u003c/h2\u003e\n\u003cp\u003eOnce all required web services are developed, they need to be registered and\nhooked up to make a workflow. This section deals with all things done\n\"graphically\" via the tools provided on the portal.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-basic-workflow-editing\" class=\"anchor\" href=\"#basic-workflow-editing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBasic workflow editing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDon\u0027t forget to have a look at the \u003ca href=\"./infrastructure_overview/demos.md\"\u003edemo\nworkflows\u003c/a\u003e\nto learn from some pre-defined workflow examples.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"tutorials/workflows/basics_portal_overview.md\"\u003eTutorial: Overview over the portal GUI\u003c/a\u003e:\nIn this tutorial you will get to know the portal GUI, start a workflow and\ninspect its results.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"tutorials/workflows/basics_editing.md\"\u003eTutorial: Basic workflow editing\u003c/a\u003e:\nIn this tutorial you will load, modify, and save an existing workflow using\nthe graphical workflow editor.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"tutorials/workflows/basics_service_registration.md\"\u003eTutorial: Registration of new services\u003c/a\u003e:\nAny newly created service needs to be registered properly to be usable in the\nSemWES platform. Learn about all details and caveats of service\nregistration here.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/service_upgrades.md\"\u003eUpgrading services\u003c/a\u003e:\nSometimes, an already deployed and registered service needs to be upgraded,\npossibly with changes to the input and output parameters. Read here what to\nkeep in mind when performing such upgrades.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-the-hpc-service\" class=\"anchor\" href=\"#using-the-hpc-service\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the HPC service\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/HPC_service.md\"\u003eOverview over the generic HPC service\u003c/a\u003e:\nIntroduces the generic HPC service and showcases how it can be used to\nexecute Singularity images on the available HPC resources.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/HPC_gss_conversion.md\"\u003eConverting from GSS URIs to file paths and back\u003c/a\u003e:\nFiles and folders are handled using GSS URIs within the SemWES\nplatform, but on an HPC cluster, absolute paths are required. Learn how\nto convert between the two here.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/HPC_prepost.md\"\u003ePre- and post-processor services\u003c/a\u003e:\nExplains how one can interface with the generic HPC service by writing pre-\nand post-processor services.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/HPC_background.md\"\u003eLaunching HPC jobs in the background\u003c/a\u003e:\nExplains how HPC jobs can be launched in the background, such that other\nparts of the workflow can run in parallel.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-available-utility-services\" class=\"anchor\" href=\"#available-utility-services\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailable utility services\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/utilities_filechooser.md\"\u003eFile selection using the FileChooser service\u003c/a\u003e:\nThe FileChooser service is one of two ways to easily let the user upload and\nselect files for a workflow. Learn how to integrate the file chooser into\nyour workflows.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/utilities_auto_gui.md\"\u003eAutomatic creation of graphical interfaces for user input\u003c/a\u003e:\nWant an HTML form for user input at the start of a workflow? Then use this\ntool which automatically creates one for you with minimal input on your side.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/utilities_dfki.md\"\u003eDFKI\u2019s utility services\u003c/a\u003e:\nWant to show some HTML during a workflow? Need a user decision somewhere inside\nthe workflow? DFKI\u0027s utility suite offers ready-made services just for that.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-workflow-debugging\" class=\"anchor\" href=\"#workflow-debugging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow debugging\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"code_examples/Python/app_debugger/README.md\"\u003eParameter debugger\u003c/a\u003e:\nThis simple application offers the option to pause a workflow and display any\nparameters that are currently in use. Great for debugging failing workflows\nor services. Offered as a code example with complete source code.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-advanced-topics-1\" class=\"anchor\" href=\"#advanced-topics-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvanced topics\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/advanced_workflow_nesting.md\"\u003eUsing workflows inside workflows\u003c/a\u003e:\nOne of the great strengths of the SemWES workflow concepts is that one can\nuse complete workflows as services inside another workflow. Learn about this\nworkflow nesting here.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"workflow_creation/advanced_branching_looping.md\"\u003eBranching and looping\u003c/a\u003e:\nRead how you can implement basic branches and loops using only the workflow\neditor.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-stream-data-integration\" class=\"anchor\" href=\"#stream-data-integration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStream data integration\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"infrastructure_overview/stream_data_integration.md\"\u003eStream data handling\u003c/a\u003e: Due to its flexible webservice-based workflow structure, SemWES does natively support the integration of stream data into SemWES workflows.\nThis section features a python based prototype and explanations, and links to demo services and workflows.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-emgora-integration\" class=\"anchor\" href=\"#emgora-integration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eemGORA integration\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"infrastructure_overview/emgora_integration.md\"\u003eemGORA integration manual\u003c/a\u003e:\nThis section will address how to integrate SemWES workflows with the emgora marketplace.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reference-documentation-of-platform-services\" class=\"anchor\" href=\"#reference-documentation-of-platform-services\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference documentation of platform services\u003c/h2\u003e\n\u003cp\u003eIf you need information on a specific method of one of the platform services,\nhave a look at our API references:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"service_APIs/api_wfm.md\"\u003eWorkflow manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"service_APIs/api_wfe.md\"\u003eWorkflow editor\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"service_APIs/api_authentication.md\"\u003eAuthentication manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"service_APIs/api_gss.md\"\u003eGSS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"service_APIs/api_refissh.md\"\u003erefissh\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"service_APIs/api_servicectl.md\"\u003eservicectl\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605725180.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Gimp.",
    "filenames": [
      "2.10.8/Singularity"
    ],
    "full_name": "icaoberg/singularity-gimp",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-gimp\" class=\"anchor\" href=\"#singularity-gimp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-gimp\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://www.gimp.org\" rel=\"nofollow\"\u003eGimp\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "gimp"
    ],
    "updated_at": 1619080700.0
  },
  {
    "data_format": 2,
    "description": "Planet CLI in a Docker",
    "filenames": [
      "Singularity",
      "osgeo/Singularity",
      "basic/Singularity",
      "basic/Singularity-geo"
    ],
    "full_name": "samapriya/planet-cyverse",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/711\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-remote-sensing-environments-in-scalable-hpc-singularity-images\" class=\"anchor\" href=\"#remote-sensing-environments-in-scalable-hpc-singularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemote Sensing Environments in Scalable HPC Singularity Images\u003c/h1\u003e\n\u003cp\u003eThe purpose of this singularity images and setup is to allow for development of an active pipeline between image resources, hosting, ingestion protocol into Google Earth Engine and retentation on volume as needed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-main-build-components--should-include\" class=\"anchor\" href=\"#main-build-components--should-include\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMain Build components ( Should include)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePlanet API Download Client to download images from Planet API \u003cstrong\u003eIncluded\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eFTP client to pull images from existing sftp or ftp addresses \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eInclude additional tools (shapely, pdal, rasterio) \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularity container which consists of further tools to manipulate and preprocess imagery \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularity container which is mounted with large shared volume but individual user folder \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eEarth Engine Upload and processing client including Earth Engine API client \u003cstrong\u003eIncluded\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eJupyter notebook to connnect to mounted volume for local analysis and export of image or remote analysis and export from Google Earth Engine \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eDrive to access google drive: Check if all tasks have completed and download from google drive, Verify and delete to preserve google drive storage \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-possible-integrations-and-enhancements\" class=\"anchor\" href=\"#possible-integrations-and-enhancements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePossible Integrations and Enhancements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eLeaflet based imagery and vector visualization\u003c/li\u003e\n\u003cli\u003eImage tiling so user can visualize on the leaflet window within the Jupyter notebooks\u003c/li\u003e\n\u003cli\u003eEnd results include QGIS with X11 support to allow user to generate maps and export as images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-planet-cli-in-a-docker-container\" class=\"anchor\" href=\"#planet-cli-in-a-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlanet CLI in a Docker Container\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eIn Progress\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-planet-cli-in-a-singularity-container\" class=\"anchor\" href=\"#planet-cli-in-a-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlanet CLI in a Singularity Container\u003c/h2\u003e\n\u003cp\u003eSingularity files are in the \u003ccode\u003e/basic\u003c/code\u003e and \u003ccode\u003e/osgeo\u003c/code\u003e folders. The \u003ccode\u003e/osgeo\u003c/code\u003e container is maintained by Tyson Swetnam on \u003ca href=\"\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTo build a container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd basic/\nsudo singularity build basic.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003ecd osgeo/\nsudo singularity build osgeo.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\" class=\"anchor\" href=\"#moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMoving files from Drive.Google into an Atmosphere or Jetstream VM\u003c/h3\u003e\n\u003cp\u003eIn the US, academic institutions have increasingly established email accounts through Google.\nSome institutions have unlimited storage on \u003ca href=\"https://drive.google.com\" rel=\"nofollow\"\u003eDrive\u003c/a\u003e\nas a service for faculty and students.\u003c/p\u003e\n\u003cp\u003eOne of the issues with uploading / downloading a large number of images to or from a Drive\naccount through the web browser (Chrome or Firefox) is the number of files allowed,\nthe speed of the uploads, and the size of the downloads.\u003c/p\u003e\n\u003cp\u003eTypically a download directly from a Drive account through Chrome is limited to \u0026lt;2Gb\nand is zipped by Google before starting.\u003c/p\u003e\n\u003cp\u003eWhile the browser can work well for easily uploading a large number sUAS images from a collection,\ndownloading the images as .zip files from the Google.drive can become tiresome and difficult.\u003c/p\u003e\n\u003cp\u003eTo get around these problems we can use 3rd party applications like FUSE and\n\u003ca href=\"https://github.com/odeke-em/drive\"\u003e\u003ccode\u003edrive\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fuse-client-ocamfluse\" class=\"anchor\" href=\"#fuse-client-ocamfluse\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFUSE client \u003ccode\u003eocamfluse\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/astrada/google-drive-ocamlfuse\"\u003e\u003ccode\u003egoogle-drive-ocamlfuse\u003c/code\u003e\u003c/a\u003e is a Google Drive Client written in OCaml. This is tested for Ubuntu systems.\u003c/p\u003e\n\u003cp\u003eIt can mount your google drive as a folder visible in the file tree. FUSE is slower than other methods like iRODS or \u003ccode\u003eDrive\u003c/code\u003e, but allows for GUI based drag and drop transfers.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo add-apt-repository ppa:alessandro-strada/ppa\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get update\nsudo apt-get install google-drive-ocamlfuse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the app the first time to get the authentication certificate from Google\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egoogle-drive-ocamlfuse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate a folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir ~/googledrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMount the new googledrive\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egoogle-drive-ocamlfuse ~/googledrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOpen in your favorite file explorer.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-go\" class=\"anchor\" href=\"#install-go\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall \u003ccode\u003eGo\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eDrive uses the \u003ccode\u003ego\u003c/code\u003e language. In order to work with it you need to \u003ca href=\"https://golang.org/doc/install\" rel=\"nofollow\"\u003einstall \u003ccode\u003ego\u003c/code\u003e\u003c/a\u003e onto the VM.\u003c/p\u003e\n\u003cp\u003eRemove any other go packages (particularly gccgo-go)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get -y autoremove gccgo-go\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://storage.googleapis.com/golang/go1.8.1.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.8.1.linux-amd64.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn \u003ccode\u003e/etc/profile\u003c/code\u003e add: \u003ccode\u003eexport PATH=$PATH:/usr/local/go/bin\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecat \u0026lt;\u0026lt; ! \u0026gt;\u0026gt; /etc/profile\nexport PATH=$PATH:/usr/local/go/bin\n!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn \u003ccode\u003e~/.bashrc\u003c/code\u003e - \u003ccode\u003esudo nano ~/.bashrc\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAdd the GOPATH directly from terminal:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecat \u0026lt;\u0026lt; ! \u0026gt;\u0026gt; ~/.bashrc\nexport GOPATH=\\$HOME/go\nexport PATH=\\$GOPATH:\\$GOPATH/bin:\\$PATH\n!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSource the new \u003ccode\u003e~/.bashrc\u003c/code\u003e close the terminal and reopen\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esource ~/.bashrc \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFollow the \u003ccode\u003ego\u003c/code\u003einstructions to \u003ca href=\"https://golang.org/doc/install#testing\" rel=\"nofollow\"\u003etest the installation\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-drive-a-drivegoogle-client-for-commandline\" class=\"anchor\" href=\"#install-drive-a-drivegoogle-client-for-commandline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall \u003ccode\u003edrive\u003c/code\u003e a drive.google client for commandline\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/odeke-em/drive#installing\"\u003e\u003ccode\u003edrive\u003c/code\u003e\u003c/a\u003e is a command line client using Go language\u003c/p\u003e\n\u003cp\u003eInstall \u003ccode\u003edrive\u003c/code\u003e using \u003ccode\u003ego\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#install git\nsudo apt-get install git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003ecd ~/go\ngo get -u github.com/odeke-em/drive/cmd/drive\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-initialize-drive-with-your-google-account\" class=\"anchor\" href=\"#initialize-drive-with-your-google-account\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInitialize \u003ccode\u003edrive\u003c/code\u003e with your Google account\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003emkdir ~/gdrive\ndrive init ~/gdrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFollow the instructions for copying and pasting the html for authentication\u003c/p\u003e\n\u003cp\u003eTest your installation\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive ls\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pull-data-from-your-googledrive-account-onto-the-vm\" class=\"anchor\" href=\"#pull-data-from-your-googledrive-account-onto-the-vm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePull data from your Google.drive account onto the VM\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003edrive pull your/google/drive/folders/here\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-drive-on-ua-hpc\" class=\"anchor\" href=\"#using-drive-on-ua-hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Drive on UA HPC\u003c/h3\u003e\n\u003cp\u003eDrive is currently available on the ElGato login node\u003c/p\u003e\n\u003cp\u003eLoad \u003ccode\u003ego\u003c/code\u003e and \u003ccode\u003edrive\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load go\nmodule load drive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate a directory where you want to initiate \u003ccode\u003edrive\u003c/code\u003e - preferrably on your \u003ccode\u003e/xdisk/\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /xdisk/uid/\nmkdir gdrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInitiate the drive\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive init /xdisk/uid/gdrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will get a request to \u003ccode\u003eVisit this URL to get an authorization code\u003c/code\u003e with a link to a long \u003ccode\u003ehttps://accounts.google.com/o/oauth2/xxxx\u003c/code\u003e URL - copy paste that link into your preferred browser.\u003c/p\u003e\n\u003cp\u003eYou will be taken to a Google login page, type in your email address (\u003ca href=\"mailto:uid@email.arizona.edu\"\u003euid@email.arizona.edu\u003c/a\u003e) and password.\u003c/p\u003e\n\u003cp\u003eCopy/Paste the code provided by Google back in your Terminal window where prompted: \u003ccode\u003ePaste the authorization code:\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eNow, check to see if your \u003ccode\u003edrive.google.com\u003c/code\u003e acount is active:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive ls\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA list of the directories in your \u003ccode\u003edrive.google.com\u003c/code\u003e account should be listed, e.g.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/project1\n/project2\n/reports1\n/pictures1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can \u003ccode\u003epull\u003c/code\u003e files or directories from your \u003ccode\u003edrive.google.com\u003c/code\u003e now using commands like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive pull project1/subfolder/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will see:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eResolving...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003efollowed by a spinning \u003ccode\u003e\\\u003c/code\u003e \u003ccode\u003e|\u003c/code\u003e \u003ccode\u003e/\u003c/code\u003e \u003ccode\u003e-\u003c/code\u003e set of symbols\u003c/p\u003e\n\u003cp\u003eThe folder contents will be displayed:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e /project1/subfolder/file1.csv\n...\n+ /project1/subfolder/file955.csv\n+ /project1/subfolder/file966.csv\nAddition count 966 src: 5.02GB\nProceed with the changes? [Y/n]:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSelect \u003ccode\u003ey\u003c/code\u003e and the download will proceed.\u003c/p\u003e\n\u003cp\u003eTypical speeds are between 10 and 50 Mb/s.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eProceed with the changes? [Y/n]:y\n 5392682146 / 5392682146 [==========================================================================================================================] 100.00% 1m55s\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setting-up-cyverse-data-store-and-irods-icommands\" class=\"anchor\" href=\"#setting-up-cyverse-data-store-and-irods-icommands\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetting up CyVerse Data Store and iRods iCommands\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\" rel=\"nofollow\"\u003eInstructions\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ iinit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will be queried to set up your \u003ccode\u003eirods_environment.json\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eEnter the following:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003estatement\u003c/th\u003e\n\u003cth\u003einput\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eDNS\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003edata.cyverse.org\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eport number\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003e1247\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003euser name\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003eyour user name\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ezone\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003eiplant\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSet up auto-complete for iCommands\n\u003ca href=\"https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\" rel=\"nofollow\"\u003einstructions\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload \u003ca href=\"https://pods.iplantcollaborative.org/wiki/download/attachments/6720192/i-commands-auto.bash\" rel=\"nofollow\"\u003ei-commands-auto.bash\u003c/a\u003e.\nIn your home directory, rename i-commands-auto.bash to .i-commands-auto.bash\nIn your .bashrc or .bash_profile, enter the following:\nsource .i-commands-auto.bash\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1520625186.0
  },
  {
    "data_format": 2,
    "description": "Python Gene Expression Spatial Toolkit",
    "filenames": [
      "singularity/Singularity.stretch"
    ],
    "full_name": "mfschmidt/PyGEST",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/711\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-remote-sensing-environments-in-scalable-hpc-singularity-images\" class=\"anchor\" href=\"#remote-sensing-environments-in-scalable-hpc-singularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemote Sensing Environments in Scalable HPC Singularity Images\u003c/h1\u003e\n\u003cp\u003eThe purpose of this singularity images and setup is to allow for development of an active pipeline between image resources, hosting, ingestion protocol into Google Earth Engine and retentation on volume as needed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-main-build-components--should-include\" class=\"anchor\" href=\"#main-build-components--should-include\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMain Build components ( Should include)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePlanet API Download Client to download images from Planet API \u003cstrong\u003eIncluded\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eFTP client to pull images from existing sftp or ftp addresses \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eInclude additional tools (shapely, pdal, rasterio) \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularity container which consists of further tools to manipulate and preprocess imagery \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularity container which is mounted with large shared volume but individual user folder \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eEarth Engine Upload and processing client including Earth Engine API client \u003cstrong\u003eIncluded\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eJupyter notebook to connnect to mounted volume for local analysis and export of image or remote analysis and export from Google Earth Engine \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eDrive to access google drive: Check if all tasks have completed and download from google drive, Verify and delete to preserve google drive storage \u003cstrong\u003eIn progress\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-possible-integrations-and-enhancements\" class=\"anchor\" href=\"#possible-integrations-and-enhancements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePossible Integrations and Enhancements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eLeaflet based imagery and vector visualization\u003c/li\u003e\n\u003cli\u003eImage tiling so user can visualize on the leaflet window within the Jupyter notebooks\u003c/li\u003e\n\u003cli\u003eEnd results include QGIS with X11 support to allow user to generate maps and export as images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-planet-cli-in-a-docker-container\" class=\"anchor\" href=\"#planet-cli-in-a-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlanet CLI in a Docker Container\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eIn Progress\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-planet-cli-in-a-singularity-container\" class=\"anchor\" href=\"#planet-cli-in-a-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlanet CLI in a Singularity Container\u003c/h2\u003e\n\u003cp\u003eSingularity files are in the \u003ccode\u003e/basic\u003c/code\u003e and \u003ccode\u003e/osgeo\u003c/code\u003e folders. The \u003ccode\u003e/osgeo\u003c/code\u003e container is maintained by Tyson Swetnam on \u003ca href=\"\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTo build a container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd basic/\nsudo singularity build basic.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003ecd osgeo/\nsudo singularity build osgeo.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\" class=\"anchor\" href=\"#moving-files-from-drivegoogle-into-an-atmosphere-or-jetstream-vm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMoving files from Drive.Google into an Atmosphere or Jetstream VM\u003c/h3\u003e\n\u003cp\u003eIn the US, academic institutions have increasingly established email accounts through Google.\nSome institutions have unlimited storage on \u003ca href=\"https://drive.google.com\" rel=\"nofollow\"\u003eDrive\u003c/a\u003e\nas a service for faculty and students.\u003c/p\u003e\n\u003cp\u003eOne of the issues with uploading / downloading a large number of images to or from a Drive\naccount through the web browser (Chrome or Firefox) is the number of files allowed,\nthe speed of the uploads, and the size of the downloads.\u003c/p\u003e\n\u003cp\u003eTypically a download directly from a Drive account through Chrome is limited to \u0026lt;2Gb\nand is zipped by Google before starting.\u003c/p\u003e\n\u003cp\u003eWhile the browser can work well for easily uploading a large number sUAS images from a collection,\ndownloading the images as .zip files from the Google.drive can become tiresome and difficult.\u003c/p\u003e\n\u003cp\u003eTo get around these problems we can use 3rd party applications like FUSE and\n\u003ca href=\"https://github.com/odeke-em/drive\"\u003e\u003ccode\u003edrive\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fuse-client-ocamfluse\" class=\"anchor\" href=\"#fuse-client-ocamfluse\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFUSE client \u003ccode\u003eocamfluse\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/astrada/google-drive-ocamlfuse\"\u003e\u003ccode\u003egoogle-drive-ocamlfuse\u003c/code\u003e\u003c/a\u003e is a Google Drive Client written in OCaml. This is tested for Ubuntu systems.\u003c/p\u003e\n\u003cp\u003eIt can mount your google drive as a folder visible in the file tree. FUSE is slower than other methods like iRODS or \u003ccode\u003eDrive\u003c/code\u003e, but allows for GUI based drag and drop transfers.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo add-apt-repository ppa:alessandro-strada/ppa\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get update\nsudo apt-get install google-drive-ocamlfuse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the app the first time to get the authentication certificate from Google\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egoogle-drive-ocamlfuse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate a folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir ~/googledrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMount the new googledrive\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egoogle-drive-ocamlfuse ~/googledrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOpen in your favorite file explorer.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-go\" class=\"anchor\" href=\"#install-go\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall \u003ccode\u003eGo\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eDrive uses the \u003ccode\u003ego\u003c/code\u003e language. In order to work with it you need to \u003ca href=\"https://golang.org/doc/install\" rel=\"nofollow\"\u003einstall \u003ccode\u003ego\u003c/code\u003e\u003c/a\u003e onto the VM.\u003c/p\u003e\n\u003cp\u003eRemove any other go packages (particularly gccgo-go)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get -y autoremove gccgo-go\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://storage.googleapis.com/golang/go1.8.1.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.8.1.linux-amd64.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn \u003ccode\u003e/etc/profile\u003c/code\u003e add: \u003ccode\u003eexport PATH=$PATH:/usr/local/go/bin\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecat \u0026lt;\u0026lt; ! \u0026gt;\u0026gt; /etc/profile\nexport PATH=$PATH:/usr/local/go/bin\n!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn \u003ccode\u003e~/.bashrc\u003c/code\u003e - \u003ccode\u003esudo nano ~/.bashrc\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAdd the GOPATH directly from terminal:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecat \u0026lt;\u0026lt; ! \u0026gt;\u0026gt; ~/.bashrc\nexport GOPATH=\\$HOME/go\nexport PATH=\\$GOPATH:\\$GOPATH/bin:\\$PATH\n!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSource the new \u003ccode\u003e~/.bashrc\u003c/code\u003e close the terminal and reopen\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esource ~/.bashrc \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFollow the \u003ccode\u003ego\u003c/code\u003einstructions to \u003ca href=\"https://golang.org/doc/install#testing\" rel=\"nofollow\"\u003etest the installation\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-drive-a-drivegoogle-client-for-commandline\" class=\"anchor\" href=\"#install-drive-a-drivegoogle-client-for-commandline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall \u003ccode\u003edrive\u003c/code\u003e a drive.google client for commandline\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/odeke-em/drive#installing\"\u003e\u003ccode\u003edrive\u003c/code\u003e\u003c/a\u003e is a command line client using Go language\u003c/p\u003e\n\u003cp\u003eInstall \u003ccode\u003edrive\u003c/code\u003e using \u003ccode\u003ego\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#install git\nsudo apt-get install git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003ecd ~/go\ngo get -u github.com/odeke-em/drive/cmd/drive\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-initialize-drive-with-your-google-account\" class=\"anchor\" href=\"#initialize-drive-with-your-google-account\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInitialize \u003ccode\u003edrive\u003c/code\u003e with your Google account\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003emkdir ~/gdrive\ndrive init ~/gdrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFollow the instructions for copying and pasting the html for authentication\u003c/p\u003e\n\u003cp\u003eTest your installation\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive ls\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pull-data-from-your-googledrive-account-onto-the-vm\" class=\"anchor\" href=\"#pull-data-from-your-googledrive-account-onto-the-vm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePull data from your Google.drive account onto the VM\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003edrive pull your/google/drive/folders/here\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-drive-on-ua-hpc\" class=\"anchor\" href=\"#using-drive-on-ua-hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Drive on UA HPC\u003c/h3\u003e\n\u003cp\u003eDrive is currently available on the ElGato login node\u003c/p\u003e\n\u003cp\u003eLoad \u003ccode\u003ego\u003c/code\u003e and \u003ccode\u003edrive\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load go\nmodule load drive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate a directory where you want to initiate \u003ccode\u003edrive\u003c/code\u003e - preferrably on your \u003ccode\u003e/xdisk/\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /xdisk/uid/\nmkdir gdrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInitiate the drive\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive init /xdisk/uid/gdrive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will get a request to \u003ccode\u003eVisit this URL to get an authorization code\u003c/code\u003e with a link to a long \u003ccode\u003ehttps://accounts.google.com/o/oauth2/xxxx\u003c/code\u003e URL - copy paste that link into your preferred browser.\u003c/p\u003e\n\u003cp\u003eYou will be taken to a Google login page, type in your email address (\u003ca href=\"mailto:uid@email.arizona.edu\"\u003euid@email.arizona.edu\u003c/a\u003e) and password.\u003c/p\u003e\n\u003cp\u003eCopy/Paste the code provided by Google back in your Terminal window where prompted: \u003ccode\u003ePaste the authorization code:\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eNow, check to see if your \u003ccode\u003edrive.google.com\u003c/code\u003e acount is active:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive ls\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA list of the directories in your \u003ccode\u003edrive.google.com\u003c/code\u003e account should be listed, e.g.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/project1\n/project2\n/reports1\n/pictures1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can \u003ccode\u003epull\u003c/code\u003e files or directories from your \u003ccode\u003edrive.google.com\u003c/code\u003e now using commands like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edrive pull project1/subfolder/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will see:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eResolving...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003efollowed by a spinning \u003ccode\u003e\\\u003c/code\u003e \u003ccode\u003e|\u003c/code\u003e \u003ccode\u003e/\u003c/code\u003e \u003ccode\u003e-\u003c/code\u003e set of symbols\u003c/p\u003e\n\u003cp\u003eThe folder contents will be displayed:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e /project1/subfolder/file1.csv\n...\n+ /project1/subfolder/file955.csv\n+ /project1/subfolder/file966.csv\nAddition count 966 src: 5.02GB\nProceed with the changes? [Y/n]:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSelect \u003ccode\u003ey\u003c/code\u003e and the download will proceed.\u003c/p\u003e\n\u003cp\u003eTypical speeds are between 10 and 50 Mb/s.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eProceed with the changes? [Y/n]:y\n 5392682146 / 5392682146 [==========================================================================================================================] 100.00% 1m55s\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setting-up-cyverse-data-store-and-irods-icommands\" class=\"anchor\" href=\"#setting-up-cyverse-data-store-and-irods-icommands\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetting up CyVerse Data Store and iRods iCommands\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\" rel=\"nofollow\"\u003eInstructions\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ iinit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will be queried to set up your \u003ccode\u003eirods_environment.json\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eEnter the following:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003estatement\u003c/th\u003e\n\u003cth\u003einput\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eDNS\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003edata.cyverse.org\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eport number\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003e1247\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003euser name\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003eyour user name\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ezone\u003c/td\u003e\n\u003ctd\u003e\u003cem\u003eiplant\u003c/em\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSet up auto-complete for iCommands\n\u003ca href=\"https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands\" rel=\"nofollow\"\u003einstructions\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload \u003ca href=\"https://pods.iplantcollaborative.org/wiki/download/attachments/6720192/i-commands-auto.bash\" rel=\"nofollow\"\u003ei-commands-auto.bash\u003c/a\u003e.\nIn your home directory, rename i-commands-auto.bash to .i-commands-auto.bash\nIn your .bashrc or .bash_profile, enter the following:\nsource .i-commands-auto.bash\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1611393577.0
  },
  {
    "data_format": 2,
    "description": "A Containerized Shiny App",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jekriske/shinyapp",
    "latest_release": null,
    "readme": "\u003cp\u003eThis is a proof of concept demonstrating a Shiny application within a container.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1519301630.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for CAESAR software tool",
    "filenames": [
      "caesar/Singularity-fromdocker.xenial",
      "caesar/Singularity.xenial",
      "caesar-base/Singularity.xenial"
    ],
    "full_name": "SKA-INAF/caesar-container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-caesar-container\" class=\"anchor\" href=\"#caesar-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecaesar-container\u003c/h1\u003e\n\u003cp\u003eSingularity container for CAESAR software tool\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eAbout\u003c/strong\u003e\n\u003c/h2\u003e\n\u003cp\u003eThis repository provides the following Singularity recipe files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ecaesar-base\u003c/strong\u003e: recipe to build a Singularity base container for CAESAR tool (maintained at \u003ca href=\"https://github.com/SKA-INAF/caesar.git\"\u003ehttps://github.com/SKA-INAF/caesar.git\u003c/a\u003e) from a docker Ubuntu image (Ubuntu 16 xenial). The built container will provide all the dependencies needed to build CAESAR:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eROOT\u003c/li\u003e\n\u003cli\u003eOpenCV\u003c/li\u003e\n\u003cli\u003eBOOST\u003c/li\u003e\n\u003cli\u003eLog4Cxx\u003c/li\u003e\n\u003cli\u003eJsoncpp\u003c/li\u003e\n\u003cli\u003ecfitsio\u003c/li\u003e\n\u003cli\u003eprotobuf\u003c/li\u003e\n\u003cli\u003eMPICH\u003c/li\u003e\n\u003cli\u003eOpenMPI\u003c/li\u003e\n\u003cli\u003epython 2.7 + modules (numpy, astropy, pyfits)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe container is used mainly to build CAESAR upon it.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003ecaesar\u003c/strong\u003e: recipe to build a CAESAR singularity container from latest release (\u003ca href=\"https://github.com/SKA-INAF/caesar.git\"\u003ehttps://github.com/SKA-INAF/caesar.git\u003c/a\u003e)) using a local caesar base image (built using previous recipe).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-build-caesar-images\" class=\"anchor\" href=\"#how-to-build-caesar-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eHow to build CAESAR images?\u003c/strong\u003e\n\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003ePrerequisites\u003c/strong\u003e\n\u003c/h3\u003e\n\u003cp\u003eTo build images on your system you must have a recent version of Singularity (\u003ca href=\"https://github.com/singularityware/singularity.git\"\u003ehttps://github.com/singularityware/singularity.git\u003c/a\u003e) installed. CAESAR makes use of singularity apps so a singularity version \u0026gt;2.3 is required. A system-wide installation is required, e.g. you need root permissions on the build system.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build-images\" class=\"anchor\" href=\"#build-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eBuild images\u003c/strong\u003e\n\u003c/h3\u003e\n\u003cp\u003eTo build images do the following:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity build [IMAGE_NAME] [RECIPE_FILE]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eIMAGE_NAME\u003c/code\u003e: Name of image file you want to be created (e.g. caesar-base.simg for base container or caesar.simg for caesar container).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eRECIPE_FILE\u003c/code\u003e: Name of the recipe file provided in this repository (e.g. Singularity.xenial)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use-caesar-images\" class=\"anchor\" href=\"#how-to-use-caesar-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eHow to use CAESAR images?\u003c/strong\u003e\n\u003c/h2\u003e\n\u003cp\u003eTo see the list of apps installed in your CAESAR container (say named \u003ccode\u003ecaesar.simg\u003c/code\u003e):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e [...]$ singularity apps caesar.simg\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo run one app (say the \u003ccode\u003esfinder\u003c/code\u003e app):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e[...]$ singularity run --app sfinder caesar.simg --config=myconfig.cfg\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602170137.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "dockerfiles/Singularity-dota.simg",
      "dockerfiles/Singularity-dotaservice.simg"
    ],
    "full_name": "bglick13/dotaservice",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-dotaservice\" class=\"anchor\" href=\"#dotaservice\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDotaService\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"dotaservice-icon.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"dotaservice-icon.png\" alt=\"dotaservice icon\" width=\"128\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eNOTE: The project that uses the dotaservice in a k8s environment is the \u003ca href=\"https://github.com/TimZaman/dotaclient\"\u003eDotaClient\u003c/a\u003e repo.\u003c/p\u003e\n\u003cp\u003eDotaService is a service to play Dota 2 through gRPC. There are first class python bindings\nand examples, so you can play dota as you would use the OpenAI gym API.\u003c/p\u003e\n\u003cp\u003eIt\u0027s fully functional and super lightweight. Starting Dota \u003ccode\u003eobs = env.reset()\u003c/code\u003e takes 5 seconds,\nand each \u003ccode\u003eobs = env.step(action)\u003c/code\u003e in the environment takes between 10 and 30 ms.\u003c/p\u003e\n\u003cp\u003eYou can even set the config of \u003ccode\u003erender=True\u003c/code\u003e and you can watch the game play live. Each game will\nhave a uuid and folder associated where there\u0027s a Dota demo (replay) and console logs.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"demo.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"demo.gif\" alt=\"demo\" width=\"600\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-dotaservice-locally\" class=\"anchor\" href=\"#run-dotaservice-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun DotaService Locally\u003c/h2\u003e\n\u003cp\u003eRun the DotaService so you can connect your client to it later. Only one client per server\nis supported, and only one DotaService per VM (eg local or one per docker container).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython3 -m dotaservice\n\u0026gt;\u0026gt;\u0026gt; Serving on 127.0.0.1:13337\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-dotaservice-distributed\" class=\"anchor\" href=\"#run-dotaservice-distributed\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun DotaService Distributed\u003c/h2\u003e\n\u003cp\u003eSee \u003ca href=\"docker/README.md\"\u003edocker/README.md\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo run two dockerservice instances, one on port \u003ccode\u003e13337\u003c/code\u003e and one on \u003ccode\u003e13338\u003c/code\u003e, f.e. run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run -dp 13337:13337 ds\ndocker run -dp 13338:13337 ds\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can run as many as you want, until you run out of ports or ip addresses. If you are wearing\nyour fancy pants, use Kubernetes to deploy gazillions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-client-code\" class=\"anchor\" href=\"#client-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eClient Code\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egrpclib\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eclient\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eChannel\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eprotobuf\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eDotaService_grpc\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eDotaServiceStub\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eprotobuf\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eDotaService_pb2\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAction\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eprotobuf\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eDotaService_pb2\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eConfig\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# Connect to the DotaService.\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eenv\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eDotaServiceStub\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eChannel\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\u0027127.0.0.1\u0027\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e13337\u003c/span\u003e))\n\n\u003cspan class=\"pl-c\"\u003e# Get the initial observation.\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eobservation\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eawait\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eenv\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003ereset\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eConfig\u003c/span\u003e())\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-en\"\u003erange\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e8\u003c/span\u003e):\n    \u003cspan class=\"pl-c\"\u003e# Sample an action from the action protobuf\u003c/span\u003e\n    \u003cspan class=\"pl-s1\"\u003eaction\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAction\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eMoveToLocation\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ex\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e.., \u003cspan class=\"pl-s1\"\u003ey\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e.., \u003cspan class=\"pl-s1\"\u003ez\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e..)\n    \u003cspan class=\"pl-c\"\u003e# Take an action, returning the resulting observation.\u003c/span\u003e\n    \u003cspan class=\"pl-s1\"\u003eobservation\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eawait\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eenv\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003estep\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eaction\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis is very useful to provide an environment for reinforcement learning, and service aspect of it makes it\nespecially useful for distributed training. I am planning to provide a client python\nmodule for this (\u003ccode\u003ePyDota\u003c/code\u003e) that mimics typical OpenAI gym APIs. Maybe I won\u0027t even make PyDota\nand the gRPC client is enough.\u003c/p\u003e\n\u003cdiv\u003e\n\u003ca href=\"dotaservice.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"dotaservice.png\" alt=\"dotaservice connections\" width=\"680\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePython 3.7\u003c/li\u003e\n\u003cli\u003eUnix: MacOS, Ubuntu. A dockerfile is also provided see: \u003ca href=\"docker/README.md\"\u003edocker/README.md\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eInstalling from pypi:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip3 install dotaservice\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor development; installing from source:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip3 install -e \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e(Optional) Compile the protos for Python (run from repository root):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython3 -m grpc_tools.protoc -I. --python_out=. --python_grpc_out=. --grpc_python_out=. dotaservice/protos/\u003cspan class=\"pl-k\"\u003e*\u003c/span\u003e.proto\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h1\u003e\n\u003cp\u003eMy dev notes: \u003ca href=\"NOTES.md\"\u003eNOTES.md\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-acknowledgements\" class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgements\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eOpenAI Dota crew\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://karpathy.github.io/2016/05/31/rl/\" rel=\"nofollow\"\u003eKarpathy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eJan Ivanecky\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Nostrademous\"\u003eNostrademous\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1585945278.0
  },
  {
    "data_format": 2,
    "description": "Container Camp test repo",
    "filenames": [
      "tensorflow/Singularity",
      "R/Singularity",
      "makeflow/Singularity",
      "pdal/Singularity"
    ],
    "full_name": "tyson-swetnam/cc-camp",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cc-camp\" class=\"anchor\" href=\"#cc-camp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecc-camp\u003c/h1\u003e\n\u003cp\u003eContainer Camp test repo\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1525835054.0
  },
  {
    "data_format": 2,
    "description": "Atom.io editor in a singularity container.",
    "filenames": [
      "Singularity",
      "Singularity.1.24.0"
    ],
    "full_name": "jekriske/atom",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cc-camp\" class=\"anchor\" href=\"#cc-camp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecc-camp\u003c/h1\u003e\n\u003cp\u003eContainer Camp test repo\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1519814161.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for pandoc.",
    "filenames": [
      "2.2.1/Singularity"
    ],
    "full_name": "icaoberg/singularity-pandoc",
    "latest_release": "2.2.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-pandoc\" class=\"anchor\" href=\"#singularity-pandoc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-pandoc\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://pandoc.org/\" rel=\"nofollow\"\u003epandoc\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities",
      "pandoc"
    ],
    "updated_at": 1619481049.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "photocyte/recoll-webui_singularity",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity file for the recoll-webui.\u003c/p\u003e\n\u003cp\u003eImage building handled by \u003ca href=\"https://singularity-hub.org\" rel=\"nofollow\"\u003esingularity-hub.org\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://photocyte/recoll-webui_singularity`\nsingularity exec --cleanenv recoll-webui_singularity_latest.sif /recollwebui/webui-standalone.py\n## Then navigate to http://127.0.0.1:13337\n## The .recoll directory in your home directory will need to be, or be symlinked to, a real recoll index directory, included a previous indexed xapiandb \n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1610240040.0
  },
  {
    "data_format": 2,
    "description": "INA Speech Tools in a Singularity Container for AMP",
    "filenames": [
      "Singularity.recipe"
    ],
    "full_name": "AudiovisualMetadataPlatform/ina-speech-tools-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ina-speech-tools-singularity\" class=\"anchor\" href=\"#ina-speech-tools-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eina-speech-tools-singularity\u003c/h1\u003e\n\u003cp\u003eINA Speech Tools in a Singularity Container for AMP\u003c/p\u003e\n\u003cp\u003eTo build:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etime singularity build --fakeroot ina-speech-tools-singularity.sif Singularity.recipe\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./ina-speech-tools-singularity.sif \u0026lt;audio file\u0026gt;  \u0026lt;output json\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBoth the audio file and the output file should be either in the user\u0027s home directory or /tmp.  Other options can be handled by using run-time binding.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1617321582.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for boxes.",
    "filenames": [
      "1.3/Singularity"
    ],
    "full_name": "icaoberg/singularity-boxes",
    "latest_release": "1.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-boxes\" class=\"anchor\" href=\"#singularity-boxes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-boxes\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://boxes.thomasjensen.com/\" rel=\"nofollow\"\u003eboxes\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities",
      "boxes"
    ],
    "updated_at": 1619480922.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "zhuyawen/amazon-dsstne",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-amazon-dsstne-deep-scalable-sparse-tensor-network-engine\" class=\"anchor\" href=\"#amazon-dsstne-deep-scalable-sparse-tensor-network-engine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAmazon DSSTNE: Deep Scalable Sparse Tensor Network Engine\u003c/h1\u003e\n\u003cp\u003eDSSTNE (pronounced \"Destiny\") is an open source software library for training and deploying recommendation\nmodels with sparse inputs, fully connected hidden layers, and sparse outputs. Models with weight matrices\nthat are too large for a single GPU can still be trained on a single host. DSSTNE has been used at Amazon\nto generate personalized product recommendations for our customers at Amazon\u0027s scale. It is designed for\nproduction deployment of real-world applications which need to emphasize speed and scale over experimental\nflexibility.\u003c/p\u003e\n\u003cp\u003eDSSTNE was built with a number of features for production recommendation workloads:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eMulti-GPU Scale\u003c/strong\u003e: Training and prediction\nboth scale out to use multiple GPUs, spreading out computation\nand storage in a model-parallel fashion for each layer.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eLarge Layers\u003c/strong\u003e: Model-parallel scaling enables larger networks than\nare possible with a single GPU.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eSparse Data\u003c/strong\u003e: DSSTNE is optimized for fast performance on sparse datasets, common in recommendation\nproblems. Custom GPU kernels perform sparse computation on the GPU, without filling in lots of zeroes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-benchmarks\" class=\"anchor\" href=\"#benchmarks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBenchmarks\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003escottlegrand@ reported [near-linear scaling with multiple GPUs] on the MovieLens recommendation problem\n(\u003ca href=\"https://medium.com/@scottlegrand/first-dsstne-benchmarks-tldr-almost-15x-faster-than-tensorflow-393dbeb80c0f#.ghe74fu1q\" rel=\"nofollow\"\u003ehttps://medium.com/@scottlegrand/first-dsstne-benchmarks-tldr-almost-15x-faster-than-tensorflow-393dbeb80c0f#.ghe74fu1q\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eDirections on how to run a benchmark can be found in \u003ca href=\"benchmarks/Benchmark.md\"\u003ehere\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-scaling-up\" class=\"anchor\" href=\"#scaling-up\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eScaling up\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://blogs.aws.amazon.com/bigdata/post/TxGEL8IJ0CAXTK/Generating-Recommendations-at-Amazon-Scale-with-Apache-Spark-and-Amazon-DSSTNE\" rel=\"nofollow\"\u003eUsing Spark in AWS EMR and Dockers in AWS ECS \u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"LICENSE\"\u003eLicense\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eFollow \u003ca href=\"docs/getting_started/setup.md\"\u003eSetup\u003c/a\u003e for step by step instructions on installing and setting up DSSTNE\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-user-guide\" class=\"anchor\" href=\"#user-guide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUser Guide\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCheck \u003ca href=\"docs/getting_started/userguide.md\"\u003eUser Guide\u003c/a\u003e for detailed information about the features in DSSTNE\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCheck \u003ca href=\"docs/getting_started/examples.md\"\u003eExamples\u003c/a\u003e to start trying your first Neural Network Modeling using DSSTNE\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-qa\" class=\"anchor\" href=\"#qa\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQ\u0026amp;A\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"FAQ.md\"\u003eFAQ\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1517840829.0
  },
  {
    "data_format": 2,
    "description": "Docker container recipe for EnsEMBL API",
    "filenames": [
      "Singularity",
      "Singularity.100",
      "Singularity.99"
    ],
    "full_name": "ikmb/ensembl-api",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"images/ikmb_bfx_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/ikmb_bfx_logo.png\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-ensembl-perl-api\" class=\"anchor\" href=\"#ensembl-perl-api\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnsEMBL Perl API\u003c/h1\u003e\n\u003cp\u003eContainer recipe for EnsEMBL API\u003c/p\u003e\n\u003cp\u003eThe corresponding Singularity Hub URL\u003c/p\u003e\n\u003cp\u003eshub://ikmb/ensembl-api:97\nshub://ikmb/ensembl-api:99\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1592419007.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for CraiG",
    "filenames": [
      "Singularity"
    ],
    "full_name": "VEuPathDB/singularity-craig",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-craig\" class=\"anchor\" href=\"#singularity-recipe-for-craig\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipe for CraiG\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-craig-source\" class=\"anchor\" href=\"#craig-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCraiG Source\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/axl-bernal/CraiG\"\u003ehttps://github.com/axl-bernal/CraiG\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-image\" class=\"anchor\" href=\"#build-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild image\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://github.com/EuPathDB/vagrant-rpmbuild\"\u003e\u003ccode\u003evagrant-rpmbuild\u003c/code\u003e\u003c/a\u003e\nvirtual machine provisions Singularity suitable for building.\u003c/p\u003e\n\u003cp\u003eTypically you should remove any existing image so you get clean build.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ rm -f craig.simg\n$ sudo singularity build craig.simg Singularity \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003c/h2\u003e\n\u003cp\u003e$ singularity pull --name craig.simg shub://mheiges/singularity-craig\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-exec\" class=\"anchor\" href=\"#exec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExec\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec --bind /eupath craig.simg ls  /eupath\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1528493265.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "maxemil/originations-placement",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-craig\" class=\"anchor\" href=\"#singularity-recipe-for-craig\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipe for CraiG\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-craig-source\" class=\"anchor\" href=\"#craig-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCraiG Source\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/axl-bernal/CraiG\"\u003ehttps://github.com/axl-bernal/CraiG\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-image\" class=\"anchor\" href=\"#build-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild image\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://github.com/EuPathDB/vagrant-rpmbuild\"\u003e\u003ccode\u003evagrant-rpmbuild\u003c/code\u003e\u003c/a\u003e\nvirtual machine provisions Singularity suitable for building.\u003c/p\u003e\n\u003cp\u003eTypically you should remove any existing image so you get clean build.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ rm -f craig.simg\n$ sudo singularity build craig.simg Singularity \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003c/h2\u003e\n\u003cp\u003e$ singularity pull --name craig.simg shub://mheiges/singularity-craig\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-exec\" class=\"anchor\" href=\"#exec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExec\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec --bind /eupath craig.simg ls  /eupath\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603131331.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ronjagrosz/manta_singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-a-singularity-container-for-installing-manta\" class=\"anchor\" href=\"#a-singularity-container-for-installing-manta\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA singularity container for installing manta\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-with\" class=\"anchor\" href=\"#build-with\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild with:\u003c/h2\u003e\n\u003cp\u003esudo singularity build manta.simg Singularity\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuild files are found in ./build\u003c/li\u003e\n\u003cli\u003eInstall files are found in ./install\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1568290651.0
  },
  {
    "data_format": 2,
    "description": "geant4 in contianer.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ifurther/geant4-docker",
    "latest_release": "10.7.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-a-singularity-container-for-installing-manta\" class=\"anchor\" href=\"#a-singularity-container-for-installing-manta\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA singularity container for installing manta\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-with\" class=\"anchor\" href=\"#build-with\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild with:\u003c/h2\u003e\n\u003cp\u003esudo singularity build manta.simg Singularity\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuild files are found in ./build\u003c/li\u003e\n\u003cli\u003eInstall files are found in ./install\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1615754654.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for git",
    "filenames": [
      "Singularity",
      "git-2.16.1/Singularity.2.16.1",
      "git-2.15.0/Singularity.2.15.0"
    ],
    "full_name": "jekriske/git",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-a-singularity-container-for-installing-manta\" class=\"anchor\" href=\"#a-singularity-container-for-installing-manta\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA singularity container for installing manta\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-with\" class=\"anchor\" href=\"#build-with\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild with:\u003c/h2\u003e\n\u003cp\u003esudo singularity build manta.simg Singularity\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuild files are found in ./build\u003c/li\u003e\n\u003cli\u003eInstall files are found in ./install\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1517474496.0
  },
  {
    "data_format": 2,
    "description": "Images to be run on Wilson Cluster at FNAL",
    "filenames": [
      "Singularity.py3_tf114"
    ],
    "full_name": "ulorentz/singularity_images",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-a-singularity-container-for-installing-manta\" class=\"anchor\" href=\"#a-singularity-container-for-installing-manta\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA singularity container for installing manta\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-with\" class=\"anchor\" href=\"#build-with\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild with:\u003c/h2\u003e\n\u003cp\u003esudo singularity build manta.simg Singularity\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuild files are found in ./build\u003c/li\u003e\n\u003cli\u003eInstall files are found in ./install\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1586556737.0
  },
  {
    "data_format": 2,
    "description": "Singularity container with Spack",
    "filenames": [
      "Singularity.spack-root",
      "Singularity.spack-rhel",
      "Singularity.spackbase",
      "Singularity.spack-lmod",
      "Singularity.spack-fastqvalidator",
      "Singularity.spack-bowtie",
      "Singularity.spack"
    ],
    "full_name": "baberlevi/spack-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-work-in-progress\" class=\"anchor\" href=\"#work-in-progress\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ework in progress\u003c/h1\u003e\n\u003cp\u003eattempt to build a base singularity image with spack that can be used as the bootstrap for\nother singularity images that would perform the spack install of a particular package\u003c/p\u003e\n\u003cp\u003ecurrently having an issue with stage directory for spack attempting to write to\nthe immutable squashfs\u003c/p\u003e\n\u003cp\u003eas expected, the child container will happily install during %post since it can write\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1521605340.0
  },
  {
    "data_format": 2,
    "description": "Fusion genes identification pipeline ",
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "Clinical-Genomics-Lund/nextflow_rnaseqfus",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-work-in-progress\" class=\"anchor\" href=\"#work-in-progress\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ework in progress\u003c/h1\u003e\n\u003cp\u003eattempt to build a base singularity image with spack that can be used as the bootstrap for\nother singularity images that would perform the spack install of a particular package\u003c/p\u003e\n\u003cp\u003ecurrently having an issue with stage directory for spack attempting to write to\nthe immutable squashfs\u003c/p\u003e\n\u003cp\u003eas expected, the child container will happily install during %post since it can write\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1612308500.0
  },
  {
    "data_format": 2,
    "description": "Singularity container recipe for CADD v1.5",
    "filenames": [
      "Singularity"
    ],
    "full_name": "Clinical-Genomics-Lund/CADD-container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-work-in-progress\" class=\"anchor\" href=\"#work-in-progress\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ework in progress\u003c/h1\u003e\n\u003cp\u003eattempt to build a base singularity image with spack that can be used as the bootstrap for\nother singularity images that would perform the spack install of a particular package\u003c/p\u003e\n\u003cp\u003ecurrently having an issue with stage directory for spack attempting to write to\nthe immutable squashfs\u003c/p\u003e\n\u003cp\u003eas expected, the child container will happily install during %post since it can write\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1616792704.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for svn2git",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jekriske/svn2git",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-work-in-progress\" class=\"anchor\" href=\"#work-in-progress\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ework in progress\u003c/h1\u003e\n\u003cp\u003eattempt to build a base singularity image with spack that can be used as the bootstrap for\nother singularity images that would perform the spack install of a particular package\u003c/p\u003e\n\u003cp\u003ecurrently having an issue with stage directory for spack attempting to write to\nthe immutable squashfs\u003c/p\u003e\n\u003cp\u003eas expected, the child container will happily install during %post since it can write\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1511601734.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "src/archive/Singularity.gym"
    ],
    "full_name": "MostafaRizk/TS-Platform",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-work-in-progress\" class=\"anchor\" href=\"#work-in-progress\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ework in progress\u003c/h1\u003e\n\u003cp\u003eattempt to build a base singularity image with spack that can be used as the bootstrap for\nother singularity images that would perform the spack install of a particular package\u003c/p\u003e\n\u003cp\u003ecurrently having an issue with stage directory for spack attempting to write to\nthe immutable squashfs\u003c/p\u003e\n\u003cp\u003eas expected, the child container will happily install during %post since it can write\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619701199.0
  },
  {
    "data_format": 2,
    "description": "A low fat version of lolcow",
    "filenames": [
      "Singularity",
      "Singularity.lowfat"
    ],
    "full_name": "jekriske/lolcow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-work-in-progress\" class=\"anchor\" href=\"#work-in-progress\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ework in progress\u003c/h1\u003e\n\u003cp\u003eattempt to build a base singularity image with spack that can be used as the bootstrap for\nother singularity images that would perform the spack install of a particular package\u003c/p\u003e\n\u003cp\u003ecurrently having an issue with stage directory for spack attempting to write to\nthe immutable squashfs\u003c/p\u003e\n\u003cp\u003eas expected, the child container will happily install during %post since it can write\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1524977683.0
  },
  {
    "data_format": 2,
    "description": "Hierarchical Bayesian State Trace Analysis",
    "filenames": [
      "tools/Singularity"
    ],
    "full_name": "psadil/staHB",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-stahb\" class=\"anchor\" href=\"#stahb\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003estaHB\u003c/h1\u003e\n\u003cp\u003eHierarchical Bayesian State Trace Analysis\u003c/p\u003e\n\u003cp\u003eThis repository aims to compare a few different methods of conducting an STA. As such, it includes some code from both \u003ca href=\"https://github.com/michaelkalish/STA\"\u003eKalish, M. L., Dunn, J. C., Burdakov, O. P., \u0026amp; Sysoev, O. (2016). A statistical test of the equality of latent orders. Journal of Mathematical Psychology, 70, 1-11.\u003c/a\u003e and \u003ca href=\"http://dx.doi.org/10.1016/j.jmp.2015.08.004\" rel=\"nofollow\"\u003eDavis-Stober, C. P., Morey, R. D., Gretton, M., \u0026amp; Heathcote, A. (2016). Bayes factors for state-trace analysis. Journal of Mathematical Psychology, 72, 116-129.\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1538170618.0
  },
  {
    "data_format": 2,
    "description": "A base R build",
    "filenames": [
      "Singularity",
      "Singularity.3.4.4",
      "Singularity.3.4.3",
      "Singularity.3.4.2"
    ],
    "full_name": "jekriske/r-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-stahb\" class=\"anchor\" href=\"#stahb\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003estaHB\u003c/h1\u003e\n\u003cp\u003eHierarchical Bayesian State Trace Analysis\u003c/p\u003e\n\u003cp\u003eThis repository aims to compare a few different methods of conducting an STA. As such, it includes some code from both \u003ca href=\"https://github.com/michaelkalish/STA\"\u003eKalish, M. L., Dunn, J. C., Burdakov, O. P., \u0026amp; Sysoev, O. (2016). A statistical test of the equality of latent orders. Journal of Mathematical Psychology, 70, 1-11.\u003c/a\u003e and \u003ca href=\"http://dx.doi.org/10.1016/j.jmp.2015.08.004\" rel=\"nofollow\"\u003eDavis-Stober, C. P., Morey, R. D., Gretton, M., \u0026amp; Heathcote, A. (2016). Bayes factors for state-trace analysis. Journal of Mathematical Psychology, 72, 116-129.\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1521975422.0
  },
  {
    "data_format": 2,
    "description": "Rstudio Singularity Recipes",
    "filenames": [
      "Singularity",
      "Rstudio_Desktop-1.1.442/Singularity.1.1.442_desktop",
      "Rstudio_Desktop-1.1.447/Singularity.1.1.447_desktop",
      "Rstudio_Desktop-1.1.419/Singularity.1.1.419_desktop",
      "Rstudio_Desktop-1.1.414/Singularity.1.1.414_desktop",
      "Rstudio_Desktop-1.1.423/Singularity.1.1.423_desktop"
    ],
    "full_name": "jekriske/rstudio",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-stahb\" class=\"anchor\" href=\"#stahb\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003estaHB\u003c/h1\u003e\n\u003cp\u003eHierarchical Bayesian State Trace Analysis\u003c/p\u003e\n\u003cp\u003eThis repository aims to compare a few different methods of conducting an STA. As such, it includes some code from both \u003ca href=\"https://github.com/michaelkalish/STA\"\u003eKalish, M. L., Dunn, J. C., Burdakov, O. P., \u0026amp; Sysoev, O. (2016). A statistical test of the equality of latent orders. Journal of Mathematical Psychology, 70, 1-11.\u003c/a\u003e and \u003ca href=\"http://dx.doi.org/10.1016/j.jmp.2015.08.004\" rel=\"nofollow\"\u003eDavis-Stober, C. P., Morey, R. D., Gretton, M., \u0026amp; Heathcote, A. (2016). Bayes factors for state-trace analysis. Journal of Mathematical Psychology, 72, 116-129.\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1524977518.0
  },
  {
    "data_format": 2,
    "description": "VCF normalization",
    "filenames": [
      "Singularity/Singularity.v1.1",
      "Singularity/Singularity.v1.0"
    ],
    "full_name": "IARCbioinfo/vcf_normalization-nf",
    "latest_release": "v1.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-vcf_normalization-nf\" class=\"anchor\" href=\"#vcf_normalization-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003evcf_normalization-nf\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nextflow-pipeline-for-vcf-normalization\" class=\"anchor\" href=\"#nextflow-pipeline-for-vcf-normalization\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextflow pipeline for vcf normalization\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/IARCbioinfo/vcf_normalization-nf/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/972f6bfd365386090c6b3e3cd6e549a88d55259c394799f2be26101fa1495f52/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f7663665f6e6f726d616c697a6174696f6e2d6e662f747265652f6d61737465722e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/IARCbioinfo/vcf_normalization-nf/tree/master.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/iarcbioinfo/vcf_normalization-nf/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c5d5383ee3d6248c7d31b5fcaa21fc7d689b53ecb330dbfe628cd1fae38c853/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d72656164792d626c75652e737667\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/badge/docker-ready-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4381\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"vcf_normalization-nf.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"vcf_normalization-nf.png\" alt=\"Workflow representation\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eApply \u003ca href=\"http://samtools.github.io/bcftools/bcftools.html\" rel=\"nofollow\"\u003ebcftools norm\u003c/a\u003e to decompose and normalize variants from a set of VCF (compressed with gzip/bgzip).\u003c/p\u003e\n\u003cp\u003eThis scripts takes a set of a folder containing \u003ca href=\"https://samtools.github.io/hts-specs/VCFv4.2.pdf\" rel=\"nofollow\"\u003ecompressed VCF files\u003c/a\u003e (\u003ccode\u003e*.vcf.gz\u003c/code\u003e) as an input.\nIt consists at four piped steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(optional) filtering of variants (\u003ccode\u003ebcftoolvs view -f\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003esplit multiallelic sites into biallelic records (\u003ccode\u003ebcftools norm -m -\u003c/code\u003e) and left-alignment and normalization (\u003ccode\u003e-f ref\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003esorting (\u003ccode\u003ebcftools sort \u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eduplicate removal (\u003ccode\u003ebcftools norm -d exact\u003c/code\u003e) and compression (\u003ccode\u003e-Oz\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThis pipeline is based on \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003enextflow\u003c/a\u003e. As we have several nextflow pipelines, we have centralized the common information in the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository. Please read it carefully as it contains essential information for the installation, basic usage and configuration of nextflow and our pipelines.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExternal software:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://samtools.github.io/bcftools/bcftools.html\" rel=\"nofollow\"\u003ebcftools\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCaution\u003c/strong\u003e: \u003ccode\u003ebcftools\u003c/code\u003e has to be in your $PATH. Try each of the commands \u003ccode\u003ebcftools\u003c/code\u003e and \u003ccode\u003ebgzip\u003c/code\u003e, if it returns the options this is ok.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e--vcf_folder\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eFolder containing tumor zipped VCF files\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-mandatory\" class=\"anchor\" href=\"#mandatory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMandatory\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eExample value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e--ref\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/path/to/ref.fasta\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eReference fasta file indexed\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptional\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDefault value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e--output_folder\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003enormalized_VCF/\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eFolder to output resulting compressed vcf\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e--filter_opt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e-f PASS\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eOptions for bcftools view\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e--cpu\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003eNumber of cpus to use\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e--mem\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e8\u003c/td\u003e\n\u003ctd\u003eSize of memory used for mapping (in GB)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNote that the default is to filter variants with the PASS flag. To deactivate, use \u003ccode\u003e--filter_opt \" \"\u003c/code\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-flags\" class=\"anchor\" href=\"#flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFlags\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlags are special parameters without value.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e--help\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eDisplay help\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eSimple use case example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run iarcbioinfo/vcf_normalization-nf -r v1.1 -profile singularity --vcf_folder VCF/ --ref ref.fasta\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run the pipeline without singularity just remove \"-profile singularity\". Alternatively, one can run the pipeline using a docker container (-profile docker) the conda receipe containing all required dependencies (-profile conda).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eVCF.gz, VCF.gz.tbi\u003c/td\u003e\n\u003ctd\u003eCompressed normalized VCF files with indexes\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions\" class=\"anchor\" href=\"#contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eEmail\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eNicolas Alcala*\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:alcalan@iarc.fr\"\u003ealcalan@iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper to contact for support\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTiffany Delhomme\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:delhommet@students.iarc.fr\"\u003edelhommet@students.iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1590435797.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity.minimac4"
    ],
    "full_name": "h3abionet/chipimputation_evaluate_chips",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-chip-imputation-evaluation-workflow-h3abionetchipimputation_evaluate_chips\" class=\"anchor\" href=\"#chip-imputation-evaluation-workflow-h3abionetchipimputation_evaluate_chips\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChip imputation evaluation Workflow h3abionet/chipimputation_evaluate_chips\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/h3abionet/chipimputation\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9bdc33815fd93626a153c75edc2679d736b504273526d14b1f30776900d72b8b/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f63686970696d7075746174696f6e2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/h3abionet/chipimputation.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/17df893666bfa5cad487466d4476ab773ea5def560c04c50f45795017865e81c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33302e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.30.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/h3abionet/chipimputation\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a4a1755f55345897c83ea6a657c8396ebf4f9a4141d5b591d92241002094bc19/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f63686970696d7075746174696f6e2e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/chipimputation.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is to evaluate the imputation performance and accuracy of different arrays starting from sequence data.\nIt masks non tag variants for each array, and then impute to a reference panel using Minimac.\u003cbr\u003e\nIt is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner.\u003cbr\u003e\nIt comes with singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe evaluate_chips pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation and Configuration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eConfiguration for other clusters\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup-native-cluster\" class=\"anchor\" href=\"#setup-native-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup (native cluster)\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-headnode\" class=\"anchor\" href=\"#headnode\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeadnode\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e (can be installed as local user)\u003c/li\u003e\n\u003cli\u003eNXF_HOME needs to be set, and must be in the PATH\u003c/li\u003e\n\u003cli\u003eNote that we\u0027ve experienced problems running Nextflow when NXF_HOME is on an NFS mount.\u003c/li\u003e\n\u003cli\u003eThe Nextflow script also needs to be invoked in a non-NFS folder\u003c/li\u003e\n\u003cli\u003eJava 1.8+\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-compute-nodes\" class=\"anchor\" href=\"#compute-nodes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompute nodes\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe compute nodes need to have singularity installed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe compute nodes need access to shared storage for input, references, output\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe following commands need to be available in PATH on the compute nodes, in case of unavailabitity of singularity.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eminimac4\u003c/code\u003e from \u003ca href=\"http://mathgen.stats.ox.ac.uk/impute/impute_v2.html\" rel=\"nofollow\"\u003eMINIMAC4\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003evcftools\u003c/code\u003e from \u003ca href=\"https://vcftools.github.io/index.html\" rel=\"nofollow\"\u003eVCFtools\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ebcftools\u003c/code\u003efrom \u003ca href=\"https://samtools.github.io/bcftools/bcftools.html\" rel=\"nofollow\"\u003ebcftools\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ebgzip\u003c/code\u003e from \u003ca href=\"http://www.htslib.org\" rel=\"nofollow\"\u003ehtslib\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eeagle\u003c/code\u003e from \u003ca href=\"https://data.broadinstitute.org/alkesgroup/Eagle/\" rel=\"nofollow\"\u003eEagle\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epython2.7\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eR\u003c/code\u003e with the following packages ...\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1621014196.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "nasibehm/LBP3d",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lbp3d\" class=\"anchor\" href=\"#lbp3d\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLBP3d\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619057271.0
  },
  {
    "data_format": 2,
    "description": "a pipeline performing  snp variant calling using samtools mpileup",
    "filenames": [
      "Singularity"
    ],
    "full_name": "J35P312/PileupPipe",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pileuppipe\" class=\"anchor\" href=\"#pileuppipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePileupPipe\u003c/h1\u003e\n\u003cp\u003eRun SNP variant calling using gatk4 haplotypecaller. The pipeline performs annotation using vep and produces a vcf and excel file.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-locally\" class=\"anchor\" href=\"#run-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Locally\u003c/h1\u003e\n\u003cp\u003eAnalyse a single sample, using a gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow pileup_pipeline.nf --bam \u0026lt;input_bam\u0026gt; --working_dir \u0026lt;output_folder\u0026gt; --genelist \u0026lt;genelist\u0026gt; -w $TMPDIR -c \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnalyse a single sample, with no gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow pileup_pipeline.nf --bam \u0026lt;input_bam\u0026gt; --working_dir \u0026lt;output_folder\u0026gt; -w $TMPDIR -c \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-on-slurm\" class=\"anchor\" href=\"#run-on-slurm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun on slurm\u003c/h1\u003e\n\u003cp\u003eThe slurm scripts are wrappers around the pileup_pipeline.nf script.\u003c/p\u003e\n\u003cp\u003eAnalyse a single sample, using a gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch SubmitGeneList.sh \u0026lt;input_bam\u0026gt; \u0026lt;output_folder\u0026gt; \u0026lt;genelist\u0026gt; \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnalyse a single sample, with no gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch SubmitNoGeneList.sh  \u0026lt;input_bam\u0026gt; \u0026lt;output_folder\u0026gt; \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnalyse a folder containing bam files using a gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./snpPipe.sh \u0026lt;input_folder_with_bam\u0026gt; \u0026lt;output_folder\u0026gt; \u0026lt;genelist\u0026gt; \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h1\u003e\n\u003cp\u003eDownload and  install nextflow\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://www.nextflow.io/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003erun the setup script\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython setup.py \u0026gt; config.conf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDownload the singularity collection (note, the singularity collection needs to be placed in the pipeline folder)\u003c/p\u003e\n\u003cp\u003esingularity pull --name PileUp.simg shub://J35P312/PileupPipe\u003c/p\u003e\n\u003cp\u003eor build it yourself:\u003c/p\u003e\n\u003cp\u003esingularity build PileUp.simg Singularity\u003c/p\u003e\n\u003cp\u003eopen the config file using a text editor and change the path variables:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enano config.conf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe  reference (line 49) path is required:\u003c/p\u003e\n\u003cp\u003eAdditionally, you need to put a .vep folder containing the vep database insisde the PileupPipe folder.\u003c/p\u003e\n\u003cp\u003eIf you use the slurm wrappers, you need to edit the slurm account in the files,  SubmitGeneList.sh, and SubmitNoGeneList.sh\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-gene-list\" class=\"anchor\" href=\"#gene-list\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGene list\u003c/h1\u003e\n\u003cp\u003eThe gene list is a text file. Each line of the text file contains the HGNC symbol of a gene.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1552853604.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Git/Singularity.igv",
      "Git/Singularity.vcftools",
      "Git/Singularity.mcl",
      "Git/Singularity.orthofinder",
      "Git/Singularity.samtools",
      "Scaricati/Singularity.igv"
    ],
    "full_name": "DrVale83/bioinfo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pileuppipe\" class=\"anchor\" href=\"#pileuppipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePileupPipe\u003c/h1\u003e\n\u003cp\u003eRun SNP variant calling using gatk4 haplotypecaller. The pipeline performs annotation using vep and produces a vcf and excel file.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-locally\" class=\"anchor\" href=\"#run-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Locally\u003c/h1\u003e\n\u003cp\u003eAnalyse a single sample, using a gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow pileup_pipeline.nf --bam \u0026lt;input_bam\u0026gt; --working_dir \u0026lt;output_folder\u0026gt; --genelist \u0026lt;genelist\u0026gt; -w $TMPDIR -c \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnalyse a single sample, with no gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow pileup_pipeline.nf --bam \u0026lt;input_bam\u0026gt; --working_dir \u0026lt;output_folder\u0026gt; -w $TMPDIR -c \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-on-slurm\" class=\"anchor\" href=\"#run-on-slurm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun on slurm\u003c/h1\u003e\n\u003cp\u003eThe slurm scripts are wrappers around the pileup_pipeline.nf script.\u003c/p\u003e\n\u003cp\u003eAnalyse a single sample, using a gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch SubmitGeneList.sh \u0026lt;input_bam\u0026gt; \u0026lt;output_folder\u0026gt; \u0026lt;genelist\u0026gt; \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnalyse a single sample, with no gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch SubmitNoGeneList.sh  \u0026lt;input_bam\u0026gt; \u0026lt;output_folder\u0026gt; \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnalyse a folder containing bam files using a gene list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./snpPipe.sh \u0026lt;input_folder_with_bam\u0026gt; \u0026lt;output_folder\u0026gt; \u0026lt;genelist\u0026gt; \u0026lt;config_file_from_setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h1\u003e\n\u003cp\u003eDownload and  install nextflow\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://www.nextflow.io/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003erun the setup script\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython setup.py \u0026gt; config.conf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDownload the singularity collection (note, the singularity collection needs to be placed in the pipeline folder)\u003c/p\u003e\n\u003cp\u003esingularity pull --name PileUp.simg shub://J35P312/PileupPipe\u003c/p\u003e\n\u003cp\u003eor build it yourself:\u003c/p\u003e\n\u003cp\u003esingularity build PileUp.simg Singularity\u003c/p\u003e\n\u003cp\u003eopen the config file using a text editor and change the path variables:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enano config.conf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe  reference (line 49) path is required:\u003c/p\u003e\n\u003cp\u003eAdditionally, you need to put a .vep folder containing the vep database insisde the PileupPipe folder.\u003c/p\u003e\n\u003cp\u003eIf you use the slurm wrappers, you need to edit the slurm account in the files,  SubmitGeneList.sh, and SubmitNoGeneList.sh\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-gene-list\" class=\"anchor\" href=\"#gene-list\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGene list\u003c/h1\u003e\n\u003cp\u003eThe gene list is a text file. Each line of the text file contains the HGNC symbol of a gene.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1562162180.0
  },
  {
    "data_format": 2,
    "description": "Fedora Singularity image for running Montage workflows",
    "filenames": [
      "Singularity"
    ],
    "full_name": "pegasus-isi/fedora-montage",
    "latest_release": null,
    "readme": "\u003cp\u003eFedora Singularity image for running Montage workflows\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1500493956.0
  },
  {
    "data_format": 2,
    "description": "Emacs in a Docker container",
    "filenames": [
      "Singularity-ubuntu18.04-emacs27-slim"
    ],
    "full_name": "theasp/docker-emacs",
    "latest_release": null,
    "readme": "\u003cp\u003eFedora Singularity image for running Montage workflows\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "docker",
      "emacs"
    ],
    "updated_at": 1601662369.0
  },
  {
    "data_format": 2,
    "description": "Barcode/amplicon sequencing bioinformatics tool, chops up FASTQ reads into capture groups using fuzzy regular expressions instead of base-positions. Very flexible, very parallelized, order now !",
    "filenames": [
      "Singularity.v0.5.1-alpha",
      "Singularity.v0.5.0-alpha",
      "Singularity.v0.3.0-alpha"
    ],
    "full_name": "darachm/slapchop",
    "latest_release": "v0.2.0-alpha",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-slapchop\" class=\"anchor\" href=\"#slapchop\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSLAPCHOP\u003c/h1\u003e\n\u003cp\u003eSLAPCHOP.py parses Illumina reads using patterns to extract barcodes.\u003c/p\u003e\n\u003cp\u003eBy using fuzzy regular expressions we can chop robustly barcodes from\nindeterminate positions, filter the results based on sequence or match\nproperties, and reassemble a fastq record from the results.\u003c/p\u003e\n\u003cp\u003eAvailable as\n\u003ca href=\"https://www.singularity-hub.org/collections/1361\" rel=\"nofollow\"\u003ea singularity containter\u003c/a\u003e!\nSo you if you have\n\u003ca href=\"https://github.com/sylabs/singularity/releases\"\u003eSingularity\u003c/a\u003e\ninstalled you can just use it (without worrying about dependencies) with:\n\u003ccode\u003esingularity run shub://darachm/slapchop:latest -h\u003c/code\u003e (to download and show the\nargument help message for example). Then you use it like\n\u003ccode\u003esingularity run shub://darachm/slapchop:latest whatever.fastq arguments added\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eMore completely, this tool is a python script. You give it a FASTQ(Z) file and\nsome operations to do, and it\u0027ll do the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- read chunks of Illumina-format sequencing reads\n- apply a series of operations:\n    - match fuzzy regular expression to original sequence or previous\n        capture groups\n    - extract capture groups and start next operation\n- apply pythonic filters (pass/fail) on sequence or quality properties \n    (like average quality or group length)\n- apply pythonic constructors to construct new FASTQ read from the capture\n    groups (so ID plus the last four bases of the UMI plus length of whatever)\n- write out these reads to new files of pass and fail\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor tuning/debugging/designing it has some verbosity modes to spill the gory\ndetails of each operation in stats files, and should still have some memory\nprofiling functionality to debug memory leaks (fixed that one).\u003c/p\u003e\n\u003cp\u003eFor a very very verbose example of a debugging run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./slapchop.py \\\n    input.fastqz -z \\\n    output_basename \\\n    --bite-size 10 --processes 3  \\\n    --write-report --limit 10000 \\\n    -o \"Sample:  input   \u0026gt; (?P\u0026lt;sample\u0026gt;[ATCG]{5})(?P\u0026lt;fixed1\u0026gt;GTCCACGAGGTC){e\u0026lt;=1}(?P\u0026lt;rest\u0026gt;TCT.*){e\u0026lt;=1}\" \\\n    -o \"Strain:  rest    \u0026gt; (?P\u0026lt;tag\u0026gt;TCT){e\u0026lt;=1}(?P\u0026lt;strain\u0026gt;[ATCG]{10,26})CGTACGCTGCAGGTCGAC\"  \\\n    -o \"UMITail: rest    \u0026gt; (?P\u0026lt;fixed2\u0026gt;CGTACGCTGCAGGTC)(?\u0026lt;UMItail\u0026gt;GAC[ATCG]G[ATCG]A[ATCG]G[ATCG]G[ATCG]G[ATCG]GAT){s\u0026lt;=2}\"  \\\n    -o \"UMI:     UMItail \u0026gt; (GAC(?P\u0026lt;umi1\u0026gt;[ATCG])G(?\u0026lt;umi2\u0026gt;[ATCG])A(?\u0026lt;umi3\u0026gt;[ATCG])G(?\u0026lt;umi4\u0026gt;[ATCG])G(?\u0026lt;umi5\u0026gt;[ATCG])G(?\u0026lt;umi6\u0026gt;[ATCG])G){e\u0026lt;=2}\"  \\\n    --output-seq \"strain\" \\\n    --output-id \"input.id+\u0027_umi=\u0027+umi1.seq+umi2.seq+umi3.seq+ \\\n        umi4.seq+umi5.seq+umi6.seq+\u0027_sample=\u0027+sample.seq\" \\\n    --filter \"sample_length == 5 and rest_start \u0026gt;= 16 and ( min(strain.letter_annotations[\u0027phred_quality\u0027]) \u0026gt;= 30 )\"\\\n    --verbose --verbose --verbose\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat invocation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- Takes records from the `input.fastq`\n- Starts three processes that each take bites of 10 records\n- Applies the four operations to cut up the read\n- Writes the full detailed report including json reports for \n    each read, so we limit it to the first 10,000 bytes\n    of the file (about 50 records). This is for debugging.\n- Filters the records on having a `sample` barcode of 5 bases \n    and having the `rest` sequence match starting at least past\n    index 16 (so base 15 in english).\n- Re-assembles the records that pass this filter, making the ID\n    of the fastq record having the original ID plus a UMI \n    sequence and the sample barcode, then the sequence is just\n    the match to the strain barcode context. This is suitable for\n    feeding into `bwa` for example.\n- We\u0027ve got three levels of verbosity, so a per-record verbosity\n    for debugging purposes.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that the output formatting is a bit funny. This is directly evaluated\n(because security is what?) on BioPython SequenceRecords, so you need to specify\njust the name of the capture group(s) for the outputs so it can access the\n\u003ccode\u003e.seq\u003c/code\u003e and qualities. For the ID, etc, you can access \u003ccode\u003e.seq\u003c/code\u003e or \u003ccode\u003e.id\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThen if we like our thresholds we\u0027d re-run, and drop the \u003ccode\u003e--limit\u003c/code\u003e\nand \u003ccode\u003e--write-report\u003c/code\u003e flags. This will turn records like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@NB501157:100:H5J5LBGX2:1:11101:10000:6068 1:N:0:\nCTACTGTCCACGAGGTCTCTGCAGATAATACACTGTCACCCGTACGCTGCAGGTCGACCGTAGGAGGGAGATGTG\n+\nAAAAAEEEE/AEE\u0026lt;EEEEEEEEAEEAEEAEEEEE/EEE/EEEEEEEEE/EEEEEEEEEEEEE/EEEEEEEEEEEE\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003einto records like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@NB501157:100:H5J5LBGX2:1:11101:10000:6068_umi=CTGAGA_sample=CTACT\nGCAGATAATACACTGTCACC\n+\nEEAEEAEEAEEEEE/EEE/E\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe sample barcode is the first five, the strain barcode starts after\nthe \u003ccode\u003eTCT\u003c/code\u003e, and the UMI is interspersed downstream. This is modified\nyeast BarSeq, btw.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eThis script depends strongly upon (uses) the work of\n\u003ca href=\"https://pypi.org/project/regex/\" rel=\"nofollow\"\u003eregex\u003c/a\u003e\nand\n\u003ca href=\"https://pypi.org/project/biopython/\" rel=\"nofollow\"\u003eBiopython\u003c/a\u003e. Thanks! Check them out...\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1594367782.0
  },
  {
    "data_format": 2,
    "description": "Platypus germline variant calling with nextflow",
    "filenames": [
      "Singularity"
    ],
    "full_name": "IARCbioinfo/platypus-nf",
    "latest_release": "v1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-platypus-nf\" class=\"anchor\" href=\"#platypus-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlatypus-nf\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-germline-variant-calling-with-platypus\" class=\"anchor\" href=\"#germline-variant-calling-with-platypus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGermline variant calling with platypus\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"platypus-nf.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"platypus-nf.png?raw=true\" alt=\"Workflow representation\" title=\"Scheme of platypus germline variant calling Workflow\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003ePerform germline variant calling with platypus, with optional use of optimized parameters based on performance analysis on \u003ca href=\"https://www.illumina.com/platinumgenomes.html\" rel=\"nofollow\"\u003eIllumina Platinium Genome\u003c/a\u003e (both whole exome/genome sequencing).\u003c/p\u003e\n\u003cp\u003eThe platypus nextflow pipeline can also add a step to the variant calling:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecompress (\u003ccode\u003ebgzip\u003c/code\u003e) and index (\u003ccode\u003etabix\u003c/code\u003e). These two tools are part of the \u003ca href=\"http://www.htslib.org/doc/\" rel=\"nofollow\"\u003esamtools/htslib\u003c/a\u003e C library, see documentation for the installation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThis pipeline is based on \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003enextflow\u003c/a\u003e. As we have several nextflow pipelines, we have centralized the common information in the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository. Please read it carefully as it contains essential information for the installation, basic usage and configuration of nextflow and our pipelines.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePlatypus: see official installation \u003ca href=\"https://github.com/andyrimmer/Platypus\"\u003ehere\u003c/a\u003e. You can avoid installing all the external software by only installing Docker. See the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository for more information.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--input_folder\u003c/td\u003e\n\u003ctd\u003eFolder containing BAM files\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--ref\u003c/td\u003e\n\u003ctd\u003ePath fo fasta reference\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptional\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eExample value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--platypus_bin\u003c/td\u003e\n\u003ctd\u003e/usr/bin/Platypus.py\u003c/td\u003e\n\u003ctd\u003epath to platypus executable\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--region\u003c/td\u003e\n\u003ctd\u003echr1;chr1:0-1000; mybed.bed\u003c/td\u003e\n\u003ctd\u003eregion to call\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--cpu\u003c/td\u003e\n\u003ctd\u003e12\u003c/td\u003e\n\u003ctd\u003enumber of cpu used by platypus\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--mem\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003ememory in GB used by platypus\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--output_folder\u003c/td\u003e\n\u003ctd\u003e.\u003c/td\u003e\n\u003ctd\u003efolder to store output vcfs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--options\u003c/td\u003e\n\u003ctd\u003e\" --scThreshold=0.9 --qdThreshold=10 \"\u003c/td\u003e\n\u003ctd\u003eoptions to pass to platypus\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-flags\" class=\"anchor\" href=\"#flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFlags\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlags are special parameters without value.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--help\u003c/td\u003e\n\u003ctd\u003eDisplay help\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--compression\u003c/td\u003e\n\u003ctd\u003ecompress and index the VCF file\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--filter\u003c/td\u003e\n\u003ctd\u003eoutput only PASS variants\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--optimized\u003c/td\u003e\n\u003ctd\u003euse optimized parameters: --badReadsThreshold=0 --qdThreshold=0 --rmsmqThreshold=20 --hapScoreThreshold=10 --scThreshold=0.99\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download-test-data-set\" class=\"anchor\" href=\"#download-test-data-set\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload test data set\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/iarcbioinfo/data_test\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eWith Docker:\nnextflow run iarcbioinfo/platypus-nf -profile docker --input_folder data_test/BAM/ --ref data_test/REF/17.fasta\n \nWith Singularity\nnextflow run iarcbioinfo/platypus-nf -profile singularity --input_folder data_test/BAM/ --ref data_test/REF/17.fasta\n\nWith Conda\nnextflow run iarcbioinfo/platypus-nf -profile conda --input_folder data_test/BAM/ --ref data_test/REF/17.fasta\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eVCFs\u003c/td\u003e\n\u003ctd\u003eone VCF by input BAM\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directed-acyclic-graph\" class=\"anchor\" href=\"#directed-acyclic-graph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirected Acyclic Graph\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://htmlpreview.github.io/?https://github.com/IARCbioinfo/platypus-nf/blob/dev/dag.html\" rel=\"nofollow\"\u003e\u003cimg src=\"dag.png\" alt=\"DAG\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions\" class=\"anchor\" href=\"#contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eEmail\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eTiffany Delhomme*\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:delhommet@students.iarc.fr\"\u003edelhommet@students.iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper to contact for support (link to specific gitter chatroom)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1557253380.0
  },
  {
    "data_format": 2,
    "description": "This is a pipeline designed to detect and process text in recordings.",
    "filenames": [
      "Singularity",
      "Singularity.cuda"
    ],
    "full_name": "EvanYangAB/Recording-OCR-GSOC-2019",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-recording-ocr-gsoc-2019\" class=\"anchor\" href=\"#recording-ocr-gsoc-2019\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRecording-OCR-GSOC-2019\u003c/h1\u003e\n\u003cp\u003eThis is a pipeline designed to detect and process text in recordings.\u003c/p\u003e\n\u003cp\u003eThis pipeline is based on mask rcnn and tesseract OCR. Make sure to install tesseract and the supporting languages before running the program.\u003c/p\u003e\n\u003cp\u003ebrew install tesseract\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003eapt-get install tesseract\u003c/p\u003e\n\u003cp\u003eand also\u003c/p\u003e\n\u003cp\u003ebrew install tesseract-lang\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003eapt-get install tesseract-lang.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1564050703.0
  },
  {
    "data_format": 2,
    "description": "Pipeline to run software quanTIseq in parallel to quantify immune cell content from RNA-seq data",
    "filenames": [
      "Singularity/Singularity.v1.1",
      "Singularity/Singularity.v1.0"
    ],
    "full_name": "IARCbioinfo/quantiseq-nf",
    "latest_release": "v1.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-quantiseq-nf\" class=\"anchor\" href=\"#quantiseq-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003equantiseq-nf\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nextflow-pipeline-to-run-software-quantiseq-in-parallel-to-quantify-immune-cell-content-from-rna-seq-data\" class=\"anchor\" href=\"#nextflow-pipeline-to-run-software-quantiseq-in-parallel-to-quantify-immune-cell-content-from-rna-seq-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextflow pipeline to run software quanTIseq in parallel to quantify immune cell content from RNA-seq data\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/IARCbioinfo/quantiseq-nf/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/096b73c4eff8e2ab2e45b7dbb39af65ff4b7a4ccda58d7199d16b3eb2f39a79f/68747470733a2f2f636972636c6563692e636f6d2f67682f4941524362696f696e666f2f7175616e74697365712d6e662f747265652f6d61737465722e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/IARCbioinfo/quantiseq-nf/tree/master.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3065\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"quantiseq-nf.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"quantiseq-nf.png?raw=true\" alt=\"workflow\" title=\"Scheme of alignment/realignment Workflow\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis Nextflow pipeline uses the singularity image of quanTIseq to launch the quanTIseq pipeline that performs immune cell quantification of 10 cell types from RNA-seq data. See \u003ca href=\"https://icbi.i-med.ac.at/software/quantiseq/doc/\" rel=\"nofollow\"\u003ehttps://icbi.i-med.ac.at/software/quantiseq/doc/\u003c/a\u003e for general information about quanTIseq and the companion article Finotello, et al. Molecular and pharmacological modulators of the tumor immune contexture revealed by deconvolution of RNA-seq data. Genome Med. 2019;11:34. \u003ca href=\"https://doi.org/10.1186/s13073-019-0638-6\" rel=\"nofollow\"\u003ehttps://doi.org/10.1186/s13073-019-0638-6\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eNextflow : for common installation procedures see the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://singularity.lbl.gov/all-releases\" rel=\"nofollow\"\u003e\u003cem\u003esingularity\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--input_folder\u003c/td\u003e\n\u003ctd\u003ea folder with RNA-seq fastq files\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptional\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDefault value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--output_folder\u003c/td\u003e\n\u003ctd\u003e.\u003c/td\u003e\n\u003ctd\u003eOutput folder for results\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--cpu\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003enumber of CPUs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--mem\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003ememory\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--fastq_ext\u003c/td\u003e\n\u003ctd\u003efq.gz\u003c/td\u003e\n\u003ctd\u003eextension of fastq files\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--suffix1\u003c/td\u003e\n\u003ctd\u003e_1\u003c/td\u003e\n\u003ctd\u003esuffix for second element of read files pair\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--suffix2\u003c/td\u003e\n\u003ctd\u003e_2\u003c/td\u003e\n\u003ctd\u003esuffix for second element of read files pair\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--image\u003c/td\u003e\n\u003ctd\u003enull\u003c/td\u003e\n\u003ctd\u003esingularity image\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNote that if no singularity image is provided, the image is pulled from singularity hub. If the pipeline is reused frequently, it might be more efficient to pull the image manually with the command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull quantiseq2.img IARCbioinfo/quantiseq-nf:v1.1\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand then to provide the path to quantiseq2.img as a parameter.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run iarcbioinfo/quantiseq-nf -r v1.1 --input_folder input --output_folder output --image quantiseq2.img\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003equanTIseq_cell_fractions_matrix.txt\u003c/td\u003e\n\u003ctd\u003ea matrix with cell fractions (columns) for each sample (row)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003equanTIseq_gene_tpm_matrix.txt\u003c/td\u003e\n\u003ctd\u003ea matrix with gene counts (rows) for each sample (column)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eintermediate_results/quantiseqResults_sample\u003c/td\u003e\n\u003ctd\u003ea folder with quanTIseq results\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFor each sample, a folder is created in folder intermediate_results with the two quanTIseq output files (see \u003ca href=\"https://icbi.i-med.ac.at/software/quantiseq/doc/\" rel=\"nofollow\"\u003ehttps://icbi.i-med.ac.at/software/quantiseq/doc/\u003c/a\u003e for details):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003equanTIseq_gene_tpm_sample.txt, the expression quantification table (in Transcripts Per Million or TPM) with a row for each of the 19424 annotated genes\u003c/li\u003e\n\u003cli\u003equanTIseq_cell_fractions_sample.txt, the table with the proportion of cells from each cell type (columns)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directed-acyclic-graph\" class=\"anchor\" href=\"#directed-acyclic-graph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirected Acyclic Graph\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-with-default-options\" class=\"anchor\" href=\"#with-default-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWith default options\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://htmlpreview.github.io/?https://github.com/IARCbioinfo/quantiseq-nf/blob/master/dag.html\" rel=\"nofollow\"\u003e\u003cimg src=\"dag.png\" alt=\"DAG\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions\" class=\"anchor\" href=\"#contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eEmail\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eTiffany Delhomme\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:Delhommet@student.iarc.fr\"\u003eDelhommet@student.iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eNicolas Alcala*\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:AlcalaN@iarc.fr\"\u003eAlcalaN@iarc.fr\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper to contact for support\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1596489786.0
  },
  {
    "data_format": 2,
    "description": "container files for singularity-hub.org to build into images.",
    "filenames": [
      "Singularity",
      "archived/Singularity_old"
    ],
    "full_name": "mjfortier/singularityContainers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularitycontainers\" class=\"anchor\" href=\"#singularitycontainers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularityContainers\u003c/h1\u003e\n\u003cp\u003econtainer files for singularity-hub.org to build into images.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1572576861.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "Freakey17/CP4TP",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularitycontainers\" class=\"anchor\" href=\"#singularitycontainers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularityContainers\u003c/h1\u003e\n\u003cp\u003econtainer files for singularity-hub.org to build into images.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1557429544.0
  },
  {
    "data_format": 2,
    "description": "Containerized DMC application for HPCs",
    "filenames": [
      "setup/build/Singularity/Singularity.def",
      "setup/build/Singularity/SingularityUpdate.def",
      "setup/build/Singularity/Singularity.ubuntu",
      "setup/build/Singularity/SingularityCore.def"
    ],
    "full_name": "McCoyGroup/RynLib",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rynlib\" class=\"anchor\" href=\"#rynlib\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRynLib\u003c/h1\u003e\n\u003cp\u003eThis started out as a quick layer between python and entos for running DMC\u003c/p\u003e\n\u003cp\u003eIt\u0027s grown a bit...\u003c/p\u003e\n\u003cp\u003eYou can find some documentation \u003ca href=\"https//:mccoygroup.github.io/Documentation/RynLib\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1613598399.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ISU-HPC/funannotate",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-funannotate\" class=\"anchor\" href=\"#funannotate\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efunannotate\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607481566.0
  },
  {
    "data_format": 2,
    "description": "get rstudio on PSU ACI",
    "filenames": [
      "Singularity",
      "Singularity.ml"
    ],
    "full_name": "d-bohn/rstudio_aci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rstudio_aci\" class=\"anchor\" href=\"#rstudio_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erstudio_aci\u003c/h1\u003e\n\u003cp\u003eCode and workflow for building \u003ca href=\"https://www.rocker-project.org/\" rel=\"nofollow\"\u003erocker/verse\u003c/a\u003e\nin Docker Hub and modifying it with Singularity Hub for use with PSU\nACI HPC clusters.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003essh\u003c/code\u003e into the PSU ACI HPC with X11 flags.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh USERID@aci-b.aci.ics.psu.edu -X -Y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart an interactive session using \u003ccode\u003eqsub\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eqsub -A open -I -X -l walltime=24:00:00 -l nodes=2:ppn=20 -l pmem=10gb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom ACI, start \u003ccode\u003escreen\u003c/code\u003e and then execute the following code to\ncreate an \u003ccode\u003eRStudio\u003c/code\u003e image running at address \u003ccode\u003e127.0.0.1:8787\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003escreen\n\nsingularity pull -n rstudio_aci.simg shub://d-bohn/rstudio_aci\n\nsingularity exec rstudio_aci.simg rserver --www-address=127.0.0.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, press \u003ccode\u003eCTRL+A+D\u003c/code\u003e to detach the screen while allowing the process to continue running in the background.\u003c/p\u003e\n\u003cp\u003eFinally, start your preferred browser and navigate to \u003ccode\u003e127.0.0.1\u003c/code\u003e. For\nexample, firefox:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://jpetucci-firefox_icsaci\n\nsingularity exec jpetucci-firefox_icsaci-master-latest.simg /opt/firefox/./firefox\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h2\u003e\n\u003cp\u003e1). A \u003ccode\u003eshiny\u003c/code\u003e server should also start when executing this image,\nthe server should be running on port \u003ccode\u003e3838\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1562671274.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v0.5.0",
      "Singularity",
      "Singularity.v0.2.0",
      "Singularity.v0.1.0",
      "Singularity.v0.4.0"
    ],
    "full_name": "darachm/singularity_runningJobs",
    "latest_release": "v0.1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rstudio_aci\" class=\"anchor\" href=\"#rstudio_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erstudio_aci\u003c/h1\u003e\n\u003cp\u003eCode and workflow for building \u003ca href=\"https://www.rocker-project.org/\" rel=\"nofollow\"\u003erocker/verse\u003c/a\u003e\nin Docker Hub and modifying it with Singularity Hub for use with PSU\nACI HPC clusters.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003essh\u003c/code\u003e into the PSU ACI HPC with X11 flags.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh USERID@aci-b.aci.ics.psu.edu -X -Y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart an interactive session using \u003ccode\u003eqsub\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eqsub -A open -I -X -l walltime=24:00:00 -l nodes=2:ppn=20 -l pmem=10gb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom ACI, start \u003ccode\u003escreen\u003c/code\u003e and then execute the following code to\ncreate an \u003ccode\u003eRStudio\u003c/code\u003e image running at address \u003ccode\u003e127.0.0.1:8787\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003escreen\n\nsingularity pull -n rstudio_aci.simg shub://d-bohn/rstudio_aci\n\nsingularity exec rstudio_aci.simg rserver --www-address=127.0.0.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, press \u003ccode\u003eCTRL+A+D\u003c/code\u003e to detach the screen while allowing the process to continue running in the background.\u003c/p\u003e\n\u003cp\u003eFinally, start your preferred browser and navigate to \u003ccode\u003e127.0.0.1\u003c/code\u003e. For\nexample, firefox:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://jpetucci-firefox_icsaci\n\nsingularity exec jpetucci-firefox_icsaci-master-latest.simg /opt/firefox/./firefox\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h2\u003e\n\u003cp\u003e1). A \u003ccode\u003eshiny\u003c/code\u003e server should also start when executing this image,\nthe server should be running on port \u003ccode\u003e3838\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1593763396.0
  },
  {
    "data_format": 2,
    "description": "Haploid bacterial assembly and automatic annotation implemented using Nextflow",
    "filenames": [
      "Singularity"
    ],
    "full_name": "BU-ISCIII/bacterial_assembly-nf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rstudio_aci\" class=\"anchor\" href=\"#rstudio_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erstudio_aci\u003c/h1\u003e\n\u003cp\u003eCode and workflow for building \u003ca href=\"https://www.rocker-project.org/\" rel=\"nofollow\"\u003erocker/verse\u003c/a\u003e\nin Docker Hub and modifying it with Singularity Hub for use with PSU\nACI HPC clusters.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003essh\u003c/code\u003e into the PSU ACI HPC with X11 flags.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh USERID@aci-b.aci.ics.psu.edu -X -Y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart an interactive session using \u003ccode\u003eqsub\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eqsub -A open -I -X -l walltime=24:00:00 -l nodes=2:ppn=20 -l pmem=10gb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom ACI, start \u003ccode\u003escreen\u003c/code\u003e and then execute the following code to\ncreate an \u003ccode\u003eRStudio\u003c/code\u003e image running at address \u003ccode\u003e127.0.0.1:8787\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003escreen\n\nsingularity pull -n rstudio_aci.simg shub://d-bohn/rstudio_aci\n\nsingularity exec rstudio_aci.simg rserver --www-address=127.0.0.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, press \u003ccode\u003eCTRL+A+D\u003c/code\u003e to detach the screen while allowing the process to continue running in the background.\u003c/p\u003e\n\u003cp\u003eFinally, start your preferred browser and navigate to \u003ccode\u003e127.0.0.1\u003c/code\u003e. For\nexample, firefox:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://jpetucci-firefox_icsaci\n\nsingularity exec jpetucci-firefox_icsaci-master-latest.simg /opt/firefox/./firefox\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h2\u003e\n\u003cp\u003e1). A \u003ccode\u003eshiny\u003c/code\u003e server should also start when executing this image,\nthe server should be running on port \u003ccode\u003e3838\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1589387963.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "darachm/bartender_munge",
    "latest_release": null,
    "readme": "\u003cp\u003eThis container is for providing \u003ccode\u003ebartender\u003c/code\u003e for some bioinformatics pipelines.\u003c/p\u003e\n\u003cp\u003eIt\u0027s super mungy\u003c/p\u003e\n\u003cp\u003eThe primary reason for this is so that it flows nicely through SingularityHub\nfor Nextflow pipelines.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1554263689.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe file for running RStudio-Server wiht Tensorflow",
    "filenames": [
      "Singularity"
    ],
    "full_name": "tyson-swetnam/rstudio-tensorflow-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rstudio-tensorflow-singularity\" class=\"anchor\" href=\"#rstudio-tensorflow-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erstudio-tensorflow-singularity\u003c/h1\u003e\n\u003cp\u003eSingularity recipe file for running RStudio-Server wiht Tensorflow\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1554261744.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "tensorflow-gpu/from-ubuntu/Singularity"
    ],
    "full_name": "14renus/sdsc",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rstudio-tensorflow-singularity\" class=\"anchor\" href=\"#rstudio-tensorflow-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erstudio-tensorflow-singularity\u003c/h1\u003e\n\u003cp\u003eSingularity recipe file for running RStudio-Server wiht Tensorflow\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1554168191.0
  },
  {
    "data_format": 2,
    "description": "Installation of BioContainers through quay. Generate a module, bash wrapper recipe and singularity image ready for take-off on a HPC cluster",
    "filenames": [
      "templates/Singularity._quay_.File",
      "templates/Singularity._bioconda_.File"
    ],
    "full_name": "alexiswl/quay_containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-quay-containers\" class=\"anchor\" href=\"#quay-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuay Containers\u003c/h1\u003e\n\u003cp\u003eInstallation of BioContainers through quay. Generate a module, bash wrapper recipe and singularity image ready for take-off on a HPC cluster\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://alexiswl.github.io/presentations/HPC_and_Singularity/HPC_and_Singularity.html\" rel=\"nofollow\"\u003eOverall Tutorial\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://alexiswl.github.io/presentations/HPC_and_Singularity/HPC_Singularity_Presentation.html\" rel=\"nofollow\"\u003ePresentation\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-guide-to-getting-the-right-values-in-the-yaml\" class=\"anchor\" href=\"#guide-to-getting-the-right-values-in-the-yaml\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGuide to getting the right values in the yaml.\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSearch for the software through \u003ca href=\"https://bioconda.github.io/\" rel=\"nofollow\"\u003ebioconda\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eEnsure that the container is \u0027container ready\u0027 as denoted by a light green symbol under the package name.\u003c/li\u003e\n\u003cli\u003eClick on the \u0027tags\u0027 link that will direct you to the \u003ca href=\"https://quay.io/repository/\" rel=\"nofollow\"\u003equay.io\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eUse the name of the quay repository for the software_quay item.\u003c/li\u003e\n\u003cli\u003eUse the full name of the tag for the version_quay item\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003cbr\u003e\nI intend to install the star package.\u003cbr\u003e\nSearching star in bioconda leads me to \u003ca href=\"https://bioconda.github.io/recipes/star/README.html\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003cbr\u003e\nI select \u003ca href=\"https://quay.io/repository/biocontainers/star?tab=tags\" rel=\"nofollow\"\u003estar/tags\u003c/a\u003e to see the quay repo.\u003cbr\u003e\nI decide to install star version 2.7.0,\u003cbr\u003e\nso I specify \u003ccode\u003esoftware_quay\u003c/code\u003e as \u0027star\u0027 and \u003ccode\u003eversion_quay\u003c/code\u003e as \u00272.7.0d--0\u0027\u003cbr\u003e\nI then run the following\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecreate_container_from_quay \\\n--yaml yamls/star.yaml \\\n--output-dir containers \\\n--module-template templates/module \\\n--bash-template templates/bash_wrapper.sh \\\n--singularity-template templates/Singularity._quay_.File \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-git-help\" class=\"anchor\" href=\"#git-help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGit Help\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-adding-all-your-files-to-the-git-repo\" class=\"anchor\" href=\"#adding-all-your-files-to-the-git-repo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdding all your files to the git repo\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003efor file in `find -L containers -type f -not -name \u0027*.simg\u0027`; do \ngit add $file;\ndone\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-adding-just-one-of-your-software-packages-to-the-git-repo\" class=\"anchor\" href=\"#adding-just-one-of-your-software-packages-to-the-git-repo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdding just one of your software packages to the git repo\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003epackage_name=\u0027my_package\u0027\nfor file in `find -L containers/${package_name} -type f -not -name \u0027*.simg\u0027`; do \ngit add $file;\ndone\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-check-what-has-been-staged-added-to-git-but-not-committed\" class=\"anchor\" href=\"#check-what-has-been-staged-added-to-git-but-not-committed\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheck what has been staged (added to git but not committed)\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003egit diff --name-only --cached\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-check-what-has-not-been-staged-not-added-to-the-git-repo\" class=\"anchor\" href=\"#check-what-has-not-been-staged-not-added-to-the-git-repo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheck what has \u003cstrong\u003enot\u003c/strong\u003e been staged (not added to the git repo)\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003egit diff --name-only\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-some-extra-goodies\" class=\"anchor\" href=\"#some-extra-goodies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSome extra goodies\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-all-recipes-concurrently\" class=\"anchor\" href=\"#building-all-recipes-concurrently\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding all recipes concurrently\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003efor recipe in `find containers -name \u0027*.recipe\u0027`; do\n# Get image\nimage=${recipe%.recipe}.simg\n# Build image\nsudo singularity build $image $recipe\ndone\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ive-built-all-my-containers\" class=\"anchor\" href=\"#ive-built-all-my-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eI\u0027ve built all my containers\u003c/h3\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-im-ready-to-move-them-to-the-specified-container-repo-on-my-hpc\" class=\"anchor\" href=\"#im-ready-to-move-them-to-the-specified-container-repo-on-my-hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eI\u0027m ready to move them to the specified container repo on my HPC\u003c/h3\u003e\n\u003cp\u003eChances are, your module files want to go to your  \u003ccode\u003eMODULEPATH\u003c/code\u003e\nso we\u0027ll leave them behind.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHPC_CONTAINER_PATH=/path/to/containers\nrsync --links --archive --prune-empty-dirs \\\n      --include=\u0027*/\u0027 --exclude=\u0027module\u0027 \\\n      --verbose \\\n      containers/ ${HPC_CONTAINER_PATH}/\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hard-code-the-container_dir-environment-variable-for-all-my-modules\" class=\"anchor\" href=\"#hard-code-the-container_dir-environment-variable-for-all-my-modules\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHard-code the CONTAINER_DIR environment variable for all my modules\u003c/h3\u003e\n\u003cp\u003eIn each module template, the \u003cstrong\u003eCONTAINER_DIR\u003c/strong\u003e variable component remains unset.\u003cbr\u003e\nThis way, modules are transferrable between HPC clusters and organisations.\u003cbr\u003e\nTo customise this to your HPC the following code should suffice.\u003cbr\u003e\nYou may wish to also fork this repo and change the module template.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHPC_CONTAINER_PATH=/path/to/containers\nfor module in `find containers -type f -name module`; do\nsed -i \"s%__CONTAINER_DIR__%${HPC_CONTAINER_PATH}%g\" ${module}\ndone\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-copy-across-all-my-modules\" class=\"anchor\" href=\"#copy-across-all-my-modules\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCopy across all my modules\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eHPC_MODULEPATH=/path/to/modules\nfor module in `find containers -type f -name module`; do\n# Get software \nsoftware=$(basename $(dirname $(dirname $(dirname ${module}))))\n# Get version\nversion=$(basename $(dirname $(dirname ${module})))\n# Create software directory if it doesn\u0027t exist\nmkdir -p ${HPC_MODULEPATH}/$software\n# Copy module over to software \nrsync --checksum $module ${HPC_MODULEPATH}/$software/$version\ndone\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-i-get-a-warning-complaining-that-vartmp-is-already-mounted\" class=\"anchor\" href=\"#i-get-a-warning-complaining-that-vartmp-is-already-mounted\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eI get a warning complaining that /var/tmp is already mounted\u003c/h3\u003e\n\u003cp\u003eThis is likely due to /var/tmp existing during the installation of the job.\n\u003ccode\u003e/var/tmp\u003c/code\u003e links to \u003ccode\u003e/tmp\u003c/code\u003e at run-time so likely your \u003ccode\u003e/var/tmp\u003c/code\u003e during installation is also just a link to \u003ccode\u003e/tmp\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-solution\" class=\"anchor\" href=\"#solution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSolution\u003c/h4\u003e\n\u003cp\u003eAppend \u003ccode\u003erm /var/tmp\u003c/code\u003e to the end of your %post script.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eDO NOT\u003c/strong\u003e delete /tmp during post, do not use \u003ccode\u003erm -rf /var/tmp\u003c/code\u003e as this will likely delete the contents inside \u003ccode\u003e/tmp\u003c/code\u003e.\nSince \u003ccode\u003e/tmp\u003c/code\u003e is mounted to \u003ccode\u003e/tmp\u003c/code\u003e to build time\nThis is the entire host\u0027s /tmp directory.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-example-case\" class=\"anchor\" href=\"#example-case\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample Case\u003c/h4\u003e\n\u003cp\u003efgbio/0.8.0\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-i-get-an-error-about-locales-not-being-able-to-be-set\" class=\"anchor\" href=\"#i-get-an-error-about-locales-not-being-able-to-be-set\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eI get an error about locales not being able to be set.\u003c/h3\u003e\n\u003cp\u003eIf your container has been created with busybox, you\u0027re out of luck.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-solution-1\" class=\"anchor\" href=\"#solution-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSolution\u003c/h4\u003e\n\u003cp\u003eYour best bet is to use the bioconda singularity template.\nYou\u0027ll need to rename the version_quay item in the yaml to the version in bioconda\nand use the Singularity.\u003cem\u003ebioconda\u003c/em\u003e.File template rather than the quay template.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-example-case-1\" class=\"anchor\" href=\"#example-case-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample Case\u003c/h4\u003e\n\u003cp\u003eSee the picard 2.18.27 recipe for as an example.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-can-i-use-a-dry-run-mode-to-see-whats-going-on-underneath\" class=\"anchor\" href=\"#can-i-use-a-dry-run-mode-to-see-whats-going-on-underneath\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCan I use a dry-run mode to see what\u0027s going on underneath.\u003c/h3\u003e\n\u003cp\u003eCheck out the help component of the module file.\u003cbr\u003e\nYou should see by specifying DRY_RUN_\u0026lt;SOFTWARE\u0026gt; to \u00271\u0027, that you can enter dry-run mode for a given software.\u003cbr\u003e\nYou can also see the environment that the software would be running in by setting the dry-run environment variable to 2.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-i-get-this-java-error-complaining-about-an-unknownhostexception\" class=\"anchor\" href=\"#i-get-this-java-error-complaining-about-an-unknownhostexception\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eI get this Java error complaining about an UnknownHostException\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-solution-2\" class=\"anchor\" href=\"#solution-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSolution\u003c/h4\u003e\n\u003cp\u003eThe quay container has a rogue link to it\u0027s /etc/resolv.conf. By default Singularity will mount this at runtime from the host (so it doesn\u0027t need to exist). In this case you will need to remove the link during the %post script.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-example-case-2\" class=\"anchor\" href=\"#example-case-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample case\u003c/h4\u003e\n\u003cp\u003eSee the fgbio 0.8.0 recipe as an example.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# I want to see what STAR would run (but not actually run it)\nmodule load star\nexport DRY_RUN_STAR=1\nSTAR --help\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1555076056.0
  },
  {
    "data_format": 2,
    "description": "Def File of Singularity",
    "filenames": [
      "def/edge-connect.def",
      "def/contextual-attention.def",
      "def/sc-fegan.def",
      "def/singan.def",
      "def/vae-mnist.def",
      "def/wav2pix.def",
      "def/lafin.def",
      "def/stargan.def"
    ],
    "full_name": "Nahuel-Mk2/def-space",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-def-space\" class=\"anchor\" href=\"#def-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edef-space\u003c/h1\u003e\n\u003cp\u003eThis repository is def-space for Singularity\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1606215100.0
  },
  {
    "data_format": 2,
    "description": "My Singularity recipe files",
    "filenames": [
      "julia/Singularity.def",
      "root-cern/Singularity.def",
      "gerda-tgsend/Singularity.def",
      "texlive/Singularity.def",
      "bat/Singularity.def",
      "lilypond/Singularity.def",
      "itunes/Singularity.def",
      "asciinema/Singularity.def",
      "arch-base/Singularity.def",
      "centos-base/Singularity.def"
    ],
    "full_name": "gipert/Singularity.def",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-files\" class=\"anchor\" href=\"#singularity-recipe-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe files\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e containers I use the most on HPC clusters.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "containers"
    ],
    "updated_at": 1587880077.0
  },
  {
    "data_format": 2,
    "description": "Singularity definition files for various projects",
    "filenames": [
      "hauntedhouse/Singularity",
      "miniconda/Singularity",
      "hauntedhouse_freesurfer/Singularity"
    ],
    "full_name": "mvdoc/singularity-def",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-files\" class=\"anchor\" href=\"#singularity-recipe-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe files\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e containers I use the most on HPC clusters.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1495651655.0
  },
  {
    "data_format": 2,
    "description": "singularity def file for flair(fluka)",
    "filenames": [
      "flair.def",
      "flair-cern.def"
    ],
    "full_name": "ifurther/flair-def",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-files\" class=\"anchor\" href=\"#singularity-recipe-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe files\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e containers I use the most on HPC clusters.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619708213.0
  },
  {
    "data_format": 2,
    "description": "Singularity definition files for building various software to run on HPC systems",
    "filenames": [
      "torstyverse.def",
      "instrain.def",
      "checkm.def",
      "octopus.def"
    ],
    "full_name": "slhogle/singularity_def_files",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-definition-files\" class=\"anchor\" href=\"#singularity-definition-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity definition files\u003c/h1\u003e\n\u003cp\u003eCollection of def files for building some bioinformatics software I commonly use.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-instrain-v1214\" class=\"anchor\" href=\"#instrain-v1214\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003einStrain v1.2.14\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/MrOlm/inStrain\"\u003ehttps://github.com/MrOlm/inStrain\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAlso contains these functioning binaries:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/samtools/samtools\"\u003esamtools v1.10\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hyattpd/Prodigal\"\u003eprodigal v2.6.3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lh3/bwa\"\u003ebwa v0.7.17-r1198-dirty\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lh3/minimap2\"\u003eminimap2 v2.17 (r941)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dnbaker/dashing\"\u003eDashing v0.4.8-1-g47e6\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ParBLiSS/FastANI\"\u003eFastANI v1.3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/wwood/CoverM\"\u003eCoverM v0.4.0\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/slhogle/base/instrain\" rel=\"nofollow\"\u003eImage at Sylabs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload with:\u003cbr\u003e\n\u003ccode\u003esingularity pull library://slhogle/base/instrain\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-octopus-development-branch-version-v070-develop-2bde0433\" class=\"anchor\" href=\"#octopus-development-branch-version-v070-develop-2bde0433\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOctopus development branch version v0.7.0 (develop 2bde0433)\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/luntergroup/octopus\"\u003ehttps://github.com/luntergroup/octopus\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBuilt with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epatchelf v0.10\u003c/li\u003e\n\u003cli\u003eopenssl v1.1.1g\u003c/li\u003e\n\u003cli\u003epkg-config v0.29.2\u003c/li\u003e\n\u003cli\u003egpatch v2.7.6\u003c/li\u003e\n\u003cli\u003encurses v6.2\u003c/li\u003e\n\u003cli\u003ecmake v3.17.3\u003c/li\u003e\n\u003cli\u003ehtslib v1.10\u003c/li\u003e\n\u003cli\u003eboost v1.72.0\u003c/li\u003e\n\u003cli\u003eGNU C/C++ compiler v9.3.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTarget: x86_64 Linux 5.3.0-7642-generic\u003cbr\u003e\nSIMD extension: AVX2\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/slhogle/base/octopus\" rel=\"nofollow\"\u003eImage at Sylabs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload with:\u003cbr\u003e\n\u003ccode\u003esingularity pull library://slhogle/base/octopus\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-torstyverse\" class=\"anchor\" href=\"#torstyverse\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTorstyverse\u003c/h2\u003e\n\u003cp\u003eBundle of useful packages from \u003ca href=\"https://github.com/tseemann\"\u003eTorsten Seeman\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/samclip\"\u003esampclip v0.4.0\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/any2fasta\"\u003eany2fasta v0.4.2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/barrnap\"\u003ebarrnap v0.9\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/prokka\"\u003eprokka v1.14.6\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/shovill\"\u003eshovill v1.1.0\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/abricate\"\u003eabricate v1.0.1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/snippy\"\u003esnippy v4.6.0\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/slhogle/base/torstyverse\" rel=\"nofollow\"\u003eImage at Sylabs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload with:\u003cbr\u003e\n\u003ccode\u003esingularity pull library://slhogle/base/torstyverse\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1614967869.0
  },
  {
    "data_format": 2,
    "description": "Definition (recipe) files for singularity containers.",
    "filenames": [
      "comet/Singularity.def",
      "comet/chemlab-comet.def",
      "cite-seq/Singularity_rocker.def",
      "cite-seq/Singularity_publish.def",
      "cite-seq/Singularity_3.def",
      "cite-seq/Singularity_xenial.def",
      "cytof-workflow-v3/Singularity.def",
      "H5N1/Singularity_R_3.6.def",
      "H5N1/h5n1day100.def",
      "H5N1/Singularity.def",
      "generic/Singularity.def",
      "bittersweet/Singularity.def",
      "cytof-deep-cnn/Singularity.def",
      "diffnets/diffnets.def"
    ],
    "full_name": "rohitfarmer/singularity-defs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-definitionrecipe-files-for-singularity-containers\" class=\"anchor\" href=\"#definitionrecipe-files-for-singularity-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDefinition/Recipe Files for Singularity Containers\u003c/h1\u003e\n\u003cp\u003eSome of the containers are available to download from \u003ca href=\"https://cloud.sylabs.io/library/rohitfarmer\" rel=\"nofollow\"\u003ehttps://cloud.sylabs.io/library/rohitfarmer\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor feedback and collaboration write to me at \u003ca href=\"mailto:rohit.farmer@gmail.com\"\u003erohit.farmer@gmail.com\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install-singularity-on-linux\" class=\"anchor\" href=\"#install-singularity-on-linux\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Singularity on Linux\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-version-34\" class=\"anchor\" href=\"#singularity-version-34\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Version 3.4\u003c/h2\u003e\n\u003cp\u003eFollow the instructions on \u003ca href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-building-a-singularity-container\" class=\"anchor\" href=\"#building-a-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding a Singularity Container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-readonly-container\" class=\"anchor\" href=\"#readonly-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReadonly Container\u003c/h2\u003e\n\u003cp\u003eTo build a read-only SquashFS Singularity container on a local machine using a recipe/definition file.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build \u0026lt;container-name.sif\u0026gt; \u0026lt;Singularity.def\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-writable-sandbox\" class=\"anchor\" href=\"#writable-sandbox\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWritable Sandbox\u003c/h2\u003e\n\u003cp\u003eTo build a writable sandbox (essentially a folder) on a local machine using a recipe/definition file.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build --sandbox  \u0026lt;sandbox-folder-name/\u0026gt; \u0026lt;Singularity.def\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote: The advantage of building a writable sandbox is that it can be used to install and configure packages as you go, and once you are satisfied with the requirements, the sandbox can be converted into a read-only SquashFS container. To build a sandbox quickly, it\u0027s better to install a minimal set of packages via the definition file.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installconfigure-packages-in-a-writable-sandbox\" class=\"anchor\" href=\"#installconfigure-packages-in-a-writable-sandbox\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall/Configure Packages in a Writable Sandbox\u003c/h3\u003e\n\u003cp\u003eOnce a writable sandbox is created to execute it to invoke the shell of the operating installed in the container in the \"writable\" mode. If the shell is not invoked in the \"writable\" mode, all the changes will be lost once you exit from the container environment.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity shell --writable \u0026lt;sandbox-folder-name/\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eInstall packages as you would, for example, in Ubuntu from the command line.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-convert-a-writable-sandbox-to-a-readonly-container\" class=\"anchor\" href=\"#convert-a-writable-sandbox-to-a-readonly-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConvert a Writable Sandbox to a Readonly Container\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build \u0026lt;container-name.sif\u0026gt; \u0026lt;sandbox-folder-name/\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-execute-a-container\" class=\"anchor\" href=\"#execute-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecute a Container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-invoke-a-shell\" class=\"anchor\" href=\"#invoke-a-shell\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInvoke a shell\u003c/h2\u003e\n\u003cp\u003eThe command below can be used for both read-only/writable containers/sandbox.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity shell \u0026lt;container-name.sif\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote: By default, Singularity binds to your current working and home directory. Therefore, you do not need to do anything else to execute a script that is in your current working directory. It can also pull, for example, Vim settings from the .vimrc file in your home directory. Therefore, if Vim installed in the container, it can be used with the same settings from inside the container as it would from outside.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-execute-a-command-via-container\" class=\"anchor\" href=\"#execute-a-command-via-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecute a Command via Container\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec \u0026lt;container-name.sif\u0026gt; \u0026lt;command\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eFor example: \u003ccode\u003esingularity exec \u0026lt;container-name.sif\u0026gt; Rscript --vanilla hello.R\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-jupyter-notebooks-from-within-a-container\" class=\"anchor\" href=\"#running-jupyter-notebooks-from-within-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Jupyter Notebooks from Within a Container\u003c/h1\u003e\n\u003cp\u003eThis section is for containers that have Jupyter notebook installed (e.g. cite-seq).\u003c/p\u003e\n\u003cp\u003eA generic command that should work on a personal computer. \u003ccode\u003esingularity exec container-name.sif jupyter notebook --no-browser --ip=127.0.0.1 --port=8888\u003c/code\u003e\u003cbr\u003e\n\u003cem\u003eNote: The IP address and the port number mentioned in the command are the jupyter defaults. They can be changed as per need.\u003c/em\u003e\u003cbr\u003e\nCopy the URL generated by jupyter daemon and paste it in your browser; this should open Jupyter with the list of the files in your current working directory on the host computer.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-with-r-as-a-kernel\" class=\"anchor\" href=\"#running-with-r-as-a-kernel\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning with R as a Kernel\u003c/h2\u003e\n\u003cp\u003eSometimes if you already have an R kernel installed in your home directory, it conflicts with what you have inside the container. Therefore, it would require you to re-install the kernel specs in your home directory via the container.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec container-name.sif R --quiet --slave -e \u0027IRkernel::installspec()\u0027\n\n# Screen log.\n# [InstallKernelSpec] Removing existing kernelspec in /home/user/.local/share/jupyter/kernels/ir\n# [InstallKernelSpec] Installed kernelspec ir in /home/user/.local/share/jupyter/kernels/ir\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-an-hpc\" class=\"anchor\" href=\"#running-on-an-hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on an HPC\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSSH to the HPC.\u003c/li\u003e\n\u003cli\u003eClaim an interactive node.\u003c/li\u003e\n\u003cli\u003eNavigate to your project directory. Singularity container should be in your project directory.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esingularity exec container-name.sif jupyter notebook --no-browser --ip=\u00270.0.0.0\u0027\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eKeep the SSH session and Jupyter notebook session running. Copy the URL on your local browser.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cem\u003eNote: On some HPCs, you may have to initiate an additional SSH tunnel connecting your local machine to the interactive node on the HPC. In that case, follow some generic instructions here \u003ca href=\"https://rohitfarmer.github.io/docs/docs/HPC/jupyter/\" rel=\"nofollow\"\u003ehttps://rohitfarmer.github.io/docs/docs/HPC/jupyter/\u003c/a\u003e or ask your system administrator.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1598415038.0
  },
  {
    "data_format": 2,
    "description": "definition files for containers used in Hanlab",
    "filenames": [
      "singularity.Rconda/R.3.6.3.def",
      "singularity.py37.ml.openblas/py37.ml.openblas.def",
      "singularity.mkl/mkl.ubuntu.def",
      "singularity.mkl/mkl.def",
      "singularity.rnaseq/rnaseq.def",
      "singularity.SAD/SAD.def",
      "singularity.R.3.6.3.Bioc/R.3.6.3.Bioc.def",
      "singularity.phylo/phylo.def",
      "singularity.R.3.6.3.phylo/R.3.6.3.phylo.def",
      "singularity.py37.ml.mkl/py37.ml.mkl.def",
      "singularity.R.4.0.2.Bioc/R.4.0.2.Bioc.def"
    ],
    "full_name": "HanLabUNLV/hanlab_singularity_defs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc_mpi_cuda_singu_def_file\" class=\"anchor\" href=\"#hpc_mpi_cuda_singu_def_file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehpc_mpi_cuda_singu_def_file\u003c/h1\u003e\n\u003cp\u003eA collect of definition files to build images for singularity containers, which includes hpc benchmarks and mpis with cuda support.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4181\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1617151784.0
  },
  {
    "data_format": 2,
    "description": "docker and singularity containers for R",
    "filenames": [
      "images/rmd-light_4.1.0/Singularity.def",
      "images/rstan_4.1.0/Singularity.def",
      "images/myenv_4.1.0/Singularity.def"
    ],
    "full_name": "mattocci27/r-containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-docker-and-singularity-images-for-r\" class=\"anchor\" href=\"#docker-and-singularity-images-for-r\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker and singularity images for R\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-images\" class=\"anchor\" href=\"#images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImages\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003edocker\u003c/th\u003e\n\u003cth\u003esingularity\u003c/th\u003e\n\u003cth\u003edescription\u003c/th\u003e\n\u003cth\u003er-ver\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://hub.docker.com/repository/docker/mattocci/rstan\" rel=\"nofollow\"\u003erstan\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://cloud.sylabs.io/library/mattocci27/default/rstan\" rel=\"nofollow\"\u003erstan\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eadds rstan on \u003ca href=\"https://hub.docker.com/r/rocker/geospatial\" rel=\"nofollow\"\u003egeospatial\u003c/a\u003e\n\u003c/td\u003e\n\u003ctd\u003e3.6.3, 4.0.5\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://hub.docker.com/repository/docker/mattocci/myenv\" rel=\"nofollow\"\u003emyenv\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://cloud.sylabs.io/library/mattocci27/default/myenv\" rel=\"nofollow\"\u003emyenv\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eadds a bunch of packages on \u0027rstan\u0027\u003c/td\u003e\n\u003ctd\u003e3.6.3, 4.0.5\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://hub.docker.com/repository/docker/mattocci/rmd-light\" rel=\"nofollow\"\u003ermd-light\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003ctd\u003eR markdown + TinyTex + pandoc-crossref without Rstudio and Tidyverse\u003c/td\u003e\n\u003ctd\u003e4.0.5\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621973368.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "jolars/ReproduciblePythonProject",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-reproduciblepythonproject\" class=\"anchor\" href=\"#reproduciblepythonproject\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReproduciblePythonProject\u003c/h1\u003e\n\u003cp\u003eA template for reproducible projects using python, singularity, and c++ (via\npybind11).\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621967866.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "jolars/ReproducibleRProject",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-reproduciblerproject\" class=\"anchor\" href=\"#reproduciblerproject\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReproducibleRProject\u003c/h1\u003e\n\n\n\u003cp\u003eA template for a reproducible research project using Singularity, R, and\nC++, with package dependency management in R based on renv.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621967866.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "jolars/LookAheadScreening",
    "latest_release": "v0.1.0",
    "readme": "\n\u003ch1\u003e\n\u003ca id=\"user-content-code-for-look-ahead-screening-rules-for-the-lasso\" class=\"anchor\" href=\"#code-for-look-ahead-screening-rules-for-the-lasso\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCode for \u003cem\u003eLook-Ahead Screening Rules for the Lasso\u003c/em\u003e\n\u003c/h1\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/jolars/HessianScreening/actions\"\u003e\u003cimg src=\"https://github.com/jolars/LookAheadScreening/workflows/R-CMD-check/badge.svg\" alt=\"R-CMD-check\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h2\u003e\n\u003cp\u003eThe results from the simulations, which were run on a dedicated HPC\ncluster, are stored in the \u003ca href=\"results/\"\u003eresults folder\u003c/a\u003e. The figures in\nthe paper, generated from these results, are stored in\n\u003ca href=\"figures/\"\u003e\u003ccode\u003efigures/\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reproducing-the-results\" class=\"anchor\" href=\"#reproducing-the-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReproducing the Results\u003c/h2\u003e\n\u003cp\u003eThe results from our paper were run through a singularity container.\nCheck the releases for pre-built singularity containers that you can\ndownload and use.\u003c/p\u003e\n\u003cp\u003eTo reproduce the results, \u003cstrong\u003ealways\u003c/strong\u003e use the singularity container. To\nrun an experiment from the singularity container, call\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --bind results:/Project/results container.sif \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003escript\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewhere \u003ccode\u003e\u0026lt;script\u0026gt;\u003c/code\u003e should be a name of a script in the \u003ca href=\"experiments/\"\u003eexperiments\nfolder\u003c/a\u003e, such as \u003ccode\u003eexperiments/simulateddata.R\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-re-building-the-singularity-container\" class=\"anchor\" href=\"#re-building-the-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRe-building the Singularity Container\u003c/h3\u003e\n\u003cp\u003eIf you want to re-build the singularity container from scratch (or\nsimply want to clone the repo to your local drive), you can do so via\nthe following steps.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone the repository to your local hard drive. On linux, using SSH\nauthentication, run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:jolars/LookAheadScreening.git\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNavigate to the root of the repo and build the singularity container\nby calling\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e LookAheadScreening\nsudo singularity build container.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThen proceed as in \u003ca href=\"#reproducing-the-results\"\u003eReproducing the Results\u003c/a\u003e\nto run the experiments.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-experiments-without-singularity-not-recommended\" class=\"anchor\" href=\"#running-experiments-without-singularity-not-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Experiments without Singularity (Not Recommended!)\u003c/h3\u003e\n\u003cp\u003eAlternatively, you may also reproduce the results by cloning this\nrepository and starting R in the root directory of this folder (which\nwill activate the renv repository) and then run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-e\"\u003erenv\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003erestore()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eto restore the project library. Then build the R package (see below) and\nrun the simulations directly by running the scripts in the experiments\nfolder. This is \u003cstrong\u003enot recommended\u003c/strong\u003e, however, since it, unlike the\nSingularity container approach, does not exactly reproduce the software\nenvironment used when these simulations where originally run and may\nresult in discrepancies due to differences in for instance operating\nsystems, compilers, and BLAS/LAPACK implementations.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-r-package\" class=\"anchor\" href=\"#r-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR Package\u003c/h2\u003e\n\u003cp\u003eIf you want to build and experiment with the package, you can do so by\ncalling\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e R CMD INSTALL  \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eprovided you have \u003ccode\u003ecd\u003c/code\u003eed to the root folder of this repository. First\nensure, however, that you have enabled the renv project library by\ncalling \u003ccode\u003erenv::restore()\u003c/code\u003e (see the section above).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-data\" class=\"anchor\" href=\"#data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData\u003c/h2\u003e\n\u003cp\u003eThe datasets used in these simulations are stored in the \u003ca href=\"data/\"\u003edata\nfolder\u003c/a\u003e. Scripts to retrieve these datasets from their original\nsources can be found in \u003ca href=\"data-raw/\"\u003e\u003ccode\u003edata-raw/\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621967863.0
  },
  {
    "data_format": 2,
    "description": "Code for academic paper The Hessian Screening Rule",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jolars/HessianScreening",
    "latest_release": "v0.1.0",
    "readme": "\n\u003ch1\u003e\n\u003ca id=\"user-content-code-for-the-hessian-screening-rule\" class=\"anchor\" href=\"#code-for-the-hessian-screening-rule\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCode for the Hessian Screening Rule\u003c/h1\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/jolars/HessianScreening/actions\"\u003e\u003cimg src=\"https://github.com/jolars/HessianScreening/workflows/R-CMD-check/badge.svg\" alt=\"R-CMD-check\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://arxiv.org/abs/2104.13026\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0520bb4d232c0f8289fb80a5e50688bb68c43b90fd380490f1a59a25616be4b0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d313233342e35363738392d6233316231622e737667\" alt=\"arXiv\" data-canonical-src=\"https://img.shields.io/badge/arXiv-1234.56789-b31b1b.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h2\u003e\n\u003cp\u003eThe results from the simulations, which were run on a dedicated HPC\ncluster, are stored in the \u003ca href=\"results/\"\u003eresults folder\u003c/a\u003e. The figures and\ntables in the paper, generated from these results, are stored in\n\u003ca href=\"figures/\"\u003e\u003ccode\u003efigures/\u003c/code\u003e\u003c/a\u003e and \u003ca href=\"tables/\"\u003e\u003ccode\u003etables/\u003c/code\u003e\u003c/a\u003e respectively.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reproducing-the-results\" class=\"anchor\" href=\"#reproducing-the-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReproducing the Results\u003c/h2\u003e\n\u003cp\u003eThe results from our paper were run through a singularity container.\nCheck the releases for pre-built singularity containers that you can\ndownload and use.\u003c/p\u003e\n\u003cp\u003eTo reproduce the results, \u003cstrong\u003ealways\u003c/strong\u003e use the singularity container. To\nrun an experiment from the singularity container, call\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --bind results:/Project/results container.sif \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003escript\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewhere \u003ccode\u003e\u0026lt;script\u0026gt;\u003c/code\u003e should be a name of a script in the \u003ca href=\"experiments/\"\u003eexperiments\nfolder\u003c/a\u003e, such as \u003ccode\u003esimulateddata.R\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-re-building-the-singularity-container\" class=\"anchor\" href=\"#re-building-the-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRe-building the Singularity Container\u003c/h3\u003e\n\u003cp\u003eIf you want to re-build the singularity container from scratch (or\nsimply want to clone the repo to your local drive), you can do so via\nthe following steps.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eMake sure you have installed and enabled\n\u003ca href=\"https://git-lfs.github.com/\"\u003eGit-LFS\u003c/a\u003e. On ubuntu, for instance, you\ncan install Git-LFS by calling\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo apt update\nsudo apt install git-lfs\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen activate git-lfs by calling\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit lfs install\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eClone the repository to your local hard drive. On linux, using SSH\nauthentication, run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:jolars/HessianScreening.git\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNavigate to the root of the repo and build the singularity container\nby calling\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e HessianScreening\nsudo singularity build container.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThen proceed as in \u003ca href=\"#reproducing-the-results\"\u003eReproducing the Results\u003c/a\u003e\nto run the experiments.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-experiments-without-singularity-not-recommended\" class=\"anchor\" href=\"#running-experiments-without-singularity-not-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Experiments without Singularity (Not Recommended!)\u003c/h3\u003e\n\u003cp\u003eAlternatively, you may also reproduce the results by cloning this\nrepository, then either opening the \u003ccode\u003eHessianScreening.Rproj\u003c/code\u003e file in R\nStudio or starting R in the root directory of this folder (which will\nactivate the renv repository) and then run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-e\"\u003erenv\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003erestore()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eto restore the project library. Then build the R package (see below) and\nrun the simulations directly by running the scripts in the experiments\nfolder. This is \u003cstrong\u003enot recommended\u003c/strong\u003e, however, since it, unlike the\nSingularity container approach, does not exactly reproduce the software\nenvironment used when these simulations where originally run and may\nresult in discrepancies due to differences in for instance operating\nsystems, compilers, and BLAS/LAPACK implementations.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-r-package\" class=\"anchor\" href=\"#r-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR Package\u003c/h2\u003e\n\u003cp\u003eIf you want to build and experiment with the package, you can do so by\ncalling\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e R CMD INSTALL  \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eprovided you have \u003ccode\u003ecd\u003c/code\u003eed to the root folder of this repository. First\nensure, however, that you have enabled the renv project library by\ncalling \u003ccode\u003erenv::restore()\u003c/code\u003e (see the section above).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-data\" class=\"anchor\" href=\"#data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData\u003c/h2\u003e\n\u003cp\u003eThe datasets used in these simulations are stored in the \u003ca href=\"data/\"\u003edata\nfolder\u003c/a\u003e. Scripts to retrieve these datasets from their original\nsources can be found in \u003ca href=\"data-raw/\"\u003e\u003ccode\u003edata-raw/\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-forking-and-git-lfs\" class=\"anchor\" href=\"#forking-and-git-lfs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eForking and Git-LFS\u003c/h2\u003e\n\u003cp\u003eNote that pushing large files using Git-LFS against forks of this repo\n\u003ca href=\"https://docs.github.com/en/github/managing-large-files/collaboration-with-git-large-file-storage\"\u003ecounts against the bandwidth limits of this\nrepo\u003c/a\u003e,\nand so may fail if these limits are exceeded. If you for some reason\nneed to do this and it fails, please file as issue here.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [
      "screening",
      "singularity-container",
      "simulations",
      "renv",
      "statistics",
      "machine-learning",
      "lasso"
    ],
    "updated_at": 1621967868.0
  },
  {
    "data_format": 2,
    "description": "A tool to find and annotate signals in next-generation association studies",
    "filenames": [
      "Singularity"
    ],
    "full_name": "hmgu-itg/peakplotter",
    "latest_release": "v.2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-peakplotter--automatically-annotate-hits-from-genome-wide-association-results\" class=\"anchor\" href=\"#peakplotter--automatically-annotate-hits-from-genome-wide-association-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePeakPlotter : automatically annotate hits from genome-wide association results\u003c/h1\u003e\n\u003cp\u003ePeakPlotter takes away the annoying task of running regional association plots and annotating variants for your association studies results. It is compatible with sequencing as well as GWAS data. It is compatible with any format (GEMMA, SNPTEST, Bolt-LMM...) that produces the relevant columns: chromosome, position, unique ID, P-value, reference and non-reference alleles.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eAfter installing the prerequisites (see below), clone the repository and install using \u003ccode\u003epip\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/hmgu-itg/peakplotter.git\n\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e peakplotter\n\npython3 -m pip install \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\n\npeakplotter-data-setup \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e This only needs to be run once\u003c/span\u003e\n\npeakplotter --help\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or \u003c/span\u003e\npython3 -m peakplotter --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eA \u003ccode\u003eSingularity\u003c/code\u003e definition file is also available in the repository if you wish to build a container to use \u003ccode\u003epeakplotter\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003ePeakPlotter has has non-python dependencies.\u003cbr\u003e\nIn order to run PeakPlotter you need to install the following tools and add the executables to your \u003ccode\u003ePATH\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePlink 1.9 or newer (\u003ca href=\"https://www.cog-genomics.org/plink2/index\" rel=\"nofollow\"\u003eavailable here\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eLocusZoom Standalone 1.4 or newer (\u003ca href=\"http://genome.sph.umich.edu/wiki/LocusZoom_Standalone\" rel=\"nofollow\"\u003eavailable here\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eBedTools (\u003ca href=\"http://bedtools.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003eavailable here\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eTabix (\u003ca href=\"https://github.com/samtools/htslib\"\u003eavailable here\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eMoreutils (for \u003ccode\u003esponge\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePeakPlotter will throw a \u003ccode\u003eMissingExecutableError\u003c/code\u003e if you have any of the above tools missing in your \u003ccode\u003ePATH\u003c/code\u003e environment variable.\u003cbr\u003e\nAdd the necessary tools to your \u003ccode\u003ePATH\u003c/code\u003e like below:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=/path/to/locuszoom:/path/to/plink:\u003cspan class=\"pl-smi\"\u003e$PATH\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you want to make these changes permanent, do:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003eexport PATH=/path/to/locuszoom:/path/to/plink:$PATH\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.bashrc\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ peakplotter --help\nUsage: peakplotter [OPTIONS]\n\n  PeakPlotter\n\nOptions:\n  -a, --assoc-file FILE    Path to the association file. It can be gzipped,\n                           provided that it bears the .gz extension. Its first\n                           line must be a header, coherent with the name\n                           arguments below. It must be tab-separated, bgzipped\n                           and tabixed (tabix is available as part of\n                           bcftools)  [required]\n  -f, --bfiles TEXT        Binary PLINK (.bed/.bim/.fam) file base name. This\n                           should contain the genotypes \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e at least all the\n                           variants \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e the assoc_file, but it can contain\n                           more. Please note that this is the base name,\n                           without the .bed/.bim/.fam extension.  [required]\n  -o, --out DIRECTORY      Output directory to store all output files.\n                           [required]\n  -chr, --chr-col TEXT     Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e chromosome names.\n                           [required]\n  -ps, --pos-col TEXT      Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e chromosomal position.\n                           [required]\n  -rs, --rs-col TEXT       Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e unique SNP ids (RS-id or\n                           chr:pos).  [required]\n  -p, --pval-col TEXT      Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e p-values.  [required]\n  -a1, --a1-col TEXT       Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e reference or major allele\n                           (used \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e predicting consequence).  [required]\n  -a2, --a2-col TEXT       Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e alternate or minor allele.\n                           [required]\n  -maf, --maf-col TEXT     Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e non-reference or minor\n                           allele frequency.  [required]\n  -b, --build INTEGER      Assembly build (37 or 38)  [default: 38]\n  -s, --signif FLOAT       The significance level above which to \u003cspan class=\"pl-k\"\u003edeclare\u003c/span\u003e a\n                           variant significant. Scientific notation (such as\n                           5e-8) is fine.\n  -bp, --flank-bp INTEGER  Flanking size \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e base pairs \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e drawing plots\n                           (defaults to 500kb, i.e. 1Mbp plots) around lead\n                           SNPs.\n  --overwrite              Overwrite output directory \u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e it already exists.\n  --help                   Show this message and exit.\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h2\u003e\n\u003cp\u003eRun \u003ccode\u003epytest\u003c/code\u003e at the root of the repository to run the testsuite.\u003c/p\u003e\n\u003cp\u003eThere aren\u0027t a lot of tests right now, and this is a work in progress. If you encounter any bugs, please raise an issue at the \u003ca href=\"https://github.com/hmgu-itg/peakplotter/issues\"\u003eissue page\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621639878.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.guppy4.5.4gpu-conda-api",
      "Singularity.guppy3.6.0gpu-conda-api",
      "Singularity.guppy3.6.0cpu-conda-api",
      "Singularity.guppy-cpu-conda",
      "Singularity.myR_4-0-2_rstudio_1.3",
      "Singularity.guppy3.4gpu-conda-api",
      "Singularity.cpu-guppy3.4-conda-api",
      "Singularity.deepbinner-api",
      "Singularity.myR_3-6-3",
      "Singularity.guppy4.2.2gpu-conda-api",
      "Singularity.guppy4.0.14gpu-conda-api"
    ],
    "full_name": "vibaotram/singularity-container",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4054\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-images-supporting-basedmux-workflow\" class=\"anchor\" href=\"#singularity-images-supporting-basedmux-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity images supporting \u003ca href=\"https://github.com/vibaotram/baseDmux.git\"\u003ebaseDmux workflow\u003c/a\u003e\n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.guppy-cpu-conda\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 CPU, Miniconda3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:guppy-cpu-conda\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.cpu-guppy3.4-conda-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 CPU, Miniconda3, ONT_FAST5_API\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:cpu-guppy3.4-conda-api\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.guppy3.4gpu-conda-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 GPU, Miniconda3, ONT_FAST5_API\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:guppy3.4gpu-conda-api\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.deepbinner-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining deepbinner 2.0.0, ONT_FAST5_API, python3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:deepbinner-api\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [
      "singularity"
    ],
    "updated_at": 1621400501.0
  },
  {
    "data_format": 2,
    "description": "Virus assembler from amplicon sequencing reads",
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "iqbal-lab-org/viridian",
    "latest_release": "v0.1.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://www.travis-ci.com/iqbal-lab-org/viridian\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6ff381f585a29ae518070fb3871e067b0ed88ed137f3a6e7dbd32f72b1f73e00/68747470733a2f2f7777772e7472617669732d63692e636f6d2f697162616c2d6c61622d6f72672f766972696469616e2e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/iqbal-lab-org/viridian.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-viridian\" class=\"anchor\" href=\"#viridian\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eviridian\u003c/h1\u003e\n\u003cp\u003eVirus assembler from amplicon sequencing reads\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-from-source\" class=\"anchor\" href=\"#from-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFrom source\u003c/h3\u003e\n\u003cp\u003eThese must be installed and in your \u003ccode\u003e$PATH\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eracon\u003c/code\u003e (\u003ca href=\"https://github.com/lbcb-sci/racon\"\u003ehttps://github.com/lbcb-sci/racon\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eminimap2\u003c/code\u003e (\u003ca href=\"https://github.com/lh3/minimap2/\"\u003ehttps://github.com/lh3/minimap2/\u003c/a\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eClone this repository and run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython3 -m pip install .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container\u003c/h3\u003e\n\u003cp\u003eClone this repository and run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build viridian.img Singularity.def\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto build the container \u003ccode\u003eviridian.img\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-usage\" class=\"anchor\" href=\"#example-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample usage\u003c/h2\u003e\n\u003cp\u003eRequired input:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eReference FASTA file\u003c/li\u003e\n\u003cli\u003eBED file of amplicons. Column 1 = name of amplicon, columns 2 and 3 are start\nend end positions of the amplicons\u003c/li\u003e\n\u003cli\u003eReads, either in a sorted mapped indexed BAM file, or in a FASTA/FASTQ file\n(or two FASTA/FASTQ files for paired reads).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eRun using a mapped BAM file of ONT reads:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eviridian assemble --bam reads.bam ont ref.fasta amplicons.bed outdir\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun using a FASTQ file of ONT reads:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eviridian assemble --reads_to_map reads.fastq ont ref.fasta amplicons.bed outdir\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun using two FASTQ files of paired Illumina reads:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eviridian assemble \\\n  --reads_to_map reads1.fastq --mates_to_map reads2.fastq \\\n  illumina ref.fasta amplicons.bed outdir\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003cp\u003eThe important files are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003econsensus.final_assembly.fa\u003c/code\u003e: this contains the consensus sequence.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eamplicon_data.json\u003c/code\u003e: JSON file containing details of what happened when\ntrying to make a consensus sequence of each amplicon.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1621027587.0
  },
  {
    "data_format": 2,
    "description": "Run PostgreSQL server within a Singularity container against isolated directory.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "glentner/PostgreSQL-Singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-postgresql-singularity\" class=\"anchor\" href=\"#postgresql-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePostgreSQL-Singularity\u003c/h1\u003e\n\u003cp\u003eRun PostgreSQL server within a Singularity container against isolated directory.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620819429.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Pandoc.",
    "filenames": [
      "Singularity.pandoc"
    ],
    "full_name": "bast/singularity-pandoc",
    "latest_release": "0.1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-pandoc\" class=\"anchor\" href=\"#singularity-recipe-for-pandoc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe for \u003ca href=\"https://pandoc.org/\" rel=\"nofollow\"\u003ePandoc\u003c/a\u003e\n\u003c/h1\u003e\n\u003cp\u003eHow to fetch and use the image:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity pull https://github.com/bast/singularity-pandoc/releases/download/0.1.0/pandoc.sif\n$ ./pandoc.sif --from=markdown --to=rst --output=README.rst README.md\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI have used this wonderful guide as starting point and inspiration:\n\u003ca href=\"https://github.com/singularityhub/singularity-deploy\"\u003ehttps://github.com/singularityhub/singularity-deploy\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620408929.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "Clinical-Genomics-Lund/nextflow_microwgs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nextflow-pipeline-for-typing-and-marker-detection-of-bacteria\" class=\"anchor\" href=\"#nextflow-pipeline-for-typing-and-marker-detection-of-bacteria\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enextflow pipeline for typing and marker detection of bacteria\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-purpose\" class=\"anchor\" href=\"#purpose\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePurpose\u003c/h2\u003e\n\u003cp\u003eThe pipeline is aimed at producing data useful for epidemiological and surveillance purposes.\nIn v1 the pipeline is only tested using MRSA, but it should work well with\nany bacteria having a good cgMLST scheme.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-components\" class=\"anchor\" href=\"#components\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eComponents\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-qc\" class=\"anchor\" href=\"#qc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQC\u003c/h3\u003e\n\u003cp\u003eSpecies detection is performed using \u003ca href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\"\u003eKraken2\u003c/a\u003e together with \u003ca href=\"https://ccb.jhu.edu/software/bracken/\" rel=\"nofollow\"\u003eBracken\u003c/a\u003e.\nThe database used is a standard Kraken database built with \u003ccode\u003ekraken2-build --standard --db $DBNAME\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eLow levels of Intra-species contamination or erronous mapping is removed using bwa and filtering away\nthe heterozygous mapped bases.\u003c/p\u003e\n\u003cp\u003eGenome coverage is estimated by mapping with \u003ca href=\"https://github.com/lh3/bwa\"\u003ebwa mem\u003c/a\u003e and using a bed file containing the cgMLST loci.\u003c/p\u003e\n\u003cp\u003eA value on the evenness of coverage is calculated as an \u003ca href=\"https://en.wikipedia.org/wiki/Interquartile_range\" rel=\"nofollow\"\u003einterquartile range\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-epidemiological-typing\" class=\"anchor\" href=\"#epidemiological-typing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEpidemiological typing\u003c/h3\u003e\n\u003cp\u003eFor de novo asspembly \u003ca href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\u003eSPAdes\u003c/a\u003e is used. \u003ca href=\"http://cab.spbu.ru/software/quast/\" rel=\"nofollow\"\u003eQUAST\u003c/a\u003e\nis used for extraxting QC data from the assembly.\u003c/p\u003e\n\u003cp\u003eThe cgMLST reference scheme used, is branched off \u003ca href=\"https://www.cgmlst.org/ncs/schema/141106/\" rel=\"nofollow\"\u003ecgmlst.net\u003c/a\u003e\nAt the moment this fork is not synced back with new allele numbers. For extracting alleles \u003ca href=\"https://github.com/B-UMMI/chewBBACA/wiki\"\u003echewBBACA\u003c/a\u003e\nis used. Number of missing loci is calculated and used as a QC parameter.\u003c/p\u003e\n\u003cp\u003eTraditional 7-locus MLST is calculated using \u003ca href=\"https://github.com/tseemann/mlst\"\u003emlst\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-virulence-and-resistance-markers\" class=\"anchor\" href=\"#virulence-and-resistance-markers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVirulence and resistance markers\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sanger-pathogens/ariba\"\u003eARIBA\u003c/a\u003e is used as the tool to detect genetic markes.\nThe database for virulence markes is \u003ca href=\"http://www.mgc.ac.cn/VFs/\" rel=\"nofollow\"\u003eVFDB\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-report-and-visualisation\" class=\"anchor\" href=\"#report-and-visualisation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReport and visualisation\u003c/h2\u003e\n\u003cp\u003eThe QC data is aggregated in a web service CDM (repo coming) and the cgMLST is visualized using a web service\ncgviz that is combined with \u003ca href=\"https://github.com/achtman-lab/GrapeTree\"\u003egraptetree\u003c/a\u003e for manipulating trees (repo coming).\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1620341649.0
  },
  {
    "data_format": 2,
    "description": "Repository to store Singularity recipts",
    "filenames": [
      "system/hello/Singularity.hello-world",
      "system/base/Singularity.base_image",
      "physics/nextnano/Singularity.nextnano",
      "physics/mpb/Singularity.mpb_bionic",
      "physics/mpb/Singularity.mpb_xenial",
      "libraries/opencv/Singularity.opencv_cuda",
      "libraries/opencv/Singularity.opencv",
      "biology/cell-profiler/Singularity.cell-profiler",
      "biology/deeplabcut/Singularity.deeplabcut",
      "biology/deeplabcut/Singularity.deeplabcut_cpu",
      "biology/chimerax/Singularity.chimerax",
      "biology/alphafold_casp13/Singularity.alphafold_casp13",
      "languages/rstudio-server/Singularity.rstudio-server",
      "medical/mrtrix/Singularity.mrtrix3",
      "machine-learning/vico/Singularity.vico_cuda90",
      "machine-learning/vico/Singularity.vico",
      "materials/openfoam/Singularity.openfoam7_rheotool",
      "materials/openfoam/Singularity.openfoam7",
      "materials/fluidity/Singularity.fluidity_openmpi",
      "materials/fluidity/Singularity.fluidity_mpich",
      "chemistry/fpocket/Singularity.fpocket",
      "chemistry/molden/Singularity.molden",
      "chemistry/gpaw/Singularity.gpaw",
      "environment/metview/Singularity.metview",
      "environment/metview/Singularity.metview_suse",
      "environment/underworld2/Singularity.underworld2",
      "genomics/prsice/Singularity.prsice",
      "genomics/macs2/Singularity.macs2",
      "genomics/gubbins/Singularity.gubbins",
      "genomics/covid-19-signal/Singularity.covid-19-signal",
      "genomics/irap/Singularity.irap",
      "genomics/indelible/Singularity.indelible",
      "genomics/MEA/Singularity.MEA",
      "creative/k3d/Singularity.k3d",
      "creative/ipyparaview/Singularity.ipyparaview",
      "creative/tesseract/Singularity.tesseract_opencv",
      "creative/tesseract/Singularity.tesseract",
      "creative/tesseract/Singularity.tesseract_opencv_cuda",
      "creative/open3d/Singularity.open3d",
      "creative/open3d/Singularity.open3d_user"
    ],
    "full_name": "SupercomputingWales/singularity_hub",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_hub\" class=\"anchor\" href=\"#singularity_hub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_hub\u003c/h1\u003e\n\u003cp\u003eRepository to store Singularity recipts\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620243135.0
  },
  {
    "data_format": 2,
    "description": "An R package for easy execution of mouse GWAS",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "TheJacksonLaboratory/mousegwas",
    "latest_release": "GRCm38",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mousegwas\" class=\"anchor\" href=\"#mousegwas\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMouseGWAS\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis package was built to manage the GWAS analysis of mouse phenotypes. The mice in the study were genotypes using either MDA or UCLA chips and deposited in the mouse phenome database (\u003ca href=\"https://phenome.jax.org/genotypes\" rel=\"nofollow\"\u003ehttps://phenome.jax.org/genotypes\u003c/a\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eRscript -e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003elibrary(devtools); install_github(\"TheJacksonLaboratory/mousegwas\")\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h2\u003e\n\u003cp\u003eThe input for the script is the genotype csv files downloaded from the MPD website, the measured phenotypes as a csv file and a yaml file describing the input file.\nThe input csv file should contain a column for strain, a column for sex and columns for phenotype measurements. The names of the columns should be defined in the yaml file using the keywords \u003ccode\u003estrain\u003c/code\u003e and \u003ccode\u003esex\u003c/code\u003e and the phenotypes should be a list under the \u003ccode\u003ephenotypes\u003c/code\u003e keyword.\nAnother data that should reside in the yaml file is translation of strains to the strain names in the genotypes files, it is a dictionary under the \u003ccode\u003etranslate\u003c/code\u003e keyword, and \u003ccode\u003eF1\u003c/code\u003e keyword which is a dictionary translating the F1 names to their parent names, make sure the female parent is always first, it will be used to determine the X chromosome of make F1s. Confounding SNPs could be given using the \u003ccode\u003econfSNPs\u003c/code\u003e, this might be useful to control for obvious markers like coat color alleles. For sanity check you can supply coat color under \u003ccode\u003ecoat\u003c/code\u003e as a dictionary from strain name to coat color and execute a GWAS of coat color with \u003ccode\u003e--coat_phenotype\u003c/code\u003e, it can also be used as a covariate with \u003ccode\u003e--coat_covar\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-execution\" class=\"anchor\" href=\"#execution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecution\u003c/h2\u003e\n\u003cp\u003eThe script \u003ccode\u003erun_GWAS.py\u003c/code\u003e will read the input files and will prepapre the input for either GEMMA or pyLMM. In the case of GEMMA it will download version 0.98 to the working directory if it can\u0027t find the GEMMA executable, if you wish to use pyLMM it should be installed and available in the path. A common process would be creating a virtual environment in python, activating it and installing pyLMM using \u003ccode\u003epip\u003c/code\u003e, see \u003ca href=\"https://github.com/nickFurlotte/pylmm\"\u003ehttps://github.com/nickFurlotte/pylmm\u003c/a\u003e for details.\nThe mousegwas will also download METASOFT and run it on the output if there is more than one phenotype.\u003c/p\u003e\n\u003cp\u003eAs part of the data processing, mousegwas can select a subset of the individuals, restricting the number of mice in each strain x sex group or use the average phenotype of all the individuals in such a group. This is controlled by the \u003ccode\u003e-d\u003c/code\u003e option with 0 for average or any other integer for number restriction.\u003c/p\u003e\n\u003cp\u003eBy default LOCO will be used, use the \u003ccode\u003e--noloco\u003c/code\u003e argument to disable it.\u003c/p\u003e\n\u003cp\u003eA quantile-quantile normalizatin of each phenotype meausrement could be done using the \u003ccode\u003e--qqnorm\u003c/code\u003e argument.\nOther parameters will control the final Manhattan plot, it is a bit unnecessary since the \u003ccode\u003epostprocess_GWAS.R\u003c/code\u003e script will generate more and publication ready figures.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nextflow-pipeline\" class=\"anchor\" href=\"#nextflow-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextflow pipeline\u003c/h2\u003e\n\u003cp\u003eTo execute the scripts in an easy way we included a nextflow pipeline that runs the initial GWAS, the shuffled executions,\ndetermine a p-value and run the postprocess.\nTo run coat color phenotype GWAS you can simply install \u003ccode\u003enextflow\u003c/code\u003e, make sure that \u003ccode\u003esingularity\u003c/code\u003e is installed and run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run TheJacksonLaboratory/mousegwas \\\n  --yaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/coat_color_MDA.yaml \\\n  --shufyaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/coat_color_MDA.yaml \\\n  --addgwas=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e--coat_phenotype\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --addpostp=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e--coat_phenotype --colorgroup\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --pvalue 0.1 --clusters 1 --outdir coatout -profile singularity,slurm\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003eslurm\u003c/code\u003e can be changed to \u003ccode\u003epbs\u003c/code\u003e or ignored for local execution.\u003c/p\u003e\n\u003cp\u003eTo regenerate the results in the paper: \u003ca href=\"https://www.biorxiv.org/content/10.1101/2020.10.08.331017v1\" rel=\"nofollow\"\u003ehttps://www.biorxiv.org/content/10.1101/2020.10.08.331017v1\u003c/a\u003e :\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run TheJacksonLaboratory/mousegwas \\\n  --yaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/grooming_nowild.yaml \\\n  --shufyaml https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/grooming_shuffle.yaml \\\n  --input https://raw.githubusercontent.com/TheJacksonLaboratory/mousegwas/master/example/grooming_paper_strain_survey_2019_11_21.csv \u003cspan class=\"pl-cce\"\u003e\\ \u003c/span\u003e\\\n  --outdir grooming_output --addpostp=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e--loddrop 0\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e -profile slurm,singularity\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1620076690.0
  },
  {
    "data_format": 2,
    "description": "A set of recipies to build Singularity containers for analysis of omic data.",
    "filenames": [
      "metaProf/Singularity.MetaProf",
      "meta16S/Singularity.meta16S",
      "preQC/Singularity.preQC",
      "preQC/Singularity.preQC_v0_1"
    ],
    "full_name": "mticlla/OmicSingularities",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-omicsingularities\" class=\"anchor\" href=\"#omicsingularities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOmicSingularities\u003c/h1\u003e\n\u003cp\u003eA set of recipies to build Singularity containers for analysis of omic data. In addition, pre-built containers with these recipies can be downloaded from SingularityHub\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/mticlla/OmicSingularities/tree/master/preQC\"\u003e\u003cstrong\u003ePre-processing QC (preQC)\u003c/strong\u003e\u003c/a\u003e: a containerized environment with software for quality control and preprocessing of sequencing data. Resulting image is built with CentOS 7 as base OS and Python 3.6. Although this is a companion container of the MetagenomicSnake workflow, it can also be used separately.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/mticlla/OmicSingularities/tree/master/meta16S\"\u003e\u003cstrong\u003emeta16S\u003c/strong\u003e\u003c/a\u003e : a containerized environment for 16S-rRNA-gene sequencing data analysis with QIIME2.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/mticlla/OmicSingularities/blob/master/metaProf/README.md\"\u003e\u003cstrong\u003eMetaProf\u003c/strong\u003e\u003c/a\u003e: a containerized/singularized environment for taxonomic and functional profiling of metagenomic shotgun data.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619998936.0
  },
  {
    "data_format": 2,
    "description": "Singularity Image for RNA-Seq analysis",
    "filenames": [
      "Singularity",
      "Singularity.test_jupyter"
    ],
    "full_name": "duke-chsi-informatics/singularity-rnaseq",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"#singularity-rnaseq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-rnaseq\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-jupyter\" class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Jupyter\u003c/h2\u003e\n\u003cp\u003eRun this to start Jupyter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen follow the instructions that Jupyter printed to the terminal when you started it up to access Jupyter in your web browser\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAccessing Jupyter on a remote server\u003c/h3\u003e\n\u003cp\u003eIf you are running the container on a remote server, you will need to set up port forwarding with ssh to be able to access Jupyter.  Run this command to forward the default Jupyter port (8888)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -L 8888:localhost:8888 bug\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote if the default Jupyter port is not available, Jupyter will choose a different port.  In this case you will need to substitute the port that Jupyter outputs for 8888 in the ssh port forwarding command above.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-a-slurm-cluster\" class=\"anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on a SLURM Cluster\u003c/h2\u003e\n\u003cp\u003eYou can use this image interactively on a SLURM-managed cluster by running launching RStudio or Jupyter. The following instructions work on the Duke Compute Cluster (DCC).  Doing this on other cluster will require some modification and may not work, depending on how the cluster is configured.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRStudio\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003essh to DCC login node: \u003ccode\u003essh NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003erun tmux on login node: \u003ccode\u003etmux new -s container_demo\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun this on login node: \u003ccode\u003esrun -A chsi -p chsi --mem=100G -c 30 --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003ehostname -A\u003c/code\u003e on compute node and record results\u003c/li\u003e\n\u003cli\u003eRun on the following on a compute node and note the port, username, and password that the command prints:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eRun on local machine: \u003ccode\u003essh -L PORT:COMPUTE_HOSTNAME:PORT NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eWhere PORT is the port returned but the \"singularity run\" commmand\u003c/li\u003e\n\u003cli\u003eWhere COMPUTE_HOSTNAME is the hostname returned by running \"hostname -A\" on the compute node\u003c/li\u003e\n\u003cli\u003eWhere NETID is your NetID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eGo to \"localhost:PORT\" in a webrowser and enter the username and password printed by the \"singularity run\" commmand\u003c/li\u003e\n\u003cli\u003eHave fun!!\u003c/li\u003e\n\u003cli\u003eAt the end of an analysis you will probably want to copy results to your directory in \u003ccode\u003e/work\u003c/code\u003e or \u003ccode\u003e/hpc/group\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003essh to dcc-login-01.rc.duke.edu\u003c/li\u003e\n\u003cli\u003erun tmux on login node: \u003ccode\u003etmux new -s container_demo\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun this on login node: \u003ccode\u003esrun -A chsi -p chsi --mem=100G -c 30 --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun on compute node:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eRun on local machine: \u003ccode\u003essh -L PORT:COMPUTE_HOSTNAME:PORT NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eWhere PORT is the number after \u003ccode\u003ehttp://127.0.0.1:\u003c/code\u003e in the URL given by Jupyter (defaults to 8888, but Jupyter will use a different one if the default is in use, or if a different port is supplied as an argument using \u003ccode\u003e--port\u003c/code\u003e when running the singularity container\u003c/li\u003e\n\u003cli\u003eWhere COMPUTE_HOSTNAME is the hostname returned by running \"hostname -A\" on the compute node\u003c/li\u003e\n\u003cli\u003eWhere NETID is your NetID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCopy the URL supplied by jupyter that starts \u003ccode\u003ehttp://127.0.0.1:\u003c/code\u003e and paste it in a webbrowser\u003c/li\u003e\n\u003cli\u003eHave fun!!\u003c/li\u003e\n\u003cli\u003eAt the end of an analysis you will probably want to copy results to your directory in \u003ccode\u003e/work\u003c/code\u003e or \u003ccode\u003e/hpc/group\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyter-on-gpu-node\" class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter on GPU node\u003c/h3\u003e\n\u003cp\u003eSame as above, but the srun command should use the \u003ccode\u003echsi-gpu\u003c/code\u003e partition and request a gpu, but less CPUs and Memory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esrun -A chsi -p chsi-gpu --gres=gpu:1 --mem=15866 -c 2 --pty bash -i\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619748236.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.snippy",
      "Singularity.py3-matt",
      "Singularity.py2",
      "Singularity.py3-pytorch",
      "Singularity.py3-21"
    ],
    "full_name": "RationalTangle/hcc",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hcc\" class=\"anchor\" href=\"#hcc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehcc\u003c/h1\u003e\n\u003cp\u003eA collection of useful scripts and containers for use with the HCC cluster.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1619580780.0
  },
  {
    "data_format": 2,
    "description": "A Singularity container with R and Rstudio",
    "filenames": [
      "Singularity"
    ],
    "full_name": "dvav/singularity-rstudio-server",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rstudio-server\" class=\"anchor\" href=\"#singularity-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-rstudio-server\u003c/h1\u003e\n\u003cp\u003eThis is a \u003ccode\u003esingularity\u003c/code\u003e definition file and associated files for building a container\nwith the most recent (at the time of this writing) version of \u003ccode\u003eR\u003c/code\u003e and \u003ccode\u003eRstudio Server\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo build the container, use the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build rstudio.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo run the container, do the following:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --bind {host_dir1}:/var/lib/,{host_dir2}:/var/run rstudio.sif --www-port {your port}\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can run the container on \u003ccode\u003ehumbug\u003c/code\u003e (if you don\u0027t know what this is, skip this paragraph) using the following:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --bind /well,/gpfs0,/gpfs1,/gpfs2,{host_dir1}:/var/lib/,{host_dir2}:/var/run rstudio.sif --www-port {remote port}\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand setting up an \u003ccode\u003essh\u003c/code\u003e tunnel from your local machine (e.g. your laptop), for example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003essh -N -f -L {local port}:localhost:{remote port} {your username}@humbug.well.ox.ac.uk\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou need to be connected to the centre\u0027s VPN for this work.\u003c/p\u003e\n\u003cp\u003eThe following, does not work (yet):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eRSTUDIO_PASSWORD=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epassword\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n  singularity run --bind {host_dir1}:/var/lib/,{host_dir2}:/var/run rstudio.sif \\\n    --auth-none 0 \\\n    --auth-pam-helper rstudio_auth\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor more info, check the definition file \u003ccode\u003eSingularity\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eI used code from \u003ca href=\"https://github.com/nickjer/singularity-rstudio\"\u003ehttps://github.com/nickjer/singularity-rstudio\u003c/a\u003e as a point of departure. Bits of this code still remain in this repository.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1612500240.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "code/container/Singularity"
    ],
    "full_name": "JessyD/adolescent-brain-parcellation",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-adolescent-brain-parcellation\" class=\"anchor\" href=\"#adolescent-brain-parcellation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eadolescent-brain-parcellation\u003c/h1\u003e\n\u003cp\u003eThis project aims to create a brain parcellation for pre-adolescents subjects using the ABCD dataset. When performing a neuroscience study, it is common to extract regions of interest using a parcellation scheme. Although many parcellation schemes exist, these parcellations have been derived from adult data and might lead to biases when applied to a different age group. This repository contains code to create pre-adolescent brain parcellation following the methods described by \u003ca href=\"https://journals.physiology.org/doi/full/10.1152/jn.00338.2011\" rel=\"nofollow\"\u003eYeo et al. (2011)\u003c/a\u003e using the ABCD dataset.\u003c/p\u003e\n\u003cp\u003eThe code inside \u0027cbig_simplified\u0027 contains a modified version of the code provided by the \u003ca href=\"https://github.com/ThomasYeoLab/CBIG/tree/master/stable_projects/brain_parcellation/Yeo2011_fcMRI_clustering\"\u003eComputational Brain Imaging Group (CBIG)\u003c/a\u003e. This modified version make sure that the code can be run using a singularity container.\u003c/p\u003e\n\u003cp\u003eThe code was developed during the \u003ca href=\"https://www.abcd-repronim.org/about.html\" rel=\"nofollow\"\u003eABCD-ReproNim course\u0027s project week\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [
      "parcellations",
      "neuroimaging"
    ],
    "updated_at": 1617076593.0
  },
  {
    "data_format": 2,
    "description": "A general N-compartment model of ploidy states is developed and analyzed.",
    "filenames": [
      "Containers/hatchetContainers/Singularity"
    ],
    "full_name": "MathOnco/ploidyEvolution",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cytotoxic-therapy-induced-shifts-in-the-cost-to-benefit-ratio-of-high-ploidy\" class=\"anchor\" href=\"#cytotoxic-therapy-induced-shifts-in-the-cost-to-benefit-ratio-of-high-ploidy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCytotoxic therapy induced shifts in the cost-to-benefit ratio of high ploidy\u003c/h1\u003e\n\u003cp\u003eEnergetic costs of DNA content levels required for \u0026gt;75% SCNA load do not, in the absence of cytotoxic therapy, justify the masking benefits they bring. We integrate single cell sequencing with imaging and mathematical modeling of heterogeneous populations evolving through chromosome missegregations to explain observed SCNA landscapes and missegregation tolerances, and to predict effective cytotoxic therapy doses. We evaluate Oxygen, Phosphate and Glucose as rate-limiting substrates of dNTP synthesis of co-evolving subpopulations in stomach and brain tissue environments.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1621982601.0
  },
  {
    "data_format": 2,
    "description": "Feature engineering for INGV data",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ganprad/volcano-ingv",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-time-series-feature-engineering-with-ingv-volcano-prediction-competition-data\" class=\"anchor\" href=\"#time-series-feature-engineering-with-ingv-volcano-prediction-competition-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTime series feature engineering with INGV Volcano prediction competition data:\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.kaggle.com/c/predict-volcanic-eruptions-ingv-oe\" rel=\"nofollow\"\u003ehttps://www.kaggle.com/c/predict-volcanic-eruptions-ingv-oe\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe goal of the competition is the build a regression model that can accurately\nidentify time to an eruption event using time series recordings from 10 sensors.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSummary:\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e+ Some of the sensors don\u0027t have any data. These are filled with zeros.\n+ Overall strategy is to build features using the time series data that will be used by a model.\n+ Approach highlighted here is based on dynamic mode decomposition.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReferences:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.databookuw.com/\" rel=\"nofollow\"\u003ehttp://www.databookuw.com/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://dmdbook.com/\" rel=\"nofollow\"\u003ehttp://dmdbook.com/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [
      "kaggle",
      "kaggle-competition",
      "kaggle-datasets",
      "time-series"
    ],
    "updated_at": 1607669452.0
  },
  {
    "data_format": 2,
    "description": "Deep Learning Pipeline for Wrist Fracture Detection",
    "filenames": [
      "Singularity"
    ],
    "full_name": "MIPT-Oulu/DeepWrist",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-paper-link\" class=\"anchor\" href=\"#paper-link\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePaper Link\u003c/h1\u003e\n\u003cp\u003eTitle: Deep Learning for Wrist Fracture Detection: Are We There Yet? \u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2012.02577\" rel=\"nofollow\"\u003earXiv link\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-deepwrist-pipeline\" class=\"anchor\" href=\"#deepwrist-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepWrist Pipeline\u003c/h1\u003e\n\u003cp\u003eA transfer learning pipeline to detect wrist fracture from DICOM files. It has two blocks: Landmark Localization Block\nand Fracture Detection Block.\n\u003ca href=\"./figures/DeepWrist_pipeline.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./figures/DeepWrist_pipeline.png\" alt=\"DeepWrist\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBoth of the blocks are yml configuration based. We used OmegaConf for this purpose. Each executable python file can\neither run standalone or requires a yml file as \u003ccode\u003eexperiment\u003c/code\u003e argument to be passed down at command line.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-landmark-localization-block\" class=\"anchor\" href=\"#landmark-localization-block\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLandmark Localization Block\u003c/h2\u003e\n\u003cp\u003eLandmark Localization Block is adapted from \u003ca href=\"https://arxiv.org/pdf/1907.12237\" rel=\"nofollow\"\u003eKNEEL\u003c/a\u003e. However, we developed some data\naugmentation methods suited to our task.  The \u003ccode\u003elocalizer\u003c/code\u003e folder contains the source code for Landmark Localizer and\nstructured as \u003cbr\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elocalizer \n|---config \n|   |---experiment\n|---kneel_before_wrist \n|   |---data \n|   |---model \n|---scripts \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003econfig\u003c/code\u003e folder contains the initial default configuration and a configuration processor. There is a folder named\n\u003ccode\u003eexperiment\u003c/code\u003e inside \u003ccode\u003econfig\u003c/code\u003e folder which holds confgiguration for different experiments.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ekneel_before_wrist\u003c/code\u003e hosts the body of the localizer part of our pipeline. It has two sub-directory: \u003ccode\u003edata\u003c/code\u003e and \u003ccode\u003emodel\u003c/code\u003e.\nThe \u003ccode\u003edata\u003c/code\u003e folder contains utilities necessary to process and augment data for training and evaluation. \u003ccode\u003emodel\u003c/code\u003e\nsub-directory contains the pytorch lightning version of HourGlass network which we will use for training the localizer.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003escripts\u003c/code\u003e directory hosts all the experiment scripts for which the yaml configurations are created.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-train-localizer-with-your-own-data\" class=\"anchor\" href=\"#how-to-train-localizer-with-your-own-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to Train Localizer with your own data\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFirst step for training your own localizer is to collect data. For our research we used private hospital data,\nthrefore it cannot be shared. To make things simple, we used csv file to store meta data about the dataset. This way,\nyou don\u0027t have to load the full dataset to the memory rather fecth the file location from csv file and read it just-in-time.\nSo, we are dealing with wrist fracture images. We will consider posterioanterio (PA) and lateral (LAT) view of the wrist x-ray. To\nmake your own dataset, you have to create a csv metadata file containing  at least \u003ccode\u003eFname, Points,  Side\u003c/code\u003e columns. \u003ccode\u003eFname\u003c/code\u003e\nis the absolute path to the wrist image, \u003ccode\u003ePoints\u003c/code\u003e column will contain the landmark coordinates of top of distal ulna,\ntop of distal radius and assumed center of the wrist for PA view and two distinguishalbe points on top part of distal\nradio-ulna bone and the assumed center of wrist for LAT view. As the name suggest, the \u003ccode\u003eSide\u003c/code\u003e column contains the side\ninformation of corresponding wrist x-ray. Put 0 for PA and 1 for LAT. Once the metadata is ready, we can move forward.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSecond step is to clone \u003ccode\u003ewrist_landmark.yaml\u003c/code\u003e configuration file and modify the clone. Inside the yaml file modify\nfollowing\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e  data_home: # root folder that contains the data folder \n  data_folder: # your data folder name\n  meta: the csv meta file you have created. should be inside data folder \n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eOnce you are done with step 2, run the \u003ccode\u003etrain_ptl.py --experiment=YourClonedYAMLFile\u003c/code\u003e. This file is located inside\n\u003ccode\u003escripts\u003c/code\u003e folder. It will start the training.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-fracture-detection-block\" class=\"anchor\" href=\"#fracture-detection-block\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFracture Detection Block\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003eclassifier\u003c/code\u003e folder hosts the Fracture Detection Block. It has a similar structure like localizer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eclassifier \n|---config \n|---fracture_detector\n|   |---callback \n|   |---data \n|   |---model \n|---script \n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLike before \u003ccode\u003econfig\u003c/code\u003e folder hosts the script configurations. \u003ccode\u003efracture_detector\u003c/code\u003e folder hosts necessary folders and\nfiles for model, data and training related stuffs. Inside this folder, there are three folders: 1) \u003ccode\u003ecallback\u003c/code\u003e (hosts\ncallback function definitions), 2) \u003ccode\u003edata\u003c/code\u003e (hosts data related utilities) and 3) \u003ccode\u003emodel\u003c/code\u003e (hosts model definition and\ntraining methods)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-train-your-fracture-detector\" class=\"anchor\" href=\"#how-to-train-your-fracture-detector\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to train your Fracture Detector\u003c/h2\u003e\n\u003cp\u003eStep 1. First step to train your custom fracture detector is to collect data using the \u003ccode\u003elocalizer\u003c/code\u003e model trained previously.\nSave the generated ROI with the corresponding \u003ccode\u003eID\u003c/code\u003e as filename. Create a csv metadata file with \u003ccode\u003eID\u003c/code\u003e, \u003ccode\u003eSide\u003c/code\u003e, \u003ccode\u003eFname\u003c/code\u003e(optional)\nand \u003ccode\u003eFracture\u003c/code\u003e columns. Say, the meta file name is \u003ccode\u003eyour_meta.csv\u003c/code\u003eCreate a \u003ccode\u003eroot\u003c/code\u003e folder which we will use as data home where the generated ROI images and the csv\nmeta file are saved. There shoudl be \u003ccode\u003ePA\u003c/code\u003e and \u003ccode\u003eLAT\u003c/code\u003e folder in the \u003ccode\u003eroot\u003c/code\u003e folder to host PA ROI and LAT ROI respectively.\u003c/p\u003e\n\u003cp\u003eStep 2. Clone the existing training conf \u003ccode\u003efracture_detector_seresnet.yaml\u003c/code\u003e to \u003ccode\u003eyour_config_file.yaml\u003c/code\u003e.\nOpen \u003ccode\u003eyour_config_file.yaml\u003c/code\u003e and update the following field\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edata_home: root\nmeta: your_meta.csv\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStep 3. Once you are done with the config file go inside the \u003ccode\u003escripts\u003c/code\u003e folder and run \u003ccode\u003epython train_ptl.py experiment=your_config_file\u003c/code\u003e.\nthis will start the training.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inference-on-your-data\" class=\"anchor\" href=\"#inference-on-your-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInference on your Data\u003c/h2\u003e\n\u003cp\u003eStep 1. Create a csv meta file of for the data you want to predict. Use \u003ccode\u003eID\u003c/code\u003e, \u003ccode\u003eSide\u003c/code\u003e, \u003ccode\u003eFname\u003c/code\u003e, and \u003ccode\u003eFracture\u003c/code\u003e columns.\u003c/p\u003e\n\u003cp\u003eStep2. Clone \u003ccode\u003efracture_deteciton_testset_1.yaml\u003c/code\u003e to \u003ccode\u003eyour_testset.yaml\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eStep 3. Find and update the following field in \u003ccode\u003eyour_testset.yaml\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edataset:\n  train_data_home: root\n  test_data_home: /location/of/test/data\n  meta: /absolute/location/to/your_testset.csv\nsave_path: /absolute/location/to/save/prediction.csv\nsnapshot_folder: /folder/location/where/fracture/detector/models/are/saved\nsave_image: true or false\nsave_image_dir: /folder/location/if/you/want/to/save/output/images\n\nlocalizer:\n  snapshot_folder: /folder/location/where/roi/localizer/models/are/saved\n  dataset:\n    train_data_home: /folder/location/where/localization/data/are/stored\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ekeep only \u003ccode\u003eFracture\u003c/code\u003e in \u003ccode\u003egt\u003c/code\u003e. If you want to save gradcam set \u003ccode\u003esave_gradcam: true\u003c/code\u003e and define \u003ccode\u003egradcam_dir\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eStep 4. Now in the \u003ccode\u003escripts\u003c/code\u003e folder run, \u003ccode\u003epython test.py experiment=your_testset\u003c/code\u003e\nThis will do inference on your data, the predicitons will be saved in the csv file you defined.\u003c/p\u003e\n\u003ch2\u003e\u003c/h2\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-trained-models\" class=\"anchor\" href=\"#trained-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTrained Models\u003c/h1\u003e\n\u003cp\u003eUse the following commands to get the trained models.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget http://mipt-ml.oulu.fi/models/DeepWrist/Fracture_Detection_Block.tar.gz\nwget http://mipt-ml.oulu.fi/models/DeepWrist/ROI_Localization_Block.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-how-to-cite\" class=\"anchor\" href=\"#how-to-cite\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to Cite\u003c/h1\u003e\n\u003cp\u003eFor citation, please use the following bibtex\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@misc{raisuddin2020deep,\n      title={Deep Learning for Wrist Fracture Detection: Are We There Yet?}, \n      author={Abu Mohammed Raisuddin and Elias Vaattovaara and Mika Nevalainen and Marko Nikki and Elina J\u00e4rvenp\u00e4\u00e4 and Kaisa Makkonen and Pekka Pinola and Tuula Palsio and Arttu Niemensivu and Osmo Tervonen and Aleksei Tiulpin},\n      year={2020},\n      eprint={2012.02577},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1614800295.0
  },
  {
    "data_format": 2,
    "description": "4th place solution of Multi-Centre, Multi-Vendor, Multi-Disease Cardiac Image Segmentation Challenge",
    "filenames": [
      "attempts/attempt3/Singularity_v3",
      "attempts/attempt1/Singularity_v1",
      "attempts/attempt7/Singularity_v7",
      "attempts/attempt5/Singularity_v5",
      "attempts/attempt6/Singularity_v6",
      "attempts/attempt4/Singularity_v4",
      "attempts/attempt2/Singularity_v2"
    ],
    "full_name": "MarioProjects/MnMsCardiac",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mms-challenge-2020\" class=\"anchor\" href=\"#mms-challenge-2020\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eM\u0026amp;Ms Challenge 2020\u003c/h1\u003e\n\u003cp\u003eThe CMR images have been segmented by experienced clinicians from the respective institutions, including contours\nfor the left (LV) and right ventricle (RV) blood pools, as well as for the left ventricular myocardium (MYO).\nLabels are: 1 (LV), 2 (MYO) and 3 (RV)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-motivation\" class=\"anchor\" href=\"#motivation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eIn the recent years, many machine/deep learning models have been proposed to accurately segment cardiac structures\nin magnetic resonance imaging. However, when these models are tested on unseen datasets acquired from distinct\nMRI scanners or clinical centres, the segmentation accuracy can be greatly reduced.\u003c/p\u003e\n\u003cp\u003eThe M\u0026amp;Ms challenge aims to contribute to the effort of building generalisable models that can be applied consistently\nacross clinical centres. Furthermore, M\u0026amp;Ms will provide a reference dataset for the community to build and assess\nfuture generalisable models in CMR segmentation.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-environment-setup\" class=\"anchor\" href=\"#environment-setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnvironment Setup\u003c/h2\u003e\n\u003cp\u003eTo use the code, the user needs to set te environment variable to access the data. At your ~/.bashrc add:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e MMsCardiac_DATA_PATH=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e/path/to/data/M\u0026amp;MsData/\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAlso, the user needs to to pre-install a few packages:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ pip install wheel setuptools\n$ pip install -r requirements.txt\n$ pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n$ pip install torchcontrib~=0.0.2\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data-preparation\" class=\"anchor\" href=\"#data-preparation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData preparation\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-train-csv\" class=\"anchor\" href=\"#train-csv\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTrain csv\u003c/h4\u003e\n\u003cp\u003eYou can generate train csv for dataloaders using \u003ccode\u003epython3 preprocess/generate_train_df.py\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eusage: generate_train_df.py [-h] [--meta_graphs]\n\nM\u003cspan class=\"pl-k\"\u003e\u0026amp;\u003c/span\u003eMs 2020 Challenge - Training info generation\n\noptional arguments:\n  -h, --help     show this \u003cspan class=\"pl-c1\"\u003ehelp\u003c/span\u003e message and \u003cspan class=\"pl-c1\"\u003eexit\u003c/span\u003e\n  --meta_graphs  Generate train meta information graphs\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-data-refactor\" class=\"anchor\" href=\"#data-refactor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData Refactor\u003c/h4\u003e\n\u003cp\u003eLoad each volume to extract only 1 slice is time consuming. To solve this, save each slice in numpy arrays:\n\u003ccode\u003epython3 preprocess/dataloader_refactor.py\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-global-training-mean-and-std\" class=\"anchor\" href=\"#global-training-mean-and-std\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGlobal Training Mean and STD\u003c/h4\u003e\n\u003cp\u003eYou can easily get global mean and std from labeled training samples using \u003ccode\u003epython3 preprocess/get_mean_std.py\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-data-description\" class=\"anchor\" href=\"#data-description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData Description\u003c/h2\u003e\n\u003cp\u003eThe challenge cohort is composed of 350 patients with hypertrophic and dilated cardiomyopathies\nas well as healthy subjects. All subjects were scanned in clinical centres in three different\ncountries (Spain, Germany and Canada) using four different magnetic resonance\nscanner vendors (Siemens, General Electric, Philips and Canon).\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eHospital\u003c/th\u003e\n\u003cth align=\"center\"\u003eNum. studies\u003c/th\u003e\n\u003cth align=\"center\"\u003eCountry\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eClinica Sagrada Familia\u003c/td\u003e\n\u003ctd align=\"center\"\u003e50\u003c/td\u003e\n\u003ctd align=\"center\"\u003eSpain\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eHospital de la Santa Creu i Sant Pau\u003c/td\u003e\n\u003ctd align=\"center\"\u003e50\u003c/td\u003e\n\u003ctd align=\"center\"\u003eSpain\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eHospital Universitari Dexeus\u003c/td\u003e\n\u003ctd align=\"center\"\u003e50\u003c/td\u003e\n\u003ctd align=\"center\"\u003eSpain\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eHospital Vall d\u0027Hebron\u003c/td\u003e\n\u003ctd align=\"center\"\u003e100\u003c/td\u003e\n\u003ctd align=\"center\"\u003eSpain\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eMcGill University Health Centre\u003c/td\u003e\n\u003ctd align=\"center\"\u003e50\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCanada\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eUniversit\u00e4tsklinikum Hamburg-Eppendorf\u003c/td\u003e\n\u003ctd align=\"center\"\u003e50\u003c/td\u003e\n\u003ctd align=\"center\"\u003eGermany\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-training-set-15025-studies\" class=\"anchor\" href=\"#training-set-15025-studies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTraining set (150+25 studies)\u003c/h3\u003e\n\u003cp\u003eThe training set will contain 150 annotated images from two different MRI vendors (75 each) and 25 unannotated\nimages from a third vendor. The CMR images have been segmented by experienced clinicians from the respective\ninstitutions, including contours for the left (LV) and right ventricle (RV) blood pools, as well as for the\nleft ventricular myocardium (MYO). Labels are: 1 (LV), 2 (MYO) and 3 (RV).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-testing-set-200-studies\" class=\"anchor\" href=\"#testing-set-200-studies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting set (200 studies)\u003c/h3\u003e\n\u003cp\u003eThe 200 test cases correspond to 50 new studies from each of the vendors provided in the training set and\n50 additional studies from a fourth unseen vendor, that will be tested for model generalizability.\n20% of these datasets will be used for validation and the rest will be reserved for testing and ranking participants.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-standard-operating-procedure-sop-for-data-annotation\" class=\"anchor\" href=\"#standard-operating-procedure-sop-for-data-annotation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStandard Operating Procedure (SOP) for data annotation\u003c/h3\u003e\n\u003cp\u003eIn order to build a useful dataset for the community we have decided to build on top of\n\u003ca href=\"https://ieeexplore.ieee.org/document/8360453\" rel=\"nofollow\"\u003eACDC MICCAI 2017\u003c/a\u003e challenge SOP and correct our contours accordingly.\u003c/p\u003e\n\u003cp\u003eIn particular, clinical contours have been corrected by two in-house annotators that had to agree on the final result.\nThese annotators followed these rules:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLV and RV cavities must be completely covered, with papillary muscles included.\u003c/li\u003e\n\u003cli\u003eNo interpolation of the LV myocardium must be performed at the base.\u003c/li\u003e\n\u003cli\u003eRV must have a larger surface in end-diastole compared to end-systole and avoid the pulmonary artery.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe main difficulty and source of disagreement is the exact RV form in basal slices.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h2\u003e\n\u003cp\u003eUsing ACDC checkpoint:\u003c/p\u003e\n\u003cp\u003eAverage -\u0026gt; 0.7397 -\u0026gt; 0.9933 (background), 0.6931 (LV), 0.5624 (MYO), 0.71(RV)\u003c/p\u003e\n\u003cp\u003eCalculated using resnet34_unet_imagenet_encoder, Adam and constant learning rate. Fold metrics are calculated\nusing mean of averaged iou and dice values. Only mnms data.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eMethod\u003c/th\u003e\n\u003cth\u003eNormalization\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 0\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 1\u003c/th\u003e\n\u003cth\u003eFold 2\u003c/th\u003e\n\u003cth\u003eFold 3\u003c/th\u003e\n\u003cth\u003eFold 4\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.01\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7958\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8272\u003c/td\u003e\n\u003ctd\u003e0.8064\u003c/td\u003e\n\u003ctd\u003e0.8107\u003c/td\u003e\n\u003ctd\u003e0.8220\u003c/td\u003e\n\u003ctd\u003e0.8124\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8163\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8384\u003c/td\u003e\n\u003ctd\u003e0.8382\u003c/td\u003e\n\u003ctd\u003e0.8336\u003c/td\u003e\n\u003ctd\u003e0.8498\u003c/td\u003e\n\u003ctd\u003e0.8352\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.0001\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8066\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8359\u003c/td\u003e\n\u003ctd\u003e0.8235\u003c/td\u003e\n\u003ctd\u003e0.8281\u003c/td\u003e\n\u003ctd\u003e0.8310\u003c/td\u003e\n\u003ctd\u003e0.8250\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.01\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7711\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7745\u003c/td\u003e\n\u003ctd\u003e0.7993\u003c/td\u003e\n\u003ctd\u003e0.8248\u003c/td\u003e\n\u003ctd\u003e0.7791\u003c/td\u003e\n\u003ctd\u003e0.7897\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8058\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8324\u003c/td\u003e\n\u003ctd\u003e0.8322\u003c/td\u003e\n\u003ctd\u003e0.8138\u003c/td\u003e\n\u003ctd\u003e0.8433\u003c/td\u003e\n\u003ctd\u003e0.8254\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.4,0.4,0.1,0.3,0.6 - lr 0.0001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7970\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8382\u003c/td\u003e\n\u003ctd\u003e0.8212\u003c/td\u003e\n\u003ctd\u003e0.8313\u003c/td\u003e\n\u003ctd\u003e0.8344\u003c/td\u003e\n\u003ctd\u003e0.8244\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.01\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7977\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8150\u003c/td\u003e\n\u003ctd\u003e0.8053\u003c/td\u003e\n\u003ctd\u003e0.8188\u003c/td\u003e\n\u003ctd\u003e0.8212\u003c/td\u003e\n\u003ctd\u003e0.8116\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8184\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8400\u003c/td\u003e\n\u003ctd\u003e0.8339\u003c/td\u003e\n\u003ctd\u003e0.8408\u003c/td\u003e\n\u003ctd\u003e0.8469\u003c/td\u003e\n\u003ctd\u003e0.8360\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.0001\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8096\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8377\u003c/td\u003e\n\u003ctd\u003e0.8230\u003c/td\u003e\n\u003ctd\u003e0.8286\u003c/td\u003e\n\u003ctd\u003e0.8316\u003c/td\u003e\n\u003ctd\u003e0.8261\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.01\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7842\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8373\u003c/td\u003e\n\u003ctd\u003e0.8254\u003c/td\u003e\n\u003ctd\u003e0.8333\u003c/td\u003e\n\u003ctd\u003e0.8318\u003c/td\u003e\n\u003ctd\u003e0.8224\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8235\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8556\u003c/td\u003e\n\u003ctd\u003e0.7736\u003c/td\u003e\n\u003ctd\u003e0.8477\u003c/td\u003e\n\u003ctd\u003e0.8598\u003c/td\u003e\n\u003ctd\u003e0.8320\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5 - lr 0.0001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8221\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8494\u003c/td\u003e\n\u003ctd\u003e0.8349\u003c/td\u003e\n\u003ctd\u003e0.8453\u003c/td\u003e\n\u003ctd\u003e0.8503\u003c/td\u003e\n\u003ctd\u003e0.8404\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.01\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7783\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8101\u003c/td\u003e\n\u003ctd\u003e0.8041\u003c/td\u003e\n\u003ctd\u003e0.8021\u003c/td\u003e\n\u003ctd\u003e0.8331\u003c/td\u003e\n\u003ctd\u003e0.8055\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8162\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8378\u003c/td\u003e\n\u003ctd\u003e0.8330\u003c/td\u003e\n\u003ctd\u003e0.8322\u003c/td\u003e\n\u003ctd\u003e0.8456\u003c/td\u003e\n\u003ctd\u003e0.8329\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.0001\u003c/td\u003e\n\u003ctd\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7971\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8328\u003c/td\u003e\n\u003ctd\u003e0.8065\u003c/td\u003e\n\u003ctd\u003e0.8251\u003c/td\u003e\n\u003ctd\u003e0.8291\u003c/td\u003e\n\u003ctd\u003e0.8181\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.01\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7893\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7775\u003c/td\u003e\n\u003ctd\u003e0.7257\u003c/td\u003e\n\u003ctd\u003e0.8152\u003c/td\u003e\n\u003ctd\u003e0.8162\u003c/td\u003e\n\u003ctd\u003e0.7847\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8091\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8367\u003c/td\u003e\n\u003ctd\u003e0.8204\u003c/td\u003e\n\u003ctd\u003e0.8215\u003c/td\u003e\n\u003ctd\u003e0.8436\u003c/td\u003e\n\u003ctd\u003e0.8262\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.3,0.4,0.2,0.05,0.65 - lr 0.0001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7320\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8234\u003c/td\u003e\n\u003ctd\u003e0.7945\u003c/td\u003e\n\u003ctd\u003e0.8245\u003c/td\u003e\n\u003ctd\u003e0.8173\u003c/td\u003e\n\u003ctd\u003e0.7983\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_ce -\u0026gt; 0.5,0.3,0.2,0.65 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7962\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8384\u003c/td\u003e\n\u003ctd\u003e0.8157\u003c/td\u003e\n\u003ctd\u003e0.8053\u003c/td\u003e\n\u003ctd\u003e0.8181\u003c/td\u003e\n\u003ctd\u003e0.8147\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_ce -\u0026gt; 0.5,0.3,0.2,0.65 - lr 0.0001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7915\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8398\u003c/td\u003e\n\u003ctd\u003e0.8148\u003c/td\u003e\n\u003ctd\u003e0.8291\u003c/td\u003e\n\u003ctd\u003e0.8244\u003c/td\u003e\n\u003ctd\u003e0.8199\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003ePrincipal conclusions: bce_dice_border_ce with 0.5,0.2,0.2,0.2,0.5 - lr 0.001/0.0001 - standardize.\u003c/p\u003e\n\u003cp\u003eNow, using lr 0.001, standardize and bce_dice_border_ce with 0.5,0.2,0.2,0.2,0.5, explore data augmentation.\nWithout data augmentation score 0.8360.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eData Augmentation\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 0\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 1\u003c/th\u003e\n\u003cth\u003eFold 2\u003c/th\u003e\n\u003cth\u003eFold 3\u003c/th\u003e\n\u003cth\u003eFold 4\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eVertical flip\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8004\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8273\u003c/td\u003e\n\u003ctd\u003e0.8176\u003c/td\u003e\n\u003ctd\u003e0.8074\u003c/td\u003e\n\u003ctd\u003e0.8386\u003c/td\u003e\n\u003ctd\u003e0.8182\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eHorizontal flip\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8032\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8225\u003c/td\u003e\n\u003ctd\u003e0.8226\u003c/td\u003e\n\u003ctd\u003e0.8244\u003c/td\u003e\n\u003ctd\u003e0.8318\u003c/td\u003e\n\u003ctd\u003e0.8209\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eRandom Crops\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8137\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8376\u003c/td\u003e\n\u003ctd\u003e0.8208\u003c/td\u003e\n\u003ctd\u003e0.8283\u003c/td\u003e\n\u003ctd\u003e0.7876\u003c/td\u003e\n\u003ctd\u003e0.8181\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eShift\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8117\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8240\u003c/td\u003e\n\u003ctd\u003e0.8222\u003c/td\u003e\n\u003ctd\u003e0.8330\u003c/td\u003e\n\u003ctd\u003e0.8307\u003c/td\u003e\n\u003ctd\u003e0.8243\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eDownscale\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7949\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8192\u003c/td\u003e\n\u003ctd\u003e0.8166\u003c/td\u003e\n\u003ctd\u003e0.8219\u003c/td\u003e\n\u003ctd\u003e0.8384\u003c/td\u003e\n\u003ctd\u003e0.8181\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eElastic Transform\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7991\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8425\u003c/td\u003e\n\u003ctd\u003e0.8274\u003c/td\u003e\n\u003ctd\u003e0.8213\u003c/td\u003e\n\u003ctd\u003e0.8408\u003c/td\u003e\n\u003ctd\u003e0.8262\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eRotations\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8158\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8426\u003c/td\u003e\n\u003ctd\u003e0.8255\u003c/td\u003e\n\u003ctd\u003e0.8290\u003c/td\u003e\n\u003ctd\u003e0.8524\u003c/td\u003e\n\u003ctd\u003e0.8330\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eGrid Distortion\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8028\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8361\u003c/td\u003e\n\u003ctd\u003e0.7864\u003c/td\u003e\n\u003ctd\u003e0.8275\u003c/td\u003e\n\u003ctd\u003e0.8231\u003c/td\u003e\n\u003ctd\u003e0.8151\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eOptical Distortion\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7705\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8418\u003c/td\u003e\n\u003ctd\u003e0.8255\u003c/td\u003e\n\u003ctd\u003e0.7996\u003c/td\u003e\n\u003ctd\u003e0.8354\u003c/td\u003e\n\u003ctd\u003e0.8145\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-competition-models\" class=\"anchor\" href=\"#competition-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompetition Models\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-bala-1\" class=\"anchor\" href=\"#bala-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cem\u003eBala 1\u003c/em\u003e\n\u003c/h4\u003e\n\u003cp\u003eUsing standardization, data augmentation combination old and bce_dice_border_ce with 0.5,0.2,0.2,0.2,0.5.\nResnet34 Unet with lr 0.001 and adam optimizer.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eMethod\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 0\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 1\u003c/th\u003e\n\u003cth\u003eFold 2\u003c/th\u003e\n\u003cth\u003eFold 3\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eweakly -\u0026gt; labeled\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8286\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8596\u003c/td\u003e\n\u003ctd\u003e0.8505\u003c/td\u003e\n\u003ctd\u003e0.8540\u003c/td\u003e\n\u003ctd\u003e0.8482\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ecombined -\u0026gt; labeled\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8271\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8473\u003c/td\u003e\n\u003ctd\u003e0.8424\u003c/td\u003e\n\u003ctd\u003e0.8573\u003c/td\u003e\n\u003ctd\u003e0.8435\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-bala-2\" class=\"anchor\" href=\"#bala-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cem\u003eBala 2\u003c/em\u003e\n\u003c/h4\u003e\n\u003cp\u003eUsing standardization, data augmentation combination old and bce_dice_border_ce with 0.5,0.2,0.2,0.2,0.5\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eMethod\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 0\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 1\u003c/th\u003e\n\u003cth\u003eFold 2\u003c/th\u003e\n\u003cth\u003eFold 3\u003c/th\u003e\n\u003cth\u003eFold 4\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eResnet34 Unet lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8092\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8257\u003c/td\u003e\n\u003ctd\u003e0.8115\u003c/td\u003e\n\u003ctd\u003e0.8293\u003c/td\u003e\n\u003ctd\u003e0.8276\u003c/td\u003e\n\u003ctd\u003e0.8207\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-not-pretrained-model\" class=\"anchor\" href=\"#not-pretrained-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNot Pretrained Model\u003c/h3\u003e\n\u003cp\u003eFolding by patient.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eMethod\u003c/th\u003e\n\u003cth\u003eNormalization\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 0\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 1\u003c/th\u003e\n\u003cth\u003eFold 2\u003c/th\u003e\n\u003cth\u003eFold 3\u003c/th\u003e\n\u003cth\u003eFold 4\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.65 - lr 0.01\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7873\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.8263\u003c/td\u003e\n\u003ctd\u003e0.8004\u003c/td\u003e\n\u003ctd\u003e0.8195\u003c/td\u003e\n\u003ctd\u003e0.7616\u003c/td\u003e\n\u003ctd\u003e0.7990\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003ebce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.65 - lr 0.001\u003c/td\u003e\n\u003ctd\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7741\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7879\u003c/td\u003e\n\u003ctd\u003e0.7743\u003c/td\u003e\n\u003ctd\u003e0.7883\u003c/td\u003e\n\u003ctd\u003e0.8071\u003c/td\u003e\n\u003ctd\u003e0.7863\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-update-11062020-meeting\" class=\"anchor\" href=\"#update-11062020-meeting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUpdate: 11/06/2020 Meeting\u003c/h2\u003e\n\u003cp\u003eChanges and ideas:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e[x] Use 2 folds grouping by vendor (A vs. B), instead of \u003cem\u003en\u003c/em\u003e grouping by patient. Then error analysis by vendor\u003c/li\u003e\n\u003cli\u003e[x] Since is not permited the use of pre-trained models, try smaller architectures\u003c/li\u003e\n\u003cli\u003e[ ] Create convolutional network that learns to distinguish if an image comes from vendor A or vendor B. \u00bfWorks?\n\u003cul\u003e\n\u003cli\u003eIf works then we can create a DCGAN trying to apply a initial transformation to fool the discriminator and\ndo something like normalize the input images! \u003cstrong\u003eNote\u003c/strong\u003e: Do not add vendor C in CNN classification step since\nwe will use it for validate our GAN later.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[ ] Self-Supervised Learning for unseen vendor C\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-folding-by-vendor-resuts\" class=\"anchor\" href=\"#folding-by-vendor-resuts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFolding by Vendor Resuts\u003c/h2\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-wrong-folding-no-train-subpartitionpatients-to-compare\" class=\"anchor\" href=\"#wrong-folding-no-train-subpartitionpatients-to-compare\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e(Wrong folding, no train subpartition/patients to compare)\u003c/h4\u003e\n\u003cp\u003eNormalization by reescale. Criterion bce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eMethod\u003c/th\u003e\n\u003cth align=\"center\"\u003eDA\u003c/th\u003e\n\u003cth align=\"center\"\u003eA -\u0026gt; B\u003c/th\u003e\n\u003cth align=\"center\"\u003eB -\u0026gt; A\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_pspnet_unet - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7573\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7121\u003c/td\u003e\n\u003ctd\u003e0.7346\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_pspnet_unet - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6838\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.5532\u003c/td\u003e\n\u003ctd\u003e0.6185\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_pspnet_unet - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7612\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6793\u003c/td\u003e\n\u003ctd\u003e0.7202\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_pspnet_unet - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6982\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.5580\u003c/td\u003e\n\u003ctd\u003e0.6281\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_unet_scratch - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7498\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6835\u003c/td\u003e\n\u003ctd\u003e0.7166\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_unet_scratch - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6779\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.4997\u003c/td\u003e\n\u003ctd\u003e0.5888\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_unet_scratch - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7421\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6627\u003c/td\u003e\n\u003ctd\u003e0.7023\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet18_unet_scratch - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7588\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6281\u003c/td\u003e\n\u003ctd\u003e0.6934\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet34_unet_scratch - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7649\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6313\u003c/td\u003e\n\u003ctd\u003e0.6980\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet34_unet_scratch - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7189\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6273\u003c/td\u003e\n\u003ctd\u003e0.6731\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet34_unet_scratch - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7673\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6530\u003c/td\u003e\n\u003ctd\u003e0.7101\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet34_unet_scratch - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7707\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6128\u003c/td\u003e\n\u003ctd\u003e0.6917\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003enano_unet - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.5035\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.4284\u003c/td\u003e\n\u003ctd\u003e0.4659\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003enano_unet - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.4432\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.2821\u003c/td\u003e\n\u003ctd\u003e0.3626\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003enano_unet - lr 0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.4871\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.4771\u003c/td\u003e\n\u003ctd\u003e0.4821\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003enano_unet - lr 0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.4310\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.2187\u003c/td\u003e\n\u003ctd\u003e0.3248\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eGeneral conclusions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eModels can extract more information and thus make better predictions when training with Vendor \u0027A\u0027\nand then testing on \u0027B\u0027. GAN should approximate images to Vendor A?\u003c/li\u003e\n\u003cli\u003elr 0.001 works better than lower ones.\u003c/li\u003e\n\u003cli\u003eNot clear difference using data augmentation and without apply it...\u003c/li\u003e\n\u003cli\u003eIntermediate models size, resnet18_pspnet_unet, performs better than bigger ones and smaller ones.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-11-random-patients-to-compare\" class=\"anchor\" href=\"#11-random-patients-to-compare\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e11 random patients to compare\u003c/h4\u003e\n\u003cp\u003eCriterion bce_dice_border_ce -\u0026gt; 0.5,0.2,0.2,0.2,0.5. Using resnet18_pspnet_unet.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eNormalization\u003c/th\u003e\n\u003cth align=\"center\"\u003eData Augmentation\u003c/th\u003e\n\u003cth align=\"center\"\u003eLearning Rate\u003c/th\u003e\n\u003cth align=\"center\"\u003eA -\u0026gt; B\u003c/th\u003e\n\u003cth align=\"center\"\u003eB -\u0026gt; A\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination (Old)\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7328\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6915\u003c/td\u003e\n\u003ctd\u003e0.7121\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination (Old)\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7601\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6704\u003c/td\u003e\n\u003ctd\u003e0.7152\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination (Old)\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.005\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6593\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.4914\u003c/td\u003e\n\u003ctd\u003e0.5753\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination (Old)\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.005\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7499\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6342\u003c/td\u003e\n\u003ctd\u003e0.6920\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7502\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7014\u003c/td\u003e\n\u003ctd\u003e0.7258\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7561\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6723\u003c/td\u003e\n\u003ctd\u003e0.7142\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.005\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7370\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.5143\u003c/td\u003e\n\u003ctd\u003e0.6257\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.005\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7123\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6826\u003c/td\u003e\n\u003ctd\u003e0.6975\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7462\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7283\u003c/td\u003e\n\u003ctd\u003e0.7372\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7668\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6312\u003c/td\u003e\n\u003ctd\u003e0.6990\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eReescale\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.005\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7098\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6280\u003c/td\u003e\n\u003ctd\u003e0.6689\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eStandardize\u003c/td\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.005\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7606\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.6604\u003c/td\u003e\n\u003ctd\u003e0.7105\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eGeneral conclusions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen using Vendor A as training set, generalizes better to Vendor B cases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-classification-vendor-a---b-discriminator\" class=\"anchor\" href=\"#classification-vendor-a---b-discriminator\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eClassification: Vendor \u0027A\u0027 - \u0027B\u0027 Discriminator\u003c/h2\u003e\n\u003cp\u003eUsing resnet18_pspnet_classification model. Adam with bce. 60 epochs and *0.1 steps as 25 and 50.\nImg size 224x224. fold_system=\"patient\" \u0026amp; label_type=\"vendor_label\". Normalization standardize. Learning rate 0.001.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eData Augmentation\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 0\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 1\u003c/th\u003e\n\u003cth\u003eFold 2\u003c/th\u003e\n\u003cth\u003eFold 3\u003c/th\u003e\n\u003cth\u003eFold 4\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eNone\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9954\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9726\u003c/td\u003e\n\u003ctd\u003e1.0000\u003c/td\u003e\n\u003ctd\u003e0.9878\u003c/td\u003e\n\u003ctd\u003e0.9970\u003c/td\u003e\n\u003ctd\u003e0.9906\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eCombination\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9954\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9771\u003c/td\u003e\n\u003ctd\u003e0.9985\u003c/td\u003e\n\u003ctd\u003e1.0000\u003c/td\u003e\n\u003ctd\u003e0.9939\u003c/td\u003e\n\u003ctd\u003e0.9930\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-classification-vendor-a---b---c-discriminator\" class=\"anchor\" href=\"#classification-vendor-a---b---c-discriminator\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eClassification: Vendor \u0027A\u0027 - \u0027B\u0027 - \u0027C\u0027 Discriminator\u003c/h2\u003e\n\u003cp\u003eAdam with bce. 80 epochs and *0.1 steps as 25 and 60.\nImg size 224x224. fold_system=\"patient\" \u0026amp; label_type=\"vendor_label\".  Normalization standardize. Learning rate 0.001.\nData Augmentation combination (old).\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eModel\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 0\u003c/th\u003e\n\u003cth align=\"center\"\u003eFold 1\u003c/th\u003e\n\u003cth\u003eFold 2\u003c/th\u003e\n\u003cth\u003eFold 3\u003c/th\u003e\n\u003cth\u003eFold 4\u003c/th\u003e\n\u003cth\u003eMean\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet34_pspnet\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9954\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9726\u003c/td\u003e\n\u003ctd\u003e1.0000\u003c/td\u003e\n\u003ctd\u003e0.9878\u003c/td\u003e\n\u003ctd\u003e0.9970\u003c/td\u003e\n\u003ctd\u003e0.9906\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet34_pspnet\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9954\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9771\u003c/td\u003e\n\u003ctd\u003e0.9985\u003c/td\u003e\n\u003ctd\u003e1.0000\u003c/td\u003e\n\u003ctd\u003e0.9939\u003c/td\u003e\n\u003ctd\u003e0.9930\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eresnet34_unet\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9910\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.9871\u003c/td\u003e\n\u003ctd\u003e1.0000\u003c/td\u003e\n\u003ctd\u003e0.9740\u003c/td\u003e\n\u003ctd\u003e0.9805\u003c/td\u003e\n\u003ctd\u003e0.9865\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-discriminator-entropy-backwards-a---b---c\" class=\"anchor\" href=\"#discriminator-entropy-backwards-a---b---c\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDiscriminator Entropy backwards \u0027A\u0027 - \u0027B\u0027 - \u0027C\u0027\u003c/h2\u003e\n\u003cp\u003eUsing gradient gamma 0.99, max iterations 250, standardize normalization. Segmentator Training with \u0027A\u0027.\nBaseline: 0.7799 IOU on B.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eOut threshold\u003c/th\u003e\n\u003cth align=\"center\"\u003eTarget\u003c/th\u003e\n\u003cth align=\"center\"\u003eMore\u003c/th\u003e\n\u003cth align=\"center\"\u003eB\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003e----\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7827\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 2.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7825\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 5.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7827\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 10.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7829\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003e----\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7713\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 2.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7723\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 5.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7725\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 10.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7744\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003e----\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7827\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 2.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7826\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 5.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7827\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 10.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7828\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003e----\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7713\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 2.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7723\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 5.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7725\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 10.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7744\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003e----\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7827\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 2.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7826\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 5.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7828\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 10.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7828\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003e----\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7713\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 2.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7723\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 5.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7725\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eEqual\u003c/td\u003e\n\u003ctd align=\"center\"\u003eL1 10.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7744\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003eProblem with low out thresholds... Waste all iterations and stops.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-discriminator-entropy-backwards-a---b---c--with-blur-unblur-and-gamma\" class=\"anchor\" href=\"#discriminator-entropy-backwards-a---b---c--with-blur-unblur-and-gamma\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDiscriminator Entropy backwards \u0027A\u0027 - \u0027B\u0027 - \u0027C\u0027 / With blur, unblur and gamma\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eOut threshold\u003c/th\u003e\n\u003cth align=\"center\"\u003eEntropy\u003c/th\u003e\n\u003cth align=\"center\"\u003eBlur\u003c/th\u003e\n\u003cth align=\"center\"\u003eUnblur\u003c/th\u003e\n\u003cth align=\"center\"\u003eGamma\u003c/th\u003e\n\u003cth align=\"center\"\u003eTarget\u003c/th\u003e\n\u003cth align=\"center\"\u003eIters\u003c/th\u003e\n\u003cth align=\"center\"\u003eB\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.5\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.01\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003e100\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7770\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.5\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.0001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003e100\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7786\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.5\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.000001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.000001\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.000001\u003c/td\u003e\n\u003ctd align=\"center\"\u003eA\u003c/td\u003e\n\u003ctd align=\"center\"\u003e100\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7779\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-7-july\" class=\"anchor\" href=\"#7-july\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e7 July\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hausdorff-loss-tests\" class=\"anchor\" href=\"#hausdorff-loss-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHausdorff loss tests\u003c/h3\u003e\n\u003cp\u003eMean average values for 5 folds. Data combination old. Lr 0.001 with resnet_unet_scratch.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eHausdorff Weight\u003c/th\u003e\n\u003cth align=\"center\"\u003eIOU A\u003c/th\u003e\n\u003cth align=\"center\"\u003eIOU B\u003c/th\u003e\n\u003cth\u003eDICE A\u003c/th\u003e\n\u003cth\u003eDICE B\u003c/th\u003e\n\u003cth\u003eHAUSSDORF A\u003c/th\u003e\n\u003cth\u003eHAUSSDORF B\u003c/th\u003e\n\u003cth\u003eASSD A\u003c/th\u003e\n\u003cth\u003eASSD B\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7333\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7835\u003c/td\u003e\n\u003ctd\u003e0.8087\u003c/td\u003e\n\u003ctd\u003e0.8561\u003c/td\u003e\n\u003ctd\u003e4.4773\u003c/td\u003e\n\u003ctd\u003e3.4890\u003c/td\u003e\n\u003ctd\u003e1.2458\u003c/td\u003e\n\u003ctd\u003e0.9624\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.05\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7417\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7867\u003c/td\u003e\n\u003ctd\u003e0.8158\u003c/td\u003e\n\u003ctd\u003e0.8589\u003c/td\u003e\n\u003ctd\u003e4.0958\u003c/td\u003e\n\u003ctd\u003e3.4073\u003c/td\u003e\n\u003ctd\u003e1.1618\u003c/td\u003e\n\u003ctd\u003e0.9646\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.1\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7399\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7827\u003c/td\u003e\n\u003ctd\u003e0.8153\u003c/td\u003e\n\u003ctd\u003e0.8550\u003c/td\u003e\n\u003ctd\u003e4.1999\u003c/td\u003e\n\u003ctd\u003e3.4355\u003c/td\u003e\n\u003ctd\u003e1.1925\u003c/td\u003e\n\u003ctd\u003e0.9735\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.2\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7421\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7806\u003c/td\u003e\n\u003ctd\u003e0.8193\u003c/td\u003e\n\u003ctd\u003e0.8522\u003c/td\u003e\n\u003ctd\u003e4.2831\u003c/td\u003e\n\u003ctd\u003e3.4414\u003c/td\u003e\n\u003ctd\u003e1.1953\u003c/td\u003e\n\u003ctd\u003e0.9831\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e0.3\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7370\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.7790\u003c/td\u003e\n\u003ctd\u003e0.8134\u003c/td\u003e\n\u003ctd\u003e0.8534\u003c/td\u003e\n\u003ctd\u003e4.3634\u003c/td\u003e\n\u003ctd\u003e3.4972\u003c/td\u003e\n\u003ctd\u003e1.2264\u003c/td\u003e\n\u003ctd\u003e0.9886\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-other\" class=\"anchor\" href=\"#other\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOther\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDevelopment environment -\u0026gt; CUDA 10.1 and cudnn 7603. Python 3.8.2 - GCC 9.3.0\u003c/li\u003e\n\u003cli\u003eChallenge homepage \u003ca href=\"https://www.ub.edu/mnms/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eACDC nomenclature: 0, 1, 2 and 3 represent voxels located in the background, in the right ventricular cavity,\nin the myocardium, and in the left ventricular cavity, respectively.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [
      "ventricle",
      "semantinc",
      "segmentation",
      "lv",
      "rv",
      "myo",
      "computervision"
    ],
    "updated_at": 1602614455.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "AI/PyTorch/latest/gpu/src/Singularity.pytorch-latest-gpu-src",
      "AI/PyTorch/latest/gpu/pip/Singularity.pytorch-latest-gpu-pip",
      "AI/PyTorch/latest/cpu/src/Singularity.pytorch-1.5-cpu-src",
      "AI/PyTorch/latest/cpu/src/glow/Singularity",
      "AI/PyTorch/latest/cpu/pip/Singularity.pytorch-latest-cpu-pip",
      "AI/PyTorch/1.5/gpu/src/Singularity.pytorch-1.5-gpu-src",
      "AI/PyTorch/1.5/gpu/pip/Singularity.pytorch-1.5-gpu-pip",
      "AI/PyTorch/1.5/cpu/src/Singularity.pytorch-1.5-cpu-src",
      "AI/PyTorch/1.5/cpu/src/glow/Singularity.pytorch-1.5-cpu-src-glow",
      "AI/PyTorch/1.5/cpu/pip/Singularity.pytorch-1.5-cpu-pip",
      "AI/TensorFlow/latest/gpu/Singularity.tensorflow-latest-gpu-src",
      "AI/TensorFlow/latest/gpu/src/Singularity.tensorflow-latest-gpu-src",
      "AI/TensorFlow/latest/gpu/pip/Singularity.tensorflow-latest-gpu-pip",
      "AI/TensorFlow/latest/cpu/src/Singularity.tensorflow-latest-cpu-src",
      "AI/TensorFlow/latest/cpu/pip/Singularity.tensorflow-latest-cpu-pip",
      "AI/TensorFlow/2.1/gpu/Singularity.tensorflow-2.1-gpu-src",
      "AI/TensorFlow/2.1/gpu/src/Singularity.tensorflow-2.1-gpu-src",
      "AI/TensorFlow/2.1/gpu/pip/Singularity.tensorflow-2.1-gpu-pip",
      "AI/TensorFlow/2.1/cpu/src/Singularity.tensorflow-2.1-cpu-src",
      "AI/TensorFlow/2.1/cpu/pip/Singularity.tensorflow-2.1-cpu-pip",
      "AI/TensorFlow/ngraph/Singularity.tensorflow-ngraph",
      "HPC/code-aster/Singularity.code-aster-serial",
      "HPC/cp2k/Singularity.cp2k-mpich",
      "HPC/cp2k/Singularity.cp2k-openmpi",
      "base/ubuntu/18_04/Singularity.base-ubuntu-18-04",
      "base/nvidia/cuda-10.1/Singularity.base-nvidia-cuda-10-1",
      "Performance-model/mpich/Singularity.mpich_benchmarks",
      "Performance-model/mpich_ubuntu16/Singularity.mpich_ubuntu16_benchmarks",
      "test/Singularity.ubuntu-base"
    ],
    "full_name": "sodalite-hpe/modak",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainers\u003c/h1\u003e\n\u003cp\u003econtains containers for MODAK\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1617111147.0
  },
  {
    "data_format": 2,
    "description": "Scripts and pipelines used in the generation of \"An Egyptian Genome Reference\"",
    "filenames": [
      "EGP_vep/Singularityfiles/Singularity.99-GRCh38-merged",
      "EGP_vep/Singularityfiles/Singularity.99-GRCh37-merged"
    ],
    "full_name": "buschlab/Egyptref",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-egyptref\" class=\"anchor\" href=\"#egyptref\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEgyptref\u003c/h1\u003e\n\u003cp\u003eScripts and pipelines used in the generation of \"An Egyptian Genome Reference\"\u003c/p\u003e\n\u003cp\u003eForked archives and scripts of collaborators on Egypt reference genome project.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEGP_admixture  - contribution from Michael Olbrich (\u003ca href=\"https://github.com/ScienceOlbrich\"\u003ehttps://github.com/ScienceOlbrich\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eEGP_delly      - contribution from Axel K\u00fcnstner (\u003ca href=\"https://github.com/kunstner\"\u003ehttps://github.com/kunstner\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eEGP_gwas       - contribution from Matthias Munz (\u003ca href=\"https://github.com/matmu\"\u003ehttps://github.com/matmu\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eEGP_small_variant_calling  - contribution from Matthias Munz\u003c/li\u003e\n\u003cli\u003eEGP_sv_merging  - contribution from Matthias Munz\u003c/li\u003e\n\u003cli\u003eEGP_unique_insertions  - contribution from Matthias Munz\u003c/li\u003e\n\u003cli\u003eEGP_vep        - contribution from Matthias Munz (\u003ca href=\"https://github.com/matmu\"\u003ehttps://github.com/matmu\u003c/a\u003e)\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1617920957.0
  },
  {
    "data_format": 2,
    "description": "Tractography-inspired targetting for Hippocampal TMS",
    "filenames": [
      "singularity/mrtrix3-connectome-custom/Singularity",
      "singularity/mrtrix3-connectome-custom/src/Singularity"
    ],
    "full_name": "chidiugonna/DTI-hippo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-dti-hippo\" class=\"anchor\" href=\"#dti-hippo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDTI-hippo\u003c/h1\u003e\n\u003cp\u003eTractography-inspired targetting for hippocampal TMS\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences\u003c/h2\u003e\n\u003cp\u003eSmith, R. E.; Tournier, J.-D.; Calamante, F. \u0026amp; Connelly, A. \u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/22705374/\" rel=\"nofollow\"\u003eAnatomically-constrained tractography: Improved diffusion MRI streamlines tractography through effective use of anatomical information.\u003c/a\u003e NeuroImage, 2012, 62, 1924-1938\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617884735.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "Aphoh/temp_tc",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-transactive_control\" class=\"anchor\" href=\"#transactive_control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etransactive_control\u003c/h1\u003e\n\u003cp\u003eCode meant to support and simulate the Social Game that will be launched in 2020. Elements of transactive control and behavioral engineering will be tested and designed here\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone the repo\u003c/li\u003e\n\u003cli\u003eInstall \u003ca href=\"https://dvc.org/doc/install\" rel=\"nofollow\"\u003edvc\u003c/a\u003e (with google drive support)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eOn linux this is \u003ccode\u003epip install \u0027dvc[gdrive]\u0027\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eInstall Docker, if you have not already.\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003epython3 -m dvc remote add -d gdrive gdrive://1qaTn6IYd3cpiyJegDwwEhZ3LwrujK3_x\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003epython3 -m dvc pull\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eRun \u003ccode\u003e./run.sh\u003c/code\u003e from the root of the repo. This will put you in a shell in the docker container with the \u003ccode\u003erl_algos/logs\u003c/code\u003e directory mounted\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003epython3 StableBaselines.py sac test_experiment\u003c/code\u003e in the docker container to start an experiment with the name \u003ccode\u003etest_experiment\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003etensorboard --logdir rl_algos/logs\u003c/code\u003e from outside the docker container to view the logs\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-12202020\" class=\"anchor\" href=\"#12202020\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e12/20/2020\u003c/h3\u003e\n\u003cp\u003eThis repository has been cleaned and updated for use. It contains: (1) The OpenAI gym environment \"OfficeLearn\", in the \"gym-socialgame\" folder, and (2) implementations of Reinforcement learning algorithms in \"rl_algos\" folder. In the \"simulations\" folder are various datasets for setting up and training models associated with the gym simulation.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-912020\" class=\"anchor\" href=\"#912020\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e9/1/2020\u003c/h3\u003e\n\u003cp\u003eThe most recent running of the code involves navigating to the rl_algos/ directory, then running the python command for the vanilla version:\u003c/p\u003e\n\u003cp\u003epython StableBaselines.py sac\u003c/p\u003e\n\u003cp\u003eAdding in the planning model can be done with the following flags:\u003c/p\u003e\n\u003cp\u003epython StableBaselines.py sac --planning_steps=10 --planning_model=Oracle --num_steps=10000\u003c/p\u003e\n\u003cp\u003ePlease see transactive_control/gym-socialgame/gym_socialgame/envs for files pertaining to the setup of the environment. The socialgame_env.py contains a lot of the information necessary for understanding how the agent steps through the environment. The reward.py file contains information on the variety of reward types available for testing. agents.py contains information on the deterministic people that we created for our simulation.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-gym-socialgame\" class=\"anchor\" href=\"#gym-socialgame\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egym-socialgame\u003c/h3\u003e\n\u003cp\u003eOpenAI Gym environment for a social game.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1621784680.0
  },
  {
    "data_format": 2,
    "description": "A container that looks like uppmax but can be run completely offline.",
    "filenames": [
      "Singularity.default",
      "Singularity.ngsintro"
    ],
    "full_name": "UPPMAX/offline-uppmax-env",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-offline-uppmax-env\" class=\"anchor\" href=\"#offline-uppmax-env\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eoffline-uppmax-env\u003c/h1\u003e\n\u003cp\u003eA container that has the same operating system, same packages installed, and a copy of the module system (not the actual software though) at UPPMAX. The script \u003ccode\u003esoftware_packer.sh\u003c/code\u003e can be run at UPPMAX to create a tarball of the software you wish to include in container at build time. If any data needs to be accessed from inside the container it can be mounted at runtime.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-tldr\" class=\"anchor\" href=\"#tldr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTLDR\u003c/h1\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# ON UPPMAX\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e package the software you want to have in your image\u003c/span\u003e\ngit clone https://github.com/UPPMAX/offline-uppmax-env.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e offline-uppmax-env\nbash software_packer.sh bwa star samtools\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# ON LOCAL COMPUTER\u003c/span\u003e\ngit clone https://github.com/UPPMAX/offline-uppmax-env.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e offline-uppmax-env\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e download the created software.package.tar.gz to the package/ folder\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e using Docker\u003c/span\u003e\ndocker build \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\ndocker run \\\n-v offline-uppmax-env-proj:/proj \\\n-v /any/host/data/you/want/access/to:/path/inside/container \\\n-it \\\nuppmax/offline-uppmax-env:latest\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e using singularity\u003c/span\u003e\nsingularity build offline-uppmax-env.sif Singularity\nsingularity shell \\\n-b /host/path/to/persistent/projfolder:/proj \\\n-b /any/host/data/you/want/access/to:/path/inside/container \\\noffline-uppmax-env.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eWhat you get\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCentOS 7\u003c/li\u003e\n\u003cli\u003eAll yum packages installed at UPPMAX\u003c/li\u003e\n\u003cli\u003eA copy of the module files at UPPMAX (not the programs themselves)\u003c/li\u003e\n\u003cli\u003eThe option to include any of the installed programs at UPPMAX, requires you to rebuild the image.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhat you don\u0027t get\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eShared libraries, these would bloat the image quite a bit. These are solvable on a case by case basis, more on that further down.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-use-case\" class=\"anchor\" href=\"#use-case\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse case\u003c/h2\u003e\n\u003cp\u003eThis repo was created to make a offline replacement for UPPMAX for courses, in case there is some kind of problem making UPPMAX unusable at the time the course is given. If UPPMAX suddenly disappears we can just tell the students to start up a container and all data and software needed would be included, making it possible to continue the course. This will require us to build our own version of this image where we include the software we want to be installed and to provide any data we want to be accessible to the students.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-create-a-course-specific-image\" class=\"anchor\" href=\"#how-to-create-a-course-specific-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to create a course specific image\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-off-the-base-image-in-dockerhub\" class=\"anchor\" href=\"#building-off-the-base-image-in-dockerhub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding off the base image in Dockerhub\u003c/h3\u003e\n\u003cp\u003eThe base image will just have the OS and packages of UPPMAX, and the \u003ccode\u003euppmax\u003c/code\u003e and \u003ccode\u003ebioinfo-tools\u003c/code\u003e module. To include the software you want to have access to you will have to login to UPPMAX and run \u003ccode\u003esoftware_packer.sh\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e run on uppmax\u003c/span\u003e\nbash software_packer.sh bwa star R GATK\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will package everything needed to load these modules into a file called \u003ccode\u003esoftware.package.tar.gz\u003c/code\u003e.  Download this file to your computer, put it in the \u003ccode\u003epackages\u003c/code\u003e folder and build the Dockerfile in that folder (replace \u003ccode\u003erepo/name:version\u003c/code\u003e with whatever you want to name it on Dockerhub, or remove it to have it untagged). The dockerfile will copy all files in \u003ccode\u003epackages/\u003c/code\u003e and unzip all files named \u003ccode\u003e*.package.tar.gz\u003c/code\u003e, so feel free to put additional files there following this naming pattern.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e run locally\u003c/span\u003e\ndocker build -t repo/name:version \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-your-own-base-image\" class=\"anchor\" href=\"#building-your-own-base-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding your own base image\u003c/h3\u003e\n\u003cp\u003eIf the base image on Dockerhub is too old for your liking you can rebuild it yourself. Follow the same steps as above, but put the \u003ccode\u003esoftware.package.tar.gz\u003c/code\u003e you created on UPPMAX in the \u003ccode\u003ebase/packages\u003c/code\u003e folder instead. The dockerfile will copy all files in \u003ccode\u003epackages/\u003c/code\u003e and unzip all files named \u003ccode\u003e*.package.tar.gz\u003c/code\u003e, so feel free to put additional files there following this naming pattern.\u003c/p\u003e\n\u003cp\u003eTo update the list of packages installed by \u003ccode\u003eyum\u003c/code\u003e, run the following line on UPPMAX:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e list all installed packages and print the all on a single line\u003c/span\u003e\nyum list installed \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e cut -f 1 -d \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e \u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e cut -f 1 -d \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e.\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e sort \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e awk -vORS=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e \u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e{ print $1 }\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand replace the list of packages in the \u003ccode\u003eDockerfile\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThen build the Dockerfile in the \u003ccode\u003ebase\u003c/code\u003e folder.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e base\ndocker build -t repo/name:version \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis build will download and install all the yum packages from scratch so the image will be completely up-to-date, but it will take about an hour to build it.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-run-the-image-once-it-is-built\" class=\"anchor\" href=\"#how-to-run-the-image-once-it-is-built\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to run the image once it is built\u003c/h2\u003e\n\u003cp\u003eThis will create a named volume called \u003ccode\u003eoffline-uppmax-env-proj\u003c/code\u003e which will be mounted to \u003ccode\u003e/proj\u003c/code\u003e inside the container. All data put in there will persist between restarts of the container, i.e. this is where the students should put their lab work. The data used in the labs are usually so big (10+gb) that it does not make sens to put it inside the image. It\u0027s better to download it separately and mount it when starting the container.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDocker\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run \\\n-v offline-uppmax-env-proj:/proj \\\n-v /host/path/to/data:/container/path/to/data \\\n-it \\\nrepo/name:version\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e example\u003c/span\u003e\ndocker run \\\n-v offline-uppmax-env-proj:/proj \\\n-v /home/user/ngsintro_data:/sw/courses/ngsintro \\\n-it \\\nuppmax/offline-uppmax-env:latest\n\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAfter the container is running it should be just like working on uppmax. \u003ccode\u003emodule load\u003c/code\u003e should behave the same way and all modules you packed with \u003ccode\u003esoftware_packer.sh\u003c/code\u003e should be available.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity\u003c/strong\u003e\nTo get the module system to work in Singularity you have to build the Singularity file as sudo and everything should work. Package the software you need on UPPMAX like in the Docker approach, put the downloaded tarball in the \u003ccode\u003epackages/\u003c/code\u003e folder just like with Docker, and then build it with Singularity.\u003c/p\u003e\n\u003cp\u003eJust building from Dockerhub (uppmax/offline-uppmax-env:latest) will give you a container with only the \u003ccode\u003euppmax\u003c/code\u003e and \u003ccode\u003ebioinfo-tools\u003c/code\u003e in it, and the \u003ccode\u003emodule\u003c/code\u003e command will not work since it is a function that is not inherited properly when being converted by Singularity. You can get around this by manually typing \u003ccode\u003esource /etc/bashrc.module_env\u003c/code\u003e every time the container starts.\u003c/p\u003e\n\u003cp\u003eIf you build your own Docker image with the software your want, push it to Dockerhub, and convert it to Singularity, you will still have the problem of the \u003ccode\u003emodule\u003c/code\u003e command not working. The solution is the same, manually type \u003ccode\u003esource /etc/bashrc.module_env\u003c/code\u003e when the container starts and it should start working. Building the Singularity file instead will not have this problem.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e \u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-missing-shared-libraries\" class=\"anchor\" href=\"#missing-shared-libraries\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMissing shared libraries\u003c/h3\u003e\n\u003cp\u003eUnfortunately I could not find an easy way to automatically pull all the shared libraries needed by programs. I had a problem with STAR, that it needed a newer version of GCC. I could get around it by running \u003ccode\u003eldd $(which star)\u003c/code\u003e on uppmax and see that the file uses was \u003ccode\u003e/sw/comp/gcc/8.3.0_rackham/lib64/libstdc++.so.6\u003c/code\u003e. I put this file in a tar file,\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003etar -chzvf libs.package.tar.gz /sw/comp/gcc/8.3.0_rackham/lib64/libstdc++.so.6  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e note the -h option, will dereference symbolic links\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand put the \u003ccode\u003elibs.package.tar.gz\u003c/code\u003e file in the \u003ccode\u003epackages\u003c/code\u003e folder, build the image, and it worked after that.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTodos\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eTest if it works.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1604891556.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/viralevo-reporting/Singularity"
    ],
    "full_name": "nibscbioinformatics/viralevo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"\" class=\"anchor\" href=\"#\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"docs/images/nibscbioinformatics-viralevo_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/nibscbioinformatics-viralevo_logo.png\" alt=\"nibscbioinformatics/viralevo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eCharacterisation of viral genomes, intra-host diversity and viral evolution across samples\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/nibscbioinformatics/viralevo/actions\"\u003e\u003cimg src=\"https://github.com/nibscbioinformatics/viralevo/workflows/nf-core%20CI/badge.svg\" alt=\"GitHub Actions CI Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/nibscbioinformatics/viralevo/actions\"\u003e\u003cimg src=\"https://github.com/nibscbioinformatics/viralevo/workflows/nf-core%20linting/badge.svg\" alt=\"GitHub Actions Linting Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nibscbioinformatics/viralevo\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0a860878ad3e1977ef7f5327a20837856d84aafd02647b3b4a591b82a1ca68a9/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e6962736362696f696e666f726d61746963732f766972616c65766f2e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nibscbioinformatics/viralevo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/nibscbioinformatics/viralevo/workflows/Singularity%20Conversion/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/nibscbioinformatics/viralevo/workflows/Singularity%20Conversion/badge.svg\" alt=\"Singularity Conversion\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20\u0026amp;%20Push%20-%20Finishing/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20\u0026amp;%20Push%20-%20Finishing/badge.svg\" alt=\"Docker Finishing\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20\u0026amp;%20Push%20-%20Reporting/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/nibscbioinformatics/viralevo/workflows/Docker%20Build%20\u0026amp;%20Push%20-%20Reporting/badge.svg\" alt=\"Docker Reporting\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe ViralEvo pipeline is designed to characterise viral samples, and particularly SARS-CoV-2, from short read sequencing data.\u003c/p\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003ei. Install \u003ca href=\"https://nf-co.re/usage/installation\" rel=\"nofollow\"\u003e\u003ccode\u003enextflow\u003c/code\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eii. Install either \u003ca href=\"https://docs.docker.com/engine/installation/\" rel=\"nofollow\"\u003e\u003ccode\u003eDocker\u003c/code\u003e\u003c/a\u003e or \u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e for full pipeline reproducibility (please only use \u003ca href=\"https://conda.io/miniconda.html\" rel=\"nofollow\"\u003e\u003ccode\u003eConda\u003c/code\u003e\u003c/a\u003e as a last resort; see \u003ca href=\"https://nf-co.re/usage/configuration#basic-configuration-profiles\" rel=\"nofollow\"\u003edocs\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eiii. Download the pipeline and test it on a minimal dataset with a single command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run nibscbioinformatics/viralevo -profile test,nibsc --outdir /output/folder\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eiv. Start running your own analysis!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run nibscbioinformatics/viralevo -profile nibsc --outdir /output/folder --tools all --genome SARS-CoV-2 --input /path/to/sampleinfo.tsv\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee \u003ca href=\"docs/usage.md\"\u003eusage docs\u003c/a\u003e for all of the available options when running the pipeline.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eThe nibscbioinformatics/viralevo pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/installation\" rel=\"nofollow\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/local_installation\" rel=\"nofollow\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/adding_own_config\" rel=\"nofollow\"\u003eAdding your own system config\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/reference_genomes\" rel=\"nofollow\"\u003eReference genomes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/troubleshooting\" rel=\"nofollow\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003enibscbioinformatics/viralevo was originally written by Francesco Lescai and Thomas Bleazard.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions-and-support\" class=\"anchor\" href=\"#contributions-and-support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions and Support\u003c/h2\u003e\n\u003cp\u003eIf you would like to contribute to this pipeline, please see the \u003ca href=\".github/CONTRIBUTING.md\"\u003econtributing guidelines\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor further information or help, don\u0027t hesitate to get in touch on \u003ca href=\"https://nfcore.slack.com/channels/viralevo\" rel=\"nofollow\"\u003eSlack\u003c/a\u003e (you can join with \u003ca href=\"https://nf-co.re/join/slack\" rel=\"nofollow\"\u003ethis invite\u003c/a\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\n\n\u003cp\u003eYou can cite the \u003ccode\u003enf-core\u003c/code\u003e publication as follows:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThe nf-core framework for community-curated bioinformatics pipelines.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePhilip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso \u0026amp; Sven Nahnsen.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNat Biotechnol.\u003c/em\u003e 2020 Feb 13. doi: \u003ca href=\"https://dx.doi.org/10.1038/s41587-020-0439-x\" rel=\"nofollow\"\u003e10.1038/s41587-020-0439-x\u003c/a\u003e.\u003cbr\u003e\nReadCube: \u003ca href=\"https://rdcu.be/b1GjZ\" rel=\"nofollow\"\u003eFull Access Link\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1606504810.0
  },
  {
    "data_format": 2,
    "description": "A Singularity container for the LuxUS tool",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ftabaro/luxus-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-a-singularity-container-for-the-luxus-tool\" class=\"anchor\" href=\"#a-singularity-container-for-the-luxus-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA Singularity container for the LuxUS tool\u003c/h1\u003e\n\u003cp\u003eFor details about the LuxUS method see the \u003ca href=\"https://www.biorxiv.org/content/10.1101/536722v2\" rel=\"nofollow\"\u003epaper\u003c/a\u003e and the \u003ca href=\"https://github.com/hallav/LuxUS\"\u003erepository\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4296\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download-a-premade-luxus-image-from-singularity-hub\" class=\"anchor\" href=\"#download-a-premade-luxus-image-from-singularity-hub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload a premade LuxUS image from Singularity Hub\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity pull shub://ftabaro/luxus-singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will download a premade image from the Singularity Hub into the current working directory into \u003ccode\u003eluxus-singularity_latest.sif\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-local-image\" class=\"anchor\" href=\"#build-local-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild local image\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecd\u003c/code\u003e into che cloned repository\u003c/li\u003e\n\u003cli\u003eBuild\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity build Singularity luxus-singularity.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will build the image in the current folder into \u003ccode\u003eluxus-singularity.sif\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-luxus\" class=\"anchor\" href=\"#run-luxus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun LuxUS\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSet variables\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eINPUT_FOLDER=LuxUS/data/\nOUTPUT_FOLDER=test\nOUTPUT_FILE=test.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003ePrepare data\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity run --app prepare luxus-singularity.sif \\\n  -i $INPUT_FOLDER/proportion_table_test_data_diff1.txt \\\n  -d $INPUT_FOLDER/design_matrix_test_data_diff1.txt \\\n  -o $OUTPUT_FOLDER \\\n  -r 12 \\\n  -t 1 \\\n  -u 0.1 \\\n  -y $OUTPUT_FOLDER/window_mean_coverage_test_data_diff1.txt \\\n  -z $OUTPUT_FOLDER/window_number_of_cytosines_test_data_diff1.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eRun the analysis\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity run --app luxus luxus-singularity.sif \\\n  -d input_for_luxus_1.txt \\\n  -o $OUTPUT_FOLDER \\\n  -i $INPUT_FOLDER \\\n  -j $OUTPUT_FILE \\\n  -x 1 \\\n  -p 1 \\\n  -w 1\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1590164126.0
  },
  {
    "data_format": 2,
    "description": "Core-Genome Sequence Typing",
    "filenames": [
      "Singularity"
    ],
    "full_name": "CNRResistanceAntibiotic/CGST",
    "latest_release": null,
    "readme": "\u003cp\u003e#CGST\u003c/p\u003e\n\u003cp\u003eA Core-Genome Sequence Typing tool\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#requirements\"\u003eRequirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#method\"\u003eMethod\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#quick-usage\"\u003eQuick usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#full-usage\"\u003eFull usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#acknowledgments\"\u003eAcknowledgments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#license\"\u003eLicense\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eCGST (Core-Genome Sequence Typing) has goal to use the species core-genome MLST (Multiple Locus Sequence Typing) and the WGS (Whole Genome Sequencing) information at a new level of genotyping.\u003c/p\u003e\n\u003cp\u003eCGST is a pipeline that have 2 main part: \u003ca href=\"#detection\"\u003eDetection\u003c/a\u003e and \u003ca href=\"#analysis\"\u003eAnalysis\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-detection\" class=\"anchor\" href=\"#detection\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetection\u003c/h4\u003e\n\u003cp\u003eDetection part of CGST consist to detect the allele sequence for each locus of a bacterial strain describe in a cgMLST.\u003c/p\u003e\n\u003cp\u003eThe allele detection is done with \u003ca href=\"https://github.com/WGS-TB/MentaLiST\"\u003eMentaLiST\u003c/a\u003e tool. The novels allelic sequences found are directly use to update the cgMLST database.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://github.com/WGS-TB/MentaLiST\"\u003eMentaLiST\u003c/a\u003e output is enhanced by others information like classic MLST schema with \u003ca href=\"https://github.com/sanger-pathogens/ariba\"\u003eAriba\u003c/a\u003e to produce the final CGST output.\u003c/p\u003e\n\u003cp\u003eCGST contains for each species cgMLST a list of combinations alleles and the combination found in the strain are stored in the \u003ccode\u003ecombination_result.tsv\u003c/code\u003e file.\u003c/p\u003e\n\u003cp\u003eThe detection CGST part has a high memory consumption (MentaLiST consumption). More than 16Go with the \u003ccode\u003eEscherichia coli\u003c/code\u003e core-genome.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-analysis\" class=\"anchor\" href=\"#analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnalysis\u003c/h4\u003e\n\u003cp\u003eAnalysis part of CGST consist to classify, summarize and compare the information provides by the detection part of CGST in a set of strains base on the same core-genome.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe classification step consist to regroup the strains by their alleles difference on a numeric scale.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe summarize step consist to get the alignment of the allele difference provide by \u003ca href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\"\u003eMAFFT\u003c/a\u003e then use \u003ca href=\"https://github.com/sanger-pathogens/gubbins\"\u003eGubbins\u003c/a\u003e to remove the recombination SNP and get a SNP alignment. This alignment is use with \u003ca href=\"https://github.com/amkozlov/raxml-ng\"\u003eRaXML-ng\u003c/a\u003e to build the more accurate possible phylotree.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cp\u003eCGST assumes that you have \u003ca href=\"https://github.com/lh3/miniasm\"\u003eMentaLiST\u003c/a\u003e, \u003ca href=\"https://julialang.org/downloads/\" rel=\"nofollow\"\u003ejulia\u003c/a\u003e(for MentaliST), \u003ca href=\"https://github.com/sanger-pathogens/gubbins\"\u003eGubbins\u003c/a\u003e - available by \u003ccode\u003erun_gubbins\u003c/code\u003e instead of \u003ccode\u003erun_gubbins.py\u003c/code\u003e , \u003ca href=\"https://github.com/amkozlov/raxml-ng\"\u003eRaXML-ng\u003c/a\u003e, \u003ca href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\"\u003eMAFFT\u003c/a\u003e and \u003ca href=\"https://cran.r-project.org/\" rel=\"nofollow\"\u003eR\u003c/a\u003e installed and available in your PATH. If you can run \u003ccode\u003ementalist -v\u003c/code\u003e, \u003ccode\u003ejulia -v\u003c/code\u003e, \u003ccode\u003erun_gubbins --version\u003c/code\u003e, \u003ccode\u003emafft\u003c/code\u003e, \u003ccode\u003eraxml-ng --version\u003c/code\u003e and \u003ccode\u003eR --version\u003c/code\u003e on the command line, you should be good to go!\u003c/p\u003e\n\u003cp\u003eYou\u0027ll need Python 3.6 or later to run CGST (check with \u003ccode\u003epython3 --version\u003c/code\u003e). The Python package requirement are \u003ca href=\"https://biopython.org/wiki/Download\" rel=\"nofollow\"\u003eBiopython\u003c/a\u003e and \u003ca href=\"https://pypi.org/project/pandas/\" rel=\"nofollow\"\u003epandas\u003c/a\u003e. If you don\u0027t already have this package, it will be installed as part of the CGST installation process.\u003c/p\u003e\n\u003cp\u003eYou\u0027ll need R 3.6 or later to run CGST (check with \u003ccode\u003eR --version\u003c/code\u003e). The R package requirement are \u003ca href=\"https://cran.r-project.org/web/packages/optparse/index.html\" rel=\"nofollow\"\u003eoptparse\u003c/a\u003e, \u003ca href=\"https://cran.r-project.org/web/packages/questionr/index.html\" rel=\"nofollow\"\u003equestionr\u003c/a\u003e, \u003ca href=\"https://cran.r-project.org/web/packages/cluster/index.html\" rel=\"nofollow\"\u003ecluster\u003c/a\u003e and \u003ca href=\"https://cran.r-project.org/web/packages/fastcluster/index.html\" rel=\"nofollow\"\u003efastcluster\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-help-for-installation-dependencies--or-use-singularity\" class=\"anchor\" href=\"#help-for-installation-dependencies--or-use-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHelp for installation dependencies / Or use Singularity\u003c/h3\u003e\n\u003cp\u003eYou can follow the installation of the dependencies in the Singularity of CGST available on the repository.\u003c/p\u003e\n\u003cp\u003eOr you can build your own Singularity image by:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build CGST.simg Singularity\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e CGST.simg cgst\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eInstall from source\u003c/p\u003e\n\u003cp\u003eYou can install CGST using \u003ca href=\"https://pypi.org/project/pip/\" rel=\"nofollow\"\u003epip\u003c/a\u003e, either from a local copy:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/CNRResistanceAntibiotic/CGST.git\npip3 install ./CGST\ncgst --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor directly from GitHub:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip3 install git+https://github.com/CNRResistanceAntibiotic/CGST.git\ncgst --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf these installation commands aren\u0027t working for you (e.g. an error message like \u003ccode\u003eCommand \u0027pip3\u0027 not found\u003c/code\u003e or \u003ccode\u003ecommand \u0027gcc\u0027 failed with exit status 1\u003c/code\u003e) then check out the \u003ca href=\"https://github.com/rrwick/Badread/wiki/Installation-issues\"\u003einstallation issues page on the Badread wiki page\u003c/a\u003e (a different tool done by \u003ca href=\"https://github.com/rrwick\"\u003eRyan Wick\u003c/a\u003e).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-run-without-installation\" class=\"anchor\" href=\"#run-without-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun without installation\u003c/h3\u003e\n\u003cp\u003eCGST can also be run directly from its repository by using the \u003ccode\u003eCGST.py\u003c/code\u003e script, no installation required:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/CNRResistanceAntibiotic/CGST.git\nCGST/cgst/cgst.py -h\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you run CGST this way, it\u0027s up to you to make sure that \u003ca href=\"https://biopython.org/wiki/Download\" rel=\"nofollow\"\u003eBiopython\u003c/a\u003e and \u003ca href=\"https://pypi.org/project/pandas/\" rel=\"nofollow\"\u003epandas\u003c/a\u003e are installed for your Python environment.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-method\" class=\"anchor\" href=\"#method\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMethod\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1-display-core-genome-available\" class=\"anchor\" href=\"#step-1-display-core-genome-available\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1: Display Core-Genome available\u003c/h3\u003e\n\u003cp\u003eCGST mirror the \u003ca href=\"https://github.com/lh3/miniasm\"\u003eMentaLiST\u003c/a\u003e own download function that use the core-genome available in \u003ca href=\"https://www.cgmlst.org/ncs\" rel=\"nofollow\"\u003ecgmlst.org\u003c/a\u003e and mirror the CNR \u003ca href=\"https://github.com/CNRResistanceAntibiotic/core_genomes\"\u003ecore_genomes\u003c/a\u003e repo.\u003c/p\u003e\n\u003cp\u003eCommand : \u003ccode\u003ecgst available-species\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2-download-data-to-build-the-database\" class=\"anchor\" href=\"#step-2-download-data-to-build-the-database\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2: Download data to build the database\u003c/h3\u003e\n\u003cp\u003eCGST download cgMLST in mirror of the \u003ca href=\"https://github.com/lh3/miniasm\"\u003eMentaLiST\u003c/a\u003e build function.\u003c/p\u003e\n\u003cp\u003eExample for \"Escherichia coli\"\nCommand :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecgst build-database -db database_CGST_folder_path -sp \"Escherichia coli\" -t 12\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-3-check-cgst-database\" class=\"anchor\" href=\"#step-3-check-cgst-database\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3: Check CGST database\u003c/h3\u003e\n\u003cp\u003eYou can check the database of CGST.\u003c/p\u003e\n\u003cp\u003eCommand : \u003ccode\u003ecgst status-genome -db database_CGST_folder_path\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-4-cgst-detection\" class=\"anchor\" href=\"#step-4-cgst-detection\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 4: CGST detection\u003c/h3\u003e\n\u003cp\u003eCGST detection command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecgst detection -1 /illumina_reads/my_ecoli_S30_R1_001.fastq.gz -2 /illumina_reads/my_ecoli_R2_001.fastq.gz -db /database_CGST_path -w /detection_folder_output -o my_ecoli -sp \"Escherichia coli\" -t 12\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe outputs are constitute by :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e\u00ecntermediate_files\u003c/code\u003e folder contains the \u003ca href=\"https://github.com/lh3/miniasm\"\u003eMentaLiST\u003c/a\u003e output files.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emy_ecoli_output_ariba\u003c/code\u003e folder contains the ARIBA output files.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecombination_result.tsv\u003c/code\u003e file contains the detection combination.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eColumn header\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eCombination Name\u003c/td\u003e\n\u003ctd\u003eName of the locus combination\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCount Reference Locus\u003c/td\u003e\n\u003ctd\u003eSum of locus in the combination\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCount Sample Locus\u003c/td\u003e\n\u003ctd\u003eSum of locus of the combination retrieves in the strain\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRatio\u003c/td\u003e\n\u003ctd\u003ePercent ration of the \u003ccode\u003eCount Sample Locus\u003c/code\u003e in the \u003ccode\u003eCount Reference Locus\u003c/code\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eComment\u003c/td\u003e\n\u003ctd\u003eCommentary that appreciate the ratio with a scale\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eScale of Comment head columns :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- Ratio == 100  -\u0026gt; `Perfect`\n- 100\u0026gt;Ratio\u0026gt;98  -\u0026gt; `Very Close`\n- 98\u0026gt;Ratio\u0026gt;90  -\u0026gt; `Close`\n- 90\u0026gt;Ratio\u0026gt;80  -\u0026gt; `Like`\n- 80\u0026gt;Ratio\u0026gt;70  -\u0026gt; `Close Like`\n- 70\u0026gt;Ratio\u0026gt;0  -\u0026gt; `No relevant`\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003emy_ecoli_output_final\u003c/code\u003e file contains CGST final results.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003estatistics.tsv\u003c/code\u003e file contains the CGST statistics.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eColumn header\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eTotal Locus\u003c/td\u003e\n\u003ctd\u003eNumber of locus in the cgMLST\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePerfect Locus\u003c/td\u003e\n\u003ctd\u003eNumber of perfect locus match in MentaLiST\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMultiple Locus\u003c/td\u003e\n\u003ctd\u003eNumber of multiple locus match (+) in MentaLiST\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLow Coverage Locus\u003c/td\u003e\n\u003ctd\u003eNumber of low coverage locus match (-) in MentaLiST\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eNone Locus\u003c/td\u003e\n\u003ctd\u003eNumber of none locus match in MentaLiST\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-5-cgst-analysis\" class=\"anchor\" href=\"#step-5-cgst-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 5: CGST analysis\u003c/h3\u003e\n\u003cp\u003eCreate a folder for analysis and copy the output final CGST files of the wanted strains.\u003c/p\u003e\n\u003cp\u003eThe analysis can only done on 5 strains or more.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir analysis_CGST\ncp /detection_folder_output/detection_*/*_output_final analysis_CGST\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCGST analysis command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecgst analysis -dd /analysis_CGST -db /database_CGST_path -w /analysis_CGST/analysis_comput -sp \"Escherichia coli\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe outputs are constitute by 3 folder and \"all_report.tsv\" file:\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-folder--cluster\" class=\"anchor\" href=\"#folder--cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFolder : cluster\u003c/h5\u003e\n\u003cp\u003eThis folder contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe file \u003ccode\u003elvl_report.tsv\u003c/code\u003e is the strain classification by an increase scale of alleles share by the strains.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-folder--combination\" class=\"anchor\" href=\"#folder--combination\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFolder : combination\u003c/h5\u003e\n\u003cp\u003eThis folder contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe file \u003ccode\u003esimilarity_matrix.tsv\u003c/code\u003e is the similarity matrix of the relevant locus of strains.\u003c/li\u003e\n\u003cli\u003eThe file \u003ccode\u003edendrogram_with_class.pdf\u003c/code\u003e is the dendrogram that represent the 3 more relevant classification class of strains.\u003c/li\u003e\n\u003c/ul\u003e\n  \u003cp align=\"center\"\u003e\u003ca href=\"images/dendrogram.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/dendrogram.png\" alt=\"Dendrogram\" width=\"600\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe file \u003ccode\u003egroups.tsv\u003c/code\u003e is the classification class by strains for each of groups of the 3 class.\u003c/li\u003e\n\u003cli\u003eThe file \u003ccode\u003einertial_graph.pdf\u003c/code\u003e is the inertial class curve produce by the \u003ccode\u003ehclust\u003c/code\u003e R package.\u003c/li\u003e\n\u003c/ul\u003e\n \u003cp align=\"center\"\u003e\u003ca href=\"images/inertial.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/inertial.png\" alt=\"Inertial graph\" width=\"600\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe file \u003ccode\u003elogR.txt\u003c/code\u003e is the log file of the R script \u003ccode\u003er_script.R\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eThe file \u003ccode\u003egroups_alleles.tsv\u003c/code\u003e is classification of the strains, the alleles share by all strains and the alleles share more than 90% of the strains for each groups class and class.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-folder--phylotree\" class=\"anchor\" href=\"#folder--phylotree\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFolder : phylotree\u003c/h5\u003e\n\u003cp\u003eThis folder contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe sub-folder \u003ccode\u003emsa\u003c/code\u003e contains files produces by \u003ca href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\"\u003eMAFFT\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThe file \u003ccode\u003ecore-genome.aln\u003c/code\u003e is the core-genome alignment obtains by concatenation of the alignment \u003ca href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\"\u003eMAFFT\u003c/a\u003e files.\u003c/li\u003e\n\u003cli\u003eThe file \u003ccode\u003eresume_core-genome.tsv\u003c/code\u003e is the resume of each locus position in the \u003ccode\u003ecore-genome.aln\u003c/code\u003e file.\u003c/li\u003e\n\u003cli\u003eThe sub-folder \u003ccode\u003egubbins\u003c/code\u003e contains files produces by \u003ca href=\"https://github.com/sanger-pathogens/gubbins\"\u003eGubbins\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThe sub-folder \u003ccode\u003eraxml-ng\u003c/code\u003e contains files produces by \u003ca href=\"https://github.com/amkozlov/raxml-ng\"\u003eRaXML-ng\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003ca href=\"https://github.com/amkozlov/raxml-ng\"\u003eRaXML-ng\u003c/a\u003e phylotree purpose is to be the more accurate phylogenetic tree between the strains based on the SNP.\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-file-all_reporttsv\" class=\"anchor\" href=\"#file-all_reporttsv\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFile: all_report.tsv\u003c/h5\u003e\n\u003cp\u003eThis file contains all alleles detected for all strains of the analysis.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-6-add-new-combination\" class=\"anchor\" href=\"#step-6-add-new-combination\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 6: Add new combination\u003c/h3\u003e\n\u003cp\u003eIn the \u003ccode\u003ecombination\u003c/code\u003e folder, the file \u003ccode\u003egroups_alleles.tsv\u003c/code\u003e contains the information to update/create the combination database for CGST detection.\u003c/p\u003e\n\u003cp\u003eIn the species folder of the CGST-cgMLST database, update or create the file \u003ccode\u003ecombination_list.tsv\u003c/code\u003e.\nThis file contains this columns:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eColumn header\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eName\u003c/td\u003e\n\u003ctd\u003eThe name of the combination\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eNumber_Strain\u003c/td\u003e\n\u003ctd\u003eThe number of strain regroups by this combination at the declaration of this combination\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLength_Combination\u003c/td\u003e\n\u003ctd\u003eThe number of alleles of the combination\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCombination\u003c/td\u003e\n\u003ctd\u003eThe sequence of the alleles of the combination\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eIf the new combination that you have identified contains new alleles only on your local CGST database and you want to share your combination. Please go to the \u003ca href=\"https://github.com/CNRResistanceAntibiotic/CGST/issues\"\u003eissue\u003c/a\u003e page.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-full-usage\" class=\"anchor\" href=\"#full-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFull usage\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eusage: CGST [commands][options] \n\nCGST: Core-Genome Sequence Typing - Version 0.0.1\n\npositional arguments:\n  \u0026lt;commands\u0026gt;                   Description\n    check                      check Dependencies\n    available-species          Available Species in cgmlst.org\n    build-database             Build Database by species\n    status-genome              Resume of genomes available\n    detection                  Detection Sequence Typing\n    analysis                   Analysis strains that had a cgMLST detection\n\noptional arguments:\n  -f [FORCE], --force [FORCE]  Overwrite output directory (default: False)\n  -V, --version                Prints version number\n  -h, --help                   Show this help message and exit\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgments\u003c/h2\u003e\n\u003cp\u003eThis tool and this GitHub repository is mainly inspired by the work of \u003ca href=\"https://github.com/rrwick\"\u003eRyan Wick\u003c/a\u003e. A big thank to his work and the good practice in computer programming that he done.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003eIn progress...\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.gnu.org/licenses/gpl-3.0.html\" rel=\"nofollow\"\u003eGNU General Public License, version 3\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605284164.0
  },
  {
    "data_format": 2,
    "description": "A Montage workflow for Pegasus 5.0",
    "filenames": [
      "Singularity"
    ],
    "full_name": "pegasus-isi/montage-workflow-v3",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-montage-workflow-v3\" class=\"anchor\" href=\"#montage-workflow-v3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emontage-workflow-v3\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eNOTE: This is a Montage workflow version which requires Pegasus 5.0. For a version that works with\nPegasus 4, please use \u003ccode\u003emontage-workflow-v2\u003c/code\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eA new Python DAX generator version of the classic Montage workflow. This workflow uses the \u003ca href=\"http://montage.ipac.caltech.edu\" rel=\"nofollow\"\u003eMontage\ntoolkit\u003c/a\u003e to re-project, background correct and add astronomical\nimages into custom mosaics.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://pegasus.isi.edu\" rel=\"nofollow\"\u003ePegasus\u003c/a\u003e - version 5.0 or later\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://montage.ipac.caltech.edu\" rel=\"nofollow\"\u003eMontage\u003c/a\u003e - version 6.0 or later\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.astropy.org/\" rel=\"nofollow\"\u003eAstroPy\u003c/a\u003e - version 1.0 or later\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-plan-a-montage-workflow\" class=\"anchor\" href=\"#plan-a-montage-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlan a Montage Workflow\u003c/h2\u003e\n\u003cp\u003eThe \u003cem\u003e./montage-workflow.py\u003c/em\u003e Python script sets up a \u003cem\u003edata/\u003c/em\u003e directory with a Pegasus DAX,\nimage tables and region headers. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./montage-workflow.py --center \"56.7 24.0\" --degrees 2.0 \\\n          --band dss:DSS2B:blue --band dss:DSS2R:green --band dss:DSS2IR:red\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will create a 2x2 degree mosaic centered on 56.7 24.0, with 3 bands making up the\nred, green, and blue channels for the final JPEG output. A 2 degree workflow has a lot\nof input images and thus the workflow becomes wide. I simplified version of the workflow\nlooks like:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"docs/images/dax1.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/dax1.png?raw=true\" alt=\"DAX 1\" title=\"DAX 1\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cp\u003eThe quickest way to get started is to use the \u003cem\u003e./example-dss.sh\u003c/em\u003e\nscript. It shows how to use the \u003cem\u003emontage-workflow.py\u003c/em\u003e DAX generator to set up and plan\n2 degree workflows as described above. Example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ./example-dss.sh \n\nAdding band 1 (dss DSS2B -\u0026gt; blue)\nRunning sub command: mArchiveList dss DSS2B \"56.7 24.00\" 2.2 2.2 data/1-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mDAGTbls 1-images.tbl region-oversized.hdr 1-raw.tbl 1-projected.tbl 1-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mOverlaps 1-raw.tbl 1-diffs.tbl\n[struct stat=\"OK\", count=120]\n\nAdding band 2 (dss DSS2R -\u0026gt; green)\nRunning sub command: mArchiveList dss DSS2R \"56.7 24.00\" 2.2 2.2 data/2-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mDAGTbls 2-images.tbl region-oversized.hdr 2-raw.tbl 2-projected.tbl 2-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mOverlaps 2-raw.tbl 2-diffs.tbl\n[struct stat=\"OK\", count=120]\n\nAdding band 3 (dss DSS2IR -\u0026gt; red)\nRunning sub command: mArchiveList dss DSS2IR \"56.7 24.00\" 2.2 2.2 data/3-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mDAGTbls 3-images.tbl region-oversized.hdr 3-raw.tbl 3-projected.tbl 3-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mOverlaps 3-raw.tbl 3-diffs.tbl\n[struct stat=\"OK\", count=120]\n2016.06.02 21:46:32.455 PDT:    \n2016.06.02 21:46:32.461 PDT:   ----------------------------------------------------------------------- \n2016.06.02 21:46:32.466 PDT:   File for submitting this DAG to HTCondor           : montage-0.dag.condor.sub \n2016.06.02 21:46:32.471 PDT:   Log of DAGMan debugging messages                 : montage-0.dag.dagman.out \n2016.06.02 21:46:32.476 PDT:   Log of HTCondor library output                     : montage-0.dag.lib.out \n2016.06.02 21:46:32.481 PDT:   Log of HTCondor library error messages             : montage-0.dag.lib.err \n2016.06.02 21:46:32.487 PDT:   Log of the life of condor_dagman itself          : montage-0.dag.dagman.log \n2016.06.02 21:46:32.492 PDT:    \n2016.06.02 21:46:32.497 PDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: \n2016.06.02 21:46:32.507 PDT:   ----------------------------------------------------------------------- \n2016.06.02 21:46:33.387 PDT:   Your database is compatible with Pegasus version: 4.6.1 \n2016.06.02 21:46:33.392 PDT:   \n\nI have concretized your abstract workflow. The workflow has been entered \ninto the workflow database with a state of \"planned\". The next step is \nto start or execute your workflow. The invocation required is\n\npegasus-run  /data/scratch/rynge/montage2/montage-workflow-v2/work/1464929190\n\n2016.06.02 21:46:33.419 PDT:   Time taken to execute is 2.961 seconds \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRunning the workflow produces fits and jpeg mosaics for each band, as well as a combined color one:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"docs/images/pleiades.jpg?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/pleiades.jpg?raw=true\" alt=\"Pleiades\" title=\"Pleiades\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 8,
    "topics": [
      "astronomy"
    ],
    "updated_at": 1616748340.0
  },
  {
    "data_format": 2,
    "description": "A nextflow pipeline to call somatic gene fusions",
    "filenames": [
      "Singularity/Singularity.v1.0"
    ],
    "full_name": "adigenova/nf-gene-fusions",
    "latest_release": "v1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-rna-fusions\" class=\"anchor\" href=\"#nf-rna-fusions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-rna-fusions\u003c/h1\u003e\n\u003cp\u003eA nextflow pipeline to call somatic rna fusions\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603289117.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "docker/Singularity",
      "docker/railrl_hand_v3/Singularity",
      "docker/railrl_hand_v3/Singularity_cpu",
      "docker/railrl_hand_tf_v1/Singularity",
      "docker/railrl_hand_tf_v1/Singularity_cpu",
      "docker/railrl_v10_cuda10-1_mj2-0-2-2_torch0-4-1_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_hand_v1/Singularity",
      "docker/railrl_hand_v1/Singularity_cpu",
      "docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity",
      "docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity_cpu",
      "docker/railrl_v6_cuda9/Singularity",
      "docker/railrl_ray_gym-0-12-0/Singularity_from_scratch_cuda8",
      "docker/railrl_ray_gym-0-12-0/Singularity_from_scratch",
      "docker/railrl_v9_cuda10-1_mj1-50-1-59_torch0-4-1_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_v7/Singularity",
      "docker/railrl_v11_cuda10-1_mj2-0-2-2_torch0-3-1_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_v6_cuda8/Singularity",
      "docker/metac_railrl_v12_cuda10-1_mj2-0-2-2_torch1-4-0_gym0-12-5_py3-6-5/Singularity",
      "docker/metac_railrl_v12_cuda10-1_mj2-0-2-2_torch1-4-0_gym0-12-5_py3-6-5/Singularity_cpu",
      "docker/railrl_ray/Singularity",
      "docker/railrl_v7_cuda8/Singularity",
      "docker/railrl_v5/singularity/Singularity",
      "docker/railrl_v9-5_cuda10-1_mj1-50-1-59_torch1-1-0_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_hand_v2/Singularity",
      "docker/railrl_hand_v2/Singularity_cpu",
      "docker/railrl_v8_cuda10-1/Singularity",
      "docker/railrl_gpu_mujoco1-5-v4/singularity/Singularity"
    ],
    "full_name": "jcoreyes/erl",
    "latest_release": null,
    "readme": "\u003cp\u003eREADME last updated on: 01/24/2018\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-railrl\" class=\"anchor\" href=\"#railrl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erailrl\u003c/h1\u003e\n\u003cp\u003eReinforcement learning framework.\nSome implemented algorithms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"examples/ddpg.py\"\u003eDeep Deterministic Policy Gradient (DDPG)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/sac.py\"\u003eSoft Actor Critic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/dqn_and_double_dqn.py\"\u003e(Double) Deep Q-Network (DQN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/her.py\"\u003eHindsight Experience Replay (HER)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/model_based_dagger.py\"\u003eMPC with Neural Network Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/naf.py\"\u003eNormalized Advantage Function (NAF)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWARNING: I haven\u0027t tested this NAF implementation much, so it may not match the paper\u0027s performance. I\u0027m pretty confident about the other two implementations though.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo get started, checkout the example scripts, linked above.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-some-dependancies\" class=\"anchor\" href=\"#some-dependancies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSome dependancies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt-get install swig\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-create-conda-env\" class=\"anchor\" href=\"#create-conda-env\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate Conda Env\u003c/h3\u003e\n\u003cp\u003eInstall and use the included ananconda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda env create -f docker/railrl/railrl-env.yml\n$ source activate railrl-env\n(railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if you want you can use the docker image included.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-simulation-env-code\" class=\"anchor\" href=\"#download-simulation-env-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload Simulation Env Code\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/vitchyr/multiworld\"\u003emultiworld\u003c/a\u003e (contains environments):\u003ccode\u003egit clone https://github.com/vitchyr/multiworld\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-optional-install-doodad\" class=\"anchor\" href=\"#optional-install-doodad\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e(Optional) Install doodad\u003c/h3\u003e\n\u003cp\u003eI recommend installing \u003ca href=\"https://github.com/justinjfu/doodad\"\u003edoodad\u003c/a\u003e to\nlaunch jobs. Some of its nice features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasily switch between running code locally, on a remote compute with\nDocker, on EC2 with Docker\u003c/li\u003e\n\u003cli\u003eEasily add your dependencies that can\u0027t be installed via pip (e.g. you\nborrowed someone\u0027s code)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you install doodad, also modify \u003ccode\u003eCODE_DIRS_TO_MOUNT\u003c/code\u003e in \u003ccode\u003econfig.py\u003c/code\u003e to\ninclude:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePath to rllab directory\u003c/li\u003e\n\u003cli\u003ePath to railrl directory\u003c/li\u003e\n\u003cli\u003ePath to other code you want to juse\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou\u0027ll probably also need to update the other variables besides the docker\nimages/instance stuff.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup-config-file\" class=\"anchor\" href=\"#setup-config-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup Config File\u003c/h3\u003e\n\u003cp\u003eYou must setup the config file for launching experiments, providing paths to your code and data directories. Inside \u003ccode\u003erailrl/config/launcher_config.py\u003c/code\u003e, fill in the appropriate paths. You can use \u003ccode\u003erailrl/config/launcher_config_template.py\u003c/code\u003e as an example reference.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecp railrl/launchers/config-template.py railrl/launchers/config.py\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-visualizing-a-policy-and-seeing-results\" class=\"anchor\" href=\"#visualizing-a-policy-and-seeing-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVisualizing a policy and seeing results\u003c/h2\u003e\n\u003cp\u003eDuring training, the results will be saved to a file called under\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eLOCAL_LOG_DIR\u003c/code\u003e is the directory set by \u003ccode\u003erailrl.launchers.config.LOCAL_LOG_DIR\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;exp_prefix\u0026gt;\u003c/code\u003e is given either to \u003ccode\u003esetup_logger\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;foldername\u0026gt;\u003c/code\u003e is auto-generated and based off of \u003ccode\u003eexp_prefix\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003einside this folder, you should see a file called \u003ccode\u003eparams.pkl\u003c/code\u003e. To visualize a policy, run\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;/params.pkl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you have rllab installed, you can also visualize the results\nusing \u003ccode\u003erllab\u003c/code\u003e\u0027s viskit, described at\nthe bottom of \u003ca href=\"http://rllab.readthedocs.io/en/latest/user/cluster.html\" rel=\"nofollow\"\u003ethis page\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etl;dr run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython rllab/viskit/frontend.py LOCAL_LOG_DIR/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eexp_prefix\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-add-paths\" class=\"anchor\" href=\"#add-paths\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdd paths\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credit\" class=\"anchor\" href=\"#credit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredit\u003c/h2\u003e\n\u003cp\u003eA lot of the coding infrastructure is based on \u003ca href=\"https://github.com/rll/rllab\"\u003erllab\u003c/a\u003e.\nAlso, the serialization and logger code are basically a carbon copy.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603507389.0
  },
  {
    "data_format": 2,
    "description": "Nextflow pipeline for analysis of the twist myeloid panel",
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "Clinical-Genomics-Lund/nextflow_myeloid",
    "latest_release": null,
    "readme": "\u003cp\u003eREADME last updated on: 01/24/2018\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-railrl\" class=\"anchor\" href=\"#railrl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erailrl\u003c/h1\u003e\n\u003cp\u003eReinforcement learning framework.\nSome implemented algorithms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"examples/ddpg.py\"\u003eDeep Deterministic Policy Gradient (DDPG)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/sac.py\"\u003eSoft Actor Critic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/dqn_and_double_dqn.py\"\u003e(Double) Deep Q-Network (DQN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/her.py\"\u003eHindsight Experience Replay (HER)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/model_based_dagger.py\"\u003eMPC with Neural Network Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/naf.py\"\u003eNormalized Advantage Function (NAF)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWARNING: I haven\u0027t tested this NAF implementation much, so it may not match the paper\u0027s performance. I\u0027m pretty confident about the other two implementations though.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo get started, checkout the example scripts, linked above.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-some-dependancies\" class=\"anchor\" href=\"#some-dependancies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSome dependancies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt-get install swig\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-create-conda-env\" class=\"anchor\" href=\"#create-conda-env\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate Conda Env\u003c/h3\u003e\n\u003cp\u003eInstall and use the included ananconda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda env create -f docker/railrl/railrl-env.yml\n$ source activate railrl-env\n(railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if you want you can use the docker image included.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-simulation-env-code\" class=\"anchor\" href=\"#download-simulation-env-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload Simulation Env Code\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/vitchyr/multiworld\"\u003emultiworld\u003c/a\u003e (contains environments):\u003ccode\u003egit clone https://github.com/vitchyr/multiworld\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-optional-install-doodad\" class=\"anchor\" href=\"#optional-install-doodad\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e(Optional) Install doodad\u003c/h3\u003e\n\u003cp\u003eI recommend installing \u003ca href=\"https://github.com/justinjfu/doodad\"\u003edoodad\u003c/a\u003e to\nlaunch jobs. Some of its nice features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasily switch between running code locally, on a remote compute with\nDocker, on EC2 with Docker\u003c/li\u003e\n\u003cli\u003eEasily add your dependencies that can\u0027t be installed via pip (e.g. you\nborrowed someone\u0027s code)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you install doodad, also modify \u003ccode\u003eCODE_DIRS_TO_MOUNT\u003c/code\u003e in \u003ccode\u003econfig.py\u003c/code\u003e to\ninclude:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePath to rllab directory\u003c/li\u003e\n\u003cli\u003ePath to railrl directory\u003c/li\u003e\n\u003cli\u003ePath to other code you want to juse\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou\u0027ll probably also need to update the other variables besides the docker\nimages/instance stuff.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup-config-file\" class=\"anchor\" href=\"#setup-config-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup Config File\u003c/h3\u003e\n\u003cp\u003eYou must setup the config file for launching experiments, providing paths to your code and data directories. Inside \u003ccode\u003erailrl/config/launcher_config.py\u003c/code\u003e, fill in the appropriate paths. You can use \u003ccode\u003erailrl/config/launcher_config_template.py\u003c/code\u003e as an example reference.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecp railrl/launchers/config-template.py railrl/launchers/config.py\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-visualizing-a-policy-and-seeing-results\" class=\"anchor\" href=\"#visualizing-a-policy-and-seeing-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVisualizing a policy and seeing results\u003c/h2\u003e\n\u003cp\u003eDuring training, the results will be saved to a file called under\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eLOCAL_LOG_DIR\u003c/code\u003e is the directory set by \u003ccode\u003erailrl.launchers.config.LOCAL_LOG_DIR\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;exp_prefix\u0026gt;\u003c/code\u003e is given either to \u003ccode\u003esetup_logger\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;foldername\u0026gt;\u003c/code\u003e is auto-generated and based off of \u003ccode\u003eexp_prefix\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003einside this folder, you should see a file called \u003ccode\u003eparams.pkl\u003c/code\u003e. To visualize a policy, run\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;/params.pkl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you have rllab installed, you can also visualize the results\nusing \u003ccode\u003erllab\u003c/code\u003e\u0027s viskit, described at\nthe bottom of \u003ca href=\"http://rllab.readthedocs.io/en/latest/user/cluster.html\" rel=\"nofollow\"\u003ethis page\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etl;dr run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython rllab/viskit/frontend.py LOCAL_LOG_DIR/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eexp_prefix\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-add-paths\" class=\"anchor\" href=\"#add-paths\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdd paths\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credit\" class=\"anchor\" href=\"#credit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredit\u003c/h2\u003e\n\u003cp\u003eA lot of the coding infrastructure is based on \u003ca href=\"https://github.com/rll/rllab\"\u003erllab\u003c/a\u003e.\nAlso, the serialization and logger code are basically a carbon copy.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1618430781.0
  },
  {
    "data_format": 2,
    "description": "Snakemake pipeline wrapping sequana_coverage to analyse several samples at the same time",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "sequana/sequana_coverage",
    "latest_release": null,
    "readme": "\u003cp\u003eREADME last updated on: 01/24/2018\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-railrl\" class=\"anchor\" href=\"#railrl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erailrl\u003c/h1\u003e\n\u003cp\u003eReinforcement learning framework.\nSome implemented algorithms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"examples/ddpg.py\"\u003eDeep Deterministic Policy Gradient (DDPG)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/sac.py\"\u003eSoft Actor Critic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/dqn_and_double_dqn.py\"\u003e(Double) Deep Q-Network (DQN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/her.py\"\u003eHindsight Experience Replay (HER)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/model_based_dagger.py\"\u003eMPC with Neural Network Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/naf.py\"\u003eNormalized Advantage Function (NAF)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWARNING: I haven\u0027t tested this NAF implementation much, so it may not match the paper\u0027s performance. I\u0027m pretty confident about the other two implementations though.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo get started, checkout the example scripts, linked above.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-some-dependancies\" class=\"anchor\" href=\"#some-dependancies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSome dependancies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt-get install swig\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-create-conda-env\" class=\"anchor\" href=\"#create-conda-env\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate Conda Env\u003c/h3\u003e\n\u003cp\u003eInstall and use the included ananconda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda env create -f docker/railrl/railrl-env.yml\n$ source activate railrl-env\n(railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if you want you can use the docker image included.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-simulation-env-code\" class=\"anchor\" href=\"#download-simulation-env-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload Simulation Env Code\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/vitchyr/multiworld\"\u003emultiworld\u003c/a\u003e (contains environments):\u003ccode\u003egit clone https://github.com/vitchyr/multiworld\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-optional-install-doodad\" class=\"anchor\" href=\"#optional-install-doodad\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e(Optional) Install doodad\u003c/h3\u003e\n\u003cp\u003eI recommend installing \u003ca href=\"https://github.com/justinjfu/doodad\"\u003edoodad\u003c/a\u003e to\nlaunch jobs. Some of its nice features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasily switch between running code locally, on a remote compute with\nDocker, on EC2 with Docker\u003c/li\u003e\n\u003cli\u003eEasily add your dependencies that can\u0027t be installed via pip (e.g. you\nborrowed someone\u0027s code)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you install doodad, also modify \u003ccode\u003eCODE_DIRS_TO_MOUNT\u003c/code\u003e in \u003ccode\u003econfig.py\u003c/code\u003e to\ninclude:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePath to rllab directory\u003c/li\u003e\n\u003cli\u003ePath to railrl directory\u003c/li\u003e\n\u003cli\u003ePath to other code you want to juse\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou\u0027ll probably also need to update the other variables besides the docker\nimages/instance stuff.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup-config-file\" class=\"anchor\" href=\"#setup-config-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup Config File\u003c/h3\u003e\n\u003cp\u003eYou must setup the config file for launching experiments, providing paths to your code and data directories. Inside \u003ccode\u003erailrl/config/launcher_config.py\u003c/code\u003e, fill in the appropriate paths. You can use \u003ccode\u003erailrl/config/launcher_config_template.py\u003c/code\u003e as an example reference.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecp railrl/launchers/config-template.py railrl/launchers/config.py\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-visualizing-a-policy-and-seeing-results\" class=\"anchor\" href=\"#visualizing-a-policy-and-seeing-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVisualizing a policy and seeing results\u003c/h2\u003e\n\u003cp\u003eDuring training, the results will be saved to a file called under\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eLOCAL_LOG_DIR\u003c/code\u003e is the directory set by \u003ccode\u003erailrl.launchers.config.LOCAL_LOG_DIR\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;exp_prefix\u0026gt;\u003c/code\u003e is given either to \u003ccode\u003esetup_logger\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;foldername\u0026gt;\u003c/code\u003e is auto-generated and based off of \u003ccode\u003eexp_prefix\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003einside this folder, you should see a file called \u003ccode\u003eparams.pkl\u003c/code\u003e. To visualize a policy, run\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;/params.pkl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you have rllab installed, you can also visualize the results\nusing \u003ccode\u003erllab\u003c/code\u003e\u0027s viskit, described at\nthe bottom of \u003ca href=\"http://rllab.readthedocs.io/en/latest/user/cluster.html\" rel=\"nofollow\"\u003ethis page\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etl;dr run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython rllab/viskit/frontend.py LOCAL_LOG_DIR/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eexp_prefix\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-add-paths\" class=\"anchor\" href=\"#add-paths\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdd paths\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credit\" class=\"anchor\" href=\"#credit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredit\u003c/h2\u003e\n\u003cp\u003eA lot of the coding infrastructure is based on \u003ca href=\"https://github.com/rll/rllab\"\u003erllab\u003c/a\u003e.\nAlso, the serialization and logger code are basically a carbon copy.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1595187932.0
  },
  {
    "data_format": 2,
    "description": "Sequana demultiplexing pipeline ",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "sequana/demultiplex",
    "latest_release": "v1.0.4",
    "readme": "\u003cp\u003eREADME last updated on: 01/24/2018\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-railrl\" class=\"anchor\" href=\"#railrl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erailrl\u003c/h1\u003e\n\u003cp\u003eReinforcement learning framework.\nSome implemented algorithms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"examples/ddpg.py\"\u003eDeep Deterministic Policy Gradient (DDPG)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/sac.py\"\u003eSoft Actor Critic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/dqn_and_double_dqn.py\"\u003e(Double) Deep Q-Network (DQN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/her.py\"\u003eHindsight Experience Replay (HER)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/model_based_dagger.py\"\u003eMPC with Neural Network Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/naf.py\"\u003eNormalized Advantage Function (NAF)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWARNING: I haven\u0027t tested this NAF implementation much, so it may not match the paper\u0027s performance. I\u0027m pretty confident about the other two implementations though.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo get started, checkout the example scripts, linked above.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-some-dependancies\" class=\"anchor\" href=\"#some-dependancies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSome dependancies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt-get install swig\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-create-conda-env\" class=\"anchor\" href=\"#create-conda-env\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate Conda Env\u003c/h3\u003e\n\u003cp\u003eInstall and use the included ananconda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda env create -f docker/railrl/railrl-env.yml\n$ source activate railrl-env\n(railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if you want you can use the docker image included.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-simulation-env-code\" class=\"anchor\" href=\"#download-simulation-env-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload Simulation Env Code\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/vitchyr/multiworld\"\u003emultiworld\u003c/a\u003e (contains environments):\u003ccode\u003egit clone https://github.com/vitchyr/multiworld\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-optional-install-doodad\" class=\"anchor\" href=\"#optional-install-doodad\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e(Optional) Install doodad\u003c/h3\u003e\n\u003cp\u003eI recommend installing \u003ca href=\"https://github.com/justinjfu/doodad\"\u003edoodad\u003c/a\u003e to\nlaunch jobs. Some of its nice features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasily switch between running code locally, on a remote compute with\nDocker, on EC2 with Docker\u003c/li\u003e\n\u003cli\u003eEasily add your dependencies that can\u0027t be installed via pip (e.g. you\nborrowed someone\u0027s code)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you install doodad, also modify \u003ccode\u003eCODE_DIRS_TO_MOUNT\u003c/code\u003e in \u003ccode\u003econfig.py\u003c/code\u003e to\ninclude:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePath to rllab directory\u003c/li\u003e\n\u003cli\u003ePath to railrl directory\u003c/li\u003e\n\u003cli\u003ePath to other code you want to juse\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou\u0027ll probably also need to update the other variables besides the docker\nimages/instance stuff.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup-config-file\" class=\"anchor\" href=\"#setup-config-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup Config File\u003c/h3\u003e\n\u003cp\u003eYou must setup the config file for launching experiments, providing paths to your code and data directories. Inside \u003ccode\u003erailrl/config/launcher_config.py\u003c/code\u003e, fill in the appropriate paths. You can use \u003ccode\u003erailrl/config/launcher_config_template.py\u003c/code\u003e as an example reference.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecp railrl/launchers/config-template.py railrl/launchers/config.py\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-visualizing-a-policy-and-seeing-results\" class=\"anchor\" href=\"#visualizing-a-policy-and-seeing-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVisualizing a policy and seeing results\u003c/h2\u003e\n\u003cp\u003eDuring training, the results will be saved to a file called under\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eLOCAL_LOG_DIR\u003c/code\u003e is the directory set by \u003ccode\u003erailrl.launchers.config.LOCAL_LOG_DIR\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;exp_prefix\u0026gt;\u003c/code\u003e is given either to \u003ccode\u003esetup_logger\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;foldername\u0026gt;\u003c/code\u003e is auto-generated and based off of \u003ccode\u003eexp_prefix\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003einside this folder, you should see a file called \u003ccode\u003eparams.pkl\u003c/code\u003e. To visualize a policy, run\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;/params.pkl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you have rllab installed, you can also visualize the results\nusing \u003ccode\u003erllab\u003c/code\u003e\u0027s viskit, described at\nthe bottom of \u003ca href=\"http://rllab.readthedocs.io/en/latest/user/cluster.html\" rel=\"nofollow\"\u003ethis page\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etl;dr run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython rllab/viskit/frontend.py LOCAL_LOG_DIR/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eexp_prefix\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-add-paths\" class=\"anchor\" href=\"#add-paths\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdd paths\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credit\" class=\"anchor\" href=\"#credit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredit\u003c/h2\u003e\n\u003cp\u003eA lot of the coding infrastructure is based on \u003ca href=\"https://github.com/rll/rllab\"\u003erllab\u003c/a\u003e.\nAlso, the serialization and logger code are basically a carbon copy.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1615929334.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "uazhlt/pytorch-example",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-docker-container\" class=\"anchor\" href=\"#building-the-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the docker container\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eNOTE: This step is not necessary if you simply want to use an already published image to run the example code on the UA HPC.\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker build -f Dockerfile -t uazhlt/pytorch-example .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-verify-pytorch-version\" class=\"anchor\" href=\"#verify-pytorch-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVerify PyTorch version\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003edocker run --rm -it uazhlt/pytorch-example python -c \"import torch; print(torch.__version__)\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publish-to-dockerhub\" class=\"anchor\" href=\"#publish-to-dockerhub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePublish to DockerHub\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eNOTE: This step is not necessary if you simply want to use an already published image to run the example code on the UA HPC.\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# login to dockerhub registry\ndocker login --username=yourdockerhubusername --email=youremail@domain.com\n\ndocker push org/image-name:taghere\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-a-singularity-image\" class=\"anchor\" href=\"#building-a-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding a Singularity image\u003c/h2\u003e\n\u003cp\u003eBuilding a Singularity image from a def file requires sudo on a Linux system.  In this tutorial, we avoid discussing details on installing Singularity.  If you\u0027re feeling adventurous, take a look at \u003ca href=\"./Singularity\"\u003ethe example def file in this repository\u003c/a\u003e and the official documentation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.0/user-guide/installation.html\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.0/user-guide/installation.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-alternatives\" class=\"anchor\" href=\"#alternatives\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlternatives\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-cloud-builds\" class=\"anchor\" href=\"#cloud-builds\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCloud builds\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eGitHub actions:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/singularityhub/github-ci/blob/master/.github/workflows/go.yml\"\u003eExample GitHub Workflow\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://help.github.com/en/actions/automating-your-workflow-with-github-actions/virtual-environments-for-github-hosted-runners#supported-runners-and-hardware-resources\"\u003eGitHub-hosted runners\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-vms\" class=\"anchor\" href=\"#vms\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVMs\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.0/user-guide/installation.html#singularity-vagrant-box\" rel=\"nofollow\"\u003eVagrant box\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-docker---singularity\" class=\"anchor\" href=\"#docker---singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker -\u0026gt; Singularity\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/singularityhub/docker2singularity\"\u003e\u003ccode\u003edocker2singularity\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-retrieving-a-published-singularity-image\" class=\"anchor\" href=\"#retrieving-a-published-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRetrieving a published Singularity image\u003c/h2\u003e\n\u003cp\u003eInstead of building from scratch, we\u0027ll focus on a shortcut that simply wraps docker images published to DockerHub.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull uazhlt-pytorch-example.sif docker://uazhlt/pytorch-example:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-hpc\" class=\"anchor\" href=\"#hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPC\u003c/h1\u003e\n\u003cp\u003eIf you intend to test out \u003ca href=\"./example\"\u003ethe PyTorch example included here\u003c/a\u003e, you\u0027ll want to clone this repository:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/ua-hlt-program/pytorch-example.git\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-singularity-in-an-interactive-pbs-job\" class=\"anchor\" href=\"#running-singularity-in-an-interactive-pbs-job\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Singularity in an interactive PBS job\u003c/h2\u003e\n\u003cp\u003eNext, we\u0027ll request an interactive job (tested on El Gato):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eqsub -I \\\n-N interactive-gpu \\\n-W group_list=mygroupnamehere \\\n-q standard \\\n-l select=1:ncpus=2:mem=16gb:ngpus=1 \\\n-l cput=3:0:0 \\\n-l walltime=1:0:0\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e_NOTE: If you\u0027re unfamiliar with \u003ccode\u003eqsub\u003c/code\u003e and the many options in the command above seem puzzling, you can find answers by checking out the manual via \u003ccode\u003eman qsub\u003c/code\u003e _\u003c/p\u003e\n\u003cp\u003eIf the cluster isn\u0027t too busy, you should soon see a new prompt formatted something like \u003ccode\u003e[netid@gpu\\d\\d ~]\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eNow we\u0027ll run the singularity image we grabbed earlier.  Before that, though, let\u0027s ensure we\u0027re using the correct version of Singularity and that the correct CUDA version is available to Singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load singularity/3.2.1\nmodule load cuda10/10.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we\u0027re finally ready to run the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell --nv --no-home /path/to/your/uazhlt-pytorch-example.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you ran into an error, check to see if you replaced \u003ccode\u003e/path/to/your/\u003c/code\u003e with the correct path to \u003ccode\u003euazhlt-pytorch-example.sif\u003c/code\u003e before executing the command.\u003c/p\u003e\n\u003cp\u003eWe\u0027re now in our Singularity container! If everything went well, we should be able to see the gpu:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003envidia-smi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should see output like the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K20Xm         On   | 00000000:8B:00.0 Off |                    0 |\n| N/A   17C    P8    18W / 235W |      0MiB /  5700MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSuccess (I hope)!  Now let\u0027s try running PyTorch on the GPU with batching...\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pytorch-example\" class=\"anchor\" href=\"#pytorch-example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePyTorch example\u003c/h1\u003e\n\u003cp\u003eThe Pytorch example code can be found under \u003ca href=\"./example\"\u003e\u003ccode\u003eexample\u003c/code\u003e\u003c/a\u003e.  The data used in this example comes from from Delip Rao and Brian MacMahan\u0027s \u003cem\u003eNatural Language Processing with PyTorch\u003c/em\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames\"\u003ehttps://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dataset relates surnames to nationalities.  Our version (minor modifications) is nested under \u003ca href=\"./examples/data\"\u003eexamples/data\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003etrain.py\u003c/code\u003e houses a command line program for training a classifier.  The following invocation will display the tool\u0027s help text:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython train.py --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe simple model architecture operates is based on that of deep averaging networks (DANs; see \u003ca href=\"https://aclweb.org/anthology/P15-1162/\" rel=\"nofollow\"\u003ehttps://aclweb.org/anthology/P15-1162/\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eReading through train.py you can quickly see how the code is organized.  Some parts (ex. \u003ccode\u003etorchtext\u003c/code\u003e data loaders) may be unfamiliar to you.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-next-steps\" class=\"anchor\" href=\"#next-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNext steps\u003c/h1\u003e\n\u003cp\u003eNow that you\u0027ve managed to run some example PyTorch code, there are many paths forward:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExperiment with using pretrained subword embeddings (both fixed and trainable).  Do you notice any improvements in performance/faster convergence?\u003c/li\u003e\n\u003cli\u003eTry improving or replacing the naive model defined under \u003ccode\u003emodels.py\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eAdd an evaluation script for a trained model that reports macro P, R, and F1.  Feel free to use \u003ccode\u003escikit-learn\u003c/code\u003e\u0027s classification report.\u003c/li\u003e\n\u003cli\u003eAdd an inference script to classify new examples.\u003c/li\u003e\n\u003cli\u003eMonitor validation loss to and stop training if you begin to overfit.\u003c/li\u003e\n\u003cli\u003eAdapt the interactive PBS task outlined above to a PBS script that you can submit to the HPC.\u003c/li\u003e\n\u003cli\u003eAddress the class imbalance in the data through downsampling, class weighting, or another technique of your choosing.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1576591065.0
  },
  {
    "data_format": 2,
    "description": "sequana pipeline to perform parallel fastqc and summarize results with multiqc plot",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "sequana/fastqc",
    "latest_release": "v1.2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-docker-container\" class=\"anchor\" href=\"#building-the-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the docker container\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eNOTE: This step is not necessary if you simply want to use an already published image to run the example code on the UA HPC.\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker build -f Dockerfile -t uazhlt/pytorch-example .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-verify-pytorch-version\" class=\"anchor\" href=\"#verify-pytorch-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVerify PyTorch version\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003edocker run --rm -it uazhlt/pytorch-example python -c \"import torch; print(torch.__version__)\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publish-to-dockerhub\" class=\"anchor\" href=\"#publish-to-dockerhub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePublish to DockerHub\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eNOTE: This step is not necessary if you simply want to use an already published image to run the example code on the UA HPC.\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# login to dockerhub registry\ndocker login --username=yourdockerhubusername --email=youremail@domain.com\n\ndocker push org/image-name:taghere\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-a-singularity-image\" class=\"anchor\" href=\"#building-a-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding a Singularity image\u003c/h2\u003e\n\u003cp\u003eBuilding a Singularity image from a def file requires sudo on a Linux system.  In this tutorial, we avoid discussing details on installing Singularity.  If you\u0027re feeling adventurous, take a look at \u003ca href=\"./Singularity\"\u003ethe example def file in this repository\u003c/a\u003e and the official documentation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.0/user-guide/installation.html\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.0/user-guide/installation.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-alternatives\" class=\"anchor\" href=\"#alternatives\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlternatives\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-cloud-builds\" class=\"anchor\" href=\"#cloud-builds\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCloud builds\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eGitHub actions:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/singularityhub/github-ci/blob/master/.github/workflows/go.yml\"\u003eExample GitHub Workflow\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://help.github.com/en/actions/automating-your-workflow-with-github-actions/virtual-environments-for-github-hosted-runners#supported-runners-and-hardware-resources\"\u003eGitHub-hosted runners\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-vms\" class=\"anchor\" href=\"#vms\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVMs\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.0/user-guide/installation.html#singularity-vagrant-box\" rel=\"nofollow\"\u003eVagrant box\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-docker---singularity\" class=\"anchor\" href=\"#docker---singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker -\u0026gt; Singularity\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/singularityhub/docker2singularity\"\u003e\u003ccode\u003edocker2singularity\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-retrieving-a-published-singularity-image\" class=\"anchor\" href=\"#retrieving-a-published-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRetrieving a published Singularity image\u003c/h2\u003e\n\u003cp\u003eInstead of building from scratch, we\u0027ll focus on a shortcut that simply wraps docker images published to DockerHub.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull uazhlt-pytorch-example.sif docker://uazhlt/pytorch-example:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-hpc\" class=\"anchor\" href=\"#hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPC\u003c/h1\u003e\n\u003cp\u003eIf you intend to test out \u003ca href=\"./example\"\u003ethe PyTorch example included here\u003c/a\u003e, you\u0027ll want to clone this repository:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/ua-hlt-program/pytorch-example.git\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-singularity-in-an-interactive-pbs-job\" class=\"anchor\" href=\"#running-singularity-in-an-interactive-pbs-job\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Singularity in an interactive PBS job\u003c/h2\u003e\n\u003cp\u003eNext, we\u0027ll request an interactive job (tested on El Gato):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eqsub -I \\\n-N interactive-gpu \\\n-W group_list=mygroupnamehere \\\n-q standard \\\n-l select=1:ncpus=2:mem=16gb:ngpus=1 \\\n-l cput=3:0:0 \\\n-l walltime=1:0:0\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e_NOTE: If you\u0027re unfamiliar with \u003ccode\u003eqsub\u003c/code\u003e and the many options in the command above seem puzzling, you can find answers by checking out the manual via \u003ccode\u003eman qsub\u003c/code\u003e _\u003c/p\u003e\n\u003cp\u003eIf the cluster isn\u0027t too busy, you should soon see a new prompt formatted something like \u003ccode\u003e[netid@gpu\\d\\d ~]\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eNow we\u0027ll run the singularity image we grabbed earlier.  Before that, though, let\u0027s ensure we\u0027re using the correct version of Singularity and that the correct CUDA version is available to Singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load singularity/3.2.1\nmodule load cuda10/10.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we\u0027re finally ready to run the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell --nv --no-home /path/to/your/uazhlt-pytorch-example.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you ran into an error, check to see if you replaced \u003ccode\u003e/path/to/your/\u003c/code\u003e with the correct path to \u003ccode\u003euazhlt-pytorch-example.sif\u003c/code\u003e before executing the command.\u003c/p\u003e\n\u003cp\u003eWe\u0027re now in our Singularity container! If everything went well, we should be able to see the gpu:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003envidia-smi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should see output like the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K20Xm         On   | 00000000:8B:00.0 Off |                    0 |\n| N/A   17C    P8    18W / 235W |      0MiB /  5700MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSuccess (I hope)!  Now let\u0027s try running PyTorch on the GPU with batching...\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pytorch-example\" class=\"anchor\" href=\"#pytorch-example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePyTorch example\u003c/h1\u003e\n\u003cp\u003eThe Pytorch example code can be found under \u003ca href=\"./example\"\u003e\u003ccode\u003eexample\u003c/code\u003e\u003c/a\u003e.  The data used in this example comes from from Delip Rao and Brian MacMahan\u0027s \u003cem\u003eNatural Language Processing with PyTorch\u003c/em\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames\"\u003ehttps://github.com/joosthub/PyTorchNLPBook/tree/master/data#surnames\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe dataset relates surnames to nationalities.  Our version (minor modifications) is nested under \u003ca href=\"./examples/data\"\u003eexamples/data\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003etrain.py\u003c/code\u003e houses a command line program for training a classifier.  The following invocation will display the tool\u0027s help text:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython train.py --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe simple model architecture operates is based on that of deep averaging networks (DANs; see \u003ca href=\"https://aclweb.org/anthology/P15-1162/\" rel=\"nofollow\"\u003ehttps://aclweb.org/anthology/P15-1162/\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eReading through train.py you can quickly see how the code is organized.  Some parts (ex. \u003ccode\u003etorchtext\u003c/code\u003e data loaders) may be unfamiliar to you.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-next-steps\" class=\"anchor\" href=\"#next-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNext steps\u003c/h1\u003e\n\u003cp\u003eNow that you\u0027ve managed to run some example PyTorch code, there are many paths forward:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExperiment with using pretrained subword embeddings (both fixed and trainable).  Do you notice any improvements in performance/faster convergence?\u003c/li\u003e\n\u003cli\u003eTry improving or replacing the naive model defined under \u003ccode\u003emodels.py\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eAdd an evaluation script for a trained model that reports macro P, R, and F1.  Feel free to use \u003ccode\u003escikit-learn\u003c/code\u003e\u0027s classification report.\u003c/li\u003e\n\u003cli\u003eAdd an inference script to classify new examples.\u003c/li\u003e\n\u003cli\u003eMonitor validation loss to and stop training if you begin to overfit.\u003c/li\u003e\n\u003cli\u003eAdapt the interactive PBS task outlined above to a PBS script that you can submit to the HPC.\u003c/li\u003e\n\u003cli\u003eAddress the class imbalance in the data through downsampling, class weighting, or another technique of your choosing.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [
      "fastqc",
      "ngs",
      "snakemake",
      "sequana"
    ],
    "updated_at": 1620673100.0
  },
  {
    "data_format": 2,
    "description": "Sample workflow to demonstrate how Pegasus can be used to  manage the population modeling tools in the MINT project",
    "filenames": [
      "Singularity"
    ],
    "full_name": "pegasus-isi/darpa_population_modeling",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-darpa_population_modeling\" class=\"anchor\" href=\"#darpa_population_modeling\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edarpa_population_modeling\u003c/h1\u003e\n\u003cp\u003eThis is a sample workflow to demonstrate how Pegasus can be used to\nmanage the population modeling tools in the MINT project.\u003c/p\u003e\n\u003cp\u003eThe tools have been converted to command line tools.\u003c/p\u003e\n\u003cp\u003eA Singularity image, based on Ubuntu Xenial Xerus and with Python3 and\nGIS tools and libraries, are used for the compute environment for the\njobs.\u003c/p\u003e\n\u003cp\u003eThe workflow is currently set up to run on the ISI testbed, but can\nbe moved to more powerful execution environments if needed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-submitting\" class=\"anchor\" href=\"#submitting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubmitting\u003c/h2\u003e\n\u003cp\u003eCheck out this repository on \u003ccode\u003eworkflow.isi.edu\u003c/code\u003e and run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./workflow/submit.sh\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1558066303.0
  },
  {
    "data_format": 2,
    "description": "A Snakemake pipeline for RRBS data analysis",
    "filenames": [
      "singularity/Singularity.base",
      "singularity/Singularity.methylkit"
    ],
    "full_name": "ftabaro/MethylSnake",
    "latest_release": "1.1.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://hackmd.io/cksQRWI5SKOrVTogW4DzMQ\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8a507adfaf617171f3313cd95921c8d879cc5847e1a2bc0b0e4a114850c3f92b/68747470733a2f2f6861636b6d642e696f2f636b735152574935534b4f7256546f675734447a4d512f6261646765\" alt=\"hackmd-github-sync-badge\" data-canonical-src=\"https://hackmd.io/cksQRWI5SKOrVTogW4DzMQ/badge\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://zenodo.org/badge/latestdoi/216578481\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/39f91651318725769151530a0bccc5499b25df00549bb2d18aac95a564760d08/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3231363537383438312e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/216578481.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-methylsnake\" class=\"anchor\" href=\"#methylsnake\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMethylSnake\u003c/h1\u003e\n\u003cp\u003eA Snakemake pipeline for RRBS data analysis\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis pipeline implements a DNA methylation data analysis workflow. Currently, it is taylored toward RRBS data analysis but it can be easity tweaked to support other data types.\u003c/p\u003e\n\u003cp\u003eStarting from raw reads in FASTQ format, it runs basic QC reporting, reads trimming, alignment, methylation extraction, filtering of incomplete conversions and differential methylation calls both at single base resolution and fixed-size tiles. Moreover, it runs basic downstream  genomic annotation.\u003c/p\u003e\n\u003cp\u003eThe final output of the pipeline are a set of files for alignment, detected features, annotation and reports in a variety of formats (bam, bed, pdf, html). R objects are also returned for retrospective inspection and further downstream analysis.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"rulegraph.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"rulegraph.png\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003ePrepare genome indexes for the \u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/Bismark_User_Guide.pdf\" rel=\"nofollow\"\u003eBismark aligner\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#how-to-write-the-sample-sheet\"\u003eWrite a sample sheet\u003c/a\u003e with per-sample specifications\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#how-to-generate-the-config-file\"\u003eGenerate config file\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#how-to-run-the-pipeline\"\u003eStart the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tools\" class=\"anchor\" href=\"#tools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTools\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eQC: \u003cstrong\u003eFastQC\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eTrimming: \u003cstrong\u003eTrim_galore\u003c/strong\u003e (Cutadapt wrapper)\u003c/li\u003e\n\u003cli\u003eAlignment + methlyation extraction: \u003cstrong\u003eBismark\u003c/strong\u003e (using Bowtie2)\u003c/li\u003e\n\u003cli\u003eDifferential methylation detection: \u003cstrong\u003eMethylKit\u003c/strong\u003e (Bioconductor package)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003esnakemake\u003c/code\u003e 5.20.1: \u003ccode\u003epip install --user snakemake\u003c/code\u003e or via Conda\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003epyaml\u003c/code\u003e 20.4 (PyYAML 5.3.1): \u003ccode\u003epip install --user pyaml\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esingularity\u003c/code\u003e 3.6.1 has to be installed system wide, ask your admin\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eslurm\u003c/code\u003e 18.08.8 has to be installed system wide, ask your admin\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-write-the-sample-sheet\" class=\"anchor\" href=\"#how-to-write-the-sample-sheet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to write the sample sheet\u003c/h2\u003e\n\u003cp\u003eThe sample sheet has to be saved in csv format, quotes are not mandatory but \u003cstrong\u003efield separator has to a semicolon and headers are mandatory\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe table is composed of two columns:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003esample_name\u003c/code\u003e: this field should match the input file name(s), e.g. if reads for a sample are stored in \u003ccode\u003esample1.fq.gz\u003c/code\u003e, the \u003ccode\u003esample_name\u003c/code\u003e column value should be \u003ccode\u003esample1\u003c/code\u003e. In case of paired end reads, \u003ccode\u003esample1_R1.fq.gz\u003c/code\u003e and \u003ccode\u003esample1_R2.fq.gz\u003c/code\u003e the value of this column should be \u003ccode\u003esample1\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etreatment\u003c/code\u003e: a numerical value representing the treatment. The value 0 (zero) always represent control samples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn example with two treatments, control and triplicate replicates per treatment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esample_name;treatment\nsample1;0\nsample2;0\nsample3;0\nsample4;1\nsample5;1\nsample6;1\nsample7;2\nsample8;2\nsample9;2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this setup, samples 1, 2 and 3 are control; samples 4, 5, 6 correspond to treatment 1 and samples 7, 8, 9 to treatment 2.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-generate-the-config-file\" class=\"anchor\" href=\"#how-to-generate-the-config-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to generate the config file\u003c/h2\u003e\n\u003cp\u003eThe Snakemake config file holds all tunable parameters of the pipeline. Check \u003ca href=\"#complete-list-of-config-parameters\"\u003ethis section\u003c/a\u003e for a complete list and relative explanation. For simplicity, all paths should be absolute.\u003c/p\u003e\n\u003cp\u003eA wrapper script with an example invocation can be found in \u003ccode\u003erun_make_config.sh\u003c/code\u003e, It can be tweaked and run to generate a valid config file.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-run-the-pipeline\" class=\"anchor\" href=\"#how-to-run-the-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to run the pipeline\u003c/h2\u003e\n\u003cp\u003eThe pipeline runs inside a Singularity container. This container should contain all the necessary tools to run the analysis. See \u003ca href=\"#singularity-container\"\u003ebelow\u003c/a\u003e for additional details.\u003c/p\u003e\n\u003cp\u003eThe pipeline can be started with the wrapper script \u003ccode\u003erun_snakemake.sh\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe scripts parses the config YAML file searching for four folder to bind mount in the Singularity container: working directory, the genome directory, the genome index directory, the directory holding annotations and the temporary directory. All these paths can be specified when running the config script.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-slurm-configuration\" class=\"anchor\" href=\"#slurm-configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSlurm configuration\u003c/h4\u003e\n\u003cp\u003eA Slurm configuration is given in \u003ccode\u003ecluster-config/cluster.json\u003c/code\u003e. In the json files, keys correspont to Snakemake rule names. Values are in turn objects with keys corresponding to Slurm parameters and values corresponding to desired values for the parameter:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e...\n\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etrim\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e:\n    {\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etime\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e      : \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e6:00:00\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emem\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e       : \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e8G\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    },\n...\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe example above assigns maximum runtime of six hours and 8GB or memory to the \u003ccode\u003etrim\u003c/code\u003e rule. The \"__default__\" key provides default parameters applied to all rules:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e...\n\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e__default__\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e:\n    {\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ejobname\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e   : \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e{rule}\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003elog\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e       : \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eslurm-%x-%j.log\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003entasks\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e    : \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ecpus\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e      : \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emem\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e       : \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e4G\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etime\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e      : \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e00-00:20:00\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n        \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epartition\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e : \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enormal,parallel\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    }\n...\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBy default, each job is spawned with the rule name and logs to a file named after the rule and the jobid generated by Slurm. Each job is assigned 1 task, 1 CPU and 4GB of RAM. Maximum runtime is 20 minutes. Allowed partitions are normal and parallel.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container\u003c/h4\u003e\n\u003cp\u003eThe pipeline is designed to run within a Singularity container. The path to the container can be configured with the config script.\u003c/p\u003e\n\u003cp\u003eA working container can be pulled from SingularityHub:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name methylsnake.sif library://ftabaro/default/methylsnake \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheckout the \u003ca href=\"https://github.com/ftabaro/MethylSnake/tree/master/singularity\"\u003e\u003ccode\u003esingularity\u003c/code\u003e\u003c/a\u003e folder for more info.\u003c/p\u003e\n\u003cp\u003eThe container path has be specified in the command line arguments of the config script.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output-description\" class=\"anchor\" href=\"#output-description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput description\u003c/h2\u003e\n\u003cp\u003eThis pipeline generates a number of output files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/good_sequence_short_fastqc.html\" rel=\"nofollow\"\u003eFastQC reports\u003c/a\u003e for input and trimmed reads\u003c/li\u003e\n\u003cli\u003eFastq files for trimmed reads files with reports (Fastqc and text)\u003c/li\u003e\n\u003cli\u003eAlignment files with report and nucleotide statistics\u003c/li\u003e\n\u003cli\u003eAlignment files with incomplete conversions removed\u003c/li\u003e\n\u003cli\u003eMethylation files in GpG, CHG and CHH contexts\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.bioinformatics.babraham.ac.uk/projects/bismark/PE_report.html\" rel=\"nofollow\"\u003eBismark HTML reports\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/bismark_summary_report.html\" rel=\"nofollow\"\u003eBismark HTML summary\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003erds files with every object computed by \u003ca href=\"https://bioconductor.org/packages/release/bioc/vignettes/methylKit/inst/doc/methylKit.html\" rel=\"nofollow\"\u003eMethylKit\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ebed files of differential methylation calls\u003c/li\u003e\n\u003cli\u003eFigures\n\u003cul\u003e\n\u003cli\u003eprelimimary coverage and methylation state histograms\u003c/li\u003e\n\u003cli\u003esamples PCA and correlations\u003c/li\u003e\n\u003cli\u003enumber of differentially methylated features per chromosome (DMC and DMR)\u003c/li\u003e\n\u003cli\u003epie charts of annotation classes\n\u003cul\u003e\n\u003cli\u003eall differentially methylated features\u003c/li\u003e\n\u003cli\u003ehyper methylated\u003c/li\u003e\n\u003cli\u003ehypo methylated\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTables\n\u003cul\u003e\n\u003cli\u003edistance to TSS for all, hyper and hypo methylated features (csv)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ouput-folder-hierarchy\" class=\"anchor\" href=\"#ouput-folder-hierarchy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOuput folder hierarchy\u003c/h3\u003e\n\u003cp\u003eAn example of output directory (all directory names can be changed from the config file)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e.\n\u251c\u2500\u2500 alignments\n\u251c\u2500\u2500 bed\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dmc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dmr\n\u251c\u2500\u2500 fastqc\n\u251c\u2500\u2500 log\n\u251c\u2500\u2500 pictures\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dmc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dmr\n\u251c\u2500\u2500 reads\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fastqc\n\u251c\u2500\u2500 RData\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dmc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dmr\n\u251c\u2500\u2500 reports\n\u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dmc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dmr\n\u2514\u2500\u2500 trimmed\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-useful-links\" class=\"anchor\" href=\"#useful-links\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful links\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.nature.com/articles/nmeth.1828\" rel=\"nofollow\"\u003ehttps://www.nature.com/articles/nmeth.1828\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.epigenesys.eu/images/stories/protocols/pdf/20120720103700_p57.pdf\" rel=\"nofollow\"\u003ehttps://www.epigenesys.eu/images/stories/protocols/pdf/20120720103700_p57.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"nofollow\"\u003ehttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\" rel=\"nofollow\"\u003ehttps://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md\"\u003ehttps://github.com/FelixKrueger/TrimGalore/blob/master/Docs/Trim_Galore_User_Guide.md\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/\" rel=\"nofollow\"\u003ehttps://www.bioinformatics.babraham.ac.uk/projects/bismark/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/bismark/Bismark_User_Guide.pdf\" rel=\"nofollow\"\u003ehttps://www.bioinformatics.babraham.ac.uk/projects/bismark/Bismark_User_Guide.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/FelixKrueger/Bismark/tree/master/Docs\"\u003ehttps://github.com/FelixKrueger/Bismark/tree/master/Docs\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eBug reports and pull requests are welcome on GitHub at \u003ca href=\"https://github.com/ftabaro/rrbs-pipeline/issues\"\u003ehttps://github.com/ftabaro/rrbs-pipeline/issues\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-complete-list-of-config-parameters\" class=\"anchor\" href=\"#complete-list-of-config-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eComplete list of config parameters\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e  --config-path CONFIG_PATH\n                        Path for the config file\n                        \n  --wd WD               Working directory. Root directory of the project\n  \n  --genome-path GENOME_PATH\n                        Path to a fasta file with genome sequence\n                        \n  --bismark-index-path BISMARK_INDEX_PATH\n                        Path to Bismark index files (only index name required:\n                        /path/to/index/folder/hg38*)\n                        \n  --sample-sheet SAMPLE_SHEET\n                        Path to csv file holding samples information\n                        \n  --annotation-file ANNOTATION_FILE\n                        Path to a gzipped GTF file.\n                        \n  --dmr-window-size DMR_WINDOW_SIZE\n                        Window size for tiled differential methylation\n                        analysis\n                        \n  --dmr-step-size DMR_STEP_SIZE\n                        Step size for tiled differential methylation analysis\n                        \n  --dmr-difference DMR_DIFFERENCE [DMR_DIFFERENCE ...]\n                        Difference in reads coverage threshold for\n                        differential methylation analysis\n                        \n  --dmr-qvalue DMR_QVALUE [DMR_QVALUE ...]\n                        Q-value threshold for differential methylation\n                        analysis\n                        \n  --min-per-group MIN_PER_GROUP\n                        An integer denoting minimum number of samples per\n                        replicate needed to cover a region/base\n                        \n  --mate1-pattern MATE1_PATTERN\n                        Pattern to identify mate 1 in paired sequencing files e.g. R1 or _R1 or _1\n                        \n  --mate2-pattern MATE2_PATTERN\n                        Pattern to identify mate 2 in paired sequencing files e.g. R2 or _R2 or _2\n                        \n  --fastq-extension FASTQ_EXTENSION\n                        File extension of reads fastq files e.g. fastq.gz or fq.gz or fastq\n                        \n  --genome-version GENOME_VERSION\n                        Label for genome version used in the analysis e.g. hg38 or hg19\n                        \n  --singularity-container SINGULARITY_CONTAINER\n                        Path to a singularity container with all necessary\n                        tools installed.\n                        \n  --tmp-folder TMP_FOLDER\n                        Path to a temporary folder (possibly fast storage) e.g. /scratch\n                        \n  --log-folder LOG_FOLDER\n                        Path to a log folder\n                        \n  --reads-folder READS_FOLDER\n                        Path to a folder with reads to be processed\n                        \n  --trimmed-folder TRIMMED_FOLDER\n                        Path to folder to write trimmed reads in\n                        \n  --alignments-folder ALIGNMENTS_FOLDER\n                        Path to a folder to write alignments in\n                        \n  --reports-folder REPORTS_FOLDER\n                        Path to a folder to write Bismark reports in\n                        \n  --nucleotide-stats-folder NUCLEOTIDE_STATS_FOLDER\n                        Path to write Bismark nucleotide report files in\n                        \n  --methylkitdb-folder METHYLKITDB_FOLDER\n                        Path to write methylKit tabix files in\n                        \n  --rdata-folder RDATA_FOLDER\n                        Path to write RDS objects used in methylKit analysis\n                        \n  --bed-folder BED_FOLDER\n                        Path to write BED files for DMR/DMC coordinates to\n                        \n  --pictures-folder PICTURES_FOLDER\n                        Path to a folder to write plots in\n                        \n  --tables-folder TABLES_FOLDER\n                        Path to a folder to write tables in\n                        \n  --fastqc-folder FASTQC_FOLDER\n                        FastQC results folder\n                        \n  --low-coverage-count LOW_COVERAGE_COUNT\n                        Number of reads for low coverage filtering\n                        \n  --high-coverage-percentage HIGH_COVERAGE_PERCENTAGE\n                        Percentage of the coverage distribution for high\n                        coverage filtering (PCR duplicates)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-tags-snakemake-dna-methylation-rrbs\" class=\"anchor\" href=\"#tags-snakemake-dna-methylation-rrbs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etags: \u003ccode\u003esnakemake\u003c/code\u003e \u003ccode\u003eDNA-methylation\u003c/code\u003e \u003ccode\u003errbs\u003c/code\u003e\n\u003c/h6\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [
      "bioinformatics-pipeline",
      "methylation",
      "rrbs-pipeline",
      "snakemake",
      "rrbs-data-analysis"
    ],
    "updated_at": 1613470322.0
  },
  {
    "data_format": 2,
    "description": "Singularity Container for running Sol",
    "filenames": [
      "Singularity"
    ],
    "full_name": "tyson-swetnam/osgeo-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/567\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-container-for-running-osgeo-software\" class=\"anchor\" href=\"#singularity-container-for-running-osgeo-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container for running OSGEO software\u003c/h1\u003e\n\u003cp\u003eSingularity Container for running OSGEO (GRASS, GDAL, QGIS, SAGA-GIS) on a virtual machine or localhost running Singularity.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container\" class=\"anchor\" href=\"#installing-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container\u003c/h2\u003e\n\u003cp\u003eFirst, \u003ca href=\"https://singularity.lbl.gov/install-linux\" rel=\"nofollow\"\u003einstall Singularity\u003c/a\u003e on your localhost or remote system.\u003c/p\u003e\n\u003cp\u003eAs of early August 2018, Singularity is version \u003ccode\u003e2.6.0\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eVERSION=2.6.0\nwget https://github.com/singularityware/singularity/releases/download/$VERSION/singularity-$VERSION.tar.gz\ntar xvf singularity-$VERSION.tar.gz\ncd singularity-$VERSION\n./configure --prefix=/usr/local\nmake\nsudo make install\ncd ..\nsudo rm -rf singularity-$VERSION.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pull-the-container-from-singularity-hub\" class=\"anchor\" href=\"#pull-the-container-from-singularity-hub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePull the Container from Singularity Hub\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003elatest\u003c/code\u003e image is hosted on \u003ca href=\"https://www.singularity-hub.org/collections/567\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name osgeo.simg shub://tyson-swetnam/osgeo-singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-locally\" class=\"anchor\" href=\"#build-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild locally\u003c/h2\u003e\n\u003cp\u003eTo build locally, pull this repository:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/tyson-swetnam/osgeo-singularity\ncd osgeo-singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBuild the container locally:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build osgeo.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE: The Singularity file has some options in the \u003ccode\u003e%post\u003c/code\u003e section for installing NVIDIA drivers and OpenGL - these are currently commented out in the Singularity-Hub build.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-starting-the-container\" class=\"anchor\" href=\"#starting-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStarting the Container\u003c/h2\u003e\n\u003cp\u003eTo run the container as a shell:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell osgeo.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run the container with a GUI interface for GRASS:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec osgeo.simg grass74\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor QGIS GUI:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec osgeo.simg qgis\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor Saga-GIS GUI:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec osgeo.simg saga_gui\n\u003c/code\u003e\u003c/pre\u003e\n \n\u003cp\u003eIf you are accessing the container remotely, make sure to use the \u003ccode\u003essh -X\u003c/code\u003e flag\u003c/p\u003e\n \n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1552184925.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.discordance",
      "Singularity.malt",
      "Singularity.gappa",
      "Singularity.rp15",
      "Singularity.PhyloBayesMPI",
      "Singularity.PhyloBayes",
      "Singularity.megan6-ce",
      "Singularity.checkM",
      "Singularity.iqtree.1.6.1",
      "Singularity.papara",
      "Singularity.megan5",
      "Singularity.epa-ng",
      "Singularity.ale",
      "Singularity.hifix",
      "COME2018/Singularity.fsa",
      "COME2018/Singularity.muscle",
      "COME2018/Singularity.beast2",
      "COME2018/Singularity.phyml",
      "COME2018/Singularity.gnuplot",
      "COME2018/Singularity.prank",
      "COME2018/Singularity.mummer",
      "COME2018/Singularity.fasttree",
      "COME2018/Singularity.clustalx",
      "COME2018/Singularity.bpp",
      "COME2018/Singularity.seaview",
      "COME2018/Singularity.codonphyml",
      "COME2018/Singularity.mcl",
      "COME2018/Singularity.probcons",
      "COME2018/Singularity.paml",
      "COME2018/Singularity.jmodeltest2",
      "COME2018/Singularity.iqtree.1.6.3",
      "COME2018/Singularity.blast",
      "COME2018/Singularity.mafft",
      "COME2018/Singularity.raxml-ng",
      "COME2018/Singularity.swipe",
      "COME2018/Singularity.clustalw",
      "COME2018/Singularity.bali-phy",
      "COME2018/Singularity.amap-align",
      "COME2018/Singularity.modeltest-ng",
      "COME2018/Singularity.standard-raxml"
    ],
    "full_name": "maxemil/singularity-container",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Container\u003c/h2\u003e\n\u003cp\u003eTo use the containers in this repository, install the latest Singularity app (taken from \u003ca href=\"http://singularity.lbl.gov/install-linux\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/install-linux\u003c/a\u003e):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eVERSION=2.4.5\nwget https://github.com/singularityware/singularity/releases/download/\u003cspan class=\"pl-smi\"\u003e$VERSION\u003c/span\u003e/singularity-\u003cspan class=\"pl-smi\"\u003e$VERSION\u003c/span\u003e.tar.gz\ntar xvf singularity-\u003cspan class=\"pl-smi\"\u003e$VERSION\u003c/span\u003e.tar.gz\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e singularity-\u003cspan class=\"pl-smi\"\u003e$VERSION\u003c/span\u003e\n./configure --prefix=/usr/local\nmake\nsudo make install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen you can build and use the containers using the following commands:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.simg Singularity.\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e -b \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.simg \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ecommand\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003earguments\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or if only a single command is available:\u003c/span\u003e\nsingularity run -b \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.simg \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003earguments\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1613209350.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "docker/curatedmetagenomics/Singularity",
      "docker/sratoolkit/Singularity"
    ],
    "full_name": "waldronlab/curatedmetagenomics",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-project-management\" class=\"anchor\" href=\"#project-management\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eProject management\u003c/h1\u003e\n\u003cp\u003eSee \u003ca href=\"https://app.zenhub.com/workspaces/cmd-project-management-5e3d745411e3ced1cfa8fbe9/board?repos=116720695,58228080,95220777,250843441\" rel=\"nofollow\"\u003ezenhub project management\u003c/a\u003e across this and related repos.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pipeline-project-overview\" class=\"anchor\" href=\"#pipeline-project-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline project overview\u003c/h1\u003e\n\u003cp\u003eHere is a high-level \u003ca href=\"https://www.dropbox.com/s/tawgf4l49190m4o/2020-05-20%20intro%20to%20NCI%201U01%20CA230551%20.pptx?dl=0\" rel=\"nofollow\"\u003eslide deck\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href=\"https://github.com/waldronlab/curatedmetagenomics/wiki/Environment-variables-and-invocation\"\u003ewiki\u003c/a\u003e for full setup and execution instructions.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-what-is-here\" class=\"anchor\" href=\"#what-is-here\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat is here\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-metaphlan3--humann3--strainphlan--sratoolkit-dockersingularity-images\" class=\"anchor\" href=\"#metaphlan3--humann3--strainphlan--sratoolkit-dockersingularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMetaPhlan3 + HUMAnN3 + StrainPhlAn + sratoolkit Docker+Singularity images\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eDockerHub\u003c/th\u003e\n\u003cth\u003eSingularityHub\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://hub.docker.com/repository/docker/waldronlab/curatedmetagenomics\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/acf7b0916598fbca21cbe3bb57b510560a69a4bd2a8446b29408b42807a1e1c6/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f77616c64726f6e6c61622f637572617465646d65746167656e6f6d6963732e737667\" alt=\"\" data-canonical-src=\"https://images.microbadger.com/badges/version/waldronlab/curatedmetagenomics.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://singularity-hub.org/collections/4365\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e(see \u003ca href=\"https://github.com/waldronlab/curatedmetagenomics/tree/master/docker/curatedMetagenomics\"\u003edocker directory\u003c/a\u003e for Dockerfile and link to Dockerhub).\n- Instructions to run are kept in a \u003ca href=\"https://github.com/waldronlab/curatedmetagenomics/wiki/Environment-variables-and-invocation\"\u003ewiki\u003c/a\u003e.\n- There is still work to be done turning bash scripts into a Python package with improved documentation, arguments, and versioning.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sratoolkit-only-dockersingularity-images\" class=\"anchor\" href=\"#sratoolkit-only-dockersingularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esratoolkit-only Docker+Singularity images\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eDockerHub\u003c/th\u003e\n\u003cth\u003eSingularityHub\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://hub.docker.com/repository/docker/waldronlab/sratoolkit\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fff4c11c8d2a193b6723a39156009275f1a05d2daefdee785cd6ea8691c0a58f/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f77616c64726f6e6c61622f737261746f6f6c6b69742e737667\" alt=\"\" data-canonical-src=\"https://images.microbadger.com/badges/version/waldronlab/sratoolkit.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://singularity-hub.org/collections/4458\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e(see \u003ca href=\"https://github.com/waldronlab/curatedmetagenomics/tree/master/docker/sratoolkit\"\u003edocker directory\u003c/a\u003e, but this is also included in the above all-in-one image.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-a-pypi-project-for-the-curatedmetagenomics-pipeline\" class=\"anchor\" href=\"#a-pypi-project-for-the-curatedmetagenomics-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ea PyPi project for the curatedmetagenomics pipeline\u003c/h2\u003e\n\u003cp\u003eSee the \u003ca href=\"https://github.com/waldronlab/curatedmetagenomics/tree/master/python_pipeline\"\u003epython_pipeline\u003c/a\u003e directory\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 11,
    "topics": [],
    "updated_at": 1593557598.0
  },
  {
    "data_format": 2,
    "description": "A repo for my matlab 2017b runtime Singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "shots47s/matlab2017b-Singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-matlab2017b-singularity\" class=\"anchor\" href=\"#matlab2017b-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ematlab2017b-Singularity\u003c/h1\u003e\n\u003cp\u003eA repo for my matlab 2017b runtime Singularity container\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1537648868.0
  },
  {
    "data_format": 2,
    "description": "The simulation-supervised package combines different sets of code needed to train a DNN policy to fly a drone.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "kkelchte/simulation_supervised",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-simulation-supervised\" class=\"anchor\" href=\"#simulation-supervised\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esimulation-supervised\u003c/h1\u003e\n\u003cp\u003eThe simulation-supervised package combines different sets of code needed to train a DNN policy to fly a drone.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.github.com/kkelchte/pilot_online\"\u003eonline_training\u003c/a\u003e: the tensorflow code used for training and running the DNN policy.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.github.com/kkelchte/hector_quadrotor\"\u003edrone_simulator\u003c/a\u003e: a simulated drone model for Gazebo. This is a copy of the original \u003ca href=\"http://wiki.ros.org/hector_quadrotor\" rel=\"nofollow\"\u003ehector quadrotor model\u003c/a\u003e.\nOR\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/kkelchte/bebop_autonomy\"\u003ebebop_autonomy\u003c/a\u003e: a copy of the bebop autonomy package of ROS. This package allows you to test the DNN in the real-world. The copy is only for ensuring stability while performing research. There are no significant modifications so it is probably best to use \u003ca href=\"https://github.com/AutonomyLab/bebop_autonomy\"\u003ethe original\u003c/a\u003e. If you are using the Doshico docker image, it is already installed globally.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eThis package is best build in a separate \u003ca href=\"http://wiki.ros.org/catkin/Tutorials/create_a_workspace\" rel=\"nofollow\"\u003ecatkin workspace\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir -p \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/simsup_ws/src\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/simsup_ws \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e catkin_make\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/simsup_ws/src\ngit clone https://github.com/kkelchte/simulation_supervised\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/simsup_ws \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e catkin_make\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou will have to set the correct path to your tensorflow pilot_online package.\u003c/p\u003e\n\u003cp\u003eIn case you are using different drone models, you will have to adjust the \u003ca href=\"https://github.com/kkelchte/simulation-supervised/blob/master/simulation_supervised/config/sim_drone.yaml\"\u003econfig.yaml\u003c/a\u003e file in order to set the correct rosparams.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-some-experiments\" class=\"anchor\" href=\"#run-some-experiments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun some experiments\u003c/h2\u003e\n\u003cp\u003eHere are some common used setting combinations in order to remember them:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Test online the performance of a heuristic defined in tensorflow/gt_depth_online/../rosinterface.py using for instance recovery cameras flying 3 times through a generated canyon\n$ ./scripts/evaluate_model.sh -m auxd -s start_python_sing_gtdepth.sh -t test_depth_online -r true -w canyon -n 3\n# Train online in canyon, forest, sandbox\n$ ./scripts/train_model.sh -m mobilenet_025\n\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1560186991.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "markmenezes11/COMPM091",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-on-the-importance-of-the-choice-of-downstream-models-for-evaluating-sentence-representations\" class=\"anchor\" href=\"#on-the-importance-of-the-choice-of-downstream-models-for-evaluating-sentence-representations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOn the Importance of the Choice of Downstream Models for Evaluating Sentence Representations\u003c/h1\u003e\n\u003cp\u003eThis repository contains the code created/used for my MEng Computer Science Undergraduate Final Year Individual Projct (COMPM091) at University College London.\u003c/p\u003e\n\u003cp\u003eMy project primarily involved looking at sentence representations, specifically InferSent (Conneau et al., 2017) and CoVe (McCann et al., 2017) and using SentEval (Conneau et al., 2017) and the Biattentive Classification Network (McCann et al., 2017) to evaluate them.\u003c/p\u003e\n\u003cp\u003eThe code for InferSent, SentEval and CoVe were cloned from the following repositories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInferSent: \u003ca href=\"https://github.com/facebookresearch/InferSent\"\u003ehttps://github.com/facebookresearch/InferSent\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSentEval: \u003ca href=\"https://github.com/facebookresearch/SentEval\"\u003ehttps://github.com/facebookresearch/SentEval\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCoVe: \u003ca href=\"https://github.com/salesforce/cove\"\u003ehttps://github.com/salesforce/cove\u003c/a\u003e and \u003ca href=\"https://github.com/rgsachin/CoVe\"\u003ehttps://github.com/rgsachin/CoVe\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the \u003ccode\u003elibs.zip\u003c/code\u003e file, you will find snapshots of these repositories - the versions of them that were used in my project, at this time of writing. If you have problems with any newer versions of these libraries, it is therefore recommended that you use the versions contained in \u003ccode\u003elibs.zip\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote: A CUDA-enabled GPU is required, and at least 16GB RAM is recommended to run most of the scripts. A small minority of scripts will need more than this, hence why they were run on UCL\u0027s HPC cluster.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eThere are two ways to install. You can use the provided Singularity container, which already contains all of the required software, or you can install the requirements locally. It is highly recommended that you use the Singularity container.\u003c/p\u003e\n\u003cp\u003eFor the Singularity method, use steps 1, 2a and 3. For local install, use steps 1, 2b and 3.\u003c/p\u003e\n\u003cp\u003eFor common problems, see the \u003ccode\u003eTroubleshooting\u003c/code\u003e section at the bottom of this page.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1-clone-infersent-and-senteval\" class=\"anchor\" href=\"#step-1-clone-infersent-and-senteval\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1) Clone InferSent and SentEval\u003c/h3\u003e\n\u003cp\u003eYou can either clone them from here:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInferSent: \u003ca href=\"https://github.com/facebookresearch/InferSent\"\u003ehttps://github.com/facebookresearch/InferSent\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSentEval: \u003ca href=\"https://github.com/facebookresearch/SentEval\"\u003ehttps://github.com/facebookresearch/SentEval\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOr, in case any changes are made to these repositories that stop this code from working, snapshots of these repositories used at this time of writing have been provided in the \u003ccode\u003elibs.zip\u003c/code\u003e file, but obviously they may not be the latest version. A modified version of SentEval which uses far less memory can be found in the \u003ccode\u003eSentEval-modified\u003c/code\u003e folder, but only use this if you really have to. Read the README in that folder first. It works identically to the version found in \u003ccode\u003elibs.zip\u003c/code\u003e but contains various performance tweaks. So, instead of cloning directly from the InferSent and SentEval repositories, you can use the versions provided in \u003ccode\u003elibs.zip\u003c/code\u003e or the version of SentEval provided in the \u003ccode\u003eSentEval-modified\u003c/code\u003e folder.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2a-singularity-container-recommended\" class=\"anchor\" href=\"#step-2a-singularity-container-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2a) Singularity Container (Recommended)\u003c/h3\u003e\n\u003cp\u003ePull and run the latest Singularity container for this repository from Singularity Hub.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --nv shub://markmenezes11/COMPM091\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003e--nv\u003c/code\u003e runs it in nvidia mode. See \u003ccode\u003esingularity run -h\u003c/code\u003e for more options. You can also use the same command above to pull any later versions of the container in future, making sure to delete the old \u003ccode\u003e.simg\u003c/code\u003e file first.\u003c/p\u003e\n\u003cp\u003eThe container uses nvidia/cuda:8.0-cudnn6-runtime-ubuntu16.04. Make sure your host machine\u0027s CUDA and cuDNN versions match this*. If they don\u0027t, you might need to tweak the \u003ccode\u003eSingularity\u003c/code\u003e file and re-build the image locally.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2b-local-install\" class=\"anchor\" href=\"#step-2b-local-install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2b) Local Install\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eSingularity\u003c/code\u003e file contains a list of commands in the \u003ccode\u003e%post\u003c/code\u003e section to install the required libraries. You will need to run each of these commands. Python 2.7 was used for the majority of this work, and so you should only need to do it for Python 2.7 (i.e. ignore the \u003ccode\u003epip3\u003c/code\u003e commands) and use Python 2.7 when running the scripts later. You will also need CUDA and cuDNN to make use of your GPU. Depending on where/how you are running this, you might need to run the commands as \u003ccode\u003esudo\u003c/code\u003e or \u003ccode\u003e--user\u003c/code\u003e. You will also need to find/use a different URL for PyTorch and TensorFlow if you are not using Linux and CUDA 8.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-3-download-datasets\" class=\"anchor\" href=\"#step-3-download-datasets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3) Download Datasets\u003c/h3\u003e\n\u003cp\u003eRun all of the required scripts** and curl commands** to download the required datasets. These scripts can be found in the repositories you just cloned (see Step 1), and instructions can be found in their READMEs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-note-for-users-of-ucls-hpc-cluster\" class=\"anchor\" href=\"#note-for-users-of-ucls-hpc-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNote for Users of UCL\u0027s HPC Cluster\u003c/h2\u003e\n\u003cp\u003eIf using UCL\u0027s HPC cluster, it is recommended to follow the Singularity instructions above. Additionally, assuming it hasn\u0027t been moved, some outputs of running some of my code, as well as the Singularity image and the InferSent and SentEval libraries with all of the requirements already downloaded, should be in \u003ccode\u003e/cluster/project2/ishi_storage_1/mmenezes\u003c/code\u003e. If so, you can save time by just using the following Singularity command which will bind my folder to \u003ccode\u003e/mnt/mmenezes\u003c/code\u003e in the Singularity image for you, with all of the requirements pre-installed and pre-downloaded:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --nv --bind /cluster/project2/ishi_storage_1:/mnt /cluster/project2/ishi_storage_1/mmenezes/markmenezes11-COMPM091-master.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen following the \"Run\" instructions later, you can then use the above command instead of \u003ccode\u003esingularity run --nv shub://markmenezes11/COMPM091\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003cp\u003eFor \"Run\" instructions, see the relevant README(s) in the subfolder(s) within this repository. They all require all of the above installation instructions to be competed first. The folders containing the runnable scripts are as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ccode\u003eCoVe-BCN\u003c/code\u003e folder contains my replication of the Biattentive Classification Network model by McCann et al. (2017). It also includes a script for running a parameter sweep on this model, and evaluaitng both InferSent and CoVe on different transfer tasks.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eCoVe-ported\u003c/code\u003e folder contains a script (and some test scripts) for porting the Python 3 version of the Keras port of CoVe into a Python 2 compatible version. It is unlikely that you will need to run any of these scripts.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eInferSent-sweep\u003c/code\u003e folder contains code for performing a parameter sweep on InferSent, including training the InferSent model based on many different parameters, and evaluating the model using SentEval. The sweep script can either use qsub job submissions for running jobs in parallel or it can be run locally.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eSentEval-evals\u003c/code\u003e folder contains scripts and test results from evaluating InferSent (Conneau et al. (2017)), CoVe (McCann et al. (2017)) and GloVe (Pennington et al. (2014)) sentence representations using SentEval for evaluation on various transfer tasks.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eUtils\u003c/code\u003e folder contains helper scripta, for example for displaying RAM usage, for giving summaries based on lots of results files, and for manipulating datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlso have a look at the \u003ccode\u003eqsub_helper\u003c/code\u003e scripts in these folders if you want to use \u003ccode\u003eqsub\u003c/code\u003e to submit HPC jobs. They can mostly be run using \u003ccode\u003eqsub \u0026lt;qsub-params\u0026gt; \u0026lt;qsub_helper-script\u0026gt; \u0026lt;command\u0026gt;\u003c/code\u003e. The  bit can use singularity by running the \u003ccode\u003esingularity exec\u003c/code\u003e command instead of \u003ccode\u003esingularity run\u003c/code\u003e. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eqsub -cwd -o $PWD/se_log_STS12.txt -e $PWD/se_error_STS12.txt ../qsub_helper.sh singularity exec --nv --bind /cluster/project2/ishi_storage_1:/mnt /cluster/project2/ishi_storage_1/mmenezes/markmenezes11-COMPM091-master.simg python eval.py --transfertask STS12\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting--common-problems\" class=\"anchor\" href=\"#troubleshooting--common-problems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting / Common Problems\u003c/h2\u003e\n\u003cp\u003e* If your CUDA and cuDNN versions do not match, you might have to create your own Singularity file, changing the \u003ccode\u003efrom:\u003c/code\u003e line and also the line that installs PyTorch. You can then host it in a similar way, or build it in another way, then pull/run it.\u003c/p\u003e\n\u003cp\u003e**  Keep an eye on the InferSent/SentEval download scripts because curl sometimes gives an SSL error on UCL\u0027s cluster machines. If you can\u0027t update curl on your machine, you could either use the Singularity image (which has a working version of curl) or download the file(s) on a different machine and use \u003ccode\u003escp\u003c/code\u003e to send them across.\u003c/p\u003e\n\u003cp\u003e*** You may have problems when running things later, where Singularity looks at installations in your \u003ccode\u003e/home\u003c/code\u003e folder and these conflict with things that Singularity has installed within the container. You might have to remove some \u003ccode\u003e/home\u003c/code\u003e installs to get it to work.\u003c/p\u003e\n\u003cp\u003e**** Newer PyTorch versions have a few differences in how you call certain functions - if you have some weird PyTorch errors, this is likely the reason. This code has been tested with PyTorch version 0.2.0_3. At this time of writing, the examples in the InferSent, SentEval and CoVe repositories are all out of date and need minor tweaks to get them to work with the latest PyTorch version. See their GitHub issues sections for more details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/1705.02364.pdf\" rel=\"nofollow\"\u003eConneau, Alexis, Kiela, Douwe, Schwenk, Holger, Barrault, Lo\u00efc, and Bordes, Antoine. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670\u2013680. Association for Computational Linguistics, 2017.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/1708.00107.pdf\" rel=\"nofollow\"\u003eMcCann, Bryan, Bradbury, James, Xiong, Caiming, and Socher, Richard. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems 30, pp. 6297\u20136308. Curran Associates, Inc., 2017.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nlp.stanford.edu/pubs/glove.pdf\" rel=\"nofollow\"\u003ePennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532\u20131543, 2014.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eMartin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.\u003c/li\u003e\n\u003cli\u003ePytorch. [online]. Available at: \u003ca href=\"https://github.com/pytorch/pytorch\"\u003ehttps://github.com/pytorch/pytorch\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eChollet, Francois et al. Keras. [online]. Available at: \u003ca href=\"https://github.com/keras-team/keras\"\u003ehttps://github.com/keras-team/keras\u003c/a\u003e, 2015.\u003c/li\u003e\n\u003cli\u003eRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).\u003c/li\u003e\n\u003cli\u003eXin Li, Dan Roth, Learning Question Classifiers. COLING\u002702, Aug., 2002.\u003c/li\u003e\n\u003cli\u003eE. M. Voorhees and D. M. Tice. The TREC-8 question answering track evaluation. In TREC, volume 1999, page 82, 1999.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLibraries and algorithms are referenced in the files they are used.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1539289538.0
  },
  {
    "data_format": 2,
    "description": "Computational analysis of AmpSeq data for targeted comparative genomics",
    "filenames": [
      "Singularity/Singularity.AmpSeq"
    ],
    "full_name": "AmpSeq/AmpSeq",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2390\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-ampseq\" class=\"anchor\" href=\"#ampseq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAmpSeq\u003c/h1\u003e\n\u003cp\u003eAmplicon Sequencing (AmpSeq) is a practical, intuitive strategy with a semi-automated computational pipeline for analysis of highly multiplexed PCR-derived sequences. Amplicons can target from a single nucleotide to the upper limit of the sequencing platform. The flexibility of AmpSeq\u2019s wet lab methods make it a tool of broad interest for diverse species, and AmpSeq excels in flexibility, high-throughput, low-cost, accuracy, and semi-automated analysis. Here we describe the procedure to output data out of an AmpSeq project, with emphasis on the bioinformatics pipeline to generate SNPs, haplotypes and presence/absence variants in a set of diverse genotypes.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1566940831.0
  },
  {
    "data_format": 2,
    "description": "Deprecated repo. If you think you anything need from this, look at quip-docker and scream if it\u0027s not there",
    "filenames": [
      "Singularity"
    ],
    "full_name": "libAtoms/docker-quip-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-quip-base\" class=\"anchor\" href=\"#quip-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003equip-base\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/libAtoms/docker-quip-base\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a85436fd275dfe3b0867b90fdc6c7637a2d9df2d26fccc8d9f42b2e1f09becba/68747470733a2f2f7472617669732d63692e6f72672f6c696241746f6d732f646f636b65722d717569702d626173652e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/libAtoms/docker-quip-base.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/libAtoms/docker-quip-base.svg?branch=master\" rel=\"nofollow\"\u003ehttps://travis-ci.org/libAtoms/docker-quip-base.svg?branch=master\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA Docker image with a scientific stack that is used for building \u003ccode\u003eQUIP\u003c/code\u003e.\nThe image is hosted (and automatically built) on Docker hub as\n\u003ca href=\"https://hub.docker.com/r/libatomsquip/quip-base/\" rel=\"nofollow\"\u003elibatomsquip/quip-base\u003c/a\u003e.\nYou probably don\u0027t want to use this image directly, instead look for\none of the QUIP images on \u003ca href=\"https://hub.docker.com/u/libatomsquip/\" rel=\"nofollow\"\u003ehttps://hub.docker.com/u/libatomsquip/\u003c/a\u003e,\nprobably \u003ca href=\"https://hub.docker.com/r/libatomsquip/quip/\" rel=\"nofollow\"\u003elibatomsquip/quip\u003c/a\u003e.\nor use it in your \u003ccode\u003eFROM\u003c/code\u003e line. See also:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/libAtoms/QUIP\"\u003ehttps://github.com/libAtoms/QUIP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/libAtoms/QUIP/tree/public/docker\"\u003ehttps://github.com/libAtoms/QUIP/tree/public/docker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.libatoms.org\" rel=\"nofollow\"\u003ehttps://www.libatoms.org\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents\u003c/h2\u003e\n\u003cp\u003eThis image does not contain QUIP, but everything needed to build it\nplus many tools and codes that we find useful.\u003c/p\u003e\n\u003cp\u003eStack contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython 2.7 image (based on Debian)\u003c/li\u003e\n\u003cli\u003eBuild tools (gcc, gfortran)\u003c/li\u003e\n\u003cli\u003eOpenMP compiled version of OpenBLAS as default maths libraries\u003c/li\u003e\n\u003cli\u003eNumpy, SciPy, Matplotlib, ase...\u003c/li\u003e\n\u003cli\u003eJulia in \u003ccode\u003e/opt/julia\u003c/code\u003e with IJulia, PyCall, PyPlot, JuLIP...\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-data\" class=\"anchor\" href=\"#data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData\u003c/h2\u003e\n\u003cp\u003eThe image includes interatomic potentials in \u003ccode\u003e/opt/share/potentials\u003c/code\u003e\npublished on \u003ca href=\"http://www.libatoms.org/Home/DataRepository\" rel=\"nofollow\"\u003ehttp://www.libatoms.org/Home/DataRepository\u003c/a\u003e which has Gaussian\nApproximation Potentials for:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTungsten\u003c/li\u003e\n\u003cli\u003eIron\u003c/li\u003e\n\u003cli\u003eWater\u003c/li\u003e\n\u003cli\u003eAmorphous carbon\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eTo make or request changes, open a merge request or issue in the\n\u003ca href=\"https://github.com/libAtoms/docker-quip-base\"\u003eGitHub repository\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePackages should be added to where the usual istallation commands\n(e.g. \u003ccode\u003eapt-get\u003c/code\u003e, \u003ccode\u003epip\u003c/code\u003e, ...) are in the Dockerfile, with the exception\nthat Julia pacakes are listed at the beginning of the Julia section.\u003c/p\u003e\n\u003cp\u003eSmall software package builds can be added at the end of the Dockerfile.\nLarger software applications are included in the\n\u003ca href=\"https://hub.docker.com/r/libatomsquip/quip-base-software/\" rel=\"nofollow\"\u003elibatomsquip/quip-base-software\u003c/a\u003e\nimage in the \u003ccode\u003eSoftware\u003c/code\u003e subdirectory.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 15,
    "topics": [],
    "updated_at": 1602585132.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "bstriner/cuda-10.1-cudnn7-devel-ubuntu16.04",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cuda-101-cudnn7-devel-ubuntu1604\" class=\"anchor\" href=\"#cuda-101-cudnn7-devel-ubuntu1604\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecuda-10.1-cudnn7-devel-ubuntu16.04\u003c/h1\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1599516090.0
  },
  {
    "data_format": 2,
    "description": "Open source software that implements a new method for metagenomics analysis. A new mapping approach is integrated with traditional assembly and blast annotation, all integrated in a web-based user friendly visualization platform for ease interpretation.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "BU-ISCIII/PikaVirus",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pikavirus\" class=\"anchor\" href=\"#pikavirus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePikaVirus\u003c/h1\u003e\n\u003cp\u003eOpen source software that implements a new method for metagenomics analysis. A new mapping approach is integrated with traditional assembly and blast annotation, all integrated in a web-based user friendly visualization platform for ease interpretation.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1586703492.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.casacore.gpuvmem.11.0",
      "Singularity.casacore.gpuvmem.9.2",
      "Singularity.casacore.gpuvmem.10.0.ubuntu1604",
      "Singularity.HPC",
      "Singularity.casacore.gpuvmem.10.0",
      "Singularity.casacore.gpuvmem.9.2.ubuntu1604"
    ],
    "full_name": "miguelcarcamov/container_docker",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-container_docker\" class=\"anchor\" href=\"#container_docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainer_docker\u003c/h1\u003e\n\u003cp\u003eUseful containers to work with radio astronomical data\u003c/p\u003e\n\u003cp\u003eMiguel C\u00e1rcamo.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1615461418.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.BenOCR"
    ],
    "full_name": "Poulami-Sarkar/Bengali-Hindi-OCR",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-bengali-ocr\" class=\"anchor\" href=\"#bengali-ocr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBengali-OCR\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis project implements OCR for television news from Bengali and Hindi news channels. I am using OpenCV along with a pre-trained tensorflow model called EAST(An Efficient and Accurte Scene Test detector) for detecting ROI (Regions of interest) from the news videos.\nThen the detected ROIs are extracted and OCR, implemented using tesseract 4.0 is used to exract the text.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prequisites\" class=\"anchor\" href=\"#prequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrequisites\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eapt update\u003c/code\u003e\n\u003ccode\u003eapt install -y python3-pip build-essential libssl-dev\u003c/code\u003e \u003ccode\u003elibffi-dev python3-dev\u003c/code\u003e\n\u003ccode\u003eapt install  -y tesseract-ocr\u003c/code\u003e\n\u003ccode\u003eapt install -y libtesseract-dev libsm-dev\u003c/code\u003e\n\u003ccode\u003epip3 install pytesseract opencv-python numpy\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-working\" class=\"anchor\" href=\"#working\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorking\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-code\" class=\"anchor\" href=\"#running-the-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the code\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003epython3 textdetection.py \u0026lt;filename\u0026gt;\u003c/code\u003e\nSpecify additional argument \u0027O\u0027 for running OCR.\nOutput is saved to the file output/output.txt\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1566655098.0
  },
  {
    "data_format": 2,
    "description": "To build hpc benchmark and mpi with cuda support sif",
    "filenames": [
      "hpl_intel_cuda.def",
      "bert.def",
      "hpcc_intel.def",
      "hpc_mpi_cuda.def"
    ],
    "full_name": "perambluate/singularity-definition-files-for-HPC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc_mpi_cuda_singu_def_file\" class=\"anchor\" href=\"#hpc_mpi_cuda_singu_def_file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehpc_mpi_cuda_singu_def_file\u003c/h1\u003e\n\u003cp\u003eA collect of definition files to build images for singularity containers, which includes hpc benchmarks and mpis with cuda support.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4181\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1589020087.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/Singularity"
    ],
    "full_name": "CompArchCam/Cinnamon",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cinnamon\" class=\"anchor\" href=\"#cinnamon\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCinnamon\u003c/h1\u003e\n\u003cp\u003eThis directory contains the code for the Cinnamon language compiler.  This compiler is described in the paper:\u003c/p\u003e\n\u003cp\u003eCinnamon: A Domain-Specific Language for Binary Profiling and Monitoring,\nMahwish Arif, Ruoyu Zhou, Hsi-Ming Ho and Timothy M. Jones,\nCGO 2021\u003c/p\u003e\n\u003cp\u003ePlease cite this paper if you produce any work that builds upon this code and / or data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-licence\" class=\"anchor\" href=\"#licence\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicence\u003c/h2\u003e\n\u003cp\u003eCinnamon is released under an Apache licence.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-cinnamon\" class=\"anchor\" href=\"#building-cinnamon\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Cinnamon\u003c/h2\u003e\n\u003cp\u003eCinnamon can currently target three different binary frameworks; Janus, Pin and Dyninst.  To build the compiler:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003eexport CINNAMON_ROOT = /path/to/cinnamon-source\ncd $(CINNAMON_ROOT)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo build the Cinnamon backend for Janus:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003emake TARGET=janus\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo build the Cinnamon backend for Pin:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003emake TARGET=pin\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo build the Cinnamon backend for Dyninst:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003emake TARGET=dyninst\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-compiling-a-sample-program\" class=\"anchor\" href=\"#compiling-a-sample-program\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompiling a sample program\u003c/h2\u003e\n\u003cp\u003eCinnamon sample programs are available in the  \u003ccode\u003etests\u003c/code\u003e directory.  The following commands will compile the Cinnamon program \u003ccode\u003eins.dsl\u003c/code\u003e and integrate the resulting code into one of the target frameworks. You will need to set the path to your target framework installation in the respective scripts:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003e$(CINNAMON_ROOT)/Scripts/compileToJanus.py $CINNAMON_ROOT/tests/ins.dsl\n$(CINNAMON_ROOT)/Scripts/compileToPin.py $CINNAMON_ROOT/tests/ins.dsl\n$(CINNAMON_ROOT)/Scripts/compileToDyn.py $CINNAMON_ROOT/tests/ins.dsl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter this, the final tool can be built and run using the target framework\u0027s build instructions.\u003c/p\u003e\n\u003cp\u003eIf you just want to compile the Cinnamon DSL code and not yet integrate it into a target framework, run the following command.  This will generate a number of different files containing relevant code for the cinnamon program:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ecd $CINNAMON_ROOT\n./bdc $CINNAMON_ROOT/tests/ins.dsl\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-target-frameworks\" class=\"anchor\" href=\"#target-frameworks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTarget frameworks\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-janus\" class=\"anchor\" href=\"#janus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJanus\u003c/h3\u003e\n\u003cp\u003eYou can get the Janus implementation with placeholders, templates and utility libraries for Cinnamon from the main Janus repository at \u003ca href=\"https://github.com/timothymjones/Janus.git\"\u003ehttps://github.com/timothymjones/Janus.git\u003c/a\u003e, then switch to the \u003ccode\u003ecinnamon\u003c/code\u003e branch.\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003egit clone https://github.com/timothymjones/Janus.git\ncd Janus\ngit checkout -b cinnamon origin/cinnamon\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext set \u003ccode\u003eJanusPATH\u003c/code\u003e in \u003ccode\u003ecompileToJanus.py\u003c/code\u003e to be the location that you have cloned Janus.\u003c/p\u003e\n\u003cp\u003eOnce the code for Janus has been generated and integrated (after running the \u003ccode\u003ecompileToJanus.py\u003c/code\u003e script from above), you can build the final tool using the following commands:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003e(cd build; cmake ..; make -j8)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run the final tool on the target binary:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003e./janus/jdsl_run \u0026lt;target_binary\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pin\" class=\"anchor\" href=\"#pin\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePin\u003c/h3\u003e\n\u003cp\u003eEverything required for Pin is contained within the \u003ccode\u003etargets/Pin\u003c/code\u003e directory.  Copy the \u003ccode\u003eMyDSLTool\u003c/code\u003e directory to \u003ccode\u003epath-to-your-pin-root-dir/source/tools\u003c/code\u003e, where \u003ccode\u003epath-to-your-pin-root\u003c/code\u003e should be self-explanatory.\u003c/p\u003e\n\u003cp\u003eNext set \u003ccode\u003ePinPATH=your-pin-root-dir/source/tools/MyDSLTool\u003c/code\u003e in \u003ccode\u003ecompileToPin.py\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce the code for Pin has been generated and integrated (after running the \u003ccode\u003ecompileToPin.py\u003c/code\u003e script from above), you can build the final tool using the following commands:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ecd your-pin-root-dir/source/tools/MyDSLTool\nmake obj-intel64/MyDSLTool.so\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run the final tool on the target binary:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003eyour-pin-root-dir/pin -t obj-intel64/MyDSLTool.so -- \u0026lt;target_binary\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dyninst\" class=\"anchor\" href=\"#dyninst\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDyninst\u003c/h3\u003e\n\u003cp\u003eYou can obtain Dyninst version 10.1.0 as follows:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ewget https://github.com/dyninst/dyninst/archive/v10.1.0.tar.gz``\ntar xzvf v10.1.0.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce extracted, add \u003ccode\u003ec_LoadInsn\u003c/code\u003e and \u003ccode\u003ec_StoreInsn\u003c/code\u003e into \u003ccode\u003eenum InsnCategory\u003c/code\u003e in \u003ccode\u003edyninst-10.1.0/instructionAPI/h/InstructionCategories.h\u003c/code\u003e and then build by following the Dyninst build instructions.\u003c/p\u003e\n\u003cp\u003eEverything else required for Dyninst is contained within the \u003ccode\u003etargets/Dyninst\u003c/code\u003e directory.  Copy the \u003ccode\u003eMyDSLTool\u003c/code\u003e directory to \u003ccode\u003epath-to-your-dyn-root-dir/examples\u003c/code\u003e, where \u003ccode\u003epath-to-your-dyn-root-dir\u003c/code\u003e should be self-explanatory.\u003c/p\u003e\n\u003cp\u003eNext set \u003ccode\u003eDynPATH=path-to-your-dyn-root-dir/examples/MyDSLTool\u003c/code\u003e in \u003ccode\u003ecompileToDyn.py\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce the code for Dyninst has been generated and integrated (after running the \u003ccode\u003ecompileToDyn.py\u003c/code\u003e script from above), you can build the final tool using the following commands:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ecd path-to-your-dyn-root-dir/examples/MyDSLTool\nmake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run the final tool on the target binary:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003epath-to-your-dyn-root-dir/examples/MyDSLTool/DSLtool -m static -o \u0026lt;output_binary\u0026gt; \u0026lt;input_binary\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1621746520.0
  },
  {
    "data_format": 2,
    "description": "Pipeline to study intra tumor heterogeneity using HATCHet, DeCiFer and ClonEvol",
    "filenames": [
      "Singularity"
    ],
    "full_name": "IARCbioinfo/ITH_pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ith_pipeline\" class=\"anchor\" href=\"#ith_pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eITH_pipeline\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nextflow-pipeline-to-study-intra-tumoral-heterogeneity\" class=\"anchor\" href=\"#nextflow-pipeline-to-study-intra-tumoral-heterogeneity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextflow pipeline to study intra-tumoral heterogeneity\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNextflow pipeline to study intra-tumoral heterogeneity through subclonality reconstruction, using HATCHet, DeCiFer and clonEvol.\nPIPELINE IN DEVELOPMENT.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThis pipeline is based on \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003enextflow\u003c/a\u003e. As we have several nextflow pipelines, we have centralized the common information in the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository. Please read it carefully as it contains essential information for the installation, basic usage and configuration of nextflow and our pipelines.\u003c/li\u003e\n\u003cli\u003eExternal software:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/raphael-group/hatchet\"\u003eHATCHet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/raphael-group/decifer\"\u003eDeCiFer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hdng/clonevol\"\u003eClonEvol\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can avoid installing all the external software by only installing Docker. See the \u003ca href=\"https://github.com/IARCbioinfo/IARC-nf\"\u003eIARC-nf\u003c/a\u003e repository for more information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--bam_folder\u003c/td\u003e\n\u003ctd\u003eFolder containing BAM files to pass to HATCHet\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-mandatory\" class=\"anchor\" href=\"#mandatory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMandatory\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eExample value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--ref\u003c/td\u003e\n\u003ctd\u003e/whole/path/to/genome.fa\u003c/td\u003e\n\u003ctd\u003eReference genome in fasta format\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--correspondance\u003c/td\u003e\n\u003ctd\u003eTN_pairs.txt\u003c/td\u003e\n\u003ctd\u003eTabulated file containing two columns: tumor and normal bam names\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptional\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDefault value\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--cpu\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003eNumber of cpu to use\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--config\u003c/td\u003e\n\u003ctd\u003enull\u003c/td\u003e\n\u003ctd\u003eUse custom configuration file\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--mem\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003eSize of memory used in GB\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--output_folder\u003c/td\u003e\n\u003ctd\u003e.\u003c/td\u003e\n\u003ctd\u003ePath to output folder\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--samtools_folder\u003c/td\u003e\n\u003ctd\u003e/usr/bin/\u003c/td\u003e\n\u003ctd\u003esamtools installation dir\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--bcftools_folder\u003c/td\u003e\n\u003ctd\u003e/usr/bin/\u003c/td\u003e\n\u003ctd\u003ebcftools installation dir\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--hatchet_folder\u003c/td\u003e\n\u003ctd\u003e/usr/bin/\u003c/td\u003e\n\u003ctd\u003ehatchet installation dir\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--bnpy_folder\u003c/td\u003e\n\u003ctd\u003e/usr/bin/\u003c/td\u003e\n\u003ctd\u003ebnpy installation dir\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-flags\" class=\"anchor\" href=\"#flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFlags\u003c/h4\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFlags are special parameters without value.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--help\u003c/td\u003e\n\u003ctd\u003eDisplay help\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run iarcbioinfo/ITH_pipeline --bam_folder bam_folder/ --ref genome.fa --correspondance pairs.txt --cpu 24 --samtools_folder ~/bin/samtools-1.7 --bcftools_folder ~/bin/bcftools-1.7 --hatchet_folder ~/bin/hatchet --bnpy_folder ~/bin/bnpy-dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eType\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-detailed-description\" class=\"anchor\" href=\"#detailed-description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetailed description\u003c/h2\u003e\n\u003cp\u003e...\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directed-acyclic-graph\" class=\"anchor\" href=\"#directed-acyclic-graph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirected Acyclic Graph\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions\" class=\"anchor\" href=\"#contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eEmail\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eTiffany Delhomme\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:delhommet@students.iarc\"\u003edelhommet@students.iarc\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eDeveloper to contact for support\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references-optional\" class=\"anchor\" href=\"#references-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences (optional)\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-faq-optional\" class=\"anchor\" href=\"#faq-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFAQ (optional)\u003c/h2\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1621628184.0
  },
  {
    "data_format": 2,
    "description": "Bisulfite-seq data Workflow Automation Software and Protocols",
    "filenames": [
      "Singularity.v0.9",
      "Singularity",
      "Singularity.v1.1",
      "Singularity.v1.0"
    ],
    "full_name": "BrendelGroup/BWASP",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols\" class=\"anchor\" href=\"#bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBWASP : Bisulfite-seq data Workflow Automation Software and Protocols\u003c/h1\u003e\n\u003cp\u003eThe BWASP repository encompasses code and scripts developed in the\n\u003ca href=\"http://brendelgroup.org/\" rel=\"nofollow\"\u003eBrendel Group\u003c/a\u003e for analyses of bisulfite sequencing\ndata.\nThe entire workflow relies on various other open source software as well as\n\u003ca href=\"https://www.r-project.org/\" rel=\"nofollow\"\u003eR\u003c/a\u003e scripts from the companion\n\u003ca href=\"https://github.com/BrendelGroup/BWASPR\"\u003eBWASPR\u003c/a\u003e repository.\nThe code conforms to our \u003ca href=\"https://brendelgroup.github.io/\" rel=\"nofollow\"\u003eRAMOSE\u003c/a\u003e\nphilosophy: it generates \u003cstrong\u003ereproducible\u003c/strong\u003e, \u003cstrong\u003eaccurate\u003c/strong\u003e, and \u003cstrong\u003emeaningful\u003c/strong\u003e\nresults; it is \u003cstrong\u003eopen\u003c/strong\u003e (source) and designed to be \u003cstrong\u003escalable\u003c/strong\u003e and\n\u003cstrong\u003eeasy\u003c/strong\u003e to use.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start-\" class=\"anchor\" href=\"#quick-start-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start \u003ca href=\"https://singularity-hub.org/collections/1203\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h2\u003e\n\u003cp\u003eInput to the BWASP workflow consists of accession numbers or fastq files of\nbisulfite-sequencing reads as well as the appropriate genome assembly (and, if\navailable, genome annotation).\nOutput (after read quality control and mapping) are \u003cem\u003e*.mcalls\u003c/em\u003e files that list\nthe sufficiently covered genomic Cs and their methylation percentage in the\ngiven sample.\nThe scripts in the \u003cem\u003ebin\u003c/em\u003e directory take care of minor tasks in the overall\nworkflow, but configuration and execution is via\n\u003ca href=\"https://www.gnu.org/software/make/\" rel=\"nofollow\"\u003eGNU make\u003c/a\u003e using edited copies of the\nmakefiles provided in the \u003cem\u003emakefiles\u003c/em\u003e directory.\nAll the BWASP dependencies are encapsulated in a\n\u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container available from our\n\u003ca href=\"http://BrendelGroup.org/SingularityHub/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e.\nThus, once you know what you are doing, execution could be as simple as\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull http://BrendelGroup.org/SingularityHub/bwasp.sif\nsingularity exec bwasp.sif make\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(assuming you have prepared a suitable makefile in your working directory).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-realistic-start\" class=\"anchor\" href=\"#realistic-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRealistic Start\u003c/h2\u003e\n\u003cp\u003ePlease find detailed installation instructions and options in the\n\u003ca href=\"./INSTALL.md\"\u003eINSTALL\u003c/a\u003e document.\nOnce all preparatory steps are taken care of, see the \u003ca href=\"./HOWTO.md\"\u003eHOWTO\u003c/a\u003e\ndocument for a complete example of how to implement and run a workflow.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reference\" class=\"anchor\" href=\"#reference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference\u003c/h2\u003e\n\u003cp\u003eAmy L. Toth, Murat Ozturk, Saranya Sankaranarayanan, and Volker P. Brendel\n(2018) \u003cem\u003eEstimating the size and dynamics of the CpG methylome of social\ninsects.\u003c/em\u003e To be submitted.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003ePlease direct all comments and suggestions to\n\u003ca href=\"mailto:vbrendel@indiana.edu\"\u003eVolker Brendel\u003c/a\u003e\nat \u003ca href=\"http://brendelgroup.org/\" rel=\"nofollow\"\u003eIndiana University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1621473108.0
  },
  {
    "data_format": 2,
    "description": "Part of the sc-eQTLgen consortium pipeline. Step 1, where the QC is done.",
    "filenames": [
      "Singularity.WGpipeline",
      "Singularity.Imputation"
    ],
    "full_name": "sc-eQTLgen-consortium/WG1-pipeline-QC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wg1-pipeline-qc\" class=\"anchor\" href=\"#wg1-pipeline-qc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWG1-pipeline-QC\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png\" width=\"300\" height=\"140\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePart of the sceQTL-Gen consortium pipeline. Step 1, where the QC is done.\u003c/p\u003e\n\u003cp\u003ePlease see the \u003ca href=\"https://github.com/sc-eQTLgen-consortium/WG1-pipeline-QC/wiki\"\u003eWiki\u003c/a\u003e for information on running the QC pipeline.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621271961.0
  },
  {
    "data_format": 2,
    "description": "Biobb_md is the Biobb module collection to perform molecular dynamics simulations.",
    "filenames": [
      "Singularity.latest"
    ],
    "full_name": "bioexcel/biobb_md",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://biobb-md.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fb21acbebea7f37e1dbd5234470daa21656b5d221993e4eaafaf9c6452f316d6/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f62696f62622d6d642f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"\" data-canonical-src=\"https://readthedocs.org/projects/biobb-md/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/bioconda/biobb_md\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a477fdb1fd9bc9eb7ffa6cae6a019c6d4c3902fd468b3126f1b78e56c7dcff83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e7376673f7374796c653d666c6174\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://quay.io/repository/biocontainers/biobb_md\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ca418e4db0b3de91a09a5df4a59446da015b6164598a8bc255918e911484f84f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d517561792e696f2d626c7565\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/docker-Quay.io-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.singularity-hub.org/collections/2735/usage\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/Apache-2.0\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-biobb_md\" class=\"anchor\" href=\"#biobb_md\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebiobb_md\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eBiobb_md is the Biobb module collection to perform molecular dynamics simulations.\nBiobb (BioExcel building blocks) packages are Python building blocks that\ncreate new layer of compatibility and interoperability over popular\nbioinformatics tools.\nThe latest documentation of this package can be found in our readthedocs site:\n\u003ca href=\"http://biobb-md.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003elatest API documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-version\" class=\"anchor\" href=\"#version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion\u003c/h3\u003e\n\u003cp\u003ev3.5.1 2020.4\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eUsing PIP:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eImportant:\u003c/strong\u003e PIP only installs the package. All the dependencies must be installed separately. To perform a complete installation, please use ANACONDA, DOCKER or SINGULARITY.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  pip install \"biobb_md\u0026gt;=3.5.1\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage: \u003ca href=\"https://biobb-md.readthedocs.io/en/latest/modules.html\" rel=\"nofollow\"\u003ePython API documentation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing ANACONDA:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  conda install -c bioconda \"biobb_md\u0026gt;=3.5.1\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage: With conda installation BioBBs can be used with the \u003ca href=\"https://biobb-md.readthedocs.io/en/latest/modules.html\" rel=\"nofollow\"\u003ePython API documentation\u003c/a\u003e and the \u003ca href=\"https://biobb-md.readthedocs.io/en/latest/command_line.html\" rel=\"nofollow\"\u003eCommand Line documentation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing DOCKER:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker pull quay.io/biocontainers/biobb_md:3.5.1--py_0\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run quay.io/biocontainers/biobb_md:3.5.1--py_0 \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing SINGULARITY:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMacOS users\u003c/strong\u003e: it\u0027s strongly recommended to avoid Singularity and use \u003cstrong\u003eDocker\u003c/strong\u003e as containerization system.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity pull --name biobb_md.sif shub://bioexcel/biobb_md\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity exec biobb_md.sif \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe command list and specification can be found at the \u003ca href=\"https://biobb-md.readthedocs.io/en/latest/command_line.html\" rel=\"nofollow\"\u003eCommand Line documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-copyright--licensing\" class=\"anchor\" href=\"#copyright--licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCopyright \u0026amp; Licensing\u003c/h3\u003e\n\u003cp\u003eThis software has been developed in the \u003ca href=\"http://mmb.irbbarcelona.org\" rel=\"nofollow\"\u003eMMB group\u003c/a\u003e at the \u003ca href=\"http://www.bsc.es/\" rel=\"nofollow\"\u003eBSC\u003c/a\u003e \u0026amp; \u003ca href=\"https://www.irbbarcelona.org/\" rel=\"nofollow\"\u003eIRB\u003c/a\u003e for the \u003ca href=\"http://bioexcel.eu/\" rel=\"nofollow\"\u003eEuropean BioExcel\u003c/a\u003e, funded by the European Commission (EU H2020 \u003ca href=\"http://cordis.europa.eu/projects/823830\" rel=\"nofollow\"\u003e823830\u003c/a\u003e, EU H2020 \u003ca href=\"http://cordis.europa.eu/projects/675728\" rel=\"nofollow\"\u003e675728\u003c/a\u003e).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(c) 2015-2021 \u003ca href=\"https://www.bsc.es/\" rel=\"nofollow\"\u003eBarcelona Supercomputing Center\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e(c) 2015-2021 \u003ca href=\"https://www.irbbarcelona.org/\" rel=\"nofollow\"\u003eInstitute for Research in Biomedicine\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLicensed under the\n\u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\"\u003eApache License 2.0\u003c/a\u003e, see the file LICENSE for details.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67\" alt=\"\" title=\"Bioexcel\" data-canonical-src=\"https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1620768602.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for LaTeX.",
    "filenames": [
      "Singularity.pdflatex"
    ],
    "full_name": "bast/singularity-latex",
    "latest_release": "0.1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-latex\" class=\"anchor\" href=\"#singularity-recipe-for-latex\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe for LaTeX\u003c/h1\u003e\n\u003cp\u003eHow to fetch and use the image:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity pull https://github.com/bast/singularity-latex/releases/download/0.1.0/pdflatex.sif\n$ ./pdflatex.sif example.tex\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI have used this wonderful guide as starting point and inspiration:\n\u003ca href=\"https://github.com/singularityhub/singularity-deploy\"\u003ehttps://github.com/singularityhub/singularity-deploy\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [
      "latex",
      "pdflatex",
      "singularity"
    ],
    "updated_at": 1620408929.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.lofar_sksp",
      "Singularity.lofar_sksp_base_cuda",
      "Singularity.lofar_sksp_base_mkl_cuda",
      "Singularity.lofar_sksp_base"
    ],
    "full_name": "tikk3r/lofar-grid-hpccloud",
    "latest_release": "v3.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572\" data-canonical-src=\"https://img.shields.io/github/v/release/tikk3r/lofar-grid-hpccloud?sort=semver\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562\" data-canonical-src=\"https://img.shields.io/github/license/tikk3r/lofar-grid-hpccloud.svg?logo=github\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n[![DOI](\u003ca href=\"https://zenodo.org/badge/136925861.svg)%5D(https://zenodo.org/badge/latestdoi/136925861\" rel=\"nofollow\"\u003ehttps://zenodo.org/badge/136925861.svg)](https://zenodo.org/badge/latestdoi/136925861\u003c/a\u003e)\n\u003ch1\u003e\n\u003ca id=\"user-content-lofar-grid-hpccloud\" class=\"anchor\" href=\"#lofar-grid-hpccloud\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elofar-grid-hpccloud\u003c/h1\u003e\n\u003cp\u003eThis repository hold resources for deploying the LOFAR software (genericpipeline) and related tools through Singularity containers. These containers are general, but at the same time somewhat tailored for SKSP use.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003emaster\u003c/code\u003e branch is empty. Currently the images are based on the Fedora 27 Linux distribution, which is available from \u003ca href=\"https://hub.docker.com/_/fedora\" rel=\"nofollow\"\u003eDockerHub\u003c/a\u003e. Recipes to build this container can be found on the \u003ccode\u003efedora\u003c/code\u003e branch.\u003c/p\u003e\n\u003cp\u003eTo build a full LOFAR Singularity image, do the following:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eBuild Singularity.lofarbase\u003c/p\u003e\n\u003cp\u003esudo singularity build lofar_sksp_base.sif Singularity.lofar_sksp_base\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild Singularity.lofar (use the \u003ccode\u003eFrom: localimage\u003c/code\u003e part instead of the Singularity Hub part)\u003c/p\u003e\n\u003cp\u003esudo singularity build lofar_sksp.sif Singularity.lofar_sksp\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePre-built containers are public hosted at \u003ca href=\"https://lofar-webdav.grid.sara.nl/software/shub_mirror/tikk3r/lofar-grid-hpccloud/\" rel=\"nofollow\"\u003eSURFSara\u003c/a\u003e. Sort by date to find the latest container there.\u003c/p\u003e\n\u003cp\u003eVisit the  \u003ca href=\"https://github.com/tikk3r/lofar-grid-hpccloud/wiki\"\u003ewiki\u003c/a\u003e for more detailed information and build instructions.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1620055907.0
  },
  {
    "data_format": 2,
    "description": "ParaView Catalyst adaptor example for a Fortran code",
    "filenames": [
      "Singularity"
    ],
    "full_name": "niwa/lfric_catalyst_adaptor",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lfric-catalyst-adaptor\" class=\"anchor\" href=\"#lfric-catalyst-adaptor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLFRic Catalyst Adaptor\u003c/h1\u003e\n\u003cp\u003eParaView Catalyst adaptor implementation for the LFRic code\u003c/p\u003e\n\u003cp\u003eThis package builds a library for visualising simulation data with a simple VTK visualisation pipeline. The pipeline can be defined either in C++ or using a Python script.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-adaptor\" class=\"anchor\" href=\"#building-the-adaptor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the adaptor\u003c/h2\u003e\n\u003cp\u003eTo build this code, you will need to build and install ParaView with Catalyst option enabled. Once this is done, build the code using CMake as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir build\ncd build\ncmake .. -DParaView_DIR=/path/to/catalyst/install/directory/lib/cmake/paraview-5.4 -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/path/to/install/dir\nmake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you want to build a debug version of the code, add \u003ccode\u003e-DCMAKE_BUILD_TYPE=Debug\u003c/code\u003e to the CMake configuration, or use the \u003ccode\u003eccmake\u003c/code\u003e configuration tool. You can add additional compiler flags using the \u003ccode\u003e-DCMAKE_CXX_FLAGS=\u003c/code\u003e option.\u003c/p\u003e\n\u003cp\u003eOn a Cray XC50 system, the following build setup should work:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecmake .. -DCMAKE_CXX_COMPILER=CC -DCMAKE_EXE_LINKER_FLAGS=-dynamic -DParaView_DIR=/path/to/catalyst/install/directory/lib/cmake/paraview-5.4 -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/path/to/install/dir\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that dynamic linking simplifies the linking process of the Fortran application significantly.\u003c/p\u003e\n\u003cp\u003eOnce CMake has finished, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake\nmake install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto build and install the library.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-test-battery\" class=\"anchor\" href=\"#running-the-test-battery\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the test battery\u003c/h2\u003e\n\u003cp\u003eIf you want to test your build, add \u003ccode\u003e-DBUILD_TESTING=ON\u003c/code\u003e to your CMake configuration and run \u003ccode\u003emake test\u003c/code\u003e or \u003ccode\u003ectest\u003c/code\u003e after building the code. This will run a number of tests that check basic functionality.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-a-simulation-with-the-adaptor\" class=\"anchor\" href=\"#running-a-simulation-with-the-adaptor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning a simulation with the adaptor\u003c/h2\u003e\n\u003cp\u003eThe Catalyst adaptor and libraries are usually dynamically linked. If the build system of your code does not hardcode shared library paths, you will need to set (possibly adapting ParaView version)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport LD_LIBRARY_PATH=/path/to/catalyst/installation/lib/paraview-5.4:$LD_LIBRARY_PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you want to use the Python pipeline, set \u003ccode\u003ePYTHONPATH\u003c/code\u003e to something like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYTHONPATH=/path/to/catalyst/installation/lib/paraview-5.4/site-packages:/path/to/catalyst/installation/lib/paraview-5.4/site-packages/vtk:$PYTHONPATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-python-scripts\" class=\"anchor\" href=\"#python-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython scripts\u003c/h2\u003e\n\u003cp\u003eThe repository includes a number of Python scripts which define visualisation pipelines or provide some post-processing functionality.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-full_outputpy\" class=\"anchor\" href=\"#full_outputpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efull_output.py\u003c/h3\u003e\n\u003cp\u003eSimple Python pipeline for writing the model grid and data field to a VTK file.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spherical_slicepy\" class=\"anchor\" href=\"#spherical_slicepy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003espherical_slice.py\u003c/h3\u003e\n\u003cp\u003eSimple Python pipeline for creating spherical slices of model grid with a preset radius, which are written into a VTK polydata file. Full output of the model grid and data field can also be produced by setting the corresponding flag in the pipeline script.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spherical_slice_contourspy\" class=\"anchor\" href=\"#spherical_slice_contourspy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003espherical_slice_contours.py\u003c/h3\u003e\n\u003cp\u003eSame as \"spherical_slice.py\", but includes an additional output file with contours.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spherical_slice_renderedpy\" class=\"anchor\" href=\"#spherical_slice_renderedpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003espherical_slice_rendered.py\u003c/h3\u003e\n\u003cp\u003eSame as \"spherical_slice.py\", but includes a rendered image of the slice which is stored as a png file.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spherical_slice_rendered_coastlinespy\" class=\"anchor\" href=\"#spherical_slice_rendered_coastlinespy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003espherical_slice_rendered_coastlines.py\u003c/h3\u003e\n\u003cp\u003eSame as \"spherical_slice_rendered.py\", but overlays coastlines on the rendered image. Requires downloading coastlines data, see source file for instructions.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-meridional_slicepy\" class=\"anchor\" href=\"#meridional_slicepy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emeridional_slice.py\u003c/h3\u003e\n\u003cp\u003eCreates and stores a meridional slice for a chosen longitude, including a transformation from Cartesian to longitude-radius coordinates.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-map_projectpy\" class=\"anchor\" href=\"#map_projectpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emap_project.py\u003c/h3\u003e\n\u003cp\u003eThis Python program expects a spherical slice (as produced by the \u003ccode\u003espherical_slice.py\u003c/code\u003e visualisation pipeline) in VTK polydata format as input and produces a VTK polydata file with a map projection as output. The program can handle partitioned datasets, but computing map projections for multiple timesteps is not supported yet.\u003c/p\u003e\n\u003cp\u003eRunning\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./map_project.py input.vtp output.vtp\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecomputes a Mollweide map projection. Use flag \u003ccode\u003e--list-projections\u003c/code\u003e to get a list of projections and their short names (projections are provide by the PROJ library). Short names can be used to set another projection with the \u003ccode\u003e--projname\u003c/code\u003e flag, e.g., \u003ccode\u003e--projname=gall\u003c/code\u003e.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1619765892.0
  },
  {
    "data_format": 2,
    "description": "Software for Meteorology, Normally Distributed",
    "filenames": [
      "Singularity.smnd-run",
      "Singularity.mistral"
    ],
    "full_name": "ARPA-SIMC/smnd",
    "latest_release": "v2.11",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-smnd\" class=\"anchor\" href=\"#smnd\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSMND\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-software-for-meteorology-normally-distributed\" class=\"anchor\" href=\"#software-for-meteorology-normally-distributed\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware for Meteorology, Normally Distributed\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003ethe software for sure, the meteorological data not really.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSMND is a helper package for simplifying the build and the deployment\nof a collection of meteorological software packages, mainly developed\nby \u003ca href=\"http://www.arpa.emr.it/sim\" rel=\"nofollow\"\u003eArpae-SIMC\u003c/a\u003e. The current version is\nrelatively stable, including the universal binary package.\u003c/p\u003e\n\u003cp\u003eThe software packages involved, all open source and freely\nredistributable, are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://confluence.ecmwf.int/display/ECC/ecCodes+Home\" rel=\"nofollow\"\u003eeccodes\u003c/a\u003e\nfrom ECMWF\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ARPA-SIMC/wreport\"\u003ewreport\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ARPA-SIMC/bufr2netcdf\"\u003ebufr2netcdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ARPA-SIMC/dballe\"\u003eDB.All-e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ARPA-SIMC/arkimet\"\u003earkimet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ARPA-SIMC/libsim\"\u003elibsim\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-the-software-from-source\" class=\"anchor\" href=\"#building-the-software-from-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the software from source\u003c/h3\u003e\n\u003cp\u003eFor building autonomously the software collection you can follow the\nguidelines in the \u003ca href=\"doc/buildfromsource.md\"\u003ecorresponding page\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-deploying-the-software\" class=\"anchor\" href=\"#deploying-the-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploying the software\u003c/h3\u003e\n\u003cp\u003eIf you do not want to build the packages on your own, different\napproaches are possible for quickly deploying precompiled binaries:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe quick and universal way, \u003ca href=\"doc/unibin.md\"\u003ethe universal binary\npackage\u003c/a\u003e (no need to be the system administrator).\u003c/li\u003e\n\u003cli\u003eRunning from a \u003ca href=\"doc/singularity.md\"\u003esingularity container\u003c/a\u003e\n(requires agreement with the system administrator).\u003c/li\u003e\n\u003cli\u003eInstalling in a supported distribution (CentOS/Fedora) from \u003ca href=\"doc/copr.md\"\u003ecopr\nrepository\u003c/a\u003e (requires to BE the system administrator).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-notes-for-cosmo-model-users\" class=\"anchor\" href=\"#notes-for-cosmo-model-users\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes for COSMO model users\u003c/h3\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1619542108.0
  },
  {
    "data_format": 2,
    "description": "OpenCV 2 built with NVIDIA acceleration",
    "filenames": [
      "Singularity"
    ],
    "full_name": "dl-container-registry/opencv2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-opencv2-dockerfile\" class=\"anchor\" href=\"#opencv2-dockerfile\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpenCV2 Dockerfile\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/dl-container-registry/opencv2\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4cb047fbd7f8a38c0fd7c817c86e59892ff4db6f64e32a0fe2f310b40905dbd8/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f70726f6a6563742f6769746875622f646c2d636f6e7461696e65722d72656769737472792f6f70656e6376322f6d61737465722e737667\" alt=\"CircleCI branch\" data-canonical-src=\"https://img.shields.io/circleci/project/github/dl-container-registry/opencv2/master.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/willprice/opencv2-cuda8/~/settings/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/99bb6090faef97032d3bfd80b4d0cdb9d984e9e97aeb1d2750bc3e442fb117f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65726875622d3232623865622e737667\" alt=\"Dockerhub link\" data-canonical-src=\"https://img.shields.io/badge/hosted-dockerhub-22b8eb.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/530\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"Singularity hub hosted\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eBuild other containers from this base image, it contains a prebuilt version of\nOpenCV 2.4.13.4 at \u003ccode\u003e/src/opencv_build\u003c/code\u003e installed to \u003ccode\u003e/usr/local/OpenCV\u003c/code\u003e\n(containing CMake files for building other projects).\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1589337912.0
  },
  {
    "data_format": 2,
    "description": "Feichtenhofer\u0027s gpu_flow",
    "filenames": [
      "Singularity"
    ],
    "full_name": "dl-container-registry/gpu-flow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-gpu_flow-container\" class=\"anchor\" href=\"#gpu_flow-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/uob-epic/gpu_flow\"\u003egpu_flow\u003c/a\u003e container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-details\" class=\"anchor\" href=\"#details\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetails\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCUDA 8.0\u003c/li\u003e\n\u003cli\u003eOpenCV 2.4.13.4 built from \u003ccode\u003eopencv2-cuda8\u003c/code\u003e container.\u003c/li\u003e\n\u003cli\u003eQt5\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ docker run --runtime=nvidia --rm -t willprice/gpu_flow -h\n\nUsage: /src/gpu_flow_build/compute_flow [options]\nAvaible options:\n  -f, --type=[1]                flow method\n  -g, --gpuID=[1]               gpu ID\n  -h, --help=[true]             print help message\n  -i, --in_dir                  input directory of videos\n  -j, --frame_root_dir=[frames] path to directory to store jpegs\n  -o, --flow_root_dir=[flow]    path to directory to store flow\n  -s, --skip=[1]                frame skip\n  -v, --start_video=[1]         start video ID\n\n$ docker run --runtime=nvidia --rm -t willprice/gpu_flow \\\n    --gpuID=0 \\\n    --in_dir=$HOME/input_videos \\\n    --frame_root_dir=$HOME/output/frames \\\n    --flow_root_dir=$HOME/output/flow\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1555106449.0
  },
  {
    "data_format": 2,
    "description": "JupyterHub + High-Performance Computing",
    "filenames": [
      "singularity/Singularity",
      "singularity/Singularity_Tensorflow"
    ],
    "full_name": "pc2/JHub-HPC-Interface",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-jupyterhub--high-performance-computing\" class=\"anchor\" href=\"#jupyterhub--high-performance-computing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyterHub + High-Performance Computing\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eHigh performance Jupyter Notebooks\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe aim of this project is to connect JupyterHub to a high-performance computer (HPC). By automatically offloading the computations in a Jupyter notebook to the HPC system, even complex calculations are possible. While JupyterHub is deployed on a regular server, the notebooks themselves are spawned and run on the remote HPC system using a workload manager, such as Slurm.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMotivation\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe technical core of this project is the transparent integration of digital worksheets (Jupyter notebooks), in which learning content and programs can be displayed, edited and executed on the students\u0027 own laptops, with current cloud and high-performance computing (HPC) technologies. This provides the conditions for innovative, digital teaching that encourages independent and interactive development of, for example, data science applications, without imposing the complexity of using a high-performance computer system on the students. Instead, particularly computationally and data-intensive calculations are automatically offloaded to a high-performance computer, enabling even sophisticated analyses to be performed that would otherwise not be feasible on students\u0027 laptops.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFeatures and use cases\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStarting a jupyter notebook server on a remote HPC system in a pre-defined singularity container\u003c/li\u003e\n\u003cli\u003eQuick config setup when using the Slurm configuration wizard\u003c/li\u003e\n\u003cli\u003eAutomatically create a singularity overlay so that user changes are persistent\u003c/li\u003e\n\u003cli\u003eGreat for managing courses with external participants\u003c/li\u003e\n\u003cli\u003ePossibility to include files in the notebook directory using WebDAV\u003c/li\u003e\n\u003cli\u003eSuitable for HPC users who have their own JupyterHub instance running and want to use HPC resources\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#jupyterhub--high-performance-computing\"\u003eJupyterHub + High-Performance Computing\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#table-of-contents\"\u003eTable of Contents\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#installation-of-jupyterhub-server\"\u003eInstallation of JupyterHub Server\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#jupyterhub-and-batchspawner\"\u003eJupyterHub and BatchSpawner\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ssh-tunnel-user\"\u003eSSH tunnel user\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#node-mapping\"\u003eNode mapping\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#installation-on-hpc-system\"\u003eInstallation on HPC System\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#requirements\"\u003eRequirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#install-using-pip\"\u003eInstall using pip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#singularity-container\"\u003eSingularity Container\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#build-singularity-container\"\u003eBuild Singularity Container\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#compute\"\u003eCompute\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#gpu-tensorflow\"\u003eGPU (Tensorflow)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#the-configuration-file\"\u003eThe configuration file\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#slurm-configuration-wizard\"\u003eSlurm configuration wizard\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#examples\"\u003eExamples\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#debug-mode\"\u003eDebug mode\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#shibboleth-integration\"\u003eShibboleth Integration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#nbgrader-integration\"\u003eNBGrader Integration\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#changing-the-student-id-to-the-jupyterhub-logged-in-user-name\"\u003eChanging the Student ID to the JupyterHub logged in user name\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#create-nbgrader_configpy\"\u003eCreate nbgrader_config.py\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#security-precautions\"\u003eSecurity Precautions\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#singularity-host-filesystems\"\u003eSingularity Host Filesystems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#jupyterhub-api-https\"\u003eJupyterHub API (HTTPS)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#https\"\u003eHTTPS\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#tunnelbot-user\"\u003etunnelbot user\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#troubleshooting\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-of-jupyterhub-server\" class=\"anchor\" href=\"#installation-of-jupyterhub-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation of JupyterHub Server\u003c/h2\u003e\n\u003cp\u003eThis section describes the required installations and configurations on the JupyterHub server.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyterhub-and-batchspawner\" class=\"anchor\" href=\"#jupyterhub-and-batchspawner\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyterHub and BatchSpawner\u003c/h3\u003e\n\u003cp\u003eThe first thing you should do is install JupyterHub and BatchSpawner. For this purpose we provide an Ansible playbook which can be found in \u003ccode\u003e/jupyterhub-deployment/\u003c/code\u003e. See the README for details. Alternatively, you can follow the official installation instructions.\u003c/p\u003e\n\u003cp\u003eIf you decide to do the installations yourself, please proceed as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einstall \u003ca href=\"https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html\" rel=\"nofollow\"\u003eJupyterHub\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003einstall \u003ca href=\"https://github.com/jupyterhub/batchspawner\"\u003eBatchSpawner\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003einstall \u003ca href=\"https://github.com/jupyterhub/wrapspawner\"\u003eWrapSpawner\u003c/a\u003e (make sure to install it in the right environment: \u003ccode\u003e/opt/jupyterhub/bin/pip3 install git+https://github.com/jupyterhub/wrapspawner\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003ecopy the JupyterHub configuration file \u003ccode\u003e/jupyterhub-deployment/config_files/jupyterhub_config.py\u003c/code\u003e to \u003ccode\u003e/opt/jupyterhub/etc/jupyterhub/\u003c/code\u003e (you will most likely have to edit this file afterwards to make it fit your needs)\u003c/li\u003e\n\u003cli\u003erestart the JupyterHub service\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ssh-tunnel-user\" class=\"anchor\" href=\"#ssh-tunnel-user\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSSH tunnel user\u003c/h3\u003e\n\u003cp\u003eA user called \u003ccode\u003etunnelbot\u003c/code\u003e is needed on the JupyterHub server. This user is responsible for starting an SSH tunnel between the compute node and the JupyterHub server. An SSH key pair for the above mentioned purpose must be generated. See \u003ccode\u003e/examples/jupyterhub_config.py\u003c/code\u003e for more information.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-node-mapping\" class=\"anchor\" href=\"#node-mapping\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNode mapping\u003c/h3\u003e\n\u003cp\u003eJupyterHub extracts the execution host name of the HPC system (e.g. \u003ccode\u003enode01-002\u003c/code\u003e). When a notebook server is started, an SSH tunnel is established using the notebook port.\u003c/p\u003e\n\u003cp\u003eIn order for JupyterHub to be able to resolve the compute nodes host name, the \u003ccode\u003e/etc/hosts\u003c/code\u003e file must be edited. An example entry might look like the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e127.0.0.1 node01-001\n127.0.0.1 node01-002\n127.0.0.1 node01-003\n...\n127.0.0.1 node12-048\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe actual node names depend on your HPC system of course.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-on-hpc-system\" class=\"anchor\" href=\"#installation-on-hpc-system\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation on HPC System\u003c/h2\u003e\n\u003cp\u003eThis section describes the required installations and configurations of the HPC system to enable the interaction with the JuypterHub server.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eYou need a user who is allowed to allocate resources on the HPC system\n\u003cul\u003e\n\u003cli\u003eWith a SSH key pair. The public part must be deposited on the JupyterHub serer (\u003ccode\u003etunnelbot\u003c/code\u003e user)\u003c/li\u003e\n\u003cli\u003eThe public key part of the \u003ccode\u003etunnelbot\u003c/code\u003e-user created on the JupyterHub (-\u0026gt; \u003cem\u003e~/.ssh/authorized_keys\u003c/em\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularity (\u0026gt; v.3.7.0)\u003c/li\u003e\n\u003cli\u003emkfs/e2fsprogs with following option:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://git.kernel.org/pub/scm/fs/ext2/e2fsprogs.git/commit/?id=217c0bdf17899c0f79b73f76feeadd6d55863180\" rel=\"nofollow\"\u003ehttps://git.kernel.org/pub/scm/fs/ext2/e2fsprogs.git/commit/?id=217c0bdf17899c0f79b73f76feeadd6d55863180\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-using-pip\" class=\"anchor\" href=\"#install-using-pip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall using pip\u003c/h3\u003e\n\u003cp\u003eYou can download and install the required files with pip.\u003c/p\u003e\n\u003cp\u003eYou may want to build a small Python environment, or install the tools with \u003ccode\u003e--user\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython3 -m pip install --user jh-hpc-interface\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Container\u003c/h3\u003e\n\u003cp\u003eSingularity recipe examples are in the directory singularity/.\u003c/p\u003e\n\u003cp\u003eIf you do not want to use singularity, then change the value of \u003ccode\u003euse_singularity\u003c/code\u003e in jh_config.ini to false.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-build-singularity-container\" class=\"anchor\" href=\"#build-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild Singularity Container\u003c/h4\u003e\n\u003cp\u003eTo build the container with the recipe files in singularity/ you have to clone this repository.\u003c/p\u003e\n\u003cp\u003eThe following commands replace USER_ID in the recipes to the output of \u003ccode\u003eid -u\u003c/code\u003e, create a new hidden file and build the singularity container with the new created file.\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-compute\" class=\"anchor\" href=\"#compute\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompute\u003c/h5\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eUSER_ID=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003eid -u\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e sed \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003es/USER_ID/\u003cspan class=\"pl-smi\"\u003e$USER_ID\u003c/span\u003e/\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e singularity/Singularity \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e singularity/.recipefile_compute \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e singularity build --remote singularity/compute_jupyter.sif singularity/.recipefile_compute\u003c/pre\u003e\u003c/div\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-gpu-tensorflow\" class=\"anchor\" href=\"#gpu-tensorflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGPU (Tensorflow)\u003c/h5\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eUSER_ID=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003eid -u\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e sed \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003es/USER_ID/\u003cspan class=\"pl-smi\"\u003e$USER_ID\u003c/span\u003e/\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e singularity/Singularity_Tensorflow \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e singularity/.recipefile_gpu \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e singularity build --remote singularity/gpu_jupyter.sif singularity/.recipefile_gpu\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003esingularity build help section\u003c/em\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e-r, --remote\u003c/strong\u003e            build image remotely (does not require root)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003ePlease refer to the official docs on how to use the remote build feature: \u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003ehttps://sylabs.io/docs/\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-the-configuration-file\" class=\"anchor\" href=\"#the-configuration-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe configuration file\u003c/h3\u003e\n\u003cp\u003eIn the directory \u003cstrong\u003ebin/\u003c/strong\u003e is a script, which is deposited after the installation on the system.\u003c/p\u003e\n\u003cp\u003eWith the following call you can display the location of the configuration file:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ jh_wrapper getconfig\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo learn more about the configuration file, see \u003ca href=\"docs/jh_config.ini.md\"\u003edocs/jh_config.ini.md\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-slurm-configuration-wizard\" class=\"anchor\" href=\"#slurm-configuration-wizard\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSlurm configuration wizard\u003c/h3\u003e\n\u003cp\u003eWith the configuration wizard you can prepare your HPC environment.\u003c/p\u003e\n\u003cp\u003eThe script interactively goes through the configuration file and creates a temporary file which can be copied with a simple \u003ccode\u003ecp\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo start the wizard type the following:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ jh_slurm_wizard\u003c/pre\u003e\u003c/div\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cp\u003eYou will find examples for the configuration files \u003cstrong\u003ejh_config.ini\u003c/strong\u003e and \u003cstrong\u003ejupyterhub_config.py\u003c/strong\u003e in the directory \u003cem\u003eexamples/\u003c/em\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-debug-mode\" class=\"anchor\" href=\"#debug-mode\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDebug mode\u003c/h3\u003e\n\u003cp\u003eBy default the logs contain only information such as warnings or error messages.\nIt is also possible to switch on the debug mode, which writes extended information into the log files.\u003c/p\u003e\n\u003cp\u003eJust set \u003ccode\u003elog_level\u003c/code\u003e in the configuration file to \u0027DEBUG\u0027.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-shibboleth-integration\" class=\"anchor\" href=\"#shibboleth-integration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eShibboleth Integration\u003c/h2\u003e\n\u003cp\u003eShibboleth authentication was set up for a JupyterHub server in a test environment. See \u003ccode\u003e./shibboleth/\u003c/code\u003e for an example configuration.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nbgrader-integration\" class=\"anchor\" href=\"#nbgrader-integration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNBGrader Integration\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eInstallation instructions:\n\u003ca href=\"https://nbgrader.readthedocs.io/en/latest/configuration/jupyterhub_config.html\" rel=\"nofollow\"\u003ehttps://nbgrader.readthedocs.io/en/latest/configuration/jupyterhub_config.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTo create an exchange directory for every user, just create an empty directory in \u003ccode\u003e$scratch_dir\u003c/code\u003e and mount it into the container with \u003ccode\u003e$singularity_bind_extra\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-changing-the-student-id-to-the-jupyterhub-logged-in-user-name\" class=\"anchor\" href=\"#changing-the-student-id-to-the-jupyterhub-logged-in-user-name\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChanging the Student ID to the JupyterHub logged in user name\u003c/h3\u003e\n\u003cp\u003eSince the containers run as user \u003ccode\u003ejovyan\u003c/code\u003e, the value from the \u003ccode\u003e$JUPYTERHUB_USER\u003c/code\u003e variable is automatically used.\u003c/p\u003e\n\u003cp\u003eSee here for more information:\n\u003ca href=\"https://jupyter.readthedocs.io/en/latest/community/content-community.html#what-is-a-jovyan\" rel=\"nofollow\"\u003ehttps://jupyter.readthedocs.io/en/latest/community/content-community.html#what-is-a-jovyan\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-create-nbgrader_configpy\" class=\"anchor\" href=\"#create-nbgrader_configpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate nbgrader_config.py\u003c/h3\u003e\n\u003cp\u003eSee here: \u003ca href=\"https://nbgrader.readthedocs.io/en/stable/configuration/nbgrader_config.html#use-case-3-nbgrader-and-jupyterhub\" rel=\"nofollow\"\u003ehttps://nbgrader.readthedocs.io/en/stable/configuration/nbgrader_config.html#use-case-3-nbgrader-and-jupyterhub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTo make \u003cem\u003enbgrader_config.py\u003c/em\u003e available in the container, just append the file in \u003ccode\u003e$singularity_bind_extra\u003c/code\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-security-precautions\" class=\"anchor\" href=\"#security-precautions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSecurity Precautions\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-host-filesystems\" class=\"anchor\" href=\"#singularity-host-filesystems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Host Filesystems\u003c/h3\u003e\n\u003cp\u003eIn case you are using Singularity, the host file system may be automatically mounted into the container when you start a Singularity Container.\u003c/p\u003e\n\u003cp\u003eA possible cause is the option \u003ccode\u003emount hostfs\u003c/code\u003e in \u003cem\u003esingularity.conf\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eSee here: \u003ca href=\"https://sylabs.io/guides/3.5/admin-guide/configfiles.html#singularity-conf\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/admin-guide/configfiles.html#singularity-conf\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyterhub-api-https\" class=\"anchor\" href=\"#jupyterhub-api-https\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyterHub API (HTTPS)\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-https\" class=\"anchor\" href=\"#https\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHTTPS\u003c/h4\u003e\n\u003cp\u003eSee here for more information:\n\u003ca href=\"https://jupyterhub.readthedocs.io/en/stable/reference/websecurity.html\" rel=\"nofollow\"\u003ehttps://jupyterhub.readthedocs.io/en/stable/reference/websecurity.html\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tunnelbot-user\" class=\"anchor\" href=\"#tunnelbot-user\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etunnelbot user\u003c/h3\u003e\n\u003cp\u003eYou can increase the security by deactivating shell access for this user.\u003c/p\u003e\n\u003cp\u003eJust type:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eusermod -s /bin/false tunnelbot\u003c/pre\u003e\u003c/div\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h2\u003e\n\u003cp\u003eWhen problems occur with the JupyterHub, some information can be obtained from the logs when debug mode is enabled:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jupyterhub/jupyterhub/wiki/Debug-Jupyterhub\"\u003ehttps://github.com/jupyterhub/jupyterhub/wiki/Debug-Jupyterhub\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 7,
    "topics": [
      "jupyter",
      "jupyterhub",
      "hpc",
      "singularity"
    ],
    "updated_at": 1621344647.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "mjboos/voxelwiseencoding",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-voxel-wise-encoding-models-for-bids-datasets-with-naturalistic-stimuli\" class=\"anchor\" href=\"#voxel-wise-encoding-models-for-bids-datasets-with-naturalistic-stimuli\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVoxel-wise encoding models for BIDS datasets with naturalistic stimuli\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis BIDS App lets you train voxelwise-encoding models for continuous (naturalistic) stimuli provided as a BIDS-compliant continuous recording file.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mjboos/voxelwiseencoding/workflows/CI/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/mjboos/voxelwiseencoding/workflows/CI/badge.svg\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://raw.githubusercontent.com/mjboos/voxelwiseencoding/master/scheme_BIDS_encoding.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/mjboos/voxelwiseencoding/master/scheme_BIDS_encoding.png\" alt=\"Schema for voxel-wise encoding models using a BIDS dataset\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor more information about the specification of BIDS Apps see \u003ca href=\"https://docs.google.com/document/d/1E1Wi5ONvOVVnGhj21S1bmJJ4kyHFT7tkxnV3C23sjIE/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\nFor auditory stimuli \u003ca href=\"https://github.com/mjboos/audio2bidsstim/\"\u003ethis\u003c/a\u003e module can help you convert your wav file to a BIDS stimulus representation.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eIf you are only interested in using the Python module for preprocessing fMRI, lagging the stimulus, and training encoding models without the BIDS app, you can install this library by running \u003ccode\u003epython setup.py\u003c/code\u003e or \u003ccode\u003epip install -e voxelwiseencoding\u003c/code\u003e.\nYou can use the BIDS app either via Docker or directly by calling \u003ccode\u003erun.py\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThis library allows you to train and validate voxel-wise encoding models for a BIDS dataset with a BIDS-compliant stimulus representation. See below for an example on how to use it.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h3\u003e\n\u003cp\u003eWe are going to use \u003ca href=\"https://openneuro.org/datasets/ds002322/versions/1.0.4\" rel=\"nofollow\"\u003ethis\u003c/a\u003e dataset to demonstrate an example workflow using the Python package.\u003c/p\u003e\n\u003cp\u003eFirst we need to download the data and extract a stimulus representation:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e#hide_output\u003c/span\u003e\n!a\u003cspan class=\"pl-s1\"\u003ews\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003es3\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esync\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eno\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esign\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003erequest\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003es3\u003c/span\u003e:\u003cspan class=\"pl-c1\"\u003e//\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eopenneuro\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eorg\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eds002322\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eds002322\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edownload\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# these are the parameters for extracting a Mel spectrogram\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# for computational ease in this example we want 1 sec segments of 31 Mel frequencies with a max frequency of * KHz\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003emel_params\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e {\u003cspan class=\"pl-s\"\u003e\u0027n_mels\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e31\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027sr\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e16000\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027hop_length\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e16000\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027n_fft\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e16000\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027fmax\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e8000\u003c/span\u003e}\n\u003cspan class=\"pl-k\"\u003ewith\u003c/span\u003e \u003cspan class=\"pl-en\"\u003eopen\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\u0027config.json\u0027\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027w+\u0027\u003c/span\u003e) \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003efl\u003c/span\u003e:\n    \u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003edump\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003emel_params\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efl\u003c/span\u003e)\n\n!g\u003cspan class=\"pl-s1\"\u003eit\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eclone\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"pl-c1\"\u003e//\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003egithub\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ecom\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003emjboos\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eaudio2bidsstim\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\n!p\u003cspan class=\"pl-s1\"\u003eip\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003einstall\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003er\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eaudio2bidsstim\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003erequirements\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003etxt\u003c/span\u003e\n!p\u003cspan class=\"pl-s1\"\u003eython\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eaudio2bidsstim\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ewav_files_to_bids_tsv\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003epy\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eds002322\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edownload\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003estimuli\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-v\"\u003eDownTheRabbitHoleFinal_mono_exp120_NR16_pad\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ewav\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ec\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe then need to copy the extracted stimulus representation into the BIDS folder.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e!c\u003cspan class=\"pl-s1\"\u003ep\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eDownTheRabbitHoleFinal_mono_exp120_NR16_pad\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003etsv\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003egz\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eds002322\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edownload\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ederivatives\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003etask\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ealice_stim\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003etsv\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003egz\u003c/span\u003e\n!c\u003cspan class=\"pl-s1\"\u003ep\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eDownTheRabbitHoleFinal_mono_exp120_NR16_pad\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eds002322\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edownload\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ederivatives\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esub\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e22\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esub\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e22_\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003etask\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ealice_stim\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd, lastly, because for this dataset the derivatives folder is missing timing information for the BOLD files - we are only interested in the TR - we have to copy that as well.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e!c\u003cspan class=\"pl-s1\"\u003ep\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eds002322\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edownload\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esub\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e22\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esub\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e22_\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003etask\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ealice_bold\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eds002322\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edownload\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ederivatives\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esub\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e22\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esub\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e22_\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003etask\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ealice_bold\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe are now ready to define some model parameters and train the encoding model.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003evoxelwiseencoding\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eprocess_bids\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erun_model_for_subject\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# these are the parameters used for preprocessing the BOLD fMRI files\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003ebold_prep_params\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e {\u003cspan class=\"pl-s\"\u003e\u0027standardize\u0027\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u0027zscore\u0027\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027detrend\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e}\n\n\u003cspan class=\"pl-c\"\u003e# and for lagging the stimulus as well - we want to include 6 sec stimulus segments to predict fMRI\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003elagging_params\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e {\u003cspan class=\"pl-s\"\u003e\u0027lag_time\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e}\n\n\u003cspan class=\"pl-c\"\u003e# these are the parameters for sklearn\u0027s Ridge estimator\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eridge_params\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e {\u003cspan class=\"pl-s\"\u003e\u0027alphas\u0027\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e1e-1\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e100\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e],\n                \u003cspan class=\"pl-s\"\u003e\u0027cv\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027normalize\u0027\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e}\n\n\n\u003cspan class=\"pl-s1\"\u003eridges\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003escores\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ecomputed_mask\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-en\"\u003erun_model_for_subject\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\u002722\u0027\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027/data/ds002322-download/derivatives\u0027\u003c/span\u003e,\n                                                      \u003cspan class=\"pl-s1\"\u003etask\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027alice\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emask\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027epi\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ebold_prep_kwargs\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ebold_prep_params\u003c/span\u003e,\n                                                      \u003cspan class=\"pl-s1\"\u003epreprocess_kwargs\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003elagging_params\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003eencoding_kwargs\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003eridge_params\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe can now assess the quality out-of-sample prediction (in terms of product-moment correlations) of our models and visualize where we can predict well.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enilearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003emasking\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eunmask\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enilearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eplotting\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eplot_stat_map\u003c/span\u003e\n\u003cspan class=\"pl-en\"\u003eplot_stat_map\u003c/span\u003e(\u003cspan class=\"pl-en\"\u003eunmask\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003escores\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003emean\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eaxis\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e), \u003cspan class=\"pl-s1\"\u003ecomputed_mask\u003c/span\u003e), \u003cspan class=\"pl-s1\"\u003ethreshold\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e0.1\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;nilearn.plotting.displays.OrthoSlicer at 0x7fbbd11a2320\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"docs/images/output_11_1.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/output_11_1.png\" alt=\"png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://mjboos.github.io/voxelwiseencoding\" rel=\"nofollow\"\u003eSee here\u003c/a\u003e for further documentation about the Python package and consult the section \"Using this module as a BIDS app\" about how to use this library as a Docker image/BIDS app.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-report-errors\" class=\"anchor\" href=\"#how-to-report-errors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to report errors\u003c/h2\u003e\n\u003cp\u003eIf you encounter errors with this code or have any questions about its uage, please open an issue on the Github repository \u003ca href=\"https://github.com/mjboos/voxelwiseencoding/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1614124528.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity_debug"
    ],
    "full_name": "mgroth0/dnn",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003egit clone --recurse-submodules \u003ca href=\"https://github.com/mgroth0/dnn\"\u003ehttps://github.com/mgroth0/dnn\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003einstall \u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003eminiconda\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda update conda\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003econda create --name dnn --file requirements.txt\u003c/code\u003e (requirements.txt is currently not working, TODO)\u003c/li\u003e\n\u003cli\u003emight need to separately \u003ccode\u003econda install -c mgroth0 mlib-mgroth0\u003c/code\u003e-- When updating, use \u003ccode\u003econda install --file requirements.txt;\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate dnn\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e./dnn\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGenerate some images, train/test a model, run analyses, and generate plots. Tested on Mac, but not yet on linux/Windows.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e./dnn -cfg=gen_images --INTERACT=0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e./dnn -cfg=test_one --INTERACT=0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe second command will fail with a Mathematica-related error, but your results will be saved in \u003ccode\u003e_figs\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTODO: have to also consider running and developing other executables here: human_exp_1 and human_analyze\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003e-MODE: (default = FULL) is a string that can contain any combination of the following (example: \"CLEAN JUSTRUN\")\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLEAN\u003c/li\u003e\n\u003cli\u003eJUSTRUN\u003c/li\u003e\n\u003cli\u003eGETANDMAKE\u003c/li\u003e\n\u003cli\u003eMAKEREPORT\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEdit \u003ca href=\"\"\u003ecfg.yml\u003c/a\u003e to save configuration options. Feel free to push these.\u003c/p\u003e\n\u003cp\u003eIf there is anything hardcoded that you\u0027d like to be configurable, please submit an issue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h2\u003e\n\u003cp\u003etodo\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTODO: have separate development and user modes. Developer mode has PYTHONPATH link to mlib and instructions for resolving and developing in ide in parallel. User mode has mlib as normal dependency. might need to use \u003ccode\u003econda uninstall mlib-mgroth0 --force\u003c/code\u003e. Also in these public readmes or reqs.txt I have to require a specific mlib version\u003c/li\u003e\n\u003cli\u003e./dnn build\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eDarius, Xavier, Pawan\u003c/p\u003e\n\u003cp\u003eheuritech, raghakot, joel\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617456499.0
  },
  {
    "data_format": 2,
    "description": "Milwaukee School of Engineering ROSIE Supercomputer User Guide and Compute Resources",
    "filenames": [
      "singularity/Singularity.PyTorch.def",
      "singularity/Singularity.DrivSim.tf1.def",
      "singularity/Singularity.cs3450.def",
      "singularity/Singularity.DataScience.def",
      "singularity/Singularity.Tensorflow.v1.def",
      "singularity/Singularity.DeepstreamSDK.def",
      "singularity/Singularity.Tensorflow.v2.def",
      "docs/cli/Singularity.md"
    ],
    "full_name": "gagandaroach/rosie",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003egit clone --recurse-submodules \u003ca href=\"https://github.com/mgroth0/dnn\"\u003ehttps://github.com/mgroth0/dnn\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003einstall \u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003eminiconda\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda update conda\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003econda create --name dnn --file requirements.txt\u003c/code\u003e (requirements.txt is currently not working, TODO)\u003c/li\u003e\n\u003cli\u003emight need to separately \u003ccode\u003econda install -c mgroth0 mlib-mgroth0\u003c/code\u003e-- When updating, use \u003ccode\u003econda install --file requirements.txt;\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate dnn\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e./dnn\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGenerate some images, train/test a model, run analyses, and generate plots. Tested on Mac, but not yet on linux/Windows.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e./dnn -cfg=gen_images --INTERACT=0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e./dnn -cfg=test_one --INTERACT=0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe second command will fail with a Mathematica-related error, but your results will be saved in \u003ccode\u003e_figs\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTODO: have to also consider running and developing other executables here: human_exp_1 and human_analyze\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003e-MODE: (default = FULL) is a string that can contain any combination of the following (example: \"CLEAN JUSTRUN\")\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLEAN\u003c/li\u003e\n\u003cli\u003eJUSTRUN\u003c/li\u003e\n\u003cli\u003eGETANDMAKE\u003c/li\u003e\n\u003cli\u003eMAKEREPORT\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEdit \u003ca href=\"\"\u003ecfg.yml\u003c/a\u003e to save configuration options. Feel free to push these.\u003c/p\u003e\n\u003cp\u003eIf there is anything hardcoded that you\u0027d like to be configurable, please submit an issue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h2\u003e\n\u003cp\u003etodo\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTODO: have separate development and user modes. Developer mode has PYTHONPATH link to mlib and instructions for resolving and developing in ide in parallel. User mode has mlib as normal dependency. might need to use \u003ccode\u003econda uninstall mlib-mgroth0 --force\u003c/code\u003e. Also in these public readmes or reqs.txt I have to require a specific mlib version\u003c/li\u003e\n\u003cli\u003e./dnn build\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eDarius, Xavier, Pawan\u003c/p\u003e\n\u003cp\u003eheuritech, raghakot, joel\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1615357040.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "Clinical-Genomics-Lund/nextflow_tumwgs",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003egit clone --recurse-submodules \u003ca href=\"https://github.com/mgroth0/dnn\"\u003ehttps://github.com/mgroth0/dnn\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003einstall \u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003eminiconda\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda update conda\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003econda create --name dnn --file requirements.txt\u003c/code\u003e (requirements.txt is currently not working, TODO)\u003c/li\u003e\n\u003cli\u003emight need to separately \u003ccode\u003econda install -c mgroth0 mlib-mgroth0\u003c/code\u003e-- When updating, use \u003ccode\u003econda install --file requirements.txt;\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate dnn\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e./dnn\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGenerate some images, train/test a model, run analyses, and generate plots. Tested on Mac, but not yet on linux/Windows.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e./dnn -cfg=gen_images --INTERACT=0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e./dnn -cfg=test_one --INTERACT=0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe second command will fail with a Mathematica-related error, but your results will be saved in \u003ccode\u003e_figs\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTODO: have to also consider running and developing other executables here: human_exp_1 and human_analyze\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003e-MODE: (default = FULL) is a string that can contain any combination of the following (example: \"CLEAN JUSTRUN\")\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLEAN\u003c/li\u003e\n\u003cli\u003eJUSTRUN\u003c/li\u003e\n\u003cli\u003eGETANDMAKE\u003c/li\u003e\n\u003cli\u003eMAKEREPORT\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEdit \u003ca href=\"\"\u003ecfg.yml\u003c/a\u003e to save configuration options. Feel free to push these.\u003c/p\u003e\n\u003cp\u003eIf there is anything hardcoded that you\u0027d like to be configurable, please submit an issue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h2\u003e\n\u003cp\u003etodo\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTODO: have separate development and user modes. Developer mode has PYTHONPATH link to mlib and instructions for resolving and developing in ide in parallel. User mode has mlib as normal dependency. might need to use \u003ccode\u003econda uninstall mlib-mgroth0 --force\u003c/code\u003e. Also in these public readmes or reqs.txt I have to require a specific mlib version\u003c/li\u003e\n\u003cli\u003e./dnn build\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eDarius, Xavier, Pawan\u003c/p\u003e\n\u003cp\u003eheuritech, raghakot, joel\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1619189676.0
  },
  {
    "data_format": 2,
    "description": "Using singularity to create a near-identical uppmax environment (experimental)",
    "filenames": [
      "Singularity",
      "tags/200311/Singularity.200311"
    ],
    "full_name": "UPPMAX/uppmax_in_a_can",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-uppmax-in-a-can\" class=\"anchor\" href=\"#uppmax-in-a-can\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUPPMAX in a can\u003c/h1\u003e\n\u003cp\u003eThis Singularity container will let you run a near-identical UPPMAX environment on your own computer. You will have access to all of the installed software at UPPMAX, all your files and reference data on UPPMAX, but it will be your own computer that does the calculations. You can even use it to analyse data that you only have on your own computer, but using the software and reference data on UPPMAX.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-typical-use-cases\" class=\"anchor\" href=\"#typical-use-cases\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTypical use cases\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eYou have sensitive data that is not allowed to leave your computers, but you want to use the programs and references at UPPMAX to do the analysis.\u003c/li\u003e\n\u003cli\u003eYou have your own server but want to avoid installing all the software yourself.\u003c/li\u003e\n\u003cli\u003eThe queues at UPPMAX are too long or you have run out of core hours but you want the analysis done yesterday.\u003c/li\u003e\n\u003cli\u003eYou have your data and compute hours at another SNIC centre, but want to use the software installed at UPPMAX.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-what-you-get\" class=\"anchor\" href=\"#what-you-get\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat you get\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eAccess to your home folder and all project folders.\u003c/li\u003e\n\u003cli\u003eAccess to all the installed programs at UPPMAX.\u003c/li\u003e\n\u003cli\u003eAccess to all reference data at UPPMAX.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-what-you-dont-get\" class=\"anchor\" href=\"#what-you-dont-get\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat you don\u0027t get\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eUPPMAX high-performace computers. You will be limited by the computer you are running the container on.\u003c/li\u003e\n\u003cli\u003eNo slurm access. Everything runs on your computer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-important-notes\" class=\"anchor\" href=\"#important-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImportant notes\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eSince all data you read/write to the UPPMAX file system will have to travel over the internet, disk intensive programs will be much slower, and transfer rate is limited to your internet connection. Try working on your local harddrive as much as possible.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-issues\" class=\"anchor\" href=\"#issues\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIssues\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eSome of the tools at uppmax, like projinfo and uquota are using your user\u0027s group membership to determin what information to show. Since your local user won\u0027t have the same groups as you have on uppmax they misbehave and will not show you the same information as when you run the program on uppmax.\u003c/li\u003e\n\u003cli\u003eIf your user has not connected to uppmax before it will need to accept the login nodes fingerprint before connecting (a security feature). Sshfs will just hang if this happens, so you will have to connect normally to uppmax and manually accept the fingerprint. To make things more complicated there are 3 different login nodes, each with its own fingerprint, and you are randomly assigned to one. There is a possibility to turn this off in the sshfs mount command by adding \u003ccode\u003e-o StrictHostKeyChecking=no\u003c/code\u003e, but it might not be a good idea to disable it due to security. Easiest way around it is just to run \u003ccode\u003essh user@rackham.uppmax.uu.se\u003c/code\u003e a couple of times until you have seen all 3 fingerprints.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h1\u003e\n\u003cp\u003eThis tool has been developed on Ubuntu 18.04 with Singularity v.3.5. You should only need 2 things for this to work,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html\" rel=\"nofollow\"\u003eSingularity (only tested with v3.5)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/libfuse/sshfs\"\u003eSSHFS\u003c/a\u003e\u003cbr\u003e\n(Make sure that the option \u003ccode\u003euser_allow_other\u003c/code\u003e is uncommented in \u003ccode\u003e/etc/fuse.conf\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor installation instructions for these, see respective projects homepage.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-a-pre-built-image\" class=\"anchor\" href=\"#using-a-pre-built-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing a pre-built image\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull shub://UPPMAX/uppmax_in_a_can:latest\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-your-own-image-from-github\" class=\"anchor\" href=\"#building-your-own-image-from-github\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding your own image from github.\u003c/h3\u003e\n\u003cp\u003eClone the github repo and build the image:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/UPPMAX/uppmax_in_a_can.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e uppmax_in_a_can\nsingularity build uppmax_in_a_can_latest.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe build takes 10-20 minutes on a modern laptop with gigabit Ethernet.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-first-time-usage\" class=\"anchor\" href=\"#first-time-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFirst time usage\u003c/h2\u003e\n\u003cp\u003eOnce the build is done, run the initialization script in the container,\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./uppmax_in_a_can_latest.sif uppmax_init\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e to see more options, run\u003c/span\u003e\n./uppmax_in_a_can_latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-subsequent-usage\" class=\"anchor\" href=\"#subsequent-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubsequent usage\u003c/h2\u003e\n\u003cp\u003eStart the virtual node:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./uiac_node.sh -i uppmax_in_a_can_latest.sif\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e to see more options, run\u003c/span\u003e\n./uiac_node.sh -h\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou will now be on the command-line inside the container and you can run commands as if you were logged in on UPPMAX. You will see all project folders in \u003ccode\u003e/proj\u003c/code\u003e, all software in \u003ccode\u003e/sw\u003c/code\u003e, your UPPMAX home folder in \u003ccode\u003e/home/\u0026lt;UPPMAX_username\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e ex\u003c/span\u003e\nmodule load bioinf-tools samtools\nsamtools view file.bam\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo close down and return to your own computers command-line, just type \u003ccode\u003eexit\u003c/code\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-advanced-usage\" class=\"anchor\" href=\"#advanced-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvanced usage\u003c/h1\u003e\n\u003cp\u003eYou can also specify any additional singularity options to the \u003ccode\u003euiac_node.sh\u003c/code\u003e script. If you want to make your computers file system visible inside the container so that you can analyse files residing on your computer, just add a bind argument:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Entire hard drive \u003c/span\u003e\n./uiac_node.sh -e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e--bind /:/hostfs\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e -i uppmax_in_a_can_latest.sif\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Only a specific directory\u003c/span\u003e\n./uiac_node.sh -e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e--bind /home/user/data:/hostfs\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e -i uppmax_in_a_can_latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis command will make your computers file system available under \u003ccode\u003e/hostfs\u003c/code\u003e (or wherever you would like it).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-developer-notes\" class=\"anchor\" href=\"#developer-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeveloper notes\u003c/h1\u003e\n\u003cp\u003eTo get the list of installed software on UPPMAX, run this command on UPPMAX:\n\u003ccode\u003eyum list installed | cut -f 1 -d \" \" | cut -f 1 -d \".\" | sort \u0026gt; yum_installed.txt\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThis list will contain \u003cstrong\u003eall\u003c/strong\u003e installed packages, even core packages and nvidia packages that are not available in the default repos. It will just give a couple of warning messages when you send the list to \u003ccode\u003eyum install\u003c/code\u003e, but it will not break the installation process.\u003c/p\u003e\n\u003cp\u003eTo get the the list of environment variables in \u003ccode\u003eenv/99-uppmaxvars.sh\u003c/code\u003e, type \u003ccode\u003eenv | egrep -i \u0027module|lmod\u0027 | sort\u003c/code\u003e when logged in on UPPMAX and pick out everything that has to do with the module system.\u003c/p\u003e\n\u003cp\u003eThe PS1 is made to be different from the default shell the user has to make it obvious they are inside the container.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1620056516.0
  },
  {
    "data_format": 2,
    "description": "FetaL AneUploidy and FetalFraction analYsis Pipeline",
    "filenames": [
      "Singularity"
    ],
    "full_name": "J35P312/fluffy",
    "latest_release": "0.6.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg\" alt=\"Build\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/Clinical-Genomics/fluffy\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5a8950551f5fd61495950779e32145a28a2346b9809af6e347b18f58dce06213/68747470733a2f2f636f6465636f762e696f2f67682f436c696e6963616c2d47656e6f6d6963732f666c756666792f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/Clinical-Genomics/fluffy/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-fluffypipe\" class=\"anchor\" href=\"#fluffypipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFluFFyPipe\u003c/h1\u003e\n\u003cp\u003eNIPT analysis pipeline, using WisecondorX for detecting aneuplodies and large CNVs, AMYCNE for FFY and PREFACE for FF prediction (optional). FluFFYPipe produces a variety of output files, as well as a per batch csv summary.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://github.com/J35P312/FluFFyPipe/blob/master/logo/IMG_20200320_132001.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/J35P312/FluFFyPipe/raw/master/logo/IMG_20200320_132001.jpg\" width=\"400\" height=\"400\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-fluffypipe\" class=\"anchor\" href=\"#run-fluffypipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun FluFFyPipe\u003c/h1\u003e\n\u003cp\u003eRun NIPT analysis, using a previously comnputed reference:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; analyse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun NIPT analysis, using an internally computed reference (i.e the reference is built using all samples listed in samplesheet):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; analyse --batch-ref\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eoptionally, skip preface:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; --skip_preface analyse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll output will be written to the output folder, this output includes:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebam files\nwisecondorX output\ntiddit coverage summary\nFetal fraction estimation\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eas well as a summary csv and multiqc html (per batch)\u003c/p\u003e\n\u003cp\u003ethe input folder is a project folder containing one folder per sample, each of these subfolders contain the fastq file(s).\nThe samplesheet contains at least a \"sampleID\" column, the sampleID should match the subfolders in the input folder. The samplesheet may contain other columns, such as flowcell and index folder: such columns will be printed to the summary csv.\nIf the samplesheet contains a SampleName column, fluffy will name the output according to SampleName\u003c/p\u003e\n\u003cp\u003eCreate a WisecondorX reference\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; reference\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003esamplesheet should contain atleast a \"sampleID\" column. All samples in the samplesheet will be used to construct the reference, visit the WisecondorX manual for more information.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-troubleshooting-and-rerun\" class=\"anchor\" href=\"#troubleshooting-and-rerun\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting and rerun\u003c/h1\u003e\n\u003cp\u003eThere are three statuses of the fluffy pipeline:\nrunning, complete, and failed\u003c/p\u003e\n\u003cp\u003eThe status of a fluffy run is found in the\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;output_folder\u0026gt;/analysis_status.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe status of all jobs are listed in\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;output_folder\u0026gt;/sacct/fluffy_\u0026lt;date\u0026gt;.log.status\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhere  is the timepoint when the jobs were submitted\nUse grep to find the failed jobs:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egrep -v COMPLETE \u0026lt;output_folder\u0026gt;/sacct/fluffy_\u0026lt;date\u0026gt;.log.status\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe output logs are stored in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e \u0026lt;output_folder\u0026gt;/logs\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBefore continuing, you may want to generate the summary csv for all completed cases:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash \u0026lt;output_folder\u0026gt;/scripts/summarizebatch-\u0026lt;hash\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere  is a randomly generated string.\u003c/p\u003e\n\u003cp\u003euse the rerun module to rerun failed fluffy analyses:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; --skip_preface rerun\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install-fluffypipe\" class=\"anchor\" href=\"#install-fluffypipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall FluFFyPipe\u003c/h1\u003e\n\u003cp\u003eFluFFyPipe requires python 3, slurm, slurmpy, and singularity, python-coloredlogs.\u003c/p\u003e\n\u003cp\u003efluffy may be installed using pip:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip install fluffy-cg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ealternatively, fluffy is cloned and installed from github:\ngit clone \u003ca href=\"https://github.com/Clinical-Genomics/fluffy\"\u003ehttps://github.com/Clinical-Genomics/fluffy\u003c/a\u003e\ncd fluffy\npip install -e .\u003c/p\u003e\n\u003cp\u003eNext download the FluFFyPipe singularity container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecopy the example config (found in example_config), and edit the variables.\nYou will need to download/create the following files:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eReference fasta (indexed using bwa)\n\nWisecondorX reference files (created using the reference mode)\n\nPREFACE model file (optional)\n\nblacklist bed file (used by wisecondorX)\n\nFluFFyPipe singularity collection (singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe)\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1620326897.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "data-retrieval/XNAT/Singularity.xnat_crawler",
      "analyses/mriqc/Singularity.mriqc",
      "converters/heudiconv/Singularity.heudiconv",
      "postprocessing/defacing/mridefacer/Singularity.mridefacer",
      "postprocessing/fsl/Singularity.fsl"
    ],
    "full_name": "psychoinformatics-de/hirni-toolbox",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/Clinical-Genomics/fluffy/workflows/Build/badge.svg\" alt=\"Build\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/Clinical-Genomics/fluffy\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5a8950551f5fd61495950779e32145a28a2346b9809af6e347b18f58dce06213/68747470733a2f2f636f6465636f762e696f2f67682f436c696e6963616c2d47656e6f6d6963732f666c756666792f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/Clinical-Genomics/fluffy/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-fluffypipe\" class=\"anchor\" href=\"#fluffypipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFluFFyPipe\u003c/h1\u003e\n\u003cp\u003eNIPT analysis pipeline, using WisecondorX for detecting aneuplodies and large CNVs, AMYCNE for FFY and PREFACE for FF prediction (optional). FluFFYPipe produces a variety of output files, as well as a per batch csv summary.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://github.com/J35P312/FluFFyPipe/blob/master/logo/IMG_20200320_132001.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/J35P312/FluFFyPipe/raw/master/logo/IMG_20200320_132001.jpg\" width=\"400\" height=\"400\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-fluffypipe\" class=\"anchor\" href=\"#run-fluffypipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun FluFFyPipe\u003c/h1\u003e\n\u003cp\u003eRun NIPT analysis, using a previously comnputed reference:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; analyse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun NIPT analysis, using an internally computed reference (i.e the reference is built using all samples listed in samplesheet):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; analyse --batch-ref\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eoptionally, skip preface:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; --skip_preface analyse\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll output will be written to the output folder, this output includes:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebam files\nwisecondorX output\ntiddit coverage summary\nFetal fraction estimation\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eas well as a summary csv and multiqc html (per batch)\u003c/p\u003e\n\u003cp\u003ethe input folder is a project folder containing one folder per sample, each of these subfolders contain the fastq file(s).\nThe samplesheet contains at least a \"sampleID\" column, the sampleID should match the subfolders in the input folder. The samplesheet may contain other columns, such as flowcell and index folder: such columns will be printed to the summary csv.\nIf the samplesheet contains a SampleName column, fluffy will name the output according to SampleName\u003c/p\u003e\n\u003cp\u003eCreate a WisecondorX reference\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; reference\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003esamplesheet should contain atleast a \"sampleID\" column. All samples in the samplesheet will be used to construct the reference, visit the WisecondorX manual for more information.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-troubleshooting-and-rerun\" class=\"anchor\" href=\"#troubleshooting-and-rerun\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting and rerun\u003c/h1\u003e\n\u003cp\u003eThere are three statuses of the fluffy pipeline:\nrunning, complete, and failed\u003c/p\u003e\n\u003cp\u003eThe status of a fluffy run is found in the\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;output_folder\u0026gt;/analysis_status.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe status of all jobs are listed in\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;output_folder\u0026gt;/sacct/fluffy_\u0026lt;date\u0026gt;.log.status\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhere  is the timepoint when the jobs were submitted\nUse grep to find the failed jobs:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egrep -v COMPLETE \u0026lt;output_folder\u0026gt;/sacct/fluffy_\u0026lt;date\u0026gt;.log.status\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe output logs are stored in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e \u0026lt;output_folder\u0026gt;/logs\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBefore continuing, you may want to generate the summary csv for all completed cases:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash \u0026lt;output_folder\u0026gt;/scripts/summarizebatch-\u0026lt;hash\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere  is a randomly generated string.\u003c/p\u003e\n\u003cp\u003euse the rerun module to rerun failed fluffy analyses:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efluffy --sample \u0026lt;samplesheet\u0026gt;  --project \u0026lt;input_folder\u0026gt; --out \u0026lt;output_folder\u0026gt; --skip_preface rerun\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install-fluffypipe\" class=\"anchor\" href=\"#install-fluffypipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall FluFFyPipe\u003c/h1\u003e\n\u003cp\u003eFluFFyPipe requires python 3, slurm, slurmpy, and singularity, python-coloredlogs.\u003c/p\u003e\n\u003cp\u003efluffy may be installed using pip:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip install fluffy-cg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ealternatively, fluffy is cloned and installed from github:\ngit clone \u003ca href=\"https://github.com/Clinical-Genomics/fluffy\"\u003ehttps://github.com/Clinical-Genomics/fluffy\u003c/a\u003e\ncd fluffy\npip install -e .\u003c/p\u003e\n\u003cp\u003eNext download the FluFFyPipe singularity container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecopy the example config (found in example_config), and edit the variables.\nYou will need to download/create the following files:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eReference fasta (indexed using bwa)\n\nWisecondorX reference files (created using the reference mode)\n\nPREFACE model file (optional)\n\nblacklist bed file (used by wisecondorX)\n\nFluFFyPipe singularity collection (singularity pull --name FluFFyPipe.sif shub://J35P312/FluFFyPipe)\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1616079674.0
  },
  {
    "data_format": 2,
    "description": "CTA-customized version of the DIRAC middleware",
    "filenames": [
      "Singularity"
    ],
    "full_name": "cta-observatory/CTADIRAC",
    "latest_release": "v1r62test",
    "readme": "\u003cp\u003eCTADIRAC project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emoved to CTAO Observatory gitlab @ \u003ca href=\"https://gitlab.cta-observatory.org/cta-computing/dpps/CTADIRAC\" rel=\"nofollow\"\u003ehttps://gitlab.cta-observatory.org/cta-computing/dpps/CTADIRAC\u003c/a\u003e on Decembe 2020\u003c/li\u003e\n\u003cli\u003emoved to git on Septembre 5th 2017\u003c/li\u003e\n\u003cli\u003eadd things, need add a licence GPLv3 ?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAuthors from svn:\u003cbr\u003e\nAdrian Casajus \u003ca href=\"mailto:adria@ecm.ub.es\"\u003eadria@ecm.ub.es\u003c/a\u003e \u003cbr\u003e\nLuisa Arrabito \u003ca href=\"mailto:arrabito@in2p3.fr\"\u003earrabito@in2p3.fr\u003c/a\u003e \u003cbr\u003e\nJohan Bregeon \u0026lt;\u003ca href=\"mailto:bregeon@.in2p3.fr\"\u003ebregeon@.in2p3.fr\u003c/a\u003e\u0026gt; \u003cbr\u003e\nJohann Cohen Tanugi \u003ca href=\"mailto:johann.cohen-tanugi@umontpellier.fr\"\u003ejohann.cohen-tanugi@umontpellier.fr\u003c/a\u003e \u003cbr\u003e\n? Han Bcn \u003ca href=\"mailto:nhan.bcn@gmail.com\"\u003enhan.bcn@gmail.com\u003c/a\u003e \u003cbr\u003e\nRicardo Graciani \u003ca href=\"mailto:graciani@ecm.ub.edu\"\u003egraciani@ecm.ub.edu\u003c/a\u003e \u003cbr\u003e\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1609949938.0
  },
  {
    "data_format": 2,
    "description": "Think-Play-Hack: World Views",
    "filenames": [
      "containers/python/Singularity",
      "containers/r/Singularity"
    ],
    "full_name": "SouthernMethodistUniversity/think-play-hack",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-think-play-hack-world-views\" class=\"anchor\" href=\"#think-play-hack-world-views\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThink-Play-Hack: World Views\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-preparatory-readings\" class=\"anchor\" href=\"#preparatory-readings\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreparatory Readings\u003c/h2\u003e\n\u003cp\u003eDr. Guldi has compiled a list of readings to get your creative juices flowing. You can find them \u003ca href=\"https://www.dropbox.com/sh/ru4dxh6rr6uqvfl/AADlPVWVEZ1BE4OcxPnZ0dpDa?dl=0\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-conference-activities\" class=\"anchor\" href=\"#conference-activities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConference activities\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/document/d/1PdxiKuEQFj0KIQbHnvfthqG7gtBYNlN3QZavc5g1T9M/edit?usp=sharing\" rel=\"nofollow\"\u003eWednesday hike\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/document/d/1xDC_5mEJOZvfcWAvARDaLFl68Y44cDRh7jZJjpfUVKM/edit?usp=sharing\" rel=\"nofollow\"\u003eThursday rafting trip\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese two activities are here because they happen to be more technically complex. There are other opportunities that are being informally discussed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-slack-instructions\" class=\"anchor\" href=\"#slack-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://get.slack.help/hc/en-us/articles/212675257\" rel=\"nofollow\"\u003eSlack Instructions\u003c/a\u003e\n\u003c/h2\u003e\n\u003cp\u003eYou likely recieved an invitation to our Slack channel. This will be a good way to communicate with people at the conference and ask the Data Team questions if you need to. If you did not get an invite, just ask and we can send you one.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-github-instructions\" class=\"anchor\" href=\"#github-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://guides.github.com/activities/hello-world/\"\u003eGitHub Instructions\u003c/a\u003e\n\u003c/h2\u003e\n\u003cp\u003eThough not required to use this repository, having an account on GitHub is a good idea for any programmer. It provides you with a portfolio of projects you have worked on as well as a way to collaborate with other coders.\u003c/p\u003e\n\u003cp\u003eIt will also allow you to clone this repository as well as add issues and suggest changes for the data team.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-software\" class=\"anchor\" href=\"#software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware\u003c/h2\u003e\n\u003cp\u003eWe have ready-to-go software stacks for Python with Jupyter and R with RStudio.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-docker-setup-for-personal-machines\" class=\"anchor\" href=\"#docker-setup-for-personal-machines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"docs/docker.md\"\u003eDocker Setup for Personal Machines\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003eDocker is a tool that allows software to run on your computer without actually needing to install the full software. These instructions will guide you through setting up Docker and getting our image running on your personal machine.\u003c/p\u003e\n\u003cp\u003eWe have provided two images: one that runs R and RStudio and one that runs Python, Jupyter Notebooks and JupyterLab.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-maneframe-ii-m2\" class=\"anchor\" href=\"#using-maneframe-ii-m2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"docs/m2.md\"\u003eUsing ManeFrame II (M2)\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://faculty.smu.edu/csc/documentation/about.html\" rel=\"nofollow\"\u003eManeFrame II (M2)\u003c/a\u003e is SMU\u0027s high performance computing (HPC) cluster. M2 features 11,000 cores, 60 NVIDIA V100 and P100 GPU accelerators, and 256 GB, 768 GB, and 1.5 TB memory configurations per node. Guest accounts on the cluster can be requested \u003ca href=\"https://smu.az1.qualtrics.com/jfe/form/SV_2i6o7BztWg52rK5\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-data\" class=\"anchor\" href=\"#data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data-on-box\" class=\"anchor\" href=\"#data-on-box\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://smu.box.com/s/lk8mqfbgjproqda5jmlbynnbjclvybar\" rel=\"nofollow\"\u003eData on Box\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003eWe have provided access to all the data for the event on Box. Given the size, consider what you might want to work on prior to downloading it. Should you have trouble, we have flash drives and hard drives with the data stored locally as well.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-reddit\" class=\"anchor\" href=\"#reddit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"docs/reddit.md\"\u003eReddit\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003eWe have over 1 TB of reddit data available in a database. You can \u003ca href=\"docs/reddit.md\"\u003eget subsets of this data\u003c/a\u003e for analysis.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-think-prompts\" class=\"anchor\" href=\"#think-prompts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThink Prompts\u003c/h2\u003e\n\u003cp\u003eIn case you are having trouble getting started:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCan one imagine developing a method to trace narrative elements across genres? How would you formalize \"narrative element\"?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow do we align narrative elements with aspects of cultural ideology (norms, beliefs, values)?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhat aspects of storytelling can we map? To what end? (i.e. can you imagine a new historic-geographic methodology?)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhat impact do popular films (e.g. Snow White and the 7 Dwarves) have on traditional tales (and vice versa)?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCan we discover/trace the impact of traditional stories on literary works such as The Hobbit? On films?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhere did you find that lovely frosted mug filled with such an alluringly amber-hewed frothy brew? Were there nuts as well?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1566358077.0
  },
  {
    "data_format": 2,
    "description": "Install methods for UPPMAX modules plus some helper scripts",
    "filenames": [
      "singularity_info/metaWRAP_1.3.2/Singularity.metaWRAP",
      "singularity_info/gapseq-RT-227932/Singularity.gapseq"
    ],
    "full_name": "UPPMAX/install-methods",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-module-installation-methods\" class=\"anchor\" href=\"#module-installation-methods\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eModule Installation Methods\u003c/h1\u003e\n\u003cp\u003eThis is a collection of READMEs generated during installation of software\napplications on Uppmax clusters.  It is incomplete in terms of modules\navailable on Uppmax, and the individual READMEs may also be incomplete in terms\nof what was actually done to install the modules.  We are publicising these in\nthe hopes that they can be helpful.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-workflow-of-a-basic-installation\" class=\"anchor\" href=\"#example-workflow-of-a-basic-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample workflow of a basic installation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone the install methods git repo (\u003ccode\u003egit clone https://github.com/UPPMAX/install-methods.git\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eAdd the repo to your \u003ccode\u003e$PATH\u003c/code\u003e and source the \u003ccode\u003euppmax_functions.sh\u003c/code\u003e file to get access to the functions.\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003erun_makeroom\u003c/code\u003e with at least \u003ccode\u003e-t\u003c/code\u003e and \u003ccode\u003e-v\u003c/code\u003e, to generate a \u003ccode\u003e.sh\u003c/code\u003e (\u003ccode\u003emakeroom_toolname_version.sh\u003c/code\u003e) file that will create the directory structure needed in \u003ccode\u003e/sw\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun the \u003ccode\u003e.sh\u003c/code\u003e file created in the directory you are standing to create the directory structure (\u003ccode\u003e/sw/category/toolname/\u003c/code\u003e and \u003ccode\u003e/sw/mf/common/category\u003c/code\u003e) and template files.\u003c/li\u003e\n\u003cli\u003ePut the source code for the program in \u003ccode\u003e/sw/category/toolname/version/src\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eCompile and/or install the tool in \u003ccode\u003e/sw/category/toolname/version/cluster/bin\u003c/code\u003e etc.\u003c/li\u003e\n\u003cli\u003eEdit the readme file, explaining how you did the installation, in \u003ccode\u003e/sw/category/toolname/toolname-version_install-README.md\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eEdit the template module file \u003ccode\u003e/sw/category/toolname/mf/version\u003c/code\u003e to do what you want when the module loads.\u003c/li\u003e\n\u003cli\u003eCopy the module file to the live location, \u003ccode\u003e/sw/mf/common/category/[section]/toolname\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003eall_mflink toolname version\u003c/code\u003e to create links for all clusters to the module file in \u003ccode\u003e/sw/mf/common/category/[section]/toolname\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003efixup /sw/category/toolname/version /sw/mf/common/category/[section]/toolname\u003c/code\u003e to make sure the ownership and permissions are ok.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-scripts\" class=\"anchor\" href=\"#scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eScripts\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003egather-READMEs.sh\u003c/code\u003e - bash script to scan installation directories, looking for\nREADME files having a particular filename format that we create during\ninstallation of tools\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003efixup\u003c/code\u003e - bash script fixing up permissions and group membership within\ninstallation trees; our local installation group is \u003ccode\u003esw\u003c/code\u003e. With the \u003ccode\u003e-g\u003c/code\u003e option,\nthis script will \u003ccode\u003echmod g+s\u003c/code\u003e directories in the tree, too.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003euppmax_functions.sh\u003c/code\u003e - bash helper functions for SLURM job viewing and various\nmodule-related tasks, mostly to do with setting up mf files for loading\nmodules; the latter require appexpert privileges.  Source these from \u003ccode\u003e.bashrc\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-directories\" class=\"anchor\" href=\"#installation-directories\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation directories\u003c/h2\u003e\n\u003cp\u003eThe directories contain software installations in major subject areas.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-apps\" class=\"anchor\" href=\"#apps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eapps/\u003c/h3\u003e\n\u003cp\u003eGeneral applications.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-appsbioinfo\" class=\"anchor\" href=\"#appsbioinfo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eapps/bioinfo/\u003c/h3\u003e\n\u003cp\u003eBioinformatics applications.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-libs\" class=\"anchor\" href=\"#libs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elibs/\u003c/h3\u003e\n\u003cp\u003eLibraries.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-comp\" class=\"anchor\" href=\"#comp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecomp/\u003c/h3\u003e\n\u003cp\u003eCompilers, interpreters, build tools.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-database-directories\" class=\"anchor\" href=\"#database-directories\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDatabase directories\u003c/h2\u003e\n\u003cp\u003eThese directories cover installations of databases updated either manually, or via update scripts.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data_uppnex\" class=\"anchor\" href=\"#data_uppnex\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edata_uppnex/\u003c/h3\u003e\n\u003cp\u003eInstallation instructions for databases under \u003ccode\u003e/sw/data/uppnex/\u003c/code\u003e.  Database\ndirectories containing \u003ccode\u003e*-install-README.md\u003c/code\u003e files are updated manually.\nDatabase directories containing \u003ccode\u003e*-db-README.md\u003c/code\u003e files and scripts (currently,\n\u003ccode\u003eKraken\u003c/code\u003e, \u003ccode\u003ediamond_databases\u003c/code\u003e and \u003ccode\u003eRTG\u003c/code\u003e) are updated monthly via crontab entries.\u003c/p\u003e\n\u003cp\u003eBlast database updates are included here, and involve multiple scripts, crontab\nentries and a test directory.  These are updated monthly via crontab entries.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data_other\" class=\"anchor\" href=\"#data_other\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edata_other/\u003c/h3\u003e\n\u003cp\u003eInstallation instructions for databases under other locations, currently just\n\u003ccode\u003eBUSCO\u003c/code\u003e lineage sets, which are kept in the module installation directory.\nThese are updated monthly via crontab entries.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1621891662.0
  },
  {
    "data_format": 2,
    "description": "Computation of root phenes from 3D point clouds.",
    "filenames": [
      "Singularity",
      "model_preprocess/Singularity"
    ],
    "full_name": "Computational-Plant-Science/3D_model_traits_demo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-3d_model_traits_measurement\" class=\"anchor\" href=\"#3d_model_traits_measurement\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3D_model_traits_measurement\u003c/h1\u003e\n\u003cp\u003eFunction: Extract gemetrical traits of 3D model\u003c/p\u003e\n\u003cp\u003eAuthor            : Suxing Liu\u003c/p\u003e\n\u003cp\u003eDate created      : 04/04/2018\u003c/p\u003e\n\u003cp\u003eDate last modified: 04/25/2020\u003c/p\u003e\n\u003cp\u003ePython Version    : 3.7\u003c/p\u003e\n\u003cp\u003eExample of structure vs. 3D root model\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"../master/media/image3.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"../master/media/image3.png\" alt=\"Optional Text\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"../master/media/image2.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"../master/media/image2.gif\" alt=\"Optional Text\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eExample of connecting disconnected root path\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"../master/media/image7.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"../master/media/image7.png\" alt=\"Optional Text\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"../master/media/image8.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"../master/media/image8.png\" alt=\"Optional Text\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eusage:\u003c/p\u003e\n\u003cp\u003epython3 pipeline.py -p /$path_to_your_3D_model/ -m 3D_model_name.ply\u003c/p\u003e\n\u003cp\u003eSingularity test:\u003c/p\u003e\n\u003cp\u003esudo singularity build --writable model-scan.img Singularity\u003c/p\u003e\n\u003cp\u003esingularity exec model-scan.img python /opt/code/pipeline.py -p /$path_to_your_3D_model/ -m surface.ply\u003c/p\u003e\n\u003cp\u003esingularity exec shub://lsx1980/3D_model_traits_measurement python /opt/code/pipeline.py -p /$path_to_your_3D_model/ -m surface.ply\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePre-requisite:\n\u003cul\u003e\n\u003cli\u003ePython3.7\u003c/li\u003e\n\u003cli\u003eNumpy\u003c/li\u003e\n\u003cli\u003eSciPy\u003c/li\u003e\n\u003cli\u003eOpencv 3.0 for Python - \u003ca href=\"http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/\" rel=\"nofollow\"\u003eInstallation\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eVisualization requirement:\u003c/p\u003e\n\u003cp\u003epip3 install numba \u003cbr\u003e\nimagesize \u003cbr\u003e\nprogressbar2 \u003cbr\u003e\nmayavi \u003cbr\u003e\nPyQt5 \u003cbr\u003e\nnetworkx\u003c/p\u003e\n\u003cp\u003epython3 graph_compute.py -p /\u0026amp;path/active_component/\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 3,
    "topics": [
      "phenotyping",
      "phenotyping-algorithms",
      "phenomics"
    ],
    "updated_at": 1620288189.0
  },
  {
    "data_format": 2,
    "description": "Repository hosting the code associated with \"Fast and stable MAP-Elites in noisy domains using deep grids\"",
    "filenames": [
      "Singularity"
    ],
    "full_name": "adaptive-intelligent-robotics/Deep-Grid_MAP-Elites",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fast-and-stable-map-elites-in-noisy-domains-using-deep-grids\" class=\"anchor\" href=\"#fast-and-stable-map-elites-in-noisy-domains-using-deep-grids\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast and stable MAP-Elites in noisy domains using deep grids\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4459\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repository contains the code associated with \u003ca href=\"https://direct.mit.edu/isal/proceedings/isal2020/32/273/98397\" rel=\"nofollow\"\u003eFast and stable MAP-Elites in noisy domains using deep grids\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis code proposes an implementation of several MAP-Elites variants to handle uncertainty: the Explicit-averaging approach,  the two Adaptive-sampling approaches proposed in \u003ca href=\"https://dl.acm.org/doi/abs/10.1145/3319619.3321904\" rel=\"nofollow\"\u003eMAP-Elites for noisy domains by  adaptive sampling\u003c/a\u003e and the DG-MAP-Elites approach. It allows to compare these approaches on the three tasks described in the paper.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-libraries-and-dependencies\" class=\"anchor\" href=\"#libraries-and-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLibraries and dependencies\u003c/h1\u003e\n\u003cp\u003eThe implementation of all tasks and algorithms is based on the qd-branch of the C++ \u003ca href=\"https://github.com/sferes2/sferes2\"\u003eSferes2\u003c/a\u003e  library presented in \u003ca href=\"https://ieeexplore.ieee.org/abstract/document/5586158/?casa_token=EhBJLkircvMAAAAA:ls8I90Y5H2vsJk5RxCYs8X1T9yZHDhDEz5S6g5gatOzETle1LK_ib8zwodx6t5J_-Uwq_YP9\" rel=\"nofollow\"\u003eSferesv2: Evolvin\u0027 in the multi-core world\u003c/a\u003e, and the hexapod  control task uses the Dart simulator introduced in \u003ca href=\"https://joss.theoj.org/papers/10.21105/joss.00500.pdf\" rel=\"nofollow\"\u003eDart: Dynamic animation and robotics toolkit\u003c/a\u003e.\nFurthermore, the analysis of the results is based on \u003ca href=\"https://pandas.pydata.org/\" rel=\"nofollow\"\u003ePanda\u003c/a\u003e, \u003ca href=\"https://matplotlib.org/\" rel=\"nofollow\"\u003eMatplotlib\u003c/a\u003e and \u003ca href=\"https://seaborn.pydata.org/index.html\" rel=\"nofollow\"\u003eSeaborn\u003c/a\u003e libraries.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-structure\" class=\"anchor\" href=\"#structure\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStructure\u003c/h1\u003e\n\u003cp\u003eThe main part of the code contains the following files and folders:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003edeep_grid\u003c/code\u003e contains the main structure of all algorithms based on the structure of QD algorithms introduced in \u003ca href=\"https://ieeexplore.ieee.org/abstract/document/7959075/\" rel=\"nofollow\"\u003eQuality and diversity optimization: A  unifying modular  framework\u003c/a\u003e. The approaches differ by the nature of their selector, the add-function of their container and, in the specific case of Adaptive sampling, the descriptor they are using for the individuals, which is implemented in the structure of the cells of the grid.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etask\u003c/code\u003e contains a re-definition of the qd fitness from the Sferes2 library which allow to handle re-sampling of the solutions. It also contains an implementation of each of the three tasks used in the paper, build on top of the original deterministic tasks used in previous works.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emodifier\u003c/code\u003e defines the procedure to evaluate the \"true\" value of fitness and behaviour descriptor of each cell as described in the paper.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003estat\u003c/code\u003e implements all the stats used to compare the algorithms.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003erun_utils\u003c/code\u003e contains utils function used to read the options of each run and run all algorithms.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition, the \u003ccode\u003eanalysis\u003c/code\u003e folder is used for the results analysis, the \u003ccode\u003ewaf_tools\u003c/code\u003e and \u003ccode\u003ewscript\u003c/code\u003e files for compilation, and the \u003ccode\u003eressource\u003c/code\u003e folder and \u003ccode\u003eSingularity\u003c/code\u003e file to compile the Singularity container for this experiment.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-execution\" class=\"anchor\" href=\"#execution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecution\u003c/h1\u003e\n\u003cp\u003eThe results of the paper can be reproduced by running the Singularity container image of the experiment. Instructions to install Singularity can be found in \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003eSingularity documentation\u003c/a\u003e.\nTo pull the image from Singularity Hub, use the command: \u003ccode\u003esingularity pull shub://adaptive-intelligent-robotics/Deep-Grid_MAP-Elites\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThis container contains an \u003ccode\u003eapp\u003c/code\u003e for each approach-task combination, defined in the \u003ccode\u003eSingularity\u003c/code\u003e file:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eTruth_arm_var\u003c/code\u003e, \u003ccode\u003eTruth_rastrigin\u003c/code\u003e, \u003ccode\u003eTruth_hexa\u003c/code\u003e: Noise-free Baseline for each of the three tasks.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNaive_1_arm_var\u003c/code\u003e, \u003ccode\u003eNaive_1_rastrigin\u003c/code\u003e, \u003ccode\u003eNaive_1_hexa\u003c/code\u003e: Explicit-averaging approach with 1 re-sampling.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNaive_50_arm_var\u003c/code\u003e, \u003ccode\u003eNaive_50_rastrigin\u003c/code\u003e, \u003ccode\u003eNaive_50_hexa\u003c/code\u003e: Explicit-averaging approach with 50 re-sampling.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdapt_arm_var\u003c/code\u003e, \u003ccode\u003eAdapt_rastrigin\u003c/code\u003e, \u003ccode\u003eAdapt_hexa\u003c/code\u003e: Adaptive sampling approach without drifting elites.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdapt_BD_10_arm_var\u003c/code\u003e, \u003ccode\u003eAdapt_BD_10_rastrigin\u003c/code\u003e, \u003ccode\u003eAdapt_BD_10_hexa\u003c/code\u003e: Adaptive sampling approach with drifting elites.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDeep_50_arm_var\u003c/code\u003e, \u003ccode\u003eDeep_50_rastrigin\u003c/code\u003e, \u003ccode\u003eDeep_50_hexa\u003c/code\u003e: DG-MAP-Elites approach with depth 50.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAnalysis\u003c/code\u003e: apps to generate the graphs and container plots of all variants and tasks which results are stored in the folder given as input.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA given app can be run with the following command: \u003ccode\u003esingularity run --app *app_name* Deep-Grid_MAP-Elites_latest.sif\u003c/code\u003e. All run-parameters are defined inside the apps, and the results of the execution are solved in a \u003ccode\u003eresults\u003c/code\u003e folder, outside of the image, at the same location.\u003c/p\u003e\n\u003cp\u003eThis container can be recompile using the \u003ccode\u003eSingularity\u003c/code\u003e file.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618442518.0
  },
  {
    "data_format": 2,
    "description": "Code and Data for \"Biological network growth in complex environments - a computational framework\"",
    "filenames": [
      "Singularity/Singularity.def"
    ],
    "full_name": "CIA-CCTB/pythrahyper_net",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pythrahyper_net\" class=\"anchor\" href=\"#pythrahyper_net\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epythrahyper_net\u003c/h1\u003e\n\u003cp\u003eCode and data for the paper \u003cem\u003e\"Biological network growth in complex environments - a computational framework\"\u003c/em\u003e by T. Paul and P. Kollmannsberger (2020) - \u003ca href=\"https://biorxiv.org/cgi/content/short/2020.06.01.127407v1\" rel=\"nofollow\"\u003ehttps://biorxiv.org/cgi/content/short/2020.06.01.127407v1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePlease have a look at the notebook \u003ca href=\"https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Introduction.ipynb\"\u003eIntroduction.ipynb\u003c/a\u003e, or try it directly here:  \u003ca href=\"https://colab.research.google.com/github/CIA-CCTB/pythrahyper_net/blob/master/Colab/Introduction_Colab.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e (requires Google account)\u003c/p\u003e\n\u003cp\u003eThe following Jupyter notebooks reproduce the simulations shown in Figure 6 in the paper:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Multicellular-Network.ipynb\"\u003eMulticellular-Network.ipynb\u003c/a\u003e - simulation of network growth between layers of cells\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Multi-Simulation-Setup.ipynb\"\u003eMulti-Simulation-Setup.ipynb\u003c/a\u003e - generate configuration and batch files for parameter scan\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/CIA-CCTB/pythrahyper_net/blob/master/Multi-Simulation-Analysis.ipynb\"\u003eMulti-Simulation-Analysis.ipynb\u003c/a\u003e - analyze results and generate plots for parameter scan\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-instructions\" class=\"anchor\" href=\"#instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstructions\u003c/h1\u003e\n\u003cp\u003eThe framework is written in python using numpy and the multiprocessing module, and has been tested under Linux and MacOS. To run the example notebooks, first download or clone this repository, and then follow the instructions below.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-1-using-conda\" class=\"anchor\" href=\"#1-using-conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1) Using conda\u003c/h2\u003e\n\u003cp\u003eThe easiest way to install the required python packages is by using conda. Creating a new environment with this command will install all dependencies:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003econda create --name pythra python=3.7 pyqt=5 scipy tifffile jupyter networkx matplotlib\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThen change into the new environment using \u003ccode\u003econda activate pythra\u003c/code\u003e, and start a Jupyter notebook server in the \u003ccode\u003epythrahyper_net\u003c/code\u003e directory to access the notebooks.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-mayavi-visualization-in-the-browser\" class=\"anchor\" href=\"#mayavi-visualization-in-the-browser\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMayavi visualization in the browser:\u003c/h3\u003e\n\u003cp\u003eTo get interactive mayavi visualizations inside the browser, first install mayavi and ipyevents:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003econda install -c anaconda mayavi\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003econda install -c conda-forge ipyevents\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eNext, install and activate the required extension:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ejupyter nbextension install --py mayavi --user\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ejupyter nbextension enable --py mayavi --user\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eIf you get missing symbol errors upon importing \u003ccode\u003emlab\u003c/code\u003e, try this:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003econda install -c conda-force \"libnetcdf=4.6.2\"\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-interactive-matplotlib-plots\" class=\"anchor\" href=\"#interactive-matplotlib-plots\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInteractive Matplotlib plots:\u003c/h3\u003e\n\u003cp\u003eThe matplotlib plots can be made interactive using these modules:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003econda install -c conda-forge ipympl widgetsnbextension\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-using-singularity-container\" class=\"anchor\" href=\"#2-using-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2) Using Singularity container\u003c/h2\u003e\n\u003cp\u003eThe second possibility is to run the framework inside a Singularity container. A container image can be created using the included definition file:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build pythra.simg Singularity.def\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAfter successful build, you can e.g. start a Jupyter notebook server inside the container:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec pythra.simg jupyter notebook\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThen copy and paste the server URL into a web browser running outside of the container to access the notebooks.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1606837605.0
  },
  {
    "data_format": 2,
    "description": "An extended JupyterLab container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ftabaro/singularity-coderefinery2020",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-jupyterlab-container-for-coderefinery-2020\" class=\"anchor\" href=\"#jupyterlab-container-for-coderefinery-2020\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyterLab container for CodeRefinery 2020\u003c/h1\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-tags-code-refinery-singularity\" class=\"anchor\" href=\"#tags-code-refinery-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etags: \u003ccode\u003ecode-refinery\u003c/code\u003e \u003ccode\u003esingularity\u003c/code\u003e\n\u003c/h6\u003e\n\u003cp\u003eThis repository contains the recipe for the container used in CodeRefinery Workshop 2020 by the NykterLab team.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4356\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-whats-inside\" class=\"anchor\" href=\"#whats-inside\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat\u0027s inside\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eThe base container is Jupyter datascience-notebook Docker container.\u003c/li\u003e\n\u003cli\u003ePython packages have been added, e.g.:\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esphinx\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epytest\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epycodestyle\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eJupyter extensions:\n\u003cul\u003e\n\u003cli\u003eJupyterLab Git\u003c/li\u003e\n\u003cli\u003eJupyterLab GitHub\u003c/li\u003e\n\u003cli\u003eipywidgets\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSnakemake\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-option-1-singularityhub\" class=\"anchor\" href=\"#option-1-singularityhub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOption 1: SingularityHub\u003c/h3\u003e\n\u003cp\u003eWith this option, a copy of the pre-built container image will be downloaded from SingularityHub\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e start with defaults using runscript directive\u003c/span\u003e\nsingularity run shub://ftabaro/coderefinery.sif [options]\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e exec a custom Jupyter command \u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e -B /run/user shub://ftabaro/singularity-coderefinery2020 jupyter lab [options]\n\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-option-2-local-image\" class=\"anchor\" href=\"#option-2-local-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOption 2: local image\u003c/h3\u003e\n\u003cp\u003eThese commands will run an instance of a locally hosted container. Any of these options require a locally available container image file, see below.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e start with defaults\u003c/span\u003e\n./coderefinery.sif [options]\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e start with defaults using runscript directive\u003c/span\u003e\nsingularity run coderefinery.sif [options]\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e start custom command\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e -B /run/user coderefinery.sif jupyter lab [options]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn both scenarios, all the JupyterLab options are fully supported.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003cli\u003eBuild\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build coderefinery.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-configure-the-github-integration\" class=\"anchor\" href=\"#configure-the-github-integration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfigure the GitHub integration\u003c/h1\u003e\n\u003cp\u003eIn order to use the GitHub integration, an authentication token needs to be generated from GitHub and Jupyter lab needs to be configured to use it.\u003c/p\u003e\n\u003cp\u003eTo generate a token:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eNavigate to profile settings on GitHub: \u003ca href=\"https://github.com/settings/profile\"\u003ehttps://github.com/settings/profile\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eOn the menu on the left, click \"Developer Settings\"\u003c/li\u003e\n\u003cli\u003eOn the menu on the left, click \"Personal Access Tokens\"\u003c/li\u003e\n\u003cli\u003eClick on top right button \"Generate new token\"\u003c/li\u003e\n\u003cli\u003eIn the \"Note\" field insert a suitable name, e.g. \"Jupyter\"\u003c/li\u003e\n\u003cli\u003eIn the \"Select scopes\" menu, click on \"repo\" to allow full control of private repositories\u003c/li\u003e\n\u003cli\u003eAt the bottom of the page, click on \"Generate token\"\u003c/li\u003e\n\u003cli\u003eAt this point take note of the code in the green box, copy it and paste it to some secure place\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTo configure Jupyter to use the token:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eConnect to the machine running the Jupyter container instance (e.g. narvi)\u003c/li\u003e\n\u003cli\u003eIf the file \u003ccode\u003e~/.jupyter/jupyter_notebook_config.py\u003c/code\u003e exists, skip the next step\u003c/li\u003e\n\u003cli\u003eGenerate a config file with:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec containers/coderefinery.sif jupyter notebook --generate-config\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eOpen the \u003ccode\u003e~/.jupyter/jupyter_notebook_config.py\u003c/code\u003e file with a text editor\u003c/li\u003e\n\u003cli\u003eNavigate to the bottom of the file and add the following line replacing \u003ccode\u003e\u0026lt; YOUR_ACCESS_TOKEN \u0026gt;\u003c/code\u003e with the token generated above\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003ec.GitHubConfig.access_token = \u0027\u0026lt; YOUR_ACCESS_TOKEN \u0026gt;\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt this point, starting the JupyterLab instance, no error message will be displayed and the user will be able to navigate GitHub repositories from the JupyterLab UI.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h1\u003e\n\u003cp\u003eBug reports and pull requests are welcome on GitHub at \u003ca href=\"https://github.com/ftabaro/singularity-coderefinery2020\"\u003ehttps://github.com/ftabaro/singularity-coderefinery2020\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://coderefinery.github.io/2020-05-25-online/\" rel=\"nofollow\"\u003eOnline CodeRefinery workshop\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.3/user-guide/quick_start.html\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://hub.docker.com/r/jupyter/datascience-notebook\" rel=\"nofollow\"\u003eJupyter datascience-notebook Docker image\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line\"\u003eGitHub token creation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jupyterlab/jupyterlab-git\"\u003eJupyterLab Git extension\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jupyterlab/jupyterlab-github\"\u003eJupyterLab GitHub extension\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ipywidgets.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003eipywidgets\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 1,
    "topics": [
      "jupyterlab-container",
      "jupyter",
      "singularity"
    ],
    "updated_at": 1591371942.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.clustering"
    ],
    "full_name": "Himani2000/GSOC_2020",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-google-summer-of-code-2020\" class=\"anchor\" href=\"#google-summer-of-code-2020\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGoogle Summer of code 2020\u003c/h1\u003e\n\u003cp\u003eThis is my google summer of code project 2020. I worked with the Redhenlab organization. My project is based on the Image and Audio clustering and to deploy in Rapid Annotator.\nFor more details about the code and weekly progress during Google Summer of Code visit my GSoC blog:\n\u003ca href=\"https://himani2000.github.io/gsoc/index.html\" rel=\"nofollow\"\u003ehttps://himani2000.github.io/gsoc/index.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eTo install the dependencies use singualrity container .\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\n1.Pull the singularity image using \n\nsingularity pull --name clustering-image_and_audio.img shub://Himani2000/GSOC_2020:clustering\n\n2. Run the image using \n\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e -B \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e`\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e`\u003c/span\u003e\u003c/span\u003e [singularity_image_name ].img python3 [python file]\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003ePull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgments\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e1. Google summer of code 2020\n2. Redhenlab \n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607662384.0
  },
  {
    "data_format": 2,
    "description": "Singularity containers generated by the GEARS Lab at the University of Nevada, Reno",
    "filenames": [
      "singularity-definitions/general_use/Singularity.gears-general",
      "singularity-definitions/general_use/Singularity.R",
      "singularity-definitions/specialized_use/Singularity.gears-cloud-sdk",
      "singularity-definitions/specialized_use/Singularity.gears-lastools",
      "singularity-definitions/specialized_use/Singularity.gears-general-xenial",
      "singularity-definitions/specialized_use/Singularity.gears-tls_fuels",
      "singularity-definitions/specialized_use/Singularity.gears-pdal",
      "singularity-definitions/development/Singularity.treeseg",
      "singularity-definitions/development/Singularity.test_theo",
      "singularity-definitions/development/Singularity.gears-treeseg-burt",
      "singularity-definitions/development/Singularity.gears-taudem",
      "singularity-definitions/development/Singularity.gears-general-focal",
      "singularity-definitions/development/Singularity.gears-treeseg-calders",
      "singularity-definitions/development/Singularity.gears-general-eoan",
      "singularity-definitions/development/Singularity.gears-rfsrc-openmpi",
      "singularity-definitions/development/Singularity.gears-lidR",
      "singularity-definitions/development/Singularity.gears-computree",
      "singularity-definitions/development/Singularity.gears-treeseg-greenberg",
      "singularity-definitions/development/Singularity.gears-cloudcompare",
      "singularity-definitions/courses/Singularity.pronghorn-tutorial",
      "singularity-definitions/courses/Singularity.grad778-f19-module-09"
    ],
    "full_name": "gearslaboratory/gears-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-google-summer-of-code-2020\" class=\"anchor\" href=\"#google-summer-of-code-2020\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGoogle Summer of code 2020\u003c/h1\u003e\n\u003cp\u003eThis is my google summer of code project 2020. I worked with the Redhenlab organization. My project is based on the Image and Audio clustering and to deploy in Rapid Annotator.\nFor more details about the code and weekly progress during Google Summer of Code visit my GSoC blog:\n\u003ca href=\"https://himani2000.github.io/gsoc/index.html\" rel=\"nofollow\"\u003ehttps://himani2000.github.io/gsoc/index.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eTo install the dependencies use singualrity container .\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\n1.Pull the singularity image using \n\nsingularity pull --name clustering-image_and_audio.img shub://Himani2000/GSOC_2020:clustering\n\n2. Run the image using \n\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e -B \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e`\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e`\u003c/span\u003e\u003c/span\u003e [singularity_image_name ].img python3 [python file]\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003ePull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgments\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e1. Google summer of code 2020\n2. Redhenlab \n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1613698198.0
  },
  {
    "data_format": 2,
    "description": "H3A RefGraph Hackathon 2019",
    "filenames": [
      "Singularity"
    ],
    "full_name": "h3abionet/h3arefgraph",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-h3abioneth3arefgraph\" class=\"anchor\" href=\"#h3abioneth3arefgraph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eh3abionet/h3arefgraph\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eRefGraph Workflows Hackathon\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/h3abionet/h3arefgraph\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1ac1f732e5b8f802a198a533b88e24608b4b10e38d8744373f7bcb5284832ca8/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f68336172656667726170682e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/h3abionet/h3arefgraph.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/h3arefgraph\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/910d36cd9eef9bfef42722b54a531cf2c72b7ed04f37e85d72b93d50b7a3e0c1/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f68336172656667726170682e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/h3arefgraph.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThis pipeline is for the use and testing of graph based methods for variant calling.\u003c/p\u003e\n\u003cp\u003eThe aim is to allow the user to choose the reference graph construction method and the alignment / variant calling methods separately.\u003c/p\u003e\n\u003cp\u003eWe also provide tools for reporting the results of the variant calling, that take advantage of the additional contextual information that using reference graphs provides.\u003c/p\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe aim of this project is to separate the different parts of the variant calling process to allow the development of\ntask specific tools. This is more in line with traditional variant calling where specific alignment tools may preform\nbetter for different organisms, but should not require a different downstream analysis for each output.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"assets/images/Overview_slide.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"assets/images/Overview_slide.jpeg\" alt=\"Overview slide\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe h3abionet/h3arefgraph pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/reference_genomes.md\"\u003eReference genomes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003eh3abionet/h3arefgraph was originally written by the H3ABioNet RefGraph Team.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 13,
    "topics": [],
    "updated_at": 1569710209.0
  },
  {
    "data_format": 2,
    "description": "Containerizing the Canlab code",
    "filenames": [
      "Singularity"
    ],
    "full_name": "canlab/cantainer",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-h3abioneth3arefgraph\" class=\"anchor\" href=\"#h3abioneth3arefgraph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eh3abionet/h3arefgraph\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eRefGraph Workflows Hackathon\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/h3abionet/h3arefgraph\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1ac1f732e5b8f802a198a533b88e24608b4b10e38d8744373f7bcb5284832ca8/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f68336172656667726170682e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/h3abionet/h3arefgraph.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/h3arefgraph\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/910d36cd9eef9bfef42722b54a531cf2c72b7ed04f37e85d72b93d50b7a3e0c1/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f68336172656667726170682e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/h3arefgraph.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThis pipeline is for the use and testing of graph based methods for variant calling.\u003c/p\u003e\n\u003cp\u003eThe aim is to allow the user to choose the reference graph construction method and the alignment / variant calling methods separately.\u003c/p\u003e\n\u003cp\u003eWe also provide tools for reporting the results of the variant calling, that take advantage of the additional contextual information that using reference graphs provides.\u003c/p\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe aim of this project is to separate the different parts of the variant calling process to allow the development of\ntask specific tools. This is more in line with traditional variant calling where specific alignment tools may preform\nbetter for different organisms, but should not require a different downstream analysis for each output.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"assets/images/Overview_slide.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"assets/images/Overview_slide.jpeg\" alt=\"Overview slide\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe h3abionet/h3arefgraph pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/reference_genomes.md\"\u003eReference genomes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003eh3abionet/h3arefgraph was originally written by the H3ABioNet RefGraph Team.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1525841449.0
  },
  {
    "data_format": 2,
    "description": "my laboratory",
    "filenames": [
      "singularity_recipes/vnc/Singularity",
      "singularity_recipes/common/Singularity",
      "singularity_recipes/dotnet/Singularity",
      "singularity_recipes/deno/Singularity.tmp",
      "singularity_recipes/deno/Singularity.template",
      "singularity_recipes/python/Singularity.tmp",
      "singularity_recipes/python/Singularity.template",
      "singularity_recipes/node/Singularity.tmp",
      "singularity_recipes/node/Singularity.template",
      "singularity_recipes/procon/Singularity",
      "singularity_recipes/cxx/Singularity",
      "singularity_recipes/rust/Singularity"
    ],
    "full_name": "ar90n/lab",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-h3abioneth3arefgraph\" class=\"anchor\" href=\"#h3abioneth3arefgraph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eh3abionet/h3arefgraph\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eRefGraph Workflows Hackathon\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/h3abionet/h3arefgraph\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1ac1f732e5b8f802a198a533b88e24608b4b10e38d8744373f7bcb5284832ca8/68747470733a2f2f7472617669732d63692e6f72672f68336162696f6e65742f68336172656667726170682e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/h3abionet/h3arefgraph.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/h3arefgraph\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/910d36cd9eef9bfef42722b54a531cf2c72b7ed04f37e85d72b93d50b7a3e0c1/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f68336172656667726170682e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/h3arefgraph.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThis pipeline is for the use and testing of graph based methods for variant calling.\u003c/p\u003e\n\u003cp\u003eThe aim is to allow the user to choose the reference graph construction method and the alignment / variant calling methods separately.\u003c/p\u003e\n\u003cp\u003eWe also provide tools for reporting the results of the variant calling, that take advantage of the additional contextual information that using reference graphs provides.\u003c/p\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe aim of this project is to separate the different parts of the variant calling process to allow the development of\ntask specific tools. This is more in line with traditional variant calling where specific alignment tools may preform\nbetter for different organisms, but should not require a different downstream analysis for each output.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"assets/images/Overview_slide.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"assets/images/Overview_slide.jpeg\" alt=\"Overview slide\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe h3abionet/h3arefgraph pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/reference_genomes.md\"\u003eReference genomes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003eh3abionet/h3arefgraph was originally written by the H3ABioNet RefGraph Team.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617389118.0
  },
  {
    "data_format": 2,
    "description": "The Recommender Engine for Intelligent Transient Tracking",
    "filenames": [
      "Singularity"
    ],
    "full_name": "refitt/refitt",
    "latest_release": "0.16.4",
    "readme": "\u003cp\u003eSingularity recipe files for the DVC tool for Data Version Control\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 1,
    "topics": [
      "science",
      "astronomy",
      "distributed-systems",
      "machine-learning",
      "citizen-science",
      "open-source",
      "python"
    ],
    "updated_at": 1621927444.0
  },
  {
    "data_format": 2,
    "description": "Singularity containers with common radio transient search software.  ",
    "filenames": [
      "Singularity.arm",
      "Singularity",
      "Singularity.cpu",
      "Singularity.gpu"
    ],
    "full_name": "josephwkania/radio_transients",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-radio_transients\" class=\"anchor\" href=\"#radio_transients\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/88694793dd1e428a0aab6788e9bbd21141580f62cbc4d6310bbcc74fde83ab22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ff5d91d6824296a5d7ffaad36635c5ef0688d2c0e21e50ccc94735fe8387faa7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"Forks\" data-canonical-src=\"https://img.shields.io/github/forks/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6e6ccae69f7f4df8c46d4d56b7e36d27fd932cc463a486a3111796543c271ab9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d5708d5c2bcd6f7d5f3565d9e75135e1eaa086ff847e198c14d187c5612f8203/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5231\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThese are my Singularity Recipes for common radio transient software.\nThere are three containers\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-radio_transients-1\" class=\"anchor\" href=\"#radio_transients-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients\u003c/h3\u003e\n\u003cp\u003eContains everything (CPU+GPU)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCUDA 10.2\nFETCH          https://github.com/devanshkv/fetch  -- In Conda environment `FE`\nheimdall       https://sourceforge.net/p/heimdall-astro/wiki/Use/\n- dedisp       https://github.com/ajameson/dedisp\nhtop           https://htop.dev/\niqrm_apollo    https://gitlab.com/kmrajwade/iqrm_apollo\njupyterlab     https://jupyter.org/\nPRESTO         https://www.cv.nrao.edu/~sransom/presto/\npsrdada        http://psrdada.sourceforge.net/\npsrdada-python https://github.com/TRASAL/psrdada-python\npsrcat         https://www.atnf.csiro.au/people/pulsar/psrcat/download.html\npysigproc      https://github.com/devanshkv/pysigproc\nriptide        https://github.com/v-morello/riptide\nsigproc        https://github.com/SixByNine/sigproc\nTempo          http://tempo.sourceforge.net/\nRFIClean       https://github.com/ymaan4/RFIClean\nYAPP           https://github.com/jayanthc/yapp\nyour           https://github.com/thepetabyteproject/your\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGet with\n\u003ccode\u003esingularity pull shub://josephwkania/radio_transients\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e*One of FETCH\u0027s dependencies causes PRESTO\u0027s Python scripts to fail.\nThis necessitated putting them in different environments.\nEverything except for PRESTO is in \u003ccode\u003eRT\u003c/code\u003e, which is loaded by default.\nPRESTO is in \u003ccode\u003ePE\u003c/code\u003e, in the shell you can activate this\nwith \u003ccode\u003econda activate PE\u003c/code\u003e. If you need access outside the container,\nyou should use radio_transients:cpu, which has PRESTO in the default environment.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-radio_transients_cpu\" class=\"anchor\" href=\"#radio_transients_cpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients_cpu\u003c/h3\u003e\n\u003cp\u003eContains CPU based programs\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehtop\niqrm_apollo\njupyterlab   \nPRESTO\npsrcat\npysigproc\nriptide\nsigproc\nTempo \nRFIClean\nYAPP  \nyour\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGet with\n\u003ccode\u003esingularity pull shub://josephwkania/radio_transients:cpu\u003c/code\u003e\u003cbr\u003e\nThere is an arm version \u003ccode\u003eSingularity.arm\u003c/code\u003e, you can build yourself.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-radio_transients_gpu\" class=\"anchor\" href=\"#radio_transients_gpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients_gpu\u003c/h3\u003e\n\u003cp\u003eContains gpu based programs\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCUDA 10.2\nFETCH      \njupyterlab\nheimdall\n- dedisp\nhtop \npsrdada \npsrdada-python\nyour\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGet with\n\u003ccode\u003esingularity pull shub://josephwkania/radio_transients:gpu\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to use\u003c/h3\u003e\n\u003cp\u003eYour \u003ccode\u003e$HOME\u003c/code\u003e automatically gets mounted.\nYou can mount a directory with \u003ccode\u003e-B /dir/on/host:/mnt\u003c/code\u003e, which will mount \u003ccode\u003e/dir/on/host\u003c/code\u003e to \u003ccode\u003e/mnt\u003c/code\u003e in the container.\u003c/p\u003e\n\u003cp\u003eFor the gpu processes, you must pass \u003ccode\u003e--nv\u003c/code\u003e when running singularity.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity shell --nv -B /data:/mnt radio_transients_gpu.sif\u003c/code\u003e\nwill mount \u003ccode\u003e/data\u003c/code\u003e to \u003ccode\u003e/mnt\u003c/code\u003e, give you GPU access, and drop you into the interactive shell.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec --nv -B /data:/mnt radio_transients_gpu.sif your_heimdall.py -f /mnt/data.fil\u003c/code\u003e\nwill mount \u003ccode\u003e/data\u003c/code\u003e to \u003ccode\u003e/mnt\u003c/code\u003e, give you GPU access, and run your_heimdall.py without entering the container.\u003c/p\u003e\n\u003cp\u003eAll the Python scripts are installed in a Conda environment \u003ccode\u003eRT\u003c/code\u003e, this environment is automatically loaded.\u003c/p\u003e\n\u003cp\u003eYou can see the commits and corresponding dates by running \u003ccode\u003esingularity inspect radio_transients.sif\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-shub\" class=\"anchor\" href=\"#shub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eshub\u003c/h3\u003e\n\u003cp\u003eThese are built on commit by Singularity Hub at: \u003ca href=\"https://singularity-hub.org/collections/5231\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/5231\u003c/a\u003e\nThey where last built on 15-May-2021\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-improvements\" class=\"anchor\" href=\"#improvements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImprovements\u003c/h3\u003e\n\u003cp\u003eIf you come accross bug or have suggestions for improvements, let me know or submit a pull request.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks\u003c/h3\u003e\n\u003cp\u003eTo Kshitij Aggarwal for bug reports and suggestions.\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 2,
    "topics": [
      "pulsars",
      "fast-radio-bursts",
      "psrdada",
      "radio-astronomy"
    ],
    "updated_at": 1621141428.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "HippocampusGirl/HALFpipe",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-welcome-to-enigma-halfpipe\" class=\"anchor\" href=\"#welcome-to-enigma-halfpipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWelcome to ENIGMA \u003ccode\u003eHALFpipe\u003c/code\u003e\n\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4508\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22build%22\"\u003e\u003cimg src=\"https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg\" alt=\"https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22continuous+integration%22\"\u003e\u003cimg src=\"https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg\" alt=\"https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/HALFpipe/HALFpipe\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ef5be2978b13a91a1a602bc0261933d3735a7567176db1ef0c13eb65b3249056/68747470733a2f2f636f6465636f762e696f2f67682f48414c46706970652f48414c46706970652f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/HALFpipe/HALFpipe/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e is a user-friendly software that facilitates reproducible analysis of\nfMRI data, including preprocessing, single-subject, and group analysis. It\nprovides state-of-the-art preprocessing using\n\u003ca href=\"https://fmriprep.readthedocs.io/\" rel=\"nofollow\"\u003e\u003ccode\u003efmriprep\u003c/code\u003e\u003c/a\u003e, but removes the necessity to\nconvert data to the\n\u003ca href=\"https://bids-specification.readthedocs.io/en/stable/\" rel=\"nofollow\"\u003e\u003ccode\u003eBIDS\u003c/code\u003e\u003c/a\u003e format. Common\nresting-state and task-based fMRI features can then be calculated on the fly\nusing \u003ca href=\"http://fsl.fmrib.ox.ac.uk/\" rel=\"nofollow\"\u003e\u003ccode\u003eFSL\u003c/code\u003e\u003c/a\u003e and\n\u003ca href=\"https://nipype.readthedocs.io/\" rel=\"nofollow\"\u003e\u003ccode\u003enipype\u003c/code\u003e\u003c/a\u003e for statistics.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIf you encounter issues, please see the \u003ca href=\"#troubleshooting\"\u003etroubleshooting\u003c/a\u003e\nsection of this document.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#getting-started\"\u003eGetting started\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#container-platform\"\u003eContainer platform\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#download\"\u003eDownload\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running\"\u003eRunning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#user-interface\"\u003eUser interface\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#files\"\u003eFiles\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#features\"\u003eFeatures\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#models\"\u003eModels\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-on-a-high-performance-computing-cluster\"\u003eRunning on a high-performance computing cluster\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#quality-checks\"\u003eQuality checks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#outputs\"\u003eOutputs\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subject-level-features\"\u003eSubject-level features\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#preprocessed-images\"\u003ePreprocessed images\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#group-level\"\u003eGroup-level\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#troubleshooting\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#command-line-flags\"\u003eCommand line flags\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#control-command-line-logging\"\u003eControl command line logging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#automatically-remove-unneeded-files\"\u003eAutomatically remove unneeded files\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#adjust-nipype\"\u003eAdjust nipype\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#choose-which-parts-to-run-or-to-skip\"\u003eChoose which parts to run or to skip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#working-directory\"\u003eWorking directory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-file-system-root\"\u003eData file system root\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#contact\"\u003eContact\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting started\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e is distributed as a container, meaning that all required software\ncomes bundled in a monolithic file, the container. This allows for easy\ninstallation on new systems, and makes data analysis more reproducible, because\nsoftware versions are guaranteed to be the same for all users.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-container-platform\" class=\"anchor\" href=\"#container-platform\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer platform\u003c/h3\u003e\n\u003cp\u003eThe first step is to install one of the supported container platforms. If you\u0027re\nusing a high-performance computing cluster, more often than not\n\u003ca href=\"https://sylabs.io\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e will already be available.\u003c/p\u003e\n\u003cp\u003eIf not, we recommend using the latest version\nof\u003ca href=\"https://sylabs.io\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e. However, it can be somewhat cumbersome to\ninstall, as it needs to be built from source.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://neuro.debian.net/\" rel=\"nofollow\"\u003e\u003ccode\u003eNeuroDebian\u003c/code\u003e\u003c/a\u003e package repository provides an\nolder version of \u003ca href=\"https://sylabs.io/guides/2.6/user-guide/\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e for\n\u003ca href=\"https://neuro.debian.net/pkgs/singularity-container.html\" rel=\"nofollow\"\u003esome\u003c/a\u003e Linux\ndistributions.\u003c/p\u003e\n\u003cp\u003eIn contrast to \u003ccode\u003eSingularity\u003c/code\u003e, \u003ccode\u003eDocker\u003c/code\u003e always requires elevated privileges to\nrun containers. In other words, every user running a \u003ccode\u003eDocker\u003c/code\u003e container\nautomatically has administrator privileges on the computer they\u0027re using.\nTherefore, it is inherently a bad choice for multi-user environments, where the\naccess of individual users should be limited. \u003ccode\u003eDocker\u003c/code\u003e is the only option that\nis compatible with \u003ccode\u003eMac OS X\u003c/code\u003e.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eContainer platform\u003c/th\u003e\n\u003cth\u003eVersion\u003c/th\u003e\n\u003cth\u003eInstallation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eSingularity\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e3.5.3\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003eSee \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/user-guide/quick_start.html\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSingularity\u003c/td\u003e\n\u003ctd\u003e2.6.1\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003esudo apt install singularity-container\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDocker\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eSee \u003ca href=\"https://docs.docker.com/engine/install/\" rel=\"nofollow\"\u003ehttps://docs.docker.com/engine/install/\u003c/a\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h3\u003e\n\u003cp\u003eThe second step is to download the \u003ccode\u003eHALFpipe\u003c/code\u003e to your computer. This requires\napproximately 5 gigabytes of storage.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eContainer platform\u003c/th\u003e\n\u003cth\u003eInstallation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSingularity\u003c/td\u003e\n\u003ctd\u003e\n\u003ca href=\"https://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g\" rel=\"nofollow\"\u003ehttps://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g\u003c/a\u003e or \u003ccode\u003esingularity pull docker://halfpipe/halfpipe:1.1.0\u003c/code\u003e or \u003ccode\u003esingularity pull docker://ghcr.io/halfpipe/halfpipe:1.1.0\u003c/code\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDocker\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003edocker pull halfpipe/halfpipe:1.1.0\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003ccode\u003eSingularity\u003c/code\u003e version \u003ccode\u003e3.x\u003c/code\u003e creates a container image file called\n\u003ccode\u003eHALFpipe_{version}.sif\u003c/code\u003e in the directory where you run the \u003ccode\u003epull\u003c/code\u003e command. For\n\u003ccode\u003eSingularity\u003c/code\u003e version \u003ccode\u003e2.x\u003c/code\u003e the file is named\n\u003ccode\u003ehalfpipe-halfpipe-master-latest.simg\u003c/code\u003e. Whenever you want to use the container,\nyou need pass \u003ccode\u003eSingularity\u003c/code\u003e the path to this file.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e \u003ccode\u003eSingularity\u003c/code\u003e may store a copy of the container in its cache\ndirectory. The cache directory is located by default in your home directory at\n\u003ccode\u003e~/.singularity\u003c/code\u003e. If you need to save disk space in your home directory, you\ncan safely delete the cache directory after downloading, i.e. by running\n\u003ccode\u003erm -rf ~/.singularity\u003c/code\u003e. Alternatively, you could move the cache directory\nsomewhere with more free disk space using a symlink. This way, files will\nautomatically be stored there in the future. For example, if you have a lot of\nfree disk space in \u003ccode\u003e/mnt/storage\u003c/code\u003e, then you could first run\n\u003ccode\u003emv ~/.singularity /mnt/storage\u003c/code\u003e to move the cache directory, and then\n\u003ccode\u003eln -s /mnt/storage/.singularity ~/.singularity\u003c/code\u003e to create the symlink.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ccode\u003eDocker\u003c/code\u003e will store the container in its storage base directory, so it does not\nmatter from which directory you run the \u003ccode\u003epull\u003c/code\u003e command.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running\" class=\"anchor\" href=\"#running\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning\u003c/h3\u003e\n\u003cp\u003eThe third step is to run the downloaded container. You may need to replace\n\u003ccode\u003ehalfpipe_1.0.1.sif\u003c/code\u003e with the actual path and filename where \u003ccode\u003eSingularity\u003c/code\u003e\ndownloaded your container.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eContainer platform\u003c/th\u003e\n\u003cth\u003eCommand\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSingularity\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003esingularity run --no-home --cleanenv --bind /:/ext halfpipe_1.1.0.sif\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDocker\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003edocker run --interactive --tty --volume /:/ext halfpipe/halfpipe\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eYou should now see the user interface.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-background\" class=\"anchor\" href=\"#background\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBackground\u003c/h4\u003e\n\u003cp\u003eContainers are by default isolated from the host computer. This adds security,\nbut also means that the container cannot access the data it needs for analysis.\n\u003ccode\u003eHALFpipe\u003c/code\u003e expects all inputs (e.g., image files and spreadsheets) and outputs\n(the working directory) to be places in the path\u003ccode\u003e/ext\u003c/code\u003e (see also\n\u003ca href=\"#data-file-system-root---fs-root\"\u003e\u003ccode\u003e--fs-root\u003c/code\u003e\u003c/a\u003e). Using the option\n\u003ccode\u003e--bind /:/ext\u003c/code\u003e, we instruct \u003ccode\u003eSingularity\u003c/code\u003e to map all of the host file system\n(\u003ccode\u003e/\u003c/code\u003e) to that path (\u003ccode\u003e/ext\u003c/code\u003e). You can also run \u003ccode\u003eHALFpipe\u003c/code\u003e and only map only part\nof the host file system, but keep in mind that any directories that are not\nmapped will not be visible later.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eSingularity\u003c/code\u003e passes the host shell environment to the container by default.\nThis means that in some cases, the host computer\u0027s configuration can interfere\nwith the software. To avoid this, we need to pass the option \u003ccode\u003e--cleanenv\u003c/code\u003e.\n\u003ccode\u003eDocker\u003c/code\u003e does not pass the host shell environment by default, so we don\u0027t need\nto pass an option.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-user-interface\" class=\"anchor\" href=\"#user-interface\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUser interface\u003c/h2\u003e\n\u003cp\u003eThe user interface asks a series of questions about your data and the analyses\nyou want to run. In each question, you can press \u003ccode\u003eControl+C\u003c/code\u003e to cancel the\ncurrent question and go back to the previous one. \u003ccode\u003eControl+D\u003c/code\u003e exits the program\nwithout saving. Note that these keyboard shortcuts are the same on Mac.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-files\" class=\"anchor\" href=\"#files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFiles\u003c/h3\u003e\n\u003cp\u003eTo run preprocessing, at least a T1-weighted structural image and a BOLD image\nfile is required. Preprocessing and data analysis proceeds automatically.\nHowever, to be able to run automatically, data files need to be input in a way\nsuitable for automation.\u003c/p\u003e\n\u003cp\u003eFor this kind of automation, \u003ccode\u003eHALFpipe\u003c/code\u003e needs to know the relationships between\nfiles, such as which files belong to the same subject. However, even though it\nwould be obvious for a human, a program cannot easily assign a file name to a\nsubject, and this will be true as long as there are differences in naming\nbetween different researchers or labs. One researcher may name the same file\n\u003ccode\u003esubject_01_rest.nii.gz\u003c/code\u003e and another \u003ccode\u003esubject_01/scan_rest.nii.gz\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIn \u003ccode\u003eHALFpipe\u003c/code\u003e, we solve this issue by inputting file names in a specific way.\nFor example, instead of \u003ccode\u003esubject_01/scan_rest.nii.gz\u003c/code\u003e, \u003ccode\u003eHALFpipe\u003c/code\u003e expects you to\ninput \u003ccode\u003e{subject}/scan_rest.nii.gz\u003c/code\u003e. \u003ccode\u003eHALFpipe\u003c/code\u003e can then match all files on disk\nthat match this naming schema, and extract the subject ID \u003ccode\u003esubject_01\u003c/code\u003e. Using\nthe extracted subject ID, other files can now be matched to this image. If all\ninput files are available in BIDS format, then this step can be skipped.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify working directory\u003c/code\u003e All intermediate and outputs of \u003ccode\u003eHALFpipe\u003c/code\u003e will\nbe placed in the working directory. Keep in mind to choose a location with\nsufficient free disk space, as intermediates can be multiple gigabytes in\nsize for each subject.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eIs the data available in BIDS format?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the BIDS directory\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify anatomical/structural data\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003eSpecify the path of the T1-weighted image files\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify functional data\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003eSpecify the path of the BOLD image files\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCheck repetition time values\u003c/code\u003e / \u003ccode\u003eSpecify repetition time in seconds\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd more BOLD image files?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e Loop back to 2\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDo slice timing?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eCheck slice acquisition direction values\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eCheck slice timing values\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Skip this step\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify field maps?\u003c/code\u003e If the data was imported from a BIDS directory, this\nstep will be omitted.\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify the type of the field maps\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eEPI (blip-up blip-down)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the blip-up blip-down EPI image files\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePhase difference and magnitude (used by Siemens scanners)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the magnitude image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the phase/phase difference image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify echo time difference in seconds\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eScanner-computed field map and magnitude (used by GE / Philips\nscanners)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the magnitude image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the field map image files\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd more field maps?\u003c/code\u003e Loop back to 1\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify effective echo spacing for the functional data in seconds\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify phase encoding direction for the functional data\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Skip this step\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h3\u003e\n\u003cp\u003eFeatures are analyses that are carried out on the preprocessed data, in other\nwords, first-level analyses.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify first-level features?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify the feature type\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eTask-based\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the event file type\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSPM multiple conditions\u003c/code\u003e A MATLAB .mat file containing three\narrays: \u003ccode\u003enames\u003c/code\u003e (condition), \u003ccode\u003eonsets\u003c/code\u003e and \u003ccode\u003edurations\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eFSL 3-column\u003c/code\u003e One text file for each condition. Each file has its\ncorresponding condition in the filename. The first column specifies\nthe event onset, the second the duration. The third column of the\nfiles is ignored, so parametric modulation is not supported\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eBIDS TSV\u003c/code\u003e A tab-separated table with named columns \u003ccode\u003etrial_type\u003c/code\u003e\n(condition), \u003ccode\u003eonset\u003c/code\u003e and \u003ccode\u003eduration\u003c/code\u003e. While BIDS supports defining\nadditional columns, \u003ccode\u003eHALFpipe\u003c/code\u003e will currently ignore these\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the event files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSelect conditions to add to the model\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify contrasts\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify contrast name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify contrast values\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd another contrast?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e Loop back to 1\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply a temporal filter to the design matrix?\u003c/code\u003e A separate temporal\nfilter can be specified for the design matrix. In contrast, the\ntemporal filtering of the input image and any confound regressors\nadded to the design matrix is specified in 10. In general, the two\nsettings should match\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply smoothing?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify smoothing FWHM in mm\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eGrand mean scaling will be applied with a mean of 10000.000000\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eTemporal filtering will be applied using a gaussian-weighted filter\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003eSpecify the filter width in seconds\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRemove confounds?\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSeed-based connectivity\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify binary seed mask file(s)\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the binary seed mask image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eCheck space values\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAdd binary seed mask image file\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDual regression\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAtlas-based connectivity matrix\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eReHo\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003efALFF\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Skip this step\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd another first-level feature?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e Loop back to 1\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eOutput a preprocessed image?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify setting name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply smoothing?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify smoothing FWHM in mm\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDo grand mean scaling?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify grand mean\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply a temporal filter?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify the type of temporal filter\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eGaussian-weighted\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eFrequency-based\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRemove confounds?\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-models\" class=\"anchor\" href=\"#models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eModels\u003c/h3\u003e\n\u003cp\u003eModels are statistical analyses that are carried out on the features.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTODO\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-a-high-performance-computing-cluster\" class=\"anchor\" href=\"#running-on-a-high-performance-computing-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on a high-performance computing cluster\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eLog in to your cluster\u0027s head node\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRequest an interactive job. Refer to your cluster\u0027s documentation for how to\ndo this\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn the interactive job, run the \u003ccode\u003eHALFpipe\u003c/code\u003e user interface, but add the flag\n\u003ccode\u003e--use-cluster\u003c/code\u003e to the end of the command. \u003cbr\u003e\nFor example, \u003ccode\u003esingularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif --use-cluster\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs soon as you finish specifying all your data, features and models in the\nuser interface, \u003ccode\u003eHALFpipe\u003c/code\u003e will now generate everything needed to run on the\ncluster. For hundreds of subjects, this can take up to a few hours.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhen \u003ccode\u003eHALFpipe\u003c/code\u003e exits, edit the generated submit script \u003ccode\u003esubmit.slurm.sh\u003c/code\u003e\naccording to your cluster\u0027s documentation and then run it. This submit script\nwill calculate everything except group statistics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs soon as all processing has been completed, you can run group statistics.\nThis is usually very fast, so you can do this in an interactive session. Run\n\u003ccode\u003esingularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif --only-model-chunk\u003c/code\u003e\nand then select \u003ccode\u003eRun without modification\u003c/code\u003e in the user interface.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA common issue with remote work via secure shell is that the connection may\nbreak after a few hours. For batch jobs this is not an issue, but for\ninteractive jobs this can be quite frustrating. When the connection is lost,\nthe node you were connected to will automatically quit all programs you were\nrunning. To prevent this, you can run interactive jobs within \u003ccode\u003escreen\u003c/code\u003e or\n\u003ccode\u003etmux\u003c/code\u003e (whichever is available). These commands allow you to open sessions in\nthe terminal that will continue running in the background even when you close\nor disconnect. Here\u0027s a quick overview of how to use the commands (more\nin-depth documentation is available for example at\n[\u003ca href=\"http://www.dayid.org/comp/tm.html\" rel=\"nofollow\"\u003ehttp://www.dayid.org/comp/tm.html\u003c/a\u003e]).\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOpen a new screen/tmux session on the head node by running either \u003ccode\u003escreen\u003c/code\u003e\nor \u003ccode\u003etmux\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRequest an interactive job from within the session, for example with\n\u003ccode\u003esrun --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun the command that you want to run\u003c/li\u003e\n\u003cli\u003eDetach from the screen/tmux session, meaning disconnecting with the ability\nto re-connect later \u003cbr\u003e\nFor screen, this is done by first pressing \u003ccode\u003eControl+a\u003c/code\u003e, then letting go, and\nthen pressing \u003ccode\u003ed\u003c/code\u003e on the keyboard. \u003cbr\u003e\nFor tmux, it\u0027s \u003ccode\u003eControl+b\u003c/code\u003e instead of \u003ccode\u003eControl+a\u003c/code\u003e. \u003cbr\u003e\nNote that this is always \u003ccode\u003eControl\u003c/code\u003e, even if you\u0027re on a mac.\u003c/li\u003e\n\u003cli\u003eClose your connection to the head node with \u003ccode\u003eControl+d\u003c/code\u003e. \u003ccode\u003escreen\u003c/code\u003e/\u003ccode\u003etmux\u003c/code\u003e\nwill remain running in the background\u003c/li\u003e\n\u003cli\u003eLater, connect again to the head node. Run \u003ccode\u003escreen -r\u003c/code\u003e or \u003ccode\u003etmux attach\u003c/code\u003e to\ncheck back on the interactive job. If everything went well and the command\nyou wanted to run finished, close the interactive job with \u003ccode\u003eControl+d\u003c/code\u003e and\nthen the \u003ccode\u003escreen\u003c/code\u003e/\u003ccode\u003etmux\u003c/code\u003e session with \u003ccode\u003eControl+d\u003c/code\u003e again. If the command\nhasn\u0027t finished yet, detach as before and come back later\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quality-checks\" class=\"anchor\" href=\"#quality-checks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuality checks\u003c/h2\u003e\n\u003cp\u003ePlease see the manual at \u003ca href=\"https://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8\" rel=\"nofollow\"\u003ehttps://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eA visual report page \u003ccode\u003ereports/index.html\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA table with image quality metrics \u003ccode\u003ereports/reportvals.txt\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA table containing the preprocessing status \u003ccode\u003ereports/reportpreproc.txt\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe untouched \u003ccode\u003efmriprep\u003c/code\u003e derivatives. Some files have been omitted to save\ndisk space \u003ccode\u003efmriprep\u003c/code\u003e is very strict about only processing data that is\ncompliant with the BIDS standard. As such, we may need to format subjects\nnames for compliance. For example, an input subject named \u003ccode\u003esubject_01\u003c/code\u003e will\nappear as \u003ccode\u003esubject01\u003c/code\u003e in the \u003ccode\u003efmriprep\u003c/code\u003e derivatives. \u003ccode\u003ederivatives/fmriprep\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-subject-level-features\" class=\"anchor\" href=\"#subject-level-features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubject-level features\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eFor task-based, seed-based connectivity and dual regression features,\n\u003ccode\u003eHALFpipe\u003c/code\u003e outputs the statistical maps for the effect, the variance, the\ndegrees of freedom of the variance and the z-statistic. In FSL, the effect and\nvariance are also called \u003ccode\u003ecope\u003c/code\u003e and \u003ccode\u003evarcope\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-effect_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-variance_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-dof_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-z_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\nThe design and contrast matrix used for the final model will be outputted alongside\nthe statistical maps \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-design_matrix.tsv\u003c/code\u003e\n\u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-contrast_matrix.tsv\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReHo and fALFF are not calculated based on a linear model. As such, only one\nstatistical map of the z-scaled values will be output \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._alff.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._falff.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._reho.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor every feature, a JSON file containing a summary of the preprocessing\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003esettings, and a list of the raw data files that were used for the analysis\n(\u003ccode\u003eRawSources\u003c/code\u003e) \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/....json\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor every feature, the corresponding brain mask is output beside the\nstatistical maps. Masks do not differ between different features calculated,\nthey are only copied out repeatedly for convenience \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/...desc-brain_mask.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAtlas-based connectivity outputs the time series and the full covariance and\ncorrelation matrices as text files \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._timeseries.txt\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._desc-covariance_matrix.txt\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._desc-correlation_matrix.txt\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-preprocessed-images\" class=\"anchor\" href=\"#preprocessed-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreprocessed images\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMasked, preprocessed BOLD image \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._bold.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJust like for features \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._bold.json\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJust like for features \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-brain_mask.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFiltered confounds time series, where all filters that are applied to the BOLD\nimage are applied to the regressors as well. Note that this means that when\ngrand mean scaling is active, confounds time series are also scaled, meaning\nthat values such as \u003ccode\u003eframewise displacement\u003c/code\u003e can not be interpreted in terms\nof their original units anymore. \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-confounds_regressors.tsv\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-group-level\" class=\"anchor\" href=\"#group-level\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGroup-level\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egrouplevel/...\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eIf an error occurs, this will be output to the command line and simultaneously\nto the \u003ccode\u003eerr.txt\u003c/code\u003e file in the working directory\u003c/li\u003e\n\u003cli\u003eIf the error occurs while running, usually a text file detailing the error\nwill be placed in the working directory. These are text files and their file\nnames start with \u003ccode\u003ecrash\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eUsually, the last line of these text files contains the error message.\nPlease read this carefully, as may allow you to understand the error\u003c/li\u003e\n\u003cli\u003eFor example, consider the following error message:\n\u003ccode\u003eValueError: shape (64, 64, 33) for image 1 not compatible with first image shape (64, 64, 34) with axis == None\u003c/code\u003e\nThis error message may seem cryptic at first. However, looking at the\nmessage more closely, it suggests that two input images have different,\nincompatible dimensions. In this case, \u003ccode\u003eHALFpipe\u003c/code\u003e correctly recognized this\nissue, and there is no need for concern. The images in question will simply\nbe excluded from preprocessing and/or analysis\u003c/li\u003e\n\u003cli\u003eIn some cases, the cause of the error can be a bug in the \u003ccode\u003eHALFpipe\u003c/code\u003e code.\nPlease check that no similar issue has been reported\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/issues\"\u003ehere on GitHub\u003c/a\u003e. In this case,\nplease submit an\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/issues/new/choose\"\u003eissue\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-command-line-flags\" class=\"anchor\" href=\"#command-line-flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand line flags\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-control-command-line-logging\" class=\"anchor\" href=\"#control-command-line-logging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eControl command line logging\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--verbose\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBy default, only errors and warnings will be output to the command line. This\nmakes it easier to see when something goes wrong, because there is less output.\nHowever, if you want to be able to inspect what is being run, you can add the\n\u003ccode\u003e--verbose\u003c/code\u003e flag to the end of the command used to call \u003ccode\u003eHALFpipe\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eVerbose logs are always written to the \u003ccode\u003elog.txt\u003c/code\u003e file in the working directory,\nso going back and inspecting this log is always possible, even if the\n\u003ccode\u003e--verbose\u003c/code\u003e flag was not specified.\u003c/p\u003e\n\u003cp\u003eSpecifying the flag \u003ccode\u003e--debug\u003c/code\u003e will print additional, fine-grained messages. It\nwill also automatically start the\n\u003ca href=\"https://docs.python.org/3/library/pdb.html\" rel=\"nofollow\"\u003ePython Debugger\u003c/a\u003e when an error\noccurs. You should only use \u003ccode\u003e--debug\u003c/code\u003e if you know what you\u0027re doing.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-automatically-remove-unneeded-files\" class=\"anchor\" href=\"#automatically-remove-unneeded-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAutomatically remove unneeded files\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--keep\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e saves intermediate files for each pipeline step. This speeds up\nre-running with different settings, or resuming after a job after it was\ncancelled. The intermediate file are saved by the\n\u003ca href=\"https://nipype.readthedocs.io/\" rel=\"nofollow\"\u003e\u003ccode\u003enipype\u003c/code\u003e\u003c/a\u003e workflow engine, which is what\n\u003ccode\u003eHALFpipe\u003c/code\u003e uses internally. \u003ccode\u003enipype\u003c/code\u003e saves the intermediate files in the\n\u003ccode\u003enipype\u003c/code\u003e folder in the working directory.\u003c/p\u003e\n\u003cp\u003eIn environments with limited disk capacity, this can be problematic. To limit\ndisk usage, \u003ccode\u003eHALFpipe\u003c/code\u003e can delete intermediate files as soon as they are not\nneeded anymore. This behavior is controlled with the \u003ccode\u003e--keep\u003c/code\u003e flag.\u003c/p\u003e\n\u003cp\u003eThe default option \u003ccode\u003e--keep some\u003c/code\u003e keeps all intermediate files from fMRIPrep and\nMELODIC, which would take the longest to re-run. We believe this is a good\ntradeoff between disk space and computer time. \u003ccode\u003e--keep all\u003c/code\u003e turns of all\ndeletion of intermediate files. \u003ccode\u003e--keep none\u003c/code\u003e deletes as much as possible,\nmeaning that the smallest amount possible of disk space will be used.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-configure-nipype\" class=\"anchor\" href=\"#configure-nipype\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfigure nipype\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--nipype-\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eomp-nthreads\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003ememory-gb\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003en-procs\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003erun-plugin\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e chooses sensible defaults for all of these values.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-choose-which-parts-to-run-or-to-skip\" class=\"anchor\" href=\"#choose-which-parts-to-run-or-to-skip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChoose which parts to run or to skip\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eonly\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003eskip\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e-\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003espec-ui\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003eworkflow\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003erun\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003emodel-chunk\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eA \u003ccode\u003eHALFpipe\u003c/code\u003e run is divided internally into three stages, spec-ui, workflow, and\nrun.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe \u003ccode\u003espec-ui\u003c/code\u003e stage is where you specify things in the user interface. It\ncreates the \u003ccode\u003espec.json\u003c/code\u003e file that contains all the information needed to run\n\u003ccode\u003eHALFpipe\u003c/code\u003e. To only run this stage, use the option \u003ccode\u003e--only-spec-ui\u003c/code\u003e. To skip\nthis stage, use the option \u003ccode\u003e--skip-spec-ui\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eworkflow\u003c/code\u003e stage is where \u003ccode\u003eHALFpipe\u003c/code\u003e uses the \u003ccode\u003espec.json\u003c/code\u003e data to search\nfor all the files that match what was input in the user interface. It then\ngenerates a \u003ccode\u003enipype\u003c/code\u003e workflow for preprocessing, feature extraction and group\nmodels. \u003ccode\u003enipype\u003c/code\u003e then validates the workflow and prepares it for execution.\nThis usually takes a couple of minutes and cannot be parallelized. For\nhundreds of subjects, this may even take a few hours. This stage has the\ncorresponding option \u003ccode\u003e--only-workflow\u003c/code\u003e and \u003ccode\u003e--skip-workflow\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eThis stage saves several intermediate files. These are named\n\u003ccode\u003eworkflow.{uuid}.pickle.xz\u003c/code\u003e, \u003ccode\u003eexecgraph.{uuid}.pickle.xz\u003c/code\u003e and\n\u003ccode\u003eexecgraph.{n_chunks}_chunks.{uuid}.pickle.xz\u003c/code\u003e. The \u003ccode\u003euuid\u003c/code\u003e in the file name is\na unique identifier generated from the \u003ccode\u003espec.json\u003c/code\u003e file and the input files.\nIt is re-calculated every time we run this stage. The uuid algorithm produces\na different output if there are any changes (such as when new input files for\nnew subjects become available, or the \u003ccode\u003espec.json\u003c/code\u003e is changed, for example to\nadd a new feature or group model). Otherwise, the \u003ccode\u003euuid\u003c/code\u003e stays the same.\nTherefore, if a workflow file with the calculated \u003ccode\u003euuid\u003c/code\u003e already exists, then\nwe do not need to run this stage. We can simple re-use the workflow from the\nexisting file, and save some time.\u003c/li\u003e\n\u003cli\u003eIn this stage, we can also decide to split the execution into chunks. The flag\n\u003ccode\u003e--subject-chunks\u003c/code\u003e creates one chunk per subject. The flag \u003ccode\u003e--use-cluster\u003c/code\u003e\nautomatically activates \u003ccode\u003e--subject-chunks\u003c/code\u003e. The flag \u003ccode\u003e--n-chunks\u003c/code\u003e allows the\nuser to specify a specific number of chunks. This is useful if the execution\nshould be spread over a set number of computers. In addition to these, a model\nchunk is generated.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003eThe \u003ccode\u003erun\u003c/code\u003e stage loads the \u003ccode\u003eexecgraph.{n_chunks}_chunks.{uuid}.pickle.xz\u003c/code\u003e file\ngenerated in the previous step and runs it. This file usually contains two\nchunks, one for the subject level preprocessing and feature extraction\n(\"subject level chunk\"), and one for group statistics (\"model chunk\"). To run\na specific chunk, you can use the flags \u003ccode\u003e--only-chunk-index ...\u003c/code\u003e and\n\u003ccode\u003e--only-model-chunk\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-working-directory\" class=\"anchor\" href=\"#working-directory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorking directory\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--workdir\u003c/pre\u003e\u003c/div\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTODO\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data-file-system-root\" class=\"anchor\" href=\"#data-file-system-root\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData file system root\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--fs-root\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe \u003ccode\u003eHALFpipe\u003c/code\u003e container, or really most containers, contain the entire base\nsystem needed to run\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003eFor questions or support, please submit an\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/issues/new/choose\"\u003eissue\u003c/a\u003e or contact us\nvia e-mail.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eRole\u003c/th\u003e\n\u003cth\u003eE-mail address\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eLea Waller\u003c/td\u003e\n\u003ctd\u003eDeveloper\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:lea.waller@charite.de\"\u003elea.waller@charite.de\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eIlya Veer\u003c/td\u003e\n\u003ctd\u003eProject manager\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:ilya.veer@charite.de\"\u003eilya.veer@charite.de\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSusanne Erk\u003c/td\u003e\n\u003ctd\u003eProject manager\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:susanne.erk@charite.de\"\u003esusanne.erk@charite.de\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620930214.0
  },
  {
    "data_format": 2,
    "description": "Implementation of the Visual Structure from Motion algorithm optimized for plant branching structures.",
    "filenames": [
      "Singularity_colmap_vsfm",
      "Singularity_recipe/Singularity",
      "model_preprocess/Singularity"
    ],
    "full_name": "Computational-Plant-Science/3D_model_reconstruction_demo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-3d-root-model-reconstruction\" class=\"anchor\" href=\"#3d-root-model-reconstruction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3D root model reconstruction\u003c/h1\u003e\n\u003cp\u003eThe software package was integrated as a module at PlantIT website at : \u003ca href=\"https://portnoy.cyverse.org/\" rel=\"nofollow\"\u003ehttps://portnoy.cyverse.org/\u003c/a\u003e.\n(Collaborate with Cyverse \u003ca href=\"https://www.cyverse.org/\" rel=\"nofollow\"\u003ehttps://www.cyverse.org/\u003c/a\u003e ) . Users are welcomed to registered as an user to try this package via PlantIT website.\u003c/p\u003e\n\u003cp\u003eThe software package was also available at Dockerhub (\u003ca href=\"https://hub.docker.com/r/computationalplantscience/3d-model-reconstruction\" rel=\"nofollow\"\u003ehttps://hub.docker.com/r/computationalplantscience/3d-model-reconstruction\u003c/a\u003e) for advanced users to run locally via singularity at Linux environment:\u003c/p\u003e\n\u003cp\u003eThis software can be run by docker container, users do not need to install many libraries and compile complex source files.\u003c/p\u003e\n\u003cp\u003eFollowing was the demo of the comparision of real root and reconstructed 3D model model.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"../master/media/ProjectDemo.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"../master/media/ProjectDemo.gif\" alt=\"Optional Text\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-docker-container\" class=\"anchor\" href=\"#setup-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup Docker container\u003c/h1\u003e\n\u003cp\u003eOS requirements\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eTo install Docker container (https://docs.docker.com/engine/install/ubuntu/): \n\nTo install Docker Engine, you need the 64-bit version of one of these Ubuntu versions:\n\nUbuntu Groovy 20.10\nUbuntu Focal 20.04 (LTS)\nUbuntu Bionic 18.04 (LTS)\nUbuntu Xenial 16.04 (LTS)\n\nDocker Engine is supported on x86_64 (or amd64), armhf, and arm64 architectures.\n\nUninstall old versions\n$ sudo apt-get remove docker docker-engine docker.io containerd runc\n\nSet up the repository\n\nUpdate the apt package index and install packages to allow apt to use a repository over HTTPS:\n\n$ sudo apt-get update\n\n$ sudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\nAdd Docker\u2019s official GPG key:\n\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nVerify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88, by searching for the last 8 characters of the fingerprint.\n\n$ sudo apt-key fingerprint 0EBFCD88\n\npub   rsa4096 2017-02-22 [SCEA]\n      9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88\nuid           [ unknown] Docker Release (CE deb) \u0026lt;docker@docker.com\u0026gt;\nsub   rsa4096 2017-02-22 [S]\n\n$ sudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n\nUpdate the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version:\n\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\nVerify that Docker Engine is installed correctly by running the hello-world image.\n\n$ sudo docker run hello-world\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-this-container-by-building-it-locally\" class=\"anchor\" href=\"#run-this-container-by-building-it-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun this container by building it locally:\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003e# Clone source code to your local path\n$ git clone https://github.com/Computational-Plant-Science/3D_model_reconstruction_demo.git\n\n# Enter into the source code folder named as \"cd 3D_model_reconstruction_demo\"\n$ cd 3D_model_reconstruction_demo/\n\n# Build docker container locally named as \"3d_model_reconstruction\" using \"Dockerfile\" in the same folder, note: docker repository name must be lowercase.\n$ docker build -t 3d_model_reconstruction -f Dockerfile .\n\n# Run the docker container by linking docker container data path to user\u0027s image data folder local path\n# Note: please replace $path_to_image_folder as your local image data folder path, \n# Suggest to check your image folder path using \"pwd\" command\n# Example: $ docker run -v /home/suxing/example/root_images:/srv/images -it 3d_model_reconstruction\n\n$ docker run -v /$path_to_image_folder:/srv/images -it 3d_model_reconstruction\n\n# After launch the docker container, run \"pipeline.sh\" or \"pipeline.sh\" insider the container\n$ root@.....:/opt/code# python3 pipeline.py\n\n# Get 3d model result named as \"dense.0.ply\"\n# After the container was executed successfully with image data files, user should be able to see output in your command window like this:\n\u0027\u0027\u0027\nLoading option-0000.ply, 48656 vertices ...\nSave to /srv/images/dense.nvm ... done\nSave /srv/images/dense.0.ply ...done\n----------------------------------------------------------------\n\u0027\u0027\u0027\nThe 3D model file was in ply format(https://en.wikipedia.org/wiki/PLY_(file_format)), it is located inside your image folder, its name is \"dense.0.ply\".\npath = \"/$path_to_image_folder/dense.0.ply\"\n\nTo visualize the 3d model file, suggest to install Meshlab(https://www.meshlab.net/) or cloudcompare(https://www.danielgm.net/cc/)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-author\" class=\"anchor\" href=\"#author\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthor\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003esuxing liu(suxingliu@gmail.com)\nWesley Paul Bonelli(wbonelli@uga.edu)\n\nReference:\nVisualSFM\n[Anders Damsgaard](mailto:adamsgaard@ucsd.edu) with contributions by Caleb Adams and Connor P Doherty.\nChangchang Wu ( wucc1130@gmail.com )\n+ Structure from Motion\n[1] Changchang Wu, \"Towards Linear-time Incremental Structure From Motion\", 3DV 2013\n[2] Changchang Wu, \"VisualSFM: A Visual Structure from Motion System\", http://ccwu.me/vsfm/, 2011\n+ Bundle Adjustment\n[3] Changchang Wu, Sameer Agarwal, Brian Curless, and Steven M. Seitz, \"Multicore Bundle Adjustment\", CVPR 2011   \n+ Feature Detection\n[4] Changchang Wu, \"SiftGPU: A GPU implementation of Scale Invaraint Feature Transform (SIFT)\", http://cs.unc.edu/~ccwu/siftgpu, 2007\n\nCOLMAP\nhttps://colmap.github.io\nAuthor: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n@inproceedings{schoenberger2016sfm,\n    author={Sch\\\"{o}nberger, Johannes Lutz and Frahm, Jan-Michael},\n    title={Structure-from-Motion Revisited},\n    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year={2016},\n}\n\n@inproceedings{schoenberger2016mvs,\n    author={Sch\\\"{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm, Jan-Michael},\n    title={Pixelwise View Selection for Unstructured Multi-View Stereo},\n    booktitle={European Conference on Computer Vision (ECCV)},\n    year={2016},\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDocker container was maintained by Wesley Paul Bonelli. it was deployed to Plant IT website by Wesley Paul Bonelli (\u003ca href=\"mailto:wbonelli@uga.edu\"\u003ewbonelli@uga.edu\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eSingularity container overlay issues were solved by [Saravanaraj Ayyampalayam] (\u003ca href=\"https://github.com/raj76\"\u003ehttps://github.com/raj76\u003c/a\u003e) (mailto:\u003ca href=\"mailto:raj76@uga.edu\"\u003eraj76@uga.edu\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eSpecial thanks to Chris Cotter building the container recipe for testing and debugging.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-todo\" class=\"anchor\" href=\"#todo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTodo\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGPU cuda version container\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eGNU Public License\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 3,
    "topics": [
      "phenotyping",
      "phenotyping-algorithms",
      "root",
      "plant"
    ],
    "updated_at": 1619688010.0
  },
  {
    "data_format": 2,
    "description": "An oil land-spill and overland flow simulator for pipeline rupture events",
    "filenames": [
      "Singularityfiles/Singularity.v1.0.dev1",
      "Singularityfiles/Singularity.v1.0.dev3",
      "Singularityfiles/Singularity.v0.1.bionic",
      "Singularityfiles/Singularity.v1.0.dev4",
      "Singularityfiles/Singularity.v1.0.dev2",
      "Singularityfiles/Singularity.v0.1.trusty"
    ],
    "full_name": "barbagroup/geoclaw-landspill",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-geoclaw-landspill\" class=\"anchor\" href=\"#geoclaw-landspill\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egeoclaw-landspill\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/barbagroup/geoclaw-landspill/raw/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8ccf186e7288af6d88a1f6a930c0fcc4e7a8a9936b34e07629d815d1eab4d977/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.com/barbagroup/geoclaw-landspill\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7f22540fd3a0f3b9a8d8a57ead3744961fd8b2d9edd257d02e4a8e0ae1d6d7a6/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f636f6d2f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6d61737465723f6c6162656c3d5472617669732532304349\" alt=\"Travis CI\" data-canonical-src=\"https://img.shields.io/travis/com/barbagroup/geoclaw-landspill/master?label=Travis%20CI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/barbagroup/geoclaw-landspill/actions?query=workflow%3ACI\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/91894ad74ed7c9cc23b3f2fb08cc1ea432c8ed6830abd92e489e7f59ac619ab3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f43492f6d61737465723f6c6162656c3d476974487562253230416374696f6e2532304349\" alt=\"GitHub Action CI\" data-canonical-src=\"https://img.shields.io/github/workflow/status/barbagroup/geoclaw-landspill/CI/master?label=GitHub%20Action%20CI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/036ff156f41dafdb919e29a22cd5aa00a7f0ded742b75831240ea25fe720350e/68747470733a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f66623762303132373939613730633962346335356562346262306633366639372f7374617475732e737667\" alt=\"status\" data-canonical-src=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97/status.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/barbagroup/geoclaw-landspill\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9e96d790dd4b2cdbdea7b49eff75628a95ae5cbd3c8f5b7bc902b7f9b603b149/68747470733a2f2f616e61636f6e64612e6f72672f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6261646765732f696e7374616c6c65722f636f6e64612e737667\" alt=\"Conda\" data-canonical-src=\"https://anaconda.org/barbagroup/geoclaw-landspill/badges/installer/conda.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eNote: if looking for content of \u003ccode\u003egeoclaw-landspill-cases\u003c/code\u003e, please checkout tag\n\u003ccode\u003ev0.1\u003c/code\u003e. This repository has been converted to a fully working solver package.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003egeoclaw-landspill\u003c/em\u003e is a package for running oil overland flow simulations for\napplications in pipeline risk management. It includes a numerical solver and\nsome pre-/post-processing utilities.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"./doc/sample.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./doc/sample.gif\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe numerical solver is a modified version of\n\u003ca href=\"http://www.clawpack.org/geoclaw.html\" rel=\"nofollow\"\u003eGeoClaw\u003c/a\u003e.\nGeoClaw solves full shallow-water equations. We added several new features and\nutilities to it and make it usable to simulate the overland flow from pipeline\nruptures. These features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eadding point sources to mimic the rupture points\u003c/li\u003e\n\u003cli\u003eadding evaporation models\u003c/li\u003e\n\u003cli\u003eadding Darcy-Weisbach bottom friction models with land roughness\u003c/li\u003e\n\u003cli\u003eadding temperature-dependent viscosity\u003c/li\u003e\n\u003cli\u003erecording detail locations and time of oil flowing into in-land waterbodies\u003c/li\u003e\n\u003cli\u003edownloading topography and hydrology data automatically (the US only)\u003c/li\u003e\n\u003cli\u003egenerating CF-1.7 compliant NetCDF files\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"doc/deps_install_tests.md\"\u003eDependencies, installation, and tests\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"doc/usage.md\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"doc/configuration.md\"\u003eConfiguration file: \u003ccode\u003esetrun.py\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"cases/README.md\"\u003eExample cases\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"doc/container.md\"\u003eContainers: Docker and Singularity\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003cp\u003eWe only maintain compatibility with Linux. Though using \u003ccode\u003epip\u003c/code\u003e or building from\nsource may still work in Mac OS or Windows (e.g., through WSL), we are not able\nto help with the installation issues on these two systems.\u003c/p\u003e\n\u003cp\u003eBeyond this quick start, to see more details, please refer to the\n\u003ca href=\"#documentation\"\u003edocumentation\u003c/a\u003e section.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-installation\" class=\"anchor\" href=\"#1-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Installation\u003c/h3\u003e\n\u003cp\u003eThe fast way to install \u003cem\u003egeoclaw-landspill\u003c/em\u003e is through\n\u003ca href=\"https://www.anaconda.com/\" rel=\"nofollow\"\u003eAnaconda\u003c/a\u003e\u0027s \u003ccode\u003econda\u003c/code\u003e command. The following command\ncreates a conda environment (called \u003ccode\u003elandspill\u003c/code\u003e) and installs the package and\ndependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda create \\\n    -n landspill -c barbagroup -c conda-forge \\\n    python=3.8 geoclaw-landspill\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen use \u003ccode\u003econda activate landspill\u003c/code\u003e or\n\u003ccode\u003esource \u0026lt;conda installation prefix\u0026gt;/bin/activate landspill\u003c/code\u003e to activate the\nenvironment. Type \u003ccode\u003egeoclaw-landspill --help\u003c/code\u003e in the terminal to see if\n\u003cem\u003egeoclaw-landspill\u003c/em\u003e is correctly installed.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-running-an-example-case\" class=\"anchor\" href=\"#2-running-an-example-case\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Running an example case\u003c/h3\u003e\n\u003cp\u003eTo run an example case under the folder \u003ccode\u003ecases\u003c/code\u003e, users have to clone this\nrepository. We currently don\u0027t maintain another repository for cases. After\ncloning this repository, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ geoclaw-landspill run \u0026lt;path to an example case folder\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor example, to run \u003ccode\u003eutal-flat-maya\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ geoclaw-landspill run ./cases/utah-flat-maya\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUsers can use environment variable \u003ccode\u003eOMP_NUM_THREADS\u003c/code\u003e to control how many CPU\nthreads the simulation should use for OpenMP parallelization.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-creating-a-cf-compliant-netcdf-raster-file\" class=\"anchor\" href=\"#3-creating-a-cf-compliant-netcdf-raster-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Creating a CF-compliant NetCDF raster file\u003c/h3\u003e\n\u003cp\u003eAfter a simulation is done, users can convert flow depth in raw simulation data\ninto a CF-compliant NetCDF raster file. For example,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ geoclaw-landspill createnc ./case/utah-flat-maya\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReplace \u003ccode\u003e./cases/utah-flat-maya\u003c/code\u003e with the path to another desired case.\u003c/p\u003e\n\u003cp\u003eQGIS and ArcGIS should be able to read the resulting NetCDF raster file.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-third-party-codes-and-licenses\" class=\"anchor\" href=\"#third-party-codes-and-licenses\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThird-party codes and licenses\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eamrclaw: \u003ca href=\"https://github.com/clawpack/amrclaw\"\u003ehttps://github.com/clawpack/amrclaw\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/amrclaw/blob/ee85c1fe178ec319a8403503e779d3f8faf22840/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003egeoclaw: \u003ca href=\"https://github.com/clawpack/geoclaw\"\u003ehttps://github.com/clawpack/geoclaw\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/geoclaw/blob/3593cb1b418fd52739c186a8845a288037c8f575/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003epyclaw: \u003ca href=\"https://github.com/clawpack/pyclaw\"\u003ehttps://github.com/clawpack/pyclaw\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/pyclaw/blob/a85a01a5f20be1a18dde70b7bb37dc1cdcbd0b26/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eclawutil: \u003ca href=\"https://github.com/clawpack/clawutil\"\u003ehttps://github.com/clawpack/clawutil\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/clawutil/blob/116ffb792e889fbf0854d7ac599657039d7b1f3e/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eriemann: \u003ca href=\"https://github.com/clawpack/riemann\"\u003ehttps://github.com/clawpack/riemann\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/riemann/blob/597824c051d56fa0c8818e00d740867283329b24/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eSee \u003ca href=\"CONTRIBUTING.md\"\u003eCONTRIBUTING.md\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003ePi-Yueh Chuang: \u003ca href=\"mailto:pychuang@gwu.edu\"\u003epychuang@gwu.edu\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 5,
    "topics": [
      "geoclaw",
      "overland-flow",
      "pipeline",
      "shallow-water-equations",
      "pipeline-ruptures",
      "land-spill"
    ],
    "updated_at": 1621675162.0
  },
  {
    "data_format": 2,
    "description": "screening metagenomes for arbitrary lineages, using gene-centric assembly methods and phylogenetics",
    "filenames": [
      "Singularity"
    ],
    "full_name": "maxemil/PhyloMagnet",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"http://phylomagnet.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/29e96fe88e81eead9542ac2fcef38f609b35c29fcecfdf5ed2c5517a534d9b20/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7068796c6f6d61676e65742f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Docs Status\" data-canonical-src=\"https://readthedocs.org/projects/phylomagnet/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.org/maxemil/PhyloMagnet\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4fe367ba9e712aa4068f6e691da8264f15ce96872f20bdfed1656d9b6eba78b7/68747470733a2f2f7472617669732d63692e6f72672f6d6178656d696c2f5068796c6f4d61676e65742e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/maxemil/PhyloMagnet.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.singularity-hub.org/collections/978\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9b0b8670bab3cab652cf5c31fdae614cf89b2ceb2e013cd2d7dd570e9f8530f2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d73696e67756c61726974792d2d6875622d626c75652e737667\" alt=\"Hosted\" data-canonical-src=\"https://img.shields.io/badge/hosted-singularity--hub-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-phylomagnet\" class=\"anchor\" href=\"#phylomagnet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePhyloMagnet\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-for-screening-metagenomes-looking-for-arbitrary-lineages-using-gene-centric-assembly-methods-and-phylogenetics\" class=\"anchor\" href=\"#pipeline-for-screening-metagenomes-looking-for-arbitrary-lineages-using-gene-centric-assembly-methods-and-phylogenetics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline for screening metagenomes, looking for arbitrary lineages, using gene-centric assembly methods and phylogenetics\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-abstract\" class=\"anchor\" href=\"#abstract\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eMotivation: Metagenomic and metatranscriptomic sequencing analyses have become increasingly popular tools for producing massive amounts of short-read data, often used for the reconstruction of draft genomes or the detection of (active) genes in microbial communities. Unfortunately, sequence assemblies of such datasets generally remain a computationally challenging task. Frequently, researchers are only interested in a specific group of organisms or genes; yet, the assembly of multiple datasets only to identify candidate sequences for a specific question is sometimes prohibitively slow, forcing researchers to select a subset of available datasets to address their question. Here we present PhyloMagnet, a workflow to screen meta-omics datasets for taxa and genes of interest using gene-centric assembly and phylogenetic placement of sequences.\nResults: Using PhyloMagnet, we could identify up to 87% of the genera in an in vitro mock community with variable abundances, while the false positive predictions per single gene tree ranged from 0% to 23%. When applied to a group of metagenomes for which a set of MAGs have been published, we could detect the majority of the taxonomic labels that the MAGs had been annotated with. In a metatranscriptomic setting the phylogenetic placement of assembled contigs corresponds to that of transcripts obtained from transcriptome assembly. See \u003ca href=\"https://github.com/maxemil/PhyloMagnet-benchmarks\"\u003ehttps://github.com/maxemil/PhyloMagnet-benchmarks\u003c/a\u003e for benchmark experiments.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-installation--usage\" class=\"anchor\" href=\"#quick-installation--usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick installation \u0026amp; usage\u003c/h2\u003e\n\u003cp\u003eFor detailed documentation, please visit \u003ca href=\"http://phylomagnet.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ehttp://phylomagnet.readthedocs.io/en/latest/\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e download the image with all tools installed using singularity 3x\u003c/span\u003e\nsingularity pull --name PhyloMagnet.sif shub://maxemil/PhyloMagnet:latest\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e get versions of tools used in the pipeline:\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e PhyloMagnet.sif conda list -n PhyloMagnet-\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eversion\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e execute the test pipeline with nextflow\u003c/span\u003e\nnextflow run main.nf \\\n          -with-singularity PhyloMagnet.sif \\\n          --is_runs \u003cspan class=\"pl-c1\"\u003etrue\u003c/span\u003e \\\n          --fastq \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etest/*rpoB.fastq.gz\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n          --reference_packages \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etest/rpkgs/*\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n          --lineage \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eorder\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n          --megan_eggnog_map eggnog.map \\\n          --cpus 2 \\\n          --is_runs \u003cspan class=\"pl-c1\"\u003etrue\u003c/span\u003e \\\n          --queries_dir test/queries \\\n          --reference_dir test/references \\\n          --phylo_method \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003efasttree\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \\\n          --align_method \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003emafft-fftnsi\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \\\n          -w test/work -resume\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing-phylomagnet\" class=\"anchor\" href=\"#citing-phylomagnet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting PhyloMagnet\u003c/h2\u003e\n\u003cp\u003ePhyloMagnet is published in Bioinformatics:\nMax E Sch\u00f6n, Laura Eme, Thijs J G Ettema, PhyloMagnet: fast and accurate screening of short-read meta-omics data using gene-centric phylogenetics, Bioinformatics, btz799, \u003ca href=\"https://doi.org/10.1093/bioinformatics/btz799\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/bioinformatics/btz799\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePlease make sure to also cite all tools that are used in the pipeline if you use it for your research! Visit \u003ca href=\"http://phylomagnet.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ehttp://phylomagnet.readthedocs.io/en/latest/\u003c/a\u003e or see the startup message for details.\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 1,
    "topics": [
      "metagenomics",
      "next-generation-sequencing",
      "phylogenetics",
      "nextflow",
      "singularity-container",
      "evolution"
    ],
    "updated_at": 1619042816.0
  },
  {
    "data_format": 2,
    "description": "A collection of Dockerfiles and Singularity deffiles for Pawsey-supported images",
    "filenames": [
      "OpenFOAM/installationsWithAdditionalTools/openfoam-5.x_CFDEM/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-7/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-v1712/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-v2006/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-v1812/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-v1912/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-5.x/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-v2012/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-2.2.0/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-2.4.x/02_PortingToSingularity/Singularity.def",
      "OpenFOAM/basicInstallations/openfoam-8/02_PortingToSingularity/Singularity.def"
    ],
    "full_name": "PawseySC/pawsey-containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pawsey-containers\" class=\"anchor\" href=\"#pawsey-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epawsey-containers\u003c/h1\u003e\n\u003cp\u003eThis is a repo to store and track Dockerfiles and Singularity deffiles for images built and maintained by Pawsey Supercomputing Centre.\u003c/p\u003e\n\u003cp\u003eDocker repo: \u003ca href=\"https://hub.docker.com/u/pawsey\" rel=\"nofollow\"\u003ehttps://hub.docker.com/u/pawsey\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1617165302.0
  },
  {
    "data_format": 2,
    "description": "H3A variant calling pipeline",
    "filenames": [
      "containers/Singularity.gatk",
      "containers/Singularity.trimmomatic",
      "containers/Singularity.multiqc",
      "containers/Singularity.fastqc",
      "containers/Singularity.bwa"
    ],
    "full_name": "h3abionet/h3avarcall",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-h3avarcall---h3abionet-variant-calling-pipeline\" class=\"anchor\" href=\"#h3avarcall---h3abionet-variant-calling-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003eh3avarcall\u003c/code\u003e - H3ABioNet Variant Calling Pipeline\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003eh3avarcall\u003c/code\u003e is a \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003ccode\u003eNextflow\u003c/code\u003e\u003c/a\u003e pipeline developed by \u003ca href=\"https://www.h3abionet.org/\" rel=\"nofollow\"\u003e\u003ccode\u003eH3ABioNet\u003c/code\u003e\u003c/a\u003e for genomic Variant Calling allowing to detect SNPs and Indels giving raw sequence reads (fastq files) as input. \u003ccode\u003eh3avarcall\u003c/code\u003e includes the different steps from aligning raw sequence reads to variant calling and filtering using GATK. \u003cbr\u003e\nFor more details about the different steps of the pipeline, check the [H3ABionet SOPs pages] \u003ca href=\"https://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling\" rel=\"nofollow\"\u003ehttps://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling\u003c/a\u003e\n\u003ccode\u003eh3avarcall\u003c/code\u003e is a modular and extensible tool allowing users to run the whole pipeline, use only parts of it and also to easily enrich it and adapt it to their needs. \u003ccode\u003eh3avarcall\u003c/code\u003e generates a number of intermediate files where results from various steps of the pipeline are stored.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-1-obtaining-pipeline-and-preparing-data\" class=\"anchor\" href=\"#1-obtaining-pipeline-and-preparing-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Obtaining pipeline and preparing Data\u003c/h2\u003e\n\u003cp\u003eFirst, you need to clone the \u003ccode\u003eh3avarcall\u003c/code\u003e repository onto you machine:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/h3abionet/h3avarcall.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e h3avarcall\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eContent of the repository:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eh3avarcall\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--containers                       \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Folder for Singularity images and recipes (in case you want to build yourself). All downloaded images go here!\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--Singularity.bwa               \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Singularity recipe file for BWA and Samtools.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--Singularity.fastqc            \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Singularity recipe file for FastQC.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--Singularity.gatk              \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Singularity recipe file for GATK and tabix.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--Singularity.trimmomatic       \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Singularity recipe file for Trimmimatic.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--gatk-b37-bundle                  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Folder for stoding downloaded GATK-b37-bundle files.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--b37_files_minimal.txt         \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# LList of GATK-b37-bundle files to be downloaded (bundle TOO BIG! Only selected files needed for the pipeline). \u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--templates                        \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Folder for extra scripts for the pipeline.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--download_bundles.sh           \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Script for downloading GATK-b37-bundle.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--LICENSE                          \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Duh!\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--README.md                        \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Duh!\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--main.config                      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# User configuration file! All inputs, outputs and options GO HERE!! ONLY file that SHOULD be modified by user!\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--main.nf                          \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Main h3avarcall nextflow scripts.\u003c/span\u003e\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--nextflow.config                  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# Pipeline configuration file! DO NOT EDIT!!!\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe \u003ccode\u003emain.config\u003c/code\u003e file:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-groovy\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e/*\u003c/span\u003e==================================================================================================\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * THIS FILE IS USED TO SPECIFY INPUT, OUTPUTS AND PARAMETERS. THE FOLLOWING OPTIONS ARE THE ALLOWED:\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * ==================================================================================================\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * data         : Path to where the data is (FASTQ files).\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * out          : Path to store output results.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * bundle       : GATK-b37-bundle list file.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * mode         : Worflow step to perform. Can be any of [ do.GetContainers | do.GenomeIndexing | do.QC | do.ReadTrimming | do.ReadAlignment | do.VarianCalling | do.VariantFiltering | do.MultiQC].\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * trim         : Trimming options for Trimmomatic.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * resources    : Location of the GATK-b37-bundle folder.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * from         : pipeline step to resume pipeline from. Can be any of [ do.QC | do.ReadTrimming | do.ReadAlignment | do.VarianCalling | do.VariantFiltering ].\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * params.help  : Print help menu.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * ==================================================================================================\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * BELOW ARE THE DEFAULT PARAMETERS! YOU\u0027RE MORE THAN WELCOME TO CHANGE AS DESIRED!\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e * ==================================================================================================\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e \u003cspan class=\"pl-c\"\u003e*/\u003c/span\u003e\u003c/span\u003e\n\nparams {\n    data         \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$b\u003cspan class=\"pl-smi\"\u003easeDir\u003c/span\u003e\u003c/span\u003e/data\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    out          \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$b\u003cspan class=\"pl-smi\"\u003easeDir\u003c/span\u003e\u003c/span\u003e/results\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    bundle       \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$b\u003cspan class=\"pl-smi\"\u003easeDir\u003c/span\u003e\u003c/span\u003e/gatk-b37-bundle/b37_files_minimal.txt\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    mode         \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edo.QC\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    trim         \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:8:true TRAILING:28 MINLEN:40\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    resources    \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$b\u003cspan class=\"pl-smi\"\u003easeDir\u003c/span\u003e\u003c/span\u003e/gatk-b37-bundle\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    from         \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003enull\u003c/span\u003e\n    params\u003cspan class=\"pl-k\"\u003e.\u003c/span\u003ehelp  \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003enull\u003c/span\u003e\n}\n\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-11-download-test-datasets\" class=\"anchor\" href=\"#11-download-test-datasets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.1. Download test datasets:\u003c/h3\u003e\n\u003cp\u003eCreate a data directory under the \u003ccode\u003eh3avarcall\u003c/code\u003e repository:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir data\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e data\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eDownload the test data from \u003ca href=\"http://thesite.com\" rel=\"nofollow\"\u003eTHIS_SITE\u003c/a\u003e using one of the commands bellow:\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-112-using-lftp-faster\" class=\"anchor\" href=\"#112-using-lftp-faster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.1.2. Using LFTP (faster)\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003elftp -e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epget -n 20 ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R1_001.fastq.gz; bye\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nlftp -e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epget -n 20 ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R2_001.fastq.gz; bye\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-113-using-wget-slower\" class=\"anchor\" href=\"#113-using-wget-slower\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.1.3. Using WGET (slower)\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ewget ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R1_001.fastq.gz\nwget ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/Garvan_NA12878_HG001_HiSeq_Exome/NIST7035_TAAGGCGA_L001_R2_001.fastq.gz\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-12-download-the-singularity-containers-required-to-execute-the-pipeline\" class=\"anchor\" href=\"#12-download-the-singularity-containers-required-to-execute-the-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.2. Download the \u003ccode\u003eSingularity\u003c/code\u003e containers (required to execute the pipeline):\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.GetContainers\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-13-download-the-gatk-b37-bundle-required-to-execute-the-pipeline\" class=\"anchor\" href=\"#13-download-the-gatk-b37-bundle-required-to-execute-the-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.3. Download the GATK b37 bundle (required to execute the pipeline):\u003c/h3\u003e\n\u003cp\u003eThis step takes \u003cstrong\u003eFOREVER\u003c/strong\u003e to run - run it only once!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.GenomeIndexing\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf by some miracle you happen to have access to the WITS Cluster, you do not need to download the GATK-b37-bundle! Simply \u003ccode\u003ecd\u003c/code\u003e into the \u003ccode\u003egatk-b37-bundle\u003c/code\u003e folder of the \u003ccode\u003eh3avarcall\u003c/code\u003e repo and soft-link the GATK-b37-bundle data as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd gatk-b37-bundle\nln -s /global/blast/gatk-bundle/b37/* .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-executing-the-main-h3avarcall-pipeline\" class=\"anchor\" href=\"#2-executing-the-main-h3avarcall-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Executing the main \u003ccode\u003eh3avarcall\u003c/code\u003e pipeline\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-21-read-qc-optional-\" class=\"anchor\" href=\"#21-read-qc-optional-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.1. Read QC (optional): \u003cbr\u003e\n\u003c/h3\u003e\n\u003cp\u003eBefore getting started with the downstream analysis, it\u0027s always good to do some quality checks on your raw sequences to assess the quality of raw sequence data, the fastq files. FastQC tool has been used in this workflow. An html report page will be automatically created for each fastq file. You can load up these html pages in your browser to assess your data through graphs and summary tables.\u003cbr\u003e\nTo perform the QC of your fastq files, you can use this command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.QC\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-22-read-trimming-optional\" class=\"anchor\" href=\"#22-read-trimming-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.2. Read Trimming (optional):\u003cbr\u003e\n\u003c/h3\u003e\n\u003cp\u003eAfter performing the QC of your fastq files, you have an idea about the quality of your reads: some of your reads might not be of a very good quality or the quality might drop at some positions (near the begining or end of reads) across all reads and this requires to clean up your library to minimize biaises in your analysis by filtering poor quality reads and/or trim poor quality bases from our samples. Trimmomatic is the trimming tool that has been used here. \u003cbr\u003e\nFor more information about reads preprocessing, check this \u003ca href=\"https://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling#phase-1-preprocessing-of-the-raw-reads\" rel=\"nofollow\"\u003epage\u003c/a\u003e. \u003cbr\u003e\nTo run the trimming step of the \u003ccode\u003eh3avarcall\u003c/code\u003e pipeline, you can use this command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.ReadTrimming\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-23-read-alignment-\" class=\"anchor\" href=\"#23-read-alignment-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.3. Read Alignment \u003cbr\u003e\n\u003c/h3\u003e\n\u003cp\u003eOnce you have good raw sequences quality, the next step is to map your reads to a reference genome to determine where in the genome the reads originated from. The mapper used in this workflow is BWA.  For more information about the read alignement step, check this \u003ca href=\"https://h3abionet.github.io/H3ABionet-SOPs/Variant-Calling#phase-2-initial-variant-discovery\" rel=\"nofollow\"\u003epage\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCan be run with \u003ccode\u003e--from do.ReadTrimming\u003c/code\u003e or \u003ccode\u003e--from do.QC\u003c/code\u003e depending on whether these steps were run!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.ReadAlignment\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-24-variant-calling\" class=\"anchor\" href=\"#24-variant-calling\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.4. Variant Calling\u003c/h3\u003e\n\u003cp\u003eThis step uses the outputs generated by the Read Alignment STEP! \u003cstrong\u003eMUST\u003c/strong\u003e run STEP 2.3 (\u003ccode\u003e--mode do.ReadAlignment\u003c/code\u003e) before running this step.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.VariantCalling \u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-25-variant-filtering\" class=\"anchor\" href=\"#25-variant-filtering\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.5. Variant Filtering\u003c/h3\u003e\n\u003cp\u003eThis step uses the outputs generated by the Variant Calling STEP! \u003cstrong\u003eMUST\u003c/strong\u003e run STEP 2.4 (\u003ccode\u003e--mode do.VariantCalling\u003c/code\u003e) before running this step.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.VariantFiltering \u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-26-workflow-qc-multiqc---optional\" class=\"anchor\" href=\"#26-workflow-qc-multiqc---optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.6. Workflow QC (MultiQC - Optional)\u003c/h3\u003e\n\u003cp\u003eThis step performs a Quality Check of the different pipeline steps that have been ran. You need to run at least ONE step of the pipeline to be able to run this MultiQC step!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run main.nf -profile slurm --mode do.MultiQC \u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-3-explore-h3avarcall-results\" class=\"anchor\" href=\"#3-explore-h3avarcall-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Explore \u003ccode\u003eh3avarcall\u003c/code\u003e results\u003c/h2\u003e\n\u003cp\u003eAssuming you did not change the  default output folder (in the \u003ccode\u003emain.config\u003c/code\u003e file), the resulting files will be found in the \u003ccode\u003eresults\u003c/code\u003e folder of the \u003ccode\u003eh3avarcall\u003c/code\u003e repository. Resulting files for each of the main pipeline steps (\u003ccode\u003e2.1\u003c/code\u003e - \u003ccode\u003e2.5\u003c/code\u003e) are grouped in different folders as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- [1] Read QC (optional)         =\u0026gt;    `results/1_QC`\n- [2] Read Trimming (optional)   =\u0026gt;    `results/2_Read_Trimming`\n- [3] Read Alignment             =\u0026gt;    `results/3_Read_Alignment`\n- [4] Variant Calling            =\u0026gt;    `results/4_Variant_Calling`\n- [5] Variant Filtering          =\u0026gt;    `results/5_Variant_Filtering`\n- [6] MultiQC                    =\u0026gt;    `results/MultiQC`\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn each of these folders, a sub-folder \"\u003ccode\u003eworkflow_report\u003c/code\u003e\"  is created. It  contains 4 different files (\u003ccode\u003eh3avarcall_report.html\u003c/code\u003e, \u003ccode\u003eh3avarcall_timeline.html\u003c/code\u003e, \u003ccode\u003eh3avarcall_workflow.dot\u003c/code\u003e and \u003ccode\u003eh3avarcall_trace.txt\u003c/code\u003e) containing detailed information on the resources (CPU, MEMORY and TIME) usage of each process in the different pipeline steps. \u003cbr\u003e\nThe \u003ccode\u003eresults\u003c/code\u003e directory structure within \u003ccode\u003eh3avarcall\u003c/code\u003e repository can be summarized as below:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eh3avarcall\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--results\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--1_QC\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--workflow_report\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_report.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_timeline.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_workflow.dot\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_trace.txt\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_\u003cspan class=\"pl-k\"\u003e1\u0026gt;\u003c/span\u003e_R1.fastqc.html .. \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_N\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e_R1.fastqc.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_\u003cspan class=\"pl-k\"\u003e1\u0026gt;\u003c/span\u003e_R2.fastqc.html .. \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_N\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e_R1.fastqc.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--2_Read_Trimming\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--workflow_report\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_report.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_timeline.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_workflow.dot\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_trace.txt\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_\u003cspan class=\"pl-k\"\u003e1\u0026gt;\u003c/span\u003e.1P.fastq.gz .. \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_N\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.1P.fastq.gz\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_\u003cspan class=\"pl-k\"\u003e1\u0026gt;\u003c/span\u003e.2P.fastq.gz .. \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_N\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.2P.fastq.gz\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--3_Read_Alignment\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--workflow_report\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_report.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_timeline.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_workflow.dot\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_trace.txt\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_\u003cspan class=\"pl-k\"\u003e1\u0026gt;\u003c/span\u003e_md.recal.bam .. \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_N\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e_md.recal.bam\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_\u003cspan class=\"pl-k\"\u003e1\u0026gt;\u003c/span\u003e_md.recal.bai .. \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esample_N\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e_md.recal.bai\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--4_Variant_Calling\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--workflow_report\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_report.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_timeline.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_workflow.dot\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_trace.txt\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--chr_1_genotyped.vcf.gz .. chr_22_genotyped.vcf.gz\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--chr_1_genotyped.vcf.gz.tbi .. chr_22_genotyped.vcf.gz.tbi\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--5_Variant_Filtering\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--workflow_report\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_report.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_timeline.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_workflow.dot\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--h3avarcall_trace.txt\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--genome.SNP-recal.vcf.gz\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--genome.SNP-recal.vcf.gz.tbi\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--MultiQC\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--multiqc_data\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--multiqc_report.html\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--work\n  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e  \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eThere\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003es a lot of folders here! Lets not worry about them for today!\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e##We\u0027re working on further improving the pipleine and the associated documentation, feel free to share comments and suggestions!\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 8,
    "topics": [],
    "updated_at": 1618936209.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "UCLBrain/MSLS",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/UCLBrain/MSLS/issues\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f9cab98cc8f1052f0c0096b8b462cf9b2280a706b1adc0895d8a4859f3743314/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f55434c427261696e2f4d534c53\" alt=\"GitHub issues\" data-canonical-src=\"https://img.shields.io/github/issues/UCLBrain/MSLS\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UCLBrain/MSLS/network\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b6c7a087c43d2a65a6ee22ec8ce2e004a29212c4b9619b117f4b2bbabf98a6c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f55434c427261696e2f4d534c53\" alt=\"GitHub forks\" data-canonical-src=\"https://img.shields.io/github/forks/UCLBrain/MSLS\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UCLBrain/MSLS/stargazers\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/10fab859502fd8c9b24dde937e50fecba7449e94ea057774713238ab3ec754fc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f55434c427261696e2f4d534c53\" alt=\"GitHub stars\" data-canonical-src=\"https://img.shields.io/github/stars/UCLBrain/MSLS\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UCLBrain/MSLS/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0d525d837b03e3b7a24b6322fc2197a134fa12e6a96188e5f18d6cd92236aa47/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f55434c427261696e2f4d534c53\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UCLBrain/MSLS\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-multi-label-multisingle-class-image-segmentation\" class=\"anchor\" href=\"#multi-label-multisingle-class-image-segmentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMulti-Label Multi/Single-Class Image Segmentation\u003c/h1\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"images/diag.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"510\" src=\"images/diag.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-publication\" class=\"anchor\" href=\"#publication\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePublication\u003c/h1\u003e\n\u003cp\u003eLe Zhang, Ryutaro Tanno, Kevin Bronik, Chen Jin, Parashkev Nachev, Frederik Barkhof, Olga Ciccarelli, and Daniel C. Alexander, Learning to Segment When Experts Disagree, International Conference on Medical image computing and Computer-Assisted Intervention (MICCAI). Springer, Cham, 2020.\u003c/p\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"images/Miccai_2020_abs.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg align=\"center\" height=\"1000\" src=\"images/Miccai_2020_abs.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\n    \n        \u003cp\u003eClick here to see full pdf file: \u003ca href=\"https://github.com/UCLBrain/MSLS/blob/master/MICCAI_2020.pdf\"\u003eLink to PDF\u003c/a\u003e\u003c/p\u003e\n    \n\n\u003ch1\u003e\n\u003ca id=\"user-content-running-the-gui-program\" class=\"anchor\" href=\"#running-the-gui-program\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the GUI Program!\u003c/h1\u003e\n\u003cp\u003eFirst, user needs to install Anaconda \u003ca href=\"https://www.anaconda.com/\" rel=\"nofollow\"\u003ehttps://www.anaconda.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThen\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - conda env create -f conda_environment_Training_Inference.yml  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - conda activate traintestenv  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003efinally\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - python  Training_Inference_GUI.py \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAfter lunching the graphical user interface, user will need to provide necessary information to start training/testing as follows:\u003c/p\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"images/GUI.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"510\" src=\"images/GUI.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-running-the-program-from-the-command-line\" class=\"anchor\" href=\"#running-the-program-from-the-command-line\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the Program from the command line!\u003c/h1\u003e\n\u003cp\u003eFirst\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - conda activate traintestenv  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ethen for training\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - python  segmentation_network_Training_without_GUI.py  [or annotation_network_Training_without_GUI.py]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003efor testing\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - python  segmentation_network_Inference_without_GUI.py  [or annotation_network_Inference_without_GUI.py]\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-testing-the-program\" class=\"anchor\" href=\"#testing-the-program\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting the Program!\u003c/h1\u003e\n\u003cp\u003eExamples of Training and Testing subjects can be found in: \u003ca href=\"https://github.com/UCLBrain/MSLS/tree/master/examples\"\u003ehttps://github.com/UCLBrain/MSLS/tree/master/examples\u003c/a\u003e (which will allow users to quickly and easily train and test the program)\u003c/p\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"images/bin_seg_ex.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"510\" src=\"images/bin_seg_ex.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e..................................................................................................................................................................\u003c/p\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"images/multi_seg_ex.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"510\" src=\"images/multi_seg_ex.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n",
    "stargazers_count": 5,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621237070.0
  },
  {
    "data_format": 2,
    "description": "definition files and wrapper scripts used by NIH HPC staff to install user-facing apps on the Biowulf cluster",
    "filenames": [
      "sequence-analysis/saige/0.44.1/saige.def",
      "sequence-analysis/cicero/1.4.0/cicero.def",
      "sequence-analysis/sonicparanoid/1.3.2/sonicparanoid.def",
      "sequence-analysis/sonicparanoid/1.3.5/sonicparanoid.def",
      "sequence-analysis/orffinder/0.4.3-sing-install/orffinder.def",
      "sequence-analysis/smoove/0.2.5/smoove.def",
      "sequence-analysis/smoove/0.2.1/smoove.def",
      "sequence-analysis/htgtsrep/9fe74ff/htgtsrep.def",
      "sequence-analysis/xhla/2018-04-04/xhla.def",
      "sequence-analysis/accurity/20180724/accurity.def",
      "sequence-analysis/accurity/20210209/accurity.def",
      "sequence-analysis/cactus/1.2.3/cactus.def",
      "sequence-analysis/anvio/7/anvio.def",
      "sequence-analysis/glnexus/1.1.11/glnexus.def",
      "sequence-analysis/glnexus/1.2.7/glnexus.def",
      "sequence-analysis/vcf-kit/0.1.6/vcf-kit.def",
      "sequence-analysis/mmarge/1.0/mmarge.def",
      "sequence-analysis/roary/3.12.0/roary.def",
      "sequence-analysis/roary/3.13.0/roary.def",
      "sequence-analysis/wisexome/20180814/wisexome.def",
      "sequence-analysis/glu/1.0b3/glu.def",
      "sequence-analysis/arriba/1.2.0/arriba.def",
      "sequence-analysis/arriba/2.0.0/arriba.def",
      "sequence-analysis/intarna/3.2.0/intarna.def",
      "sequence-analysis/netoglyc/3.1d/netoglyc.def",
      "sequence-analysis/prokka/1.13/prokka.def",
      "sequence-analysis/prokka/1.14.6/prokka.def",
      "sequence-analysis/annogesic/1.0.2/annogesic.def",
      "sequence-analysis/acfs/20180316/acfs.def",
      "sequence-analysis/augustus/3.3.3/augustus.def",
      "sequence-analysis/phaser/1.1.1/phaser.def",
      "sequence-analysis/bamgineer/2-20200624/bamgineer.def",
      "sequence-analysis/focus/0.6.10/focus.def",
      "sequence-analysis/m-tools/20210208/m-tools.def",
      "sequence-analysis/eukrep/20180308/eukrep.def",
      "sequence-analysis/braker/2/braker.def",
      "sequence-analysis/ldsc/3d0c4464/ldsc.def",
      "sequence-analysis/deepsea/0.94c/deepsea.def",
      "sequence-analysis/asgal/1.0/asgal.def",
      "sequence-analysis/qtltools/1.3.1/qtltools.def",
      "sequence-analysis/chipseq_pipeline/1.2.0/chipseq_pipeline.def",
      "sequence-analysis/svtools/0.5.1/svtools.def",
      "molecular-modeling-graphics/blender/2.82/blender.def",
      "molecular-modeling-graphics/starseqr/0.6.7/starseqr.def",
      "molecular-modeling-graphics/chimerax/0.93/chimerax.def",
      "molecular-modeling-graphics/chimerax/1.1/chimerax.def",
      "image-analysis/fitlins/0.8.0/fitlins.def",
      "image-analysis/fitlins/0.7.0/fitlins.def",
      "image-analysis/qsiprep/0.8.0/qsiprep.def",
      "image-analysis/xcpengine/1.2.3/xcpengine.def",
      "image-analysis/xcpengine/1.0/xcpengine.def",
      "image-analysis/xcpengine/1.2.1/xcpengine.def",
      "image-analysis/deepmedic/0.8.0/deepmedic.def",
      "image-analysis/deepmedic/0.8.2/deepmedic.def",
      "image-analysis/tesseract/4.1.1/tesseract.def",
      "image-analysis/mrtrix/3.0.0/mrtrix.def",
      "image-analysis/mrtrix/3.0.2-cuda9.1/mrtrix.def",
      "image-analysis/mrtrix/3.0.1/mrtrix.def",
      "image-analysis/topaz/0.2.5/topaz.def",
      "image-analysis/minc-toolkit/1.9.16/minc-toolkit.def",
      "image-analysis/minc-toolkit/1.9.18/minc-toolkit.def",
      "image-analysis/resmap/1.95/resmap.def",
      "image-analysis/broccoli/1.0.1/broccoli.def",
      "image-analysis/civet/2.1.1/civet.def",
      "image-analysis/fmriprep/20.2.0/fmriprep.def",
      "image-analysis/fmriprep/20.1.3/fmriprep.def",
      "image-analysis/fmriprep/20.0.5/fmriprep.def",
      "image-analysis/fmriprep/20.2.1/fmriprep.def",
      "image-analysis/fmriprep/20.1.1/fmriprep.def",
      "image-analysis/baracus/1.1.4/baracus.def",
      "image-analysis/terastitcher/1.11.10/terastitcher.def",
      "image-analysis/terastitcher/1.10.8/terastitcher.def",
      "image-analysis/mriqc/0.16.1/mriqc.def",
      "image-analysis/mriqc/0.15.2-0be03bf/mriqc.def",
      "image-analysis/mriqc/0.15.1/mriqc.def",
      "image-analysis/mriqc/0.15.2/mriqc.def",
      "mathematical-statistics/omeclust/1.1.4/omeclust.def",
      "mathematical-statistics/omeclust/1.1.6/omeclust.def",
      "mathematical-statistics/m2clust/0.0.7/m2clust.def",
      "mathematical-statistics/m2clust/0.0.8/m2clust.def",
      "mathematical-statistics/m2clust/1.1.3/m2clust.def",
      "computational-chemistry/ampl/f35623d4/ampl.def",
      "linkage-phylogenetics/gubbins/2.3.4/gubbins.def",
      "linkage-phylogenetics/bali-phy/3.5/bali-phy.def",
      "mass-spectrometry/maxquant/1.6.17.0/maxquant.def",
      "mass-spectrometry/maxquant/1.6.3.3/maxquant.def",
      "mass-spectrometry/maxquant/1.6.7.0/maxquant.def",
      "systems-biology/cellphonedb/2.1.2/cellphonedb.def",
      "systems-biology/cellphonedb/2.1.7/cellphonedb.def",
      "utilities/visidata/2.2/visidata.def",
      "utilities/snp-sites/2.4.1/snp-sites.def",
      "utilities/pyega3/3.3.0/pyega3.def",
      "utilities/gdc-client/1.5.0/gdc-client.def",
      "utilities/pdf2svg/0.2.3/pdf2svg.def",
      "utilities/atom/1.13.1/atom.def",
      "utilities/vcf2db/2020.09.14/vcf2db.def",
      "utilities/xvfb/1.19.6/xvfb.def",
      "utilities/datalad/0.13.0rc2/datalad.def",
      "utilities/sysbench/1.0.11/sysbench.def",
      "utilities/sysbench/1.0.20/sysbench.def",
      "utilities/uropa/3.5.0/uropa.def",
      "utilities/longshot/0.3.5/longshot.def",
      "utilities/ariba/2.14.4/ariba.def",
      "utilities/whatshap/0.18/whatshap.def",
      "high-throughput-sequencing/tvc/5.10.1/tvc.def",
      "high-throughput-sequencing/tpmcalculator/0.0.3/tpmcalculator.def",
      "high-throughput-sequencing/tpmcalculator/0.0.4/tpmcalculator.def",
      "high-throughput-sequencing/rsd/1.1.7/rsd.def",
      "high-throughput-sequencing/xengsort/28762aac/xengsort.def",
      "high-throughput-sequencing/pvactools/2.0.1/pvactools.def",
      "high-throughput-sequencing/pvactools/1.5.5/pvactools.def",
      "high-throughput-sequencing/parliament/0.1.7/parliament.def",
      "high-throughput-sequencing/flye/2.7/flye.def",
      "high-throughput-sequencing/flye/2.8-1/flye.def",
      "high-throughput-sequencing/hicpro/2.11.4/hicpro.def",
      "high-throughput-sequencing/dropest/0.8.6/dropest.def",
      "high-throughput-sequencing/bamreadcount/cram-v0.0.1/bamreadcount.def",
      "high-throughput-sequencing/cicero/0.3.0/cicero.def",
      "high-throughput-sequencing/lefse/1.0.8/lefse.def",
      "high-throughput-sequencing/lefse/1.0.7/lefse.def",
      "high-throughput-sequencing/deeptools/3.5.0/deeptools.def",
      "high-throughput-sequencing/deeptools/3.4.2/deeptools.def",
      "high-throughput-sequencing/epic2/0.0.41/epic2.def",
      "high-throughput-sequencing/crispresso/2.0.40/crispresso.def",
      "high-throughput-sequencing/crispresso/2.0.45/crispresso.def",
      "high-throughput-sequencing/transvar/2.5.9/transvar.def",
      "high-throughput-sequencing/ricopili/2019_Jun_25.001/ricopili.def",
      "high-throughput-sequencing/hicexplorer/3.5.1/hicexplorer.def",
      "high-throughput-sequencing/cgpbattenberg/3.5.3/cgpbattenberg.def",
      "high-throughput-sequencing/gridss/2.9.4/gridss.def",
      "high-throughput-sequencing/salmon/1.4.0/salmon.def",
      "high-throughput-sequencing/bamliquidator/1.3.8/bamliquidator.def",
      "high-throughput-sequencing/pepr/1.1.24/pepr.def",
      "high-throughput-sequencing/vireosnp/0.3.2/vireosnp.def",
      "high-throughput-sequencing/vireosnp/0.5.1/vireosnp.def",
      "high-throughput-sequencing/scramble/1.0.1-32893ef/scramble.def",
      "high-throughput-sequencing/scramble/0.0.20190211.82c78b9/scramble.def",
      "high-throughput-sequencing/ascatngs/4.3.3/ascatngs.def",
      "high-throughput-sequencing/ascatngs/4.5.0/ascatngs.def",
      "high-throughput-sequencing/ascatngs/4.3.4/ascatngs.def",
      "high-throughput-sequencing/htseq/0.11.4/htseq.def",
      "high-throughput-sequencing/pychopper/2.4.0/pychopper.def",
      "high-throughput-sequencing/brass/6.3.4/brass.def",
      "high-throughput-sequencing/brass/6.1.2/brass.def",
      "high-throughput-sequencing/busco/4.1.3/busco.def",
      "high-throughput-sequencing/busco/5.0.0/busco.def",
      "high-throughput-sequencing/surpi/1.0.67/surpi.def",
      "high-throughput-sequencing/delly/0.8.7/delly.def",
      "high-throughput-sequencing/multiqc/1.10/multiqc.def",
      "high-throughput-sequencing/multiqc/1.9/multiqc.def",
      "high-throughput-sequencing/rilseq/0.75/rilseq.def",
      "high-throughput-sequencing/seqlinkage/1.0/seqlinkage.def",
      "high-throughput-sequencing/guppy/4.0.15/guppy.def",
      "high-throughput-sequencing/guppy/4.2.2/guppy.def",
      "high-throughput-sequencing/guppy/3.4.5/guppy.def",
      "high-throughput-sequencing/metaphlan/3.0.6/metaphlan.def",
      "high-throughput-sequencing/metaphlan/3.0/metaphlan.def",
      "high-throughput-sequencing/mitosuite/1.0.9b/mitosuite.def",
      "high-throughput-sequencing/abruijn/1.0/abruijn.def",
      "high-throughput-sequencing/bison/0.4.0/bison.def",
      "high-throughput-sequencing/hail/0.2.3/hail.def",
      "high-throughput-sequencing/hail/0.2.61/hail.def",
      "high-throughput-sequencing/hail/0.2.56/hail.def",
      "high-throughput-sequencing/biom-format/2.1.10/biom-format.def",
      "high-throughput-sequencing/csvkit/1.0.5/csvkit.def",
      "high-throughput-sequencing/fusioninspector/2.5.0/fusioninspector.def",
      "high-throughput-sequencing/fusioninspector/2.3.0/fusioninspector.def",
      "high-throughput-sequencing/rnapeg/current/rnapeg.def",
      "high-throughput-sequencing/cellsnp/0.3.2/cellsnp.def",
      "high-throughput-sequencing/cellsnp/0.1.7/cellsnp.def",
      "high-throughput-sequencing/sicer/2-1.0.2/sicer.def",
      "high-throughput-sequencing/freebayes/1.3.5/freebayes.def",
      "high-throughput-sequencing/svtk/0.1/svtk.def",
      "high-throughput-sequencing/cnvnator/0.4.1/cnvnator.def",
      "high-throughput-sequencing/rseqc/4.0.0/rseqc.def",
      "high-throughput-sequencing/tandemtools/current/tandemtools.def",
      "high-throughput-sequencing/humann/3.0.0-alpha.3/humann.def",
      "high-throughput-sequencing/bamutil/1.0.15/bamutil.def",
      "high-throughput-sequencing/rmats/4.0.2/rmats.def",
      "high-throughput-sequencing/stream/20180816/stream.def",
      "high-throughput-sequencing/idep/0.81/idep.def",
      "high-throughput-sequencing/sve/0.1.0/sve.def",
      "high-throughput-sequencing/humann2/2.8.1/humann2.def",
      "high-throughput-sequencing/neusomatic/0.2.1/neusomatic.def",
      "high-throughput-sequencing/flappie/1.0.0/flappie.def",
      "high-throughput-sequencing/flappie/2.1.3/flappie.def",
      "high-throughput-sequencing/canvas/1.40/canvas.def",
      "high-throughput-sequencing/maggie/0.3.4/maggie.def",
      "high-throughput-sequencing/macs/2.2.7.1/macs.def",
      "high-throughput-sequencing/cancerit-wgs/2.1.0/cancerit-wgs.def",
      "high-throughput-sequencing/mageck-vispr/0.5.4/mageck-vispr.def",
      "high-throughput-sequencing/mtoolbox/1.1/mtoolbox.def",
      "high-throughput-sequencing/slamdunk/0.4.3/slamdunk.def",
      "high-throughput-sequencing/bamsurgeon/1111e5d/bamsurgeon.def",
      "high-throughput-sequencing/crossmap/0.5.2/crossmap.def",
      "high-throughput-sequencing/bigscale2/20191119/bigscale2.def",
      "high-throughput-sequencing/metabat/2.13/metabat.def",
      "high-throughput-sequencing/hap.py/0.3.9/hap.py.def",
      "high-throughput-sequencing/megalodon/2.2.9/megalodon.def",
      "high-throughput-sequencing/taiji/1.1.0/taiji.def",
      "high-throughput-sequencing/taiji/1.2.0/taiji.def",
      "high-throughput-sequencing/cutadapt/1.18/cutadapt.def",
      "high-throughput-sequencing/cutadapt/2.10/cutadapt.def",
      "high-throughput-sequencing/cutadapt/3.0/cutadapt.def",
      "high-throughput-sequencing/vep/101/vep.def",
      "high-throughput-sequencing/vep/97/vep.def",
      "high-throughput-sequencing/vep/103/vep.def",
      "high-throughput-sequencing/atropos/1.1.18/atropos.def",
      "high-throughput-sequencing/pcap-core/4.3.5/pcap-core.def",
      "high-throughput-sequencing/umitools/1.1.1/umitools.def",
      "high-throughput-sequencing/deepsignal/0.1.8/deepsignal.def",
      "high-throughput-sequencing/tetoolkit/2.2.1/tetoolkit.def",
      "high-throughput-sequencing/tetoolkit/2.1.4/tetoolkit.def",
      "high-throughput-sequencing/cnvkit/0.9.8/cnvkit.def",
      "high-throughput-sequencing/cnvkit/0.9.6/cnvkit.def",
      "high-throughput-sequencing/gossamer/ac492a8/gossamer.def",
      "high-throughput-sequencing/svtyper/0.7.1/svtyper.def",
      "high-throughput-sequencing/raremetal/4.15.1/raremetal.def",
      "high-throughput-sequencing/vagrent/3.3.4/vagrent.def",
      "high-throughput-sequencing/eager/1.92/eager.def",
      "high-throughput-sequencing/repeatmodeler/2.0.1/repeatmodeler.def",
      "high-throughput-sequencing/atac_dnase_pipelines/0.3.4-19-gcbd2a00/atac_dnase_pipelines.def",
      "high-throughput-sequencing/deepvariant/1.1.0/deepvariant.def",
      "high-throughput-sequencing/deepvariant/0.9.0/deepvariant.def",
      "high-throughput-sequencing/deepvariant/0.10.0/deepvariant.def",
      "high-throughput-sequencing/medaka/1.0.3/medaka.def",
      "high-throughput-sequencing/medaka/1.2.0/medaka.def",
      "high-throughput-sequencing/medaka/0.12.1/medaka.def",
      "structural-biology/parsnip/20180507/parsnip.def",
      "structural-biology/pymol/2.4.0/pymol.def",
      "structural-biology/pymol/2.3.0/pymol_2.3.0.def",
      "structural-biology/rdock/2013.1/rdock.def",
      "deep-learning/unet/20180704/unet.def",
      "deep-learning/deeplab/20180816/deeplab.def",
      "deep-learning/basset/0.1.0/basset.def",
      "deep-learning/dextr-pytorch/20180710/dextr-pytorch.def",
      "deep-learning/digits/6.0/digits.def",
      "deep-learning/clairvoyante/1.0/clairvoyante.def",
      "deep-learning/caffe2/0.8.1/caffe2.def",
      "deep-learning/polyrnnpp/20180718/polyrnnpp.def",
      "deep-learning/few-shot-ssl/20180723/few-shot-ssl.def",
      "deep-learning/tensorrt/18.09/tensorrt.def"
    ],
    "full_name": "NIH-HPC/singularity-def-files",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nih-hpc-singularity-definition-files\" class=\"anchor\" href=\"#nih-hpc-singularity-definition-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNIH HPC Singularity Definition Files\u003c/h1\u003e\n\u003cp\u003eThese definition files and wrapper scripts are used by the \u003ca href=\"https://hpc.nih.gov/\" rel=\"nofollow\"\u003eNIH HPC (Biowulf)\u003c/a\u003e staff to install containerized applications using \u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e. Each app is installed in a self-contained directory and access to the app is controlled through a module system (\u003ca href=\"https://github.com/TACC/Lmod\"\u003eLmod\u003c/a\u003e). This strategy allows users to transparently access apps that are installed within containers as though they were installed directly on the host system. More details can be found \u003ca href=\"https://hpc.nih.gov/apps/singularity.html#bind-stationary\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTypically, apps are installed under in a directory structure like so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ tree appname/ver\nappname/ver\n|-- bin\n|   |-- cmd1 -\u0026gt; ../libexec/wrapper.sh\n|   |-- cmd2 -\u0026gt; ../libexec/wrapper.sh\n|   `-- cmd3 -\u0026gt; ../libexec/wrapper.sh\n`-- libexec\n    |-- app.sif\n    `-- wrapper.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBecause \u003ccode\u003ewrapper.sh\u003c/code\u003e is written to be introspective, any command symlinked to it will be carried through and executed within the associated container. The wrapper script is also sufficiently generic that it can be reused across apps with little or no modification.\u003c/p\u003e\n\u003cp\u003eEach app has its own \u003ccode\u003eREADME.md\u003c/code\u003e that contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea link to the NIH HPC app page or developer\u0027s documentation\u003c/li\u003e\n\u003cli\u003ea list of symlinks that should be created to the wrapper script to expose executables within the container\u003c/li\u003e\n\u003cli\u003eany app specific installation notes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinally, please note that these definition files \u003cstrong\u003eare not guaranteed to reproduce the same container, or even to produce any container at all\u003c/strong\u003e. The internet, upon which these definition files are based, is subject to change without notice. These definition files are therefore intended to be treated as (potentially) helpful suggestions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-computational-chemistry\" class=\"anchor\" href=\"#computational-chemistry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/computational-chemistry\"\u003eComputational Chemistry\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/computational-chemistry/ampl\"\u003eampl\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deep-learning\" class=\"anchor\" href=\"#deep-learning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/deep-learning\"\u003eDeep Learning\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/caffe2\"\u003eCaffe2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/dextr-pytorch\"\u003eDEXTR-PyTorch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/polyrnnpp\"\u003ePolyRNNpp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/basset\"\u003ebasset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/clairvoyante\"\u003eclairvoyante\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/deeplab\"\u003edeeplab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/digits\"\u003edigits\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/few-shot-ssl\"\u003efew-shot-ssl\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/tensorrt\"\u003etensorrt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/unet\"\u003eunet\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-high-throughput-sequencing\" class=\"anchor\" href=\"#high-throughput-sequencing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/high-throughput-sequencing\"\u003eHigh Throughput Sequencing\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/atac_dnase_pipelines\"\u003eATAC-Seq / DNase-Seq Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/ascatngs\"\u003eAscatNGS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/atropos\"\u003eAtropos\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamsurgeon\"\u003eBAMSurgeon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/brass\"\u003eBRASS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/canvas\"\u003eCanvas\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/maggie\"\u003eMAGGIE\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pcap-core\"\u003ePCAP-core\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pepr\"\u003ePePr\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rsd\"\u003eRSD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/surpi\"\u003eSURPI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tpmcalculator\"\u003eTPMCalculator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tvc\"\u003eTVC\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/vagrent\"\u003eVAGrENT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/vep\"\u003eVEP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/abruijn\"\u003eabruijn\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamliquidator\"\u003ebamliquidator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamreadcount\"\u003ebamreadcount\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamutil\"\u003ebamutil\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bigscale2\"\u003ebigscale2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/biom-format\"\u003ebiom-format\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bison\"\u003ebison\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/busco\"\u003ebusco\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cancerit-wgs\"\u003ecancerit-wgs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cellsnp\"\u003ecellsnp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cgpbattenberg\"\u003ecgpBattenberg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cicero\"\u003ecicero\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cnvkit\"\u003ecnvkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cnvnator\"\u003ecnvnator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/crispresso\"\u003ecrispresso\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/crossmap\"\u003ecrossmap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/csvkit\"\u003ecsvkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cutadapt\"\u003ecutadapt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/deepsignal\"\u003edeepsignal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/deeptools\"\u003edeeptools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/deepvariant\"\u003edeepvariant\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/delly\"\u003edelly\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/dropest\"\u003edropest\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/eager\"\u003eeager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/epic2\"\u003eepic2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/flappie\"\u003eflappie\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/flye\"\u003eflye\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/freebayes\"\u003efreebayes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/fusioninspector\"\u003efusioninspector\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/gossamer\"\u003egossamer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/gridss\"\u003egridss\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/guppy\"\u003eguppy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hail\"\u003ehail\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hap.py\"\u003ehap.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hicexplorer\"\u003ehicexplorer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hicpro\"\u003ehicpro\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/htseq\"\u003ehtseq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/humann2\"\u003ehumann2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/idep\"\u003eidep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/lefse\"\u003elefse\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/macs\"\u003emacs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/mageck-vispr\"\u003emageck-vispr\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/medaka\"\u003emedaka\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/megalodon\"\u003emegalodon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/metabat\"\u003emetabat\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/metaphlan\"\u003emetaphlan\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/mitosuite\"\u003emitosuite\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/mtoolbox\"\u003emtoolbox\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/multiqc\"\u003emultiqc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/neusomatic\"\u003eneusomatic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/parliament\"\u003eparliament\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pvactools\"\u003epvactools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pychopper\"\u003epychopper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/raremetal\"\u003eraremetal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/repeatmodeler\"\u003erepeatmodeler\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/ricopili\"\u003ericopili\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rilseq\"\u003erilseq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rmats\"\u003ermats\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rnapeg\"\u003ernapeg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rseqc\"\u003erseqc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/salmon\"\u003esalmon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/scramble\"\u003escramble\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/seqlinkage\"\u003eseqlinkage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/sicer\"\u003esicer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/slamdunk\"\u003eslamdunk\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/stream\"\u003estream\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/sve\"\u003esve\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/svtk\"\u003esvtk\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/svtyper\"\u003esvtyper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/taiji\"\u003etaiji\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tandemtools\"\u003etandemtools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tetoolkit\"\u003etetoolkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/transvar\"\u003etransvar\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/umitools\"\u003eumitools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/vireosnp\"\u003evireosnp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/xengsort\"\u003exengsort\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-image-analysis\" class=\"anchor\" href=\"#image-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/image-analysis\"\u003eImage Analysis\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/resmap\"\u003eResMap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/terastitcher\"\u003eTeraStitcher\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/baracus\"\u003ebaracus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/broccoli\"\u003ebroccoli\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/civet\"\u003ecivet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/ctf\"\u003ectf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/deepmedic\"\u003edeepmedic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/fitlins\"\u003efitlins\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/fmriprep\"\u003efmriprep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/minc-toolkit\"\u003eminc-toolkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/mriqc\"\u003emriqc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/mrtrix\"\u003emrtrix\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/qsiprep\"\u003eqsiprep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/tesseract\"\u003etesseract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/topaz\"\u003etopaz\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/xcpengine\"\u003excpengine\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-linkage-phylogenetics\" class=\"anchor\" href=\"#linkage-phylogenetics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/linkage-phylogenetics\"\u003eLinkage Phylogenetics\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/linkage-phylogenetics/bali-phy\"\u003ebali-phy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/linkage-phylogenetics/gubbins\"\u003egubbins\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-mass-spectrometry\" class=\"anchor\" href=\"#mass-spectrometry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/mass-spectrometry\"\u003eMass Spectrometry\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/mass-spectrometry/maxquant\"\u003emaxquant\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-mathematicalstatistics\" class=\"anchor\" href=\"#mathematicalstatistics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/mathematical-statistics\"\u003eMathematical/Statistics\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/mathematical-statistics/m2clust\"\u003em2clust\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/mathematical-statistics/omeclust\"\u003eomeClust\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-molecular-modeling-graphics\" class=\"anchor\" href=\"#molecular-modeling-graphics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/molecular-modeling-graphics\"\u003eMolecular Modeling Graphics\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/molecular-modeling-graphics/chimerax\"\u003eChimeraX\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/molecular-modeling-graphics/blender\"\u003eblender\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/molecular-modeling-graphics/starseqr\"\u003estarseqr\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sequence-analysis\" class=\"anchor\" href=\"#sequence-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/sequence-analysis\"\u003eSequence Analysis\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/acfs\"\u003eACFS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/annogesic\"\u003eANNOgesic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/asgal\"\u003eASGAL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/accurity\"\u003eAccurity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/eukrep\"\u003eEukRep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/glu\"\u003eGLU\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/htgtsrep\"\u003eHTGTSrep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/orffinder\"\u003eORFfinder\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/saige\"\u003eSAIGE\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/vcf-kit\"\u003eVCF-kit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/anvio\"\u003eanvio\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/arriba\"\u003earriba\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/augustus\"\u003eaugustus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/bamgineer\"\u003ebamgineer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/braker\"\u003ebraker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/cactus\"\u003ecactus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/chipseq_pipeline\"\u003echipseq_pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/cicero\"\u003ecicero\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/deepsea\"\u003edeepsea\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/focus\"\u003efocus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/glnexus\"\u003eglnexus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/intarna\"\u003eintarna\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/ldsc\"\u003eldsc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/m-tools\"\u003em-tools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/mmarge\"\u003emmarge\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/netoglyc\"\u003enetOglyc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/phaser\"\u003ephaser\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/prokka\"\u003eprokka\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/qtltools\"\u003eqtltools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/roary\"\u003eroary\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/smoove\"\u003esmoove\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/sonicparanoid\"\u003esonicparanoid\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/svtools\"\u003esvtools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/wisexome\"\u003ewisexome\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/xhla\"\u003exHLA\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-structural-biology\" class=\"anchor\" href=\"#structural-biology\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/structural-biology\"\u003eStructural Biology\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/structural-biology/parsnip\"\u003eparsnip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/structural-biology/pymol\"\u003epymol\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/structural-biology/rdock\"\u003erDock\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-systems-biology\" class=\"anchor\" href=\"#systems-biology\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/systems-biology\"\u003eSystems Biology\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/systems-biology/cellphonedb\"\u003ecellphonedb\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-utilities\" class=\"anchor\" href=\"#utilities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/utilities\"\u003eUtilities\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/utilities/xvfb\"\u003eXvfb\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/ariba\"\u003eariba\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/atom\"\u003eatom\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/datalad\"\u003edatalad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/gdc-client\"\u003egdc-client\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/longshot\"\u003elongshot\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/pdf2svg\"\u003epdf2svg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/pyega3\"\u003epyega3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/snp-sites\"\u003esnp-sites\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/sysbench\"\u003esysbench\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/uropa\"\u003europa\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/vcf2db\"\u003evcf2db\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/visidata\"\u003evisidata\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/whatshap\"\u003ewhatshap\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1621576784.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "caffe/Singularity.caffe_1.0",
      "argos/Singularity.argos_3.0.0-beta53",
      "argos/Singularity.argos_3.0.0-beta52",
      "meshlab/Singularity.meshlab-2019.03-cuda-9.0",
      "ilastik/Singularity.ilastik_1.3.3post3",
      "ubuntu-base-image/Singularity.2004-cuda11.0",
      "ubuntu-base-image/Singularity.1804-cuda10.1",
      "ubuntu-base-image/Singularity.1804",
      "ubuntu-base-image/Singularity.1804-cuda9",
      "ubuntu-base-image/Singularity.2004",
      "bidscoin/Singularity.bidscoin_3",
      "cryolo/Singularity.cryolo_v1_0_4",
      "cryolo/Singularity.cryolo_v1_5_6",
      "cryolo/Singularity.cryolo_v1_6_1",
      "cryolo/Singularity.cryolo_v1_0_0",
      "colmap/Singularity.colmap_3.6-dev.3",
      "colmap/Singularity.colmap_3.5",
      "mrtrix3tissue/Singularity.mrtrix3tissue-5.2.8",
      "ants/Singularity.ants_2.3.1",
      "ants/Singularity.ants_2.3.4",
      "eman/Singularity.eman_2.3",
      "eman/Singularity.eman_2.91",
      "eman/Singularity.eman_2.22",
      "eman/Singularity.eman_2.9",
      "eman/Singularity.eman_2.3.1",
      "quit/Singularity.quit_2.0.2",
      "mango/Singularity.mango_4.0.1",
      "R/Singularity.R_4.0.5",
      "omero-insight/Singularity.1804",
      "octave/Singularity.octave-4.2.2",
      "globus-cli/Singularity.globus-cli-v2.0.0",
      "deeplabcut/Singularity.latest",
      "darknet/Singularity.darknet_yolo_v3-cuda-9.0",
      "mrtrix/Singularity.mrtrix_3_beta",
      "omero.insight/Singularity.omero_5.5.10",
      "bids-validator/Singularity.bids-validator-1.2.2",
      "bids-validator/Singularity.bids-validator-1.3.1",
      "ashs/Singularity.ashs_2.0.0",
      "fiji/Singularity.fiji",
      "gimp/Singularity.gimp_2.8",
      "gimp/Singularity.gimp_2.8.22",
      "apex/Singularity.apex_master",
      "openmodelica/Singularity.openmodelica_1.14.2-cuda-10.1",
      "cytoscape/Singularity.cytoscape_3.8.0",
      "volview/Singularity.VolView_3.4-cuda-9.0",
      "pyprismatic/Singularity.pyprismatic_1_2_1-cuda-11.0",
      "atom/Singularity.atom_1.39.1",
      "atom/Singularity.atom_1.45.0",
      "amide/Singularity.amide-1.0.5",
      "paraview/Singularity.paraview_5.6.0-cuda-9.0",
      "mantid/Singularity.mantid_v_3_13_0",
      "openrefine/Singularity.openrefine-3.1",
      "datalad/Singularity.datalad_0.13.3",
      "cloudstor/Singularity.cloudstor-2.4.1",
      "imblproc/Singularity.imblproc",
      "octopus/Singularity.octopus_8.4",
      "octopus/Singularity.octopus_8.4_parallel",
      "haystack/Singularity.haystack_bio_v0_5_0",
      "anaconda3/Singularity.anaconda3_5.3.0",
      "anaconda3/Singularity.anaconda3_5.3.0-cuda-11.0.3",
      "caffe-unet/Singularity.caffe-unet_1.0",
      "crisprcas/Singularity.crisprcas",
      "cvmfs-client/Singularity.cvmfs-client",
      "libertem/Singularity.libertem-v0.6.0",
      "libertem/Singularity.libertem-v0.2.2",
      "libertem/Singularity.libertem-v0.5.1",
      "libertem/Singularity.libertem-v0.4.1",
      "libertem/Singularity.libertem-v0.4.0",
      "libertem/Singularity.libertem-v0.5.0",
      "libertem/Singularity.libertem-21-May-2019",
      "cistem/Singularity.cisTEM-1.0.0-beta",
      "dristhi/Singularity.dristhi_2.6.4",
      "imod/Singularity.imod_v4_9_9",
      "fsl/Singularity.fsl",
      "cellprofiler/Singularity.cellprofiler_3.1.9",
      "cellprofiler/Singularity.cellprofiler_3.1.5",
      "jupyter-ml/Singularity.jupyter-ml_20210415",
      "jupyter-ml/Singularity.jupyter-ml_20201120",
      "imagemagick/Singularity.imagemagick-7.0.8-68",
      "ariba/Singularity.ariba_2.14.4",
      "ariba/Singularity.ariba_2.12.1",
      "dragondisk/Singularity.dragondisk_v1_0_5",
      "globus-connect-personal/Singularity.globus-connect-personal_latest",
      "mydata-python/Singularity.mydata-python_20200603",
      "3dslicer/Singularity.3dslicer_4.10.2",
      "3dslicer/Singularity.3dslicer_4.8.1",
      "imagej/Singularity.imagej_1.50e",
      "mydata/Singularity.mydata_0.9.2-1",
      "git-annex/Singularity.git-annex.6.20180227",
      "matlab/Singularity.MATLAB_SAMPLE",
      "graphviz/Singularity.graphviz-2.40.1",
      "connectome-workbench/Singularity.connectome-workbench_1.4.2"
    ],
    "full_name": "Characterisation-Virtual-Laboratory/CharacterisationVL-Software",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-characterisationvl-software\" class=\"anchor\" href=\"#characterisationvl-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCharacterisationVL-Software\u003c/h1\u003e\n\u003cp\u003eThe purpose of this repository is for storing definition files to submit to \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you are new to Singularity containers, please refer to \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/user-guide/\u003c/a\u003e or a newer version of this documentation.\u003c/p\u003e\n\u003cp\u003eEach software package is located in its own folder. The files are tagged with the software name and version number or date of build. Please read below for the naming convention.\u003c/p\u003e\n\u003cp\u003eTo add software to the repository you will need to create a new branch. The new branch is the name of the software product. By convention, the new branch will be checked and merged into the master branch and then deleted.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-steps-to-add-a-software-package\" class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSteps to add a software package\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003cli\u003eCreate a branch\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git branch \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eMake a subdirectory for the software product.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ mkdir \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eAdd all the necessary files.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity definition file or installation script\u003c/li\u003e\n\u003cli\u003eReadme file including install and testing notes\u003c/li\u003e\n\u003cli\u003eDesktop files for adding to menus with necessary tags\u003c/li\u003e\n\u003cli\u003eFor full details, \u003ca href=\"template/README.md\"\u003eplease refer to the \u0027template\u0027 folder in this repository.\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eCommit all changes, including a helpful message\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git commit -m \"\u0026lt;software name\u0026gt; added as requested in support ticket\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003ePush to the remote repository. i.e. this one.\u003c/li\u003e\n\u003cli\u003eSubmit merge request\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\" class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNaming your Singularity definition file, Singularity Hub and Licensing\u003c/h2\u003e\n\u003cp\u003eFor all Singularity recipes where the software licensing permits redistribution, please use this naming convention:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   Singularity.applicationName_version\n   Singularity.applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is where Singularity Hub fits into the equation. There is a webhook between this repository and \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e. When a commit is merged into the master branch, Singularity Hub will build the container.\u003c/p\u003e\n\u003cp\u003eIf successfully built, the path to the container on Singularity Hub is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor software where licensing does not support redistribution, the container recipe can still be defined, but the container should not be built on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eAn example on how to handle this situation is the recipe for CCP-EM.\nThe \u003ca href=\"ccp-em/README.md\"\u003eREADME.md\u003c/a\u003e contains a section on Prerequisites. This section lists the required files to build the container. The license must be accepted by the end user to obtain them.\u003c/p\u003e\n\u003cp\u003ePrerequisite files should not be committed to this repository.\u003c/p\u003e\n\u003cp\u003eTo prevent Singularity Hub from attempting to build the container, we simply use a different recipe naming convention as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   applicationName_version.def\n   applicationName_version-cuda-cudaVersion.def\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUbuntu Base Images\u003c/h2\u003e\n\u003cp\u003eThe folder \u0027ubuntu-base-image\u0027 contains recipes for pre built base containers. These can be used as a starting point to aid/speed up the development of your container recipe.\u003c/p\u003e\n\u003cp\u003eThe current versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.\u003c/p\u003e\n\u003cp\u003eThese are available on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFor example: from the Graphviz Singularity.graphviz-2.40.1 recipe\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBootstrap: shub\nFrom:      Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese two lines, will tell Singularity to use the \u0027shub\u0027 bootstrap to obtain the \u00271804\u0027 ubuntu-base-image container from Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFrom here you just need to add the requirements to build a container for your required piece of software. Please see \u003ca href=\"graphviz/Singularity.graphviz-2.40.1\"\u003eSingularity.graphviz-2.40.1\u003c/a\u003e\nfor the full recipe.\u003c/p\u003e\n\u003cp\u003eThe current ubuntu-base-images include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning GUI applications on a non-GPU node\u003c/h2\u003e\n\u003cp\u003eThe applications in the Singularity container should run without the need for a dedicated GPU.\u003c/p\u003e\n\u003cp\u003eHowever, an X server needs to be running for this to work. On nodes with GPU, X Server is started with NVIDIA driver, and on non-GPU nodes, the X Server is started with MESA library.\u003c/p\u003e\n\u003cp\u003eX Server can be started during boot (for example, using \u003ccode\u003esystemctl set-default graphical.target\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eMake sure that VirtualGL package is installed in the container. The code below will download and install VirtualGL.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\ndpkg -i virtualgl_2.6.2_amd64.deb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe application startup script doesn\u0027t need to be modified, however, if the application needs to be manually started, then \u003ccode\u003evglrun\u003c/code\u003e needs to be appended before running the application. For example: \u003ccode\u003esingularity exec --nv -B /projects:/projects -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1620048175.0
  },
  {
    "data_format": 2,
    "description": " A Pytorch deep learning singularity container",
    "filenames": [
      "Singularity.1.7.0"
    ],
    "full_name": "ylugithub/pytorch_gpu_singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-characterisationvl-software\" class=\"anchor\" href=\"#characterisationvl-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCharacterisationVL-Software\u003c/h1\u003e\n\u003cp\u003eThe purpose of this repository is for storing definition files to submit to \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you are new to Singularity containers, please refer to \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/user-guide/\u003c/a\u003e or a newer version of this documentation.\u003c/p\u003e\n\u003cp\u003eEach software package is located in its own folder. The files are tagged with the software name and version number or date of build. Please read below for the naming convention.\u003c/p\u003e\n\u003cp\u003eTo add software to the repository you will need to create a new branch. The new branch is the name of the software product. By convention, the new branch will be checked and merged into the master branch and then deleted.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-steps-to-add-a-software-package\" class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSteps to add a software package\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003cli\u003eCreate a branch\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git branch \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eMake a subdirectory for the software product.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ mkdir \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eAdd all the necessary files.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity definition file or installation script\u003c/li\u003e\n\u003cli\u003eReadme file including install and testing notes\u003c/li\u003e\n\u003cli\u003eDesktop files for adding to menus with necessary tags\u003c/li\u003e\n\u003cli\u003eFor full details, \u003ca href=\"template/README.md\"\u003eplease refer to the \u0027template\u0027 folder in this repository.\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eCommit all changes, including a helpful message\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git commit -m \"\u0026lt;software name\u0026gt; added as requested in support ticket\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003ePush to the remote repository. i.e. this one.\u003c/li\u003e\n\u003cli\u003eSubmit merge request\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\" class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNaming your Singularity definition file, Singularity Hub and Licensing\u003c/h2\u003e\n\u003cp\u003eFor all Singularity recipes where the software licensing permits redistribution, please use this naming convention:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   Singularity.applicationName_version\n   Singularity.applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is where Singularity Hub fits into the equation. There is a webhook between this repository and \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e. When a commit is merged into the master branch, Singularity Hub will build the container.\u003c/p\u003e\n\u003cp\u003eIf successfully built, the path to the container on Singularity Hub is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor software where licensing does not support redistribution, the container recipe can still be defined, but the container should not be built on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eAn example on how to handle this situation is the recipe for CCP-EM.\nThe \u003ca href=\"ccp-em/README.md\"\u003eREADME.md\u003c/a\u003e contains a section on Prerequisites. This section lists the required files to build the container. The license must be accepted by the end user to obtain them.\u003c/p\u003e\n\u003cp\u003ePrerequisite files should not be committed to this repository.\u003c/p\u003e\n\u003cp\u003eTo prevent Singularity Hub from attempting to build the container, we simply use a different recipe naming convention as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   applicationName_version.def\n   applicationName_version-cuda-cudaVersion.def\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUbuntu Base Images\u003c/h2\u003e\n\u003cp\u003eThe folder \u0027ubuntu-base-image\u0027 contains recipes for pre built base containers. These can be used as a starting point to aid/speed up the development of your container recipe.\u003c/p\u003e\n\u003cp\u003eThe current versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.\u003c/p\u003e\n\u003cp\u003eThese are available on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFor example: from the Graphviz Singularity.graphviz-2.40.1 recipe\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBootstrap: shub\nFrom:      Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese two lines, will tell Singularity to use the \u0027shub\u0027 bootstrap to obtain the \u00271804\u0027 ubuntu-base-image container from Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFrom here you just need to add the requirements to build a container for your required piece of software. Please see \u003ca href=\"graphviz/Singularity.graphviz-2.40.1\"\u003eSingularity.graphviz-2.40.1\u003c/a\u003e\nfor the full recipe.\u003c/p\u003e\n\u003cp\u003eThe current ubuntu-base-images include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning GUI applications on a non-GPU node\u003c/h2\u003e\n\u003cp\u003eThe applications in the Singularity container should run without the need for a dedicated GPU.\u003c/p\u003e\n\u003cp\u003eHowever, an X server needs to be running for this to work. On nodes with GPU, X Server is started with NVIDIA driver, and on non-GPU nodes, the X Server is started with MESA library.\u003c/p\u003e\n\u003cp\u003eX Server can be started during boot (for example, using \u003ccode\u003esystemctl set-default graphical.target\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eMake sure that VirtualGL package is installed in the container. The code below will download and install VirtualGL.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\ndpkg -i virtualgl_2.6.2_amd64.deb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe application startup script doesn\u0027t need to be modified, however, if the application needs to be manually started, then \u003ccode\u003evglrun\u003c/code\u003e needs to be appended before running the application. For example: \u003ccode\u003esingularity exec --nv -B /projects:/projects -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619654142.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "dl-container-registry/ffmpeg",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nvidia-accelerated-ffmpeg\" class=\"anchor\" href=\"#nvidia-accelerated-ffmpeg\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNVIDIA accelerated ffmpeg\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/dl-container-registry/ffmpeg\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7976a6946b4090b514092b4f29d50f2b3ee4b012d5fa9f485f10552d81fe6284/68747470733a2f2f7472617669732d63692e6f72672f646c2d636f6e7461696e65722d72656769737472792f66666d7065672e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/dl-container-registry/ffmpeg.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/willprice/nvidia-ffmpeg/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/99bb6090faef97032d3bfd80b4d0cdb9d984e9e97aeb1d2750bc3e442fb117f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65726875622d3232623865622e737667\" alt=\"Dockerhub link\" data-canonical-src=\"https://img.shields.io/badge/hosted-dockerhub-22b8eb.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/521\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"Singularity hub link\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://developer.nvidia.com/nvidia-video-codec-sdk#NVENCFeatures\" rel=\"nofollow\"\u003eNVENCODE acceleration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://developer.nvidia.com/nvidia-video-codec-sdk#NVDECFeatures\" rel=\"nofollow\"\u003eNVDECODE acceleration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.videolan.org/developers/x264.html\" rel=\"nofollow\"\u003evideo codec: x264\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.videolan.org/developers/x265.html\" rel=\"nofollow\"\u003evideo codec: x265\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/mstorsjo/fdk-aac\"\u003eaudio codec: AAC\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNVENCODE (nvenc) and NVDECODE (formerly CUVID) are packaged in the \u003ca href=\"https://developer.nvidia.com/nvidia-video-codec-sdk\" rel=\"nofollow\"\u003eNVIDIA Video Codec\nSDK\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hardware-accelerated-encoders\" class=\"anchor\" href=\"#hardware-accelerated-encoders\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHardware Accelerated Encoders:\u003c/h3\u003e\n\u003cp\u003eList options of an encoder using \u003ccode\u003effmpeg -h encoder=XXXX\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eh264_nvenc\u003c/code\u003e, \u003ccode\u003envenc\u003c/code\u003e, \u003ccode\u003envenc_h264\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003envenc_hevc\u003c/code\u003e, \u003ccode\u003ehevc_nvenc\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hardware-accelerated-decoders\" class=\"anchor\" href=\"#hardware-accelerated-decoders\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHardware Accelerated Decoders:\u003c/h3\u003e\n\u003cp\u003eList options of a decoder using \u003ccode\u003effmpeg -h decoder=XXXX\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eh264_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ehevc_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emjpeg_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003empeg1_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003empeg2_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003empeg4_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evc1_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evp8_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evp9_cuvid\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hardware-accelerated-filters\" class=\"anchor\" href=\"#hardware-accelerated-filters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHardware Accelerated Filters:\u003c/h3\u003e\n\u003cp\u003eList options of a filter using \u003ccode\u003effmpeg -h filter=XXXX\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ehwupload_cuda\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003escale_cuda\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003escale_npp\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ethumnail_cuda\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eRun the container mounting the current directory to \u003ccode\u003e/workspace\u003c/code\u003e processing\n\u003ccode\u003einput.mp4\u003c/code\u003e to \u003ccode\u003eoutput.mp4\u003c/code\u003e without any hardware acceleration\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ docker run --rm -it --runtime=nvidia \\\n    --volume \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e:/workspace \\\n    willprice/nvidia-ffmpeg -i input.mp4 output.avi\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ docker run --rm -it --runtime=nvidia \\\n    --volume \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e:/workspace \\\n    willprice/nvidia-ffmpeg \\\n      -hwaccel_device 0 \\\n      -hwaccel cuvid \\\n      -c:v h264_cuvid \\\n      -i input.mp4 \\\n      -c:v hevc_nvenc\n      out.mkv\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eGet a shell prompt inside the container, useful for debugging:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ docker run --rm -it --runtime=nvidia \\\n    --volume \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e:/workspace \\\n    --entrypoint bash\n    willprice/nvidia-ffmpeg\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eThe docker image is a multistage build. The initial stage, the \u003cem\u003ebuild\u003c/em\u003e stage, builds a statically linked ffmpeg binary\nthat is then copied over into the runtime image. By statically linking we minimize the number of external dependencies\nand shrink the runtime image.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-resources\" class=\"anchor\" href=\"#resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://trac.ffmpeg.org/wiki/HWAccelIntro\" rel=\"nofollow\"\u003eFFmpeg hardware acceleration guide with examples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gist.github.com/jniltinho/9c009e9771651aa4a004ad3d1f6857e3\"\u003eStatic FFmpeg build on Ubuntu 16.04 guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gist.github.com/Brainiarc7/18fca697891aea0e879f13ed092cb213\"\u003eUsing FFmpeg with GNU parallel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gist.github.com/Brainiarc7/c6164520f082c27ae7bbea9556d4a3ba\"\u003eListing NVENC and NPP capabilities of FFmpeg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gist.github.com/Brainiarc7/8b471ff91319483cdb725f615908286e\"\u003eEncoding HEVC using FFmpeg with NVENC\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gist.github.com/Brainiarc7/ebf3091efd2bf0a0ded0f9715cd43a38\"\u003eFFmpeg cheatsheet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/zimbatm/ffmpeg-static\"\u003eFFmpeg-static build scripts\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1612197004.0
  },
  {
    "data_format": 2,
    "description": "ENIGMA Halfpipe is a user-friendly software that facilitates reproducible analysis of fMRI data",
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "HALFpipe/HALFpipe",
    "latest_release": "1.1.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-welcome-to-enigma-halfpipe\" class=\"anchor\" href=\"#welcome-to-enigma-halfpipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWelcome to ENIGMA \u003ccode\u003eHALFpipe\u003c/code\u003e\n\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4508\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22build%22\"\u003e\u003cimg src=\"https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg\" alt=\"https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22continuous+integration%22\"\u003e\u003cimg src=\"https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg\" alt=\"https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/HALFpipe/HALFpipe\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ef5be2978b13a91a1a602bc0261933d3735a7567176db1ef0c13eb65b3249056/68747470733a2f2f636f6465636f762e696f2f67682f48414c46706970652f48414c46706970652f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/HALFpipe/HALFpipe/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e is a user-friendly software that facilitates reproducible analysis of\nfMRI data, including preprocessing, single-subject, and group analysis. It\nprovides state-of-the-art preprocessing using\n\u003ca href=\"https://fmriprep.readthedocs.io/\" rel=\"nofollow\"\u003e\u003ccode\u003efmriprep\u003c/code\u003e\u003c/a\u003e, but removes the necessity to\nconvert data to the\n\u003ca href=\"https://bids-specification.readthedocs.io/en/stable/\" rel=\"nofollow\"\u003e\u003ccode\u003eBIDS\u003c/code\u003e\u003c/a\u003e format. Common\nresting-state and task-based fMRI features can then be calculated on the fly\nusing \u003ca href=\"http://fsl.fmrib.ox.ac.uk/\" rel=\"nofollow\"\u003e\u003ccode\u003eFSL\u003c/code\u003e\u003c/a\u003e and\n\u003ca href=\"https://nipype.readthedocs.io/\" rel=\"nofollow\"\u003e\u003ccode\u003enipype\u003c/code\u003e\u003c/a\u003e for statistics.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIf you encounter issues, please see the \u003ca href=\"#troubleshooting\"\u003etroubleshooting\u003c/a\u003e\nsection of this document.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#getting-started\"\u003eGetting started\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#container-platform\"\u003eContainer platform\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#download\"\u003eDownload\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running\"\u003eRunning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#user-interface\"\u003eUser interface\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#files\"\u003eFiles\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#features\"\u003eFeatures\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#models\"\u003eModels\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-on-a-high-performance-computing-cluster\"\u003eRunning on a high-performance computing cluster\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#quality-checks\"\u003eQuality checks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#outputs\"\u003eOutputs\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#subject-level-features\"\u003eSubject-level features\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#preprocessed-images\"\u003ePreprocessed images\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#group-level\"\u003eGroup-level\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#troubleshooting\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#command-line-flags\"\u003eCommand line flags\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#control-command-line-logging\"\u003eControl command line logging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#automatically-remove-unneeded-files\"\u003eAutomatically remove unneeded files\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#adjust-nipype\"\u003eAdjust nipype\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#choose-which-parts-to-run-or-to-skip\"\u003eChoose which parts to run or to skip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#working-directory\"\u003eWorking directory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#data-file-system-root\"\u003eData file system root\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#contact\"\u003eContact\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting started\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e is distributed as a container, meaning that all required software\ncomes bundled in a monolithic file, the container. This allows for easy\ninstallation on new systems, and makes data analysis more reproducible, because\nsoftware versions are guaranteed to be the same for all users.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-container-platform\" class=\"anchor\" href=\"#container-platform\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer platform\u003c/h3\u003e\n\u003cp\u003eThe first step is to install one of the supported container platforms. If you\u0027re\nusing a high-performance computing cluster, more often than not\n\u003ca href=\"https://sylabs.io\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e will already be available.\u003c/p\u003e\n\u003cp\u003eIf not, we recommend using the latest version\nof\u003ca href=\"https://sylabs.io\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e. However, it can be somewhat cumbersome to\ninstall, as it needs to be built from source.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://neuro.debian.net/\" rel=\"nofollow\"\u003e\u003ccode\u003eNeuroDebian\u003c/code\u003e\u003c/a\u003e package repository provides an\nolder version of \u003ca href=\"https://sylabs.io/guides/2.6/user-guide/\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e for\n\u003ca href=\"https://neuro.debian.net/pkgs/singularity-container.html\" rel=\"nofollow\"\u003esome\u003c/a\u003e Linux\ndistributions.\u003c/p\u003e\n\u003cp\u003eIn contrast to \u003ccode\u003eSingularity\u003c/code\u003e, \u003ccode\u003eDocker\u003c/code\u003e always requires elevated privileges to\nrun containers. In other words, every user running a \u003ccode\u003eDocker\u003c/code\u003e container\nautomatically has administrator privileges on the computer they\u0027re using.\nTherefore, it is inherently a bad choice for multi-user environments, where the\naccess of individual users should be limited. \u003ccode\u003eDocker\u003c/code\u003e is the only option that\nis compatible with \u003ccode\u003eMac OS X\u003c/code\u003e.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eContainer platform\u003c/th\u003e\n\u003cth\u003eVersion\u003c/th\u003e\n\u003cth\u003eInstallation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eSingularity\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003e3.5.3\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003eSee \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/user-guide/quick_start.html\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSingularity\u003c/td\u003e\n\u003ctd\u003e2.6.1\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003esudo apt install singularity-container\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDocker\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eSee \u003ca href=\"https://docs.docker.com/engine/install/\" rel=\"nofollow\"\u003ehttps://docs.docker.com/engine/install/\u003c/a\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h3\u003e\n\u003cp\u003eThe second step is to download the \u003ccode\u003eHALFpipe\u003c/code\u003e to your computer. This requires\napproximately 5 gigabytes of storage.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eContainer platform\u003c/th\u003e\n\u003cth\u003eInstallation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSingularity\u003c/td\u003e\n\u003ctd\u003e\n\u003ca href=\"https://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g\" rel=\"nofollow\"\u003ehttps://charitede-my.sharepoint.com/:f:/g/personal/lea_waller_charite_de/EukRziExhTVBrEAai2oEpi8B2jsnn7P3YQuFo2pycKp6-g\u003c/a\u003e or \u003ccode\u003esingularity pull docker://halfpipe/halfpipe:1.1.1\u003c/code\u003e or \u003ccode\u003esingularity pull docker://ghcr.io/halfpipe/halfpipe:1.1.1\u003c/code\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDocker\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003edocker pull halfpipe/halfpipe:1.1.1\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003ccode\u003eSingularity\u003c/code\u003e version \u003ccode\u003e3.x\u003c/code\u003e creates a container image file called\n\u003ccode\u003eHALFpipe_{version}.sif\u003c/code\u003e in the directory where you run the \u003ccode\u003epull\u003c/code\u003e command. For\n\u003ccode\u003eSingularity\u003c/code\u003e version \u003ccode\u003e2.x\u003c/code\u003e the file is named\n\u003ccode\u003ehalfpipe-halfpipe-master-latest.simg\u003c/code\u003e. Whenever you want to use the container,\nyou need pass \u003ccode\u003eSingularity\u003c/code\u003e the path to this file.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e \u003ccode\u003eSingularity\u003c/code\u003e may store a copy of the container in its cache\ndirectory. The cache directory is located by default in your home directory at\n\u003ccode\u003e~/.singularity\u003c/code\u003e. If you need to save disk space in your home directory, you\ncan safely delete the cache directory after downloading, i.e. by running\n\u003ccode\u003erm -rf ~/.singularity\u003c/code\u003e. Alternatively, you could move the cache directory\nsomewhere with more free disk space using a symlink. This way, files will\nautomatically be stored there in the future. For example, if you have a lot of\nfree disk space in \u003ccode\u003e/mnt/storage\u003c/code\u003e, then you could first run\n\u003ccode\u003emv ~/.singularity /mnt/storage\u003c/code\u003e to move the cache directory, and then\n\u003ccode\u003eln -s /mnt/storage/.singularity ~/.singularity\u003c/code\u003e to create the symlink.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ccode\u003eDocker\u003c/code\u003e will store the container in its storage base directory, so it does not\nmatter from which directory you run the \u003ccode\u003epull\u003c/code\u003e command.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running\" class=\"anchor\" href=\"#running\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning\u003c/h3\u003e\n\u003cp\u003eThe third step is to run the downloaded container. You may need to replace\n\u003ccode\u003ehalfpipe_1.1.1.sif\u003c/code\u003e with the actual path and filename where \u003ccode\u003eSingularity\u003c/code\u003e\ndownloaded your container.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eContainer platform\u003c/th\u003e\n\u003cth\u003eCommand\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSingularity\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003esingularity run --no-home --cleanenv --bind /:/ext halfpipe_1.1.1.sif\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eDocker\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003edocker run --interactive --tty --volume /:/ext halfpipe/halfpipe\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eYou should now see the user interface.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-background\" class=\"anchor\" href=\"#background\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBackground\u003c/h4\u003e\n\u003cp\u003eContainers are by default isolated from the host computer. This adds security,\nbut also means that the container cannot access the data it needs for analysis.\n\u003ccode\u003eHALFpipe\u003c/code\u003e expects all inputs (e.g., image files and spreadsheets) and outputs\n(the working directory) to be places in the path\u003ccode\u003e/ext\u003c/code\u003e (see also\n\u003ca href=\"#data-file-system-root---fs-root\"\u003e\u003ccode\u003e--fs-root\u003c/code\u003e\u003c/a\u003e). Using the option\n\u003ccode\u003e--bind /:/ext\u003c/code\u003e, we instruct \u003ccode\u003eSingularity\u003c/code\u003e to map all of the host file system\n(\u003ccode\u003e/\u003c/code\u003e) to that path (\u003ccode\u003e/ext\u003c/code\u003e). You can also run \u003ccode\u003eHALFpipe\u003c/code\u003e and only map only part\nof the host file system, but keep in mind that any directories that are not\nmapped will not be visible later.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eSingularity\u003c/code\u003e passes the host shell environment to the container by default.\nThis means that in some cases, the host computer\u0027s configuration can interfere\nwith the software. To avoid this, we need to pass the option \u003ccode\u003e--cleanenv\u003c/code\u003e.\n\u003ccode\u003eDocker\u003c/code\u003e does not pass the host shell environment by default, so we don\u0027t need\nto pass an option.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-user-interface\" class=\"anchor\" href=\"#user-interface\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUser interface\u003c/h2\u003e\n\u003cp\u003eThe user interface asks a series of questions about your data and the analyses\nyou want to run. In each question, you can press \u003ccode\u003eControl+C\u003c/code\u003e to cancel the\ncurrent question and go back to the previous one. \u003ccode\u003eControl+D\u003c/code\u003e exits the program\nwithout saving. Note that these keyboard shortcuts are the same on Mac.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-files\" class=\"anchor\" href=\"#files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFiles\u003c/h3\u003e\n\u003cp\u003eTo run preprocessing, at least a T1-weighted structural image and a BOLD image\nfile is required. Preprocessing and data analysis proceeds automatically.\nHowever, to be able to run automatically, data files need to be input in a way\nsuitable for automation.\u003c/p\u003e\n\u003cp\u003eFor this kind of automation, \u003ccode\u003eHALFpipe\u003c/code\u003e needs to know the relationships between\nfiles, such as which files belong to the same subject. However, even though it\nwould be obvious for a human, a program cannot easily assign a file name to a\nsubject, and this will be true as long as there are differences in naming\nbetween different researchers or labs. One researcher may name the same file\n\u003ccode\u003esubject_01_rest.nii.gz\u003c/code\u003e and another \u003ccode\u003esubject_01/scan_rest.nii.gz\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIn \u003ccode\u003eHALFpipe\u003c/code\u003e, we solve this issue by inputting file names in a specific way.\nFor example, instead of \u003ccode\u003esubject_01/scan_rest.nii.gz\u003c/code\u003e, \u003ccode\u003eHALFpipe\u003c/code\u003e expects you to\ninput \u003ccode\u003e{subject}/scan_rest.nii.gz\u003c/code\u003e. \u003ccode\u003eHALFpipe\u003c/code\u003e can then match all files on disk\nthat match this naming schema, and extract the subject ID \u003ccode\u003esubject_01\u003c/code\u003e. Using\nthe extracted subject ID, other files can now be matched to this image. If all\ninput files are available in BIDS format, then this step can be skipped.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify working directory\u003c/code\u003e All intermediate and outputs of \u003ccode\u003eHALFpipe\u003c/code\u003e will\nbe placed in the working directory. Keep in mind to choose a location with\nsufficient free disk space, as intermediates can be multiple gigabytes in\nsize for each subject.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eIs the data available in BIDS format?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the BIDS directory\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify anatomical/structural data\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003eSpecify the path of the T1-weighted image files\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify functional data\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003eSpecify the path of the BOLD image files\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCheck repetition time values\u003c/code\u003e / \u003ccode\u003eSpecify repetition time in seconds\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd more BOLD image files?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e Loop back to 2\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDo slice timing?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eCheck slice acquisition direction values\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eCheck slice timing values\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Skip this step\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify field maps?\u003c/code\u003e If the data was imported from a BIDS directory, this\nstep will be omitted.\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify the type of the field maps\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eEPI (blip-up blip-down)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the blip-up blip-down EPI image files\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePhase difference and magnitude (used by Siemens scanners)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the magnitude image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the phase/phase difference image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify echo time difference in seconds\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eScanner-computed field map and magnitude (used by GE / Philips\nscanners)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the magnitude image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the field map image files\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd more field maps?\u003c/code\u003e Loop back to 1\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify effective echo spacing for the functional data in seconds\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify phase encoding direction for the functional data\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Skip this step\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h3\u003e\n\u003cp\u003eFeatures are analyses that are carried out on the preprocessed data, in other\nwords, first-level analyses.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify first-level features?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify the feature type\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eTask-based\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the event file type\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSPM multiple conditions\u003c/code\u003e A MATLAB .mat file containing three\narrays: \u003ccode\u003enames\u003c/code\u003e (condition), \u003ccode\u003eonsets\u003c/code\u003e and \u003ccode\u003edurations\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eFSL 3-column\u003c/code\u003e One text file for each condition. Each file has its\ncorresponding condition in the filename. The first column specifies\nthe event onset, the second the duration. The third column of the\nfiles is ignored, so parametric modulation is not supported\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eBIDS TSV\u003c/code\u003e A tab-separated table with named columns \u003ccode\u003etrial_type\u003c/code\u003e\n(condition), \u003ccode\u003eonset\u003c/code\u003e and \u003ccode\u003eduration\u003c/code\u003e. While BIDS supports defining\nadditional columns, \u003ccode\u003eHALFpipe\u003c/code\u003e will currently ignore these\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the event files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSelect conditions to add to the model\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify contrasts\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify contrast name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify contrast values\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd another contrast?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e Loop back to 1\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply a temporal filter to the design matrix?\u003c/code\u003e A separate temporal\nfilter can be specified for the design matrix. In contrast, the\ntemporal filtering of the input image and any confound regressors\nadded to the design matrix is specified in 10. In general, the two\nsettings should match\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply smoothing?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify smoothing FWHM in mm\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eGrand mean scaling will be applied with a mean of 10000.000000\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eTemporal filtering will be applied using a gaussian-weighted filter\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003eSpecify the filter width in seconds\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRemove confounds?\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSeed-based connectivity\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify binary seed mask file(s)\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify the path of the binary seed mask image files\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eCheck space values\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAdd binary seed mask image file\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDual regression\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAtlas-based connectivity matrix\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eReHo\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003efALFF\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify feature name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTODO\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Skip this step\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAdd another first-level feature?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e Loop back to 1\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eOutput a preprocessed image?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify setting name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpecify images to use\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply smoothing?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify smoothing FWHM in mm\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDo grand mean scaling?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eSpecify grand mean\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eApply a temporal filter?\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYes\u003c/code\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ccode\u003eSpecify the type of temporal filter\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eGaussian-weighted\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eFrequency-based\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRemove confounds?\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNo\u003c/code\u003e Continue\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-models\" class=\"anchor\" href=\"#models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eModels\u003c/h3\u003e\n\u003cp\u003eModels are statistical analyses that are carried out on the features.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTODO\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-a-high-performance-computing-cluster\" class=\"anchor\" href=\"#running-on-a-high-performance-computing-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on a high-performance computing cluster\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eLog in to your cluster\u0027s head node\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRequest an interactive job. Refer to your cluster\u0027s documentation for how to\ndo this\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn the interactive job, run the \u003ccode\u003eHALFpipe\u003c/code\u003e user interface, but add the flag\n\u003ccode\u003e--use-cluster\u003c/code\u003e to the end of the command. \u003cbr\u003e\nFor example, \u003ccode\u003esingularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif --use-cluster\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs soon as you finish specifying all your data, features and models in the\nuser interface, \u003ccode\u003eHALFpipe\u003c/code\u003e will now generate everything needed to run on the\ncluster. For hundreds of subjects, this can take up to a few hours.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhen \u003ccode\u003eHALFpipe\u003c/code\u003e exits, edit the generated submit script \u003ccode\u003esubmit.slurm.sh\u003c/code\u003e\naccording to your cluster\u0027s documentation and then run it. This submit script\nwill calculate everything except group statistics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs soon as all processing has been completed, you can run group statistics.\nThis is usually very fast, so you can do this in an interactive session. Run\n\u003ccode\u003esingularity run --no-home --cleanenv --bind /:/ext halfpipe_latest.sif --only-model-chunk\u003c/code\u003e\nand then select \u003ccode\u003eRun without modification\u003c/code\u003e in the user interface.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA common issue with remote work via secure shell is that the connection may\nbreak after a few hours. For batch jobs this is not an issue, but for\ninteractive jobs this can be quite frustrating. When the connection is lost,\nthe node you were connected to will automatically quit all programs you were\nrunning. To prevent this, you can run interactive jobs within \u003ccode\u003escreen\u003c/code\u003e or\n\u003ccode\u003etmux\u003c/code\u003e (whichever is available). These commands allow you to open sessions in\nthe terminal that will continue running in the background even when you close\nor disconnect. Here\u0027s a quick overview of how to use the commands (more\nin-depth documentation is available for example at\n[\u003ca href=\"http://www.dayid.org/comp/tm.html\" rel=\"nofollow\"\u003ehttp://www.dayid.org/comp/tm.html\u003c/a\u003e]).\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOpen a new screen/tmux session on the head node by running either \u003ccode\u003escreen\u003c/code\u003e\nor \u003ccode\u003etmux\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRequest an interactive job from within the session, for example with\n\u003ccode\u003esrun --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun the command that you want to run\u003c/li\u003e\n\u003cli\u003eDetach from the screen/tmux session, meaning disconnecting with the ability\nto re-connect later \u003cbr\u003e\nFor screen, this is done by first pressing \u003ccode\u003eControl+a\u003c/code\u003e, then letting go, and\nthen pressing \u003ccode\u003ed\u003c/code\u003e on the keyboard. \u003cbr\u003e\nFor tmux, it\u0027s \u003ccode\u003eControl+b\u003c/code\u003e instead of \u003ccode\u003eControl+a\u003c/code\u003e. \u003cbr\u003e\nNote that this is always \u003ccode\u003eControl\u003c/code\u003e, even if you\u0027re on a mac.\u003c/li\u003e\n\u003cli\u003eClose your connection to the head node with \u003ccode\u003eControl+d\u003c/code\u003e. \u003ccode\u003escreen\u003c/code\u003e/\u003ccode\u003etmux\u003c/code\u003e\nwill remain running in the background\u003c/li\u003e\n\u003cli\u003eLater, connect again to the head node. Run \u003ccode\u003escreen -r\u003c/code\u003e or \u003ccode\u003etmux attach\u003c/code\u003e to\ncheck back on the interactive job. If everything went well and the command\nyou wanted to run finished, close the interactive job with \u003ccode\u003eControl+d\u003c/code\u003e and\nthen the \u003ccode\u003escreen\u003c/code\u003e/\u003ccode\u003etmux\u003c/code\u003e session with \u003ccode\u003eControl+d\u003c/code\u003e again. If the command\nhasn\u0027t finished yet, detach as before and come back later\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quality-checks\" class=\"anchor\" href=\"#quality-checks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuality checks\u003c/h2\u003e\n\u003cp\u003ePlease see the manual at \u003ca href=\"https://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8\" rel=\"nofollow\"\u003ehttps://docs.google.com/document/d/1evDkVaoXqSaxulp5eSxVqgaxro7yZl-gao70D0S2dH8\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eA visual report page \u003ccode\u003ereports/index.html\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA table with image quality metrics \u003ccode\u003ereports/reportvals.txt\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA table containing the preprocessing status \u003ccode\u003ereports/reportpreproc.txt\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe untouched \u003ccode\u003efmriprep\u003c/code\u003e derivatives. Some files have been omitted to save\ndisk space \u003ccode\u003efmriprep\u003c/code\u003e is very strict about only processing data that is\ncompliant with the BIDS standard. As such, we may need to format subjects\nnames for compliance. For example, an input subject named \u003ccode\u003esubject_01\u003c/code\u003e will\nappear as \u003ccode\u003esubject01\u003c/code\u003e in the \u003ccode\u003efmriprep\u003c/code\u003e derivatives. \u003ccode\u003ederivatives/fmriprep\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-subject-level-features\" class=\"anchor\" href=\"#subject-level-features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubject-level features\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eFor task-based, seed-based connectivity and dual regression features,\n\u003ccode\u003eHALFpipe\u003c/code\u003e outputs the statistical maps for the effect, the variance, the\ndegrees of freedom of the variance and the z-statistic. In FSL, the effect and\nvariance are also called \u003ccode\u003ecope\u003c/code\u003e and \u003ccode\u003evarcope\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-effect_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-variance_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-dof_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._stat-z_statmap.nii.gz\u003c/code\u003e \u003cbr\u003e\nThe design and contrast matrix used for the final model will be outputted alongside\nthe statistical maps \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-design_matrix.tsv\u003c/code\u003e\n\u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-contrast_matrix.tsv\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReHo and fALFF are not calculated based on a linear model. As such, only one\nstatistical map of the z-scaled values will be output \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._alff.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._falff.nii.gz\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._reho.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor every feature, a JSON file containing a summary of the preprocessing\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003esettings, and a list of the raw data files that were used for the analysis\n(\u003ccode\u003eRawSources\u003c/code\u003e) \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/....json\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor every feature, the corresponding brain mask is output beside the\nstatistical maps. Masks do not differ between different features calculated,\nthey are only copied out repeatedly for convenience \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/...desc-brain_mask.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAtlas-based connectivity outputs the time series and the full covariance and\ncorrelation matrices as text files \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._timeseries.txt\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._desc-covariance_matrix.txt\u003c/code\u003e \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._desc-correlation_matrix.txt\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-preprocessed-images\" class=\"anchor\" href=\"#preprocessed-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreprocessed images\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMasked, preprocessed BOLD image \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._bold.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJust like for features \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/..._bold.json\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJust like for features \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-brain_mask.nii.gz\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFiltered confounds time series, where all filters that are applied to the BOLD\nimage are applied to the regressors as well. Note that this means that when\ngrand mean scaling is active, confounds time series are also scaled, meaning\nthat values such as \u003ccode\u003eframewise displacement\u003c/code\u003e can not be interpreted in terms\nof their original units anymore. \u003cbr\u003e\n\u003ccode\u003ederivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-confounds_regressors.tsv\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-group-level\" class=\"anchor\" href=\"#group-level\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGroup-level\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egrouplevel/...\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eIf an error occurs, this will be output to the command line and simultaneously\nto the \u003ccode\u003eerr.txt\u003c/code\u003e file in the working directory\u003c/li\u003e\n\u003cli\u003eIf the error occurs while running, usually a text file detailing the error\nwill be placed in the working directory. These are text files and their file\nnames start with \u003ccode\u003ecrash\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eUsually, the last line of these text files contains the error message.\nPlease read this carefully, as may allow you to understand the error\u003c/li\u003e\n\u003cli\u003eFor example, consider the following error message:\n\u003ccode\u003eValueError: shape (64, 64, 33) for image 1 not compatible with first image shape (64, 64, 34) with axis == None\u003c/code\u003e\nThis error message may seem cryptic at first. However, looking at the\nmessage more closely, it suggests that two input images have different,\nincompatible dimensions. In this case, \u003ccode\u003eHALFpipe\u003c/code\u003e correctly recognized this\nissue, and there is no need for concern. The images in question will simply\nbe excluded from preprocessing and/or analysis\u003c/li\u003e\n\u003cli\u003eIn some cases, the cause of the error can be a bug in the \u003ccode\u003eHALFpipe\u003c/code\u003e code.\nPlease check that no similar issue has been reported\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/issues\"\u003ehere on GitHub\u003c/a\u003e. In this case,\nplease submit an\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/issues/new/choose\"\u003eissue\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-command-line-flags\" class=\"anchor\" href=\"#command-line-flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand line flags\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-control-command-line-logging\" class=\"anchor\" href=\"#control-command-line-logging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eControl command line logging\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--verbose\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBy default, only errors and warnings will be output to the command line. This\nmakes it easier to see when something goes wrong, because there is less output.\nHowever, if you want to be able to inspect what is being run, you can add the\n\u003ccode\u003e--verbose\u003c/code\u003e flag to the end of the command used to call \u003ccode\u003eHALFpipe\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eVerbose logs are always written to the \u003ccode\u003elog.txt\u003c/code\u003e file in the working directory,\nso going back and inspecting this log is always possible, even if the\n\u003ccode\u003e--verbose\u003c/code\u003e flag was not specified.\u003c/p\u003e\n\u003cp\u003eSpecifying the flag \u003ccode\u003e--debug\u003c/code\u003e will print additional, fine-grained messages. It\nwill also automatically start the\n\u003ca href=\"https://docs.python.org/3/library/pdb.html\" rel=\"nofollow\"\u003ePython Debugger\u003c/a\u003e when an error\noccurs. You should only use \u003ccode\u003e--debug\u003c/code\u003e if you know what you\u0027re doing.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-automatically-remove-unneeded-files\" class=\"anchor\" href=\"#automatically-remove-unneeded-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAutomatically remove unneeded files\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--keep\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e saves intermediate files for each pipeline step. This speeds up\nre-running with different settings, or resuming after a job after it was\ncancelled. The intermediate file are saved by the\n\u003ca href=\"https://nipype.readthedocs.io/\" rel=\"nofollow\"\u003e\u003ccode\u003enipype\u003c/code\u003e\u003c/a\u003e workflow engine, which is what\n\u003ccode\u003eHALFpipe\u003c/code\u003e uses internally. \u003ccode\u003enipype\u003c/code\u003e saves the intermediate files in the\n\u003ccode\u003enipype\u003c/code\u003e folder in the working directory.\u003c/p\u003e\n\u003cp\u003eIn environments with limited disk capacity, this can be problematic. To limit\ndisk usage, \u003ccode\u003eHALFpipe\u003c/code\u003e can delete intermediate files as soon as they are not\nneeded anymore. This behavior is controlled with the \u003ccode\u003e--keep\u003c/code\u003e flag.\u003c/p\u003e\n\u003cp\u003eThe default option \u003ccode\u003e--keep some\u003c/code\u003e keeps all intermediate files from fMRIPrep and\nMELODIC, which would take the longest to re-run. We believe this is a good\ntradeoff between disk space and computer time. \u003ccode\u003e--keep all\u003c/code\u003e turns of all\ndeletion of intermediate files. \u003ccode\u003e--keep none\u003c/code\u003e deletes as much as possible,\nmeaning that the smallest amount possible of disk space will be used.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-configure-nipype\" class=\"anchor\" href=\"#configure-nipype\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfigure nipype\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--nipype-\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eomp-nthreads\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003ememory-gb\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003en-procs\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003erun-plugin\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003eHALFpipe\u003c/code\u003e chooses sensible defaults for all of these values.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-choose-which-parts-to-run-or-to-skip\" class=\"anchor\" href=\"#choose-which-parts-to-run-or-to-skip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChoose which parts to run or to skip\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eonly\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003eskip\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e-\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003espec-ui\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003eworkflow\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003erun\u003cspan class=\"pl-k\"\u003e|\u003c/span\u003emodel-chunk\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eA \u003ccode\u003eHALFpipe\u003c/code\u003e run is divided internally into three stages, spec-ui, workflow, and\nrun.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe \u003ccode\u003espec-ui\u003c/code\u003e stage is where you specify things in the user interface. It\ncreates the \u003ccode\u003espec.json\u003c/code\u003e file that contains all the information needed to run\n\u003ccode\u003eHALFpipe\u003c/code\u003e. To only run this stage, use the option \u003ccode\u003e--only-spec-ui\u003c/code\u003e. To skip\nthis stage, use the option \u003ccode\u003e--skip-spec-ui\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eworkflow\u003c/code\u003e stage is where \u003ccode\u003eHALFpipe\u003c/code\u003e uses the \u003ccode\u003espec.json\u003c/code\u003e data to search\nfor all the files that match what was input in the user interface. It then\ngenerates a \u003ccode\u003enipype\u003c/code\u003e workflow for preprocessing, feature extraction and group\nmodels. \u003ccode\u003enipype\u003c/code\u003e then validates the workflow and prepares it for execution.\nThis usually takes a couple of minutes and cannot be parallelized. For\nhundreds of subjects, this may even take a few hours. This stage has the\ncorresponding option \u003ccode\u003e--only-workflow\u003c/code\u003e and \u003ccode\u003e--skip-workflow\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eThis stage saves several intermediate files. These are named\n\u003ccode\u003eworkflow.{uuid}.pickle.xz\u003c/code\u003e, \u003ccode\u003eexecgraph.{uuid}.pickle.xz\u003c/code\u003e and\n\u003ccode\u003eexecgraph.{n_chunks}_chunks.{uuid}.pickle.xz\u003c/code\u003e. The \u003ccode\u003euuid\u003c/code\u003e in the file name is\na unique identifier generated from the \u003ccode\u003espec.json\u003c/code\u003e file and the input files.\nIt is re-calculated every time we run this stage. The uuid algorithm produces\na different output if there are any changes (such as when new input files for\nnew subjects become available, or the \u003ccode\u003espec.json\u003c/code\u003e is changed, for example to\nadd a new feature or group model). Otherwise, the \u003ccode\u003euuid\u003c/code\u003e stays the same.\nTherefore, if a workflow file with the calculated \u003ccode\u003euuid\u003c/code\u003e already exists, then\nwe do not need to run this stage. We can simple re-use the workflow from the\nexisting file, and save some time.\u003c/li\u003e\n\u003cli\u003eIn this stage, we can also decide to split the execution into chunks. The flag\n\u003ccode\u003e--subject-chunks\u003c/code\u003e creates one chunk per subject. The flag \u003ccode\u003e--use-cluster\u003c/code\u003e\nautomatically activates \u003ccode\u003e--subject-chunks\u003c/code\u003e. The flag \u003ccode\u003e--n-chunks\u003c/code\u003e allows the\nuser to specify a specific number of chunks. This is useful if the execution\nshould be spread over a set number of computers. In addition to these, a model\nchunk is generated.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003eThe \u003ccode\u003erun\u003c/code\u003e stage loads the \u003ccode\u003eexecgraph.{n_chunks}_chunks.{uuid}.pickle.xz\u003c/code\u003e file\ngenerated in the previous step and runs it. This file usually contains two\nchunks, one for the subject level preprocessing and feature extraction\n(\"subject level chunk\"), and one for group statistics (\"model chunk\"). To run\na specific chunk, you can use the flags \u003ccode\u003e--only-chunk-index ...\u003c/code\u003e and\n\u003ccode\u003e--only-model-chunk\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-working-directory\" class=\"anchor\" href=\"#working-directory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorking directory\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--workdir\u003c/pre\u003e\u003c/div\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTODO\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data-file-system-root\" class=\"anchor\" href=\"#data-file-system-root\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData file system root\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e--fs-root\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe \u003ccode\u003eHALFpipe\u003c/code\u003e container, or really most containers, contain the entire base\nsystem needed to run\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003eFor questions or support, please submit an\n\u003ca href=\"https://github.com/HALFpipe/HALFpipe/issues/new/choose\"\u003eissue\u003c/a\u003e or contact us\nvia e-mail.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eName\u003c/th\u003e\n\u003cth\u003eRole\u003c/th\u003e\n\u003cth\u003eE-mail address\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eLea Waller\u003c/td\u003e\n\u003ctd\u003eDeveloper\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:lea.waller@charite.de\"\u003elea.waller@charite.de\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eIlya Veer\u003c/td\u003e\n\u003ctd\u003eProject manager\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:ilya.veer@charite.de\"\u003eilya.veer@charite.de\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSusanne Erk\u003c/td\u003e\n\u003ctd\u003eProject manager\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"mailto:susanne.erk@charite.de\"\u003esusanne.erk@charite.de\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 7,
    "subscribers_count": 1,
    "topics": [
      "neuroimaging"
    ],
    "updated_at": 1621624087.0
  },
  {
    "data_format": 2,
    "description": "Antonino Furnari\u0027s fork of Feichtenhofer\u0027s gpu_flow, with temporal dilation.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "dl-container-registry/furnari-flow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-gpu-based-optical-flow-extraction-from-videos\" class=\"anchor\" href=\"#gpu-based-optical-flow-extraction-from-videos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGPU based optical flow extraction from videos\u003c/h1\u003e\n\u003cp\u003eForked from \u003ca href=\"https://github.com/feichtenhofer/gpu_flow\"\u003ehttps://github.com/feichtenhofer/gpu_flow\u003c/a\u003e by Antonino Furnari\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/dl-container-registry/furnari-flow\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/22af16742ba115e53d8c72ecae46310b24dacb32e78ec3f7172c231c7cbc7c73/68747470733a2f2f7472617669732d63692e6f72672f646c2d636f6e7461696e65722d72656769737472792f6675726e6172692d666c6f772e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/dl-container-registry/furnari-flow.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/willprice/furnari-flow/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/99bb6090faef97032d3bfd80b4d0cdb9d984e9e97aeb1d2750bc3e442fb117f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65726875622d3232623865622e737667\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/badge/hosted-dockerhub-22b8eb.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/575\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"Singularity Hub\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNews\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2020-01-09\" class=\"anchor\" href=\"#2020-01-09\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2020-01-09\u003c/h3\u003e\n\u003cp\u003eThe semantics of the dilation parameter have changed to allow finer grained configuration. Previously optical flow was\ncomputed between frames I_{st} and I_{s(t+d)} where s is the stride and d the dilation. The code now computes flow\nbetween I_{st} and I_{st+d}--this makes the stride and dilation parameters completely independent which is more intuitive.\nIf you wish to continue using the old code then use the docker image tagged with \u003ccode\u003ev1\u003c/code\u003e. All subsequent images and the\n\u003ccode\u003elatest\u003c/code\u003e tag will adopt the new behaviour described above.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eWe support running via docker and singularity.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEnsure you\u0027re running\n\u003ca href=\"https://github.com/NVIDIA/nvidia-docker\"\u003e\u003ccode\u003envidia-docker\u003c/code\u003e\u003c/a\u003e as this software is\nGPU accelerated. If using docker 19.03 or above then you can use the native docker nvidia GPU support.\u003c/li\u003e\n\u003cli\u003ePull the docker image:\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003edocker pull willprice/furnari-flow\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003eDump out frames from the video you wish to compute flow for:\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003emkdir my_video\u003cspan class=\"pl-k\"\u003e;\u003c/span\u003e ffmpeg -i my_video.mp4 -qscale 3 my_video/img_%06d.jpg\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003eCompute the flow using \u003ccode\u003efurnari-flow\u003c/code\u003e:\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003emkdir my_video_flow\u003c/span\u003e\n$ \u003cspan class=\"pl-s1\"\u003edocker run \\\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e    --runtime=nvidia \\\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e    --rm \\\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e    --mount \"type=bind,source=$PWD/my_video,target=/input\" \\\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e    --mount \"type=bind,source=$PWD/my_video_flow,target=/output\" \\\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e    --mount \"type=bind,source=$HOME/.nv,target=/cache/nv\" \\\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e    willprice/furnari-flow \\\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e    img_%06d.jpg\u003c/span\u003e\n$ \u003cspan class=\"pl-s1\"\u003els my_video_flow\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eu v\u003c/span\u003e\n$ \u003cspan class=\"pl-s1\"\u003els my_video_flow/u\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eimg_0000001.jpg\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eimg_0000002.jpg\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-details\" class=\"anchor\" href=\"#details\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetails\u003c/h3\u003e\n\u003cp\u003eThe software assumes that all video frames have been extracted in a directory. Files should be named according to some pattern, e.g., \u003ccode\u003eimg_%07d.jpg\u003c/code\u003e. The software will put flow files in the same directory using a provided filename pattern, e.g., \u003ccode\u003eflow_%s_%07d.jpg\u003c/code\u003e, where the %s will be subsituted with \"x\" for the x flows and \"y\" for the y flows. For example, if DIR is a directory containing 4 images:\u003c/p\u003e\n\u003cp\u003eDIR:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000001.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000002.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000003.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000004.jpg\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ethe command \u003ccode\u003ecompute_flow DIR img_%07d.jpg flow_%s_%07d.jpg\u003c/code\u003e will read the images in order and compute optical flows. The content of DIR will be as follows after the execution of the command:\u003c/p\u003e\n\u003cp\u003eDIR:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000001.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000002.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000003.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimg_0000004.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_x_0000001.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_x_0000002.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_x_0000003.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_x_0000004.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_y_0000001.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_y_0000002.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_y_0000003.jpg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eflow_y_0000004.jpg\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ewhere \u003ccode\u003eflow_x_{n}.jpg\u003c/code\u003e is the x flow computed between \u003ccode\u003eimg_{n}.jpg\u003c/code\u003e and \u003ccode\u003eimg_{n+1}.jpg\u003c/code\u003e (if no dilation is used - see help).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou only need to build this software if you intend on tweaking the source, otherwise you\nshould just use the pre-built docker images.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://opencv.org/downloads.html\" rel=\"nofollow\"\u003eOpenCV 2.4\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://cmake.org/\" rel=\"nofollow\"\u003ecmake\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eFirst, build opencv with gpu support. To do so, download opencv 2.4.x sources\nfrom \u003ca href=\"https://opencv.org/releases.html\" rel=\"nofollow\"\u003ehttps://opencv.org/releases.html\u003c/a\u003e. Unzip the downloaded archive, then enter\nthe opencv folder and issue the following commands:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emkdir build\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecd build\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecmake -DCUDA_USE_STATIC_CUDA_RUNTIME=OFF ..\u003c/code\u003e (inspect the \u003ca href=\"./Dockerfile\"\u003e\u003ccode\u003eDockerfile\u003c/code\u003e\u003c/a\u003e for further flags that might\nbe of use)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emake -j $(nproc)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen clone the current repository and enter the \u003ccode\u003ecompute_flow_video\u003c/code\u003e folder. Type:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eexport OpenCV_DIR=path_to_opencv_build_directory\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emkdir build\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecd build\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecmake -DCUDA_USE_STATIC_CUDA_RUNTIME=OFF ..\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emake -j $(nproc)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 7,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1600461582.0
  },
  {
    "data_format": 2,
    "description": "Code and documentation supporting Markello \u0026 Misic, 2021, \"Comparing spatial null models for brain maps\" (NeuroImage)",
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "netneurolab/markello_spatialnulls",
    "latest_release": "0.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-spatially-constrained-null-models-in-neuroimaging\" class=\"anchor\" href=\"#spatially-constrained-null-models-in-neuroimaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpatially-constrained null models in neuroimaging\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-whats-in-this-repository\" class=\"anchor\" href=\"#whats-in-this-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"What\u0027s in this repository?\"\u003c/h2\u003e\n\u003cp\u003eThis repository contains data, code, and results for the manuscript \"\u003ca href=\"https://doi.org/10.1016/j.neuroimage.2021.118052\" rel=\"nofollow\"\u003eComparing spatial null models for brain maps\u003c/a\u003e\" by Ross Markello \u0026amp; Bratislav Misic (\u003cem\u003eNeuroImage\u003c/em\u003e, 2021).\nWe investigated how well different null model implementations account for spatial autocorrelation in statistical analyses of whole-brain neuroimaging data.\u003c/p\u003e\n\u003cp\u003eWe\u0027ve tried to document the various aspects of this repository with a whole bunch of README files, so feel free to jump around and check things out.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-just-let-me-run-the-things\" class=\"anchor\" href=\"#just-let-me-run-the-things\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"Just let me run the things!\"\u003c/h2\u003e\n\u003cp\u003eItching to just run the analyses?\nYou\u0027ll need to make sure you have installed the appropriate software packages, have access to the HCP, and have downloaded the appropriate data files (check out our \u003ca href=\"https://netneurolab.github.io/markello_spatialnulls\" rel=\"nofollow\"\u003ewalkthrough\u003c/a\u003e for more details!).\nOnce you\u0027ve done that, you can get going with the following:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/netneurolab/markello_spatialnulls\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e markello_spatialnulls\nconda env create -f environment.yml\nconda activate markello_spatialnulls\npip install parspin/\nmake all\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you don\u0027t want to deal with the hassle of creating a new Python environment, download the Singularity image that we used to run our analyses and run things in there:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/netneurolab/markello_spatialnulls\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e markello_spatialnulls\nwget -O container/markello_spatialnulls.simg https://osf.io/za7fn/download\nsingularity run container/markello_spatialnulls.simg make all\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-id-like-more-information\" class=\"anchor\" href=\"#id-like-more-information\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"I\u0027d like more information.\"\u003c/h2\u003e\n\u003cp\u003eIf you want a step-by-step through all the methods + analyses, take a look at our \u003ca href=\"https://netneurolab.github.io/markello_spatialnulls\" rel=\"nofollow\"\u003ewalkthrough\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-i-have-some-questions\" class=\"anchor\" href=\"#i-have-some-questions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"I have some questions...\"\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/netneurolab/markello_spatialnulls/issues\"\u003eOpen an issue\u003c/a\u003e on this repository and someone will try and get back to you as soon as possible!\u003c/p\u003e\n",
    "stargazers_count": 7,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1621535597.0
  },
  {
    "data_format": 2,
    "description": "A tool for generating bacterial genomes from metagenomes with nanopore long read sequencing",
    "filenames": [
      "singularity/Singularity.longread",
      "singularity/Singularity.htsbox",
      "singularity/Singularity.quickmerge"
    ],
    "full_name": "elimoss/lathe",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lathe\" class=\"anchor\" href=\"#lathe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLathe\u003c/h1\u003e\n\u003cp\u003eA tool for generating bacterial genomes from metagenomes with Nanopore long read sequencing\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h1\u003e\n\u003cp\u003eFirst, install \u003ca href=\"https://conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003eminiconda3\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThen install snakemake.  This can be done with the following.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda install snakemake\nsnakemake --version #please ensure this is \u0026gt;=5.4.3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, clone this github directory to some location where it can be stored permanently.  Remember to keep it updated with \u003ccode\u003egit pull\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/elimoss/lathe.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInstructions to enable cluster execution with SLURM can be found at \u003ca href=\"https://github.com/bhattlab/slurm\"\u003ehttps://github.com/bhattlab/slurm\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-alter-configyaml-to-provide-the-following\" class=\"anchor\" href=\"#alter-configyaml-to-provide-the-following\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlter config.yaml to provide the following:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003esample_name\u003c/strong\u003e: Name of sample and output directory\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003efast5_dirs_list\u003c/strong\u003e: text file containing a list of absolute paths to run/fast5/* subfolders containing .fast5 files.  A good way to generate this is with \u003ccode\u003efind -maxdepth 2 -mindepth 2 fast5_parent_dir \u0026gt; fodn.txt\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eflowcell\u003c/strong\u003e: flowcell code, e.g. FLO-MIN106, passed to basecaller\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ekit\u003c/strong\u003e: kit code, e.g. SQK-LSK109, passed to basecaller\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003egenome_size\u003c/strong\u003e: Estimated genome size, e.g. 50m, passed to Canu.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003esingularity\u003c/strong\u003e: location (including on the internet) of a singularity image to be used for the workflow.  Don\u0027t change this.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eshort_reads\u003c/strong\u003e: location of short reads to be used for Pilon polishing, or empty quotes for long-read polishing.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003euse_grid\u003c/strong\u003e: should Canu execute in distributed mode on a cluster?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003egrid_options\u003c/strong\u003e: Extra options for execution on a cluster\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ecanu_args\u003c/strong\u003e: Extra options for Canu\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eskip_circularization\u003c/strong\u003e: Should circularization be omitted from the workflow?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLathe uses the Flye assembler by default. For Canu, please specify \u0027canu\u0027 for the assembler parameter in the config. For cluster Canu execution, please note: if set to True, you will need to install Canu, e.g. \u003ccode\u003econda install -c conda-forge -c bioconda Canu=1.8\u003c/code\u003e as well as provide any additional required parameters for your job scheduler in the config.yaml file.  Please see the example config file. When executing on a cluster, Canu will appear to fail, as the first process does not produce an assembly and instead spawns subsequent jobs on the cluster.  Don\u0027t worry, just re-run Lathe when the assembly completes.\u003c/p\u003e\n\u003cp\u003eTo execute please run the following.  Please note, you must substitute a parent directory containing all of your data and working directories for \u003ccode\u003e/labs/\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --use-singularity --singularity-args \u0027--bind /labs/ --bind /scratch/ --bind /scg/ \u0027 -s /path/to/lathe/Snakefile \\\n--configfile path/to/modified_config.yaml --restart-times 0 --keep-going --latency-wait 30\n# --profile scg #enable cluster support, highly recommended.  See above.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cp\u003eThe outputs generated by this workflow will look like the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esamplename/\n\u251c\u2500\u2500 0.basecall\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename.fq\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 nanoplots\n\u251c\u2500\u2500 1.assemble\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_merged.fasta\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa.amb\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa.ann\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa.bwt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa.fai\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa.pac\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa.paf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_raw_assembly.fa.sa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 assemble_100m (if specified)\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 assemble_250m (if specified)\n\u251c\u2500\u2500 2.polish\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_polished.corrected.fasta\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_polished.fasta\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_polished.fasta.bam\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_polished.fasta.bam.bai\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_polished.fasta.fai\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samplename_polished.fasta.misassemblies.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 medaka (if specified)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pilon (if specified)\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 racon (if specified)\n\u251c\u2500\u2500 3.circularization\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.candidate_genomes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2.circularization\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3.circular_sequences #circularized genomes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4.samplename_circularized.corrected.fasta\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4.samplename_circularized.fasta\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4.samplename_circularized.fasta.bam\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4.samplename_circularized.fasta.bam.bai\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4.samplename_circularized.fasta.fai\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 4.samplename_circularized.fasta.misassemblies.tsv\n\u2514\u2500\u2500 5.final\n \u00a0\u00a0 \u251c\u2500\u2500 samplename_final.fa\n \u00a0\u00a0 \u2514\u2500\u2500 samplename_final.fa.fai\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 7,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1619198094.0
  },
  {
    "data_format": 2,
    "description": "Grow virtual creatures in static and physics simulated environments.",
    "filenames": [
      "cluster/Singularity"
    ],
    "full_name": "cfusting/conditional-growth",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-growing-virtual-creatures\" class=\"anchor\" href=\"#growing-virtual-creatures\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGrowing Virtual Creatures\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.com/cfusting/conditional-growth\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5d1f5f4e97ec14d197ffc19e3f24ec51393310f3052d9db011fade1eb7a0a581/68747470733a2f2f7472617669732d63692e636f6d2f6366757374696e672f636f6e646974696f6e616c2d67726f7774682e7376673f6272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/cfusting/conditional-growth.svg?branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout\u003c/h2\u003e\n\u003cp\u003eThis package provides the necessary tooling to grow virtual creatures made from three-dimensional blocks called voxels (3d pixels). Starting with one or more voxels new voxels are iteratively added based on the composition of nearby voxels and the current position. Environments exist for gridworld and \u003ca href=\"https://github.com/voxcraft/voxcraft-sim\"\u003evoxcraft-sim\u003c/a\u003e. Gridworld has no physics engine and is thus extremely fast to run.\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-with-docker\" class=\"anchor\" href=\"#building-with-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding with Docker\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h3\u003e\n\u003cp\u003eIf you would like to use a GPU make sure to install \u003ca href=\"https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime\" rel=\"nofollow\"\u003envidia-container-runtime\u003c/a\u003e. Other than that the Dockerfile will handle all the dependencies.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installing-nvidia-container-runtime\" class=\"anchor\" href=\"#installing-nvidia-container-runtime\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling Nvidia container runtime\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edistribution=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003e. /etc/os-release\u003cspan class=\"pl-k\"\u003e;\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$ID$VERSION_ID\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e \\\n   \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e sudo apt-key add - \\\n   \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e curl -s -L https://nvidia.github.io/nvidia-docker/\u003cspan class=\"pl-smi\"\u003e$distribution\u003c/span\u003e/nvidia-docker.list \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n   \nsudo apt-get update \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e sudo apt-get install -y nvidia-docker2\nsudo systemctl restart docker\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h3\u003e\n\u003cp\u003eClone this repository and navigate into the root folder. Build the Dockerfile and tag it \"grow\".\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker build -t grow \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cbr\u003e\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-optimizing-a-creature-in-gridworld\" class=\"anchor\" href=\"#example-optimizing-a-creature-in-gridworld\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample: Optimizing a Creature in gridworld\u003c/h2\u003e\n\u003cp\u003eIn this example we will build a creature for which surface area is maximized and volume is minimized.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h3\u003e\n\u003cp\u003eRun the optimization script, storing the results in the host environment\u0027s /tmp directory.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run --rm --gpus all -v /tmp:/home/ray/ray_results --shm-size 2G grow python examples/optimize_grid.py\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-metrics\" class=\"anchor\" href=\"#metrics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMetrics\u003c/h3\u003e\n\u003cp\u003eMetrics are captured by the \u003ca href=\"https://docs.ray.io/en/master/\" rel=\"nofollow\"\u003eray\u003c/a\u003e framework in /tmp/expname where expname is specified in the optimize_grid.py script. The easiest way to view the metrics is to use tensorboard. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run -p 6006:6006 --rm -v /tmp:/tmp tensorflow/tensorflow tensorboard --logdir /tmp/max_surface_area --host 0.0.0.0 --port 6000\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ca href=\"./docs/tensorboard.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/tensorboard.png\" alt=\"tensorboard\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-viewing-the-creatures\" class=\"anchor\" href=\"#viewing-the-creatures\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eViewing the Creatures\u003c/h3\u003e\n\u003cp\u003eUncommenting \u003ccode\u003emonitor=True\u003c/code\u003e in optimize_grid.py will enable the recording of a creature being built. The resulting movies can be found in /tmp/ray_results/expname/trialname (as can all the other logs). Refer to \u003ca href=\"https://docs.ray.io/en/master/rllib.html\" rel=\"nofollow\"\u003eRLlib\u003c/a\u003e for more details.\u003c/p\u003e\n\u003cp\u003eDue to a memory leak in vtk (which is the graphics library used to create the movies), enabling monitoring will eventually cause the trial to crash. To avoid this run your experiment until convergence and turn on monitoring after loading a checkpoint to capture a few movies at that point in training.\u003c/p\u003e\n\u003cp\u003eBelow are some videos of this example mid-way through training and at convergence. At convergence the growth function builds a pillar. This is the optimal creature given the space is unconstrained and voxels must connect.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"./docs/midway.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/midway.gif\" alt=\"midway\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"./docs/column.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/column.gif\" alt=\"column\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cbr\u003e\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-theory\" class=\"anchor\" href=\"#theory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTheory\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"./docs/theory1.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/theory1.jpg\" alt=\"theory1\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"./docs/theory2.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/theory2.jpg\" alt=\"theory2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 8,
    "subscribers_count": 1,
    "topics": [
      "virtual-creatures",
      "reinforcement-learning",
      "reinforcement-learning-environments"
    ],
    "updated_at": 1621465175.0
  },
  {
    "data_format": 2,
    "description": "Bayesian model selection to detect zero-inflated genes",
    "filenames": [
      "Singularity"
    ],
    "full_name": "churchill-lab/scRATE",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4398\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/kbchoi/scrate\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/803db9bd957916e2b4e1dc773894c7890a271e4dd3dec7137ec4b9ac01351461/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f636c6f75642f6175746f6d617465642f6b6263686f692f736372617465\" alt=\"Docker Cloud Automated build\" data-canonical-src=\"https://img.shields.io/docker/cloud/automated/kbchoi/scrate\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-scrate\" class=\"anchor\" href=\"#scrate\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003escRATE\u003c/h1\u003e\n\u003cp\u003escRATE is a model selection-based scRNA-seq quantitation algorithm that controls overfitting and unwarranted imputation of technical zeros. We first fit UMI counts with Poisson (P), Negative-Binomial (NB), Zero-inflated Poisson (ZIP), and Zero-inflated Negative Binomial models (ZINB). These models are applicable to UMI counts directly and therefore no need of arbitrary preprocessing of counts, e.g., normalization (by scaling to, for example, CPM) or log-transformation (with pseudocounts). Our model comparison enables us to compute denoised rates of gene expression using the best model which each gene data is conforming to.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFree software: GPLv3 license\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eInstallation of scRATE is simple, although it may take a while as it has to compile rstan, the related packages, and all their dependencies.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003echurchill-lab/scRATE\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e library(\u003cspan class=\"pl-smi\"\u003escRATE\u003c/span\u003e)\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e version()\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003e____ ____ ____ ____ ___ ____\n[__  |    |__/ |__|  |  |___\n___] |___ |  \\ |  |  |  |___\n                   Ver:0.1.2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis might hick up because of hdf5r or loomR. Then, try the following.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e install.packages(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ehdf5r\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-v\"\u003erepo\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emojaveazure/loomR\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003eref\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edevelop\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eHere is a quick example on how to use it. We first load \u003ccode\u003escRATE\u003c/code\u003e and \u003ccode\u003eloomR\u003c/code\u003e package. We use loom format files to store single-cell gene expression data as it is supported in both \u003ccode\u003eR\u003c/code\u003e and \u003ccode\u003epython\u003c/code\u003e. In \u003ccode\u003escRATE\u003c/code\u003e, we also provide handy features that facilitate the handling of model selection results with loom format file.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e library(\u003cspan class=\"pl-smi\"\u003escRATE\u003c/span\u003e)\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e library(\u003cspan class=\"pl-smi\"\u003eloomR\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eA data file in the example is available at \u003ca href=\"ftp://churchill-lab.jax.org/analysis/scRATE/DC-like_cells.loom\" rel=\"nofollow\"\u003eftp://churchill-lab.jax.org/analysis/scRATE/DC-like_cells.loom\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eds\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e connect(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003eDC-like_cells.loom\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003ecntmat\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e t(\u003cspan class=\"pl-smi\"\u003eds\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003ematrix\u003c/span\u003e[,])\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003egsymb\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eds\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003erow.attrs\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003eGeneID\u003c/span\u003e[]\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eds\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003eclose_all()\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e head(\u003cspan class=\"pl-smi\"\u003egsymb\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003e[1] \"Xkr4\"    \"Gm1992\"  \"Gm37381\" \"Rp1\"     \"Rp1.1\"   \"Sox17\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe recommend using offset (or exposure) in order to reflect the difference in depth of coverage across cells while fitting the count models. The models we compare use log link function, and therefore, the offsets should be log transformed too.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eexposure\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e log(colSums(\u003cspan class=\"pl-smi\"\u003ecntmat\u003c/span\u003e))\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e head(\u003cspan class=\"pl-smi\"\u003eexposure\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003e  [1] 8.815518 8.996157 9.025816 9.037771 9.062420 9.397732\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe will pick a gene, \u003cem\u003eCybb\u003c/em\u003e (one that we know that it fits to ZINB model significantly better than the other models), and load its UMI counts into a data frame.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003egg\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e4153\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003egsymb\u003c/span\u003e[\u003cspan class=\"pl-smi\"\u003egg\u003c/span\u003e]\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003e[1] \"Cybb\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003ey\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003ecntmat\u003c/span\u003e[\u003cspan class=\"pl-smi\"\u003egg\u003c/span\u003e,]\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003ey\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003e  [1]  1  0  8  7  3  6  7  1  4  0  0  2  2 10  0  3  1  1  2  4  6  3  9  3  1\n [26]  4  1  2  3  8  0  0  1  6  2  6  3  0  0  1  0  4  1  3  0  2 17  1  2  5\n [51]  0  1  1  0  6  3  4  4  3  1  3  1  7  8  0  2  4  6  2  7  0  4  0  1  0\n [76]  2  0  8  2  0  1  4  9  3  0  2  2  3  0  3  0  0  4  1  4  3  2  3  2  2\n[101]  8  0  5  8  2  3  0  3  0  3  6  0  4  2 11  5  1  2 12  2  4  2  6  8  6\n[126]  3  3  1  0  2  1  1 10  2  2  0  2  2  1  0  1  0  3 11  0  7  2  2  2  3\n[151]  7  0  1 10  4  1  4  5  1  0  1  3  3  0  5  4  9  4  2  0  4  1  2  4  1\n[176]  2  1  4  2  1  0  1  0  1  4  6  8  9  4  3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003egexpr\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"pl-k\"\u003edata.frame\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003ey\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003ey\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003eexposure\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003eexposure\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe must have \u003ccode\u003ey\u003c/code\u003e and \u003ccode\u003eexposure\u003c/code\u003e as variables. We can also append covariates to the data frame.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003egexpr\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan class=\"pl-k\"\u003edata.frame\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003ey\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003ey\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003eexposure\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003eexposure\u003c/span\u003e, \u003cspan class=\"pl-smi\"\u003ecelltype\u003c/span\u003e, \u003cspan class=\"pl-smi\"\u003esex\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe are now ready to fit the models.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003emodel_fit\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e fit_count_models(\u003cspan class=\"pl-smi\"\u003egexpr\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOr we can specify our own model formula (See brms documentation) if there are covariates. Note we should not use \u003ccode\u003eoffset()\u003c/code\u003e function in the formula.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003emodel_fit\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e fit_count_models(\u003cspan class=\"pl-smi\"\u003egexpr\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ey ~ 1 + (1|sex) + (1|celltype)\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003eFitting models for Cybb\nFitting data with Poisson model...\nFitting data with Negative Binomial model...\nFitting data with Zero-Inflated Poisson model...\nCompiling the C++ model\n\nStart sampling\nSAMPLING FOR MODEL \u0027zip\u0027 NOW (CHAIN 1).\n\nChain 1:\nChain 1: Gradient evaluation took 0.000177 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.77 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1:\nChain 1:\n...\n\nFitting data with Zero-Inflated Negative Binomial model...\nCompiling the C++ model\nStart sampling\n\nSAMPLING FOR MODEL \u0027zinb\u0027 NOW (CHAIN 1).\nChain 1:\nChain 1: Gradient evaluation took 0.000329 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.29 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1:\nChain 1:\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen we compare the models with the leave-one-out cross validation test, and select the model that best fits the data.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eelpd_loo\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;-\u003c/span\u003e compare_count_models(\u003cspan class=\"pl-smi\"\u003emodel_fit\u003c/span\u003e)\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eelpd_loo\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003e     elpd_diff se_diff\nZINB    0.0       0.0\nNB     -3.0       2.7\nZIP   -35.6       9.5\nP    -103.9      19.6\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e select_model(\u003cspan class=\"pl-smi\"\u003eelpd_loo\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003emargin\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n[\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e] \u003cspan class=\"pl-c1\"\u003e4\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-cite\" class=\"anchor\" href=\"#how-to-cite\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to cite\u003c/h2\u003e\n\u003cp\u003eK. Choi, Y. Chen, D.A. Skelly, G.A. Churchill. \u201cBayesian model selection reveals biological origins of zero inflation in single-cell transcriptomics.\u201d Genome Biology, 21, 183 (2020). \u003ca href=\"https://doi.org/10.1186/s13059-020-02103-2\" rel=\"nofollow\"\u003ehttps://doi.org/10.1186/s13059-020-02103-2\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 8,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1614372806.0
  },
  {
    "data_format": 2,
    "description": "Training examples for SYCL",
    "filenames": [
      "Singularity"
    ],
    "full_name": "kevin-harms/sycltrain",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-sycltrain\" class=\"anchor\" href=\"#sycltrain\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esycltrain\u003c/h1\u003e\n\u003cp\u003eThis work has been moved to the github repo:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/alcf-perfengr/sycltrain\"\u003ehttps://github.com/alcf-perfengr/sycltrain\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 8,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1613349330.0
  },
  {
    "data_format": 2,
    "description": "An automated pipeline for integrated preprocessing and quality assurance of diffusion weighted MRI images",
    "filenames": [
      "Singularity"
    ],
    "full_name": "MASILab/PreQual",
    "latest_release": "v1.0.5",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-prequal-dtiqa-v7-multi-user-guide\" class=\"anchor\" href=\"#prequal-dtiqa-v7-multi-user-guide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreQual (dtiQA v7 Multi) User Guide\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#overview\"\u003eOverview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#authors-and-reference\"\u003eAuthors and Reference\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#getting-started\"\u003eGetting Started\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#containerization-of-source-code\"\u003eContainerization of Source Code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#command\"\u003eCommand\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#arguments-and-io\"\u003eArguments and I/O\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#configuration-file\"\u003eConfiguration File\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#examples\"\u003eExamples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-bids-data\"\u003eRunning BIDS Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#options\"\u003eOptions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pipeline-assumptions\"\u003ePipeline Assumptions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pipeline-processing-steps\"\u003ePipeline Processing Steps\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pipeline-quality-assurance-steps\"\u003ePipeline Quality Assurance Steps\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#outputs\"\u003eOutputs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#note-on-versioning-for-vuiis-xnat-users\"\u003eNote on Versioning for VUIIS XNAT Users\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/MASILab/PreQual/blob/master/overview.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/MASILab/PreQual/raw/master/overview.png?raw=true\" alt=\"Pipeline Overview\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Perform integrated preprocessing and quality assurance of diffusion MRI data\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePreprocessing Steps:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMP-PCA denoising (default on)\u003c/li\u003e\n\u003cli\u003eGibbs de-ringing (default off)\u003c/li\u003e\n\u003cli\u003eRician correction (default off)\u003c/li\u003e\n\u003cli\u003eInter-scan normalization (default on)\u003c/li\u003e\n\u003cli\u003eSusceptibility-induced distortion correction, with or without reverse gradient images or field maps\u003c/li\u003e\n\u003cli\u003eEddy current-induced distortion correction\u003c/li\u003e\n\u003cli\u003eInter-volume motion correction\u003c/li\u003e\n\u003cli\u003eSlice-wise signal dropout imputation\u003c/li\u003e\n\u003cli\u003eN4 B1 bias field correction (default off)\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eQuality Assurance Steps:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eVerification of phase encoding schemes\u003c/li\u003e\n\u003cli\u003eAnalysis of gradient directions\u003c/li\u003e\n\u003cli\u003eShell-wise analysis of signal-to-noise and contrast-to-noise ratios\u003c/li\u003e\n\u003cli\u003eVisualization of Gibbs de-ringing changes (if applicable)\u003c/li\u003e\n\u003cli\u003eVisualization of within brain intensity distributions before and after Rician correction (if applicable)\u003c/li\u003e\n\u003cli\u003eCorrection (if applicable) or visualization of inter-scan intensity relationships\u003c/li\u003e\n\u003cli\u003eShell-wise analysis of distortion corrections\u003c/li\u003e\n\u003cli\u003eAnalysis of inter-volume motion and slice-wise signal dropout\u003c/li\u003e\n\u003cli\u003eAnalysis of B1 bias fields (if applicable)\u003c/li\u003e\n\u003cli\u003eVerification of intra-pipeline masking\u003c/li\u003e\n\u003cli\u003eAnalysis of tensor goodness-of-fit\u003c/li\u003e\n\u003cli\u003eVoxel-wise and region-wise quantification of FA\u003c/li\u003e\n\u003cli\u003eVoxel-wise quantification of MD\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors-and-reference\" class=\"anchor\" href=\"#authors-and-reference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors and Reference\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"mailto:leon.y.cai@vanderbilt.edu\"\u003eLeon Y. Cai\u003c/a\u003e, Qi Yang, Colin B. Hansen, Vishwesh Nath, Karthik Ramadass, Graham W. Johnson, Benjamin N. Conrad, Brian D. Boyd, John P. Begnoche, Lori L. Beason-Held, Andrea T. Shafer, Susan M. Resnick, Warren D. Taylor, Gavin R. Price, Victoria L. Morgan, Baxter P. Rogers, Kurt G. Schilling, Bennett A. Landman. \u003cem\u003ePreQual: An automated pipeline for integrated preprocessing and quality assurance of diffusion weighted MRI images\u003c/em\u003e. \u003ca href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.28678\" rel=\"nofollow\"\u003eMagnetic Resonance in Medicine\u003c/a\u003e, 2021.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://my.vanderbilt.edu/masi\" rel=\"nofollow\"\u003eMedical-image Analysis and Statistical Interpretation (MASI) Lab\u003c/a\u003e, Vanderbilt University, Nashville, TN, USA\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eThe PreQual software is designed to run inside a \u003ca href=\"#containerization-of-source-code\"\u003eSingularity container\u003c/a\u003e. The container requires an \"\u003ca href=\"#arguments-and-io\"\u003einputs\u003c/a\u003e\" folder that holds all required input diffusion image files (i.e., .nii.gz, .bval, and .bvec files) and a \u003ca href=\"#configuration-file\"\u003econfiguration file\u003c/a\u003e. For those running Synb0-DisCo to correct susceptibility distortions without reverse phase-encoded images, this folder will also contain the \u003ca href=\"#arguments-and-io\"\u003estructural T1 image\u003c/a\u003e. The container also requires an \"\u003ca href=\"#arguments-and-io\"\u003eoutputs\u003c/a\u003e\" folder that will hold all the outputs after the pipeline runs. We also need to know the image \u003cem\u003e\u003ca href=\"#arguments-and-io\"\u003eaxis\u003c/a\u003e\u003c/em\u003e on which phase encoding was performed for all inputs (i.e., \"i\" for the first dimension, \"j\" for the second). To build the configuration file, we need to know the \u003cem\u003e\u003ca href=\"#configuration-file\"\u003edirection\u003c/a\u003e\u003c/em\u003e along said axis in which each image was phase encoded (i.e., \"+\" for positive direction and \"-\" for the negative direction) and the \u003ca href=\"#configuration-file\"\u003ereadout time\u003c/a\u003e for each input image. Once we have this information, we bind the inputs and outputs directories into the container to \u003ca href=\"#command\"\u003erun the pipeline\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNote: The phase encoding axis, direction, and readout time must be known ahead of time, as this information is not stored in NIFTI headers. Depending on the scanner used, they may be available in JSON sidecars when NIFTIs are converted from DICOMs with \u003ca href=\"#pipeline-assumptions\"\u003edcm2niix\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-containerization-of-source-code\" class=\"anchor\" href=\"#containerization-of-source-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainerization of Source Code\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/MASILab/PreQual.git\ncd /path/to/repo/PreQual\ngit checkout v1.0.5\nsudo singularity build /path/to/prequal.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe use Singularity version 3.4 with root permissions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-command\" class=\"anchor\" href=\"#command\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run \n-e \n--contain\n-B /path/to/inputs/directory/:/INPUTS\n-B /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n--nv\n/path/to/prequal.simg\npe_axis\n[options]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eBinding the freesurfer license is optional and only needed for Synb0-DisCo\u003c/li\u003e\n\u003cli\u003eBinding the tmp directory is necessary when running the image with \u003ccode\u003e--contain\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--nv\u003c/code\u003e is optional. See options \u003ccode\u003e--eddy_cuda\u003c/code\u003e and \u003ccode\u003e--eddy_extra_args\u003c/code\u003e. \u003cstrong\u003eGPU support is currently experimental.\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-arguments-and-io\" class=\"anchor\" href=\"#arguments-and-io\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eArguments and I/O\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInput Directory:\u003c/strong\u003e The dtiQA_config.csv configuration file and at least one diffusion weighted image must be provided.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003edtiQA_config.csv (see \u003ca href=\"#configuration-file\"\u003ebelow\u003c/a\u003e for format, must be named exactly)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;.nii.gz (diffusion weighted image)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;.bval (units of s/mm\u003csup\u003e2\u003c/sup\u003e, in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;.bvec (normalized unit vectors in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;.nii.gz (diffusion weighted image)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;.bval (units of s/mm\u003csup\u003e2\u003c/sup\u003e, in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;.bvec (normalized unit vectors in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003et1.nii.gz (Optional, used for Synb0-DisCo, must be named exactly)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOther files as needed (see \u003ccode\u003e--extra_eddy_args\u003c/code\u003e for more information)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOutput Directory:\u003c/strong\u003e Full outputs listed at the \u003ca href=\"#outputs\"\u003eend\u003c/a\u003e of this document\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe output preprocessed images are available in the PREPROCESSED subfolder in the output directory:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePREPROCESSED/dwmri.nii.gz\u003c/li\u003e\n\u003cli\u003ePREPROCESSED/dwmri.bval\u003c/li\u003e\n\u003cli\u003ePREPROCESSED/dwmri.bvec\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe QA document is available in the PDF subfolder in the output directory:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePDF/dtiQA.pdf\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003epe_axis:\u003c/strong\u003e Phase encoding axis of all the input images. We do NOT support different phase encoding axes between different input images at this time. The options are i and j and correspond to the first and second dimension of the input images, respectively. Note that FSL does not currently support phase encoding in the third dimension (i.e. k, the dimension in which the image slices were acquired, commonly axial for RAS and LAS oriented images). \u003cstrong\u003eThis parameter is direction AGNOSTIC\u003c/strong\u003e. The phase encoding directions of the input images along this axis are specified in the dtiQA_config.csv file. See \u003ca href=\"#configuration-file\"\u003eConfiguration File\u003c/a\u003e and \u003ca href=\"#examples\"\u003eExamples\u003c/a\u003e for more information.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration-file\" class=\"anchor\" href=\"#configuration-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration File\u003c/h2\u003e\n\u003cp\u003eThe format for the lines of the configuration CSV file, dtiQA_config.csv (must be named exactly), are as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;image1\u0026gt;,pe_dir,readout_time\n:\n\u0026lt;imageN\u0026gt;,pe_dir,readout_time\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026lt;image\u0026gt;\u003c/strong\u003e is the shared file PREFIX between the corresponding NIFTI, BVAL, and BVEC files for that particular image in the input directory (i.e., my_dwi.nii.gz/.bval/.bvec -\u0026gt; my_dwi). Do NOT include the path to the input directory.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003epe_dir\u003c/strong\u003e is either + or -, corresponding to the direction along the phase encoding axis (as defined by the parameter \u003ccode\u003epe_axis\u003c/code\u003e) on which the image is phase encoded.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNote that a combination of phase encoding axis and direction map to specific anatomical (i.e. APA, APP, etc.) directions based on the orientation of the image. So, for instance in a RAS image, an axis of j and direction of + map to APP. We infer the orientation of the image from the header of the NIFTI using nibabel tools and output the best anatomical phase encoding direction interpretation of the input direction in the PDF for QA.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ereadout_time\u003c/strong\u003e is a non-negative number, the readout_time parameter required by FSL\u2019s eddy. The absolute value of this parameter is used to scale the estimated b0 field. Note a value of 0 indicates that the images are infinite bandwidth (i.e. no susceptibility distortion). See \u003ca href=\"#examples\"\u003eExamples\u003c/a\u003e for more information.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cp\u003eHere are some different example combinations of pe_axis, pe_dir, and readout_time parameters and the corresponding FSL acquisition parameters lines:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003epe_axis\u003c/th\u003e\n\u003cth\u003epe_dir\u003c/th\u003e\n\u003cth\u003ereadout_time\u003c/th\u003e\n\u003cth\u003eacqparams line\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ei\u003c/td\u003e\n\u003ctd\u003e+\u003c/td\u003e\n\u003ctd\u003e0.05\u003c/td\u003e\n\u003ctd\u003e1, 0, 0, 0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003ctd\u003e0.1\u003c/td\u003e\n\u003ctd\u003e0, -1, 0, 0.1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThese are examples of common use cases. They also all share the same command, as detailed above. The PREPROCESSED output folder will contain the final outputs and the PDF folder will contain the QA report.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003ePhase Encoding\u003cbr\u003eAxis\u003c/th\u003e\n\u003cth\u003eReverse Phase\u003cbr\u003eEncoded (RPE) Image\u003c/th\u003e\n\u003cth\u003eT1\u003cbr\u003eImage\u003c/th\u003e\n\u003cth\u003eContents of\u003cbr\u003eInput Directory\u003c/th\u003e\n\u003cth\u003eContents of\u003cbr\u003edtiQA_config.csv\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003edti1.nii.gz\u003cbr\u003edti1.bval\u003cbr\u003edti1.bvec\u003cbr\u003edti2.nii.gz\u003cbr\u003edti2.bval\u003cbr\u003edti2.bvec\u003cbr\u003erpe.nii.gz\u003cbr\u003erpe.bval\u003cbr\u003erpe.bvec\u003cbr\u003edtiQA_config.csv\u003c/td\u003e\n\u003ctd\u003edti1,+,0.05\u003cbr\u003edti2,+,0.05\u003cbr\u003erpe,-,0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003edti1.nii.gz\u003cbr\u003edti1.bval\u003cbr\u003edti1.bvec\u003cbr\u003edti2.nii.gz\u003cbr\u003edti2.bval\u003cbr\u003edti2.bvec\u003cbr\u003et1.nii.gz\u003cbr\u003edtiQA_config.csv\u003c/td\u003e\n\u003ctd\u003edti1,+,0.05\u003cbr\u003edti2,+,0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003edti1.nii.gz\u003cbr\u003edti1.bval\u003cbr\u003edti1.bvec\u003cbr\u003edti2.nii.gz\u003cbr\u003edti2.bval\u003cbr\u003edti2.bvec\u003cbr\u003edtiQA_config.csv\u003c/td\u003e\n\u003ctd\u003edti1,+,0.05\u003cbr\u003edti2,+,0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-bids-data\" class=\"anchor\" href=\"#running-bids-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning BIDS Data\u003c/h2\u003e\n\u003cp\u003eWhile not a BIDS pipeline, data in BIDS format can be run with PreQual without moving or copying data. The key is that the input directory structure must be as described relative to \u003cem\u003einside the container\u003c/em\u003e. By creatively binding files/folders into the container, we can achieve the same effect:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e-B /path/to/sub-X/ses-X/dwi/:/INPUTS\n-B /path/to/sub-X/ses-X/anat/sub-X_ses-X_T1w.nii.gz:/INPUTS/t1.nii.gz (optional, Synb0-DisCo only)\n-B /path/to/config/file.csv:/INPUTS/dtiQA_config.csv\n-B /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe outputs directory and configuration file can be created wherever makes the most sense for the user. The contents of the configuration file will look something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esub-X_ses-X_acq-1_dwi,pe_dir,readout_time\n:\nsub-X_ses-X_acq-N_dwi,pe_dir,readout_time\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptions\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e--bval_threshold N\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA non-negative integer threshold under which to consider a b-value to be zero. Useful when some MRI machines do not allow for more than one b0 volume to be acquired so some users acquire scans with extremely low b-values to be treated like b0 volumes. Setting this value to 0 results in no thresholding. Units = s/mm\u003csup\u003e2\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eDefault = 50\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--nonzero_shells s1,s2,...,sn/auto\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA comma separated list of positive integers (s/mm\u003csup\u003e2\u003c/sup\u003e) indicating nonzero shells for SNR/CNR analysis when there are more unique b-values than shells determined by eddy or automatically determine shells by rounding to nearest 100. Useful when b-values are modulated around a shell value instead of set exactly at that value. Only used when determining shells for SNR/CNR analysis. Original b-values used elsewhere in pipeline.\u003c/p\u003e\n\u003cp\u003eDefault = auto\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--denoise on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDenoise images prior to preprocessing using Marchenko-Pastur PCA \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwidenoise.html\" rel=\"nofollow\"\u003eimplemented in MRTrix3\u003c/a\u003e. The SNR of the b0s of the final preprocessed images are reported in the PDF output regardless of whether this option is on or off.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--degibbs on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRemove Gibbs ringing artifacts using the local subvoxel-shifts method as \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/mrdegibbs.html\" rel=\"nofollow\"\u003eimplemented in MRTrix3\u003c/a\u003e. We caution against using this feature as it not designed for the partial Fourier schemes with which most echo planar diffusion images are acquired. It is also difficult to quality check, but we include a visualization of averaged residuals across all b = 0 s/mm\u003csup\u003e2\u003c/sup\u003e volumes, looking for larger signals near high contrast (i.e. parenchyma-CSF) interfaces.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--rician on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePerform Rician correction using the method of moments. We normally do not perform this step as we empirically do not find it to affect results drastically. It is also difficult to quality check, but we include a plot of the shell-wise within brain intensity distributions for each input before and after correction, looking for a slight drop in intensity with correction.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--prenormalize on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIntensity normalize images prior to preprocessing by maximizing the intra-mask intensity-histogram intersections between the averaged b0s of the scans. If this option is on, these histograms before and after prenormalization will be reported in the output PDF. This is done to avoid gain differences between different diffusion scans. If this option is off, we assume that the various input images all have the same gain. That being said, we still estimate and report the gain factors and intensity histograms in a gain QA page and report warnings if estimated gains greater than 5% are found.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--synb0 on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003etopup\u003c/code\u003e with a synthetic b0 generated with the Synb0-DisCo deep-learning framework if no reverse phase encoded images are supplied and a T1 image is supplied. Synb0-DisCo requires at least 24GB of RAM.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--topup_first_b0s_only\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003etopup\u003c/code\u003e with only the first b0 from each input image. At the time of writing, \u003cstrong\u003eFSL\u0027s topup cannot be parallelized\u003c/strong\u003e, and the runtime of topup can increase dramatically as more b0 volumes are included. This flag allows for faster processing at the expense of information lost from any interleaved b0s.\u003c/p\u003e\n\u003cp\u003eDefault = use ALL b0s\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--extra_topup_args=\"string\u201d\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eExtra arguments to pass to FSL\u2019s \u003ccode\u003etopup\u003c/code\u003e. \u003ccode\u003eTopup\u003c/code\u003e will run with the following by default (as listed in the \u003ccode\u003e/SUPPLEMENTAL/topup.cnf\u003c/code\u003e configuration file) but will be overwritten by arguments passed to \u003ccode\u003e--extra_topup_args\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Resolution (knot-spacing) of warps in mm\n--warpres=20,16,14,12,10,6,4,4,4\n# Subsampling level (a value of 2 indicates that a 2x2x2 neighbourhood is collapsed to 1 voxel)\n--subsamp=1,1,1,1,1,1,1,1,1\n# FWHM of gaussian smoothing\n--fwhm=8,6,4,3,3,2,1,0,0\n# Maximum number of iterations\n--miter=10,10,10,10,10,20,20,30,30\n# Relative weight of regularisation\n--lambda=0.00033,0.000067,0.0000067,0.000001,0.00000033,0.000000033,0.0000000033,0.000000000033,0.00000000000067\n# If set to 1 lambda is multiplied by the current average squared difference\n--ssqlambda=1\n# Regularisation model\n--regmod=bending_energy\n# If set to 1 movements are estimated along with the field\n--estmov=1,1,1,1,1,0,0,0,0\n# 0=Levenberg-Marquardt, 1=Scaled Conjugate Gradient\n--minmet=0,0,0,0,0,1,1,1,1\n# Quadratic or cubic splines\n--splineorder=3\n# Precision for calculation and storage of Hessian\n--numprec=double\n# Linear or spline interpolation\n--interp=spline\n# If set to 1 the images are individually scaled to a common mean intensity \n--scale=0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor \u003ccode\u003etopup\u003c/code\u003e options that require additional inputs, place the file in the inputs directory and use the following syntax: \u003ccode\u003e--\u0026lt;myinputoption\u0026gt; /INPUTS/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. For \u003ccode\u003etopup\u003c/code\u003e options that produce additional outputs, the file will save in the output directory under the \u201cTOPUP\u201d folder by using the following syntax: \u003ccode\u003e--\u0026lt;myoutputoption\u0026gt; /OUTPUTS/TOPUP/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. Note that in this case \u003ccode\u003e/INPUTS\u003c/code\u003e and \u003ccode\u003e/OUTPUTS\u003c/code\u003e should be named exactly as is and are NOT the path to the input and output directory on your file system.\u003c/p\u003e\n\u003cp\u003eDefault = none\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--eddy_cuda 8.0/9.1/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun FSL\u2019s \u003ccode\u003eeddy\u003c/code\u003e with NVIDIA GPU acceleration. If this parameter is 8.0 or 9.1, either CUDA 8.0 or 9.1 must be installed and properly configured on your system, respectively, and the \u003ccode\u003e--nv\u003c/code\u003e flag must be run in the singularity command. If this parameter is off, \u003ccode\u003eeddy\u003c/code\u003e is run with OPENMP CPU multithreading. See \u003ccode\u003e--num_threads\u003c/code\u003e for more information. CUDA is required to run \u003ccode\u003eeddy\u003c/code\u003e with \u003ccode\u003e--mporder\u003c/code\u003e (intra-volume slice-wise motion correction). See \u003ccode\u003e--extra_eddy_args\u003c/code\u003e for more information.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--eddy_mask on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003eeddy\u003c/code\u003e with or without a brain mask. If on, FSL\u2019s brain extraction tool (\u003ccode\u003ebet\u003c/code\u003e) is used with a low threshold to create a rough brain mask for \u003ccode\u003eeddy\u003c/code\u003e. This can sometimes produce poor results. If off, no mask is used and produces empirically minor differences in results than when a mask is used. If this option is on, the contour of this mask is drawn in the PDF.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--eddy_bval_scale N/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003eeddy\u003c/code\u003e with b-values scaled by the positive number N. All other steps of the pipeline use the original b-values. This can help \u003ccode\u003eeddy\u003c/code\u003e finish distortion correction when extremely low b-values (\u0026lt;200) are involved. If off, no scaling of b-values is used.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--extra_eddy_args=\"string\u201d\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eExtra arguments to pass to FSL\u2019s \u003ccode\u003eeddy\u003c/code\u003e. \u003ccode\u003eEddy\u003c/code\u003e will always run with the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--repol\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that if \u003ccode\u003e--mporder\u003c/code\u003e is passed here, \u003ccode\u003e--eddy_cuda\u003c/code\u003e must be 8.0 or 9.1 and the singularity option \u003ccode\u003e--nv\u003c/code\u003e must be passed into the container, as intra-volume slice-wise motion correction requires GPU acceleration.\u003c/p\u003e\n\u003cp\u003eFor \u003ccode\u003eeddy\u003c/code\u003e options that require additional inputs, place the file in the inputs directory and use the following syntax: \u003ccode\u003e--\u0026lt;myinputoption\u0026gt; /INPUTS/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. For \u003ccode\u003eeddy\u003c/code\u003e options that produce additional outputs, the file will save in the output directory under the \u201cEDDY\u201d folder by using the following syntax: \u003ccode\u003e--\u0026lt;myoutputoption\u0026gt; /OUTPUTS/EDDY/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. Note that in this case \u003ccode\u003e/INPUTS\u003c/code\u003e and \u003ccode\u003e/OUTPUTS\u003c/code\u003e should be named exactly as is and are NOT the path to the input and output directory on your file system.\u003c/p\u003e\n\u003cp\u003eDefault = none\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--postnormalize on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIntensity normalize images after preprocessing by maximizing the intra-mask intensity-histogram intersections between the averaged b0s of the scans. If this option is on, these histograms before and after postnormalization will be reported in the output PDF.\u003c/p\u003e\n\u003cp\u003eNote: This option was intended for testing and is left for posterity. It is not recommended at this time and will be deprecated.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--correct_bias on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePerform \u003ca href=\"https://manpages.debian.org/testing/ants/N4BiasFieldCorrection.1.en.html\" rel=\"nofollow\"\u003eANTs N4 bias field correction\u003c/a\u003e as \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwibiascorrect.html\" rel=\"nofollow\"\u003ecalled in MRTrix3\u003c/a\u003e. If this option is on, the calculated bias field will be visualized in the output PDF.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--glyph_type tensor/vector\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eVisualize either tensors or principal eigenvectors in the QA document.\u003c/p\u003e\n\u003cp\u003eDefault = tensor\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--atlas_reg_type FA/b0\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePerform JHU white matter atlas registration to the subject by either deformably registering the subject\u0027s FA map or average b0 to the MNI FA or T2 template, respectively. Empirically, the FA approach tends to be more accurate for white matter whereas the b0 approach tends to be more accurate globally. The b0 approach is more robust for acquisitions with low shells (i.e., b \u0026lt; 500 s/mm\u003csup\u003e2\u003c/sup\u003e) or poor masking that result in the inclusion of a lot of facial structure.\u003c/p\u003e\n\u003cp\u003eDefault = FA\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--split_outputs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSplit the fully preprocessed output (a concatenation of the input images) back into their component parts and do NOT keep the concatenated preprocessed output.\u003c/p\u003e\n\u003cp\u003eDefault = Do NOT split and return only the concatenated output\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--keep_intermediates\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eKeep intermediate copies of diffusion data (i.e. denoised, prenormalized, bias-corrected, etc.) used to generate final preprocessed data. Using this flag may result in a large consumption of hard disk space.\u003c/p\u003e\n\u003cp\u003eNote: Due to space concerns, special permission needed to use this option on XNAT.\u003c/p\u003e\n\u003cp\u003eDefault = do NOT keep intermediates\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--num_threads N\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA positive integer indicating the number of threads to use when running portions of the pipeline that can be multithreaded (i.e. MRTrix3, ANTs, and FSL\u2019s eddy without GPU acceleration). Please note that at the time of writing, \u003cstrong\u003eFSL\u0027s topup cannot be parallelized\u003c/strong\u003e, and that the runtime of topup can increase dramatically as more b0 volumes are included. See \u003ccode\u003e--topup_first_b0s_only\u003c/code\u003e for more information.\u003c/p\u003e\n\u003cp\u003eNote: Due to resource concerns, special permission needed to multi-thread on XNAT.\u003c/p\u003e\n\u003cp\u003eDefault = 1 (do NOT multithread)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--project string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eString describing project in which the input data belong to label PDF output\u003c/p\u003e\n\u003cp\u003eDefault = proj\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--subject string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eString describing subject from which the input data were acquired to label PDF output\u003c/p\u003e\n\u003cp\u003eDefault = subj\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--session string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eString describing session in which the input data were acquired to label PDF output\u003c/p\u003e\n\u003cp\u003eDefault = sess\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--help, -h\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-assumptions\" class=\"anchor\" href=\"#pipeline-assumptions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Assumptions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAll NIFTI images are consistent with a conversion from a DICOM using \u003ccode\u003edcm2niix\u003c/code\u003e (\u003ca href=\"https://github.com/rordenlab/dcm2niix/releases/tag/v1.0.20180622\"\u003eat least v1.0.20180622\u003c/a\u003e) by Chris Rorden and are raw NIFTIs without distortion correction. We require this as dcm2niix exports b-value/b-vector files in FSL format and removes ADC or trace images auto-generated in some Philips DICOMs. In addition \u003ccode\u003edcm2niix\u003c/code\u003e correctly moves the gradients from scanner to subject space and does not re-order volumes, both of which can cause spurious results or pipeline failure.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWe expect raw volumes only, no ADC or trace volumes.\u003c/strong\u003e ADC volumes are sometimes encoded as having a b-value greater than 0 with a corresponding b-vector of (0,0,0) and trace volumes are sometimes encoded as having a b-value of 0 with a corresponding non-unit normalized b-vector, as in the case of some Philips PARREC converters. We check for these cases, remove the affected volumes, and report a warning in the console and in the PDF.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe cannot, unfortunately, account for failure of reorientation of gradients into subject space. Visualization of tensor glyphs or principal eigenvectors can be helpful in distinguishing this. However, this error can be subtle so we suggest proper DICOM to NIFTI conversion with the above release of \u003ccode\u003edcm2niix\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImages will be processed in the order they are listed in dtiQA_config.csv.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe size of all the volumes across all images must all be the same.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe location of b0 images inside the input images do not matter.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs per the FSL format, we do not support non-unit normalized gradients. We also do not support gradient directions of 0,0,0 when the corresponding b-value is non-zero. Gradients with the latter configurations may cause pipeline failure. We report warnings in the output PDF if we identify these.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe phase encoding axis of all volumes across all images is the same.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe phase encoding direction along the axis is the same for all volumes inside an image and is specified in the dtiQA_config.csv file.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUnless \u003ccode\u003e--prenormalize\u003c/code\u003e is on, we assume all input images have the same gain.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe will preferentially preprocess images with FSL\u2019s topup using available images with complementary phase encoding directions (i.e. + and -, \"reverse phase encodings\"). If none are available and a T1 is available, we will synthesize a susceptibility-corrected b0 from the first image listed in dtiQA_config.csv with Synb0-DisCo for use with topup, unless the user turns the \u003ccode\u003e--synb0\u003c/code\u003e parameter off. The readout time of this synthetic b0 will be zero and the phase encoding direction will be equal to that of the first image in dtiQA_config.csv. Otherwise, we will preprocess without topup and move straight to FSL\u2019s eddy.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe use topup and eddy for preprocessing, both of which at the present moment do NOT officially support DSI acquisitions but only single- and multi-shell. We will force topup and eddy to run on DSI data, but may not produce quality results. Please carefully check the PDF output as we report a warning if eddy detected non-shelled data and thus required the use of the force flag.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNote that eddy may erroneously detect data as non-shelled if there are fewer directions in one of the shells than others. Because we merge the images for preprocessing, a notable example of this is when a reverse-phase encoded image uses a different shell than the forward images and has significantly fewer directions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor preprocessing, eddy will motion correct to the first b0 of each image.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMRTrix3 by default preferentially uses the qform for understanding NIFTI orientations. Nibabel uses the sform. We set MRTrix3 to use the sform in our pipeline, and thus we preferentially use the sform when the two don\u2019t match.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNo b0 drift correction is performed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe use the fit tensor model primarily for QA. If b-values less than 500 s/mm\u003csup\u003e2\u003c/sup\u003e or greater than 1500 s/mm\u003csup\u003e2\u003c/sup\u003e are present, we suggest careful review of the fit prior to use for non-QA purposes.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-processing-steps\" class=\"anchor\" href=\"#pipeline-processing-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Processing Steps\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThreshold all b-values such that values less than the \u003ccode\u003e--bval_threshold\u003c/code\u003e parameter are 0.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck that all b-vectors are unit normalized and all b-values greater than zero have associated non-zero b-vectors. For any volumes where this is not the case, we remove them, flag a warning for the output PDF, and continue the pipeline.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, denoise all diffusion scans with \u003ccode\u003edwidenoise\u003c/code\u003e (Marchenko-Pastur PCA) from MrTrix3 and save the noise profiles (needed for Rician correction later).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, perform Gibbs de-ringing on all diffusion scans with \u003ccode\u003emrdegibbs\u003c/code\u003e from MRTrix3.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, perform Rician correction on all diffusion scans with the method of moments.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, prenormalize all diffusion scans. To accomplish this, extract all b0 images from each diffusion scan and average them. Then find a rough brain-mask with FSL\u2019s bet and calculate an intensity scale factor such that the histogram intersection between the intra-mask histogram of the different scans\u2019 averaged b0s to that of the first scan is maximized. Apply this scale factor to the entire diffusion weighted scan. This is done to avoid gain differences between different diffusion scans.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf prenormalization is not indicated, we still run the prenormalization algorithms to calculate rough gain differences and report the gain factors and intensity histograms in a gain QA page. The outputs of the algorithms, however, are NOT propagated through to the rest of the pipeline.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePrepare data for and run preprocessing with topup and eddy\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eTopup:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eExtract all b0s from all scans, maintaining their relative order.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e(Optional) If a T1 is supplied and no complementary (i.e. reverse) phase encoded images are provided, use Synb0-DisCo to convert the first b0 of the first scan to a susceptibility-corrected b0.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the acquisition parameters file required by both topup and eddy\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFor the number of b0s from each image, add the same phase encoding and readout time line to the acquisition parameters file, as outlined in \"Example Phase Encoding Schemes\".\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExample: In the case where we have a phase encoding axis of j and two images, one with 7 b0s, + direction, and 0.05 readout time and one with 3 b0s, - direction, and 0.02 readout time, this file will have 10 lines. The first 7 lines are identical and equal to [0, 1, 0, 0.05]. The last three lines are also identical and equal to [0, -1, 0, 0.02].\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e(Optional) If Synb0-DisCo is run because no complementary phase encoding directions are supplied and --synb0 is not off, we add an additional line to the end of the file. This line is the same as the first line of the file except that the readout time is 0 instead.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExample: In the case where we have a phase encoding axis of j and two images, one with 7 b0s, + direction, and 0.05 readout time and one with 3 b0s, + direction, and 0.02 readout time, this file will have 11 lines. The first 7 lines are identical and equal to [0, 1, 0, 0.05]. The next three lines are also identical and equal to [0, 1, 0, 0.02]. Finally, the last line is equal to [0, 1, 0, 0].\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then concatenate all the b0s maintaining their order and run topup with the acquisition parameters file if images with complementary phase encoding directions are supplied or if a T1 was supplied. Otherwise, we move on to the next step, eddy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEddy\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUsing the acquisition parameters file from the topup step, regardless of whether topup was performed, we build the eddy index file such that each volume in each image corresponds to the line in the acquisition parameters file associated with the first b0 of each scan. This is done to tell eddy that each volume in a given scan has the same underlying phase encoding scheme as the first b0 of that scan.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExample: In the case where we have two images, one with 7 b0s and 100 total volumes and one with 3 b0s and 10 total volumes, the eddy index file has 100 1\u2019s followed by 10 8\u2019s.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEddy is then run with either a mask generated with bet and the -f 0.25 and -R options or without a mask (aka with a mask of all 1\u2019s), depending on user input (see the --eddy_mask option) and with the output of topup if topup was run. Eddy also runs with the --repol option for outlier slice replacement. We also first run eddy with a check looking for shelled data. If the check fails, eddy is then run with the --data_is_shelled flag to force eddy to run on all scans, DSI included. Note that DSI data is not officially supported by FSL\u2026 yet?\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eIf eddy detects data is not shelled, we report this as a warning\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs noted in the assumptions section above, eddy may erroneously detect data as non-shelled if there are fewer directions in one of the shells than others. Because we merge the images for preprocessing, a notable example of this is when a reverse-phase encoded image uses a different shell than the forward images and has significantly fewer directions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEddy also performs bvec rotation correction and calculates the voxel-wise signal-to-noise ratios of the b0 images and the voxel-wise contrast-to-noise ratios for the diffusion weighted images. SNR is defined as the mean value divided by the standard deviation. CNR is defined as the standard deviation of the Gaussian Process predictions (GP) divided by the standard deviation of the residuals between the measured data and the GP predictions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf the user chooses to, we then perform post-normalization in the same fashion as pre-normalization.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf the user chooses to, we then wrap up preprocessing with an N4 bias field correction as implemented in ANTs via MRTrix3\u2019s dwibiascorrect.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe generate a brain mask using FSL\u2019s bet2 with the following options:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-f 0.25 -R\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then apply the mask to the preprocessed images while we calculate tensors using MRTrix3\u2019s dwi2tensor function. For visualization we discard tensors that have diagonal elements greater than 3 times the apparent diffusion coefficient of water at 37\u00b0C (~0.01).\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe also reconstruct the preprocessed image from the tensor fit for further analysis later. dwi2tensor does this for us.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then convert the tensor to FA and MD images (and visualize them later too) as well as AD, RD, and V1 eigenvector images for the user. The latter 3 are not visualized.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-quality-assurance-steps\" class=\"anchor\" href=\"#pipeline-quality-assurance-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Quality Assurance Steps\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWe start with the brain mask generated above to generate a mask used for the following quantification of tensor fit using a chi-squared statistic.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFirst, we calculate the mean image for each unique b-value (0 not included). Then we run FSL\u2019s FAST to isolate the CSF on each meaned image. We then take the average probability of a voxel being CSF across all unique b-values and assign \u0026gt;15% probability to be a positive CSF voxel.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThen we call the final chi-squared mask to be the intersection of the inverted CSF mask and a 1-pixel eroded version of the brain mask.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOn the voxels inside the chi-squared mask, we perform the following quality assurance:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWe perform a chi-squared analysis for each slice for each volume in the main image by calculating the ratio between the sum-squared error of the fit and the sum-squared intensities of the slice.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe extract the average FA for a number of white matter ROIs defined by the Hopkins atlas. We do this by non-rigidly registering the atlas to our FA output and extracting the FA values contained in each ROI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe check the gradients output by eddy (i.e. the preprocessed gradients) with \u003ca href=\"https://mrtrix.readthedocs.io/en/3.0.0/reference/commands/dwigradcheck.html\" rel=\"nofollow\"\u003edwigradcheck from MRTrix3\u003c/a\u003e. This performs tractography and finds the optimal sign and order permutation of the b-vectors such that the average tract length in the brain is most physiological.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThese optimized gradients are saved in the OPTIMIZED_BVECS output folder, and the gradients output by eddy in the PREPROCESSED folder are NOT overwritten.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe original, preprocessed, and preprocessed + optimized gradients are visualized as outlined below.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize the entire pipeline.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eOn the first page we describe the methods used for that run of the pipeline (what inputs were provided, what sort of preprocessing happened, etc.).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize the raw images with the interpreted phase encoding schemes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Gibbs de-ringing was run, we visualize central slices of the averaged residuals across b0 volumes before and after Gibbs de-ringing, looking for large residuals near high contrast interfaces (i.e. parenchyma against CSF)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Rician correction was performed, we visualize the within brain intensity distributions of each shell of each image before and after correction, looking for downward shifts after correction.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Synb0-DisCo was run, we then visualize the distorted b0 (first b0 of first scan) and T1 used as inputs as well as the output susceptibility corrected b0 in their native space.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf pre- or post-normalization was performed, we then visualize the intra-mask histograms before and after these steps as well as the calculated scaling factors. If pre-normalization is not performed, we visualize the histograms that would have been generated with pre-normalization ONLY as a check for gain differences.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize the first b0 of the images before and after preprocessing with the contours of the brain and stats masks overlaid as well as the contours of the eddy mask overlaid if it is used.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe plot the motion and angle correction done by eddy as well as the RMS displacement and median intensity for each volume and the volume\u2019s associated b-value. These values are read in from an eddy output text file and we also compute and save the average of these values. In addition, we plot the outlier slices removed and then imputed by eddy as well as the chi-squared fit, with maximal bounds 0 to 0.2. The median chi-squared values are shown across volumes and slices.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then plot the original raw b-vectors scaled by their b-values, the preprocessed ones output by eddy, and the optimized ones determined by \u003ccode\u003edwigradcheck\u003c/code\u003e applied to the preprocessed ones.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf bias field correction was performed, we then visualize the calculated fields.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize some central slices of the average volumes for all unique b-values, including b = 0 and report the median intra-mask SNR or CNR calculated by eddy as appropriate. If there are more unique b-values than shells deteremined by eddy, we round the b-values to the nearest 100 by default to assign volumes to shells or we choose the nearest shell indicated by the user (see \u003ccode\u003e--nonzero_shells\u003c/code\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe visualize the tensors (or principal eigenvectors depending on \u003ccode\u003e--glyph_type\u003c/code\u003e) using MRTrix3\u2019s mrview, omitting the tensors with negative eigenvalues or eigenvalues greater than 3 times the ADC of water at 37\u00b0C.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize some central slices of the FA map clipped from 0 to 1 as well as the average FA for the Hopkins ROIs and the quality of the atlas registration.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLastly, we visualize some central slices of the MD map clipped from 0 to 0.003 (ADC of water at 37\u00b0C).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cp\u003e\u0026lt;imageN_%\u0026gt; denotes the original prefix of imageN with the preceding preprocessing step descriptors tacked on the end. For example, in the case of the PRENORMALIZED directory, the prefix for imageJ may or may not include \"_denoised\" depending on whether the denoising step was run.\u003c/p\u003e\n\u003cp\u003eFolders and files in \u003cstrong\u003ebold\u003c/strong\u003e are always included.\u003c/p\u003e\n\u003cp\u003eFolders and files in \u003cem\u003eitalics\u003c/em\u003e are removed if \u003ccode\u003e--keep_intermediates\u003c/code\u003e is NOT indicated\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTHRESHOLDED_BVALS\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026lt;image1\u0026gt;.bval\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026lt;imageN\u0026gt;.bval\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eCHECKED\u003c/em\u003e (these contain the volumes that have passed the bval/bvec checks)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1\u0026gt;_checked.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1\u0026gt;_checked.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1\u0026gt;_checked.bvec\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN\u0026gt;_checked.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN\u0026gt;_checked.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN\u0026gt;_checked.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eDENOISED\u003c/em\u003e (these files are only created if \u003ccode\u003e--denoise\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_denoised.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_noise.nii.gz\u003c/em\u003e (needed for Rician correction)\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_denoised.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_noise.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eDEGIBBS\u003c/em\u003e (these files are only created if \u003ccode\u003e--degibbs\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_degibbs.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_degibbs.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eRICIAN\u003c/em\u003e (these files are only created if \u003ccode\u003e--rician\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_rician.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_rician.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003ePRENORMALIZED\u003c/em\u003e (these files are only created if \u003ccode\u003e--prenormalize\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eGAIN_CHECK\u003c/em\u003e (these files are only created if \u003ccode\u003e--prenormalize\u003c/code\u003e is off)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTOPUP\u003c/strong\u003e (these files are only created if \u003ccode\u003etopup\u003c/code\u003e was run)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eacqparams.txt (same as OUTPUTS/EDDY/acqparams.txt)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_b0_first.nii.gz\u003c/em\u003e (only if Synb0-DisCo is run)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eb0_syn.nii.gz (only if Synb0-DisCo is run)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_b0_all.nii.gz\u003c/em\u003e or \u003cem\u003epreproc_input_b0_all_smooth_with_b0_syn.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_b0_all_topped_up.nii.gz\u003c/em\u003e or \u003cem\u003epreproc_input_b0_all_smooth_with_b0_syn_topped_up.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003epreproc_input_b0_all.topup_log or preproc_input_b0_all_smooth_with_b0_syn.topup_log\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etopup_field.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etopup_results_fieldcoef.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etopup_results_movpar.txt\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEDDY\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eacqparams.txt\u003c/strong\u003e (same as OUTPUTS/TOPUP/acqparams.txt)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eindex.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_eddyed.nii.gz\u003c/em\u003e (renamed from \"eddy_results.nii.gz\")\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_eddyed.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_eddyed.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eeddy_mask.nii.gz (only included if \u003ccode\u003e--eddy_mask\u003c/code\u003e is on)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_command_txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_movement_rms\u003c/strong\u003e (describes volume-wise RMS displacement)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_free_data.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_map\u003c/strong\u003e (describes which slices were deemed outliers)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_n_sqr_stdev_map\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_n_stdev_map\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_report\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_parameters\u003c/strong\u003e (describes volume-wise rotation and translation)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_post_eddy_shell_alignment_parameters\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_post_eddy_shell_PE_translation_parameters\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_restricted_movement_rms\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_rotated_bvecs (describes properly rotated b-vectors)\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_values_of_all_input_parameters\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_cnr_maps.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003ePOSTNORMALIZED\u003c/em\u003e (these files are only created if \u003ccode\u003e--postnormalize\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_topup_eddy_norm.nii.gz\u003c/em\u003e (\"_topup\" only applies if topup was run)\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_topup_eddy_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eUNBIASED\u003c/em\u003e (these files are only created if \u003ccode\u003e--correct_bias\u003c/code\u003e is on; this folder is removed if \u003ccode\u003e--correct_bias\u003c/code\u003e is off)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003enormed_unbiased.nii.gz\u003c/em\u003e (if postnormalization is run) or \u003cem\u003epreproc_input_eddyed_unbiased.nii.gz\u003c/em\u003e (if postnormalization is not run)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ebias_field.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePREPROCESSED\u003c/strong\u003e (these represent the final output of the pipeline)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri.nii.gz\u003c/em\u003e (dwmri* files deleted only if \u003ccode\u003e--split_outputs\u003c/code\u003e is also set)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;_preproc.nii.gz (*_preproc files exist only if \u003ccode\u003e--split_outputs\u003c/code\u003e is set)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;_preproc.bval\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;_preproc.bvec\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;_preproc.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;_preproc.bval\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;_preproc.bvec\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003emask.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTENSOR\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri_recon.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSCALARS\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_fa.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_md.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_ad.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_rd.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_v1.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSTATS\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eatlas2subj.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eb02template_0GenericAffine.mat\u003c/strong\u003e or \u003cstrong\u003efa2template_0GenericAffine.mat\u003c/strong\u003e depending on \u003ccode\u003e--atlas_reg_type\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eb02template_1Warp.nii.gz\u003c/strong\u003e or \u003cstrong\u003efa2template_1Warp.nii.gz\u003c/strong\u003e depending on \u003ccode\u003e--atlas_reg_type\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eb02template_1InverseWarp.nii.gz\u003c/strong\u003e or \u003cstrong\u003efa2template_1InverseWarp.nii.gz\u003c/strong\u003e depending on \u003ccode\u003e--atlas_reg_type\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003echisq_mask.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003echisq_matrix.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_abs_displacement.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_median_cnr.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_rel_displacement.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_rotations.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_translations.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eroi_avg_fa.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003estats.csv\u003c/strong\u003e (contains summary of all motion, SNR/CNR, and average FA stats)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOPTIMIZED_BVECS\u003c/strong\u003e (these are sign/axis permuted per \u003ccode\u003edwigradcheck\u003c/code\u003e and are only used for QA purposes)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri.bval\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri.bvec\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePDF\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003edtiQA.pdf\u003c/strong\u003e (final QA document)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-note-on-versioning-for-vuiis-xnat-users\" class=\"anchor\" href=\"#note-on-versioning-for-vuiis-xnat-users\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNote on Versioning for VUIIS XNAT Users\u003c/h2\u003e\n\u003cp\u003ePreQual was developed at Vanderbilt under the project name \"dtiQA v7 Multi\". PreQual v1.0.0 represents dtiQA v7.2.0. Thus, on XNAT, dtiQA v7.2.x refers to PreQual v1.0.x.\u003c/p\u003e\n",
    "stargazers_count": 9,
    "subscribers_count": 0,
    "topics": [
      "diffusion",
      "mri",
      "preprocessing",
      "quality",
      "assurance"
    ],
    "updated_at": 1621986378.0
  },
  {
    "data_format": 2,
    "description": "Astronomical Calibration and Imaging Software",
    "filenames": [
      "deploy/singularity/Singularity.openmpi"
    ],
    "full_name": "ATNF/yandasoft",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-yandasoft\" class=\"anchor\" href=\"#yandasoft\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eYandaSoft\u003c/h1\u003e\n\u003cp\u003eYandasoft is a suite of applications and software developed by CSIRO for the calibration and imaging of Interferometric Radio Telescope data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eA git submodule for the full calibration and imaging documentation is included\nin the docs sibdirectory. On an initial clone of this repository you have to\nrun, \u003ccode\u003egit submodule init\u003c/code\u003e and \u003ccode\u003egit submodule update\u003c/code\u003e to obtain the latest\nversions. Also you must have the sphinx document tool installed and run \u003ccode\u003emake html\u003c/code\u003e to generate the documentation.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-in-this-version\" class=\"anchor\" href=\"#in-this-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIn this version\u003c/h2\u003e\n\u003cp\u003eThis release of the software is the first and consequently \u003cem\u003ebeta\u003c/em\u003e release. It contains the (at least) the following applications:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-measurement-set-creation-and-manipulation\" class=\"anchor\" href=\"#measurement-set-creation-and-manipulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMeasurement Set creation and manipulation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ecsimulator\u003c/code\u003e: simulation of visibilities.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eccontsubtract\u003c/code\u003e: continuum subtraction.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emslist\u003c/code\u003e: measurement set interogation.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emsconcat\u003c/code\u003e: concatenation of measurement sets.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emsmerge\u003c/code\u003e: merging of measurement sets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-calibration-tools\" class=\"anchor\" href=\"#calibration-tools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCalibration tools\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ecbpcalibrator\u003c/code\u003e: bandpass calibrator.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eccalibrator\u003c/code\u003e: for performing gain calibration\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eccalapply\u003c/code\u003e: for the application of calibration solutions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-imaging-tasks\" class=\"anchor\" href=\"#imaging-tasks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImaging tasks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ecimager\u003c/code\u003e: Original ASKAP imager.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eimager\u003c/code\u003e: New imager - permits more parallisation options.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003elinmos\u003c/code\u003e: Linear mosaicking of images\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pipeline-and-analysis-tasks\" class=\"anchor\" href=\"#pipeline-and-analysis-tasks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline and Analysis tasks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003emsplit\u003c/code\u003e: Manipulate measurement sets\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecmodel\u003c/code\u003e: Generate model images from component lists\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eselavy\u003c/code\u003e: Source detection tools\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-yandasoft\" class=\"anchor\" href=\"#about-yandasoft\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout Yandasoft\u003c/h2\u003e\n\u003cp\u003eThese tasks were originally developed to form the real-time calibration and imaging pipeline for the ASKAP telescope. In order to distribute this software more widely we have extracted these tools from the main codebase of the telescope system and distributed it separately.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h3\u003e\n\u003cp\u003eThe dependencies are listed in detail in the INSTALL.txt file. But it should be noted that there are internal \"ASKAP\" dependencies that are required. They are all public and are automatically pulled from their respective repositories by the included \u003ccode\u003ebuild_all.sh\u003c/code\u003e script.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to-get-it\" class=\"anchor\" href=\"#how-to-get-it\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to get it\u003c/h3\u003e\n\u003cp\u003eYadasoft and its required ASKAP dependencies are available from the CSIRO bitbucket server at:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/lofar-common.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/lofar-common.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/lofar-blob.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/lofar-blob.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/base-askap.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/base-askap.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/base-logfilters.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/base-logfilters.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/base-imagemath.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/base-imagemath.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/base-scimath.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/base-scimath.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/base-askapparallel.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/base-askapparallel.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/base-accessors.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/base-accessors.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/yandasoft.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/yandasoft.git\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are some extra tasks available from:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/askap-pipelinetasks.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/askap-pipelinetasks.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitbucket.csiro.au/scm/askapsdp/askap-analysis.git\" rel=\"nofollow\"\u003ehttps://bitbucket.csiro.au/scm/askapsdp/askap-analysis.git\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 9,
    "subscribers_count": 13,
    "topics": [
      "astronomy",
      "astronomical-algorithms"
    ],
    "updated_at": 1619096304.0
  },
  {
    "data_format": 2,
    "description": "Scripts for building Singularity images",
    "filenames": [
      "tensorflow/ubuntu.def",
      "caffe/ubuntu.def",
      "mxnet/ubuntu.def",
      "caffe2/ubuntu.def",
      "circuitscape/ubuntu.def",
      "dl/ubuntu.def"
    ],
    "full_name": "clemsonciti/singularity-images",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-image-scripts\" class=\"anchor\" href=\"#singularity-image-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity image scripts\u003c/h1\u003e\n\u003cp\u003eScripts to generate singularity images\nfor running different software on Palmetto cluster.\u003c/p\u003e\n",
    "stargazers_count": 10,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1597407988.0
  },
  {
    "data_format": 2,
    "description": "A simple example of running a MongoDB instance to query a database",
    "filenames": [
      "v1.0/mongodb-build/mongodb/Singularity",
      "v1.0/django-nginx-upload/Singularity",
      "v1.0/django-nginx-upload/db/Singularity",
      "v1.0/django-nginx-upload/nginx/Singularity",
      "v1.0/apache-simple/httpd/Singularity",
      "v1.0/rstudio-simple/rstudio/Singularity",
      "v1.0/rstudio-simple/nginx/Singularity",
      "v1.0/jupyter-simple/jupyter/Singularity",
      "v1.0/bert-as-compose/server/Singularity",
      "v1.0/bert-as-compose/server/Singularity.gpu",
      "v1.0/bert-as-compose/client/Singularity",
      "v2.0/ping/alp2/Singularity",
      "v2.0/ping/alp1/Singularity",
      "v2.0/start-args/Singularity",
      "v2.0/deephyperx/Singularity",
      "v2.0/code-server/Singularity"
    ],
    "full_name": "singularityhub/singularity-compose-examples",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-compose-examples\" class=\"anchor\" href=\"#singularity-compose-examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Compose Examples\u003c/h1\u003e\n\u003cp\u003eThis is a repository of examples for\n\u003ca href=\"https://singularityhub.github.io/singularity-compose\" rel=\"nofollow\"\u003eSingularity Compose\u003c/a\u003e. For the \"simple\"\nexample that is used during testing, see \u003ca href=\"https://github.com/singularityhub/singularity-compose-simple\"\u003esingularity-compose-simple\u003c/a\u003e. Otherwise, all examples are provided here. You can browse based on the spec version of Singularity Compose:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"v1.0\"\u003ev1.0\u003c/a\u003e is supported for Singularity compose less than v0.1.0\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"v2.0\"\u003ev2.0\u003c/a\u003e is supported for Singularity compose equal to or greater than v0.1.0\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 10,
    "subscribers_count": 4,
    "topics": [
      "singularity-compose",
      "mongodb"
    ],
    "updated_at": 1620468411.0
  },
  {
    "data_format": 2,
    "description": "Code supporting the recent preprint Markello et al., 2020",
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "netneurolab/markello_ppmisnf",
    "latest_release": "0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-similarity-network-fusion-in-parkinsons-disease\" class=\"anchor\" href=\"#similarity-network-fusion-in-parkinsons-disease\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimilarity network fusion in Parkinson\u0027s disease\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://zenodo.org/badge/latestdoi/245268776\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b54624e2e59ecddb7b78002cff4e39e47013c88ff0d58f1944cc28f16185036b/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3234353236383737362e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/245268776.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-whats-in-this-repository\" class=\"anchor\" href=\"#whats-in-this-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"What\u0027s in this repository?\"\u003c/h2\u003e\n\u003cp\u003eThis repository contains data, code, and results for the manuscript \"\u003ca href=\"https://www.biorxiv.org/content/10.1101/2020.03.05.979526v1\" rel=\"nofollow\"\u003eIntegrated morphometric, molecular, and clinical characterization of Parkinson\u0027s disease pathology\u003c/a\u003e.\"\nThe study examines the application of similarity network fusion to Parkinson\u0027s disease using data from the \u003ca href=\"https://www.ppmi-info.org\" rel=\"nofollow\"\u003eParkinson\u0027s Progression Markers Initiative\u003c/a\u003e (PPMI).\u003c/p\u003e\n\u003cp\u003eWe\u0027ve tried to document the various aspects of this repository with a whole bunch of README files, so feel free to jump around and check things out.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-just-let-me-run-the-things\" class=\"anchor\" href=\"#just-let-me-run-the-things\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"Just let me run the things!\"\u003c/h2\u003e\n\u003cp\u003eItching to just run the analyses?\nYou\u0027ll need to make sure you have \u003ca href=\"./walkthrough/01_accessing_data.md\"\u003eaccess to the PPMI\u003c/a\u003e and have set the appropriate environmental variables (\u003ccode\u003e$PPMI_USER\u003c/code\u003e) and (\u003ccode\u003e$PPMI_PASSWORD\u003c/code\u003e).\nOnce you\u0027ve done that, you can get going with the following:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone --recurse-submodules https://github.com/netneurolab/markello_ppmisnf\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e markello_ppmisnf\npip install -r requirements.txt\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PYTHONPATH=\u003cspan class=\"pl-smi\"\u003e$PYTHONPATH\u003c/span\u003e:\u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e/code\nmake all\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you don\u0027t want to deal with the hassle of creating a new Python environment, download the Singularity image that we used to run our analyses and run things in there:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone --recurse-submodules https://github.com/netneurolab/markello_ppmisnf\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e markello_ppmisnf\nwget -O container/ppmi_snf.simg https://osf.io/h6jwx/download\nbash container/run.sh\nmake all\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-i-want-to-take-things-slow\" class=\"anchor\" href=\"#i-want-to-take-things-slow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"I want to take things slow.\"\u003c/h2\u003e\n\u003cp\u003eIf you want a step-by-step through all the methods + analyses, take a look at out our \u003ca href=\"./walkthrough\"\u003ewalkthrough\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-i-have-some-questions\" class=\"anchor\" href=\"#i-have-some-questions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"I have some questions...\"\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/netneurolab/markello_ppmisnf/issues\"\u003eOpen an issue\u003c/a\u003e on this repository and someone will try and get back to you as soon as possible!\u003c/p\u003e\n",
    "stargazers_count": 10,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1621527226.0
  },
  {
    "data_format": 2,
    "description": "R bindings for the Fused Matrix Library (fml)",
    "filenames": [
      "containers/singularity/dev/Singularity",
      "containers/singularity/dev-gpu/Singularity"
    ],
    "full_name": "fml-fam/fmlr",
    "latest_release": "v0.3-0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fmlr\" class=\"anchor\" href=\"#fmlr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efmlr\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eVersion:\u003c/strong\u003e 0.3-0\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eLicense:\u003c/strong\u003e \u003ca href=\"http://opensource.org/licenses/BSL-1.0\" rel=\"nofollow\"\u003eBSL-1.0\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eProject home\u003c/strong\u003e: \u003ca href=\"https://github.com/fml-fam/fmlr\"\u003ehttps://github.com/fml-fam/fmlr\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eBug reports\u003c/strong\u003e: \u003ca href=\"https://github.com/fml-fam/fmlr/issues\"\u003ehttps://github.com/fml-fam/fmlr/issues\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eDocumentation\u003c/strong\u003e: \u003ca href=\"https://fml-fam.github.io/fmlr\" rel=\"nofollow\"\u003ehttps://fml-fam.github.io/fmlr\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efmlr is an R interface to the \u003ca href=\"https://github.com/fml-fam/fml\"\u003efml library\u003c/a\u003e. It is a \"medium-level\" interface for multiple dense matrix types, principally CPU, GPU, and MPI. Each supports multiple fundamental types (int, float, double), and data is held externally to R and operations that modify data generally occur in-place. The interface largely tracks with the core \u0027fml\u0027 interface. The interface is written such that generally an \u0027fmlr\u0027 R code can be easily translated to an \u0027fml\u0027 C++ code.\u003c/p\u003e\n\u003cp\u003eDifferences between fmlr and other matrix interfaces (including the core R interface):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSingle interface supporting multiple fundamental types (\u003ccode\u003e__half\u003c/code\u003e, \u003ccode\u003efloat\u003c/code\u003e, \u003ccode\u003edouble\u003c/code\u003e) and backends (CPU, GPU, MPI).\u003c/li\u003e\n\u003cli\u003eData is always held externally to R (although CPU objects can inherit R data without a copy).\u003c/li\u003e\n\u003cli\u003eOperations modifying data occur in-place (make your own copy if you don\u0027t want the data modified).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor a high-level interface on top of fmlr, see the \u003ca href=\"https://github.com/fml-fam/craze\"\u003ecraze package\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eIn principle, installation can be as simple as:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003einstall.packages(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efmlr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003erepos\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003ec(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehttps://hpcran.org\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehttps://cran.rstudio.com\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e))\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will build support for the CPU backend. If you want GPU or MPI support, please see the \u003ca href=\"https://fml-fam.github.io/fmlr/html/articles/01-installation.html\" rel=\"nofollow\"\u003eInstallation Guide\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-use\" class=\"anchor\" href=\"#example-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample Use\u003c/h2\u003e\n\u003cp\u003eCalculating singular values on CPU:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003esuppressMessages(library(\u003cspan class=\"pl-smi\"\u003efmlr\u003c/span\u003e))\n\u003cspan class=\"pl-v\"\u003ex\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e cpumat(\u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003etype\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efloat\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-smi\"\u003ex\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003efill_linspace(\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e)\n\u003cspan class=\"pl-smi\"\u003ex\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003einfo()\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# # cpumat 3x2 type=f\u003c/span\u003e\n\u003cspan class=\"pl-smi\"\u003ex\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# 1.0000 4.0000 \u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# 2.0000 5.0000 \u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# 3.0000 6.0000 \u003c/span\u003e\n\n\u003cspan class=\"pl-v\"\u003es\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e cpuvec(\u003cspan class=\"pl-v\"\u003etype\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efloat\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\nlinalg_svd(\u003cspan class=\"pl-smi\"\u003ex\u003c/span\u003e, \u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e)\n\n\u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003einfo()\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# # cpuvec 3 type=f\u003c/span\u003e\n\u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# 9.5080 0.7729 \u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand on GPU:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-v\"\u003ec\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e card()\n\u003cspan class=\"pl-smi\"\u003ec\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# GPU 0 (GeForce GTX 1070 Ti) 1139/8116 MB - CUDA 10.2\u003c/span\u003e\n\n\u003cspan class=\"pl-v\"\u003ex\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e gpumat(\u003cspan class=\"pl-smi\"\u003ec\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003etype\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efloat\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-smi\"\u003ex\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003efill_linspace(\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e)\n\u003cspan class=\"pl-smi\"\u003ex\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003einfo()\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# # gpumat 3x2 type=f \u003c/span\u003e\n\n\u003cspan class=\"pl-v\"\u003es\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e gpuvec(\u003cspan class=\"pl-smi\"\u003ec\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003etype\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efloat\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\nlinalg_svd(\u003cspan class=\"pl-smi\"\u003ex\u003c/span\u003e, \u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e)\n\n\u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e$\u003c/span\u003einfo()\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# # gpuvec 2 type=f \u003c/span\u003e\n\u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e# 9.5080 0.7729 \u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor more information and examples, see:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://fml-fam.github.io/fmlr\" rel=\"nofollow\"\u003ePackage documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eArticles:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://fml-fam.github.io/fmlr/html/articles/01-installation.html\" rel=\"nofollow\"\u003eInstallation Guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://fml-fam.github.io/fmlr/html/articles/02-overview.html\" rel=\"nofollow\"\u003eOverview of fmlr\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://fml-fam.github.io/fmlr/html/articles/03-backends.html\" rel=\"nofollow\"\u003eManaging Backends\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://fml-fam.github.io/fmlr/html/articles/04-data.html\" rel=\"nofollow\"\u003eData Management\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-fml-from-c\" class=\"anchor\" href=\"#fml-from-c\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efml from C++\u003c/h2\u003e\n\u003cp\u003eA copy of the core fml library is included in the fmlr package source in \u003ccode\u003einst/include/fml\u003c/code\u003e. If you wish to link with fml to create your own C++ kernels, you can add \u003ccode\u003eLinkingTo: fml\u003c/code\u003e to your R package DESCRIPTION file.\u003c/p\u003e\n\u003cp\u003eBefore you write your own C++ code using fml, you should check the \u003ca href=\"https://github.com/fml-fam/fml#api-stability\"\u003efml API stability\u003c/a\u003e progress, as some things may be subject to change.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-similar-projects\" class=\"anchor\" href=\"#similar-projects\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimilar Projects\u003c/h2\u003e\n\u003cp\u003eSome similar R projects worth mentioning:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMartin Maechler\u0027s (et al.) \u003ca href=\"https://cran.r-project.org/web/packages/Matrix/index.html\" rel=\"nofollow\"\u003eMatrix package\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCharles Determan\u0027s \u003ca href=\"https://github.com/cdeterman/gpuR\"\u003egpuR\u003c/a\u003e and \u003ca href=\"https://github.com/gpuRcore\"\u003egpuR-related packages\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eNorm Matloff\u0027s \u003ca href=\"https://github.com/Rth-org/Rth\"\u003eRth\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSome related R packages I have worked on:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/wrathematics/float\"\u003efloat\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/RBigData/kazaam\"\u003ekazaam\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/RBigData/pbdDMAT\"\u003epbdDMAT\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor C/C++ projects, see \u003ca href=\"https://github.com/fml-fam/fml#philosophy-and-similar-projects\"\u003ethe fml README\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 10,
    "subscribers_count": 2,
    "topics": [
      "r",
      "linear-algebra",
      "matrix",
      "blas",
      "cuda",
      "mpi",
      "scalapack",
      "hpc"
    ],
    "updated_at": 1620718561.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "MASILab/Synb0-DISCO",
    "latest_release": "v2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-synb0-disco\" class=\"anchor\" href=\"#synb0-disco\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSynb0-DISCO\u003c/h1\u003e\n\u003cp\u003eThis repository implements the paper \"Synthesized b0 for diffusion distortion correction\". For deployment we provide a docker container which uses the trained model to predict the undistorted b0 to be used in susceptability distortion correction for diffusion weighted MRI. Please use the following citation to refer to this work:\u003c/p\u003e\n\u003cp\u003eSchilling KG, Blaber J, Huo Y, Newton A, Hansen C, Nath V, Shafer AT, Williams O, Resnick SM, Rogers B, Anderson AW, Landman BA. Synthesized b0 for diffusion distortion correction (Synb0-DisCo). Magn Reson Imaging. 2019 Dec;64:62-70. doi: 10.1016/j.mri.2019.05.008. Epub 2019 May 7. PMID: 31075422; PMCID: PMC6834894.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-synb0_25iso_app\" class=\"anchor\" href=\"#synb0_25iso_app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esynb0_25iso_app\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/repository/docker/hansencb/synb0\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-instructions\" class=\"anchor\" href=\"#run-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Instructions:\u003c/h1\u003e\n\u003cp\u003eFor docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run --rm \\\n-v $(pwd)/INPUTS/:/INPUTS/ \\\n-v $(pwd)/OUTPUTS:/OUTPUTS/ \\\n-v \u0026lt;path to license.txt\u0026gt;:/extra/freesurfer/license.txt \\\n--user $(id -u):$(id -g) \\\nhansencb/synb0\n\nFlags:\n--notopup Skips the application of FSL\u0027s topup susceptibility correction \n* as a default, we run topup for you, although you may want to run this on\n your own (for example with your own config file, or if you would like to \n utilize multiple b0\u0027s)\n\nSee INPUTS/OUTPUTS sections below.\nIn short, if within your current directory you have your INPUTS \nand OUTPUTS folder, you can run this command copy/paste with the \nonly change being \u0026lt;path to license.txt\u0026gt; should point to \nfreesurfer licesnse.txt file on your system.\n\nIf INPUTS and OUTPUTS are not within your current directory, you\nwill need to change $(pwd)/INPUTS/ to the full path to your \ninput directory, and similarly for OUTPUTS.\n\n*** For Mac users, Docker defaults allows only 2gb of RAM \nand 2 cores - we suggest giving Docker access to \u0026gt;8Gb \nof RAM\n*** Additionally on MAC, if permissions issues prevent binding the\npath to the license.txt file, we suggest moving the freesurfer\nlicense.txt file to the current path and replacing the path line to\n\" $(pwd)/license.txt:/extra/freesurfer/license.txt \"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor singularity:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuild synb0.simg in the current directory:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull docker://hansencb/synb0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eRun the synb0.simg container:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -e \\\n-B INPUTS/:/INPUTS \\\n-B OUTPUTS/:/OUTPUTS \\\n-B \u0026lt;path to license.txt\u0026gt;:/extra/freesurfer/license.txt \\\n\u0026lt;path to synb0.simg\u0026gt;\n\n\u0026lt;path to license.txt\u0026gt; should point to freesurfer licesnse.txt file\n\u0026lt;path to synb0.simg\u0026gt; should point to the singularity container \n\nFlags:\n--notopup Skips the application of FSL\u0027s topup susceptibility correction \n* as a default, we run topup for you, although you may want to run this on\n your own (for example with your own config file, or if you would like to \n utilize multiple b0\u0027s)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eINPUTS:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eINPUTS directory must contain the following:\nb0.nii.gz, T1.nii.gz, and acqparams.txt\n\nb0.nii.gz includes the non-diffusion weighted image(s). \nT1.nii.gz is the T1-weighted image.\nacqparams.txt describes the acqusition parameters, and is described in detail \non the FslWiki for topup (https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/topup).Briefly,\nit describes the direction of distortion and tells TOPUP that the synthesized image\nhas an effective echo spacing of 0 (infinite bandwidth). An example acqparams.txt is\ndisplayed below, in which distortion is in the second dimension, note that the second\nrow corresponds to the synthesized, undistorted, b0:\n$ cat acqparams.txt \n0 1 0 0.062\n0 1 0 0.000\n\nT1_mask.nii.gz is an optional user provided mask which excludes the skull in the T1 image. \n               If not provided a mask will be estimated using FSL\u0027s BET.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWithout singularity or Docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eIf you choose to run this  in bash, the script that is containerized is located in\nsrc/pipeline.sh. The paths in pipeline.sh are specific to the docker/singularity file\nsystem, but the processing can be replicated using the scripts in src.\n\nThese utilize freesurfer, FSL, ANTS, and a python environment with pytorch.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOUTPUTS:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eAfter running, the outputs directory contains the following:\nT1_mask.nii.gz: brain extracted (skullstripped) T1   \nT1_norm.nii.gz: normalized T1\nepi_reg_d.mat: epi_reg b0 to T1 in FSL format\nepi_reg_d_ANTS.txt: epi_reg to T1 in ANTS format\n\nAnts registration of T1_norm to/from MNI space:\nANTS0GenericAffine.mat\nANTS1InverseWarp.nii.gz  \nANTS1Warp.nii.gz\n   \nT1_norm_lin_atlas_2_5.nii.gz: linear transform T1 to MNI   \nb0_d_lin_atlas_2_5.nii.gz  : linear transform distorted b0 in MNI space   \nT1_norm_nonlin_atlas_2_5.nii.gz: nonlinear transform T1 to MNI   \nb0_d_nonlin_atlas_2_5.nii.gz  : nonlinear transform distorted b0 in MNI space  \n\nInferences (predictions) for each of five folds:\nT1 input path: /OUTPUTS/T1_norm_lin_atlas_2_5.nii.gz\nb0 input path: /OUTPUTS/b0_d_lin_atlas_2_5.nii.gz\nb0_u_lin_atlas_2_5_FOLD_1.nii.gz  \nb0_u_lin_atlas_2_5_FOLD_2.nii.gz  \nb0_u_lin_atlas_2_5_FOLD_3.nii.gz  \nb0_u_lin_atlas_2_5_FOLD_4.nii.gz  \nb0_u_lin_atlas_2_5_FOLD_5.nii.gz  \n\nEnsemble average of inferences:\nb0_u_lin_atlas_2_5_merged.nii.gz  \nb0_u_lin_atlas_2_5.nii.gz         \n\nb0_u.nii.gz: Syntehtic b0 native space                      \n\nb0_d_smooth.nii.gz: smoothed b0\n\nb0_all.nii.gz: stack of distorted and synthetized image as input to topup        \n\ntopup outputs to be used for eddy:\ntopup_movpar.txt\nb0_all_topup.nii.gz\nb0_all.topup_log         \ntopup_fieldcoef.nii.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAFTER RUNNING:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eAfter running, we envision using the topup outputs directly with FSL\u0027s \neddy command, exactly as would be done if a full set of reverse PE \nscans was acquired. For example:\n\neddy --imain=path/to/diffusiondata.nii.gz --mask=path/to/brainmask.nii.gz \\\n--acqp=path/to/acqparams.txt --index=path/to/index.txt \\\n--bvecs=path/to/bvecs.txt --bvals=path/to/bvals.txt \n--topup=path/to/OUTPUTS/topup --out=eddy_unwarped_images\n\nwhere imain is the original diffusion data, mask is a brain mask, acqparams\nis from before, index is the traditional eddy index file which contains an \nindex (most likely a 1) for every volume in the diffusion dataset, topup points \nto the output of the singularity/docker pipeline, and out is the eddy-corrected\nimages utilizing the field coefficients from the previous step.\n\nAlternatively, if you choose to run --notopup flag, the file you are interested in\nis b0_all. This is a concatenation of the real b0 and the synthesized undistorted\nb0. We run topup with this file, although you may chose to do so utilizing your \ntopup version or config file. \n\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 11,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1619075973.0
  },
  {
    "data_format": 2,
    "description": "R in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.3.6.2"
    ],
    "full_name": "nickjer/singularity-r",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-r\" class=\"anchor\" href=\"#singularity-r\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity R\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nickjer/singularity-r\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e2d6deb1aeb1e18fbd9d72b2bdb73c7ab12099a81313d76bea0546cdfdb1c6/68747470733a2f2f7472617669732d63692e6f72672f6e69636b6a65722f73696e67756c61726974792d722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nickjer/singularity-r.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/462\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"Singularity Hub\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"https://www.r-project.org/\" rel=\"nofollow\"\u003eR\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is still a work in progress.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003esingularity-r.simg\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build singularity-r.simg Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from\n\u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name singularity-r.simg shub://nickjer/singularity-r\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-r\" class=\"anchor\" href=\"#r\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eR\u003c/code\u003e command is launched using the default run command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run singularity-r.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor as an explicit app:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --app R singularity-r.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003esingularity run --app R singularity-r.simg --version\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eR version 3.4.3 (2017-11-30) -- \"Kite-Eating Tree\"\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eCopyright (C) 2017 The R Foundation for Statistical Computing\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ePlatform: x86_64-pc-linux-gnu (64-bit)\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003eR is free software and comes with ABSOLUTELY NO WARRANTY.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eYou are welcome to redistribute it under the terms of the\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eGNU General Public License versions 2 or 3.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eFor more information about these matters see\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ehttp://www.gnu.org/licenses/.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rscript\" class=\"anchor\" href=\"#rscript\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRscript\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eRscript\u003c/code\u003e command is launched as an explicit app:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --app Rscript singularity-r.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003esingularity run --app Rscript singularity-r.simg --version\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eR scripting front-end version 3.4.3 (2017-11-30)\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eBug reports and pull requests are welcome on GitHub at\n\u003ca href=\"https://github.com/nickjer/singularity-r\"\u003ehttps://github.com/nickjer/singularity-r\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 12,
    "subscribers_count": 2,
    "topics": [
      "r",
      "singularity-image"
    ],
    "updated_at": 1617172672.0
  },
  {
    "data_format": 2,
    "description": "Multi-modal Image Registration And Connectivity anaLysis",
    "filenames": [
      "Singularity"
    ],
    "full_name": "mgoubran/MIRACL",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/mgoubran/MIRACL/blob/master/LICENSE.md\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1075b8b9dd5e5633cedcda2b5117cd7f11935b060bbd3469fcc1e8dbb3d4a302/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d676f756272616e2f4d495241434c\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/mgoubran/MIRACL\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://camo.githubusercontent.com/baa20110996179dced47b627519e4f6abf39a884364dae9b6761089520ae0cb5/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d676f756272616e2f6d697261636c\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/baa20110996179dced47b627519e4f6abf39a884364dae9b6761089520ae0cb5/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d676f756272616e2f6d697261636c\" alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/mgoubran/miracl\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"docs/gallery/icon.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/gallery/icon.png\" alt=\"alt text\" width=\"400\" height=\"250\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eMIRACL (Multi-modal Image Registration And Connectivity anaLysis)\nis a general-purpose, open-source pipeline for automated:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e1) Registration of mice clarity data to the Allen reference atlas\n2) Segmentation \u0026amp; feature extraction of mice clarity data in 3D (Sparse \u0026amp; nuclear staining)\n3) Registration of mice multimodal imaging data (MRI \u0026amp; CT, in-vivo \u0026amp; ex-vivo) to Allen reference atlas\n4) Tract or label specific connectivity analysis based on the Allen connectivity atlas\n5) Comparison of diffusion tensort imaging (DTI)/tractography, virus tracing using CLARITY \u0026amp;\n   Allen connectivity atlas\n6) Statistical analysis of CLARITY \u0026amp; Imaging data\n7) Atlas generation \u0026amp; Label manipulation\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCopyright (c) 2019 Maged Goubran, \u003ca href=\"mailto:maged.goubran@utoronto.ca\"\u003emaged.goubran@utoronto.ca\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAll Rights Reserved.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eWe provide containers for using the software (Docker and Singularity) as well as\nlocal install instructions. For more details, see our \u003ca href=\"https://miracl.readthedocs.io\" rel=\"nofollow\"\u003edocs\u003c/a\u003e.\nNote that the base image for the docker container can be found in \u003ca href=\"docker\"\u003edocker\u003c/a\u003e and\nthe container \u003ccode\u003emgoubran/miracl\u003c/code\u003e is built on top of that.\u003c/p\u003e\n",
    "stargazers_count": 13,
    "subscribers_count": 8,
    "topics": [
      "neuroscience",
      "brain",
      "atlases",
      "connectivity",
      "network",
      "clarity",
      "mri",
      "neuroimaging",
      "biomedical",
      "image-registration",
      "image-segmentation",
      "python"
    ],
    "updated_at": 1621933238.0
  },
  {
    "data_format": 2,
    "description": "This an example app that can serve as a template.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "BIDS-Apps/example",
    "latest_release": "0.0.7",
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-an-example-bids-app-template-repository\" class=\"anchor\" href=\"#an-example-bids-app-template-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAn example BIDS App (template repository)\u003c/h2\u003e\n\u003cp\u003eEvery BIDS App needs to follow a minimal set of command arguments common across\nall of the Apps. This allows users and developers to easily use and integrate\nBIDS Apps with their environment.\u003c/p\u003e\n\u003cp\u003eThis is a minimalist example of a BIDS App consisting of a Dockerfile and a simple\nentry point script (written in this case in Python) accepting the standard BIDS\nApps command line arguments. This repository can be used as a template for new BIDS Apps.\u003c/p\u003e\n\u003cp\u003eFor more information about the specification of BIDS Apps see \u003ca href=\"https://docs.google.com/document/d/1E1Wi5ONvOVVnGhj21S1bmJJ4kyHFT7tkxnV3C23sjIE/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h3\u003e\n\u003cp\u003eThis is a placeholder for a short description explaining to the user what your App will doing.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eProvide a link to the documentation of your pipeline.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to-report-errors\" class=\"anchor\" href=\"#how-to-report-errors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to report errors\u003c/h3\u003e\n\u003cp\u003eProvide instructions for users on how to get help and report errors.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgments\u003c/h3\u003e\n\u003cp\u003eDescribe how would you would like users to acknowledge use of your App in their papers (citation, a paragraph that can be copy pasted, etc.)\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cp\u003eThis App has the following command line arguments:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\tusage: run.py [-h]\n\t              [--participant_label PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]]\n\t              bids_dir output_dir {participant,group}\n\n\tExample BIDS App entry point script.\n\n\tpositional arguments:\n\t  bids_dir              The directory with the input dataset formatted\n\t                        according to the BIDS standard.\n\t  output_dir            The directory where the output files should be stored.\n\t                        If you are running a group level analysis, this folder\n\t                        should be prepopulated with the results of\n\t                        the participant level analysis.\n\t  {participant,group}   Level of the analysis that will be performed. Multiple\n\t                        participant level analyses can be run independently\n\t                        (in parallel).\n\n\toptional arguments:\n\t  -h, --help            show this help message and exit\n\t  --participant_label PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]\n\t                        The label(s) of the participant(s) that should be\n\t                        analyzed. The label corresponds to\n\t                        sub-\u0026lt;participant_label\u0026gt; from the BIDS spec (so it does\n\t                        not include \"sub-\"). If this parameter is not provided\n\t                        all subjects will be analyzed. Multiple participants\n\t                        can be specified with a space separated list.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run it in participant level mode (for one participant):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -i --rm \\\n\t-v /Users/filo/data/ds005:/bids_dataset:ro \\\n\t-v /Users/filo/outputs:/outputs \\\n\tbids/example \\\n\t/bids_dataset /outputs participant --participant_label 01\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter doing this for all subjects (potentially in parallel), the group level analysis\ncan be run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -i --rm \\\n\t-v /Users/filo/data/ds005:/bids_dataset:ro \\\n\t-v /Users/filo/outputs:/outputs \\\n\tbids/example \\\n\t/bids_dataset /outputs group\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-special-considerations\" class=\"anchor\" href=\"#special-considerations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpecial considerations\u003c/h3\u003e\n\u003cp\u003eDescribe whether your app has any special requirements. For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultiple map reduce steps (participant, group, participant2, group2 etc.)\u003c/li\u003e\n\u003cli\u003eUnusual memory requirements\u003c/li\u003e\n\u003cli\u003eetc.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 13,
    "subscribers_count": 2,
    "topics": [
      "bids",
      "bids-apps"
    ],
    "updated_at": 1621680020.0
  },
  {
    "data_format": 2,
    "description": "Intel HPC Containers using Singularity",
    "filenames": [
      "definitionFiles/namd/namdBuild.def",
      "definitionFiles/namd/namdRun.def",
      "definitionFiles/lammps/lammpsBuild.def",
      "definitionFiles/lammps/lammpsRun.def",
      "definitionFiles/WRF/wrfRun.def",
      "definitionFiles/WRF/wrfBuild.def",
      "definitionFiles/base/base.def",
      "definitionFiles/gromacs/gromacsRun.def",
      "definitionFiles/gromacs/gromacsBuild.def"
    ],
    "full_name": "intel/HPC-containers-from-Intel",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-goal\" class=\"anchor\" href=\"#goal\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGoal:\u003c/h1\u003e\n\u003cp\u003eCreate containers using Singularity definition file for HPC apps and run them on the cloud or bare metal for Single and Cluster runs.\u003c/p\u003e\n\u003cp\u003eThis repo should have definition files only for few HPC applications. Users can utilize them to generate containers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-get-help\" class=\"anchor\" href=\"#get-help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGet Help\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/intel/HPC-containers-from-Intel/issues\"\u003ePost an issue\u003c/a\u003e if you face any problem building or running a container\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 16,
    "subscribers_count": 9,
    "topics": [
      "hpc",
      "cluster",
      "singularity-containers",
      "cloud"
    ],
    "updated_at": 1619733161.0
  },
  {
    "data_format": 2,
    "description": "A RNN trained on Donald Trumps tweets",
    "filenames": [
      "Singularity"
    ],
    "full_name": "wyattferguson/trumpbot-rnn",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-trumpbot-v10\" class=\"anchor\" href=\"#trumpbot-v10\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTrumpbot v1.0\u003c/h1\u003e\n\u003cp\u003eTrumpbot was my attempt at creating a RNN trained on Donald Trumps(DT) tweets. I used this as a sort of practice project for learning a bit about RNN\u0027s and Tensorflow 2. The result was a chaos and a learning experience so let\u0027s dive in.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-with-containers\" class=\"anchor\" href=\"#run-with-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun with Containers\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h3\u003e\n\u003cp\u003eIf you don\u0027t want to install dependencies to your host, you can build a Docker container\nwith the included Dockerfile:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ docker build -t trumpbot \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe entrypoint is the script to generate the tweets:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ docker run trumpbot\n...\n obamas Top and France at 900 PM on FoxNews. Anderson Congratulations to the House vote \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e MittRomney o\n\n hillary Clinton has been a total disaster. I have an idea \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eher great speech on CNN\u003c/span\u003e \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e the world  a great honor \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e me and his partisan hotel and every spor\n\n friends support Trump International Golf Club on the Paris About that Right School is started by the DNC and Clinton and the DNC that will be a great show with t\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you want to interact with the container (perhaps training first) you can shell inside instead:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ docker run -it --entrypoint bash trumpbot\nroot@b53b98f12c34:/code# ls\nDockerfile  README.md  __init__.py  learn.py  raw_tweets.txt  requirements.txt\ttraining_checkpoints  trumpbot.py  tweets.txt\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou\u0027ll be in the \u003ccode\u003e/code\u003c/code\u003e directory that contains the source code.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h3\u003e\n\u003cp\u003eFor users that want to perhaps use GPU (or better leverage the host) the recommendation is to\nuse a \u003ca href=\"https://www.sylabs.io/guides/3.2/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container, and a recipe file \u003ca href=\"Singularity\"\u003eSingularity\u003c/a\u003e is provided\nto build the container.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sudo singularity build trumpbot.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd then to run (add the --nv flag if you want to leverage any host libraries).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity run trumpbot.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you need to change the way that tensorflow or numpy are installed, you can edit the Singularity or Docker recipes.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h2\u003e\n\u003cp\u003eSetup is pretty straightforward. It only needs numpy and tensorflow 2 alpha just run the start pip install:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install -r requirements.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dataset\" class=\"anchor\" href=\"#dataset\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDataset\u003c/h2\u003e\n\u003cp\u003eThe entire dataset was just tweets scraped from the DT twitter account. I used Jefferson Henrique\u0027s library \u003ca href=\"https://github.com/Jefferson-Henrique/GetOldTweets-python\"\u003eGetOldTweets-python\u003c/a\u003e that I modified a little bit. All the raw tweets can be found in the raw_tweets.txt file FYI all the links in any tweet have been removed.\u003c/p\u003e\n\u003cp\u003eThe first thing about using Tweets as a dataset for training is that they are filled with garbage that wreaks havoc when training. Heres what I did:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRemoved any links or urls to photos\u003c/li\u003e\n\u003cli\u003eSimplified all the puncuation, with Trump this is a big thing, his tweets are a clown fiesta of periods and exclemation marks.\u003c/li\u003e\n\u003cli\u003eCleaned out any invisible or non-english characters, any foreign characters just casuases trouble.\u003c/li\u003e\n\u003cli\u003eRemoved the \u0027@\u0027 symbol, I\u0027ll explain why later.\u003c/li\u003e\n\u003cli\u003eRemoved the first couple of months of tweets, they were mostly about the celebrity apprentice and not really core to what I was trying to capture.\u003c/li\u003e\n\u003cli\u003eRemoved any retweets or super short @replies\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe final training text is in tweets.txt which altogether is about 20,000 tweets.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-training\" class=\"anchor\" href=\"#training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTraining\u003c/h2\u003e\n\u003cp\u003eI trained the model twice, the first time for 30 epochs which took around 6 hours. The result was absolute garbage, at the time I hadn\u0027t removed hidden or foreign characters so it took 6 hours to spit out complete nonsense. So after I cleaned out the tweets again, I ran the training overnight for 50 epochs this time.\u003c/p\u003e\n\u003cp\u003eJust run the learn.py file to train it again if you want, the model check points are stored in the \u0027training_checkpoints\u0027 folder\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython3 learn.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-generating-tweets\" class=\"anchor\" href=\"#generating-tweets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerating Tweets\u003c/h2\u003e\n\u003cp\u003eSo now the fun part, you can run the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython3 trumpbot.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will generate 10 tweets from a random group of topics. If you open the trumpbot.py file theres a few things you can play with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etweets - Number of messages you want generated\n\ntemperature - This controls how predictable the tweet will be, by \n    default its random from 0.1 -\u0026gt; 0.4, anything above about 0.7 generates\n    garbage.\n\ntalking_points - Is a list of inputs to feed the network, try out \n    differnt words and see what works.\n\nnum_generate - This controls the length of the message you want to\n     get generated.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-result\" class=\"anchor\" href=\"#result\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResult\u003c/h2\u003e\n\u003cp\u003eFor my first crack at text generation Im happy with the results. Here are some sample tweets:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehillary Clinton has been a total disaster. If you cant admit that \nthe U.S. more than has been treated big baster I am a g\n\nDonald Trump is 45% Iran\n\nhealthe lobbyist now wants to raise taxes for our country in the \nfirst place! If only one thing is clea\n\nfriends support Trump Rally Anger Golf Club of Caporate legislation \nat the WhiteHouse today! #MakeAmericaGreatAgain Thank you for your\n support! #Trump2016 \n\nkoreau like you it was great being in the last election then will be\n a great show. I have a fan o\n\nkoreau lies and losers and losers will be a great show with the U.S.\n The President has a various past c\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-what-i-learned\" class=\"anchor\" href=\"#what-i-learned\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat I learned\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTweets make for a tough training set. Things like @ mentions just pollute the hell out of the text so unless you want your bot to be constantly @ing everything I need to find a better way to deal with that.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThings I thought the bot would love talking about stuff like #MAGA, Russia, China, and collusion just generate garbage strings.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eText generation is really hard, and takes a ton of training time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI could probably get a bit better results if I let it train a bit longer but for any drastic improvements I probably need to try another method or spend alot more time tuning the training set.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePick a subject that doesn\u0027t tweet like hes a dad yelling at a little league game. I think because his tweets are short little outbursts its hard to generate a predictable pattern across them.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe words it groups together for differnt topics is probably worth looking at, like whenever you use \u0027hillary\u0027 as a input it usually has the words \u0027liar\u0027 or \u0027disaster\u0027 in the sentence. or how it loves telling you when its gonna be on @Foxandfriends\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWith the method I used spelling its like to add random \u0027u\u0027 infront of words.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI feel like this is good starting point, and with some work we might have a digital orange man bot in our future.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-postbox-credit-contact--support\" class=\"anchor\" href=\"#postbox-credit-contact--support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cg-emoji class=\"g-emoji\" alias=\"postbox\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4ee.png\"\u003e\ud83d\udcee\u003c/g-emoji\u003e Credit, Contact \u0026amp; Support\u003c/h2\u003e\n\u003cp\u003eCreated by \u003ca href=\"https://twitter.com/programmingsux\" rel=\"nofollow\"\u003eWyatt Ferguson\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor any comments or questions your can reach me on Twitter \u003ca href=\"https://twitter.com/programmingsux\" rel=\"nofollow\"\u003e@programmingsux\u003c/a\u003e or visit my little portfolio at \u003ca href=\"https://wyattf.dev\" rel=\"nofollow\"\u003ewyattf.dev\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you like my theme and want to support me\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-coffee-buy-me-a-coffee\" class=\"anchor\" href=\"#coffee-buy-me-a-coffee\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.buymeacoffee.com/wyattferguson\" rel=\"nofollow\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"coffee\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2615.png\"\u003e\u2615\u003c/g-emoji\u003e Buy Me A Coffee\u003c/a\u003e\n\u003c/h3\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-zap-follow-me-on-twitter\" class=\"anchor\" href=\"#zap-follow-me-on-twitter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://twitter.com/programmingsux\" rel=\"nofollow\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"zap\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/26a1.png\"\u003e\u26a1\u003c/g-emoji\u003e Follow me on Twitter\u003c/a\u003e\n\u003c/h3\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-bus-follow-me-on-devto\" class=\"anchor\" href=\"#bus-follow-me-on-devto\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://dev.to/wyattferguson\" rel=\"nofollow\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"bus\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f68c.png\"\u003e\ud83d\ude8c\u003c/g-emoji\u003e Follow me on DEV.to\u003c/a\u003e\n\u003c/h3\u003e\n",
    "stargazers_count": 16,
    "subscribers_count": 2,
    "topics": [
      "tensorflow",
      "python"
    ],
    "updated_at": 1620777753.0
  },
  {
    "data_format": 2,
    "description": "A snakemake pipeline to assembly, polishing, correction and quality check from Oxford nanopore reads.",
    "filenames": [
      "Containers/Singularity.report.def",
      "Containers/Singularity.blobtools-1.1.1.def",
      "Containers/Singularity.kat-latest.def",
      "Containers/Singularity.conda.def",
      "Containers/Singularity.shasta-0.7.0.def",
      "Containers/Singularity.pilon-1.24.def",
      "Containers/Singularity.busco-4.1.4.def",
      "Containers/Singularity.assemblytics-1.2.def"
    ],
    "full_name": "SouthGreenPlatform/CulebrONT_pipeline",
    "latest_release": "1.4",
    "readme": "\u003cp\u003e\u003ca href=\"./docs/source/SupplementaryFiles/culebront_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/source/SupplementaryFiles/culebront_logo.png\" alt=\"Culebront Logo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.python.org/downloads\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e4779c52a0f8acf7c62517ff771deebcf8ab8913544dd508ccdd6cec2f2b400a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e372532422d626c7565\" alt=\"PythonVersions\" data-canonical-src=\"https://img.shields.io/badge/python-3.7%2B-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://snakemake.readthedocs.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/35030e6ddc253302ffcdf599ce8a8e387c27d88eb3de9cfe4e103b3ec6161f96/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d254532253839254135352e31302e302d627269676874677265656e2e7376673f7374796c653d666c6174\" alt=\"SnakemakeVersions\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%E2%89%A55.10.0-brightgreen.svg?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a324f41bf4495d7dc95ac4693962834b38ff77e1a6ed7f5c4dca9c3e3f92a6d3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d254532253839254135332e332e302d3745344337342e737667\" alt=\"Singularity\" data-canonical-src=\"https://img.shields.io/badge/singularity-%E2%89%A53.3.0-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://docs.conda.io/projects/conda/en/latest/index.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cacdb0b19bd30d76ae4faaee3355a6d65ecc448b587bac638adbd5eb04339c20/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612d342e382e352532302d677265656e\" alt=\"Conda\" data-canonical-src=\"https://img.shields.io/badge/conda-4.8.5%20-green\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsing data from long reads obtained by Oxford Nanopore Technologies sequencing makes genome assembly easier, in particular to solve repeats and structural variants, in prokaryotic as well as in eukaryotic genomes, resulting in increased contiguity and accuracy.\u003c/p\u003e\n\u003cp\u003eBunch of softwares and tools are released or updated every week, and a lot of species see their genome assembled using those.\u003c/p\u003e\n\u003cp\u003eThat\u2019s right.\u003c/p\u003e\n\u003cp\u003e\"\u003cem\u003eBut which assembly tool could give the best results for my favorite organism?\u003c/em\u003e\"\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCulebrONT can help you!\u003c/strong\u003e CulebrONT is an open-source, scalable, modulable and traceable snakemake pipeline, able to launch multiple assembly tools in parallel and providing help for choosing the best possible assembly between all possibilities.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHomepage: \u003ca href=\"https://culebront-pipeline.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ehttps://culebront-pipeline.readthedocs.io/en/latest/\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"user-content-citation\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003e@Authors:\u003c/p\u003e\n\u003cp\u003eJulie Orjuela (IRD), Aurore Comte(IRD), S\u00e9bastien Ravel(CIRAD), Florian Charriat(INRAE), Tram Vi(IRD, AGI), Francois Sabot(IRD) and S\u00e9bastien Cunnac(IRD).\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"user-content-notes\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-useful-notes\" class=\"anchor\" href=\"#useful-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful notes\u003c/h2\u003e\n\u003cp\u003eBefore launching CulebrONT, you could base-calling of arbitrarily multiplexed libraries across several Minion runs with sequencing quality control and gather the output files by genome for subsequent steps. For that use \u003ca href=\"https://github.com/vibaotram/baseDmux\"\u003ehttps://github.com/vibaotram/baseDmux\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks\u003c/h4\u003e\n\u003cp\u003eThanks to Ndomassi Tando (i-Trop IRD) by administration support.\u003c/p\u003e\n\u003cp\u003eThe authors acknowledge the IRD i-Trop HPC (South Green Platform) at IRD Montpellier for providing HPC resources that have contributed to this work. \u003ca href=\"https://bioinfo.ird.fr/\" rel=\"nofollow\"\u003ehttps://bioinfo.ird.fr/\u003c/a\u003e - \u003ca href=\"http://www.southgreen.fr\" rel=\"nofollow\"\u003ehttp://www.southgreen.fr\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThanks to Yann Delorme for this beautiful logo \u003ca href=\"https://nimarell.github.io/resume\" rel=\"nofollow\"\u003ehttps://nimarell.github.io/resume\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"user-content-licence\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eLicencied under CeCill-C (\u003ca href=\"http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html\" rel=\"nofollow\"\u003ehttp://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html\u003c/a\u003e) and GPLv3\nIntellectual property belongs to IRD and authors.\u003c/p\u003e\n",
    "stargazers_count": 17,
    "subscribers_count": 16,
    "topics": [],
    "updated_at": 1621527295.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "basebuilds/Singularity.juliabase",
      "basebuilds/Singularity.jupyterbase"
    ],
    "full_name": "Crown421/Singularity.jl",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h1\u003e\n\u003cp\u003eThis package was presented at JuliaCon, and the presentation is available \u003ca href=\"https://musing-pare-2ba365.netlify.app/#/\" rel=\"nofollow\"\u003eunder this link\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis package provides a rough interface to create \u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity containers\u003c/a\u003e from DrWatson Projects.\nIt currently works best on Linux systems, as the build command currently not available on Mac.\u003c/p\u003e\n\u003cp\u003eMost of the code is still very WIP and based on my own processes and needs. If you use Singularity, or have a use case that I haven\u0027t considered yet, please reach out either by email or by opening an issue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install-singularity\" class=\"anchor\" href=\"#install-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall singularity\u003c/h2\u003e\n\u003cp\u003eThe \u003ca href=\"https://sylabs.io/guides/3.0/user-guide/installation.html\" rel=\"nofollow\"\u003eSylab documentation\u003c/a\u003e contains instructions to install Singularity, but appears to be slightly out of date.\nYou can find additional information for\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinux on this \u003ca href=\"https://github.com/hpcng/singularity/blob/master/INSTALL.md\"\u003eGithub readme\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMac on the \u003ca href=\"https://sylabs.io/singularity-desktop-macos/\" rel=\"nofollow\"\u003edownload page for the beta release\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eWindows, see this \u003ca href=\"https://github.com/hpcng/singularity/issues/4518\"\u003ethis issue\u003c/a\u003e stating that WSL 2 is required (\u003ca href=\"https://docs.microsoft.com/en-us/windows/wsl/install-win10\" rel=\"nofollow\"\u003einstructions\u003c/a\u003e). After installing it, follow the instructions for Linux.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlternatively, you can use the \u003ca href=\"https://sylabs.io/guides/3.0/user-guide/installation.html#install-on-windows-or-mac\" rel=\"nofollow\"\u003einstructions for Vagrant\u003c/a\u003e on Windows and Mac.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-basis\" class=\"anchor\" href=\"#basis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBasis\u003c/h3\u003e\n\u003cp\u003eThis package uses a minimal debian-based container with Julia installed as a base. On the \u003ca href=\"https://cloud.sylabs.io/home\" rel=\"nofollow\"\u003eSylab cloud\u003c/a\u003e you can find the Juliabase image, and an experimental container also including jupyter\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://cloud.sylabs.io/library/_container/5e418a1b2758e9ed1175de24\" rel=\"nofollow\"\u003ejuliabase\u003c/a\u003e: 1.4.2, 1.3.1\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://cloud.sylabs.io/library/_container/5f20adbeae86dd3232dec1d1\" rel=\"nofollow\"\u003ejupyterbase\u003c/a\u003e: 1.4.2\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you would prefer to build them yourself, the def files are available in the \u003ccode\u003ebasebuilds\u003c/code\u003e folder.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-assumptions\" class=\"anchor\" href=\"#assumptions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAssumptions:\u003c/h3\u003e\n\u003cp\u003eThe package assumes that that the folder structure contains the following elements\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u251c\u2500\u2500 scripts\n\u2502   \u251c\u2500\u2500 run\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 module1\n\u2502   \u251c\u2500\u2500 module2\n\u251c\u2500\u2500 container\n\u251c\u2500\u2500 \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eother folders\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u251c\u2500\u2500 Project.toml\n\u251c\u2500\u2500 Manifest.toml\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand everything is under the control of a single git repository. This will be automatically the case if the project folder was created by \u003ca href=\"https://github.com/JuliaDynamics/DrWatson.jl\"\u003eDrWatson.jl\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe src and scripts folder will be copied into the container, and it is further assumed that the modules in it are registered as \u003ccode\u003edev\u003c/code\u003eed in the project \u003ccode\u003eManifest.toml\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-warning\" class=\"anchor\" href=\"#warning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWarning:\u003c/h3\u003e\n\u003cp\u003eCalling \u003ccode\u003ebuildsif\u003c/code\u003e will ask for root privileges, as the underlying \u003ccode\u003esingularity build\u003c/code\u003e commands requires it. This is clearly a potential security risk, so if you are unsure, please inspect the \u003ccode\u003eSingularity.pack\u003c/code\u003e file in the \u003ccode\u003econtainer\u003c/code\u003e folder.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eThe package provides the following functions. All these functions work from any folder as long as the correct project environment is loaded. They are also still WIP, so there is very little error checking being done.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    generate_deffile(; excludepkgs = [], commit = \"master\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates the \u003ccode\u003econtainer\u003c/code\u003e folder if it does not exist yet, and generates the \u003ccode\u003eSingularity.pack\u003c/code\u003e def file.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eexcludepkgs\u003c/code\u003e accepts and array of package names. These packages will be removed from \u003ccode\u003eProject.toml\u003c/code\u003e inside the container. This is for packages that are needed locally, for example for visualization, but are not needed in the container and would only add bloat.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecommit\u003c/code\u003e accept any project commit hash, and will build the container using the \u003ccode\u003esrc\u003c/code\u003e and \u003ccode\u003escript\u003c/code\u003e folder from that commit. Requires the git setting below.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e    buildsif(;verbose = false, force = true)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBuilds the container image into the \u003ccode\u003econtainer\u003c/code\u003e folder based on the existing def file.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003everbose\u003c/code\u003e sends all the output of the build process to the REPL if set to \u003ccode\u003etrue\u003c/code\u003e, otherwise it will be written to file.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eforce\u003c/code\u003e set to \u003ccode\u003etrue\u003c/code\u003e causes an existing image to be overwritten without asking for confirmation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e    recreatedata(file::String; dir = [])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExtracts the git commit hash and script name from a \u003ccode\u003eDrWatson.@tagsave\u003c/code\u003ed file, and generated a def file. The resulting container, when \u003ccode\u003esingularity run\u003c/code\u003e, should recreate the initial file directly.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003edir\u003c/code\u003e allows the specification of a subdirectory of the \u003ccode\u003eDrWatson.datadir()\u003c/code\u003e directory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e    servertransfer(host)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTransfers the image to the \u003ccode\u003ehost\u003c/code\u003e into a folder in the home directory of the same name as the project folder. This assumed that everything is configured such that \u003ccode\u003essh host\u003c/code\u003e just works.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-further-info\" class=\"anchor\" href=\"#further-info\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFurther info\u003c/h2\u003e\n\u003cp\u003eCurrently, the commands build a single read-only image. This means, that after any change in the project the entire image needs to be rebuilt. This is partly as intended, as the result is a tamper-proof complete environment, that can be used at any point in the future the return the exact same results.\nHowever for projects that are still under more rapid development, I have possible ideas to make that initial phase not require frequent lengthy rebuilds.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-further-work\" class=\"anchor\" href=\"#further-work\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFurther work:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGenerate different def files\u003c/li\u003e\n\u003cli\u003eadd interaction with singularity cloud and hub (pushing and pulling)\u003c/li\u003e\n\u003cli\u003esigning\u003c/li\u003e\n\u003cli\u003eadd tests\u003c/li\u003e\n\u003cli\u003eadd various error handling and options\u003c/li\u003e\n\u003cli\u003e(big) do some remote builder magic to make this work on windows/ mac\n\u003cul\u003e\n\u003cli\u003eAutomate image build on repo push, as mentioned on \u003ca href=\"https://singularityhub.github.io/singularityhub-docs/docs/builds/automated\" rel=\"nofollow\"\u003esingularity hub\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e(bigger) add singularity binary ?\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 17,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1616523817.0
  },
  {
    "data_format": 2,
    "description": "From annotated genomes to metabolic screening in large scale microbiotas",
    "filenames": [
      "recipes/Singularity"
    ],
    "full_name": "AuReMe/metage2metabo",
    "latest_release": "1.5.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://pypi.org/project/Metage2Metabo/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/68c19eb988f7da820e489e7d773438373c65af075fe846cb90e18836a7d7f9d4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d6574616765326d657461626f2e737667\" alt=\"PyPI version\" data-canonical-src=\"https://img.shields.io/pypi/v/metage2metabo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/AuReMe/metage2metabo/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fccd34831109fd6bdad80ef75ccdd11796acfad9808526a620456def8d9d9352/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f417552654d652f6d6574616765326d657461626f2e737667\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/AuReMe/metage2metabo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/AuReMe/metage2metabo/actions\"\u003e\u003cimg src=\"https://github.com/AuReMe/metage2metabo/workflows/Python%20package/badge.svg\" alt=\"Actions Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://metage2metabo.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/88095555c8fdcf3d56ae1cc3261918958b072a5308cc1d5113522bc284afd1b3/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6d6574616765326d657461626f2f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/metage2metabo/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://doi.org/10.7554/eLife.61968\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9e55e07ba04fd05d5e7a35a3dadc73af2472409b8403497e7056fb037f5e7875/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f692d31302e373535342f654c6966652e36313936382d626c756576696f6c65742e737667\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/doi-10.7554/eLife.61968-blueviolet.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-m2m---metage2metabo\" class=\"anchor\" href=\"#m2m---metage2metabo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eM2M - metage2metabo\u003c/h1\u003e\n\u003cp\u003eMetage2metabo is a Python3 (Python \u0026gt;= 3.6, tested with 3.6 and 3.7) tool to perform graph-based metabolic analysis starting from annotated genomes (\u003cstrong\u003ereference genomes or metagenome-assembled genomes\u003c/strong\u003e). It uses \u003cem\u003ePathway Tools\u003c/em\u003e in a automatic and parallel way to \u003cstrong\u003ereconstruct metabolic networks\u003c/strong\u003e for a large number of genomes. The obtained metabolic networks are then \u003cstrong\u003eanalyzed individually and collectively\u003c/strong\u003e in order to get the \u003cstrong\u003eadded value of metabolic cooperation in microbiota over individual metabolism\u003c/strong\u003e and to \u003cstrong\u003eidentify and screen interesting organisms\u003c/strong\u003e among all.\u003c/p\u003e\n\u003cp\u003em2m can be used as a whole workflow (\u003ccode\u003em2m workflow\u003c/code\u003e, \u003ccode\u003em2m metacom\u003c/code\u003e) or steps can be performed individually (\u003ccode\u003em2m recon\u003c/code\u003e , \u003ccode\u003em2m iscope\u003c/code\u003e , \u003ccode\u003em2m cscope\u003c/code\u003e, \u003ccode\u003em2m addedvalue\u003c/code\u003e, \u003ccode\u003em2m mincom\u003c/code\u003e, \u003ccode\u003em2m seeds\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIf you use M2M, please cite\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBelcour* A, Frioux* C, Aite M, Bretaudeau A, Hildebrand F, Siegel A. Metage2Metabo, microbiota-scale metabolic complementarity for the identification of key species. eLife 2020;9:e61968 \u003ca href=\"https://doi.org/10.7554/eLife.61968\" rel=\"nofollow\"\u003ehttps://doi.org/10.7554/eLife.61968\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor a summary of M2M and its applications, you can take a look at these \u003ca href=\"https://hal.inria.fr/hal-03151934/document\" rel=\"nofollow\"\u003eposter-slides\u003c/a\u003e, presented during the \u003ca href=\"https://jobim2020.sciencesconf.org/?forward-action=index\u0026amp;forward-controller=index\u0026amp;lang=en\" rel=\"nofollow\"\u003eJOBIM 2020 conference\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#m2m---metage2metabo\"\u003eM2M - metage2metabo\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#table-of-contents\"\u003eTable of contents\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#general-information-about-the-modelling\"\u003eGeneral information about the modelling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#license\"\u003eLicense\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#documentation\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#technologies\"\u003eTechnologies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#requirements\"\u003eRequirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#installation-with-pip\"\u003eInstallation with pip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#availability-on-docker-and-singularity\"\u003eAvailability on Docker and Singularity\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#m2m-commands\"\u003eM2M commands\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#analysis-of-the-minimal-solutions\"\u003eAnalysis of the minimal solutions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#release-notes\"\u003eRelease Notes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#additional-features\"\u003eAdditional features\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#article-data\"\u003eArticle data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#authors\"\u003eAuthors\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#acknowledgement\"\u003eAcknowledgement\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-general-information-about-the-modelling\" class=\"anchor\" href=\"#general-information-about-the-modelling\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeneral information about the modelling\u003c/h2\u003e\n\u003cp\u003eM2M has two main dependencies for modelling metabolic networks: \u003ca href=\"https://github.com/cfrioux/MeneTools\"\u003eMeneTools\u003c/a\u003e and \u003ca href=\"https://github.com/cfrioux/miscoto\"\u003eMiscoto\u003c/a\u003e. Accordingly metabolic models in M2M follow the producibility in metabolic networks as defined by the \u003ca href=\"http://www.ncbi.nlm.nih.gov/pubmed/15712108\" rel=\"nofollow\"\u003enetwork expansion\u003c/a\u003e algorithm.\nMainly, two rules are followed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea \u003cem\u003erecursive rule\u003c/em\u003e: the products of a reactions are producible if \u003cstrong\u003eall\u003c/strong\u003e reactants of this reaction are themselves producible\u003c/li\u003e\n\u003cli\u003ean \u003cem\u003einitiation rule\u003c/em\u003e: producibility is initiated by the presence of nutrients, called \u003cem\u003eseeds\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA metabolite that is producible from a set of nutrients is described as being \"in the scope of the seeds\".\nThe computation is made using logic solvers (Answer Set Programming). The present modelling ignores the stoichiometry of reactions (2A + B --\u0026gt; C is considered equivalent to A + B --\u0026gt; C), and is therefore suited to non-curated or draft metabolic networks, as the ones built using M2M with the PathoLogic software of \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5036846/pdf/bbv079.pdf\" rel=\"nofollow\"\u003ePathway Tools\u003c/a\u003e handled by \u003ca href=\"https://github.com/AuReMe/mpwt\"\u003eMpwt\u003c/a\u003e. Many works have relied on network expansion to study organisms (\u003ca href=\"http://doi.wiley.com/10.1111/tpj.12627\" rel=\"nofollow\"\u003ehere\u003c/a\u003e, \u003ca href=\"https://dx.plos.org/10.1371/journal.pcbi.1000049\" rel=\"nofollow\"\u003ehere\u003c/a\u003e or \u003ca href=\"http://dx.plos.org/10.1371/journal.pcbi.1005276\" rel=\"nofollow\"\u003ethere\u003c/a\u003e) and communities (\u003ca href=\"https://academic.oup.com/bioinformatics/article/34/17/i934/5093211\" rel=\"nofollow\"\u003ehere\u003c/a\u003e, \u003ca href=\"https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4786-7\" rel=\"nofollow\"\u003ehere\u003c/a\u003e, or \u003ca href=\"https://www.ncbi.nlm.nih.gov/pubmed/18546499\" rel=\"nofollow\"\u003ehere\u003c/a\u003e). It has been \u003ca href=\"http://www.ncbi.nlm.nih.gov/pubmed/19425125\" rel=\"nofollow\"\u003ecompared\u003c/a\u003e, \u003ca href=\"https://www.cambridge.org/core/product/identifier/S1471068418000455/type/journal_article\" rel=\"nofollow\"\u003ecombined\u003c/a\u003e to steady-state modelling (Flux Balance Analysis).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis project is licensed under the GNU General Public License - see the \u003ca href=\"https://github.com/AuReMe/metage2metabo/blob/master/LICENSE\"\u003eLICENSE.md\u003c/a\u003e file for details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eA more detailled documentation is available at: \u003ca href=\"https://metage2metabo.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ehttps://metage2metabo.readthedocs.io\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-technologies\" class=\"anchor\" href=\"#technologies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTechnologies\u003c/h2\u003e\n\u003cp\u003ePython 3 (Python 3.6 is tested). M2M uses a certain number of Python dependencies. An example of all these dependencies working for Ubuntu 18.04 is available in \u003ca href=\"https://github.com/AuReMe/metage2metabo/blob/master/requirements.txt\"\u003erequirements.txt\u003c/a\u003e.\nThey can be installed with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip install -r requirements.txt --no-cache-dir\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn particular, m2m relies on:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/AuReMe/mpwt\"\u003empwt\u003c/a\u003e to automatize metabolic network reconstruction with Pathway Tools\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/AuReMe/padmet\"\u003epadmet\u003c/a\u003e to manage metabolic networks\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/cfrioux/MeneTools\"\u003emenetools\u003c/a\u003e to analyze individual metabolic capabilities using logic programming\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/cfrioux/miscoto\"\u003emiscoto\u003c/a\u003e to analyze collective metabolic capabilities and select communities within microbiota using logic programming\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAlso, m2m_analysis relies on other packages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/networkx/networkx\"\u003enetworkx\u003c/a\u003e to create graph from miscoto results\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/etetoolkit/ete\"\u003eete3\u003c/a\u003e to add taxonomy information on the graph if you used mpwt taxon file\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/Aluriak/PowerGrASP\"\u003epowergrasp\u003c/a\u003e to compress networkx graph\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"http://bioinformatics.ai.sri.com/ptools/\" rel=\"nofollow\"\u003ePathway Tools\u003c/a\u003e version 23.0 or higher (free for \u003ca href=\"https://biocyc.org/download-bundle.shtml\" rel=\"nofollow\"\u003eacademic users\u003c/a\u003e) is \u003cstrong\u003erequired for m2m workflow and m2m recon\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePathway Tools requirements\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eLinux\u003c/strong\u003e: Gnome terminal and Libxm4\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eapt-get update \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e apt-get install gnome-terminal libxm4\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eAll OS\u003c/strong\u003e: \u003ca href=\"https://www.ncbi.nlm.nih.gov/books/NBK279671/\" rel=\"nofollow\"\u003eNCBI Blast\u003c/a\u003e and a ncbirc file in user\u0027s home directory\n\u003cul\u003e\n\u003cli\u003eInstall with apt-get\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eapt-get update \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e apt-get install gnome-terminal libxm4 ncbi-blast+ \n\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e[ncbi]\\nData=/usr/bin/data\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.ncbirc\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eInstall with a dmg installer on MacOS\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePathway Tools install\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLinux\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003echmod +x ./pathway-tools-22.5-linux-64-tier1-install \n./pathway-tools-22.5-linux-64-tier1-install \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eand follow the instructions during the interactive install\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFor a silent install\u003c/em\u003e: \u003ccode\u003e./pathway-tools-22.5-linux-64-tier1-install --InstallDir your/install/directory/pathway-tools --PTOOLS_LOCAL_PATH your/chosen/directory/for/data/ptools --InstallDesktopShortcuts 0 --mode unattended\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMacOS\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDmg installer with a graphical interface.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eWarning\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e/!\\ For all OS, Pathway Tools must be in \u003ccode\u003e$PATH\u003c/code\u003e.\nOn Linux and MacOS: \u003ccode\u003eexport PATH=$PATH:your/install/directory/pathway-tools\u003c/code\u003e.\nConsider adding Pathway Tools in \u003ccode\u003e$PATH\u003c/code\u003e permanently by running\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003eexport PATH=\"$PATH:your/install/directory/pathway-tools:\"\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.bashrc\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"http://www.biotec.tu-dresden.de/research/schroeder/powergraphs/download-command-line-tool.html\" rel=\"nofollow\"\u003eOog Power Graph Command line tool\u003c/a\u003e to create a svg file from the compressed graph at the end of m2m_analysis. This tool is a jar file, Java is needed to use it.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eDeveloped and tested on Linux (Ubuntu, Fedora, Debian) and MacOs (version 10.14) with Python3.6.\u003c/p\u003e\n\u003cp\u003eContinuous Integration using GitHub Actions with Python3.6 and Python3.7 on ubuntu-latest, macos-latest and windows-latest (\u003ca href=\"https://docs.github.com/en/free-pro-team@latest/actions/reference/specifications-for-github-hosted-runners#supported-runners-and-hardware-resources\"\u003ecorresponding virtual environment\u003c/a\u003e).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation-with-pip\" class=\"anchor\" href=\"#installation-with-pip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation with pip\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003epip install Metage2Metabo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-availability-on-docker-and-singularity\" class=\"anchor\" href=\"#availability-on-docker-and-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailability on Docker and Singularity\u003c/h3\u003e\n\u003cp\u003eDue to Pathway-Tools license, Docker or Singularity images are not available publicly.\u003c/p\u003e\n\u003cp\u003eBut you can create these images by using the Dockerfile and Singularity recipes available inside the recipes folder.\nWith these files, you can create container with Pathway-Tools and m2m.\u003c/p\u003e\n\u003cp\u003eMore informations in the \u003ca href=\"https://metage2metabo.readthedocs.io/en/latest/install.html#installation-with-docker\" rel=\"nofollow\"\u003eDocker and Singularity Documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-m2m-commands\" class=\"anchor\" href=\"#m2m-commands\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eM2M commands\u003c/h2\u003e\n\u003cp\u003eM2M commands are listed in the \u003ca href=\"https://metage2metabo.readthedocs.io/en/latest/command.html\" rel=\"nofollow\"\u003eCommands Documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCopyright (C) Dyliss \u0026amp; Pleiade\nLicense GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt;\nm2m is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\nusage: m2m [-h] [-v]\n        {recon,iscope,cscope,addedvalue,mincom,seeds,workflow,metacom,test}\n        ...\n\nFrom metabolic network reconstruction with annotated genomes to metabolic\ncapabilities screening to identify organisms of interest in a large\nmicrobiota. For specific help on each subcommand use: m2m {cmd} --help\n\noptional arguments:\n-h, --help            show this help message and exit\n-v, --version         show program\u0027s version number and exit\n\nsubcommands:\nvalid subcommands:\n\n{recon,iscope,cscope,addedvalue,mincom,seeds,workflow,metacom,test}\n    recon               metabolic network reconstruction\n    iscope              individual scope computation\n    cscope              community scope computation\n    addedvalue          added value of microbiota\u0027s metabolism over\n                        individual\u0027s\n    mincom              minimal communtity selection\n    seeds               creation of seeds SBML file\n    workflow            whole workflow\n    metacom             whole metabolism community analysis\n    test                test on sample data from rumen experiments\n\nRequires: Pathway Tools installed and in $PATH, and NCBI Blast\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-analysis-of-the-minimal-solutions\" class=\"anchor\" href=\"#analysis-of-the-minimal-solutions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnalysis of the minimal solutions\u003c/h2\u003e\n\u003cp\u003eM2M performs a community minimization to find the union and intersection of the minimal communities. But it is possible to analyze all the minimal communities.\nM2M has a second command-line, named m2m_analysis that performs this analysis. This method is slower than m2m as all sollutions are enumerated.\nThen it creates a solutions graph and compresses it in a powergraph. Then it creates visualization (html file and optionnaly svg files).\u003c/p\u003e\n\u003cp\u003eMore information about this command in the \u003ca href=\"https://metage2metabo.readthedocs.io/en/latest/m2m_analysis.html\" rel=\"nofollow\"\u003em2m_analysis Documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eusage: m2m_analysis [-h] [-v] {enum,graph,powergraph,workflow} ...\n\nDetection of key species among communities.\n For specific help on each subcommand use: m2m_analysis {cmd} --help\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         show program\u0027s version number and exit\n\nsubcommands:\n  valid subcommands:\n\n  {enum,graph,powergraph,workflow}\n    enum                enumeration using miscoto\n    graph               graph creation with enumeration solution\n    powergraph          powergraph creation and visualization\n    workflow            whole workflow\n\nOog jar file (http://www.biotec.tu-dresden.de/research/schroeder/powergraphs/download-command-line-tool.html) for powergraph svg creation.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-release-notes\" class=\"anchor\" href=\"#release-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRelease Notes\u003c/h2\u003e\n\u003cp\u003eChanges between version are listed on the \u003ca href=\"https://github.com/AuReMe/metage2metabo/releases\"\u003erelease page\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-additional-features\" class=\"anchor\" href=\"#additional-features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdditional features\u003c/h2\u003e\n\u003cp\u003eM2M relies on packages that can also be used independantly with more features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/AuReMe/mpwt\"\u003empwt\u003c/a\u003e: command-line and multi-process solutions to run Pathway Tools. Suitable to multiple reconstruction, for example genomes of a microbiota\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/cfrioux/MeneTools\"\u003emenetools\u003c/a\u003e: individual metabolic capabilities analysis using graph-based producibility criteria\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/cfrioux/miscoto\"\u003emiscoto\u003c/a\u003e: community selection and metabolic screening in large-scal microbiotas, with or without taking a host into account\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003eBelcour* A, Frioux* C, Aite M, Bretaudeau A, Hildebrand F, Siegel A. Metage2Metabo, microbiota-scale metabolic complementarity for the identification of key species. eLife 2020;9:e61968 \u003ca href=\"https://doi.org/10.7554/eLife.61968\" rel=\"nofollow\"\u003ehttps://doi.org/10.7554/eLife.61968\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-article-data\" class=\"anchor\" href=\"#article-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eArticle data\u003c/h2\u003e\n\u003cp\u003eData used to create figures and tables are listed in the \u003ca href=\"https://github.com/AuReMe/metage2metabo/tree/master/article_data\"\u003earticle_data\u003c/a\u003e folder, it contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/AuReMe/metage2metabo/tree/master/article_data/gsmn_characteristics\"\u003egsmn_characteristics\u003c/a\u003e: scripts and tables to show the characteristics of draft metabolic networks created by M2M for gut, rumen and diabetes dataset.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/AuReMe/metage2metabo/tree/master/article_data/diabetes_study\"\u003ediabetes_study\u003c/a\u003e: scripts and tables to create the figures of the diabetes analyses in the article.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors\" class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://cfrioux.github.io/\" rel=\"nofollow\"\u003eCl\u00e9mence Frioux\u003c/a\u003e and \u003ca href=\"https://arnaudbelcour.github.io/blog/\" rel=\"nofollow\"\u003eArnaud Belcour\u003c/a\u003e, Univ Rennes, Inria, CNRS, IRISA, Rennes, France.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgement\" class=\"anchor\" href=\"#acknowledgement\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgement\u003c/h2\u003e\n\u003cp\u003ePeople of Pathway Tools (SRI International) for their help integrating Pathway Tools with command line and multiprocessing in the \u003ca href=\"https://github.com/AuReMe/mpwt\"\u003empwt\u003c/a\u003e package, used in M2M.\u003c/p\u003e\n",
    "stargazers_count": 18,
    "subscribers_count": 4,
    "topics": [
      "bioinformatics",
      "bioinformatics-pipeline",
      "metabolic-models"
    ],
    "updated_at": 1621979076.0
  },
  {
    "data_format": 2,
    "description": "Multiscale simulation of multi-cellular system",
    "filenames": [
      "MaBoSS-env-2.0/containers/singularity/Singularity"
    ],
    "full_name": "gletort/PhysiBoSS",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-physiboss\" class=\"anchor\" href=\"#physiboss\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePhysiBoSS\u003c/h1\u003e\n\u003cp\u003eMultiscale simulation of multi-cellular system\u003c/p\u003e\n\u003cp\u003eOverview:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#presentation\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#usage\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#documentation\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#remarks\"\u003eRemarks\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-presentation\" class=\"anchor\" href=\"#presentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePresentation\u003c/h2\u003e\n\u003cp\u003ePhysiBoSS (PhysiCell-MaBoSS) is C++ software for multiscale simulation of heterogeneous multi-cellular system. It integrates together cell\u0027s internal signalling pathway model (boolean formalism), physical representation of cell (agent-based) and extra-cellular matrix diffusing or fixed entities.\nIt is adapted from \u003ca href=\"http://physicell.mathcancer.org\" rel=\"nofollow\"\u003ePhysiCell\u003c/a\u003e sources, with the boolean network computation inside each cell from \u003ca href=\"http://maboss.curie.fr\" rel=\"nofollow\"\u003eMaBoSS\u003c/a\u003e software.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"./doc/imgs/hello.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./doc/imgs/hello.png?raw=true\" alt=\"Hello world image\" title=\"PhysiBoSS simulation example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-compiling-physiboss\" class=\"anchor\" href=\"#compiling-physiboss\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompiling PhysiBoSS\u003c/h3\u003e\n\u003cp\u003ePhysiBoSS should run and be easily installed on Linux and MacOS system.\nIt requires moderatly recent version of C++ (at least c++11) and OpenMP support. Compilation of MaBoSS library requires \u003ccode\u003eflex\u003c/code\u003e and \u003ccode\u003ebison\u003c/code\u003e library, usually already present (and can be easily installed on e.g. Linux ubuntu with \u003ccode\u003esudo apt-get install bison flex\u003c/code\u003e). We also provide a \u003ca href=\"https://github.com/gletort/PhysiBoSS/tree/master/Docker\"\u003eDocker image\u003c/a\u003e of PhysiBoSS that can be used if it cannot be installed in your machine. It can also be used without any installation via a Web interface for specific simulations on \u003ca href=\"#nanohub\"\u003enanohub\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo install it on Linux system, from a Terminal:\nClone the repository on your local machine, and go inside the main directory. Type \u003ccode\u003emake install\u003c/code\u003e, which will install and compile MaBoSS then PhysiBoSS. The executables will be created in the \u0027bin\u0027 directory if all goes well.\nIt can be compiled in \u0027Debug\u0027, \u0027Release\u0027 or \u0027Proliling\u0027 modes, to set in the \u0027Makefile\u0027 file. Default is \u0027Release\u0027 mode (fastest).\nYou might also have to change your c++ compiler in the Makefile according to your operating system.\u003c/p\u003e\n\u003cp\u003eCommands list:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/gletort/PhysiBoSS.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e PhysiBoSS\nmake install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf errors happened during the compilation, please refer to the \u003ca href=\"https://github.com/gletort/PhysiBoSS/wiki/Installation\"\u003einstallation\u003c/a\u003e page.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-one-simulation\" class=\"anchor\" href=\"#running-one-simulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning one simulation\u003c/h3\u003e\n\u003cp\u003eTo run a simulation, you need (at least) a XML parameter file indicating the conditions of the simulation, and the networks file (you can find some on \u003ca href=\"http://maboss.curie.fr\" rel=\"nofollow\"\u003eMaBoSS website\u003c/a\u003e and on our \u003ca href=\"https://github.com/ArnauMontagud/Logical_modelling_pipeline\"\u003elogical modelling pipeline repository\u003c/a\u003e).\nOther options are possible, cf the code-documentation or this repository wiki for more informations.\u003c/p\u003e\n\u003cp\u003eExample of a parameter file (with only few parameters shown):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-xml\"\u003e\u003cpre\u003e  \u0026lt;?\u003cspan class=\"pl-ent\"\u003exml\u003c/span\u003e\u003cspan class=\"pl-e\"\u003e version\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e1.0\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"pl-e\"\u003e encoding\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eUTF-8\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e ?\u0026gt;\n \n  \u0026lt;\u003cspan class=\"pl-ent\"\u003esimulation\u003c/span\u003e\u0026gt;\n \t\t\u0026lt;\u003cspan class=\"pl-ent\"\u003etime_step\u003c/span\u003e\u0026gt; 0.2 \u0026lt;/\u003cspan class=\"pl-ent\"\u003etime_step\u003c/span\u003e\u0026gt;\n \t\t\u0026lt;\u003cspan class=\"pl-ent\"\u003emechanics_time_step\u003c/span\u003e\u0026gt; 0.1 \u0026lt;/\u003cspan class=\"pl-ent\"\u003emechanics_time_step\u003c/span\u003e\u0026gt;\n \t\t....\n  \u0026lt;/\u003cspan class=\"pl-ent\"\u003esimulation\u003c/span\u003e\u0026gt;\n \n  \u0026lt;\u003cspan class=\"pl-ent\"\u003ecell_properties\u003c/span\u003e\u0026gt;\n \t\t\u0026lt;\u003cspan class=\"pl-ent\"\u003emode_motility\u003c/span\u003e\u0026gt; 1 \u0026lt;/\u003cspan class=\"pl-ent\"\u003emode_motility\u003c/span\u003e\u0026gt;\n \t\t\u0026lt;\u003cspan class=\"pl-ent\"\u003epolarity_coefficient\u003c/span\u003e\u0026gt; 0.5 \u0026lt;/\u003cspan class=\"pl-ent\"\u003epolarity_coefficient\u003c/span\u003e\u0026gt;\n \t\t...\n  \u0026lt;/\u003cspan class=\"pl-ent\"\u003ecell_properties\u003c/span\u003e\u0026gt;\n \n  \u0026lt;\u003cspan class=\"pl-ent\"\u003enetwork\u003c/span\u003e\u0026gt;\n \t\t\u0026lt;\u003cspan class=\"pl-ent\"\u003enetwork_update_step\u003c/span\u003e\u0026gt; 10 \u0026lt;/\u003cspan class=\"pl-ent\"\u003enetwork_update_step\u003c/span\u003e\u0026gt;\n \t\t...\n  \u0026lt;/\u003cspan class=\"pl-ent\"\u003enetwork\u003c/span\u003e\u0026gt;\n \n  \u0026lt;\u003cspan class=\"pl-ent\"\u003einitial_configuration\u003c/span\u003e\u0026gt;\n \t\t\u0026lt;\u003cspan class=\"pl-ent\"\u003eload_cells_from_file\u003c/span\u003e\u0026gt; init.txt \u0026lt;/\u003cspan class=\"pl-ent\"\u003eload_cells_from_file\u003c/span\u003e\u0026gt;\n \t\t...\n  \u0026lt;/\u003cspan class=\"pl-ent\"\u003einitial_configuration\u003c/span\u003e\u0026gt;\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-image-and-analyse-a-simulation\" class=\"anchor\" href=\"#image-and-analyse-a-simulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImage and analyse a simulation\u003c/h3\u003e\n\u003cp\u003eTo visualize graphically the result of a simulation, with use the software Paraview (or you can also generate a \u003ccode\u003e.svg\u003c/code\u003e snapshot of the simulation). Analysis of the result files were done with python scripts proposed in this directory. For documentation on how to use Paraview to set-up the rendering of PhysiBoSS outputs, see \u003ca href=\"https://github.com/gletort/PhysiBoSS/wiki/Paraviewing\"\u003ehere\u003c/a\u003e, with the explication on how to draw spheres from a set of points (x, y, z, radius).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-nanohub\" class=\"anchor\" href=\"#nanohub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNanohub\u003c/h3\u003e\n\u003cp\u003ePhysiBoSS can be directly used via a Web interface on nanohub. This allows to run it without any installation and running directly on the server and can be used without any coding skills. The parameters of the simulation can be entered in the interface and then the simulation will be runned on the nanohub server. It just required a nanohub account.\nAvailable simulations tools of PhysiBoSS can be found on \u003ca href=\"https://nanohub.org/resources/tools\" rel=\"nofollow\"\u003ehttps://nanohub.org/resources/tools\u003c/a\u003e, under the keywords PhysiBoSS or PhysiBoSSa.\u003c/p\u003e\n\u003cp\u003eA model of tumors cell spheroid growing and invading into the surrounding extra-cellular matrix (ECM) is currently available \u003ca href=\"https://proxy.nanohub.org/weber/1663662/howueVcBQ44wNqfD/12/apps/project_repo_ecm_simul.ipynb?\" rel=\"nofollow\"\u003ePhysiBoSSa_ECM\u003c/a\u003e. Various parameters as the density of the extracellular matrix, the cell motility, ECM degradation from the cells, TGF-beta production... can be tuned by the user.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eCode-oriented documentation can be generated with Doxygen:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emake doc\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ein the main directory.\nIt can be configured in the Doxyfile file present in this directory.\nIt will generate the html documentation in the doc/html directory.\nYou can visualize it in a browser, e.g.:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003efirefox doc/html/index.html \u003cspan class=\"pl-k\"\u003e\u0026amp;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can also refer to (future) publications with PhysiBoSS for scientific applications of this software and description of the models.\u003c/p\u003e\n\u003cp\u003eStep-by-step examples with the necessary files to run them are also proposed in the \u0027examples\u0027 directory and on the \u003ca href=\"https://github.com/gletort/PhysiBoSS/wiki\"\u003eWiki\u003c/a\u003e of this repository.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences\u003c/h2\u003e\n\u003cp\u003e For PhysiBoSS: \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePhysiBoSS publication: Letort G, Montagud A, Stoll G, Heiland R, Barillot E, Macklin P, Zinovyev A, Calzone L . \u003ca href=\"https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty766/5087713\" rel=\"nofollow\"\u003e PhysiBoSS: a multi-scale agent-based modelling framework integrating physical dimension and cell signalling. \u003c/a\u003e Bioinformatics, bty766, doi:10.1093/bioinformatics/bty766\n\u003c/li\u003e\n\u003cbr\u003e\n \u003c/ul\u003e\n\u003cp\u003eFor PhysiCell: \u003c/p\u003e\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://physicell.mathcancer.org\" rel=\"nofollow\"\u003ePaul Macklin\u0027s lab website \u003c/a\u003e \u003c/li\u003e\n\u003cli\u003ePhysiCell publication: A. Ghaffarizadeh, S.H. Friedman, S.M. Mumenthaler, and P. Macklin, PhysiCell: an Open Source Physics-Based \u003ca href=\"class_cell.html\" title=\"Dynamic (alive) cell (move, interact, divide, die...) \"\u003eCell\u003c/a\u003e Simulator for 3-D Multicellular Systems, bioRxiv 088773, 2016. DOI: 10.1101/088773. \u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://biofvm.mathcancer.org\" rel=\"nofollow\"\u003eBioFVM website \u003c/a\u003e \u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"namespace_bio_f_v_m.html\"\u003eBioFVM\u003c/a\u003e publication: A. Ghaffarizadeh, S.H. Friedman, and P. Macklin. \u003ca href=\"namespace_bio_f_v_m.html\"\u003eBioFVM\u003c/a\u003e: an efficient, parallelized diffusive transport solver for 3-D biological simulations. Bioinformatics, 2015. \u003c/li\u003e\n \u003cbr\u003e\n \u003c/ul\u003e\n\u003cp\u003eFor MaBoSS:\u003c/p\u003e\u003cul\u003e\n \u003cli\u003e\n\u003ca href=\"http://maboss.curie.fr\" rel=\"nofollow\"\u003eMaBoSS website \u003c/a\u003e \u003c/li\u003e\n\u003cli\u003eMaBoSS publication: Stoll G, Viara E, Barillot E, Calzone L. Continuous time Boolean modeling for biological signaling: application of Gillespie algorithm. BMC Syst Biol. 2012 Aug 29;6:116. doi: 10.1186/1752-0509-6-116. \u003c/li\u003e\n \u003cbr\u003e\n \u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-remarks\" class=\"anchor\" href=\"#remarks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemarks\u003c/h2\u003e\n\u003cp\u003ePlease, refer to the \u003ca href=\"https://github.com/gletort/PhysiBoSS/wiki\"\u003eWiki\u003c/a\u003e of this repository for a much more extended documentation, with step by step examples instructions.\u003c/p\u003e\n\u003cp\u003ePhysiCell is developed in \u003ca href=\"http://mathcancer.org\" rel=\"nofollow\"\u003ePaul Macklin\u0027s lab\u003c/a\u003e.\nMaBoSS and PhysiBoSS are developed in the \u003ca href=\"http://sysbio.curie.fr\" rel=\"nofollow\"\u003eComputational Systems Biology of Cancer group\u003c/a\u003e at Institut Curie (Paris, France).\u003c/p\u003e\n\u003cp\u003eWe invite you to use PhysiBoSS for you research and give feedbacks to us. Any help in developing it further is more than welcome.\nDo not hesitate to contact us for any comments or difficulties in using PhysiBoSS: \u003ca href=\"mailto:physiboss@gmail.com\"\u003ephysiboss@gmail.com\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWishing you to enjoy using PhysiBoSS,\u003c/p\u003e\n\u003cp\u003ePhysiBoSS\u0027s team.\u003c/p\u003e\n",
    "stargazers_count": 18,
    "subscribers_count": 5,
    "topics": [
      "cell",
      "biology",
      "boolean-logic",
      "agent-based-modeling",
      "agent-based-simulation",
      "multiscale-simulation"
    ],
    "updated_at": 1616791911.0
  },
  {
    "data_format": 2,
    "description": "All opam packages using dune",
    "filenames": [
      "packages/bistro.0.5.0/etc/singularity-images/bowtie/1.1.2/Singularity"
    ],
    "full_name": "dune-universe/dune-universe",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-dune-universe\" class=\"anchor\" href=\"#dune-universe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDune universe\u003c/h1\u003e\n\u003cp\u003eThis repository contains a snapshot of the latest versions of all opam\npackages depending on Dune (Jbuilder). It is updated daily. It is used\nto analyse the usage of Dune in opam.\u003c/p\u003e\n",
    "stargazers_count": 21,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1621838498.0
  },
  {
    "data_format": 2,
    "description": "Fused Matrix Library",
    "filenames": [
      "containers/singularity/dev/Singularity",
      "containers/singularity/dev-gpu/Singularity"
    ],
    "full_name": "fml-fam/fml",
    "latest_release": "v0.4-0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fml\" class=\"anchor\" href=\"#fml\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efml\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eVersion:\u003c/strong\u003e 0.5-0\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eStatus:\u003c/strong\u003e \u003ca href=\"https://travis-ci.org/fml-fam/fml\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8a8fb1ec27c4ea1a5dfbd38b1f0d7b8c2e6c6bf27e8c45a447acbda0b6fa5b57/68747470733a2f2f7472617669732d63692e6f72672f666d6c2d66616d2f666d6c2e706e67\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/fml-fam/fml.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eLicense:\u003c/strong\u003e \u003ca href=\"http://opensource.org/licenses/BSL-1.0\" rel=\"nofollow\"\u003eBSL-1.0\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eProject home\u003c/strong\u003e: \u003ca href=\"https://github.com/fml-fam/fml\"\u003ehttps://github.com/fml-fam/fml\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eBug reports\u003c/strong\u003e: \u003ca href=\"https://github.com/fml-fam/fml/issues\"\u003ehttps://github.com/fml-fam/fml/issues\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eDocumentation\u003c/strong\u003e: \u003ca href=\"https://fml-fam.github.io/fml\" rel=\"nofollow\"\u003ehttps://fml-fam.github.io/fml\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"./docs/logo/fml_med.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg align=\"right\" src=\"./docs/logo/fml_med.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003efml is the Fused Matrix Library, a multi-source, header-only C++ library for dense matrix computing. The emphasis is on real-valued matrix types (\u003ccode\u003efloat\u003c/code\u003e, \u003ccode\u003edouble\u003c/code\u003e, and \u003ccode\u003e__half\u003c/code\u003e) for numerical operations useful for data analysis.\u003c/p\u003e\n\u003cp\u003eThe goal of fml is to be \"medium-level\". That is, high-level compared to working directly with e.g. the BLAS or CUDA\u2122, but low(er)-level compared to other C++ matrix frameworks. Some knowledge of the use of LAPACK will make many choices in fml make more sense.\u003c/p\u003e\n\u003cp\u003eThe library provides 4 main classes: \u003ccode\u003ecpumat\u003c/code\u003e, \u003ccode\u003egpumat\u003c/code\u003e, \u003ccode\u003eparmat\u003c/code\u003e, and \u003ccode\u003empimat\u003c/code\u003e. These are mostly what they sound like, but the particular details are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU: Single node cpu computing (multi-threaded if using multi-threaded BLAS and linking with OpenMP).\u003c/li\u003e\n\u003cli\u003eGPU: Single gpu computing.\u003c/li\u003e\n\u003cli\u003eMPI: Multi-node computing via ScaLAPACK (+gpus if using \u003ca href=\"http://icl.utk.edu/slate/\" rel=\"nofollow\"\u003eSLATE\u003c/a\u003e).\u003c/li\u003e\n\u003cli\u003ePAR: Multi-node and/or multi-gpu computing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are some differences in how objects of any particular type are constructed. But the high level APIs are largely the same between the objects. The goal is to be able to quickly create laptop-scale prototypes that are then easily converted into large scale gpu/multi-node/multi-gpu/multi-node+multi-gpu codes.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eThe library is header-only so no installation is strictly necessary. You can just include a copy/submodule in your project. However, if you want some analogue of \u003ccode\u003emake install\u003c/code\u003e, then you could do something like:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eln -s ./src/fml /usr/include/\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies-and-other-software\" class=\"anchor\" href=\"#dependencies-and-other-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies and Other Software\u003c/h2\u003e\n\u003cp\u003eThere are no external header dependencies, but there are some shared libraries you need to have (more information below):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU code needs \u003ca href=\"http://performance.netlib.org/lapack/\" rel=\"nofollow\"\u003eLAPACK\u003c/a\u003e (I recommend \u003ca href=\"https://github.com/xianyi/OpenBLAS\"\u003eOpenBLAS\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eGPU code needs \u003ca href=\"https://developer.nvidia.com/cuda-downloads\" rel=\"nofollow\"\u003eNVIDIA\u00ae CUDA\u2122\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMPI code needs \u003ca href=\"http://performance.netlib.org/scalapack/\" rel=\"nofollow\"\u003eScaLAPACK\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ePAR code needs the libraries required by the CPU and/or GPU features, noted above.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOther software we use:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTests use \u003ca href=\"https://github.com/catchorg/Catch2\"\u003ecatch2\u003c/a\u003e (a copy of which is included under \u003ccode\u003etests/\u003c/code\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can find some examples of how to use the library in the \u003ccode\u003eexamples/\u003c/code\u003e tree. Right now there is no real build system beyond some ad hoc makefiles; but ad hoc is better than no hoc.\u003c/p\u003e\n\u003cp\u003eDepending on which class(es) you want to use, here are some general guidelines for using the library in your own project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU: \u003ccode\u003ecpumat\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eCompile with your favorite C++ compiler.\u003c/li\u003e\n\u003cli\u003eLink with LAPACK and BLAS (and ideally with OpenMP).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eGPU: \u003ccode\u003egpumat\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eCompile with \u003ccode\u003envcc\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eFor most functionality, link with libcudart, libcublas, and libcusolver. Link with libcurand if using the random generators.  Link with libnvidia-ml if using nvml (if you\u0027re only using this, then you don\u0027t need \u003ccode\u003envcc\u003c/code\u003e; an ordinary C++ compiler will do). If you have CUDA installed and do not know what to link with, there is no harm in linking with all of these.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMPI: \u003ccode\u003empimat\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eCompile with \u003ccode\u003empicxx\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eLink with libscalapack.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePAR: \u003ccode\u003eparmat\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eCompile with \u003ccode\u003empicxx\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eLink with CPU stuff if using \u003ccode\u003eparmat_cpu\u003c/code\u003e; link with GPU stuff if using \u003ccode\u003eparmat_gpu\u003c/code\u003e (you can use both).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck the makefiles in the \u003ccode\u003eexamples/\u003c/code\u003e tree if none of that makes sense.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h2\u003e\n\u003cp\u003eHere\u0027s a simple example computing the SVD with some data held on a single CPU:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-c++\"\u003e\u003cpre\u003e#\u003cspan class=\"pl-k\"\u003einclude\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0026lt;\u003c/span\u003efml/cpu.hh\u003cspan class=\"pl-pds\"\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eusing\u003c/span\u003e \u003cspan class=\"pl-k\"\u003enamespace\u003c/span\u003e \u003cspan class=\"pl-en\"\u003efml\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"pl-k\"\u003eint\u003c/span\u003e \u003cspan class=\"pl-en\"\u003emain\u003c/span\u003e()\n{\n  \u003cspan class=\"pl-c1\"\u003elen_t\u003c/span\u003e m = \u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e;\n  \u003cspan class=\"pl-c1\"\u003elen_t\u003c/span\u003e n = \u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e;\n  \n  cpumat\u0026lt;\u003cspan class=\"pl-k\"\u003efloat\u003c/span\u003e\u0026gt; \u003cspan class=\"pl-c1\"\u003ex\u003c/span\u003e(m, n);\n  x.\u003cspan class=\"pl-c1\"\u003efill_linspace\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e.\u003cspan class=\"pl-smi\"\u003ef\u003c/span\u003e, (\u003cspan class=\"pl-k\"\u003efloat\u003c/span\u003e)m*n);\n  \n  x.\u003cspan class=\"pl-c1\"\u003einfo\u003c/span\u003e();\n  x.\u003cspan class=\"pl-c1\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e);\n  \n  cpuvec\u0026lt;\u003cspan class=\"pl-k\"\u003efloat\u003c/span\u003e\u0026gt; s;\n  \u003cspan class=\"pl-c1\"\u003elinalg::svd\u003c/span\u003e(x, s);\n  \n  s.\u003cspan class=\"pl-c1\"\u003einfo\u003c/span\u003e();\n  s.\u003cspan class=\"pl-c1\"\u003eprint\u003c/span\u003e();\n  \n  \u003cspan class=\"pl-k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e;\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSave as \u003ccode\u003esvd.cpp\u003c/code\u003e and build with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eg++ -I/path/to/fml/src -fopenmp svd.cpp -o svd -llapack -lblas\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou should see output like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# cpumat 3x2 type=f\n1 4 \n2 5 \n3 6 \n\n# cpuvec 2 type=f\n9.5080 0.7729 \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe API is largely the same if we change the object storage, but we have to change the object initialization. For example, if \u003ccode\u003ex\u003c/code\u003e is an object of class \u003ccode\u003empimat\u003c/code\u003e, we still call \u003ccode\u003elinalg::svd(x, s)\u003c/code\u003e. The differences lie in the creation of the objects. Here is how we might change the above example to use distributed data:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-c++\"\u003e\u003cpre\u003e#\u003cspan class=\"pl-k\"\u003einclude\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0026lt;\u003c/span\u003efml/mpi.hh\u003cspan class=\"pl-pds\"\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eusing\u003c/span\u003e \u003cspan class=\"pl-k\"\u003enamespace\u003c/span\u003e \u003cspan class=\"pl-en\"\u003efml\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"pl-k\"\u003eint\u003c/span\u003e \u003cspan class=\"pl-en\"\u003emain\u003c/span\u003e()\n{\n  grid g = \u003cspan class=\"pl-c1\"\u003egrid\u003c/span\u003e(PROC_GRID_SQUARE);\n  g.\u003cspan class=\"pl-c1\"\u003einfo\u003c/span\u003e();\n  \n  \u003cspan class=\"pl-c1\"\u003elen_t\u003c/span\u003e m = \u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e;\n  \u003cspan class=\"pl-c1\"\u003elen_t\u003c/span\u003e n = \u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e;\n  \n  mpimat\u0026lt;\u003cspan class=\"pl-k\"\u003efloat\u003c/span\u003e\u0026gt; \u003cspan class=\"pl-c1\"\u003ex\u003c/span\u003e(g, m, n, \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e);\n  x.\u003cspan class=\"pl-c1\"\u003efill_linspace\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e.\u003cspan class=\"pl-smi\"\u003ef\u003c/span\u003e, (\u003cspan class=\"pl-k\"\u003efloat\u003c/span\u003e)m*n);\n  \n  x.\u003cspan class=\"pl-c1\"\u003einfo\u003c/span\u003e();\n  x.\u003cspan class=\"pl-c1\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e);\n  \n  cpuvec\u0026lt;\u003cspan class=\"pl-k\"\u003efloat\u003c/span\u003e\u0026gt; s;\n  \u003cspan class=\"pl-c1\"\u003elinalg::svd\u003c/span\u003e(x, s);\n  \n  \u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e (g.\u003cspan class=\"pl-c1\"\u003erank0\u003c/span\u003e())\n  {\n    s.\u003cspan class=\"pl-c1\"\u003einfo\u003c/span\u003e();\n    s.\u003cspan class=\"pl-c1\"\u003eprint\u003c/span\u003e();\n  }\n  \n  g.\u003cspan class=\"pl-c1\"\u003eexit\u003c/span\u003e();\n  g.\u003cspan class=\"pl-c1\"\u003efinalize\u003c/span\u003e();\n  \n  \u003cspan class=\"pl-k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e;\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn practice, using such small block sizes for an MPI matrix is probably not a good idea; we only do so for the sake of demonstration (we want each process to own some data). We can build this new example via:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003empicxx -I/path/to/fml/src svd.cpp -fopenmp  svd.cpp -o svd -lscalapack-openmpi\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe can launch the example with multiple processes via\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003empirun -np 4 ./svd\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd here we see:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e## Grid 0 2x2\n\n# mpimat 3x2 on 2x2 grid type=f\n1 4 \n2 5 \n3 6 \n\n# cpuvec 2 type=f\n9.5080 0.7729 \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-high-level-language-bindings\" class=\"anchor\" href=\"#high-level-language-bindings\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHigh-Level Language Bindings\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eR bindings: \u003ca href=\"https://github.com/fml-fam/fmlr\"\u003efmlr\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-header-and-api-stability\" class=\"anchor\" href=\"#header-and-api-stability\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeader and API Stability\u003c/h2\u003e\n\u003cp\u003etldr:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse the super headers (or read the long explanation)\n\u003cul\u003e\n\u003cli\u003eCPU - \u003ccode\u003efml/cpu.hh\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eGPU - \u003ccode\u003efml/gpu.hh\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eMPI - \u003ccode\u003efml/mpi.hh\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ePAR still evolving\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eExisting API\u0027s are largely stable. Most changes will be additions rather than modifications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe project is young and things are still mostly evolving. The current status is:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-headers\" class=\"anchor\" href=\"#headers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeaders\u003c/h3\u003e\n\u003cp\u003eThere are currently \"super headers\" for CPU (\u003ccode\u003efml/cpu.hh\u003c/code\u003e), GPU (\u003ccode\u003efml/gpu.hh\u003c/code\u003e), and MPI (\u003ccode\u003efml/mpi.hh\u003c/code\u003e) backends. These include all relevant sub-headers. These are \"frozen\" in the sense that they will not move and will always include everything. However, as more namespaces are added, those too will be included in the super headers. The headers one folder level deep (e.g. those in \u003ccode\u003efml/cpu\u003c/code\u003e) are similarly frozen, although more may be added over time. Headers two folder levels\u003c/p\u003e\n\u003cp\u003eInternals are evolving and subject to change at basically any time. Notable changes will be mentioned in the changelog.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-api\" class=\"anchor\" href=\"#api\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAPI\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eFrozen\u003c/strong\u003e: Existing APIs will not be developed further.\n\u003cul\u003e\n\u003cli\u003enone\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eStable\u003c/strong\u003e: Existing APIs are not expected to change. Some new features may be added slowly.\n\u003cul\u003e\n\u003cli\u003ecpumat/gpumat/mpimat classes\u003c/li\u003e\n\u003cli\u003ecopy namespace functions\u003c/li\u003e\n\u003cli\u003elinalg namespace functions (all but parmat)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eStabilizing\u003c/strong\u003e: Core class naming and construction/destruction is probably finalized. Function/method names and arguments are solidifying, but may change somewhat. New features are still being developed.\n\u003cul\u003e\n\u003cli\u003edimops namespace functions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eEvolving\u003c/strong\u003e: Function/method names and arguments are subject to change. New features are actively being developed.\n\u003cul\u003e\n\u003cli\u003estats namespace functions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eExperimental\u003c/strong\u003e: Nothing is remotely finalized.\n\u003cul\u003e\n\u003cli\u003eparmat - all functions and methods\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInternals are evolving and subject to change at basically any time. Notable changes will be mentioned in the changelog.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-philosophy-and-similar-projects\" class=\"anchor\" href=\"#philosophy-and-similar-projects\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePhilosophy and Similar Projects\u003c/h2\u003e\n\u003cp\u003eSome similar C/C++ projects worth mentioning:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://arma.sourceforge.net/\" rel=\"nofollow\"\u003eArmadillo\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://eigen.tuxfamily.org/\" rel=\"nofollow\"\u003eEigen\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.boost.org/\" rel=\"nofollow\"\u003eBoost\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.mcs.anl.gov/petsc/\" rel=\"nofollow\"\u003ePETSc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.gnu.org/software/gsl/\" rel=\"nofollow\"\u003eGSL\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese are all great libraries which have stood the test of time. Armadillo in particular is worthy of a look, as it has a very nice interface and very extensive set of functions. However, to my knowledge, all of these focus exclusively on CPU computing. There are some extensions to Armadillo and Eigen for GPU computing. And for gemm-heavy codes, you can use \u003ca href=\"https://docs.nvidia.com/cuda/nvblas/index.html\" rel=\"nofollow\"\u003envblas\u003c/a\u003e to offload some work to the GPU, but this doesn\u0027t always achieve good performance. And none of the above include distributed computing, except for PETSc which focuses on sparse matrices.\u003c/p\u003e\n\u003cp\u003eThere are probably many other C++ frameworks in this arena, but none to my knowledge have a similar scope to fml.\u003c/p\u003e\n\u003cp\u003eProbably the biggest influence on my thinking for this library is the \u003ca href=\"https://github.com/RBigData\"\u003epbdR package ecosystem\u003c/a\u003e for HPC with the R language, which I have worked on for many years now. Some obvious parallels are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/wrathematics/float\"\u003efloat\u003c/a\u003e - CPU/GPU\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/RBigData/kazaam\"\u003ekazaam\u003c/a\u003e - PAR\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/RBigData/pbdDMAT\"\u003epbdDMAT\u003c/a\u003e - MPI\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe basic philosophy of fml is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBe relatively small and self-contained.\u003c/li\u003e\n\u003cli\u003eFollow general C++ conventions by default (like RAII and exceptions), but give the ability to break these for the sake of performance.\u003c/li\u003e\n\u003cli\u003eChanging a code from one object type to another should be very simple, ideally with no changes to the source (the internals will simply \u003cstrong\u003eDo The Right Thing (tm)\u003c/strong\u003e), with the exception of:\n\u003cul\u003e\n\u003cli\u003eobject creation\u003c/li\u003e\n\u003cli\u003eprinting (e.g. printing on only one MPI rank)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUse a permissive open source license.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 22,
    "subscribers_count": 2,
    "topics": [
      "linear-algebra",
      "matrix",
      "blas",
      "cuda",
      "mpi",
      "scalapack",
      "hpc"
    ],
    "updated_at": 1600895582.0
  },
  {
    "data_format": 2,
    "description": "SingularityCE is the Community Edition of Singularity, an open source container platform designed to be simple, fast, and secure.",
    "filenames": [
      "examples/legacy/2.3/contrib/raspbian.def",
      "examples/legacy/2.2/centos.def",
      "examples/legacy/2.2/arch.def",
      "examples/legacy/2.2/ubuntu.def",
      "examples/legacy/2.2/docker.def",
      "examples/legacy/2.2/debian.def",
      "examples/legacy/2.2/busybox.def",
      "examples/legacy/2.2/scientific.def",
      "examples/legacy/2.2/contrib/r_python_julia.def",
      "examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1.def",
      "examples/legacy/2.2/contrib/ubuntu-root.def",
      "examples/legacy/2.2/contrib/centos7-ompi_master.def",
      "examples/legacy/2.2/contrib/fedora.def",
      "examples/legacy/2.2/contrib/ubuntu-bio.def",
      "examples/legacy/2.2/contrib/ubuntu-openfoam.def",
      "examples/legacy/2.2/contrib/centos7-ompi_cuda.def",
      "examples/legacy/2.2/contrib/linuxbrew_and_non-root_software_example.def",
      "examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1-gpu.def",
      "examples/legacy/2.2/contrib/debian85-tensorflow-0.10.def",
      "examples/legacy/2.2/contrib/centos-minimal.def",
      "examples/build-singularity/build-singularity.def",
      "e2e/testdata/Docker_registry.def",
      "e2e/testdata/sshfs.def",
      "e2e/testdata/inspecter_container.def",
      "e2e/testdata/regressions/issue_4583.def",
      "e2e/testdata/regressions/issue_5250.def",
      "e2e/testdata/regressions/issue_5399.def",
      "e2e/testdata/regressions/issue_4203.def",
      "e2e/testdata/regressions/issue_5315.def",
      "e2e/testdata/regressions/issue_4967.def",
      "e2e/testdata/regressions/issue_4969.def",
      "e2e/testdata/regressions/issue_4820.def"
    ],
    "full_name": "sylabs/singularity",
    "latest_release": "v3.7.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularityce\" class=\"anchor\" href=\"#singularityce\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularityCE\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/sylabs/singularity/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"CONTRIBUTING.md\"\u003eGuidelines for Contributing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\".github/PULL_REQUEST_TEMPLATE.md\"\u003ePull Request Template\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"LICENSE.md\"\u003eProject License\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#support\"\u003eSupport\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citing-singularity\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSingularityCE is the Community Edition of Singularity, an open source container\nplatform designed to be simple, fast, and secure. Singularity is optimized\nfor compute focused enterprise and HPC workloads, allowing untrusted users\nto run untrusted containers in a trusted way.\u003c/p\u003e\n\u003cp\u003eCheck out \u003ca href=\"https://www.sylabs.io/videos\" rel=\"nofollow\"\u003etalks about Singularity\u003c/a\u003e and some \u003ca href=\"https://sylabs.io/case-studies\" rel=\"nofollow\"\u003euse\ncases of Singularity\u003c/a\u003e on our website.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started-with-singularityce\" class=\"anchor\" href=\"#getting-started-with-singularityce\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started with SingularityCE\u003c/h2\u003e\n\u003cp\u003eTo install SingularityCE from source, see the \u003ca href=\"INSTALL.md\"\u003einstallation\ninstructions\u003c/a\u003e. For other installation options, see \u003ca href=\"https://www.sylabs.io/guides/latest/admin-guide/\" rel=\"nofollow\"\u003eour\nguide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSystem administrators can learn how to configure SingularityCE, and get an\noverview of its architecture and security features in the \u003ca href=\"https://www.sylabs.io/guides/latest/admin-guide/\" rel=\"nofollow\"\u003eadministrator\nguide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor users, see the \u003ca href=\"https://www.sylabs.io/guides/latest/user-guide/\" rel=\"nofollow\"\u003euser\nguide\u003c/a\u003e for details on how to use\nand build Singularity containers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing-to-singularityce\" class=\"anchor\" href=\"#contributing-to-singularityce\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing to SingularityCE\u003c/h2\u003e\n\u003cp\u003eCommunity contributions are always greatly appreciated. To start developing\nSingularityCE, check out the \u003ca href=\"CONTRIBUTING.md\"\u003eguidelines for contributing\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe also welcome contributions to our \u003ca href=\"https://github.com/sylabs/singularity-userdocs\"\u003euser\nguide\u003c/a\u003e and \u003ca href=\"https://github.com/sylabs/singularity-admindocs\"\u003eadmin\nguide\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupport\u003c/h2\u003e\n\u003cp\u003eTo get help with SingularityCE, check out the community spaces\ndetailed at our \u003ca href=\"https://www.sylabs.io/singularity/community/\" rel=\"nofollow\"\u003eCommunity\nPortal\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSee also our \u003ca href=\"SUPPORT.md\"\u003eSupport Guidelines\u003c/a\u003e for further\ninformation about the best place, and how, to raise different kinds of\nissues and questions.\u003c/p\u003e\n\u003cp\u003eFor additional support, \u003ca href=\"https://www.sylabs.io/contact/\" rel=\"nofollow\"\u003econtact us\u003c/a\u003e to receive\nmore information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing-singularity\" class=\"anchor\" href=\"#citing-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting Singularity\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe also have a Zenodo citation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer, Gregory M. et. al. Singularity - Linux application and environment\ncontainers for science. 10.5281/zenodo.1310023\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.5281/zenodo.1310023\" rel=\"nofollow\"\u003ehttps://doi.org/10.5281/zenodo.1310023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is an \u0027all versions\u0027 DOI. Follow the link to Zenodo to obtain a DOI specific\nto a particular version of Singularity.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eUnless otherwise noted, this project is licensed under a 3-clause BSD license\nfound in the \u003ca href=\"LICENSE.md\"\u003elicense file\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 23,
    "subscribers_count": 6,
    "topics": [
      "containers",
      "hpc",
      "linux"
    ],
    "updated_at": 1621988935.0
  },
  {
    "data_format": 2,
    "description": "APPIAN is an open-source automated software pipeline for analyzing PET images in conjunction with MRI. The goal of APPIAN is to make PET tracer kinetic data analysis easy for users with moderate computing skills and to facilitate reproducible research. ",
    "filenames": [
      "build/Singularity",
      "build/Singularity.base"
    ],
    "full_name": "APPIAN-PET/APPIAN",
    "latest_release": "v2.0.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-appian\" class=\"anchor\" href=\"#appian\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAPPIAN\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#documentation\"\u003eDocumentation\u003c/a\u003e\u003cbr\u003e\n3.1 \u003ca href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/USERGUIDE.md\"\u003eUser Guide\u003c/a\u003e\u003cbr\u003e\n3.2 \u003ca href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/CONTRIBUTING.md\"\u003eDeveloper Guide\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#publications\"\u003ePublications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#getting-help\"\u003eGetting Help\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#about-us\"\u003eAbout us\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#terms-and-conditions\"\u003eTerms and Conditions\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe APPIAN pipeline is implemented in Python using the \u003ca href=\"http://nipype.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003eNipype\u003c/a\u003e library. Although the core of the code is written in Python, the pipeline can use tools or incorporate modules written in any programming language. The only condition is that the tools must be capable of being run from a command line with well-defined inputs and outputs. In this sense, APPIAN is  language agnostic.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-cost\" class=\"anchor\" href=\"#cost\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCost\u003c/h4\u003e\n\u003cp\u003eAPPIAN is 100% free and open-source, but in exchange we would greatly appreciate your feedback, whether it be as bug reports, pull requests to add new features, questions on our \u003ca href=\"https://groups.google.com/forum/#!forum/appian-users\" rel=\"nofollow\"\u003emailing list\u003c/a\u003e, or suggestions on how to improve the documentation or the code. You can even just send us an email to let us know what kind of project you are working on!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eAPPIAN\u003c/code\u003e is currently only available through \u003ca href=\"https://docs.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e. Docker is a platform for creating containers that package a given software in a complete filesystem that contains everything it needs to run, and ensures that the software can always be run in the same environment. This means that all of the dependencies required by \u003ccode\u003eAPPIAN\u003c/code\u003e are within its Docker container (no need to fumble about trying to compile obscure libraries). However, it also means that you will need to install Singularity or Docker before proceeding. Don\u2019t worry it\u2019s very easy (except maybe for Windows). For a guide on how to install Docker on Ubuntu, Debian, Mac, Windows, or other operating system, please \u003ca href=\"https://docs.docker.com/install/\" rel=\"nofollow\"\u003evisit this link\u003c/a\u003e and [Singularity][link_singularityinstall].\u003c/p\u003e\n\u003cp\u003eThe pipeline is implemented in Python using the \u003ca href=\"https://nipype.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003eNipype\u003c/a\u003e library. Although the core is coded in Python, the pipeline can use tools or incorporate modules written in any programming language. The only condition is that these tools must be run from a command line, with well-defined inputs and outputs. In this sense, \u003ccode\u003eAPPIAN\u003c/code\u003e is  language agnostic.\nOnce Docker or Singularity is installed, simply run the following command line on your terminal:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull tffunck/appian:latest-dev\n\nsingularity pull shub://APPIAN-PET/APPIAN:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat\u2019s it, \u003ccode\u003eAPPIAN\u003c/code\u003e is installed on your computer.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-developers\" class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopers\u003c/h3\u003e\n\u003cp\u003eFor those interested in extending or contributing to APPIAN please check out our \u003ca href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/CONTRIBUTING.md\"\u003edeveloper guide\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-users\" class=\"anchor\" href=\"#users\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsers\u003c/h3\u003e\n\u003cp\u003eFor more information please read our \u003ca href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/documentation/USERGUIDE.md\"\u003euser guide\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-developers-1\" class=\"anchor\" href=\"#developers-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopers\u003c/h3\u003e\n\u003cp\u003eFor those interested in extending or contributing to APPIAN please check out our \u003ca href=\"https://github.com/APPIAN-PET/APPIAN/blob/master/CONTRIBUTING.md\"\u003econtributors guidelines\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publications\" class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePublications\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFunck T, Larcher K, Toussaint PJ, Evans AC, Thiel A (2018) APPIAN: Automated Pipeline for PET Image Analysis. \u003cem\u003eFront Neuroinform\u003c/em\u003e. PMCID: \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6178989/\" rel=\"nofollow\"\u003ePMC6178989\u003c/a\u003e, DOI: \u003ca href=\"https://doi.org/10.3389/fninf.2018.00064\" rel=\"nofollow\"\u003e10.3389/fninf.2018.00064\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAPPIAN automated QC (\u003cem\u003ein preparation\u003c/em\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting help\u003c/h2\u003e\n\u003cp\u003eIf you get stuck or don\u0027t know how to get started please send a mail to the APPIAN mailing list :\n\u003ca href=\"https://groups.google.com/forum/#!forum/appian-users\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/appian-users\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor bugs, please post \u003ca href=\"#https://github.com/APPIAN-PET/APPIAN/issues\"\u003ehere\u003c/a\u003e on the Github repository.\u003c/p\u003e\n\u003cp\u003eTo join the discussion for APPIAN development, join our developers mailing list :\n\u003ca href=\"https://groups.google.com/forum/#!forum/appian-dev\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/appian-dev\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-us\" class=\"anchor\" href=\"#about-us\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout us\u003c/h2\u003e\n\u003cp\u003eThomas Funck, PhD Candidate (\u003ca href=\"mailto:thomas.funck@mail.mcgill.ca\"\u003ethomas.funck@mail.mcgill.ca\u003c/a\u003e)\u003cbr\u003e\nKevin Larcher, MSc Eng.\u003cbr\u003e\nPaule-Joanne Toussaint, PhD\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-terms-and-conditions\" class=\"anchor\" href=\"#terms-and-conditions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTerms and Conditions\u003c/h2\u003e\n\u003cp\u003eCopyright 2017 Thomas Funck, Kevin Larcher\u003c/p\u003e\n\u003cp\u003ePermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\u003c/p\u003e\n\u003cp\u003eThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\u003c/p\u003e\n\u003cp\u003eTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 23,
    "subscribers_count": 5,
    "topics": [
      "neuroscience",
      "neuroimaging",
      "pipeline",
      "automation",
      "reproducible-research",
      "openscience"
    ],
    "updated_at": 1619163843.0
  },
  {
    "data_format": 2,
    "description": "MONET : MOdularising NEtwork Toolbox - https://doi.org/10.1093/bioinformatics/btaa236",
    "filenames": [
      ".containers/M1/singularity/Singularity",
      ".containers/K1/singularity/Singularity",
      ".containers/R1/singularity/Singularity"
    ],
    "full_name": "BergmannLab/MONET",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-monet\" class=\"anchor\" href=\"#monet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMONET\u003c/h1\u003e\n\u003cp\u003eThis repository holds the source code for \u003cstrong\u003eMONET\u003c/strong\u003e, a Linux/MacOS command-line toolbox to mine molecular and genetic networks, leveraging the top performing methods of the \u003cstrong\u003eDisease Module Identification (DMI) DREAM Challenge\u003c/strong\u003e (see DREAM Challenge paper under section PUBLICATIONS and \u003ca href=\"https://www.synapse.org/modulechallenge\" rel=\"nofollow\"\u003ehttps://www.synapse.org/modulechallenge\u003c/a\u003e)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePREREQUISITES\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eOperating System\u003c/strong\u003e: MONET can be run on \u003cstrong\u003eeither\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinux (it was tested on \u003cem\u003eUbuntu Linux\u003c/em\u003e 20.04, \u003cem\u003eCentOS Linux\u003c/em\u003e 7.5)\u003c/li\u003e\n\u003cli\u003eMacOS (it was tested on \u003cem\u003emacOS Sierra\u003c/em\u003e 10.12)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSoftware\u003c/strong\u003e: MONET requires \u003cstrong\u003eeither\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eDocker\u003c/code\u003e (see \"Install using the repository\" \u003ca href=\"https://docs.docker.com/engine/install/\" rel=\"nofollow\"\u003ehttps://docs.docker.com/engine/install/\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSingularity\u003c/code\u003e (see \u003ca href=\"http://singularity.lbl.gov\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eHardware\u003c/strong\u003e: MONET was tested both on server and on commodity hardware (i.e., regular desktop). For details, please refer to section COMPUTATIONAL RESOURCES below.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eINSTALLATION\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eJust like you can \u003ccode\u003els\u003c/code\u003e a folder, after installation will be able to \u003ccode\u003emonet\u003c/code\u003e a network\u003c/strong\u003e from any location on your system.\u003c/p\u003e\n\u003cp\u003eSimply run:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git clone https://github.com/BergmannLab/MONET.git \u0026amp;\u0026amp; cd MONET \u0026amp;\u0026amp; ./install.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eA folder MONET will have been created with the source code: you are free to remove it, if you are not interested. This will not affect MONET, which has now been installed in your system: the command \u003ccode\u003emonet\u003c/code\u003e can be invoked from any location on the system.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-need-some-more-guidance\" class=\"anchor\" href=\"#if-you-need-some-more-guidance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU NEED SOME MORE GUIDANCE\u003c/h4\u003e\n\u003cp\u003eYou can follow this \u003ca href=\"https://form.jotform.com/tomasonimattia/monet-installation\" rel=\"nofollow\"\u003esurvey-tutorial\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit will guide you step by step (assumes no prior knowledge)\u003c/li\u003e\n\u003cli\u003e(optionally) guides you through running some examples (feel free to skip those)\u003c/li\u003e\n\u003cli\u003eit will help us collect information about possible errors on different platforms\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-are-on-windows\" class=\"anchor\" href=\"#if-you-are-on-windows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU ARE ON WINDOWS\u003c/h4\u003e\n\u003cp\u003eUsers using Windows are encouraged to install a hypervisor (i.e., a software that allows to creates and run virtual machines): for example, install VirtualBox \u003ca href=\"https://www.virtualbox.org/wiki/Downloads\" rel=\"nofollow\"\u003ehttps://www.virtualbox.org/wiki/Downloads\u003c/a\u003e and configure it up to run a virtual Ubuntu Linux inside which to install MONET (using the instructions above).\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-are-a-singularity-user-without-sudo-rights\" class=\"anchor\" href=\"#if-you-are-a-singularity-user-without-sudo-rights\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU ARE A SINGULARITY USER WITHOUT SUDO RIGHTS\u003c/h4\u003e\n\u003cp\u003eSudo rights will be required at installation time for Singularity users: Singularity users will not need sudo rights while running MONET (i.e., Singularity does not require sudo right to run containers), but they will need it at installation time (i.e., at the time the Singularity images are first created).\u003c/p\u003e\n\u003cp\u003eUsers that don\u0027t have sudo rights should follow the regular installation procedure explained above, then refer to MONET/docs/installation_no_sudo.txt where they will find a workaround to complete the installation manually without needing sudo.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing-the-installation\" class=\"anchor\" href=\"#testing-the-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTESTING THE INSTALLATION\u003c/h2\u003e\n\u003cp\u003eAt the end of the install process, you will be asked whether you want to test MONET. This test is completely automatic.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-monet-help-command\" class=\"anchor\" href=\"#monet-help-command\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMONET HELP COMMAND\u003c/h2\u003e\n\u003cp\u003eAfter installing MONET, the help command \u003ccode\u003emonet --help\u003c/code\u003e will be available from any location on your system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running\" class=\"anchor\" href=\"#running\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRUNNING\u003c/h2\u003e\n\u003cp\u003eOnce installed, from any location on your system, you can run the following example command: it will run a method called M1 (see section METHODS for details), on a network contained in your /tmp folder (see section INPUT for details), using docker virtualization (see section PREREQUISITES for details). In the remainder of this document, you will find details about what parameters you can use, what to expect as an output and resource usage (in the PARAMETERS, OUTPUT and COMPUTATIONAL RESOURCES sections respectively).\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ monet --help\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ monet --input=/tmp/input_network.txt \u2014-method=M1 --container=docker\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eINPUT\u003c/h2\u003e\n\u003cp\u003eThe input file is provided to MONET using the \u003ccode\u003e--input\u003c/code\u003e parameter (see section RUNNING and section PARAMETERS).\u003c/p\u003e\n\u003cp\u003eThe format for the input network is the following: a \u003cstrong\u003etab-separated\u003c/strong\u003e file containing one line for each edge.\u003c/p\u003e\n\u003cp\u003eIf an edge is connecting two nodes, gene_a and gene_b, with a certain weight, the file will contain the line:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egene_a \\t gene_b \\t weight \\n\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eDetails:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egene_a and gene_b, the gene ids, can be either \u003cem\u003estring\u003c/em\u003e or \u003cem\u003einteger\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003eweight can be of type \u003cem\u003einteger\u003c/em\u003e or \u003cem\u003efloat\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003e\"\\t\" indicates the tab character and \"\\n\" the newline character\u003c/li\u003e\n\u003cli\u003eno blank spaces should appear, neither as separators nor as part of the gene ids\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor an example, see MONET/.test/system_test/input/zachary_karate_club.txt. The same folder containing the actual inputs to the Disease Module Identification (DMI) DREAM Challenge. Beware that some of the inputs will require high amounts of computational resources and are not suited to be run on a simple laptop or desktop computer; please refer to section COMPUTATIONAL RESOURCES for details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOUTPUT\u003c/h2\u003e\n\u003cp\u003eThe output location is provided to MONET using the \u003ccode\u003e--output\u003c/code\u003e parameter (see section OPTIONAL PARAMETERS).\u003c/p\u003e\n\u003cp\u003eTwo output files will be generated in the directory where you run the command. They are marked with a timestamp, the name of the selected method and the name of your input network. For example, let\u0027s assume if you run M1 on 1st January 2020 at midday on a file called input_network.txt:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea \u003cstrong\u003econsole-output\u003c/strong\u003e file, which will contain the run-time outputs generated by the method you have selected, providing details about the steps that the M1 algorithm took to generate your output. Any errors would also be redirected here. The file would be called: \u003ccode\u003e2020-01-01-120000__M1__console-output__input_network.txt\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ea \u003cstrong\u003eresult-modules\u003c/strong\u003e file, containing the results of your analysis and it will not be generated in case of errors. The file would be called: \u003ccode\u003e2020-01-01-120000__M1__result-modules__input_network.txt\u003c/code\u003e. It will be in tab-separated format, containing one module per line:\n\u003cul\u003e\n\u003cli\u003ethe first value of each line will be a module identifier (in the form of an integer number starting from 1)\u003c/li\u003e\n\u003cli\u003ethe second is a fixed numerical value and can be ignored (curerntly set to \u003ccode\u003e1.0\u003c/code\u003e, it was originally used in the DREAM Challenge to provide module-level confidence scores)\u003c/li\u003e\n\u003cli\u003ethe rest of the values on the line will be the gene ids container in the input (like gene_a and gene_b, see section INPUT)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-methods\" class=\"anchor\" href=\"#methods\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMETHODS\u003c/h2\u003e\n\u003cp\u003eThree methods are available as part of MONET, which emerged as the top-performing methods of the DREAM Challenge.\u003c/p\u003e\n\u003cp\u003eIn order to run one of the three methods, adapt the example command provided in section RUNNING providing the --method option with the name of the chosen method (--method=[K1|M1|R1], for details, see section PARAMETERS).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eK1\u003c/strong\u003e: KERNEL CLUSTERING OPTIMISATION algorithm. K1 is based on the \u201cDiffusion State Distance\u201d (DSD), a novel graph metric which is built on the premise that paths through low-degree nodes are stronger indications of functional similarity than paths that traverse high degree nodes by Cao et al. (2014). The DSD metric is used to define a pairwise distance matrix between all nodes, on which a spectral clustering algorithm is applied. In parallel, dense bipartite sub-graphs are identified using standard graph techniques. Finally, results are merged into a single set of non-overlapping clusters. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7349492/wiki/407359\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7349492/wiki/407359\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eM1\u003c/strong\u003e: MODULARITY OPTIMIZATION algorithm. M1 employs an original technique named Multiresolution introduced by (Arenas et al., 2008) to explore all topological scales at which modules may be found. The novelty of this approach relies on the introduction of a parameter, called resistance, which controls the aversion of nodes to form modules. Modularity (Newman and Girvan, 2004; Arenas et al., 2007) is optimized using an ensemble of algorithms: Extremal optimization (Duch and Arenas, 2005), Spectral optimization (Newman, 2006), Fast algorithm (Newman, 2004), Tabu search (Arenas et al., 2008), and fine-tuning by iterative repositioning of individual nodes in adjacent modules. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7352969/wiki/407384\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7352969/wiki/407384\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eR1\u003c/strong\u003e: RANDOM-WALK-BASED algorithm. R1 is based on a variant of Markov Cluster Algorithm known as balanced Multi-layer Regularized Markov Cluster Algorithm(bMLRMCL) (Satuluriet al., 2010) which scales well to large graphs and minimizes the number of oversized clusters. First, a pre-processing step is applied so that edges with low weights are discarded and all remaining edges are scaled to integer values. Then, bMLRMCL is applied iteratively on modules of size grater than a user-defined threshold. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7286597/wiki/406659\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7286597/wiki/406659\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePARAMETERS\u003c/h2\u003e\n\u003cp\u003ePlease, provide values for the following MANDATORY parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--input\u003c/strong\u003e: path to the network file to be analysed\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--method\u003c/strong\u003e: method to be used to analyse the input: [K1|M1|R1]\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--container\u003c/strong\u003e: virtualisation technology available on the system: [docker|singularity]\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-optional-parameters\" class=\"anchor\" href=\"#optional-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOPTIONAL PARAMETERS\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--output\u003c/strong\u003e: directory in which to output results (default is current directory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select K1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--nclusters\u003c/strong\u003e: initial number of output clusters for spectral clustering step; final number may differ (default is 100)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select M1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--smallest\u003c/strong\u003e: min size of output clusters (default is 3)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--largest\u003c/strong\u003e: max size of output clusters (default is 100)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--linksdir\u003c/strong\u003e: directionality of links: [undirected|directed] (default is undirected)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--avgk\u003c/strong\u003e: desired average degree for nodes in output (default is 25)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select R1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--smallest\u003c/strong\u003e: min size of output clusters (default is 3)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--largest\u003c/strong\u003e: max size of output clusters (default is 100)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--c\u003c/strong\u003e: trade-off parameter for computational efficiency; for larger c, the algorithm will run slower, but may provide more accurate results (default is 800)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--i\u003c/strong\u003e: inflation parameter for standard Markov Clustering algorithm on which R1 is based (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--b\u003c/strong\u003e: parameter controlling how balanced the clustering results should be; for b=0, R1 behaves like standard Regularized Markov Cluster (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--threshold\u003c/strong\u003e: remove edges smaller than threshold from the input (default is 4)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--post\u003c/strong\u003e: decide whether to recursively cluster (recluster) or discard too large output clusters: [recluster|discard] (default is discard)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--c2\u003c/strong\u003e: (only used if --post=recluster) sets --c for reclustering round (default is 500)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--i2\u003c/strong\u003e: (only used if --post=recluster) sets --i for reclustering round (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--b2\u003c/strong\u003e: (only used if --post=recluster) sets --b for reclustering round (default is 2)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-computational-resources\" class=\"anchor\" href=\"#computational-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCOMPUTATIONAL RESOURCES\u003c/h2\u003e\n\u003cp\u003eSome of the methods require large amount of resources, depending on your input (please, refer to the MONET paper in the PUBLICATIONS section for details about how resource needs will scale with the size of the input, for the different methods).\u003c/p\u003e\n\u003cp\u003eTo reproduce the results of the DREAM Challenge, you can run MONET/.test/system_test/reproduce_challenge/reproduce_challenge.sh. This might fail on commodity hardware (i.e., a regular laptop or desktop) as about 8GB or RAM need to be available. In that case, you can allocate a larger SWAP partition (on Linux) or run the experiment on more powerful hardware, such as a server. Please browser the rest of the contents of MONET/.test/system_test/reproduce_challenge to view the exact RAM usage (ram_usage.txt) and the challenge outputs produced by MONET (disease_modules_output directory).\u003c/p\u003e\n\u003cp\u003eTo monitor resource usage when running on your own input (and thus determine the amount or RAM / swap needed by your particular input network for a particular method), two simple scripts have been added to MONET/.test/helper_scripts (for Unix and one for MacOS systems): launch them before execution of MONET and redirect their output to file for simple inspection (no other task should be running).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-benchmarking\" class=\"anchor\" href=\"#benchmarking\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBENCHMARKING\u003c/h2\u003e\n\u003cp\u003eFor details about the modularization performance of the MONET methods on a set of artificial benchmarks (Louvain algorithm is shown as a baseline), please refer to the MONET paper in the PUBLICATIONS section; in particular, Fig. 1. MONET/.test/benchmarking for a detailed output of the experiments that have been carried out.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-source-code\" class=\"anchor\" href=\"#source-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSOURCE CODE\u003c/h2\u003e\n\u003cp\u003eThe source code is hosted at: \u003ca href=\"https://github.com/BergmannLab/MONET.git\"\u003ehttps://github.com/BergmannLab/MONET.git\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCONTRIBUTING\u003c/h2\u003e\n\u003cp\u003eIf you are interested in contributing to MONET, we encourage you to get in touch! We will be happy to add you to the list of our developers \u003ca href=\"https://github.com/BergmannLab/MONET/graphs/contributors\"\u003ehttps://github.com/BergmannLab/MONET/graphs/contributors\u003c/a\u003e. \u003cstrong\u003eTHANK YOU!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - CREATING A BRANCH\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFirst, we will create an issue for the specific feature you are willing to contribute; let\u0027s say yours will happen to be issue 999. You will be then asked to create a new git branch where to implement your changes; run the following from the cloned MONET directory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git checkout -b issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git push origin issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAt this point, you are free to make changes to your local code in your laptop. Don\u0027t worry if you mess things up, it\u0027s no problem to add mistakes to a branch.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - TESTING YOUR CHANGES\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce you are done with your changes, you can test them locally by \u003cstrong\u003ereinstalling\u003c/strong\u003e from the modified MONET directory.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - PUBLISHING YOUR CHANGES\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce you have tested your changes, run the following from the cloned MONET directory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git add .\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git commit -m \"adding code for feature # issues_999\"\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git push --set-upstream origin issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git checkout master\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOne of the MONET developers will test the changes in your branch then merge to Master.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-implementing-local-changes-to-monet\" class=\"anchor\" href=\"#implementing-local-changes-to-monet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIMPLEMENTING LOCAL CHANGES TO MONET\u003c/h2\u003e\n\u003cp\u003eIf you wish to implement local changes to MONET, independently from our github repository, you can simply modify the code in your local cloned repository and \u003cstrong\u003ereinstall\u003c/strong\u003e after having made those changes (i.e. run or re-run the \u003ccode\u003einstall.sh\u003c/code\u003e script and confirm if you are asked to reinstall). This procedure can be repeated as many times as you like.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting-common-problems\" class=\"anchor\" href=\"#troubleshooting-common-problems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTROUBLESHOOTING COMMON PROBLEMS\u003c/h2\u003e\n\u003cp\u003eIf a MONET run is suddenly interrupted or if the expected outputs has not been generated, here are few common problems that can occur:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elack of RAM: if the console-output file (see section OUTPUT) contains the word \"Killed\", the MONET processed were stopped by the Operating System, likely due to a lack of RAM. To confirm this, please read section COMPUTATIONAL RESOURCES to learn how to monitor your resource usage while running MONET.\u003c/li\u003e\n\u003cli\u003eoutdated kernel: Singularity users that work on Linux distributions with old kernels (e.g. CentOS 6.1, kernel 2.6) will encounter trouble during the install process; they need to contact their system administrator to inquire whether a kernel upgrade is possible.\u003c/li\u003e\n\u003cli\u003ecan\u0027t implement local changes: please, refer to section IMPLEMENTING LOCAL CHANGES TO MONET.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bug-reports\" class=\"anchor\" href=\"#bug-reports\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBUG-REPORTS\u003c/h2\u003e\n\u003cp\u003ePlease, address your questions and bug reports to Mattia Tomasoni, \u0026lt;mattia.tomasoni AT unil.ch\u0026gt;. An issue will be opened here to address your problem: \u003ca href=\"https://github.com/BergmannLab/MONET/issues\"\u003ehttps://github.com/BergmannLab/MONET/issues\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publications\" class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePUBLICATIONS\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMONET paper\u003c/strong\u003e: Mattia Tomasoni, Sergio G\u00f3mez, Jake Crawford, Weijia Zhang, Sarvenaz Choobdar, Daniel Marbach and Sven Bergmann. MONET: a toolbox integrating top-performing methods for network modularization. Bioinformatics 36 (12), 3920-3921. doi: \u003ca href=\"https://doi.org/10.1093/bioinformatics/btaa236\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/bioinformatics/btaa236\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDREAM Challenge paper\u003c/strong\u003e: Sarvenaz Choobdar, Mehmet Ahsen, Jake Crawford, Mattia Tomasoni, Tao Fang, David Lamparter, Junyuan Lin, Benjamin Hescott, Xiaozhe Hu, Johnathan Mercer, Ted Natoli, Rajiv Narayan, The DREAM Module Identification Challenge Consortium, Aravind Subramanian, Jitao David Zhang, Gustavo Stolovitzky, Zolt\u00e1n Kutalik, Kasper Lage, Donna Slonim, Julio Saez-Rodriguez, Lenore Cowen, Sven Bergmann, Daniel Marbach. Assessment of network module identification across complex diseases. Nature Methods 16 (2019) 843-852. doi: \u003ca href=\"https://doi.org/10.1038/s41592-019-0509-5\" rel=\"nofollow\"\u003ehttps://doi.org/10.1038/s41592-019-0509-5\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 29,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1620780040.0
  },
  {
    "data_format": 2,
    "description": "A repository of definition files for bootstrapping Singularity containers around the software applications, frameworks, and libraries you need to run on high-performance computing systems.",
    "filenames": [
      "definition-files/tensorflow/Singularity.tensorflow-2.3.0",
      "definition-files/pytorch/Singularity.pytorch-1.5.0",
      "definition-files/visit/Singularity.visit-3.1.4-ubuntu-18.04-openmpi-3.1.6",
      "definition-files/anaconda/Singularity.anaconda2-py27-2019.10-ubuntu-18.04",
      "definition-files/anaconda/Singularity.anaconda3-py38-2020.11-ubuntu-18.04",
      "definition-files/mxnet/Singularity.mxnet-1.7.0",
      "definition-files/miniconda/Singularity.miniconda3-py37-4.9.2-ubuntu-18.04",
      "definition-files/miniconda/Singularity.miniconda2-py27-4.8.3-ubuntu-18.04",
      "definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-mvapich-2.3.2",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-mvapich-2.3.2",
      "definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-openmpi-4.0.5",
      "definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-cuda-11.2-openmpi-4.0.5",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.6",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.4",
      "definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-openmpi-3.1.4",
      "definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5",
      "definition-files/paraview/Singularity.paraview-5.9.0-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5",
      "definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.4-osmesa-20.1.5",
      "definition-files/xcrysden/Singularity.xcrysden-1.6.2",
      "definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.4-openblas-0.3.10",
      "definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.6-openblas-0.3.10",
      "definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-4.0.5-openblas-0.3.14",
      "definition-files/stream/Singularity.stream-5.10-ubuntu-18.04",
      "definition-files/spark/Singularity.spark-2.3.1-hadoop-2.7-ubuntu-18.04",
      "definition-files/fenics/Singularity.fenics-2019.1.0",
      "definition-files/ciml/Singularity.esm-0.3.1",
      "definition-files/ciml/Singularity.tape-0.4",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2-openmpi-3.1.6",
      "definition-files/ubuntu/Singularity.ubuntu-18.04",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.6",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2-openmpi-4.0.5",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-4.0.5",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2",
      "definition-files/ubuntu/Singularity.ubuntu-20.04",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.4",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-mvapich-2.3.2",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4",
      "definition-files/beast/Singularity.beast-2.6.1",
      "definition-files/beast/Singularity.beast-1.10.4",
      "definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.6",
      "definition-files/ior/Singularity.ior-3.3.0-ubuntu-18.04-openmpi-4.0.5",
      "definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.4",
      "definition-files/singularity/Singularity.singularity-3.5.3",
      "definition-files/centos/Singularity.centos-7.9.2009-openmpi-3.1.4",
      "definition-files/centos/Singularity.centos-7.9.2009",
      "definition-files/centos/Singularity.centos-7.9.2009-cuda-10.1.168",
      "definition-files/centos/Singularity.centos-7.9.2009-mvapich-2.3.2",
      "definition-files/gromacs/Singularity.gromacs-2020.3-ubuntu-18.04-cuda-10.1.168-tmpi-avx-512-cuda-70"
    ],
    "full_name": "mkandes/naked-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-naked-singularity\" class=\"anchor\" href=\"#naked-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enaked-singularity\u003c/h1\u003e\n\u003cp\u003eA repository of definition files for building\n\u003ca href=\"https://sylabs.io/guides/latest/user-guide\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e containers\naround the software applications, frameworks, and libraries you need to\nrun on high-performance computing systems.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install-singularity\" class=\"anchor\" href=\"#install-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Singularity\u003c/h2\u003e\n\u003cp\u003eInstall Singularity on your Linux desktop, laptop, or virtual machine.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./naked-singularity.sh install\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-a-singularity-container-from-a-definition-file\" class=\"anchor\" href=\"#build-a-singularity-container-from-a-definition-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild a Singularity container from a definition file\u003c/h2\u003e\n\u003cp\u003eBuild an Ubuntu Singularity container from one of the definition files\navailable in this repository.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build ubuntu.sif definition-files/ubuntu/Singularity.ubuntu-18.04\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download-an-existing-singularity-container\" class=\"anchor\" href=\"#download-an-existing-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload an existing Singularity container\u003c/h2\u003e\n\u003cp\u003eA number of pre-built containers from this repository are also now\nhosted at Singularity Hub.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull shub://mkandes/naked-singularity:ubuntu-18.04\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-status\" class=\"anchor\" href=\"#status\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStatus\u003c/h2\u003e\n\u003cp\u003eA work in progress. But an important note on the immediate future ...\u003c/p\u003e\n\u003cp\u003eThe future availablity of Singularity Hub is currently unknown and\nmay be slated for retirement as early as April 2021. If and when\nSingularity Hub is retired, the current set of naked-sinularity\ndefinition files will be affected due to the recent move in the past\nyear to using a layered dependency chain of multiple containers built\nand hosted on Singularity Hub. Alternative container build and hosting\noptions are currently under consideration. This upcoming change will\nlikely affect how a given container\u0027s dependency chain is rebuilt and\nsupported long-term.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contribute\" class=\"anchor\" href=\"#contribute\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContribute\u003c/h2\u003e\n\u003cp\u003eIf you would like to contribute one of your own Singularity container\ndefinition files for a specific application OR request a modification to\nan existing container definition, then please submit a pull request.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-author\" class=\"anchor\" href=\"#author\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthor\u003c/h2\u003e\n\u003cp\u003eMarty Kandes, Ph.D.\u003cbr\u003e\nComputational \u0026amp; Data Science Research Specialist\u003cbr\u003e\nHigh-Performance Computing User Services Group\u003cbr\u003e\nSan Diego Supercomputer Center\u003cbr\u003e\nUniversity of California, San Diego\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-version\" class=\"anchor\" href=\"#version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion\u003c/h2\u003e\n\u003cp\u003e1.3.3\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-last-updated\" class=\"anchor\" href=\"#last-updated\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLast Updated\u003c/h2\u003e\n\u003cp\u003eSaturday, March 13th, 2021\u003c/p\u003e\n",
    "stargazers_count": 30,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1620436929.0
  },
  {
    "data_format": 2,
    "description": "Collection of hyperparameter optimization benchmark problems",
    "filenames": [
      "hpobench/container/recipes/Singularity.template",
      "hpobench/container/recipes/surrogates/Singularity.ParamnetBenchmark",
      "hpobench/container/recipes/nas/Singularity.nasbench_1shot1",
      "hpobench/container/recipes/nas/Singularity.nasbench_101",
      "hpobench/container/recipes/nas/Singularity.TabularBenchmarks",
      "hpobench/container/recipes/nas/Singularity.nasbench_201",
      "hpobench/container/recipes/rl/Singularity.learnaBenchmark",
      "hpobench/container/recipes/rl/Singularity.Cartpole",
      "hpobench/container/recipes/ml/Singularity.PyBNN",
      "hpobench/container/recipes/ml/Singularity.XGBoostBenchmark",
      "hpobench/container/recipes/ml/Singularity.SupportVectorMachine"
    ],
    "full_name": "automl/HPOBench",
    "latest_release": "v0.0.5",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpobench\" class=\"anchor\" href=\"#hpobench\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPOBench\u003c/h1\u003e\n\u003cp\u003eHPOBench is a library for hyperparameter optimization and black-box optimization benchmark with a focus on reproducibility.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e HPOBench is under active construction. Stay tuned for more benchmarks. Information on how to contribute a new benchmark will follow shortly.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e If you are looking for a different or older version of our benchmarking library, you might be looking for\n\u003ca href=\"https://github.com/automl/HPOlib1.5\"\u003eHPOlib1.5\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-in-4-lines-of-code\" class=\"anchor\" href=\"#in-4-lines-of-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIn 4 lines of code\u003c/h2\u003e\n\u003cp\u003eRun a random configuration within a singularity container\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003econtainer\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003econtainer_source\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027library://phmueller/automl\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"dataset_fraction\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e}, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAll benchmarks can also be queried with fewer or no fidelities:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003econtainer\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003econtainer_source\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027library://phmueller/automl\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e,}, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eContainerized benchmarks do not rely on external dependencies and thus do not change. To do so, we rely on \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003eSingularity (version 3.5)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFurther requirements are: \u003ca href=\"https://github.com/automl/ConfigSpace\"\u003eConfigSpace\u003c/a\u003e, \u003cem\u003escipy\u003c/em\u003e and \u003cem\u003enumpy\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Each benchmark can also be run locally, but the dependencies must be installed manually and might conflict with other benchmarks.\nThis can be arbitrarily complex and further information can be found in the docstring of the benchmark.\u003c/p\u003e\n\u003cp\u003eA simple example is the XGBoost benchmark which can be installed with \u003ccode\u003epip install .[xgboost]\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"dataset_fraction\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e}, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eBefore we start, we recommend using a virtual environment. To run any benchmark using its singularity container,\nrun the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/automl/HPOBench.git\ncd HPOBench \npip install .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e This does not install \u003cem\u003esingularity (version 3.5)\u003c/em\u003e. Please follow the steps described here: \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003euser-guide\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-available-containerized-benchmarks\" class=\"anchor\" href=\"#available-containerized-benchmarks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailable Containerized Benchmarks\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"left\"\u003eBenchmark Name\u003c/th\u003e\n\u003cth\u003eContainer Name\u003c/th\u003e\n\u003cth\u003eAdditional Info\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eBNNOn*\u003c/td\u003e\n\u003ctd\u003epybnn\u003c/td\u003e\n\u003ctd\u003eThere are 4 benchmark in total (ToyFunction, BostonHousing, ProteinStructure, YearPrediction)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eCartpoleFull\u003c/td\u003e\n\u003ctd\u003ecartpole\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eCartpoleReduced\u003c/td\u003e\n\u003ctd\u003ecartpole\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eSliceLocalizationBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eProteinStructureBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eNavalPropulsionBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eParkinsonsTelemonitoringBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eNASCifar10*Benchmark\u003c/td\u003e\n\u003ctd\u003enasbench_101\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes. There are 3 benchmark in total (A, B, C)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003e*NasBench201Benchmark\u003c/td\u003e\n\u003ctd\u003enasbench_201\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes. There are 3 benchmarks in total (Cifar10Valid, Cifar100, ImageNet)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eNASBench1shot1SearchSpace*Benchmark\u003c/td\u003e\n\u003ctd\u003enasbench_1shot1\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes. There are 3 benchmarks in total (1,2,3)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eParamNet*OnStepsBenchmark\u003c/td\u003e\n\u003ctd\u003eparamnet\u003c/td\u003e\n\u003ctd\u003eThere are 6 benchmarks in total (Adult, Higgs, Letter, Mnist, Optdigits, Poker)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eParamNet*OnTimeBenchmark\u003c/td\u003e\n\u003ctd\u003eparamnet\u003c/td\u003e\n\u003ctd\u003eThere are 6 benchmarks in total (Adult, Higgs, Letter, Mnist, Optdigits, Poker)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eLearna\u207a\u003c/td\u003e\n\u003ctd\u003elearna_benchmark\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eMetaLearna\u207a\u003c/td\u003e\n\u003ctd\u003elearna_benchmark\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eXGBoostBenchmark\u207a\u003c/td\u003e\n\u003ctd\u003exgboost_benchmark\u003c/td\u003e\n\u003ctd\u003eWorks with OpenML task ids.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eXGBoostExtendedBenchmark\u207a\u003c/td\u003e\n\u003ctd\u003exgboost_benchmark\u003c/td\u003e\n\u003ctd\u003eWorks with OpenML task ids + Contains Additional Parameter `Booster\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eSupportVectorMachine\u207a\u003c/td\u003e\n\u003ctd\u003esvm_benchmark\u003c/td\u003e\n\u003ctd\u003eWorks with OpenML task ids.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u207a these benchmarks are not yet final and might change\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e All containers are uploaded \u003ca href=\"https://gitlab.tf.uni-freiburg.de/muelleph/hpobench-registry/container_registry\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-further-notes\" class=\"anchor\" href=\"#further-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFurther Notes\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-configure-the-hpobench\" class=\"anchor\" href=\"#configure-the-hpobench\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfigure the HPOBench\u003c/h3\u003e\n\u003cp\u003eAll of HPOBench\u0027s settings are stored in a file, the \u003ccode\u003ehpobenchrc\u003c/code\u003e-file.\nIt is a yaml file, which is automatically generated at the first use of HPOBench.\nBy default, it is placed in \u003ccode\u003e$XDG_CONFIG_HOME\u003c/code\u003e. If \u003ccode\u003e$XDG_CONFIG_HOME\u003c/code\u003e is not set, then the\n\u003ccode\u003ehpobenchrc\u003c/code\u003e-file is saved to \u003ccode\u003e\u0027~/.config/hpobench\u0027\u003c/code\u003e.\nMake sure to have write permissions in this directory.\u003c/p\u003e\n\u003cp\u003eIn the \u003ccode\u003ehpobenchrc\u003c/code\u003e, you can specify for example the directory, in that the benchmark-containers are\ndownloaded. We encourage you to take a look into the \u003ccode\u003ehpobenchrc\u003c/code\u003e, to find out more about all\npossible settings.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to-build-a-container-locally\" class=\"anchor\" href=\"#how-to-build-a-container-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to build a container locally\u003c/h3\u003e\n\u003cp\u003eWith singularity installed run the following to built the xgboost container\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e hpobench/container/recipes/ml\nsudo singularity build xgboost_benchmark Singularity.XGBoostBenchmark\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can use this local image with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003econtainer\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003econtainer_name\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"xgboost_benchmark\"\u003c/span\u003e, \n                     \u003cspan class=\"pl-s1\"\u003econtainer_source\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027./\u0027\u003c/span\u003e) \u003cspan class=\"pl-c\"\u003e# path to hpobench/container/recipes/ml\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"dataset_fraction\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e})\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-remove-all-caches\" class=\"anchor\" href=\"#remove-all-caches\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemove all caches\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-hpobench-data\" class=\"anchor\" href=\"#hpobench-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPOBench data\u003c/h4\u003e\n\u003cp\u003eHPOBench stores downloaded containers and datasets at the following locations:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-smi\"\u003e$XDG_CONFIG_HOME\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e ~/.config/hpobench\u003c/span\u003e\n\u003cspan class=\"pl-smi\"\u003e$XDG_CACHE_HOME\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e ~/.cache/hpobench\u003c/span\u003e\n\u003cspan class=\"pl-smi\"\u003e$XDG_DATA_HOME\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e ~/.local/share/hpobench\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor crashes or when not properly shutting down containers, there might be socket files left under \u003ccode\u003e/tmp/\u003c/code\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-openml-data\" class=\"anchor\" href=\"#openml-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpenML data\u003c/h4\u003e\n\u003cp\u003eOpenML data additionally maintains it\u0027s own cache which is located at \u003ccode\u003e~/.openml/\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container\u003c/h4\u003e\n\u003cp\u003eSingularity additionally maintains it\u0027s own cache which can be removed with \u003ccode\u003esingularity cache clean\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity throws an \u0027Invalid Image format\u0027 exception\u003c/strong\u003e\nUse a singularity version \u0026gt; 3. For users of the Meta-Cluster in Freiburg, you have to set the following path:\n\u003ccode\u003eexport PATH=/usr/local/kislurm/singularity-3.5/bin/:$PATH\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eA Benchmark fails with \u003ccode\u003eSystemError: Could not start a instance of the benchmark. Retried 5 times\u003c/code\u003e but the container\ncan be started locally with \u003ccode\u003esingularity instance start \u0026lt;pathtocontainer\u0026gt; test\u003c/code\u003e\u003c/strong\u003e\nSee whether in \u003ccode\u003e~/.singularity/instances/sing/$HOSTNAME/*/\u003c/code\u003e there is a file that does not end with \u0027}\u0027. If yes delete this file and retry.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-status\" class=\"anchor\" href=\"#status\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStatus\u003c/h2\u003e\n\u003cp\u003eStatus for Master Branch:\n\u003ca href=\"https://https://github.com/automl/HPOBench/actions\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/automl/HPOBench/workflows/Test%20Pull%20Requests/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/automl/HPOBench\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a034097751613923b89642227271f5932b385694eb079b833b66cc6bce2b924a/68747470733a2f2f636f6465636f762e696f2f67682f6175746f6d6c2f48504f42656e63682f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/automl/HPOBench/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStatus for Development Branch:\n\u003ca href=\"https://https://github.com/automl/HPOBench/actions\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/automl/HPOBench/workflows/Test%20Pull%20Requests/badge.svg?branch=development\" alt=\"Build Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/automl/HPOBench\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/86b984292042d233294c09940c1b7dade7c877c1220bc419f2e2227eb0f775f0/68747470733a2f2f636f6465636f762e696f2f67682f6175746f6d6c2f48504f42656e63682f6272616e63682f646576656c6f706d656e742f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/automl/HPOBench/branch/development/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 31,
    "subscribers_count": 7,
    "topics": [
      "hyperparameter-optimization",
      "benchmarking",
      "bayesian-optimization",
      "benchmark",
      "containerized-benchmarks",
      "hpolib"
    ],
    "updated_at": 1621464102.0
  },
  {
    "data_format": 2,
    "description": "Supervised and RL Models for No Press Diplomacy",
    "filenames": [
      "diplomacy_research/containers/redis/Singularity",
      "diplomacy_research/containers/ubuntu-cuda10/Singularity",
      "diplomacy_research/containers/albert-ai/Singularity",
      "diplomacy_research/containers/research/Singularity",
      "diplomacy_research/containers/tensorflow-serving/Singularity"
    ],
    "full_name": "diplomacy/research",
    "latest_release": "v1.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-supervised-and-rl-models-for-no-press-diplomacy\" class=\"anchor\" href=\"#supervised-and-rl-models-for-no-press-diplomacy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupervised and RL Models for No Press Diplomacy\u003c/h1\u003e\n\u003cp\u003eThis repository contains the source code used to develop a supervised and RL agent that can play the No Press version of Diplomacy.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"docs/images/map_overview.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"500\" src=\"docs/images/map_overview.png\" alt=\"Diplomacy Map Overview\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis project is licensed under the MIT License - see the \u003ca href=\"LICENSE\"\u003eLICENSE\u003c/a\u003e file for details.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRestrictions: The trained weights provided with this repository are for research purposes only and cannot be used to power any bots on any website without my prior written consent, which may be withheld without reasons.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe data provider also prevents using its data to train any bots accessible on any website.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eYou can play against the trained model by playing against \"KestasBot\" on webdiplomacy.net\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dataset\" class=\"anchor\" href=\"#dataset\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDataset\u003c/h2\u003e\n\u003cp\u003eThe model was trained by using a dataset of 156,468 games (diplomacy-v1-27k-msgs.zip), which consists of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e16,633 games on non-standard maps (e.g. modern and ancmed) (other_maps.jsonl)\u003c/li\u003e\n\u003cli\u003e33,279 no-press games on the standard map (standard_no_press.jsonl)\u003c/li\u003e\n\u003cli\u003e50 press games on the standard map with messages (standard_press_with_msgs.jsonl)\u003c/li\u003e\n\u003cli\u003e106,456 press games on the standard map without messages (standard_press_without_msgs.jsonl)\u003c/li\u003e\n\u003cli\u003e50 public press games on the standard map with messages (standard_public_press.jsonl)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA dataset of 156,458 games with 13,469,536 messages is also being prepared, but it is not yet available.\u003c/p\u003e\n\u003cp\u003eAccess to the dataset used to train the model can be requested by sending an email to \u003ca href=\"mailto:webdipmod@gmail.com\"\u003ewebdipmod@gmail.com\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eThe repository can be installed in a conda environment with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-s1\"\u003econda\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ecreate\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003en\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ediplomacy\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003econda\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eactivate\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ediplomacy\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003epip\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003einstall\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003er\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erequirements\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003etxt\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003epip\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003einstall\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e-\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003er\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erequirements_dev\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003etxt\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis package depends on Redis and singularity 3+. Singularity can be installed with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Installing Singularity v3.2.0\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e VERSION=v3.2.0\nsudo apt-get update -y\nsudo apt-get install -y build-essential libssl-dev uuid-dev libgpgme11-dev libseccomp-dev pkg-config squashfs-tools\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Installing GO 1.12.5\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e GO_VERSION=1.12.5 OS=linux ARCH=amd64\nwget -nv https://dl.google.com/go/go\u003cspan class=\"pl-smi\"\u003e$GO_VERSION\u003c/span\u003e.\u003cspan class=\"pl-smi\"\u003e$OS\u003c/span\u003e-\u003cspan class=\"pl-smi\"\u003e$ARCH\u003c/span\u003e.tar.gz\nsudo tar -C /usr/local -xzf go\u003cspan class=\"pl-smi\"\u003e$GO_VERSION\u003c/span\u003e.\u003cspan class=\"pl-smi\"\u003e$OS\u003c/span\u003e-\u003cspan class=\"pl-smi\"\u003e$ARCH\u003c/span\u003e.tar.gz\nrm -f go\u003cspan class=\"pl-smi\"\u003e$GO_VERSION\u003c/span\u003e.\u003cspan class=\"pl-smi\"\u003e$OS\u003c/span\u003e-\u003cspan class=\"pl-smi\"\u003e$ARCH\u003c/span\u003e.tar.gz\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e GOPATH=\u003cspan class=\"pl-smi\"\u003e$HOME\u003c/span\u003e/.go\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=/usr/local/go/bin:\u003cspan class=\"pl-smi\"\u003e${PATH}\u003c/span\u003e:\u003cspan class=\"pl-smi\"\u003e${GOPATH}\u003c/span\u003e/bin\nmkdir -p \u003cspan class=\"pl-smi\"\u003e$GOPATH\u003c/span\u003e\ngo get github.com/golang/dep/cmd/dep\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Building from source\u003c/span\u003e\nmkdir -p \u003cspan class=\"pl-smi\"\u003e$GOPATH\u003c/span\u003e/src/github.com/sylabs\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$GOPATH\u003c/span\u003e/src/github.com/sylabs\ngit clone https://github.com/sylabs/singularity.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e singularity\ngit checkout \u003cspan class=\"pl-smi\"\u003e$VERSION\u003c/span\u003e\n./mconfig -p /usr/local\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e ./builddir\nmake\nsudo make install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe package is compatible with Python 3.5, 3.6, and 3.7.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-training-models\" class=\"anchor\" href=\"#training-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTraining models\u003c/h3\u003e\n\u003cp\u003eTo train a model:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e WORKING_DIR=/path/to/some/directory\n$ cp diplomacy-v1-27k-msgs.zip \u003cspan class=\"pl-smi\"\u003e$WORKING_DIR\u003c/span\u003e\n$ conda activate diplomacy\n$ python diplomacy_research/scripts/build_dataset.py\n$ python diplomacy_research/models/policy/order_based/train.py --model_id 12\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-playing-against-the-sl-and-rl-agents\" class=\"anchor\" href=\"#playing-against-the-sl-and-rl-agents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlaying against the SL and RL agents\u003c/h3\u003e\n\u003cp\u003eIt is possible to play against the published results by using the \u003ccode\u003eDipNetSLPlayer\u003c/code\u003e and \u003ccode\u003eDipNetRLPlayer\u003c/code\u003e players in \u003ccode\u003ediplomacy_research.players.benchmark_player\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThese players will automatically download a singularity container with the trained weights, and then launch a TF serving server to handle the requests.\u003c/p\u003e\n\u003cp\u003eA simple example on how to play a 7 bots game is:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003etornado\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egen\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eujson\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ediplomacy\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eGame\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ediplomacy\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eutils\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eexport\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eto_saved_game_format\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ediplomacy_research\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eplayers\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmark_player\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eDipNetSLPlayer\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ediplomacy_research\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eutils\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ecluster\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003estart_io_loop\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003estop_io_loop\u003c/span\u003e\n\n\u003cspan class=\"pl-en\"\u003e@\u003cspan class=\"pl-s1\"\u003egen\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ecoroutine\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003edef\u003c/span\u003e \u003cspan class=\"pl-en\"\u003emain\u003c/span\u003e():\n    \u003cspan class=\"pl-s\"\u003e\"\"\" Plays a local game with 7 bots \"\"\"\u003c/span\u003e\n    \u003cspan class=\"pl-s1\"\u003eplayer\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eDipNetSLPlayer\u003c/span\u003e()\n    \u003cspan class=\"pl-s1\"\u003egame\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eGame\u003c/span\u003e()\n\n    \u003cspan class=\"pl-c\"\u003e# Playing game\u003c/span\u003e\n    \u003cspan class=\"pl-k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003enot\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egame\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eis_game_done\u003c/span\u003e:\n        \u003cspan class=\"pl-s1\"\u003eorders\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eyield\u003c/span\u003e {\u003cspan class=\"pl-s1\"\u003epower_name\u003c/span\u003e: \u003cspan class=\"pl-s1\"\u003eplayer\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_orders\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003egame\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003epower_name\u003c/span\u003e) \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003epower_name\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egame\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003epowers\u003c/span\u003e}\n        \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003epower_name\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003epower_orders\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eorders\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eitems\u003c/span\u003e():\n            \u003cspan class=\"pl-s1\"\u003egame\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eset_orders\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003epower_name\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003epower_orders\u003c/span\u003e)\n        \u003cspan class=\"pl-s1\"\u003egame\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eprocess\u003c/span\u003e()\n\n    \u003cspan class=\"pl-c\"\u003e# Saving to disk\u003c/span\u003e\n    \u003cspan class=\"pl-k\"\u003ewith\u003c/span\u003e \u003cspan class=\"pl-en\"\u003eopen\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\u0027game.json\u0027\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027w\u0027\u003c/span\u003e) \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003efile\u003c/span\u003e:\n        \u003cspan class=\"pl-s1\"\u003efile\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003ewrite\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ejson\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003edumps\u003c/span\u003e(\u003cspan class=\"pl-en\"\u003eto_saved_game_format\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003egame\u003c/span\u003e)))\n    \u003cspan class=\"pl-en\"\u003estop_io_loop\u003c/span\u003e()\n\n\u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003e__name__\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e==\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u0027__main__\u0027\u003c/span\u003e:\n    \u003cspan class=\"pl-en\"\u003estart_io_loop\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003emain\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-playing-against-a-model\" class=\"anchor\" href=\"#playing-against-a-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlaying against a model\u003c/h3\u003e\n\u003cp\u003eIt is also possible for humans to play against bots using the web interface. The player can be changed in \u003ccode\u003ediplomacy_research.scripts.launch_bot\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e In a terminal window or tab - Launch React server (from diplomacy/diplomacy)\u003c/span\u003e\nnpm start\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e In another terminal window or tab - Launch diplomacy server\u003c/span\u003e\npython -m diplomacy.server.run\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e In a third terminal window or tab - Launch the bot script\u003c/span\u003e\npython diplomacy_research/scripts/launch_bot.py\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-trained-weights-and-experiment-logs\" class=\"anchor\" href=\"#trained-weights-and-experiment-logs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTrained weights and experiment logs\u003c/h3\u003e\n\u003cp\u003eTo facilitate reproducibility, the experiments can be downloaded using the following links. These include hyperparameters, tensorboard graphs, output logs, and weights for each epoch.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOrder based LSTM model (order-based v12 - Accuracy of 61.3% - \u003cstrong\u003eDipNet SL\u003c/strong\u003e) \u003ca href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/order-based-lstm.zip\" rel=\"nofollow\"\u003eDownload - 5.4GB\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eOrder based Transformer model (order-based v15 - Accuracy of 60.7%) \u003ca href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/order-based-trsf.zip\" rel=\"nofollow\"\u003eDownload - 8.2GB\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eToken based LSTM model (token-based v10 - Accuracy of 60.3%) \u003ca href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/token-based-lstm.zip\" rel=\"nofollow\"\u003eDownload - 6.0GB\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eToken based Transformer model (token-based v11 - Accuracy of 58.9%) \u003ca href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/token-based-trsf.zip\" rel=\"nofollow\"\u003eDownload - 3.5GB\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRL Model (Bootstrapped from order-based v12 and value v1 - \u003cstrong\u003eDipNet RL\u003c/strong\u003e) \u003ca href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/rl-model.zip\" rel=\"nofollow\"\u003eDownload - 11.1GB\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-games-against-albert-daide\" class=\"anchor\" href=\"#games-against-albert-daide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGames against Albert (DAIDE)\u003c/h3\u003e\n\u003cp\u003eThe 1v6 and 6v1 games played between DipNet SL and Albert (DAIDE) can be downloaded below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList of games with power assignments \u003ca href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/daide_albert_results.xlsx\" rel=\"nofollow\"\u003eDownload - 53KB\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eVisualisation of each game (svg and json) \u003ca href=\"https://f002.backblazeb2.com/file/ppaquette-public/benchmarks/experiments/daide_albert_games.zip\" rel=\"nofollow\"\u003eDownload - 2.3GB\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 31,
    "subscribers_count": 13,
    "topics": [],
    "updated_at": 1620651034.0
  },
  {
    "data_format": 2,
    "description": "SKA Radio Telescope Simulator",
    "filenames": [
      "singularity/Singularity.base-dep",
      "singularity/Singularity.base",
      "singularity/Singularity.python3"
    ],
    "full_name": "OxfordSKA/OSKAR",
    "latest_release": "2.7.6",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/OxfordSKA/OSKAR/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2b31f28bf12daa344a4541064866fa171f3f3989a53604cec781f6c15206daab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f4f78666f7264534b412f4f534b41522e7376673f7374796c653d666c61742d737175617265\" alt=\"GitHub release\" data-canonical-src=\"https://img.shields.io/github/release/OxfordSKA/OSKAR.svg?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://doi.org/10.5281/zenodo.3758491\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/db055c7e33d6db9ce4f5f69f83628cd0827c9a0e13aa2aa78a369d938a4ef137/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333735383439312e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3758491.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-oskar-a-gpu-accelerated-simulator-for-the-square-kilometre-array\" class=\"anchor\" href=\"#oskar-a-gpu-accelerated-simulator-for-the-square-kilometre-array\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOSKAR: A GPU-accelerated simulator for the Square Kilometre Array\u003c/h1\u003e\n\u003cp\u003eOSKAR has been designed to produce simulated visibility data from radio\ntelescopes containing aperture arrays, such as those envisaged for the\nSquare Kilometre Array.\u003c/p\u003e\n\u003cp\u003eA source code archive, and pre-built binary packages for Linux (using\nSingularity), macOS and Windows platforms are available to download from\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/OxfordSKA/OSKAR/releases\"\u003ehttps://github.com/OxfordSKA/OSKAR/releases\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOSKAR is licensed under the terms of the 3-clause BSD License.\nPlease see the \u003ca href=\"LICENSE\"\u003eLICENSE\u003c/a\u003e file for details.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-image\" class=\"anchor\" href=\"#singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity image\u003c/h3\u003e\n\u003cp\u003eA pre-built \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e SIF container image\nis available for Linux which can be used to run OSKAR command line\napplications or Python scripts directly, without needing to compile or install\nanything. For Singularity 3.0 or later, an application or script can be run\nusing the downloaded \u003ca href=\"https://github.com/OxfordSKA/OSKAR/releases\"\u003econtainer\u003c/a\u003e\nwith the \u003ccode\u003esingularity exec\u003c/code\u003e command, which takes the form:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec [flags] \u0026lt;container_path\u0026gt; \u0026lt;app_name\u0026gt; \u0026lt;arguments\u0026gt;...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUse the \u003ccode\u003e--nv\u003c/code\u003e flag to enable NVIDIA GPU support in Singularity, if\napplicable.\u003c/p\u003e\n\u003cp\u003eNote also that Singularity will mount the home directory into the container by\ndefault, unless configured otherwise. If you have packages installed in your\nhome area that should be kept isolated from those in the container (for\nexample, because of conflicting packages or Python versions, or if you see\nother errors caused by trying to load wrong versions of shared libraries when\nstarting the container) then it may be necessary to disable this either by\nusing the \u003ccode\u003e--no-home\u003c/code\u003e flag, or re-bind the home directory in the container\nto somewhere other than your actual $HOME using the \u003ccode\u003e-H\u003c/code\u003e flag.\u003c/p\u003e\n\u003cp\u003eAs an example, to run the application \u003ccode\u003eoskar_sim_interferometer\u003c/code\u003e\nwith a parameter file \u003ccode\u003esettings.ini\u003c/code\u003e and a container image file\n\u003ccode\u003eOSKAR-Python3.sif\u003c/code\u003e (both in the current directory) on a GPU use:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec --no-home --nv ./OSKAR-Python3.sif oskar_sim_interferometer settings.ini\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimilarly, to run a Python script \u003ccode\u003esim_script.py\u003c/code\u003e that uses OSKAR:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec --no-home --nv ./OSKAR-Python3.sif python3 sim_script.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h3\u003e\n\u003cp\u003eIf hardware acceleration is required, be sure to install appropriate GPU\ndrivers which are supported by the hardware manufacturer. Third-party graphics\ndrivers are unlikely to work.\u003c/p\u003e\n\u003cp\u003eWhen building from source, the only required dependency is\n\u003ca href=\"https://cmake.org\" rel=\"nofollow\"\u003eCMake \u0026gt;= 3.1\u003c/a\u003e.\nAll other dependencies are optional, but functionality will be\nlimited if these are not found by CMake.\n\u003cem\u003eNote that these dependencies are required only if building from source\u003c/em\u003e, not\nif using a \u003ca href=\"https://github.com/OxfordSKA/OSKAR/releases\"\u003epre-built package\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://cmake.org\" rel=\"nofollow\"\u003eCMake \u0026gt;= 3.1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e(Optional) \u003ca href=\"https://developer.nvidia.com/cuda-downloads\" rel=\"nofollow\"\u003eCUDA \u0026gt;= 7.0\u003c/a\u003e\nor OpenCL, required for GPU acceleration on supported hardware.\u003c/li\u003e\n\u003cli\u003e(Optional) \u003ca href=\"https://qt.io\" rel=\"nofollow\"\u003eQt 5\u003c/a\u003e,\nrequired to build the graphical user interface.\u003c/li\u003e\n\u003cli\u003e(Optional) \u003ca href=\"https://github.com/casacore/casacore\"\u003ecasacore \u0026gt;= 2.0\u003c/a\u003e,\nrequired to use CASA Measurement Sets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePackages for these dependencies are available in the package repositories\nof many recent Linux distributions, including Debian and Ubuntu.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build-commands\" class=\"anchor\" href=\"#build-commands\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild commands\u003c/h3\u003e\n\u003cp\u003eTo build from source, either clone the repository using\n\u003ccode\u003egit clone https://github.com/OxfordSKA/OSKAR.git\u003c/code\u003e (for the current master\nbranch) or download and unpack the source archive, then:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ mkdir build\n$ cd build\n$ cmake [OPTIONS] ../path/to/top/level/source/folder\n$ make -j4\n$ make install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen running the \u0027cmake\u0027 command a number of options can be specified:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e* -DCUDA_ARCH=\"\u0026lt;arch\u0026gt;\" (default: all)\n    Sets the target architecture for the compilation of CUDA device code.\n    \u0026lt;arch\u0026gt; must be one of either: 2.0, 2.1, 3.0, 3.2, 3.5, 3.7,\n                                  5.0, 5.2, 6.0, 6.1, 6.2, 7.0, 7.5,\n                                  8.0, 8.6 or ALL.\n    ALL is for all currently supported architectures.\n    Separate multiple architectures using semi-colons, if required.\n\n* -DCMAKE_INSTALL_PREFIX=\u0026lt;path\u0026gt; (default: /usr/local/)\n    Path prefix used to install OSKAR (with make install).\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-advanced-build-options\" class=\"anchor\" href=\"#advanced-build-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvanced build options\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003e* -DCASACORE_LIB_DIR=\u0026lt;path\u0026gt; (default: searches the system library paths)\n    Specifies a location to search for the casacore libraries\n    (libcasa_ms.so and others) if they are not in the system library path.\n\n* -DCASACORE_INC_DIR=\u0026lt;path\u0026gt; (default: searches the system include paths)\n    Specifies a location to search for the casacore library headers if they\n    are not in the system include path.\n    This is the path to the top level casacore include folder.\n\n* -DCMAKE_PREFIX_PATH=\u0026lt;path\u0026gt; (default: None)\n    Specifies a location to search for Qt 5 if it is not in a standard\n    system path. For example, if using Homebrew on macOS, this may need\n    to be set to /usr/local/opt/qt5/\n\n* -DFIND_CUDA=ON|OFF (default: ON)\n    Can be used not to find or link against CUDA.\n\n* -DFIND_OPENCL=ON|OFF (default: OFF)\n    Can be used not to find or link against OpenCL.\n    OpenCL support in OSKAR is currently experimental.\n\n* -DNVCC_COMPILER_BINDIR=\u0026lt;path\u0026gt; (default: None)\n    Specifies a nvcc compiler binary directory override. See nvcc help.\n    This is likely to be needed only on macOS when the version of the\n    compiler picked up by nvcc (which is related to the version of XCode\n    being used) is incompatible with the current version of CUDA.\n    Set this to \u0027clang\u0027 on macOS if using GCC to build the rest of OSKAR.\n\n* -DFORCE_LIBSTDC++=ON|OFF (default: OFF)\n    If ON forces the use of libstdc++ with the Clang compiler.\n    Used for controlling linking behaviour when using clang\n    or clang-omp compilers with dependencies which may have been compiled\n    against libstdc++\n\n* -DCMAKE_BUILD_TYPE=\u0026lt;release or debug\u0026gt; (default: release)\n    Build in release or debug mode.\n\n* -DBUILD_INFO=ON|OFF (default: OFF)\n    If ON enables the display of diagnostic build information when\n    running CMake.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-unit-tests\" class=\"anchor\" href=\"#unit-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUnit tests\u003c/h3\u003e\n\u003cp\u003eFrom the build directory, the unit tests can be run by typing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ctest [--verbose]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python-interface\" class=\"anchor\" href=\"#python-interface\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython interface\u003c/h3\u003e\n\u003cp\u003eAfter installing OSKAR, the Python interface to it can be installed to\nmake it easier to use from Python scripts.\nStraightforward instructions for installation with \u003ccode\u003epip\u003c/code\u003e can be\n\u003ca href=\"python/README.md\"\u003efound in the python subdirectory\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-example-simulation\" class=\"anchor\" href=\"#example-simulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample simulation\u003c/h3\u003e\n\u003cp\u003eThe example simulation described in the\n\u003ca href=\"https://github.com/OxfordSKA/OSKAR/releases\"\u003edocumentation\u003c/a\u003e\ncan be run to check that a simple simulation behaves as expected.\u003c/p\u003e\n",
    "stargazers_count": 31,
    "subscribers_count": 8,
    "topics": [
      "astronomy",
      "radio-telescopes",
      "simulator"
    ],
    "updated_at": 1621880107.0
  },
  {
    "data_format": 2,
    "description": "A flexible Nextflow-based framework for the definition of sequencing data processing pipelines",
    "filenames": [
      "Singularity"
    ],
    "full_name": "montilab/pipeliner",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pipeliner\" class=\"anchor\" href=\"#pipeliner\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeliner\u003c/h1\u003e\n\u003cp\u003e\u003ci\u003eA flexible Nextflow-based framework for the definition of sequencing data processing pipelines\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e2da22e8e369f5a60ec2e656caa3a42257dad04437afcb1bd670bf395f066899/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275696c74253230576974682d4e657874666c6f772d627269676874677265656e2e737667\" alt=\"Built With\" data-canonical-src=\"https://img.shields.io/badge/Built%20With-Nextflow-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/8a2216c3829e364a4b2d50a3b2b28b85bb2bfb8847e2345f341026872d9a9d11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4672616d65776f726b2d507974686f6e253230332e362d626c75652e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8a2216c3829e364a4b2d50a3b2b28b85bb2bfb8847e2345f341026872d9a9d11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4672616d65776f726b2d507974686f6e253230332e362d626c75652e737667\" alt=\"Python\" data-canonical-src=\"https://img.shields.io/badge/Framework-Python%203.6-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0ab30b068b2dd5ec6fe37a2b6c2e0d722b12bc61d745bba31407499a217262f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6d7061746962696c6974792d4c696e75782532302532462532304f53582d6f72616e67652e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0ab30b068b2dd5ec6fe37a2b6c2e0d722b12bc61d745bba31407499a217262f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6d7061746962696c6974792d4c696e75782532302532462532304f53582d6f72616e67652e737667\" alt=\"Compatibility\" data-canonical-src=\"https://img.shields.io/badge/Compatibility-Linux%20%2F%20OSX-orange.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/80a1153c429992993a5fc1d8009c2f9ed74f95263366dc21a2daec8fb25077c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646570656e64656e636965732d7570253230746f253230646174652d627269676874677265656e2e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/80a1153c429992993a5fc1d8009c2f9ed74f95263366dc21a2daec8fb25077c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646570656e64656e636965732d7570253230746f253230646174652d627269676874677265656e2e737667\" alt=\"Dependencies\" data-canonical-src=\"https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/montilab/pipeliner/issues\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aca381c660052e22bf17ac3b2f6d3a0035067b28a4e5746b27e7e2c9306b4cbf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d6f6e74696c61622f706970656c696e65722e737667\" alt=\"GitHub Issues\" data-canonical-src=\"https://img.shields.io/github/issues/montilab/pipeliner.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp align=\"left\"\u003e\u003ca href=\"https://raw.githubusercontent.com/montilab/pipeliner/master/media/framework_schematic.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/montilab/pipeliner/master/media/framework_schematic.png\" width=\"80%\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n\u003c/p\u003e\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eModular directory structure: It is designed to generate automated result directory based on the names of the samples and tools used to process them.\u003c/li\u003e\n\u003cli\u003ePlatform independent: It is bundled with an anaconda repository which contains pre-compiled tools as well as pre-built environments that can be used directly.\u003c/li\u003e\n\u003cli\u003eModular architecture: It allows the expert users to customize, modify processes, or add additional tools based on their needs.\u003c/li\u003e\n\u003cli\u003eAutomated job parallelization, job recovery, and reproducibility.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eFor more information, please refer to the \u003ca href=\"https://pipeliner.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003efull documentation\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-clone-repository\" class=\"anchor\" href=\"#clone-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eClone Repository\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git clone https://github.com/montilab/pipeliner\n\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-dependencies\" class=\"anchor\" href=\"#install-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Dependencies\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ conda env create -f pipeliner/envs/linux_env.yml \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Linux\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ conda env create -f pipeliner/envs/osx_env.yml \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Mac\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-activate-conda-environment\" class=\"anchor\" href=\"#activate-conda-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eActivate Conda Environment\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e activate pipeliner\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-update-local-paths\" class=\"anchor\" href=\"#update-local-paths\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUpdate Local Paths\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ python pipeliner/scripts/paths.py\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-nextflow-executable\" class=\"anchor\" href=\"#download-nextflow-executable\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload Nextflow Executable\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd pipeliner/pipelines\n$ curl -s https://get.nextflow.io | bash\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-locally-run-example-data\" class=\"anchor\" href=\"#locally-run-example-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocally Run Example Data\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ ./nextflow rnaseq.nf -c rnaseq.config\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-expected-output\" class=\"anchor\" href=\"#expected-output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExpected Output\u003c/h3\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003eN E X T F L O W  ~  version 0.31.1\nLaunching `rnaseq.nf` [nasty_pauling] - revision: cd3f572ab2\n[warm up] executor \u0026gt; local\n[31/1b2066] Submitted process \u0026gt; pre_fastqc (ggal_alpha)\n[23/de6d60] Submitted process \u0026gt; pre_fastqc (ggal_theta)\n[7c/28ee53] Submitted process \u0026gt; pre_fastqc (ggal_gamma)\n[97/9ad6c1] Submitted process \u0026gt; check_reads (ggal_alpha)\n[ab/c3eedf] Submitted process \u0026gt; check_reads (ggal_theta)\n[2d/050633] Submitted process \u0026gt; check_reads (ggal_gamma)\n[1d/f3af6d] Submitted process \u0026gt; pre_multiqc\n[32/b1db1d] Submitted process \u0026gt; hisat_indexing (genome_reference.fa)\n[3b/d93c6d] Submitted process \u0026gt; trim_galore (ggal_alpha)\n[9c/3fa50b] Submitted process \u0026gt; trim_galore (ggal_theta)\n[62/25fce0] Submitted process \u0026gt; trim_galore (ggal_gamma)\n[66/ccc9db] Submitted process \u0026gt; hisat_mapping (ggal_alpha)\n[28/69fff5] Submitted process \u0026gt; hisat_mapping (ggal_theta)\n[5c/5ed2b6] Submitted process \u0026gt; hisat_mapping (ggal_gamma)\n[b4/e559ab] Submitted process \u0026gt; gtftobed (genome_annotation.gtf)\n[bc/6f490c] Submitted process \u0026gt; rseqc (ggal_alpha)\n[71/80aa9e] Submitted process \u0026gt; rseqc (ggal_theta)\n[17/ca0d9f] Submitted process \u0026gt; rseqc (ggal_gamma)\n[d7/7d391b] Submitted process \u0026gt; counting (ggal_alpha)\n[df/936854] Submitted process \u0026gt; counting (ggal_theta)\n[11/143c2c] Submitted process \u0026gt; counting (ggal_gamma)\n[31/4c11f9] Submitted process \u0026gt; expression_matrix\n[1f/3af548] Submitted process \u0026gt; multiqc\nSuccess: Pipeline Completed!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003ePlease refer to \u003ca href=\"https://github.com/montilab/pipeliner/blob/master/docs/reports.md\"\u003ereports\u003c/a\u003e for examples\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cite\" class=\"anchor\" href=\"#cite\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCite\u003c/h2\u003e\n\u003cp\u003eFederico A, Karagiannis T, Karri K, Kishore D, Koga Y, Campbell J, Monti S (2019) Pipeliner: A Nextflow-based framework for the definition of sequencing data processing pipelines. \u003cem\u003eFrontiers in Genetics\u003c/em\u003e. \u003ca href=\"https://doi.org/10.3389/fgene.2019.00614\" rel=\"nofollow\"\u003ehttps://doi.org/10.3389/fgene.2019.00614\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 35,
    "subscribers_count": 6,
    "topics": [
      "nextflow",
      "workflow",
      "bioinformatics",
      "rna-seq",
      "computational-biology"
    ],
    "updated_at": 1615524762.0
  },
  {
    "data_format": 2,
    "description": "RStudio Server in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.3.6.2"
    ],
    "full_name": "nickjer/singularity-rstudio",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rstudio-server\" class=\"anchor\" href=\"#singularity-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity RStudio Server\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nickjer/singularity-rstudio\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/291de9d065fa77b739def518b0430f977c5793f78b1b4ce88d235e61c42332ee/68747470733a2f2f7472617669732d63692e6f72672f6e69636b6a65722f73696e67756c61726974792d7273747564696f2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nickjer/singularity-rstudio.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/463\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"Singularity Hub\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"https://www.rstudio.com/products/rstudio/\" rel=\"nofollow\"\u003eRStudio Server\u003c/a\u003e. It was built on top of the base\nSingularity image \u003ca href=\"https://github.com/nickjer/singularity-r\"\u003enickjer/singularity-r\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is still a work in progress.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003esingularity-rstudio.simg\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build singularity-rstudio.simg Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from\n\u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name singularity-rstudio.simg shub://nickjer/singularity-rstudio\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rstudio-server\" class=\"anchor\" href=\"#rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRStudio Server\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003erserver\u003c/code\u003e command is launched using the default run command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run singularity-rstudio.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor as an explicit app:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --app rserver singularity-rstudio.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003esingularity run --app rserver singularity-rstudio.simg --help\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecommand-line options:\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003everify:\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --verify-installation arg (=0)        verify the current installation\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003eserver:\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-working-dir arg (=/)         program working directory\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-user arg (=rstudio-server)   program user\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-daemonize arg (=0)           run program as daemon\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-app-armor-enabled arg (=1)   is app armor enabled for this session\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-set-umask arg (=1)           set the umask to 022 on startup\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-simple-password-authentication\" class=\"anchor\" href=\"#simple-password-authentication\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimple Password Authentication\u003c/h4\u003e\n\u003cp\u003eTo secure the RStudio Server you will need to:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLaunch the container with the environment variable \u003ccode\u003eRSTUDIO_PASSWORD\u003c/code\u003e set to\na password of your choosing.\u003c/li\u003e\n\u003cli\u003eLaunch the \u003ccode\u003erserver\u003c/code\u003e command with the PAM helper script \u003ccode\u003erstudio_auth\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAn example is given as:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eRSTUDIO_PASSWORD=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epassword\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e singularity run singularity-rstudio.simg \\\n  --auth-none 0 \\\n  --auth-pam-helper rstudio_auth\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow when you attempt to access the RStudio Server you will be presented with a\nlog in form. You can log in with your current user name and password you set in\n\u003ccode\u003eRSTUDIO_PASSWORD\u003c/code\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-ldap-authentication\" class=\"anchor\" href=\"#ldap-authentication\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLDAP Authentication\u003c/h4\u003e\n\u003cp\u003eAnother option is using an LDAP (or Active Directory) server for\nauthentication. Configuration of the LDAP authentication script \u003ccode\u003eldap_auth\u003c/code\u003e is\nhandled through the following environment variables:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eLDAP_HOST\u003c/code\u003e - the host name of the LDAP server\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eLDAP_USER_DN\u003c/code\u003e - the formatted string (where \u003ccode\u003e%s\u003c/code\u003e is replaced with the\nusername supplied during log in) of the bind DN used for LDAP authentication\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eLDAP_CERT_FILE\u003c/code\u003e - the file containing the CA certificates used by\nthe LDAP server (default: use system CA certificates)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn example for an LDAP server with signed SSL certificate from a trusted CA:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_HOST=ldap.example.com\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_USER_DN=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ecn=%s,dc=example,dc=com\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\nsingularity run singularity-rstudio.simg \\\n  --auth-none 0 \\\n  --auth-pam-helper-path ldap_auth\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAn example for an LDAP server with a self-signed SSL certificate:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_HOST=ldap.example.com\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_USER_DN=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ecn=%s,dc=example,dc=com\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_CERT_FILE=/ca-certs.pem\nsingularity run \\\n  --bind /path/to/ca-certs.pem:/ca-certs.pem \\\n  singularity-rstudio.simg \\\n    --auth-none 0 \\\n    --auth-pam-helper-path ldap_auth\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNote that we had to bind mount the CA certificates file from the host machine\ninto the container and specify the container\u0027s path in \u003ccode\u003eLDAP_CERT_FILE\u003c/code\u003e (not\nthe host\u0027s path).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-r-and-rscript\" class=\"anchor\" href=\"#r-and-rscript\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR and Rscript\u003c/h3\u003e\n\u003cp\u003eSee \u003ca href=\"https://github.com/nickjer/singularity-r\"\u003enickjer/singularity-r\u003c/a\u003e for more information on how to run \u003ccode\u003eR\u003c/code\u003e and\n\u003ccode\u003eRscript\u003c/code\u003e from within this Singularity image.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eBug reports and pull requests are welcome on GitHub at\n\u003ca href=\"https://github.com/nickjer/singularity-rstudio\"\u003ehttps://github.com/nickjer/singularity-rstudio\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 39,
    "subscribers_count": 5,
    "topics": [
      "rstudio-server",
      "singularity-image"
    ],
    "updated_at": 1621773800.0
  },
  {
    "data_format": 2,
    "description": "The ultimate computational platform for Neuroscience",
    "filenames": [
      "singularity/Singularity",
      "singularity/Singularity.10.20190801",
      "singularity/Singularity.obs"
    ],
    "full_name": "neurodebian/neurodebian",
    "latest_release": "debian/0.40.1",
    "readme": "\u003cpre\u003e\u003ccode\u003e                              ==  ===============   === =\n                       =  ================================\n                  = ======= =        =  ==      ==============\n                ======                             = ============ = =\n            ====== =                                  = ============ =\n        = =======                                        = ============\n      ===========                                             ==========\n      ===== =                                                 = ==========\n   ===== =                                                        ========\n  ======                                                            ======\n =====                                           =                  =======\n ====                                            =                    ======\n ====                        ==   ==  =  == = ==                      ======\n=====                  = ======== =  ==   =                            ======\n====              =  =====  ==                                         ======\n===               ===== =                                               ======\n===               ===                                                 =======\n  =            = ==                                                 = ========\n  =  =        =====                                             =============\n             =  ==                                        ==  =============\n             == ===                                 =   ================ =\n               =====                          == ================== ==\n              ==  ==== =                 =  ============ = ===  =\n                   = ===== ===   ==  =============   == = =\n                      = ====================   ==\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"http://neuro.debian.net\" rel=\"nofollow\"\u003eNeuroDebian\u003c/a\u003e is a popular turnkey platform for\nNeuroscience, where software is integrated, tested, and delivered\nconveniently and reliably so you could concentrate on your research and\nnot on \"system maintenance\".  It provides a large collection of popular\nneuroscience research software for the Debian operating system as well\nas Ubuntu and other derivatives.  Please visit our\n\u003ca href=\"http://neuro.debian.net\" rel=\"nofollow\"\u003emain website\u003c/a\u003e to discover more.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/neurodebian/neurodebian\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bde3e9fa0661cc91b1b2ff523416ac95dbdd5365404526a9f08d42f0c33c2f8d/68747470733a2f2f7365637572652e7472617669732d63692e6f72672f6e6575726f64656269616e2f6e6575726f64656269616e2e706e673f6272616e63683d6d6173746572\" alt=\"Travis tests status\" data-canonical-src=\"https://secure.travis-ci.org/neurodebian/neurodebian.png?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/_/neurodebian/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fb1cf50814364d650c82cf7258ea60fae8c313d571b513c7c3ad3e2f729c30bb/687474703a2f2f646f636b6572692e636f2f696d6167652f5f2f6e6575726f64656269616e\" alt=\"Docker\" data-canonical-src=\"http://dockeri.co/image/_/neurodebian\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/209\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-related-projects-from-the-neurodebian-authors\" class=\"anchor\" href=\"#related-projects-from-the-neurodebian-authors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRelated projects from the NeuroDebian authors\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://open-brain-consent.readthedocs.io\" rel=\"nofollow\"\u003eOpen Brain Consent\u003c/a\u003e - samples\nand an ultimate wording for experiment participant consent forms to make\nopen data sharing possible\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://datalad.org\" rel=\"nofollow\"\u003eDataLad\u003c/a\u003e - a data distribution and management\nplatform, which addresses shortcomings of solutions of software/code-oriented\nsolutions (such as NeuroDebian and pure git), when applied to data\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://reproin.repronim.org\" rel=\"nofollow\"\u003eReproIn\u003c/a\u003e - a turnkey solution for collecting\nMRI data directly as \u003ca href=\"http://bids.neuroimaging.io\" rel=\"nofollow\"\u003eBIDS\u003c/a\u003e DataLad datasets\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"duecredit.org\"\u003eDueCredit\u003c/a\u003e - Automated collection and reporting of\ncitations for used software/methods/datasets\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://studyforrest.org\" rel=\"nofollow\"\u003eStudy Forrest\u003c/a\u003e -  a diverse and\never-expanding collection of data and studies on our favorite shrimping,\ncross-country running, international ping-pong champion: \u003cem\u003eForrest Gump\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://pymvpa.org\" rel=\"nofollow\"\u003ePyMVPA\u003c/a\u003e - a machine learning framework for the analysis\nof (not only) neuroimaging data\u003c/li\u003e\n\u003cli\u003eDiscover more about these and other projects from\n\u003ca href=\"http://centerforopenneuroscience.org\" rel=\"nofollow\"\u003eCenter for Open Neuroscience\u003c/a\u003e and\n\u003ca href=\"http://psychoinformatics.de\" rel=\"nofollow\"\u003ePsychoinformatics\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 49,
    "subscribers_count": 19,
    "topics": [],
    "updated_at": 1617993416.0
  },
  {
    "data_format": 2,
    "description": "Clustering scRNAseq by genotypes",
    "filenames": [
      "Singularity"
    ],
    "full_name": "wheaton5/souporcell",
    "latest_release": "2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-souporcell\" class=\"anchor\" href=\"#souporcell\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esouporcell\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/wheaton5/souporcell/blob/master/souporcell_star.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/wheaton5/souporcell/raw/master/souporcell_star.png\" width=\"100\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePreprint manuscript of this method available at \u003ca href=\"https://www.biorxiv.org/content/10.1101/699637v1\" rel=\"nofollow\"\u003ehttps://www.biorxiv.org/content/10.1101/699637v1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003esouporcell is a method for clustering mixed-genotype scRNAseq experiments by individual.\u003c/p\u003e\n\u003cp\u003eThe inputs are just the possorted_genome_bam.bam, and barcodes.tsv as output from \u003ca href=\"https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger\" rel=\"nofollow\"\u003ecellranger\u003c/a\u003e.\nsouporcell is comprised of 6 steps with the first 3 using external tools and the final using the code provided here.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRemapping (\u003ca href=\"https://github.com/lh3/minimap2\"\u003eminimap2\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eCalling candidate variants (\u003ca href=\"https://github.com/ekg/freebayes\"\u003efreebayes\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eCell allele counting (\u003ca href=\"https://github.com/10XGenomics/vartrix\"\u003evartrix\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eClustering cells by genotype (souporcell.py)\u003c/li\u003e\n\u003cli\u003eCalling doublets (troublet)\u003c/li\u003e\n\u003cli\u003eCalling cluster genotypes and inferring amount of ambient RNA (consensus.py)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-easy-installation-linux-recommended\" class=\"anchor\" href=\"#easy-installation-linux-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEasy Installation (Linux) (recommended)\u003c/h2\u003e\n\u003cp\u003eDownload singularity image (1.3gb) (singularity is similar to docker but safe for clusters)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://wheaton5/souporcell\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you are running on a scientific cluster, they will likely have singularity, contact your sysadmin for more details.\nIf you are running on your own linux box you may need to install \u003ca href=\"https://www.sylabs.io/guides/3.2/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003esingularity\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003erequires singularity \u0026gt;= 3.0\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewhich singularity\nsingularity --version\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should now be able to run souporcell_pipeline.py through the singularity container. Singularity automatically mounts the current working directory and directories downstream from where you run it, otherwise you would need to manually mount those directories. Therefor you can run it from a directory that is upstream of all of the inputs. Input files are the cellranger bam, cellranger barcodes file, and a reference fasta. The cellranger bam is located in the cellranger outs directory and is called possorted_genome_bam.bam. The barcodes file is located in the cellranger outs/filtered_gene_bc_matrices/\u0026lt;ref_name\u0026gt;/barcodes.tsv. The reference fasta should be of the same species but does not necessarily need to be the exact cellranger reference.\u003c/p\u003e\n\u003cp\u003eThe options for using souporcell are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec souporcell_latest.sif souporcell_pipeline.py -h\nusage: souporcell_pipeline.py [-h] -i BAM -b BARCODES -f FASTA -t THREADS -o\n                              OUT_DIR -k CLUSTERS [-p PLOIDY]\n                              [--min_alt MIN_ALT] [--min_ref MIN_REF]\n                              [--max_loci MAX_LOCI] [--restarts RESTARTS]\n                              [--common_variants COMMON_VARIANTS]\n                              [--known_genotypes KNOWN_GENOTYPES]\n                              [--known_genotypes_sample_names KNOWN_GENOTYPES_SAMPLE_NAMES [KNOWN_GENOTYPES_SAMPLE_NAMES ...]]\n                              [--skip_remap SKIP_REMAP] [--ignore IGNORE]\n\nsingle cell RNAseq mixed genotype clustering using sparse mixture model\nclustering with tensorflow.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i BAM, --bam BAM     cellranger bam\n  -b BARCODES, --barcodes BARCODES\n                        barcodes.tsv from cellranger\n  -f FASTA, --fasta FASTA\n                        reference fasta file\n  -t THREADS, --threads THREADS\n                        max threads to use\n  -o OUT_DIR, --out_dir OUT_DIR\n                        name of directory to place souporcell files\n  -k CLUSTERS, --clusters CLUSTERS\n                        number cluster, tbd add easy way to run on a range of\n                        k\n  -p PLOIDY, --ploidy PLOIDY\n                        ploidy, must be 1 or 2, default = 2\n  --min_alt MIN_ALT     min alt to use locus, default = 10.\n  --min_ref MIN_REF     min ref to use locus, default = 10.\n  --max_loci MAX_LOCI   max loci per cell, affects speed, default = 2048.\n  --restarts RESTARTS   number of restarts in clustering, when there are \u0026gt; 12\n                        clusters we recommend increasing this to avoid local\n                        minima\n                         --common_variants COMMON_VARIANTS\n                        common variant loci or known variant loci vcf, must be\n                        vs same reference fasta\n  --known_genotypes KNOWN_GENOTYPES\n                        known variants per clone in population vcf mode, must\n                        be .vcf right now we dont accept gzip or bcf sorry\n  --known_genotypes_sample_names KNOWN_GENOTYPES_SAMPLE_NAMES [KNOWN_GENOTYPES_SAMPLE_NAMES ...]\n                        which samples in population vcf from known genotypes\n                        option represent the donors in your sample\n  --skip_remap SKIP_REMAP\n                        don\u0027t remap with minimap2 (not recommended unless in\n                        conjunction with --common_variants\n  --ignore IGNORE       set to True to ignore data error assertions\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA typical command looks like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec /path/to/souporcell_latest.sif souporcell_pipeline.py -i /path/to/possorted_genome_bam.bam -b /path/to/barcodes.tsv -f /path/to/reference.fasta -t num_threads_to_use -o output_dir_name -k num_clusters\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe above command will run all six steps of the pipeline and it will require up to 24gb of ram for human (minimap2 bam index is high water mark for memory). For smaller genomes, fewer clusters, lower --max-loci will require less memory. Note that souporcell will require roughly 2x the amount of diskspace that the input bam file takes up. This dataset should take several hours to run on 8 threads mostly due to read processing, remapping, and variant calling.\u003c/p\u003e\n\u003cp\u003eIf you have a common snps file you may want to use the --common_variants option with or without the --skip_remap option. This option will skip conversion to fastq, remapping with minimap2, and reattaching barcodes, and the --common_variants will remove the freebayes step. Each which will save a significant amount of time, but --skip-remap isn\u0027t recommended without --common_variants.\u003c/p\u003e\n\u003cp\u003eCommon variant files from 1k genomes filtered to variants \u0026gt;= 2% allele frequency in the population and limited to SNPs can be found here for GRCh38\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \u0027https://docs.google.com/uc?export=download\u0026amp;id=13aebUpEKrtjliyT9rYzRijtkNJVUk5F_\u0027 -O- | sed -rn \u0027s/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p\u0027)\u0026amp;id=13aebUpEKrtjliyT9rYzRijtkNJVUk5F_\" -O common_variants_grch38.vcf \u0026amp;\u0026amp; rm -rf /tmp/cookies.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor for hg19 here\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download\u0026amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \u0027https://docs.google.com/uc?export=download\u0026amp;id=1lw4T6d7uXsm9dt39ZtEwpuB2VTY3wK1y\u0027 -O- | sed -rn \u0027s/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p\u0027)\u0026amp;id=1lw4T6d7uXsm9dt39ZtEwpuB2VTY3wK1y\" -O common_variants_hg19.vcf \u0026amp;\u0026amp; rm -rf /tmp/cookies.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-practicetesting-data-set-demuxlet-paper-data\" class=\"anchor\" href=\"#practicetesting-data-set-demuxlet-paper-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePractice/Testing data set: Demuxlet paper data\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://sra-pub-src-1.s3.amazonaws.com/SRR5398235/A.merged.bam.1 -O A.merged.bam\nwget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2560nnn/GSM2560245/suppl/GSM2560245_barcodes.tsv.gz\ngunzip GSM2560245_barcodes.tsv.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd if you don\u0027t have a human reference sitting around, grab one here\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget http://cf.10xgenomics.com/supp/cell-exp/refdata-cellranger-GRCh38-3.0.0.tar.gz\ntar -xzvf refdata-cellranger-GRCh38-3.0.0.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow you should be ready to test it out\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec /path/to/souporcell_latest.sif souporcell_pipeline.py -i A.merged.bam -b GSM2560245_barcodes.tsv -f refdata-cellranger-GRCh38-3.0.0/fasta/genome.fa -t 8 -o demux_data_test -k 4\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis should require about 20gb of ram mostly because of the minimap2 indexing step. I might soon host an index and reference for human to make this less painful.\u003c/p\u003e\n\u003cp\u003eThe important files are\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eclusters.tsv\u003c/li\u003e\n\u003cli\u003ecluster_genotypes.vcf\u003c/li\u003e\n\u003cli\u003eambient_rna.txt\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eclusters.tsv will look like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebarcode status  assignment      log_loss_singleton      log_loss_doublet        cluster0        cluster1\nAAACCTGAGATCCGAG-1      singlet 0       -152.7778890920112      -190.5463095948822      -43.95302689281067      -101.63377524087669\nAAACCTGAGCACCGTC-1      singlet 0       -78.56014177554212      -96.66255440088581      -20.949294849836267     -52.57478083591962\nAAACCTGAGTACGATA-1      singlet 0       -216.0188863327174      -281.3888392065457      -63.059016939362536     -159.5450834682198\nAAACCTGGTACATGTC-1      singlet 1       -47.189434469216565     -96.30865717225866      -62.652900832546955     -15.284168900754413\nAAACCTGTCTACTCAT-1      singlet 0       -129.30104434183454     -167.87811467946756     -41.09158213888751      -106.3201962010145\nAAACCTGTCTTGTCAT-1      singlet 0       -85.99781433701455      -110.81701038967158     -24.518165091815554     -60.05279033826837\nAAACGGGCACTGTTAG-1      singlet 0       -154.26595878718032     -191.05465308213363     -31.356408693487197     -81.61186496254497\nAAACGGGCATCATCCC-1      singlet 1       -46.33205678267174      -80.24152434540565      -50.78221280006256      -14.615983876840312\nAAACGGGGTAGGGTAC-1      singlet 0       -240.5237900569412      -302.91575436035924     -71.79370547349878      -154.08594135029728\nAAACGGGTCGGCATCG-1      singlet 0       -166.66827966974532     -226.56795157885028     -51.08790637893961      -148.04625123166286\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWith the cell barcode, singlet/doublet status, cluster, log_loss_singleton, log_loss_doublet, followed by log loss for each cluster.\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003ecluster_genotypes.vcf is a vcf with genotypes for each cluster for each variant in the input vcf from freebayes\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eand\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eambient_rna.txt just contains the ambient RNA percentage detected\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-hard-install\" class=\"anchor\" href=\"#hard-install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHard install\u003c/h2\u003e\n\u003cp\u003eInstead of using singularity you can install everything independently (not recommended, but shouldn\u0027t be too bad)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/wheaton5/souporcell.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eput souporcell directory on your PATH\nrequires samtools, bcftools, htslib, python3, freebayes, vartrix, minimap2 all on your PATH\nI suggest you use the conda env I have set up by using the following command if you have conda or miniconda\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create -f /path/to/souporcell/souporcell_env.yaml\nconda activate souporcell\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will also need Rust and to compile the two rust binaries\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecurl --proto \u0027=https\u0027 --tlsv1.2 -sSf https://sh.rustup.rs | sh\ncd /path/to/souporcell/souporcell \u0026amp;\u0026amp; cargo build --release\ncd /path/to/souporcell/troublet \u0026amp;\u0026amp; cargo build --release\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eotherwise python packages tensorflow, pyvcf, pystan, pyfaidx, numpy, scipy are required, but as the versions change, I do recommend using the presetup env.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-through-the-pipeline-script\" class=\"anchor\" href=\"#to-run-through-the-pipeline-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run through the pipeline script\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esouporcell_pipeline.py -i /path/to/possorted_genome_bam.bam -b /path/to/barcodes.tsv -f /path/to/reference.fasta -t num_threads_to_use -o output_dir_name -k num_clusters\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-things-step-by-step-not-through-the-pipeline-script\" class=\"anchor\" href=\"#to-run-things-step-by-step-not-through-the-pipeline-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run things step by step not through the pipeline script\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-remapping\" class=\"anchor\" href=\"#1-remapping\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Remapping\u003c/h3\u003e\n\u003cp\u003eWe discuss the need for remapping in our manuscript. We need to keep track of cell barcodes and and UMIs, so we first create a fastq with those items encoded in the readname.\nRequires python 3.0, modules pysam, argparse (pip install/conda install depending on environment)\nEasiest to first add the souporcell directory to your PATH variable with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport PATH=/path/to/souporcell:$PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen run the renamer.py script to put some of the quality information in the read name. For human data this step will typically take several hours and the output fq file will be somewhat larger than the input bam\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython renamer.py --bam possorted_genome_bam.bam --barcodes barcodes.tsv --out fq.fq\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen we must remap these reads using minimap2 (similar results have been seen with hisat2)\nRequires \u003ca href=\"https://github.com/lh3/minimap2\"\u003eminimap2\u003c/a\u003e\nand add /path/to/minimap2 to your PATH. For human data the remapping will typically require more than 12 Gb memory and may take a few hours to run.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eminimap2 -ax splice -t 8 -G50k -k 21 -w 11 --sr -A2 -B8 -O12,32 -E2,1 -r200 -p.5 -N20 -f1000,5000 -n2 -m20 -s40 -g2000 -2K50m --secondary=no \u0026lt;reference_fasta_file\u0026gt; \u0026lt;fastq_file\u0026gt; \u0026gt; minimap.sam\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(note the -t 8 as the number of threads, change this as needed)\nNow we must retag the reads with their cell barcodes and UMIs\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython retag.py --sam minimap.sam --out minitagged.bam\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen we must sort and index our bam.\nRequires \u003ca href=\"http://www.htslib.org/\" rel=\"nofollow\"\u003esamtools\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esamtools sort minitagged.bam minitagged_sorted.bam\nsamtools index minitagged_sorted.bam\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-calling-candidate-variants\" class=\"anchor\" href=\"#2-calling-candidate-variants\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Calling candidate variants\u003c/h3\u003e\n\u003cp\u003eYou may wish to break this into multiple jobs such as 1 job per chromosome and merge after but the basic command is the following.\nRequires \u003ca href=\"https://github.com/ekg/freebayes\"\u003efreebayes\u003c/a\u003e and add /path/to/freebayes/bin to your PATH\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efreebayes -f \u0026lt;reference_fasta\u0026gt; -iXu -C 2 -q 20 -n 3 -E 1 -m 30 --min-coverage 6 --max-coverage 100000 minitagged_sorted.bam\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-cell-allele-counting\" class=\"anchor\" href=\"#3-cell-allele-counting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Cell allele counting\u003c/h3\u003e\n\u003cp\u003eRequires \u003ca href=\"https://github.com/10XGenomics/vartrix\"\u003evartrix\u003c/a\u003e\nand add /path/to/vartrix to your PATH\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evartrix --umi --mapq 30 -b \u0026lt;bam file\u0026gt; -c \u0026lt;barcode tsv\u0026gt; --scoring-method coverage --threads 8 --ref-matrix ref.mtx --out-matrix alt.mtx -v \u0026lt;freebayes vcf\u0026gt; --fasta \u0026lt;fasta file used for remapping\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003enote the --threads argument and use an appropriate number of threads for your system.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-clustering-cells-by-genotype\" class=\"anchor\" href=\"#4-clustering-cells-by-genotype\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. Clustering cells by genotype\u003c/h3\u003e\n\u003cp\u003eRust required. To install rust:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecurl https://sh.rustup.rs -sSf | sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand to build souporcell clustering\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /path/to/souporcell/souporcell\ncargo build --release\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd add /path/to/souporcell/souporcell/target/release to your path\nusage\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esouporcell -h\nsouporcell 2.4\nHaynes Heaton \u0026lt;whheaton@gmail.com\u0026gt;\nclustering scRNAseq cells by genotype\n\nUSAGE:\n    souporcell [OPTIONS] --alt_matrix \u0026lt;alt_matrix\u0026gt; --barcodes \u0026lt;barcodes\u0026gt; --num_clusters \u0026lt;num_clusters\u0026gt; --ref_matrix \u0026lt;ref_matrix\u0026gt;\n\nFLAGS:\n    -h, --help       Prints help information\n    -V, --version    Prints version information\n\nOPTIONS:\n    -a, --alt_matrix \u0026lt;alt_matrix\u0026gt;                                           alt matrix from vartrix\n    -b, --barcodes \u0026lt;barcodes\u0026gt;                                               cell barcodes\n        --initialization_strategy \u0026lt;initialization_strategy\u0026gt;\n            cluster initialization strategy, defaults to kmeans++, valid values are kmeans++, random_uniform,\n            middle_variance, random_cell_assignment\n        --known_cell_assignments \u0026lt;known_cell_assignments\u0026gt;\n            tsv with barcodes and their known assignments\n\n    -g, --known_genotypes \u0026lt;known_genotypes\u0026gt;\n            NOT YET IMPLEMENTED population vcf/bcf of known genotypes if available.\n            \n        --known_genotypes_sample_names \u0026lt;known_genotypes_sample_names\u0026gt;...\n            NOT YET IMPLEMENTED sample names, must be samples from the known_genotypes vcf\n\n        --min_alt \u0026lt;min_alt\u0026gt;\n            minimum number of cells containing the alt allele for the variant to be used for clustering\n\n        --min_alt_umis \u0026lt;min_alt_umis\u0026gt;                                       min alt umis to use locus for clustering\n        --min_ref \u0026lt;min_ref\u0026gt;\n            minimum number of cells containing the ref allele for the variant to be used for clustering\n\n        --min_ref_umis \u0026lt;min_ref_umis\u0026gt;                                       min ref umis to use locus for clustering\n    -k, --num_clusters \u0026lt;num_clusters\u0026gt;                                       number of clusters\n    -r, --ref_matrix \u0026lt;ref_matrix\u0026gt;                                           ref matrix from vartrix\n    -r, --restarts \u0026lt;restarts\u0026gt;                                               number of random seedings\n        --seed \u0026lt;seed\u0026gt;                                                       optional random seed\n    -t, --threads \u0026lt;threads\u0026gt;                                                 number of threads to use\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSo generally something along the lines of\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esouporcell -a alt.mtx -r ref.mtx -b barcodes.tsv -k \u0026lt;num_clusters\u0026gt; -t 8 \u0026gt; clusters_tmp.tsv\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(note clusters_tmp.tsv output as the doublet caller outputs the final clusters file)\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-5-calling-doublets\" class=\"anchor\" href=\"#5-calling-doublets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5. Calling doublets\u003c/h3\u003e\n\u003cp\u003eRust required.\nBuild troublet:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /path/to/souporcell/troublet\ncargo build --release\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd add /path/to/souporcell/troublet/target/release to your path\nThe usage is\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etroublet -h\ntroublet 2.4\nHaynes Heaton \u0026lt;whheaton@gmail.com\u0026gt;\nIntergenotypic doublet detection given cluster assignments and cell allele counts\n\nUSAGE:\n    troublet [OPTIONS] --alts \u0026lt;alts\u0026gt; --clusters \u0026lt;clusters\u0026gt;\n\nFLAGS:\n    -h, --help       Prints help information\n    -V, --version    Prints version information\n\nOPTIONS:\n    -a, --alts \u0026lt;alts\u0026gt;                              alt allele counts per cell in sparse matrix format out of vartrix\n    -c, --clusters \u0026lt;clusters\u0026gt;                      cluster file output from schism\n    -b, --debug \u0026lt;debug\u0026gt;...                         print debug info for index of cells listed\n    -d, --doublet_prior \u0026lt;doublet_prior\u0026gt;            prior on doublets. Defaults to 0.5\n        --doublet_threshold \u0026lt;doublet_threshold\u0026gt;    doublet posterior threshold, defaults to 0.90\n    -r, --refs \u0026lt;refs\u0026gt;                              ref allele counts per cell in sparse matrix format out of vartrix\n        --singlet_threshold \u0026lt;singlet_threshold\u0026gt;    singlet posterior threshold, defaults to 0.90\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSo generally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etroublet -a alt.mtx -r ref.mtx --clusters clusters_tmp.tsv \u0026gt; clusters.tsv\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-6-genotype-and-ambient-rna-coinference\" class=\"anchor\" href=\"#6-genotype-and-ambient-rna-coinference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e6. Genotype and ambient RNA coinference\u003c/h3\u003e\n\u003cp\u003ePython3 required with modules pystan, pyvcf, pickle, math, scipy, gzip (pip install should work for each)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econsensus.py -h\nusage: consensus.py [-h] -c CLUSTERS -a ALT_MATRIX -r REF_MATRIX [-p PLOIDY]\n                    --soup_out SOUP_OUT --vcf_out VCF_OUT --output_dir\n                    OUTPUT_DIR -v VCF\n\nconsensus genotype calling and ambient RNA estimation\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CLUSTERS, --clusters CLUSTERS\n                        tsv cluster file from the troublet output\n  -a ALT_MATRIX, --alt_matrix ALT_MATRIX\n                        alt matrix file\n  -r REF_MATRIX, --ref_matrix REF_MATRIX\n                        ref matrix file\n  -p PLOIDY, --ploidy PLOIDY\n                        ploidy, must be 1 or 2, defaults to 2\n  --soup_out SOUP_OUT   soup output\n  --vcf_out VCF_OUT     vcf output\n  --output_dir OUTPUT_DIR\n                        output directory\n  -v VCF, --vcf VCF     vcf file from which alt and ref matrix were created\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSo generally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econsensus.py -c clusters.tsv -a alt.mtx -r ref.mtx --soup_out soup.txt -v \u0026lt;freebayes vcf\u0026gt; --vcf_out cluster_genotypes.vcf --output_dir .\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 62,
    "subscribers_count": 9,
    "topics": [
      "scrna-seq",
      "scrnaseq",
      "scrna-seq-analysis",
      "bioinformatics",
      "computational-biology",
      "genomics"
    ],
    "updated_at": 1621879001.0
  },
  {
    "data_format": 2,
    "description": "Pegasus Workflow Management System - Automate, recover, and debug scientific computations.",
    "filenames": [
      "share/pegasus/init/population/Singularity",
      "test/core/044-singularity-nonsharedfs-minimal/image/Singularity"
    ],
    "full_name": "pegasus-isi/pegasus",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"doc/sphinx/images/pegasusfront-black-reduced.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"doc/sphinx/images/pegasusfront-black-reduced.png\" alt=\"Pegasus\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pegasus-workflow-management-system\" class=\"anchor\" href=\"#pegasus-workflow-management-system\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePegasus Workflow Management System\u003c/h2\u003e\n\u003cp\u003ePegasus WMS is a configurable system for mapping and executing scientific\nworkflows over a wide range of computational infrastructures including laptops,\ncampus clusters, supercomputers, grids, and commercial and academic clouds.\nPegasus has been used to run workflows with up to 1 million tasks that process\ntens of terabytes of data at a time.\u003c/p\u003e\n\u003cp\u003ePegasus WMS bridges the scientific domain and the execution environment by\nautomatically mapping high-level workflow descriptions onto distributed\nresources. It automatically locates the necessary input data and computational\nresources required by a workflow, and plans out all of the required data\ntransfer and job submission operations required to execute the workflow.\nPegasus enables scientists to construct workflows in abstract terms without\nworrying about the details of the underlying execution environment or the\nparticulars of the low-level specifications required by the middleware (Condor,\nGlobus, Amazon EC2, etc.). In the process, Pegasus can plan and optimize the\nworkflow to enable efficient, high-performance execution of large\nworkflows on complex, distributed infrastructures.\u003c/p\u003e\n\u003cp\u003ePegasus has a number of features that contribute to its usability and\neffectiveness:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePortability / Reuse \u2013 User created workflows can easily be run in different\nenvironments without alteration. Pegasus currently runs workflows on top of\nCondor pools, Grid infrastructures such as Open Science Grid and XSEDE,\nAmazon EC2, Google Cloud, and HPC clusters. The same workflow can run on a\nsingle system or across a heterogeneous set of resources.\u003c/li\u003e\n\u003cli\u003ePerformance \u2013 The Pegasus mapper can reorder, group, and prioritize tasks in\norder to increase overall workflow performance.\u003c/li\u003e\n\u003cli\u003eScalability \u2013 Pegasus can easily scale both the size of the workflow, and\nthe resources that the workflow is distributed over. Pegasus runs workflows\nranging from just a few computational tasks up to 1 million. The number of\nresources involved in executing a workflow can scale as needed without any\nimpediments to performance.\u003c/li\u003e\n\u003cli\u003eProvenance \u2013 By default, all jobs in Pegasus are launched using the\nKickstart wrapper that captures runtime provenance of the job and helps in\ndebugging. Provenance data is collected in a database, and the data can be\nqueried with tools such as pegasus-statistics, pegasus-plots, or directly\nusing SQL.\u003c/li\u003e\n\u003cli\u003eData Management \u2013 Pegasus handles replica selection, data transfers and\noutput registration in data catalogs. These tasks are added to a workflow as\nauxilliary jobs by the Pegasus planner.\u003c/li\u003e\n\u003cli\u003eReliability \u2013 Jobs and data transfers are automatically retried in case of\nfailures. Debugging tools such as pegasus-analyzer help the user to debug the\nworkflow in case of non-recoverable failures.\u003c/li\u003e\n\u003cli\u003eError Recovery \u2013 When errors occur, Pegasus tries to recover when possible\nby retrying tasks, by retrying the entire workflow, by providing workflow-level\ncheckpointing, by re-mapping portions of the workflow, by trying alternative\ndata sources for staging data, and, when all else fails, by providing a rescue\nworkflow containing a description of only the work that remains to be done.\nIt cleans up storage as the workflow is executed so that data-intensive\nworkflows have enough space to execute on storage-constrained resources.\nPegasus keeps track of what has been done (provenance) including the locations\nof data used and produced, and which software was used with which parameters.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eYou can find more information about Pegasus on the \u003ca href=\"http://pegasus.isi.edu\" rel=\"nofollow\"\u003ePegasus Website\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePegasus has an extensive \u003ca href=\"http://pegasus.isi.edu/documentation/\" rel=\"nofollow\"\u003eUser Guide\u003c/a\u003e\nthat documents how to create, plan, and monitor workflows.\u003c/p\u003e\n\u003cp\u003eWe recommend you start by completing the Pegasus Tutorial from \u003ca href=\"https://pegasus.isi.edu/documentation/user-guide/tutorial.html\" rel=\"nofollow\"\u003eChapter 3 of the\nPegasus User Guide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe easiest way to install Pegasus is to use one of the binary packages\navailable on the \u003ca href=\"http://pegasus.isi.edu/downloads\" rel=\"nofollow\"\u003ePegasus downloads page\u003c/a\u003e.\nConsult \u003ca href=\"https://pegasus.isi.edu/documentation/user-guide/installation.html\" rel=\"nofollow\"\u003eChapter 2 of the Pegasus User Guide\u003c/a\u003e\nfor more information about installing Pegasus from binary packages.\u003c/p\u003e\n\u003cp\u003eThere is documentation on the Pegasus website for the Python, Java and R\n\u003ca href=\"https://pegasus.isi.edu/documentation/reference-guide/api-reference.html\" rel=\"nofollow\"\u003eAbstract Workflow Generator APIs\u003c/a\u003e.\nWe strongly recommend using the Python API which is feature complete, and also\nallows you to invoke all the pegasus command line tools.\u003c/p\u003e\n\u003cp\u003eYou can use \u003cem\u003epegasus-init\u003c/em\u003e command line tool to run several examples\non your local machine. Consult \u003ca href=\"https://pegasus.isi.edu/documentation/user-guide/example-workflows.html\" rel=\"nofollow\"\u003eChapter 4 of the Pegasus\nUser Guide\u003c/a\u003e\nfor more information.\u003c/p\u003e\n\u003cp\u003eThere are also examples of how to \u003ca href=\"https://pegasus.isi.edu/documentation/user-guide/execution-environments.html\" rel=\"nofollow\"\u003eConfigure Pegasus for Different Execution\nEnvironments\u003c/a\u003e\nin the Pegasus User Guide.\u003c/p\u003e\n\u003cp\u003eIf you need help using Pegasus, please contact us. See the [contact page]\n(\u003ca href=\"http://pegasus.isi.edu/contact\" rel=\"nofollow\"\u003ehttp://pegasus.isi.edu/contact\u003c/a\u003e) on the Pegasus website for more information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-from-source\" class=\"anchor\" href=\"#building-from-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding from Source\u003c/h2\u003e\n\u003cp\u003ePegasus can be compiled on any recent Linux or Mac OS X system.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-source-dependencies\" class=\"anchor\" href=\"#source-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSource Dependencies\u003c/h3\u003e\n\u003cp\u003eIn order to build Pegasus from source, make sure you have the following installed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGit\u003c/li\u003e\n\u003cli\u003eJava 8 or higher\u003c/li\u003e\n\u003cli\u003ePython 3.5 or higher\u003c/li\u003e\n\u003cli\u003eR\u003c/li\u003e\n\u003cli\u003eAnt\u003c/li\u003e\n\u003cli\u003egcc\u003c/li\u003e\n\u003cli\u003eg++\u003c/li\u003e\n\u003cli\u003emake\u003c/li\u003e\n\u003cli\u003etox 3.14.5 or higher\u003c/li\u003e\n\u003cli\u003emysql (optional, required to access MySQL databases)\u003c/li\u003e\n\u003cli\u003epostgresql (optional, required to access PostgreSQL databases)\u003c/li\u003e\n\u003cli\u003ePython pyyaml\u003c/li\u003e\n\u003cli\u003ePython GitPython\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOther packages may be required to run unit tests, and build MPI tools.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-compiling\" class=\"anchor\" href=\"#compiling\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompiling\u003c/h3\u003e\n\u003cp\u003eAnt is used to compile Pegasus.\u003c/p\u003e\n\u003cp\u003eTo get a list of build targets run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ant -p\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe targets that begin with \"dist\" are what you want to use.\u003c/p\u003e\n\u003cp\u003eTo build a basic binary tarball (excluding documentation), run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ant dist\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo build the release tarball (including documentation), run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ant dist-release\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe resulting packages will be created in the \u003ccode\u003edist\u003c/code\u003e subdirectory.\u003c/p\u003e\n",
    "stargazers_count": 114,
    "subscribers_count": 31,
    "topics": [
      "workflow",
      "workflow-management-system",
      "bioinformatics",
      "distributed-systems",
      "hpc"
    ],
    "updated_at": 1621923885.0
  },
  {
    "data_format": 2,
    "description": "MRtrix3 provides a set of tools to perform various advanced diffusion MRI analyses, including constrained spherical deconvolution (CSD), probabilistic tractography, track-density imaging, and apparent fibre density",
    "filenames": [
      "Singularity"
    ],
    "full_name": "MRtrix3/mrtrix3",
    "latest_release": "3.0.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mrtrix\" class=\"anchor\" href=\"#mrtrix\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMRtrix\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/MRtrix3/mrtrix3/actions\"\u003e\u003cimg src=\"https://github.com/MRtrix3/mrtrix3/workflows/checks/badge.svg\" alt=\"Build Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMRtrix3\u003c/em\u003e can be installed / run through multiple avenues:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.mrtrix.org/download/\" rel=\"nofollow\"\u003eDirect download\u003c/a\u003e through mechanisms tailored for different OS platforms;\u003c/li\u003e\n\u003cli\u003eCompiled from the source code in this repository, for which \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/installation/build_from_source.html\" rel=\"nofollow\"\u003ecomprehensive instructions\u003c/a\u003e are provided in the \u003ca href=\"https://mrtrix.readthedocs.io/en/\" rel=\"nofollow\"\u003eonline documentation\u003c/a\u003e;\u003c/li\u003e\n\u003cli\u003eVia containerisation technology using Docker or Singularity; see \u003ca href=\"https://mrtrix.readthedocs.org/en/latest/installation/using_containers.html\" rel=\"nofollow\"\u003eonline documentation page\u003c/a\u003e for details.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting help\u003c/h2\u003e\n\u003cp\u003eInstructions on software setup and use are provided in the \u003ca href=\"https://mrtrix.readthedocs.org\" rel=\"nofollow\"\u003eonline documentation\u003c/a\u003e.\nSupport and general discussion is hosted on the \u003ca href=\"http://community.mrtrix.org/\" rel=\"nofollow\"\u003e\u003cem\u003eMRtrix3\u003c/em\u003e Community Forum\u003c/a\u003e.\nPlease also look through the Frequently Asked Questions on the \u003ca href=\"http://community.mrtrix.org/c/wiki\" rel=\"nofollow\"\u003ewiki section of the forum\u003c/a\u003e.\nYou can address all \u003cem\u003eMRtrix3\u003c/em\u003e-related queries there, using your GitHub or Google login to post questions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-install\" class=\"anchor\" href=\"#quick-install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick install\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eInstall dependencies by whichever means your system uses.\nThese include: Python (\u0026gt;=2.6), a C++ compiler with full C++11 support (\u003ccode\u003eg++\u003c/code\u003e 4.9 or later, \u003ccode\u003eclang++\u003c/code\u003e),\nEigen (\u0026gt;=3.2.8), zlib, OpenGL (\u0026gt;=3.3), and Qt (\u0026gt;=4.8, or at least 5.1 on MacOSX).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eClone Git repository and compile:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ git clone https://github.com/MRtrix3/mrtrix3.git\n $ cd mrtrix3/\n $ ./configure\n $ ./build\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSet the \u003ccode\u003ePATH\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBash shell:\u003c/p\u003e\n\u003cp\u003erun the \u003ccode\u003eset_path\u003c/code\u003e script provided:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  $ ./set_path\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor edit the startup \u003ccode\u003e~/.bashrc\u003c/code\u003e or \u003ccode\u003e/etc/bash.bashrc\u003c/code\u003e file manually by adding this line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  $ export PATH=/\u0026lt;edit as appropriate\u0026gt;/mrtrix3/bin:$PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eC shell:\u003c/p\u003e\n\u003cp\u003eedit the startup \u003ccode\u003e~/.cshrc\u003c/code\u003e or \u003ccode\u003e/etc/csh.cshrc\u003c/code\u003e file manually by adding this line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  $ setenv PATH /\u0026lt;edit as appropriate\u0026gt;/mrtrix3/bin:$PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTest installation:\u003c/p\u003e\n\u003cp\u003eCommand-line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ mrconvert\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGUI:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ mrview\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-keeping-mrtrix3-up-to-date\" class=\"anchor\" href=\"#keeping-mrtrix3-up-to-date\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKeeping MRtrix3 up to date\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eYou can update your installation at any time by opening a terminal in the mrtrix3 folder, and typing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git pull\n ./build\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf this doesn\u0027t work immediately, it may be that you need to re-run the configure script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e ./configure\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand re-run step 1 again.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-a-specific-release-of-mrtrix3\" class=\"anchor\" href=\"#building-a-specific-release-of-mrtrix3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding a specific release of MRtrix3\u003c/h2\u003e\n\u003cp\u003eYou can build a particular release of MRtrix3 by checking out the corresponding \u003cem\u003etag\u003c/em\u003e, and using the same procedure as above to build it:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit checkout 3.0_RC3\n./configure\n./build\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eThank you for your interest in contributing to \u003cem\u003eMRtrix3\u003c/em\u003e! Please read on \u003ca href=\"CONTRIBUTING.md\"\u003ehere\u003c/a\u003e to find out how to report issues, request features and make direct contributions.\u003c/p\u003e\n",
    "stargazers_count": 181,
    "subscribers_count": 35,
    "topics": [],
    "updated_at": 1621885646.0
  },
  {
    "data_format": 2,
    "description": "A Learning Environment for Theorem Proving with the Coq proof assistant",
    "filenames": [
      "Singularity"
    ],
    "full_name": "princeton-vl/CoqGym",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coqgym\" class=\"anchor\" href=\"#coqgym\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCoqGym\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"images/example_proof.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/example_proof.jpg\" alt=\"Example proof\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCode for the paper:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1905.09381\" rel=\"nofollow\"\u003eLearning to Prove Theorems via Interacting with Proof Assistants\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://www.cs.princeton.edu/~kaiyuy/\" rel=\"nofollow\"\u003eKaiyu Yang\u003c/a\u003e and \u003ca href=\"https://www.cs.princeton.edu/~jiadeng/\" rel=\"nofollow\"\u003eJia Deng\u003c/a\u003e\u003cbr\u003e\nInternational Conference on Machine Learning (ICML) 2019\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{yang2019coqgym,\n  title={Learning to Prove Theorems via Interacting with Proof Assistants},\n  author={Yang, Kaiyu and Deng, Jia},\n  booktitle={International Conference on Machine Learning},\n  year={2019},\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor potential bugs, please open an issue. For any other questions, please ask in \u003ca href=\"https://github.com/princeton-vl/CoqGym/discussions\"\u003eDiscussions\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"#1-installing-coqgym\"\u003eInstalling CoqGym\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  1.1 \u003ca href=\"#11-dependencies\"\u003eDependencies\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  1.2 \u003ca href=\"#12-building-coq-serapi-coqhammer-and-the-coq-projects\"\u003eBuilding Coq, SerAPI, CoqHammer, and the Coq Projects\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  1.3 \u003ca href=\"#13-extracting-the-proofs-from-coq-code-optional\"\u003eExtracting the Proofs from Coq Code\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  1.4 \u003ca href=\"#14-downloading-the-pre-extracted-proofs-recommended\"\u003eDownloading the Pre-extracted Proofs\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#2-using-coqgym-in-a-container\"\u003eUsing CoqGym in a Container\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  2.1 \u003ca href=\"#21-dependencies\"\u003eDependencies\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  2.2 \u003ca href=\"#22-downloading-the-pre-built-container-image\"\u003eDownloading the Pre-built Container Image\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  2.3 \u003ca href=\"#23-using-the-container\"\u003eUsing the Container\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  2.4 \u003ca href=\"#24-building-the-container-by-yourself\"\u003eBuilding the Container by Yourself\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#3-data-format\"\u003eData Format\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  3.1 \u003ca href=\"#31-json-files\"\u003eJSON Files\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  3.2 \u003ca href=\"#32-lmdb-file\"\u003eLMDB File\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  3.3 \u003ca href=\"#33-glossary\"\u003eGloassary\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#4-data-utilities\"\u003eData Utilities\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  4.1 \u003ca href=\"#41-interacting-with-coqgym\"\u003eInteracting with CoqGym\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  4.2 \u003ca href=\"#42-parsing-coq-terms\"\u003eParsing Coq Terms\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  4.3 \u003ca href=\"#43-computing-dataset-statistics\"\u003eComputing Dataset Statistics\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#5-the-astactic-model\"\u003eThe ASTactic Model\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  5.1 \u003ca href=\"#51-prerequisites\"\u003ePrerequisites\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  5.2 \u003ca href=\"#52-extracting-proof-steps\"\u003eExtracting Proof Steps\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  5.3 \u003ca href=\"#53-training\"\u003eTraining\u003c/a\u003e\u003cbr\u003e\n\u00a0 \u00a0  5.4 \u003ca href=\"#54-testing\"\u003eTesting\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#6-credits\"\u003eCredits\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#7-contributing\"\u003eContributing\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-1-installing-coqgym\" class=\"anchor\" href=\"#1-installing-coqgym\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Installing CoqGym\u003c/h2\u003e\n\u003cp\u003eFollow these steps to obtain the CoqGym dataset and build the environment for interacting with it.\nAlternatively, you may also use CoqGym in a \u003ca href=\"#2-using-coqgym-in-a-container\"\u003econtainer\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-11-dependencies\" class=\"anchor\" href=\"#11-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.1 Dependencies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://opam.ocaml.org/\" rel=\"nofollow\"\u003eOPAM\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.anaconda.com/distribution/\" rel=\"nofollow\"\u003eAnaconda Python 3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://symas.com/lmdb/\" rel=\"nofollow\"\u003eLMDB\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.ruby-lang.org/en/\" rel=\"nofollow\"\u003eRuby\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-12-building-coq-serapi-coqhammer-and-the-coq-projects\" class=\"anchor\" href=\"#12-building-coq-serapi-coqhammer-and-the-coq-projects\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.2 Building Coq, SerAPI, CoqHammer, and the Coq Projects\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eCreate an OPAM switch for OCaml 4.07.1+flambda: \u003ccode\u003eopam switch create 4.07.1+flambda \u0026amp;\u0026amp; eval $(opam env)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eUpgrade the installed OPAM packages (optional): \u003ccode\u003eopam upgrade \u0026amp;\u0026amp; eval $(opam env)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eClone the repository: \u003ccode\u003egit clone https://github.com/princeton-vl/CoqGym\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eInstall Coq, SerAPI and CoqHammer: \u003ccode\u003ecd CoqGym \u0026amp;\u0026amp; source install.sh\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eBuild the Coq projects (can take a while): \u003ccode\u003ecd coq_projects \u0026amp;\u0026amp; make \u0026amp;\u0026amp; cd ..\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eCreate and activate the conda environment: \u003ccode\u003econda env create -f coq_gym.yml \u0026amp;\u0026amp; conda activate coq_gym\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cem\u003eNote\u003c/em\u003e: \u003ca href=\"https://github.com/coq/coq\"\u003eCoq\u003c/a\u003e, \u003ca href=\"https://github.com/ejgallego/coq-serapi\"\u003eSerAPI\u003c/a\u003e, \u003ca href=\"https://github.com/lukaszcz/coqhammer\"\u003eCoqHammer\u003c/a\u003e, and the Coq projects in \u003ca href=\"./coq_projects\"\u003ecoq_projects\u003c/a\u003e directory are indendent software projects with their own code repositories, but please follow the instructions above to build the specific versions we need.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-13-extracting-the-proofs-from-coq-code-optional\" class=\"anchor\" href=\"#13-extracting-the-proofs-from-coq-code-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.3 Extracting the Proofs from Coq Code (Optional)\u003c/h3\u003e\n\u003cp\u003eWe include the code for extracting CoqGym from Coq source code. However, it is not guaranteed to reproduce exactly the same data. The timeout and other miscellaneous errors during the data extraction may be machine-dependent. For example, a faster machine is likely to have fewer timeout errors and thus can extract more proofs.\nFor benchmark purpose, please download and use our pre-extracted version of CoqGym.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCheck all Coq files and locate the proofs:\u003cbr\u003e\nFor each \u003ccode\u003e*.meta\u003c/code\u003e file in \u003ccode\u003e./coq_projects/\u003c/code\u003e, run \u003ccode\u003epython check_proofs.py --file /path/to/*.meta\u003c/code\u003e\u003cbr\u003e\nNow you have generated a \u003ccode\u003e*.json\u003c/code\u003e file in \u003ccode\u003e./data/\u003c/code\u003e corresponding to each \u003ccode\u003e*.meta\u003c/code\u003e file. The \u003ccode\u003eproofs\u003c/code\u003e field of the JSON object is a list containing the proof names.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExtract the proofs:\u003cbr\u003e\nFor each \u003ccode\u003e*.meta\u003c/code\u003e file and each proof, run:\u003cbr\u003e\n\u003ccode\u003epython extract_proof.py --file /path/to/*.meta --proof $PROOF_NAME\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003epython extract_synthetic_proofs.py --file /path/to/*.meta --proof $PROOF_NAME\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePost-processing: \u003ccode\u003epython postprocess.py\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cem\u003eCaveat\u003c/em\u003e: The steps above are computationally expensive. When we say \"For each XXX, run \u003ccode\u003eYYY\u003c/code\u003e\", the tasks are embarrassingly parallel, which means you can run them in parallel in any order. We do not provide the code for that because it depends on a particular HPC infrastructure.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-14-downloading-the-pre-extracted-proofs-recommended\" class=\"anchor\" href=\"#14-downloading-the-pre-extracted-proofs-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1.4 Downloading the Pre-extracted Proofs (Recommended)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDownload the CoqGym dataset \u003ca href=\"https://drive.google.com/drive/folders/149m_17VkYYkl0kdSB4AI8zodCuTmPaA6?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUnzip the data and set the paths: \u003ccode\u003epython unzip_data.py\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cem\u003eCaveat\u003c/em\u003e: The second step sets the absolute paths in the data. You have to re-do it whenever the absolote path of the \u003ccode\u003edata/\u003c/code\u003e directory changes (e.g. after moving the entire repo to another directory).\u003c/p\u003e\n\u003cp\u003eNow you are ready to interact with CoqGym! Run \u003ccode\u003epython eval_env.py\u003c/code\u003e to check if it terminates normally without raising an error.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-using-coqgym-in-a-container\" class=\"anchor\" href=\"#2-using-coqgym-in-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Using CoqGym in a Container\u003c/h2\u003e\n\u003cp\u003eAs a less painful alternative to \u003ca href=\"#1-installing-coqgym\"\u003einstalling CoqGym\u003c/a\u003e from scratch, we provide a pre-built Singularity container.\nFeel free to skip these steps if you have finished installing CoqGym.\nCurrently we do not support GPUs for the container, therefore you have to complete the installation steps manually if you want to train models on CoqGym using GPUs.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-21-dependencies\" class=\"anchor\" href=\"#21-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.1 Dependencies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-22-downloading-the-pre-built-container-image\" class=\"anchor\" href=\"#22-downloading-the-pre-built-container-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.2 Downloading the Pre-built Container Image\u003c/h3\u003e\n\u003cp\u003eThe container image can be downloaded \u003ca href=\"https://drive.google.com/drive/folders/13Rwa5no6W4MwSvdjRrENAdDCQqTWhcqy?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-23-using-the-container\" class=\"anchor\" href=\"#23-using-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.3 Using the Container\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eStart a shell session inside the container: \u003ccode\u003esingularity shell coq_gym.simg\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003esource /.bashrc \u0026amp;\u0026amp; cd /CoqGym \u0026amp;\u0026amp; eval $(opam env) \u0026amp;\u0026amp; conda activate coq_gym\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou are now ready to use CoqGym! Try \u003ccode\u003epython eval_env.py\u003c/code\u003e to see if it terminates normally without raising an error.\u003cbr\u003e\nFor further instructions about how to use a Singularity container, please consult the documentation of Singularity.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-24-building-the-container-by-yourself\" class=\"anchor\" href=\"#24-building-the-container-by-yourself\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2.4 Building the Container by Yourself\u003c/h3\u003e\n\u003cp\u003eWe provide a \u003ca href=\"./Singularity\"\u003eSingularity recipe\u003c/a\u003e from which you can build the container by yourself.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eYou need to be on a Linux machine of which you have sudo privileges.\u003c/li\u003e\n\u003cli\u003eDownload the dataset \u003ca href=\"https://drive.google.com/drive/folders/149m_17VkYYkl0kdSB4AI8zodCuTmPaA6?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e and put the files in your \u003ccode\u003eCoqGym/\u003c/code\u003e directory.\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003esudo singularity build coq_gym.simg Singularity\u003c/code\u003e to build the container image \u003ccode\u003ecoq_gym.simg\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cem\u003eCaveat\u003c/em\u003e: If you run out of disk space when building the container, it may because your \u003ccode\u003e/tmp\u003c/code\u003e directory is not large enough. See \u003ca href=\"https://singularity.lbl.gov/build-environment#temporary-folders\" rel=\"nofollow\"\u003ehttps://singularity.lbl.gov/build-environment#temporary-folders\u003c/a\u003e for a workaround.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-3-data-format\" class=\"anchor\" href=\"#3-data-format\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Data Format\u003c/h2\u003e\n\u003cp\u003eThe dataset contains three parts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe \u003ccode\u003edata\u003c/code\u003e directory: \u003ccode\u003e*.json\u003c/code\u003e files corresponding to the \u003ccode\u003e*.v\u003c/code\u003e files in Coq source code, whose format is explained below. The \u003ccode\u003e*.json\u003c/code\u003e files contain all important information about the proofs: environment, local context, goals, tactics, proof trees, etc.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe \u003ccode\u003esexp_cache\u003c/code\u003e directory: A LMDB file that serves as an index for the S-expressions in \u003ccode\u003e*.json\u003c/code\u003e files. The \u003ccode\u003e*.json\u003c/code\u003e files contain keys for querying \u003ccode\u003esexp_cache\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eprojs_split.json\u003c/code\u003e: A JSON object containing the training/validation/testing split\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-31-json-files\" class=\"anchor\" href=\"#31-json-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3.1 JSON Files\u003c/h3\u003e\n\u003cp\u003eEach \u003ccode\u003e*.json\u003c/code\u003e file in \u003ccode\u003edata/\u003c/code\u003e corresponds to a Coq source file \u003ccode\u003e*.v\u003c/code\u003e in \u003ccode\u003ecoq_projects/\u003c/code\u003e. For example, \u003ccode\u003edata/StructTact/Assoc.json\u003c/code\u003e corresponds to \u003ca href=\"https://github.com/princeton-vl/CoqGym/blob/master/coq_projects/StructTact/Assoc.v\"\u003ecoq_projects/StructTact/Assoc.v\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe format of the JSON files is described below.\nThe hash codes are used as keys to query the LMDB \u003ccode\u003esexp_cache\u003c/code\u003e.\nConsult the \u003ca href=\"#33-glossary\"\u003eglossary\u003c/a\u003e for the terminology.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\n    \u0027filename\u0027: \u0027Assoc.v\u0027,            # the path of the Coq source file relative to the root directory of the Coq project\n    \u0027coq_project\u0027: \u0027StructTact\u0027,      # the name of the Coq project\n    \u0027vernac_cmds\u0027: [                  # a list of Coq commands [6] in the source file\n        [\u0027Cd \"$COQGYM_ROOT/coq_projects/StructTact\".\u0027, \u0027VernacChdir\u0027,  \u00273701e61f37b72b3e61788fce6317466b7bb92b55\u0027],     # [raw command, command type, command AST (in hash code)]\n        [\u0027Arguments a_equiv {_} {_} _ _ _.\u0027, \u0027VernacArguments\u0027, \u00276777d3c472595dae20427d0892ad03d38f70fde9\u0027],\n        ...\n        [\u0027Arguments a_equiv {_} {_} _ _ _.\u0027, \u0027VernacArguments\u0027, \u00276777d3c472595dae20427d0892ad03d38f70fde9\u0027],\n    ],\n   \u0027num_extra_cmds\u0027: 107,             # the code in the original Coq file starts at vernac_cmds[num_extra_cmds]\n   \u0027proofs\u0027: [                        # a list of human-written proofs\n       ...\n    ],\n    \u0027synthetic_proofs\u0027: [             # a list of synthetic proofs\n       ...\n    ],\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe format for a proof is as follows, taking the \u003ca href=\"https://github.com/princeton-vl/CoqGym/blob/9267aeb9acd82b7735d0228c4f366f7b843a852b/coq_projects/StructTact/Assoc.v#L47\"\u003e\u003ccode\u003eget_set_same\u003c/code\u003e\u003c/a\u003e in \u003ccode\u003ecoq_projects/StructTact/Assoc.v\u003c/code\u003e as an example.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\n    \u0027name\u0027: get_set_same,             # the name of the theorem\n    \u0027line_nb\u0027: 118,                   # the theorem is defined in $file_data[\u0027vernac_cmds\u0027][$line_nb]\n    \n    \u0027env_delta\u0027: {                          # the global environment relative to the previous proof in the same file\n        \u0027add\u0027 : {                           # entries that should be added to the environment\n            \u0027constants\u0027 : [\n                {\n                    \u0027physical_path\u0027: \u0027coq/theories/Arith/PeanoNat.vo:Nat.mul_wd\u0027,       # the unique identifier\n                    \u0027short_ident\u0027: \u0027PeanoNat.Nat.mul_wd\u0027,                               # the short identifier\n                    \u0027qualid\u0027: \u0027Coq.Arith.PeanoNat.Nat.mul_wd\u0027,             # the qualified identifier [1]\n                    \u0027type\u0027: \u0027@Morphisms.Proper (forall (_ : nat) (_ : nat), nat) (@Morphisms.respectful nat (forall _ : nat, nat) (@eq nat) (@Morphisms.respectful nat nat (@eq nat) (@eq nat))) Nat.mul\u0027,    # the type [7] of the constant\n                    \u0027sort\u0027: \u0027Prop\u0027,                                        # the sort [2] of the constant (the type of its type) [2]\n                    \u0027opaque\u0027: False,                                       # whether the constant is opaque or transparent [3]\n                    \u0027sexp\u0027: \u0027333b2895c8e62d21856476bf89fa9681c9058bb9\u0027     # the S-expression [4] of the constant produced by SerAPI\n            },   \n            ...\n            ],\n            \u0027inductives\u0027 : [\n                {\n                    \u0027physical_path\u0027 : \u0027coq/theories/Init/Wf.vo:Acc\u0027,\n                    \u0027blocks\u0027: [           # a list of blocks in a mutual inductive definition [5]. For regular inductive definitions (most cases), the list has length 1\n                        {\n                            \u0027short_ident\u0027: \u0027Acc\u0027,\n                            \u0027qualid\u0027: \u0027Coq.Init.Wf.Acc\u0027,\n                            \u0027constructors\u0027: [\n                                [\u0027Acc_intro\u0027, \u0027forall (A : Type) (R : forall (_ : A) (_ : A), Prop) (x : A) (_ : forall (y : A) (_ : R y x), _UNBOUND_REL_6 A R y), _UNBOUND_REL_5 A R x\u0027],      # [constructor name, constructor type]\n                                ...\n                            ]\n                        }\n                    ],\n                    \u0027is_record\u0027: False,\n                    \u0027sexp\u0027: \u002731537cb98179ad7d2de0dd2cc783b4672b34b25b\u0027\n            }\n            ...\n            \n            ],\n        },\n        \u0027subtract\u0027 : {                      # entries that should be removed from the environment\n            \u0027constants\u0027 : [],\n            \u0027inductives\u0027 : [],\n        },\n    },\n    \n    \u0027steps\u0027: [                        # a list of proof steps\n        {\n            \u0027command\u0027: [\u0027induction l; intros; simpl; repeat (break_match; simpl); subst; congruence.\u0027, \u0027VernacExtend\u0027, \u0027f6d2cb314d72d23562e5f2ef2657bd2589d44794\u0027],        # (raw command, command type, command AST (in hash code)) the Coq command (usually a tactic but also includes other commands such as +, -, *, etc.) \n            \u0027goal_ids\u0027: {             # the IDs of the goals in the current proof step\n                \u0027fg\u0027: [27],           # focused goals \n                \u0027bg\u0027: [] .            # unfocused goals\n            }\n        }\n        ...\n    ],      \n    \n    \u0027goals\u0027 {                         # the goals\n        27\u0027: {                        # $goal_id -\u0026gt; goal\n                 \u0027id\u0027: 27,            # the goal ID\n                 \u0027type\u0027: \u0027forall (k : K) (v : V) (l : list (prod K V)), @eq (option V) (assoc (assoc_set l k v) k) (@Some V v)\u0027,      # the type (logical statement) of the goal\n                 \u0027hypotheses\u0027: [      # the local context, a list of local premises\n                     {\u0027idents\u0027: [\u0027K\u0027, \u0027V\u0027], \u0027term\u0027: [], \u0027type\u0027: \u0027Type\u0027, \u0027sexp\u0027: \u0027cd1531c49fce6657997962b5375a3ef0a59db34a\u0027}       # {\u0027idents\u0027: [a list of identifiers of the premises (usually of length one)], \u0027term\u0027: [a list of Coq terms (usually empty)], \u0027type\u0027: the type (logical statement) of the premise}\n                     {\u0027idents\u0027: [\u0027K_eq_dec\u0027], \u0027term\u0027: [], \u0027type\u0027: \"forall k k\u0027 : K, sumbool (@eq K k k\u0027) (not (@eq K k k\u0027))\", \u0027sexp\u0027: \u00275f5f5bcf9e10621f8c0c4642c0eba3ff36cbfff8\u0027},\n                 ],\n             }\n    },    \n    \n    \u0027proof_tree\u0027 : {                  # the proof tree\n        \u0027goal_id\u0027: 27, \u0027children\u0027: []\n    },                     \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-32-lmdb-file\" class=\"anchor\" href=\"#32-lmdb-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3.2 LMDB File\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003esexp_cache\u003c/code\u003e is a LMDB mapping hash codes in \u003ccode\u003e*.json\u003c/code\u003e files to their corresponding S-expressions. Below is a code snippet in Python for accessing them.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eutils\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eSexpCache\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003esexp_cache\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eSexpCache\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\u0027sexp_cache\u0027\u003c/span\u003e)\n\u003cspan class=\"pl-en\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003esexp_cache\u003c/span\u003e[\u003cspan class=\"pl-s\"\u003e\u0027333b2895c8e62d21856476bf89fa9681c9058bb9\u0027\u003c/span\u003e])\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-33-glossary\" class=\"anchor\" href=\"#33-glossary\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3.3 Glossary\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#qualified-identifiers-and-simple-identifiers\" rel=\"nofollow\"\u003equalified identifier\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#sorts\" rel=\"nofollow\"\u003esort\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#sorts\" rel=\"nofollow\"\u003eopaque, transparent\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/S-expression\" rel=\"nofollow\"\u003eS-expression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#mutually-defined-inductive-types\" rel=\"nofollow\"\u003einductive definition, mutual inductive definition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#mutually-defined-inductive-types\" rel=\"nofollow\"\u003eCoq (vernac) command\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://coq.inria.fr/distrib/current/refman/language/gallina-specification-language.html#types\" rel=\"nofollow\"\u003etype\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-4-data-utilities\" class=\"anchor\" href=\"#4-data-utilities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. Data Utilities\u003c/h2\u003e\n\u003cp\u003eWe include some tools for interacting with CoqGym, but they are NOT a part of the dataset.\nYou may implement your own tools for similar purposes.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-41-interacting-with-coqgym\" class=\"anchor\" href=\"#41-interacting-with-coqgym\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4.1 Interacting with CoqGym\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eeval_env.py\u003c/code\u003e enables the interaction with the proofs in CoqGym.\nSee \u003ca href=\"ASTactic/agent.py\"\u003eASTactic/agent.py\u003c/a\u003e for examples.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-42-parsing-coq-terms\" class=\"anchor\" href=\"#42-parsing-coq-terms\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4.2 Parsing Coq Terms\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003egallina.py\u003c/code\u003e: a parser the S-expressions of Coq terms.\nIt may be useful for learning the embeddings of the term ASTs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eutils.py\u003c/code\u003e: functions for iterating through all proofs or Coq files in the dataset.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-43-computing-dataset-statistics\" class=\"anchor\" href=\"#43-computing-dataset-statistics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4.3 Computing Dataset Statistics\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003estats/count_human_proofs.py\u003c/code\u003e: count the number of human-written proofs in CoqGym.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003estats/count_synthetic_proofs.py\u003c/code\u003e: count the number of synthetic-written proofs in CoqGym.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003estats/proofs.py\u003c/code\u003e: compute some statistics of the proofs.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-5-the-astactic-model\" class=\"anchor\" href=\"#5-the-astactic-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5. The ASTactic Model\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/images/astactic.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/images/astactic.jpg\" alt=\"ASTactic\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHere we describe how to train and test the ASTactic model on CoqGym.\nThe following content is NOT a part of the CoqGym dataset, and therefore you do not need it if you only want to access the data.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-51-prerequisites\" class=\"anchor\" href=\"#51-prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5.1 Prerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMake sure CoqGym has been properly installed and configured. The \u003ccode\u003ecoq_gym\u003c/code\u003e conda environment is activated, the OPAM switch is on \u003ccode\u003e4.07.1+flambda\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eAutomated theorem provers: \u003ca href=\"https://vprover.github.io\" rel=\"nofollow\"\u003eVampire\u003c/a\u003e, \u003ca href=\"http://cvc4.cs.stanford.edu/\" rel=\"nofollow\"\u003eCVC4\u003c/a\u003e, \u003ca href=\"http://www.eprover.org\" rel=\"nofollow\"\u003eEprover\u003c/a\u003e, and \u003ca href=\"https://github.com/Z3Prover/z3\"\u003eZ3\u003c/a\u003e. Install all of them and make sure they are accessible in PATH, otherwise you may see a performance degradation of the hammer baseline.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://pytorch.org/\" rel=\"nofollow\"\u003ePyTorch\u003c/a\u003e: Install the correct version for your hardware in the conda environment \u003ccode\u003ecoq_gym\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eThe instructions below assume that you are in the \u003ca href=\"./ASTactic/\"\u003eASTactic\u003c/a\u003e directory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-52-extracting-proof-steps\" class=\"anchor\" href=\"#52-extracting-proof-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5.2 Extracting Proof Steps\u003c/h3\u003e\n\u003cp\u003eThe ASTactic model is trained on individual proof steps, rather than entire proofs.\nAfter obtaining the CoqGym dataset, run \u003ccode\u003epython extract_proof_steps.py\u003c/code\u003e. This can take a while, and you have the option to run it in parallel, please see the \u003ccode\u003e--filter\u003c/code\u003e option in the source code for details.\u003c/p\u003e\n\u003cp\u003eThe extracted proof steps are in \u003ccode\u003eproof_steps/\u003c/code\u003e. You can double-check the number of proof steps to make sure everything works as expected:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eDirectory\u003c/th\u003e\n\u003cth\u003e# files\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eproof_steps/train\u003c/td\u003e\n\u003ctd\u003e121,644\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eproof_steps/valid\u003c/td\u003e\n\u003ctd\u003e68,180\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-53-training\" class=\"anchor\" href=\"#53-training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5.3 Training\u003c/h3\u003e\n\u003cp\u003eTo train on the proof steps in training + validation set: \u003ccode\u003epython main.py --no_validation --exp_id astactic\u003c/code\u003e\u003cbr\u003e\nThe \"astactic\" above is an experiment ID, and you may change it to other IDs. Model checkpoints will be saved to \u003ccode\u003eruns/astactic/checkpoints/\u003c/code\u003e. See \u003ccode\u003eoptions.py\u003c/code\u003e for command line options.\u003c/p\u003e\n\u003cp\u003eA pre-trained model can be downloaded \u003ca href=\"https://drive.google.com/drive/folders/1AzLaEpoGS3BPMUz9Bl63MHAFRqlF4CtH?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-54-testing\" class=\"anchor\" href=\"#54-testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5.4 Testing\u003c/h3\u003e\n\u003cp\u003eAssuming you want to test the model checkpoint \u003ccode\u003eruns/astactic/checkpoints/model_003.pth\u003c/code\u003e on the proof \u003ccode\u003eget_set_same\u003c/code\u003e in \u003ccode\u003e../data/StructTact/Assoc.json\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTesting ASTactic:\u003cbr\u003e\n\u003ccode\u003epython evaluate.py ours ours-TEST --path runs/astactic/checkpoints/model_003.pth --file ../data/StructTact/Assoc.json --proof \"get_set_same\"\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTesting an automated tactic X (may be \"auto\", \"trivial\", \"easy\", \"intuition\", or \"hammer\"):\u003cbr\u003e\n\u003ccode\u003epython -u evaluate.py X X-TEST --file ../data/StructTact/Assoc.json --proof \"get_set_same\"\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTesting ASTactic+X:\u003cbr\u003e\n\u003ccode\u003epython -u evaluate.py ours+X ours+X-TEST --path runs/astactic/checkpoints/model_003.pth --file ../data/StructTact/Assoc.json --proof \"get_set_same\"\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eCaveat\u003c/em\u003e: Testing is computationally expensive, but the workloads are embarrassingly parallel, which means you can run them in parallel in any order. We do not provide the code for that because it depends on a particular HPC infrastructure.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-6-credits\" class=\"anchor\" href=\"#6-credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e6. Credits\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThe code is formatted using \u003ca href=\"https://github.com/psf/black\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\" alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThis repo includes the codebase of \u003ca href=\"https://github.com/coq/coq\"\u003eCoq\u003c/a\u003e, \u003ca href=\"https://github.com/ejgallego/coq-serapi\"\u003eSerAPI\u003c/a\u003e, \u003ca href=\"https://github.com/lukaszcz/coqhammer\"\u003eCoqHammer\u003c/a\u003e, and the Coq projects in \u003ca href=\"./coq_projects\"\u003ecoq_projects\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-7-contributing\" class=\"anchor\" href=\"#7-contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e7. Contributing\u003c/h2\u003e\n\u003cp\u003eWe welcome and appreciate contributions from the community. For bug fixes and relatively minor changes (such as comments, typos, etc.), feel free to submit a pull request directly. For anything beyond, please first post in \u003ca href=\"https://github.com/princeton-vl/CoqGym/discussions\"\u003eDiscussions\u003c/a\u003e before implementing.\u003c/p\u003e\n",
    "stargazers_count": 210,
    "subscribers_count": 16,
    "topics": [
      "theorem-proving",
      "machine-learning",
      "icml-2019"
    ],
    "updated_at": 1621517065.0
  },
  {
    "data_format": 2,
    "description": "HPC Container Maker",
    "filenames": [
      "recipes/hpccm/Singularity.def"
    ],
    "full_name": "NVIDIA/hpc-container-maker",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/NVIDIA/hpc-container-maker/actions?query=workflow%3A%22Python+3%22\"\u003e\u003cimg src=\"https://github.com/NVIDIA/hpc-container-maker/workflows/Python%203/badge.svg\" alt=\"Python 3\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/NVIDIA/hpc-container-maker/actions?query=workflow%3A%22Python+2%22\"\u003e\u003cimg src=\"https://github.com/NVIDIA/hpc-container-maker/workflows/Python%202/badge.svg\" alt=\"Python 2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/conda-forge/hpccm\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/90b084021ad87cd5d22b3c0a5eb48d35f96c19aed786c703e9c4b6f735e59479/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f636f6e64612d666f7267652f687063636d3f6c6162656c3d436f6e6461253230646f776e6c6f616473\" alt=\"Conda\" data-canonical-src=\"https://img.shields.io/conda/dn/conda-forge/hpccm?label=Conda%20downloads\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/hpccm/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fc8ae25e1ec1ff10f43999db82f62d053fd8f7d7fe735335cd87eae369f0640c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f687063636d3f6c6162656c3d50795049253230646f776e6c6f616473\" alt=\"PyPI - Downloads\" data-canonical-src=\"https://img.shields.io/pypi/dm/hpccm?label=PyPI%20downloads\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://lgtm.com/projects/g/NVIDIA/hpc-container-maker/context:python\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c5cb9f60bc18eec63ef91be12f6390a9eccaa2ebcccd607a526aa17076d011cb/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f4e56494449412f6870632d636f6e7461696e65722d6d616b65722e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138\" alt=\"Language grade: Python\" data-canonical-src=\"https://img.shields.io/lgtm/grade/python/g/NVIDIA/hpc-container-maker.svg?logo=lgtm\u0026amp;logoWidth=18\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/NVIDIA/hpc-container-maker/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c1dbf7021ca84175a5ff16f8b927fa120abad46a944de3ef991aeb54e1c8cba6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f4e56494449412f6870632d636f6e7461696e65722d6d616b6572\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/NVIDIA/hpc-container-maker\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-hpc-container-maker\" class=\"anchor\" href=\"#hpc-container-maker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPC Container Maker\u003c/h1\u003e\n\u003cp\u003eHPC Container Maker (HPCCM - pronounced H-P-see-M) is an open source\ntool to make it easier to generate container specification files.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"/docs\"\u003eDocumentation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/docs/getting_started.md\"\u003eGetting Started\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/tutorial.md\"\u003eTutorial\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/recipes.md\"\u003eRecipes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/workflows.md\"\u003eWorkflows\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/building_blocks.md\"\u003eAPI: Building Blocks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/primitives.md\"\u003eAPI: Primitives\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/misc_api.md\"\u003eAPI: Miscellaneous\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/recipes/\"\u003eExamples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/citation.md\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/LICENSE\"\u003eLicense\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eHPC Container Maker generates Dockerfiles or Singularity definition\nfiles from a high level Python recipe.  HPCCM recipes have some\ndistinct advantages over \"native\" container specification formats.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eA library of HPC \u003ca href=\"/docs/building_blocks.md\"\u003ebuilding blocks\u003c/a\u003e that\nseparate the choice of what to include in a container image from\nthe details of how it\u0027s done.  The building blocks transparently\nprovide the latest component and container best practices.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePython provides increased flexibility over static container\nspecification formats.  Python-based recipes can branch, validate\nuser input, etc. - the same recipe can generate multiple container\nspecifications.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGenerate either Dockerfiles or Singularity definition files from\nthe same recipe.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-additional-resources\" class=\"anchor\" href=\"#additional-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/HPCSYSPROS/Workshop18/blob/master/Making_Containers_Easier_with_HPC_Container_Maker/ws_hpcsysp103.pdf\"\u003eMaking Containers Easier With HPC Container Maker (paper)\u003c/a\u003e, presented at the \u003ca href=\"/docs/citation.md\"\u003eHPC Systems Professionals Workshop at SC18\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://on-demand.gputechconf.com/supercomputing/2018/video/sc1843-making-containers-easier-hpc-container-maker.html\" rel=\"nofollow\"\u003eOverview presentation at SC18 (video)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.nvidia.com/content/webinar-portal/src/webinar-portal.html?D2C=1802760\u0026amp;isSocialSharing=Y\u0026amp;partnerref=emailShareFromGateway\" rel=\"nofollow\"\u003eMaking Containers Easier with HPC Container Maker (webinar)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.admin-magazine.com/HPC/Articles/HPC-Container-Maker\" rel=\"nofollow\"\u003eADMIN Magazine article\u003c/a\u003e by Jeff Layton\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://devblogs.nvidia.com/making-containers-easier-with-hpc-container-maker/\" rel=\"nofollow\"\u003eNVIDIA Developer Blog\u003c/a\u003e by Scott McMillan\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 278,
    "subscribers_count": 22,
    "topics": [
      "containers",
      "hpc",
      "docker",
      "singularity"
    ],
    "updated_at": 1621304635.0
  },
  {
    "data_format": 2,
    "description": "Language Savant. If your repository\u0027s language is being reported incorrectly, send us a pull request!",
    "filenames": [
      "samples/Singularity/filenames/Singularity"
    ],
    "full_name": "github/linguist",
    "latest_release": "v7.14.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-linguist\" class=\"anchor\" href=\"#linguist\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLinguist\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/github/linguist/actions\"\u003e\u003cimg src=\"https://github.com/github/linguist/workflows/Run%20Tests/badge.svg\" alt=\"Actions Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis library is used on GitHub.com to detect blob languages, ignore binary or vendored files, suppress generated files in diffs, and generate language breakdown graphs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/docs/how-linguist-works.md\"\u003eHow Linguist works\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/overrides.md\"\u003eChange Linguist\u0027s behaviour with overrides\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"CONTRIBUTING.md\"\u003eContributing guidelines\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eInstall the gem:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egem install github-linguist\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h3\u003e\n\u003cp\u003eLinguist is a Ruby library so you will need a recent version of Ruby installed.\nThere are known problems with the macOS/XCode supplied version of Ruby that causes problems installing some of the dependencies.\nAccordingly, we highly recommend you install a version of Ruby using Homebrew, \u003ccode\u003erbenv\u003c/code\u003e, \u003ccode\u003ervm\u003c/code\u003e, \u003ccode\u003eruby-build\u003c/code\u003e, \u003ccode\u003easdf\u003c/code\u003e or other packaging system, before attempting to install Linguist and the dependencies.\u003c/p\u003e\n\u003cp\u003eLinguist uses \u003ca href=\"https://github.com/brianmario/charlock_holmes\"\u003e\u003ccode\u003echarlock_holmes\u003c/code\u003e\u003c/a\u003e for character encoding and \u003ca href=\"https://github.com/libgit2/rugged\"\u003e\u003ccode\u003erugged\u003c/code\u003e\u003c/a\u003e for libgit2 bindings for Ruby.\nThese components have their own dependencies.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003echarlock_holmes\n\u003cul\u003e\n\u003cli\u003ecmake\u003c/li\u003e\n\u003cli\u003epkg-config\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://site.icu-project.org/\" rel=\"nofollow\"\u003eICU\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://zlib.net/\" rel=\"nofollow\"\u003ezlib\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003erugged\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://curl.haxx.se/libcurl/\" rel=\"nofollow\"\u003elibcurl\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.openssl.org\" rel=\"nofollow\"\u003eOpenSSL\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou may need to install missing dependencies before you can install Linguist.\nFor example, on macOS with \u003ca href=\"http://brew.sh/\" rel=\"nofollow\"\u003eHomebrew\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ebrew install cmake pkg-config icu4c\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOn Ubuntu:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo apt-get install cmake pkg-config libicu-dev zlib1g-dev libcurl4-openssl-dev libssl-dev ruby-dev\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-application-usage\" class=\"anchor\" href=\"#application-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eApplication usage\u003c/h3\u003e\n\u003cp\u003eLinguist can be used in your application as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-ruby\"\u003e\u003cpre\u003e\u003cspan class=\"pl-en\"\u003erequire\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u0027rugged\u0027\u003c/span\u003e\n\u003cspan class=\"pl-en\"\u003erequire\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u0027linguist\u0027\u003c/span\u003e\n\n\u003cspan class=\"pl-s1\"\u003erepo\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eRugged\u003c/span\u003e::\u003cspan class=\"pl-v\"\u003eRepository\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e.\u003c/span\u003e\u003cspan class=\"pl-en\"\u003enew\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e(\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027.\u0027\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e)\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eproject\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eLinguist\u003c/span\u003e::\u003cspan class=\"pl-v\"\u003eRepository\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e.\u003c/span\u003e\u003cspan class=\"pl-en\"\u003enew\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e(\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003erepo\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e,\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erepo\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e.\u003c/span\u003e\u003cspan class=\"pl-en\"\u003ehead\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e.\u003c/span\u003e\u003cspan class=\"pl-en\"\u003etarget_id\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e)\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eproject\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e.\u003c/span\u003e\u003cspan class=\"pl-en\"\u003elanguage\u003c/span\u003e       \u003cspan class=\"pl-c\"\u003e#=\u0026gt; \"Ruby\"\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eproject\u003c/span\u003e\u003cspan class=\"pl-kos\"\u003e.\u003c/span\u003e\u003cspan class=\"pl-en\"\u003elanguages\u003c/span\u003e      \u003cspan class=\"pl-c\"\u003e#=\u0026gt; { \"Ruby\" =\u0026gt; 119387 }\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-line-usage\" class=\"anchor\" href=\"#command-line-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand line usage\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-git-repository\" class=\"anchor\" href=\"#git-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGit Repository\u003c/h4\u003e\n\u003cp\u003eA repository\u0027s languages stats can also be assessed from the command line using the \u003ccode\u003egithub-linguist\u003c/code\u003e executable.\nWithout any options, \u003ccode\u003egithub-linguist\u003c/code\u003e will output the breakdown that correlates to what is shown in the language stats bar.\nThe \u003ccode\u003e--breakdown\u003c/code\u003e flag will additionally show the breakdown of files by language.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e /path-to-repository/\ngithub-linguist\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can try running \u003ccode\u003egithub-linguist\u003c/code\u003e on the root directory in this repository itself:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003egithub-linguist --breakdown\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e68.57%  Ruby\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e22.90%  C\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e6.93%   Go\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e1.21%   Lex\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e0.39%   Shell\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003eRuby:\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eGemfile\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eRakefile\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ebin/git-linguist\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ebin/github-linguist\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eext/linguist/extconf.rb\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003egithub-linguist.gemspec\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003elib/linguist.rb\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e\u2026\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-single-file\" class=\"anchor\" href=\"#single-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingle file\u003c/h4\u003e\n\u003cp\u003eAlternatively you can find stats for a single file using the \u003ccode\u003egithub-linguist\u003c/code\u003e executable.\u003c/p\u003e\n\u003cp\u003eYou can try running \u003ccode\u003egithub-linguist\u003c/code\u003e on files in this repository itself:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003egithub-linguist grammars.yml\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003egrammars.yml: 884 lines (884 sloc)\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  type:      Text\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  mime type: text/x-yaml\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  language:  YAML\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h4\u003e\n\u003cp\u003eIf you have Docker installed you can build an image and run Linguist within a container:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003edocker build -t linguist \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/span\u003e\n$ \u003cspan class=\"pl-s1\"\u003edocker run --rm -v \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e -w \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e -t linguist\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e68.57%  Ruby\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e22.90%  C\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e6.93%   Go\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e1.21%   Lex\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e0.39%   Shell\u003c/span\u003e\n$ \u003cspan class=\"pl-s1\"\u003edocker run --rm -v \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e -w \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e -t linguist github-linguist --breakdown\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e68.57%  Ruby\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e22.90%  C\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e6.93%   Go\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e1.21%   Lex\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e0.39%   Shell\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003eRuby:\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eGemfile\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eRakefile\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ebin/git-linguist\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ebin/github-linguist\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eext/linguist/extconf.rb\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003egithub-linguist.gemspec\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003elib/linguist.rb\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e\u2026\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003ePlease check out our \u003ca href=\"CONTRIBUTING.md\"\u003econtributing guidelines\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe language grammars included in this gem are covered by their repositories\u0027 respective licenses.\n\u003ca href=\"/vendor/README.md\"\u003e\u003ccode\u003evendor/README.md\u003c/code\u003e\u003c/a\u003e lists the repository for each grammar.\u003c/p\u003e\n\u003cp\u003eAll other files are covered by the MIT license, see \u003ca href=\"./LICENSE\"\u003e\u003ccode\u003eLICENSE\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 8769,
    "subscribers_count": 385,
    "topics": [
      "syntax-highlighting",
      "language-grammars",
      "language-statistics",
      "linguistic"
    ],
    "updated_at": 1621976099.0
  }
]
